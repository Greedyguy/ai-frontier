#!/usr/bin/env python3
"""19ì¼ daily ìš”ì•½ ìƒì„± ìŠ¤í¬ë¦½íŠ¸"""

import sys
import os
from pathlib import Path
from datetime import datetime
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from ai_frontier.reporting.generator import ReportGenerator
from ai_frontier.main import ArxivReportingService

def generate_daily_summary_19():
    """19ì¼ daily ìš”ì•½ ìƒì„±"""
    
    print("=== 2025-09-19 Daily ìš”ì•½ ìƒì„± ===\n")
    
    try:
        # 19ì¼ ë…¼ë¬¸ íŒŒì¼ë“¤ ì°¾ê¸°
        papers_dir = Path("reports/individual_papers")
        date_pattern = "_20250919.md"
        
        # 19ì¼ ë…¼ë¬¸ íŒŒì¼ë“¤ í•„í„°ë§
        papers_19 = []
        for md_file in papers_dir.glob("*.md"):
            if date_pattern in md_file.name:
                papers_19.append(md_file)
        
        print(f"ğŸ“„ 2025-09-19 ë…¼ë¬¸ íŒŒì¼: {len(papers_19)}ê°œ")
        
        if not papers_19:
            print("âŒ 19ì¼ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        papers_info = []
        for paper_file in papers_19:
            try:
                with open(paper_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ì œëª© ì¶”ì¶œ
                title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
                title = title_match.group(1) if title_match else paper_file.stem
                
                # ArXiv ID ì¶”ì¶œ
                arxiv_match = re.search(r'\*\*ArXiv ID\*\*: \[([^\]]+)\]', content)
                arxiv_id = arxiv_match.group(1) if arxiv_match else ""
                
                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                category_match = re.search(r'\*\*Category\*\*: ([^\n]+)', content)
                category = category_match.group(1) if category_match else ""
                
                # ìš”ì•½ ì¶”ì¶œ
                summary_match = re.search(r'## ìš”ì•½\n\n(.*?)(?=\n## |$)', content, re.DOTALL)
                summary = summary_match.group(1).strip() if summary_match else ""
                
                papers_info.append({
                    'title': title,
                    'arxiv_id': arxiv_id,
                    'category': category,
                    'summary': summary,
                    'file_path': paper_file
                })
                
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {paper_file}: {e}")
        
        print(f"âœ… ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ: {len(papers_info)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        categories = {}
        for paper in papers_info:
            cat = paper['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(paper)
        
        print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜:")
        for cat, papers in categories.items():
            print(f"   - {cat}: {len(papers)}ê°œ")
        
        # Daily ìš”ì•½ ìƒì„±
        print(f"\nğŸ“ Daily ìš”ì•½ ìƒì„± ì¤‘...")
        
        # ìš”ì•½ í…œí”Œë¦¿ ìƒì„±
        summary_content = f"""# 2025-09-19 AI Frontier Daily Digest

## ğŸ“Š ìˆ˜ì§‘ í˜„í™©
- **ì´ ë…¼ë¬¸ ìˆ˜**: {len(papers_info)}ê°œ
- **ìˆ˜ì§‘ ì¼ì**: 2025ë…„ 9ì›” 19ì¼
- **ì¹´í…Œê³ ë¦¬ ìˆ˜**: {len(categories)}ê°œ

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ë³„ ë…¼ë¬¸ ë¶„í¬
"""
        
        for cat, papers in sorted(categories.items()):
            summary_content += f"- **{cat}**: {len(papers)}ê°œ\n"
        
        summary_content += f"""

## ğŸ”¥ ì£¼ìš” ë…¼ë¬¸ í•˜ì´ë¼ì´íŠ¸

### Computer Vision (cs.CV)
"""
        
        # cs.CV ë…¼ë¬¸ë“¤ ì¶”ê°€
        if 'cs.CV' in categories:
            cv_papers = categories['cs.CV'][:5]  # ìƒìœ„ 5ê°œ
            for paper in cv_papers:
                summary_content += f"- **{paper['title']}**\n"
                summary_content += f"  - ArXiv ID: {paper['arxiv_id']}\n"
                if paper['summary']:
                    summary_content += f"  - ìš”ì•½: {paper['summary'][:200]}...\n"
                summary_content += "\n"
        
        summary_content += f"""
### Artificial Intelligence (cs.AI)
"""
        
        # cs.AI ë…¼ë¬¸ë“¤ ì¶”ê°€
        if 'cs.AI' in categories:
            ai_papers = categories['cs.AI'][:5]  # ìƒìœ„ 5ê°œ
            for paper in ai_papers:
                summary_content += f"- **{paper['title']}**\n"
                summary_content += f"  - ArXiv ID: {paper['arxiv_id']}\n"
                if paper['summary']:
                    summary_content += f"  - ìš”ì•½: {paper['summary'][:200]}...\n"
                summary_content += "\n"
        
        summary_content += f"""
### Machine Learning (cs.LG)
"""
        
        # cs.LG ë…¼ë¬¸ë“¤ ì¶”ê°€
        if 'cs.LG' in categories:
            lg_papers = categories['cs.LG'][:5]  # ìƒìœ„ 5ê°œ
            for paper in lg_papers:
                summary_content += f"- **{paper['title']}**\n"
                summary_content += f"  - ArXiv ID: {paper['arxiv_id']}\n"
                if paper['summary']:
                    summary_content += f"  - ìš”ì•½: {paper['summary'][:200]}...\n"
                summary_content += "\n"
        
        summary_content += f"""
## ğŸ“ˆ ì—°êµ¬ ë™í–¥ ë¶„ì„

### ì£¼ìš” í‚¤ì›Œë“œ
- **Large Language Models (LLM)**: {len([p for p in papers_info if 'LLM' in p['title'] or 'Language Model' in p['title']])}ê°œ ë…¼ë¬¸
- **Computer Vision**: {len([p for p in papers_info if 'Vision' in p['title'] or 'Image' in p['title']])}ê°œ ë…¼ë¬¸
- **Reinforcement Learning**: {len([p for p in papers_info if 'Reinforcement' in p['title'] or 'RL' in p['title']])}ê°œ ë…¼ë¬¸
- **Multi-Agent Systems**: {len([p for p in papers_info if 'Multi-Agent' in p['title'] or 'Agent' in p['title']])}ê°œ ë…¼ë¬¸

### ì—°êµ¬ íŠ¸ë Œë“œ
1. **AI ì—ì´ì „íŠ¸ ë° ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**ì˜ ì§€ì†ì ì¸ ë°œì „
2. **ì»´í“¨í„° ë¹„ì „ê³¼ ìì—°ì–´ ì²˜ë¦¬ì˜ ìœµí•©** ì—°êµ¬ ì¦ê°€
3. **ê°•í™”í•™ìŠµì˜ ì‹¤ìš©ì  ì‘ìš©** í™•ëŒ€
4. **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ìµœì í™” ë° íš¨ìœ¨ì„±** ê°œì„ 

## ğŸ”— ê´€ë ¨ ë§í¬
- [ì „ì²´ ë…¼ë¬¸ ëª©ë¡](reports/individual_papers/)
- [ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜](reports/categories/)
- [í‚¤ì›Œë“œë³„ ë¶„ë¥˜](reports/keywords/)

---
*ì´ ìš”ì•½ì€ AI Frontier ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ìš”ì•½ íŒŒì¼ ì €ì¥
        output_file = Path("reports/digests/daily_digest_20250919.md")
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        print(f"âœ… Daily ìš”ì•½ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
        print(f"ğŸ“„ íŒŒì¼ í¬ê¸°: {len(summary_content)} ë¬¸ì")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = generate_daily_summary_19()
    if success:
        print(f"\nğŸ‰ 2025-09-19 Daily ìš”ì•½ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâŒ Daily ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


