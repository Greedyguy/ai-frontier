# Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs

**Korean Title:** ì„ë² ë””ë“œ FPGAì—ì„œ Tiny Transformerë¥¼ í™œìš©í•œ ë‹¤ëª©ì  ì‹œê³„ì—´ ë¶„ì„ ìë™í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/specific/Quantization-Aware Training|Quantization-Aware Training]] [[keywords/specific/Hardware-Aware Hyperparameter Search|Hardware-Aware Hyperparameter Search]] [[keywords/broad/Transformer|Transformer]] [[keywords/broad/Time-Series Analysis|Time-Series Analysis]] [[keywords/unique/Tiny Transformers|Tiny Transformers]] [[categories/cs.LG|cs.LG]] [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (80.7% similar) [[2025-09-19/MaRVIn_ A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration_20250919|MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration]] (79.9% similar) [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (79.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Quantization-Aware Training, Hardware-Aware Hyperparameter Search
**ğŸ”¬ Broad Technical**: Transformer, Time-Series Analysis
**â­ Unique Technical**: Tiny Transformers
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (80.7% similar)
- [[2025-09-19/MaRVIn_ A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration_20250919|MaRVIn A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration]] (79.9% similar)
- [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (79.0% similar)
- [[2025-09-19/Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization_20250919|Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization]] (78.9% similar)
- [[2025-09-19/Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention A Scalable Multilevel Attention Mechanism for Text and Images]] (78.8% similar)


**ArXiv ID**: [2505.17662](https://arxiv.org/abs/2505.17662)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2505.17662.pdf)


**ArXiv ID**: [2505.17662](https://arxiv.org/abs/2505.17662)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2505.17662.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Quantization-Aware Training, Hardware-Aware Hyperparameter Search
**â­ Unique Technical**: Tiny Transformers
**ğŸ”¬ Broad Technical**: Transformer, Time-Series Analysis

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Transformer` â€¢ 

`Time-Series Analysis` â€¢ 

`Quantization-Aware Training` â€¢ 

`Hardware-Aware Hyperparameter Search` â€¢ 

`Tiny Transformers`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.17662v5 Announce Type: replace 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.17662v5 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: Transformer ê¸°ë°˜ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‹œê³„ì—´ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ë†’ì€ ë©”ëª¨ë¦¬ ë° ê³„ì‚° ìš”êµ¬ë¡œ ì¸í•´ ìì›ì´ ì œí•œëœ ì¥ì¹˜ì—ì„œì˜ ë°°í¬ëŠ” ì—¬ì „íˆ ì–´ë ¤ìš´ ê³¼ì œì…ë‹ˆë‹¤. ë§ˆì´í¬ë¡œì»¨íŠ¸ë¡¤ëŸ¬ ìœ ë‹›(MCU)ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì´ì „ ì—°êµ¬ì—ì„œëŠ” í•˜ë“œì›¨ì–´ íŠ¹ì • ìµœì í™”ë¥¼ íƒêµ¬í–ˆì§€ë§Œ, ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ì¢…ì¢… ì‘ì—…ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©° 8ë¹„íŠ¸ ê³ ì • ì†Œìˆ˜ì  ì •ë°€ë„ì— ì œí•œë©ë‹ˆë‹¤. í•„ë“œ í”„ë¡œê·¸ë˜ë¨¸ë¸” ê²Œì´íŠ¸ ì–´ë ˆì´(FPGA)ëŠ” ë°ì´í„° ì •ë°€ë„ì™€ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì„¸ë°€í•œ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë” í° ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ê¸°ì¡´ì˜ FPGA ê¸°ë°˜ Transformer ë°°í¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ë™ êµ¬ì„±ì˜ ê³ ë°€ë„ í”Œë«í¼ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì„ë² ë””ë“œ FPGAì—ì„œ Tiny Transformerì˜ í†µí•©ë˜ê³  ì™„ì „ ìë™í™”ëœ ë°°í¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ì‹œê³„ì—´ ì‘ì—…(ì˜ˆì¸¡, ë¶„ë¥˜, ì´ìƒ íƒì§€)ì— ê±¸ì³ ì»´íŒ©íŠ¸í•œ ì¸ì½”ë” ì „ìš© Transformer ì•„í‚¤í…ì²˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ëŠ” 4ë¹„íŠ¸ê¹Œì§€ì˜ ì–‘ìí™” ì¸ì‹ í›ˆë ¨, Optunaë¥¼ ì‚¬ìš©í•œ í•˜ë“œì›¨ì–´ ì¸ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰, ì›í™œí•œ ë°°í¬ë¥¼ ìœ„í•œ ìë™ VHDL ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°œì˜ ì„ë² ë””ë“œ FPGA í”Œë«í¼ì—ì„œ ì—¬ì„¯ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì„ í†µí•´ ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ê°€ AMD Spartan-7ì—ì„œ ë°€ë¦¬ì´ˆ ì§€ì—° ì‹œê°„ìœ¼ë¡œ ì¶”ë¡ ë‹¹ 0.033 mJë§Œí¼ ë‚®ì€ ì—ë„ˆì§€ë¥¼ ì†Œëª¨í•˜ëŠ” ì •ìˆ˜ ì „ìš©, ì‘ì—… íŠ¹í™” Transformer ê°€ì†ê¸°ë¥¼ ìƒì„±í•˜ë©°, Lattice iCE40ì—ì„œì˜ ë°°í¬ ê°€ëŠ¥ì„±ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œëŠ” GitHub ì €ì¥ì†Œ(https://github.com/Edwina1030/TinyTransformer4TS)ì— ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì„ë² ë””ë“œ FPGAì—ì„œì˜ Tiny Transformer ëª¨ë¸ ë°°í¬ë¥¼ ìœ„í•œ ìë™í™”ëœ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” 4ë¹„íŠ¸ê¹Œì§€ì˜ ì–‘ìí™” ì¸ì‹ í›ˆë ¨ê³¼ Optunaë¥¼ í™œìš©í•œ í•˜ë“œì›¨ì–´ ì¸ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰, ìë™ VHDL ìƒì„± ë“±ì„ ê²°í•©í•˜ì—¬ ì‹œê³„ì—´ ì˜ˆì¸¡, ë¶„ë¥˜, ì´ìƒ íƒì§€ ë“±ì˜ ì‘ì—…ì— ìµœì í™”ëœ ì»´íŒ©íŠ¸í•œ ì¸ì½”ë” ì „ìš© Transformer ì•„í‚¤í…ì²˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. AMD Spartan-7 í”Œë«í¼ì—ì„œ 0.033 mJì˜ ë‚®ì€ ì—ë„ˆì§€ ì†Œëª¨ì™€ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ì§€ì—° ì‹œê°„ì„ ë‹¬ì„±í–ˆìœ¼ë©°, Lattice iCE40ì—ì„œì˜ ë°°í¬ ê°€ëŠ¥ì„±ë„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì— ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Transformer ê¸°ë°˜ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‹œê³„ì—´ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìì› ì œì•½ì´ ìˆëŠ” ì¥ì¹˜ì— ë°°í¬í•˜ê¸°ì—ëŠ” ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ìš”êµ¬ê°€ ë†’ì•„ ë„ì „ì ì…ë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ MCUë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ì—°êµ¬ëŠ” í•˜ë“œì›¨ì–´ íŠ¹ì • ìµœì í™”ë¥¼ íƒêµ¬í–ˆì§€ë§Œ, ì´ëŠ” ì£¼ë¡œ ì‘ì—…ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©° 8ë¹„íŠ¸ ê³ ì • ì†Œìˆ˜ì  ì •ë°€ë„ì— ì œí•œë©ë‹ˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì€ ì„ë² ë””ë“œ FPGAì—ì„œ Tiny Transformerë¥¼ ìœ„í•œ í†µí•©ë˜ê³  ì™„ì „ ìë™í™”ëœ ë°°í¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

- 4. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” 4ë¹„íŠ¸ê¹Œì§€ì˜ ì–‘ìí™” ì¸ì‹ í›ˆë ¨, Optunaë¥¼ ì‚¬ìš©í•œ í•˜ë“œì›¨ì–´ ì¸ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰, ìë™ VHDL ìƒì„±ì„ ê²°í•©í•˜ì—¬ ë§¤ë„ëŸ¬ìš´ ë°°í¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

- 5. í”„ë ˆì„ì›Œí¬ëŠ” AMD Spartan-7ì—ì„œ 0.033 mJì˜ ë‚®ì€ ì—ë„ˆì§€ ì†Œë¹„ì™€ ë°€ë¦¬ì´ˆ ì§€ì—° ì‹œê°„ì„ ë‹¬ì„±í•˜ë©°, Lattice iCE40ì—ì„œì˜ ë°°í¬ ê°€ëŠ¥ì„±ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 15:59:07*