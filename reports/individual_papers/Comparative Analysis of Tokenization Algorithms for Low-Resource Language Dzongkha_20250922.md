# Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha

**Korean Title:** 저자원 언어 리소스가 적은 종카어를 위한 토큰화 알고리즘의 비교 분석

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Low-Resource Language Processing|Low-Resource Language Processing]] [[keywords/specific/Tokenization Algorithms|Tokenization Algorithms]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/unique/Dzongkha Tokenization|Dzongkha Tokenization]] [[categories/cs.CL|cs.CL]] [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (78.1% similar) [[2025-09-17/Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications_20250917|Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications]] (77.8% similar) [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (77.1% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Low-resource Language Processing
**🔗 Specific Connectable**: Tokenization Algorithms
**🔬 Broad Technical**: Natural Language Processing, Large Language Models
**⭐ Unique Technical**: Dzongkha Tokenization
## 🔗 유사한 논문
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony On Multilingual Data Allocation for Large Language Models Pretraining]] (78.1% similar)
- [[2025-09-17/Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications_20250917|Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications]] (77.8% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang A Systematic Comparison between Human and Machine-Generated Slang Usages]] (77.1% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (77.1% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM Language of Browsing]] (77.0% similar)


**ArXiv ID**: [2509.15255](https://arxiv.org/abs/2509.15255)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15255.pdf)


**ArXiv ID**: [2509.15255](https://arxiv.org/abs/2509.15255)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15255.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Low-Resource Language Processing
**🔗 Specific Connectable**: Tokenization Algorithms
**⭐ Unique Technical**: Dzongkha Tokenization
**🔬 Broad Technical**: Natural Language Processing, Large Language Models

## 🏷️ 추출된 키워드



`Natural Language Processing` • 

`Large Language Models` • 

`Tokenization Algorithms` • 

`Dzongkha Tokenization` • 

`Tailored Approaches for Low-Resource Languages`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15255v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are gaining popularity and improving rapidly. Tokenizers are crucial components of natural language processing, especially for LLMs. Tokenizers break down input text into tokens that models can easily process while ensuring the text is accurately represented, capturing its meaning and structure. Effective tokenizers enhance the capabilities of LLMs by improving a model's understanding of context and semantics, ultimately leading to better performance in various downstream tasks, such as translation, classification, sentiment analysis, and text generation. Most pre-trained tokenizers are suitable for high-resource languages like English but perform poorly for low-resource languages. Dzongkha, Bhutan's national language spoken by around seven hundred thousand people, is a low-resource language, and its linguistic complexity poses unique NLP challenges. Despite some progress, significant research in Dzongkha NLP is lacking, particularly in tokenization. This study evaluates the training and performance of three common tokenization algorithms in comparison to other popular methods. Specifically, Byte-Pair Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their suitability for Dzongkha. Performance was assessed using metrics like Subword Fertility, Proportion of Continued Words, Normalized Sequence Length, and execution time. The results show that while all three algorithms demonstrate potential, SentencePiece is the most effective for Dzongkha tokenization, paving the way for further NLP advancements. This underscores the need for tailored approaches for low-resource languages and ongoing research. In this study, we presented three tokenization algorithms for Dzongkha, paving the way for building Dzongkha Large Language Models.

## 🔍 Abstract (한글 번역)

arXiv:2509.15255v1 발표 유형: 신규  
초록: 대형 언어 모델(LLMs)은 인기를 얻고 있으며 빠르게 발전하고 있습니다. 토크나이저는 자연어 처리, 특히 LLMs에서 중요한 구성 요소입니다. 토크나이저는 입력 텍스트를 모델이 쉽게 처리할 수 있는 토큰으로 분해하여 텍스트가 정확하게 표현되고 그 의미와 구조를 포착할 수 있도록 합니다. 효과적인 토크나이저는 모델의 문맥 및 의미 이해를 개선하여 번역, 분류, 감정 분석 및 텍스트 생성과 같은 다양한 다운스트림 작업에서 더 나은 성능을 발휘할 수 있도록 LLMs의 기능을 향상시킵니다. 대부분의 사전 학습된 토크나이저는 영어와 같은 자원이 풍부한 언어에 적합하지만, 자원이 부족한 언어에서는 성능이 저조합니다. 약 70만 명이 사용하는 부탄의 국어인 종카어는 자원이 부족한 언어이며, 그 언어적 복잡성은 고유한 NLP 과제를 제기합니다. 일부 진전에도 불구하고, 특히 토큰화 분야에서 종카어 NLP에 대한 상당한 연구가 부족합니다. 이 연구는 다른 인기 있는 방법과 비교하여 세 가지 일반적인 토큰화 알고리즘의 학습 및 성능을 평가합니다. 구체적으로, Byte-Pair Encoding (BPE), WordPiece, SentencePiece (Unigram)이 종카어에 적합한지 평가되었습니다. 성능은 서브워드 비옥도, 연속된 단어의 비율, 정규화된 시퀀스 길이 및 실행 시간과 같은 지표를 사용하여 평가되었습니다. 결과는 세 가지 알고리즘 모두 잠재력을 보여주지만, SentencePiece가 종카어 토큰화에 가장 효과적임을 보여주며, 이는 추가적인 NLP 발전의 길을 열어줍니다. 이는 자원이 부족한 언어에 대한 맞춤형 접근 방식과 지속적인 연구의 필요성을 강조합니다. 본 연구에서는 종카어 대형 언어 모델 구축의 길을 열어주는 세 가지 토큰화 알고리즘을 제시하였습니다.

## 📝 요약

이 연구는 저자들이 부탄의 국가 언어인 종카어에 적합한 토크나이저를 평가한 것이다. 종카어는 저자원이 언어로, 기존의 토크나이저들이 효과적이지 않다. 연구에서는 Byte-Pair Encoding (BPE), WordPiece, SentencePiece (Unigram) 세 가지 알고리즘을 비교하여 그 성능을 평가했다. 평가 결과, SentencePiece가 종카어 토크나이제이션에 가장 효과적임을 발견했다. 이는 저자원 언어에 맞춘 접근법과 지속적인 연구의 필요성을 강조하며, 종카어 대형 언어 모델 개발의 기초를 마련한다.

## 🎯 주요 포인트


- 1. 토크나이저는 대형 언어 모델(LLM)의 성능을 향상시키며, 특히 문맥과 의미 이해를 개선합니다.

- 2. 대부분의 사전 훈련된 토크나이저는 고자원 언어에 적합하지만, 저자원 언어에는 성능이 떨어집니다.

- 3. Dzongkha는 저자원 언어로, 그 복잡한 언어적 특성 때문에 NLP 연구에 어려움이 있습니다.

- 4. Byte-Pair Encoding, WordPiece, SentencePiece 세 가지 토크나이저 알고리즘이 Dzongkha에 대해 평가되었습니다.

- 5. 연구 결과, SentencePiece가 Dzongkha 토크나이징에 가장 효과적이며, 저자원 언어를 위한 맞춤형 접근의 필요성을 강조합니다.


---

*Generated on 2025-09-22 16:18:43*