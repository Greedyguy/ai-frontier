# Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases

**Korean Title:** 사실이 끝나고 공정성이 시작되는 지점: 인지 편향을 통해 AI 편향 평가의 재정의

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Fact-Fair Trade-offs|Fact-Fair Trade-offs]] [[keywords/specific/Cognitive Biases|Cognitive Biases]] [[keywords/broad/AI Bias Evaluation|AI Bias Evaluation]] [[keywords/unique/Fact-or-Fair Benchmark|Fact-or-Fair Benchmark]] [[categories/cs.CL|cs.CL]] [[2025-09-22/Algorithmic Fairness_ Not a Purely Technical but Socio-Technical Property_20250922|Algorithmic Fairness: Not a Purely Technical but Socio-Technical Property]] (85.5% similar) [[2025-09-22/The Psychology of Falsehood_ A Human-Centric Survey of Misinformation Detection_20250922|The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection]] (83.2% similar) [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (82.6% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Fact-Fair Trade-offs
**🔗 Specific Connectable**: Fairness Benchmark
**🔬 Broad Technical**: AI Bias Evaluation, Cognitive Psychology
**⭐ Unique Technical**: Fact-or-Fair
## 🔗 유사한 논문
- [[2025-09-22/Algorithmic Fairness_ Not a Purely Technical but Socio-Technical Property_20250922|Algorithmic Fairness Not a Purely Technical but Socio-Technical Property]] (85.5% similar)
- [[2025-09-22/The Psychology of Falsehood_ A Human-Centric Survey of Misinformation Detection_20250922|The Psychology of Falsehood A Human-Centric Survey of Misinformation Detection]] (83.2% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (82.6% similar)
- [[2025-09-17/APFEx_ Adaptive Pareto Front Explorer for Intersectional Fairness_20250917|APFEx Adaptive Pareto Front Explorer for Intersectional Fairness]] (82.4% similar)
- [[2025-09-22/Understanding AI Evaluation Patterns_ How Different GPT Models Assess Vision-Language Descriptions_20250922|Understanding AI Evaluation Patterns How Different GPT Models Assess Vision-Language Descriptions]] (81.9% similar)


**ArXiv ID**: [2502.05849](https://arxiv.org/abs/2502.05849)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2502.05849.pdf)


**ArXiv ID**: [2502.05849](https://arxiv.org/abs/2502.05849)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2502.05849.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Fact-Fair Trade-offs
**🔗 Specific Connectable**: Cognitive Psychology
**⭐ Unique Technical**: Fact-or-Fair Benchmark
**🔬 Broad Technical**: AI Bias Evaluation

## 🏷️ 추출된 키워드



`AI Bias Evaluation` • 

`Cognitive Psychology` • 

`Fact-or-Fair Benchmark` • 

`Fact-Fair Trade-offs`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.05849v2 Announce Type: replace 
Abstract: Recent failures such as Google Gemini generating people of color in Nazi-era uniforms illustrate how AI outputs can be factually plausible yet socially harmful. AI models are increasingly evaluated for "fairness," yet existing benchmarks often conflate two fundamentally different dimensions: factual correctness and normative fairness. A model may generate responses that are factually accurate but socially unfair, or conversely, appear fair while distorting factual reality. We argue that identifying the boundary between fact and fair is essential for meaningful fairness evaluation. We introduce Fact-or-Fair, a benchmark with (i) objective queries aligned with descriptive, fact-based judgments, and (ii) subjective queries aligned with normative, fairness-based judgments. Our queries are constructed from 19 statistics and are grounded in cognitive psychology, drawing on representativeness bias, attribution bias, and ingroup-outgroup bias to explain why models often misalign fact and fairness. Experiments across ten frontier models reveal different levels of fact-fair trade-offs. By reframing fairness evaluation, we provide both a new theoretical lens and a practical benchmark to advance the responsible model assessments. Our test suite is publicly available at https://github.com/uclanlp/Fact-or-Fair.

## 🔍 Abstract (한글 번역)

arXiv:2502.05849v2 발표 유형: 교체  
초록: 구글 제미니가 나치 시대 유니폼을 입은 유색 인종을 생성하는 최근의 실패 사례는 AI 출력물이 사실적으로 그럴듯하지만 사회적으로 해로울 수 있음을 보여줍니다. AI 모델은 점점 더 "공정성"에 대해 평가되고 있지만, 기존의 벤치마크는 종종 사실적 정확성과 규범적 공정성이라는 두 가지 근본적으로 다른 차원을 혼동합니다. 모델은 사실적으로 정확하지만 사회적으로 불공정한 응답을 생성할 수 있으며, 반대로 사실적 현실을 왜곡하면서 공정해 보일 수도 있습니다. 우리는 의미 있는 공정성 평가를 위해 사실과 공정성의 경계를 식별하는 것이 필수적이라고 주장합니다. 우리는 (i) 기술적이고 사실 기반의 판단과 일치하는 객관적 질문과 (ii) 규범적이고 공정성 기반의 판단과 일치하는 주관적 질문을 포함하는 Fact-or-Fair라는 벤치마크를 소개합니다. 우리의 질문은 19개의 통계로 구성되어 있으며, 인지 심리학에 기반하여 대표성 편향, 귀인 편향, 내집단-외집단 편향을 활용하여 모델이 종종 사실과 공정성을 잘못 정렬하는 이유를 설명합니다. 10개의 최첨단 모델을 대상으로 한 실험은 사실과 공정성 간의 다양한 수준의 절충을 드러냅니다. 공정성 평가를 재구성함으로써 우리는 책임 있는 모델 평가를 진전시키기 위한 새로운 이론적 관점과 실용적인 벤치마크를 제공합니다. 우리의 테스트 스위트는 https://github.com/uclanlp/Fact-or-Fair에서 공개적으로 이용 가능합니다.

## 📝 요약

이 논문은 AI 모델의 공정성 평가에서 사실적 정확성과 규범적 공정성을 구분하는 것이 중요하다고 주장합니다. 기존의 평가 기준은 이 두 가지 차원을 혼동하는 경우가 많습니다. 이를 해결하기 위해 저자들은 Fact-or-Fair라는 벤치마크를 제안합니다. 이 벤치마크는 사실 기반의 객관적 질문과 공정성 기반의 주관적 질문으로 구성되어 있으며, 인지 심리학의 대표성 편향, 귀인 편향, 내집단-외집단 편향을 활용하여 AI 모델이 사실과 공정성을 잘못 정렬하는 이유를 설명합니다. 실험 결과, 10개의 최첨단 모델에서 사실과 공정성 간의 다양한 트레이드오프가 나타났습니다. 이 연구는 공정성 평가를 재구성하여 책임 있는 모델 평가를 위한 새로운 이론적 관점과 실용적 벤치마크를 제공합니다. Fact-or-Fair 테스트 스위트는 공개적으로 이용 가능합니다.

## 🎯 주요 포인트


- 1. AI 모델의 출력은 사실적으로 그럴듯하지만 사회적으로 해로울 수 있으며, 이는 사실적 정확성과 규범적 공정성을 혼동하는 기존 벤치마크의 문제를 드러낸다.

- 2. Fact-or-Fair 벤치마크는 사실 기반의 객관적 판단과 공정성 기반의 주관적 판단을 구분하여 평가하는 새로운 접근 방식을 제안한다.

- 3. 이 연구는 인지 심리학의 대표성 편향, 귀인 편향, 내집단-외집단 편향을 활용하여 모델이 사실과 공정성을 왜곡하는 이유를 설명한다.

- 4. 실험 결과, 10개의 최첨단 모델에서 사실과 공정성 간의 다양한 수준의 트레이드오프가 발견되었다.

- 5. 제안된 벤치마크는 책임 있는 모델 평가를 위한 새로운 이론적 관점과 실용적 도구를 제공하며, 공개적으로 이용 가능하다.


---

*Generated on 2025-09-22 16:33:52*