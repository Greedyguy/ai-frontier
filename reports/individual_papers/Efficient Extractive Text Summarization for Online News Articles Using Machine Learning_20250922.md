# Efficient Extractive Text Summarization for Online News Articles Using Machine Learning

**Korean Title:** ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ íš¨ìœ¨ì ì¸ ì¶”ì¶œì  í…ìŠ¤íŠ¸ ìš”ì•½ì„ ìœ„í•œ ê¸°ê³„ í•™ìŠµ ì‚¬ìš©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: BERT Embeddings, Long Short-Term Memory Networks

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Re-FRAME the Meeting Summarization SCOPE_ Fact-Based Summarization and Personalization via Questions_20250922|Re-FRAME the Meeting Summarization SCOPE Fact-Based Summarization and Personalization via Questions]] (81.1% similar)
- [[2025-09-22/Deep learning and abstractive summarisation for radiological reports_ an empirical study for adapting the PEGASUS models' family with scarce data_20250922|Deep learning and abstractive summarisation for radiological reports an empirical study for adapting the PEGASUS models' family with scarce data]] (80.7% similar)
- [[2025-09-19/MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE Metadata Extraction and Validation in Scientific Papers Using LLMs]] (80.4% similar)
- [[2025-09-22/CORE-RAG_ Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning_20250922|CORE-RAG Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning]] (79.9% similar)
- [[2025-09-19/When Content is Goliath and Algorithm is David_ The Style and Semantic Effects of Generative Search Engine_20250919|When Content is Goliath and Algorithm is David The Style and Semantic Effects of Generative Search Engine]] (79.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15614v1 Announce Type: new 
Abstract: In the age of information overload, content management for online news articles relies on efficient summarization to enhance accessibility and user engagement. This article addresses the challenge of extractive text summarization by employing advanced machine learning techniques to generate concise and coherent summaries while preserving the original meaning. Using the Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we developed a pipeline leveraging BERT embeddings to transform textual data into numerical representations. By framing the task as a binary classification problem, we explored various models, including logistic regression, feed-forward neural networks, and long short-term memory (LSTM) networks. Our findings demonstrate that LSTM networks, with their ability to capture sequential dependencies, outperform baseline methods like Lede-3 and simpler models in F1 score and ROUGE-1 metrics. This study underscores the potential of automated summarization in improving content management systems for online news platforms, enabling more efficient content organization and enhanced user experiences.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15614v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì •ë³´ ê³¼ë¶€í•˜ ì‹œëŒ€ì— ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì½˜í…ì¸  ê´€ë¦¬ëŠ” ì ‘ê·¼ì„±ê³¼ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ íš¨ìœ¨ì ì¸ ìš”ì•½ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ì›ë˜ì˜ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ê°„ê²°í•˜ê³  ì¼ê´€ëœ ìš”ì•½ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ê³ ê¸‰ ê¸°ê³„ í•™ìŠµ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì¶”ì¶œì  í…ìŠ¤íŠ¸ ìš”ì•½ì˜ ë„ì „ì„ ë‹¤ë£¹ë‹ˆë‹¤. 130ë§Œ ê°œì˜ ê¸°ì‚¬-ìš”ì•½ ìŒìœ¼ë¡œ êµ¬ì„±ëœ Cornell Newsroom ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” BERT ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ì  í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì„ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì„¤ì •í•˜ì—¬, ë¡œì§€ìŠ¤í‹± íšŒê·€, í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§, ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬ (LSTM) ë„¤íŠ¸ì›Œí¬ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ìˆœì°¨ì  ì˜ì¡´ì„±ì„ í¬ì°©í•˜ëŠ” ëŠ¥ë ¥ì„ ê°€ì§„ LSTM ë„¤íŠ¸ì›Œí¬ê°€ Lede-3ì™€ ê°™ì€ ê¸°ë³¸ ë°©ë²• ë° ë” ê°„ë‹¨í•œ ëª¨ë¸ë³´ë‹¤ F1 ì ìˆ˜ì™€ ROUGE-1 ì§€í‘œì—ì„œ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì˜¨ë¼ì¸ ë‰´ìŠ¤ í”Œë«í¼ì˜ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ì—¬ ë” íš¨ìœ¨ì ì¸ ì½˜í…ì¸  ì¡°ì§ê³¼ í–¥ìƒëœ ì‚¬ìš©ì ê²½í—˜ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìë™ ìš”ì•½ì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì •ë³´ ê³¼ë¶€í•˜ ì‹œëŒ€ì— ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íš¨ìœ¨ì ì¸ ìš”ì•½ì„ í†µí•´ ì ‘ê·¼ì„±ê³¼ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. Cornell Newsroom ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ BERT ì„ë² ë”©ì„ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í™”í•˜ê³ , ì´ë¥¼ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì„¤ì •í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë¸ì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ìˆœì°¨ì  ì˜ì¡´ì„±ì„ í¬ì°©í•˜ëŠ” LSTM ë„¤íŠ¸ì›Œí¬ê°€ Lede-3 ë“±ì˜ ê¸°ì¤€ ë°©ë²•ë³´ë‹¤ F1 ì ìˆ˜ì™€ ROUGE-1 ì§€í‘œì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ìë™ ìš”ì•½ì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•˜ë©°, ì˜¨ë¼ì¸ ë‰´ìŠ¤ í”Œë«í¼ì˜ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œ ê°œì„ ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì •ë³´ ê³¼ë¶€í•˜ ì‹œëŒ€ì— ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íš¨ìœ¨ì ì¸ ìš”ì•½ì€ ì ‘ê·¼ì„±ê³¼ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ë†’ì´ëŠ” ë° ì¤‘ìš”í•˜ë‹¤.

- 2. ë³¸ ì—°êµ¬ëŠ” ê³ ê¸‰ ê¸°ê³„ í•™ìŠµ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì›ë˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°„ê²°í•˜ê³  ì¼ê´€ëœ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.

- 3. Cornell Newsroom ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ BERT ì„ë² ë”©ì„ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í™”í•˜ê³ , ì´ë¥¼ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì ‘ê·¼í•˜ì˜€ë‹¤.

- 4. LSTM ë„¤íŠ¸ì›Œí¬ê°€ ìˆœì°¨ì  ì˜ì¡´ì„±ì„ í¬ì°©í•˜ëŠ” ëŠ¥ë ¥ ë•ë¶„ì— Lede-3 ë° ë‹¨ìˆœ ëª¨ë¸ë³´ë‹¤ F1 ì ìˆ˜ì™€ ROUGE-1 ì§€í‘œì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

- 5. ìë™ ìš”ì•½ì˜ ì ì¬ë ¥ì€ ì˜¨ë¼ì¸ ë‰´ìŠ¤ í”Œë«í¼ì˜ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ê³  ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.

---

*Generated on 2025-09-22 15:21:09*