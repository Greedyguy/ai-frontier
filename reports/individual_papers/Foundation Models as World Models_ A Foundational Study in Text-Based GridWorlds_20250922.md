# Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds

**Korean Title:** í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë¦¬ë“œì›”ë“œì—ì„œì˜ ê¸°ì´ˆ ì—°êµ¬: ì„¸ê³„ ëª¨ë¸ë¡œì„œì˜ ê¸°ì´ˆ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Foundation Agents

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (82.5% similar)
- [[2025-09-19/(P)rior(D)yna(F)low_ A Priori Dynamic Workflow Construction via Multi-Agent Collaboration_20250919|(P)rior(D)yna(F)low A Priori Dynamic Workflow Construction via Multi-Agent Collaboration]] (82.2% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (81.9% similar)
- [[2025-09-19/WebCoT_ Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback_20250919|WebCoT Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback]] (81.5% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (81.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15915v1 Announce Type: cross 
Abstract: While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15915v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê°•í™” í•™ìŠµì´ íš¨ìœ¨ì ì¸ ì‹œë®¬ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœì°¨ì  ì˜ì‚¬ ê²°ì • ì‘ì—…ì„ í•´ê²°í•˜ëŠ” ë° ì¸ìƒì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ìƒí˜¸ì‘ìš© ë¹„ìš©ì´ ë†’ì€ ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ë” ìƒ˜í”Œ íš¨ìœ¨ì ì¸ ì—ì´ì „íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. ê¸°ì´ˆ ëª¨ë¸(FMs)ì€ ê´‘ë²”ìœ„í•œ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆì–´ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í›„ë³´ì´ì§€ë§Œ, ì´ë¥¼ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°©ë²•ì€ ì•„ì§ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ ìœ ë§í•œ ì „ëµì„ ì˜ˆì¸¡í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•˜ê²ŒëŠ” í‰ê°€í•©ë‹ˆë‹¤. ì²«ì§¸, FMsì˜ ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ëœ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ê³  í‰ê°€í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ì„¸ê³„ ëª¨ë¸(FWMs)ì˜ ì‚¬ìš©ì„ ê³ ë ¤í•©ë‹ˆë‹¤. ë‘˜ì§¸, FMsì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê¸°ì´ˆ ì—ì´ì „íŠ¸(FAs)ì˜ ì‚¬ìš©ì„ ê³ ë ¤í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í˜„ì¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLMs)ì— ì í•©í•œ ê·¸ë¦¬ë“œ ì›”ë“œ í™˜ê²½ì˜ ê°€ì¡±ì—ì„œ ë‘ ì ‘ê·¼ ë°©ì‹ì„ ì‹¤ì¦ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” LLMsì˜ ê°œì„ ì´ ì´ë¯¸ ë” ë‚˜ì€ FWMsì™€ FAsë¡œ ì´ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•˜ë©°, í˜„ì¬ LLMs ê¸°ë°˜ì˜ FAsê°€ ì¶©ë¶„íˆ ë‹¨ìˆœí•œ í™˜ê²½ì—ì„œëŠ” ì´ë¯¸ ìš°ìˆ˜í•œ ì •ì±…ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ FWMsì™€ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ì˜ ê²°í•©ì´ ë¶€ë¶„ ê´€ì°°ì„±ê³¼ í™•ë¥ ì  ìš”ì†Œë¥¼ ê°€ì§„ ë” ë³µì¡í•œ ì„¤ì •ì— ëŒ€í•´ ë§¤ìš° ìœ ë§í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì˜ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ê¸°ì´ˆ ëª¨ë¸(FMs)ì„ í™œìš©í•˜ëŠ” ë‘ ê°€ì§€ ì „ëµì„ í‰ê°€í•©ë‹ˆë‹¤. ì²«ì§¸, ê¸°ì´ˆ ì„¸ê³„ ëª¨ë¸(FWMs)ì„ ì‚¬ìš©í•˜ì—¬ FMsì˜ ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•´ ì‹œë®¬ë ˆì´ì…˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨ ë° í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë‘˜ì§¸, FMsì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•œ ê¸°ì´ˆ ì—ì´ì „íŠ¸(FAs)ë¥¼ í†µí•œ ì˜ì‚¬ê²°ì • ë°©ì‹ì„ ê³ ë ¤í•©ë‹ˆë‹¤. ê·¸ë¦¬ë“œ ì›”ë“œ í™˜ê²½ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ FWMsì™€ FAsì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ê°„ë‹¨í•œ í™˜ê²½ì—ì„œëŠ” FAsê°€ ìš°ìˆ˜í•œ ì •ì±…ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, FWMsì™€ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ì˜ ê²°í•©ì´ ë¶€ë¶„ ê´€ì°° ë° í™•ë¥ ì  ìš”ì†Œê°€ ìˆëŠ” ë³µì¡í•œ í™˜ê²½ì—ì„œ ìœ ë§í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµì€ íš¨ìœ¨ì ì¸ ì‹œë®¬ë ˆì´í„°ë¥¼ í†µí•´ ìˆœì°¨ì  ì˜ì‚¬ ê²°ì • ì‘ì—…ì„ í•´ê²°í•˜ëŠ” ë° ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ë” ë§ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ìš”êµ¬í•©ë‹ˆë‹¤.

- 2. íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(FM)ì€ ê´‘ë²”ìœ„í•œ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆì–´ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í›„ë³´ì…ë‹ˆë‹¤.

- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” íŒŒìš´ë°ì´ì…˜ ì›”ë“œ ëª¨ë¸(FWM)ê³¼ íŒŒìš´ë°ì´ì…˜ ì—ì´ì „íŠ¸(FA)ì˜ ë‘ ê°€ì§€ ì „ëµì„ í‰ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.

- 4. ê·¸ë¦¬ë“œ ì›”ë“œ í™˜ê²½ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ FAëŠ” ê°„ë‹¨í•œ í™˜ê²½ì—ì„œ ìš°ìˆ˜í•œ ì •ì±…ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- 5. FWMê³¼ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ì˜ ê²°í•©ì€ ë¶€ë¶„ ê´€ì°°ì„±ê³¼ í™•ë¥ ì  ìš”ì†Œê°€ ìˆëŠ” ë³µì¡í•œ í™˜ê²½ì—ì„œ ë§¤ìš° ìœ ë§í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:17:00*