
# Apertus: Democratizing Open and Compliant LLMs for Global Language Environments

**Korean Title:** 아페르투스: 전 세계 언어 환경을 위한 개방적이고 준수하는 LLMs 민주화

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Transparent Audit|Transparent Audit]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Multilingual Representation|Multilingual Representation]] [[keywords/specific/Goldfish Objective|Goldfish Objective]] [[keywords/unique/Apertus|Apertus]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Transparent Audit
**🔬 Broad Technical**: Large Language Models, Multilingual Representation
**🔗 Specific Connectable**: Goldfish Objective
**⭐ Unique Technical**: Apertus

**ArXiv ID**: [2509.14233](https://arxiv.org/abs/2509.14233)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.14233.pdf)


## 🏷️ 추출된 키워드



`Large Language Models` • 

`Multilingual Representation` • 

`Goldfish Objective` • 

`Apertus` • 

`Transparent Audit`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14233v1 Announce Type: cross 
Abstract: We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.

## 🔍 Abstract (한글 번역)

arXiv:2509.14233v1 발표 유형: 교차
요약: 우리는 오늘날의 오픈 모델 생태계에서 두 가지 시스템적인 결점을 해결하기 위해 설계된 완전히 오픈 소스의 대규모 언어 모델 (LLMs) 스위트인 Apertus를 제시합니다: 데이터 준수 및 다국어 표현. 이전 모델과는 달리, Apertus 모델은 재현 가능한 데이터 파이프라인이나 콘텐츠 소유자의 권리를 고려하지 않고 가중치를 공개하는 많은 이전 모델과 달리, Apertus 모델은 오직 공개적으로 이용 가능한 데이터로 사전 훈련되어, robots.txt 제외 사항을 후행적으로 존중하고 비허가, 유해 및 개인 식별 가능한 콘텐츠를 필터링합니다. 기억 위험을 완화하기 위해 사전 훈련 중에 Goldfish 목표를 채택하여, 데이터의 단순 반복 기억을 강력하게 억제하면서 하류 작업 성능을 유지합니다. Apertus 모델은 또한 다국어 커버리지를 확대하여, 1800개 이상의 언어로부터 15조 토큰을 학습하며, 사전 훈련 데이터의 약 40%가 영어 이외의 콘텐츠에 할당됩니다. 8B 및 70B 규모로 공개된 Apertus는 다국어 벤치마크에서 완전히 오픈된 모델 중 최신 결과에 근접하거나 초월하여 오픈 가중치 대응 모델과 경쟁합니다. 모델 가중치 이외에도, 우리는 허용 라이선스로 개발 주기에서 모든 과학적 자산을 공개하며, 데이터 준비 스크립트, 체크포인트, 평가 스위트 및 훈련 코드를 포함하여 투명한 감사와 확장을 가능하게 합니다.

## 📝 요약

한국어 요약:
본 연구는 Apertus라는 완전히 오픈된 대형 언어 모델 스위트를 제시한다. 이 모델은 데이터 준수 및 다국어 표현이라는 두 가지 시스템적인 결함을 해결하기 위해 설계되었다. Apertus 모델은 공개적으로 이용 가능한 데이터만을 사용하여 사전 훈련되며, robots.txt 제외 사항을 존중하고 비허가, 유해 및 개인 식별 가능한 콘텐츠를 필터링한다. 또한, Goldfish 목표를 채택하여 기억 위험을 완화하고, 다국어 커버리지를 확대하여 1800개 이상의 언어로부터 15조 토큰을 학습한다. 이러한 모델은 다국어 벤치마크에서 최첨단 결과를 보여주며, 모든 과학적 자산을 허용하는 라이선스로 공개하여 투명한 감사와 확장을 가능하게 한다.

## 🎯 주요 포인트


- 1. Apertus는 데이터 준수 및 다국어 표현을 위해 설계된 완전히 오픈 소스의 대형 언어 모델 스위트이다.

- 2. Apertus 모델은 공개적으로 사용 가능한 데이터만을 사용하여 사전 훈련되며, 다국어 커버리지를 확장한다.

- 3. Apertus는 다국어 벤치마크에서 최첨단 결과를 보여주며, 과거의 오픈 소스 모델을 능가한다.

- 4. Apertus 개발 주기에서 모든 과학적 자산을 퍼미시브 라이센스로 공개하여 투명한 감사와 확장을 가능하게 한다.


---

*Generated on 2025-09-18 16:26:24*