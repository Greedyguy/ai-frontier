# Subjective Behaviors and Preferences in LLM: Language of Browsing

**Korean Title:** ì£¼ê´€ì  í–‰ë™ê³¼ ì„ í˜¸ë„ì— ê´€í•œ LLM: ë¸Œë¼ìš°ì§• ì–¸ì–´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Language of Browsing

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox A practical guide to getting the most out of human ratings]] (85.4% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (85.1% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony On Multilingual Data Allocation for Large Language Models Pretraining]] (84.9% similar)
- [[2025-09-18/Catch Me If You Can Not Yet_ LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors_20250918|Catch Me If You Can Not Yet LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors]] (84.9% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang A Systematic Comparison between Human and Machine-Generated Slang Usages]] (84.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.15474v3 Announce Type: replace-cross 
Abstract: A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed "language", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the "language of browsing" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.15474v3 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì™€ ì‘ì—…ì—ì„œ ìœ ì—°ì„±ì„ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ í–‰ë™ê³¼ ì„ í˜¸ë„ë¥¼ ê°€ì§„ ì‚¬ìš©ìì—ê²Œ ì´ì ì„ ì œê³µí•œë‹¤ê³  ì£¼ì¥ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‚¬ìš©ìê°€ ì›¹ì‚¬ì´íŠ¸ë‚˜ ì•±ì„ ë…íŠ¹í•˜ê³  íŠ¹ì´í•˜ê²Œ íƒìƒ‰í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³¸ì§ˆì ìœ¼ë¡œ ì£¼ê´€ì ì¸ í–‰ë™ê³¼ ì„ í˜¸ë„ë¥¼ ê°€ì§ˆ ë•Œ LLMì— ëŒ€í•œ ì´ëŸ¬í•œ ì¸ì‹ì— ì˜ë¬¸ì„ ì œê¸°í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ìƒì„±ëœ í˜ì´ì§€ì˜ ìˆœì°¨ì  í–‰ë™ ë¡œê·¸ëŠ” ìì—°ì–´ì— ë‚´ì¬ëœ êµ¬ì¡°ì™€ ë¬¸ë²• ì—†ì´ ê° ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ êµ¬ì„±í•œ "ì–¸ì–´"ì™€ ìœ ì‚¬í•œ ê²ƒì„ í˜•ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ ë˜ì§‘ë‹ˆë‹¤: (i) ì‘ì€ ì–¸ì–´ ëª¨ë¸(LM)ì´ "íƒìƒ‰ ì–¸ì–´"ë¥¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ë³´ë‹¤ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€? (ii) ë‹¨ì¼ ë§¤ê°œë³€ìˆ˜ ì„¸íŠ¸(ë˜ëŠ” ë‹¨ì¼ LM)ë¥¼ ê°€ì§„ LMì´ ë‹¤ì–‘í•œ ì‚¬ìš©ìì˜ ì´ì§ˆì ì´ê³  ì£¼ê´€ì ì¸ í–‰ë™ê³¼ ì„ í˜¸ë„ë¥¼ ì ì ˆíˆ í¬ì°©í•  ìˆ˜ ìˆëŠ”ê°€? (iii) ë†’ì€ í‰ê·  ì„±ëŠ¥ì„ ê°€ì§„ ë‹¨ì¼ LMì´ ì‚¬ìš©ì ìˆ˜ì¤€ì—ì„œì˜ ì •ë ¬ì„ ì¢‹ê²Œ í•˜ê¸° ìœ„í•´ ì„±ëŠ¥ì˜ ë¶„ì‚°ì„ ë‚®ê²Œ ìœ ì§€í•  ìˆ˜ ìˆëŠ”ê°€? ìš°ë¦¬ëŠ” ì£¼ê´€ì ì¸ í–‰ë™ì— ì í•©í•œ í´ëŸ¬ìŠ¤í„°ë³„ LM í›ˆë ¨, HeTLM(ì´ì§ˆì„± ì¸ì‹ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” (i) í˜ì´ì§€ ìˆ˜ì¤€ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ ì‘ì€ LMì´ ëŒ€í˜• ì‚¬ì „ í›ˆë ¨ ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •ëœ LMë³´ë‹¤ ë›°ì–´ë‚˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤; (ii) ì´ì§ˆì ì¸ í´ëŸ¬ìŠ¤í„°ë³„ ë§¤ê°œë³€ìˆ˜ ì„¸íŠ¸ë¥¼ ê°€ì§„ HeTLMì´ ë™ì¼í•œ íŒ¨ë°€ë¦¬ì˜ ë‹¨ì¼ LMì„ ë§¤ê°œë³€ìˆ˜ ìˆ˜ë¥¼ ì œì–´í•˜ë©´ì„œ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤; ê·¸ë¦¬ê³  (iii) ìƒì„±ì—ì„œ ë” ë†’ì€ í‰ê· ê³¼ ë” ë‚®ì€ ë¶„ì‚°ì´ ë°œìƒí•˜ì—¬ ì •ë ¬ì´ ê°œì„ ë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ìš©ìë“¤ì˜ ì£¼ê´€ì ì¸ í–‰ë™ê³¼ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìš©ì„±ì„ ì¬ê²€í† í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì›¹ì‚¬ì´íŠ¸ë‚˜ ì•±ì—ì„œì˜ ì‚¬ìš©ì í–‰ë™ ë¡œê·¸ê°€ ê°ìì˜ "ë¸Œë¼ìš°ì§• ì–¸ì–´"ë¥¼ í˜•ì„±í•œë‹¤ê³  ë³´ê³ , ì‘ì€ ì–¸ì–´ ëª¨ë¸ì´ ì´ë¥¼ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. ì œì•ˆëœ HeTLM(ì´ì§ˆì„± ì¸ì‹ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨)ì€ ì‚¬ìš©ì í–‰ë™ì˜ ì´ì§ˆì„±ì„ ê³ ë ¤í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë³„ë¡œ í›ˆë ¨ëœ ëª¨ë¸ë¡œ, ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, í‰ê·  ì„±ëŠ¥ì´ ë†’ê³  ì„±ëŠ¥ ë³€ë™ì„±ì´ ë‚®ì•„ ì‚¬ìš©ì ìˆ˜ì¤€ì—ì„œì˜ ì •ë ¬ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‚¬ìš©ìì˜ ì£¼ê´€ì ì¸ í–‰ë™ê³¼ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•˜ëŠ” "ë¸Œë¼ìš°ì§• ì–¸ì–´"ëŠ” ì‘ì€ ì–¸ì–´ ëª¨ë¸(LM)ì´ í° ì–¸ì–´ ëª¨ë¸ë³´ë‹¤ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

- 2. HeTLM(ì´ì§ˆì„± ì¸ì‹ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨)ì€ ì‚¬ìš©ì ê°„ì˜ ì´ì§ˆì ì¸ í–‰ë™ì„ ë°˜ì˜í•˜ì—¬ íŠ¹ì • í´ëŸ¬ìŠ¤í„°ë³„ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë‹¨ì¼ ì–¸ì–´ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤.

- 3. í˜ì´ì§€ ìˆ˜ì¤€ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ ì‘ì€ ì–¸ì–´ ëª¨ë¸ì€ ëŒ€í˜• ì‚¬ì „ í›ˆë ¨ ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •ëœ ì–¸ì–´ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë‹¤.

- 4. ë†’ì€ í‰ê·  ì„±ëŠ¥ê³¼ ë‚®ì€ ì„±ëŠ¥ ë¶„ì‚°ì„ í†µí•´ ì‚¬ìš©ì ìˆ˜ì¤€ì—ì„œì˜ ì •ë ¬ì´ ê°œì„ ëœë‹¤.

---

*Generated on 2025-09-22 14:58:35*