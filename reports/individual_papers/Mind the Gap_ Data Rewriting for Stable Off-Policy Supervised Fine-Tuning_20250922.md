# Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning

**Korean Title:** ê°­ì— ì£¼ì˜í•˜ë¼: ì•ˆì •ì ì¸ ì˜¤í”„-ì •ì±… ê°ë… ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ë°ì´í„° ì¬ì‘ì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Two-Stage Approach|Two-Stage Approach]] [[keywords/specific/Importance Sampling|Importance Sampling]] [[keywords/broad/Supervised Fine-Tuning|Supervised Fine-Tuning]] [[keywords/broad/Off-Policy Learning|Off-Policy Learning]] [[keywords/unique/Data Rewriting Framework|Data Rewriting Framework]] [[categories/cs.LG|cs.LG]] [[2025-09-18/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250918|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (97.8% similar) [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (85.4% similar) [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (84.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Two-Stage Approach
**ğŸ”— Specific Connectable**: Importance Sampling
**ğŸ”¬ Broad Technical**: Supervised Fine-Tuning, Off-Policy Learning
**â­ Unique Technical**: Data Rewriting Framework
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250918|Mind the Gap Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (97.8% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (85.4% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (84.0% similar)
- [[2025-09-19/From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (83.3% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (83.2% similar)


**ArXiv ID**: [2509.15157](https://arxiv.org/abs/2509.15157)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.15157.pdf)


**ArXiv ID**: [2509.15157](https://arxiv.org/abs/2509.15157)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.15157.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Importance Sampling, KL Penalties
**â­ Unique Technical**: Data Rewriting Framework
**ğŸ”¬ Broad Technical**: Supervised Fine-Tuning, Off-Policy Learning

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Supervised Fine-Tuning` â€¢ 

`Off-Policy Learning` â€¢ 

`Importance Sampling` â€¢ 

`Data Rewriting Framework` â€¢ 

`Two-Stage Approach`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15157v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to skewed weights, high variance, and unstable optimization. Existing methods mitigate this issue with KL penalties or clipping, which passively restrict updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap before training. For each problem, correct model-generated solutions are kept as on-policy data, while incorrect ones are rewritten through guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy, reducing variance and improving stability. To handle residual mismatch after rewriting, we additionally apply importance sampling during training, forming a two-stage approach that combines data-level alignment with lightweight optimization-level correction. Experiments on five mathematical reasoning benchmarks show consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. Data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15157v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ì¡°ì •(SFT)ì€ ê³ ì •ëœ í–‰ë™ ì •ì±…ì—ì„œ ì „ë¬¸ê°€ ì‹œì—°ì´ ì œê³µë˜ê³  í›ˆë ¨ì´ ëª©í‘œ ì •ì±…ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ì˜¤í”„ ì •ì±… í•™ìŠµ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”ë„ ìƒ˜í”Œë§ì€ ì´ëŸ¬í•œ ë¶„í¬ ë¶ˆì¼ì¹˜ë¥¼ ìˆ˜ì •í•˜ê¸° ìœ„í•œ í‘œì¤€ ë„êµ¬ì´ì§€ë§Œ, í° ì •ì±… ê²©ì°¨ëŠ” ì™œê³¡ëœ ê°€ì¤‘ì¹˜, ë†’ì€ ë¶„ì‚°, ë¶ˆì•ˆì •í•œ ìµœì í™”ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ KL í˜ë„í‹°ë‚˜ í´ë¦¬í•‘ì„ í†µí•´ ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ì§€ë§Œ, ì´ëŠ” ê²©ì°¨ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¤„ì´ê¸°ë³´ë‹¤ëŠ” ìˆ˜ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ì œí•œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í›ˆë ¨ ì „ì— ì •ì±… ê²©ì°¨ë¥¼ ì‚¬ì „ì— ì¤„ì´ëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë°ì´í„° ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê° ë¬¸ì œì— ëŒ€í•´, ëª¨ë¸ì´ ìƒì„±í•œ ì˜¬ë°”ë¥¸ ì†”ë£¨ì…˜ì€ ì˜¨ ì •ì±… ë°ì´í„°ë¡œ ìœ ì§€ë˜ê³ , ì˜ëª»ëœ ì†”ë£¨ì…˜ì€ ì•ˆë‚´ëœ ì¬í•´ê²°ì„ í†µí•´ ì¬ì‘ì„±ë˜ë©°, í•„ìš”í•  ë•Œë§Œ ì „ë¬¸ê°€ ì‹œì—°ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ ë¶„í¬ë¥¼ ëª©í‘œ ì •ì±…ê³¼ ì¼ì¹˜ì‹œì¼œ ë¶„ì‚°ì„ ì¤„ì´ê³  ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì¬ì‘ì„± í›„ ë‚¨ì•„ ìˆëŠ” ë¶ˆì¼ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì¶”ê°€ë¡œ í›ˆë ¨ ì¤‘ì— ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì¤€ì˜ ì •ë ¬ê³¼ ê²½ëŸ‰ ìµœì í™” ìˆ˜ì¤€ì˜ ìˆ˜ì •ì„ ê²°í•©í•œ 2ë‹¨ê³„ ì ‘ê·¼ ë°©ì‹ì„ í˜•ì„±í•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°€ì§€ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ê¸°ë³¸ SFTì™€ ìµœì²¨ë‹¨ ë™ì  ë¯¸ì„¸ ì¡°ì •(DFT) ì ‘ê·¼ ë°©ì‹ì„ ëª¨ë‘ ëŠ¥ê°€í•˜ëŠ” ì¼ê´€ë˜ê³  ìœ ì˜ë¯¸í•œ ì„±ê³¼ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë°ì´í„°ì™€ ì½”ë“œëŠ” https://github.com/NKU-HLT/Off-Policy-SFTì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •ì„ ì˜¤í”„ ì •ì±… í•™ìŠµ ë¬¸ì œë¡œ ë³´ê³ , ì •ì±… ê°„ ê²©ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë°ì´í„° ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ KL íŒ¨ë„í‹°ë‚˜ í´ë¦¬í•‘ì„ ì‚¬ìš©í•´ ì—…ë°ì´íŠ¸ë¥¼ ì œí•œí•˜ì§€ë§Œ, ì´ ì—°êµ¬ëŠ” í›ˆë ¨ ì „ ì •ì±… ê²©ì°¨ë¥¼ ëŠ¥ë™ì ìœ¼ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤. ë¬¸ì œë³„ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ì˜¬ë°”ë¥¸ í•´ë‹µì€ ì •ì±… ë‚´ ë°ì´í„°ë¡œ ìœ ì§€í•˜ê³ , ì˜ëª»ëœ í•´ë‹µì€ ì „ë¬¸ê°€ ì‹œì—°ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì¬í•´ê²°ì„ í†µí•´ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ í›ˆë ¨ ë¶„í¬ê°€ ëª©í‘œ ì •ì±…ê³¼ ì¼ì¹˜í•˜ì—¬ ë¶„ì‚°ì´ ì¤„ê³  ì•ˆì •ì„±ì´ í–¥ìƒë©ë‹ˆë‹¤. ì¬ì‘ì„± í›„ ë‚¨ì€ ë¶ˆì¼ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì¤‘ ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì¶”ê°€ë¡œ ì ìš©í•˜ì—¬ ë°ì´í„° ì •ë ¬ê³¼ ìµœì í™” ìˆ˜ì¤€ì˜ ë³´ì •ì„ ê²°í•©í•œ 2ë‹¨ê³„ ì ‘ê·¼ë²•ì„ í˜•ì„±í•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°œì˜ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ì¼ê´€ë˜ê³  ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë°ì´í„°ì™€ ì½”ë“œëŠ” ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT)ì€ ê³ ì •ëœ í–‰ë™ ì •ì±…ì—ì„œ ì „ë¬¸ê°€ ì‹œì—°ì„ ê°€ì ¸ì™€ ëª©í‘œ ì •ì±…ì„ ìµœì í™”í•˜ëŠ” ì˜¤í”„ ì •ì±… í•™ìŠµ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 2. ê¸°ì¡´ ë°©ë²•ì€ KL í˜ë„í‹°ë‚˜ í´ë¦¬í•‘ì„ ì‚¬ìš©í•˜ì—¬ ì •ì±… ê°„ì˜ ì°¨ì´ë¥¼ ìˆ˜ë™ì ìœ¼ë¡œ ì œí•œí•˜ì§€ë§Œ, ì œì•ˆëœ ë°ì´í„° ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ëŠ” í›ˆë ¨ ì „ì— ì •ì±… ì°¨ì´ë¥¼ ëŠ¥ë™ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

- 3. ë¬¸ì œë³„ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ì˜¬ë°”ë¥¸ ì†”ë£¨ì…˜ì€ ì˜¨ ì •ì±… ë°ì´í„°ë¡œ ìœ ì§€í•˜ê³ , ì˜ëª»ëœ ì†”ë£¨ì…˜ì€ ì „ë¬¸ê°€ ì‹œì—°ìœ¼ë¡œ ëŒì•„ê°€ì•¼ í•  ë•Œë§Œ ê°€ì´ë“œëœ ì¬í•´ê²°ì„ í†µí•´ ì¬ì‘ì„±í•©ë‹ˆë‹¤.

- 4. ë°ì´í„° ì¬ì‘ì„± í›„ ë‚¨ì•„ ìˆëŠ” ë¶ˆì¼ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì¤‘ ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì¶”ê°€ë¡œ ì ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì¤€ì˜ ì •ë ¬ê³¼ ìµœì í™” ìˆ˜ì¤€ì˜ ê²½ëŸ‰ ìˆ˜ì •ì´ ê²°í•©ëœ 2ë‹¨ê³„ ì ‘ê·¼ ë°©ì‹ì„ í˜•ì„±í•©ë‹ˆë‹¤.

- 5. ë‹¤ì„¯ ê°€ì§€ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ì˜ SFTì™€ ìµœì‹ ì˜ ë™ì  ë¯¸ì„¸ ì¡°ì •(DFT) ì ‘ê·¼ë²•ë³´ë‹¤ ì¼ê´€ë˜ê³  ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:05:01*