# Double descent in quantum kernel methods

**Korean Title:** ì–‘ì ì»¤ë„ ë°©ë²•ì—ì„œì˜ ì´ì¤‘ í•˜ê°•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/specific/Quantum Kernel Methods|Quantum Kernel Methods]] [[keywords/specific/Random Matrix Theory|Random Matrix Theory]] [[keywords/broad/Quantum Machine Learning|Quantum Machine Learning]] [[keywords/unique/Double Descent in Quantum Feature Spaces|Double Descent in Quantum Feature Spaces]] [[categories/cs.LG|cs.LG]] [[2025-09-17/Learning quantum many-body data locally_ A provably scalable framework_20250917|Learning quantum many-body data locally: A provably scalable framework]] (85.6% similar) [[2025-09-19/Trainability of Quantum Models Beyond Known Classical Simulability_20250919|Trainability of Quantum Models Beyond Known Classical Simulability]] (83.4% similar) [[2025-09-22/Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning_20250922|Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Double Descent Phenomenon
**ğŸ”— Specific Connectable**: Quantum Kernel Methods, Random Matrix Theory
**ğŸ”¬ Broad Technical**: Quantum Machine Learning, Statistical Learning Theory
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Learning quantum many-body data locally_ A provably scalable framework_20250917|Learning quantum many-body data locally A provably scalable framework]] (85.6% similar)
- [[2025-09-19/Trainability of Quantum Models Beyond Known Classical Simulability_20250919|Trainability of Quantum Models Beyond Known Classical Simulability]] (83.4% similar)
- [[2025-09-22/Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning_20250922|Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning]] (83.1% similar)
- [[2025-09-17/Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment_20250917|Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment]] (82.6% similar)
- [[2025-09-22/Efficient Learning for Linear Properties of Bounded-Gate Quantum Circuits_20250922|Efficient Learning for Linear Properties of Bounded-Gate Quantum Circuits]] (82.0% similar)


**ArXiv ID**: [2501.10077](https://arxiv.org/abs/2501.10077)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2501.10077.pdf)


**ArXiv ID**: [2501.10077](https://arxiv.org/abs/2501.10077)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2501.10077.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Double Descent Phenomenon
**ğŸ”— Specific Connectable**: Quantum Kernel Methods, Random Matrix Theory
**ğŸ”¬ Broad Technical**: Quantum Machine Learning, Statistical Learning Theory

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Quantum Machine Learning` â€¢ 

`Statistical Learning Theory` â€¢ 

`Quantum Kernel Methods` â€¢ 

`Random Matrix Theory` â€¢ 

`Double Descent Phenomenon`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.10077v2 Announce Type: replace-cross 
Abstract: The double descent phenomenon challenges traditional statistical learning theory by revealing scenarios where larger models do not necessarily lead to reduced performance on unseen data. While this counterintuitive behavior has been observed in a variety of classical machine learning models, particularly modern neural network architectures, it remains elusive within the context of quantum machine learning. In this work, we analytically demonstrate that linear regression models in quantum feature spaces can exhibit double descent behavior by drawing on insights from classical linear regression and random matrix theory. Additionally, our numerical experiments on quantum kernel methods across different real-world datasets and system sizes further confirm the existence of a test error peak, a characteristic feature of double descent. Our findings provide evidence that quantum models can operate in the modern, overparameterized regime without experiencing overfitting, potentially opening pathways to improved learning performance beyond traditional statistical learning theory.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2501.10077v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ì´ì¤‘ í•˜ê°•(double descent) í˜„ìƒì€ ë” í° ëª¨ë¸ì´ ë°˜ë“œì‹œ ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§€ì§€ ì•ŠëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì— ë„ì „í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§ê´€ì— ë°˜í•˜ëŠ” í–‰ë™ì€ ë‹¤ì–‘í•œ ê³ ì „ì  ê¸°ê³„ í•™ìŠµ ëª¨ë¸, íŠ¹íˆ í˜„ëŒ€ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œ ê´€ì°°ë˜ì—ˆì§€ë§Œ, ì–‘ì ê¸°ê³„ í•™ìŠµì˜ ë§¥ë½ì—ì„œëŠ” ì—¬ì „íˆ ëª¨í˜¸í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ ì „ì  ì„ í˜• íšŒê·€ì™€ ëœë¤ í–‰ë ¬ ì´ë¡ ì—ì„œ ì–»ì€ í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ ì–‘ì íŠ¹ì§• ê³µê°„ì—ì„œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ì´ì¤‘ í•˜ê°• í–‰ë™ì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ë¶„ì„ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ ì‹œìŠ¤í…œ í¬ê¸°ì— ê±¸ì¹œ ì–‘ì ì»¤ë„ ë°©ë²•ì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì€ ì´ì¤‘ í•˜ê°•ì˜ íŠ¹ì§•ì ì¸ íŠ¹ì„±ì¸ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ í”¼í¬ì˜ ì¡´ì¬ë¥¼ ì¶”ê°€ë¡œ í™•ì¸í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ì–‘ì ëª¨ë¸ì´ ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì„ ë„˜ì–´ ê°œì„ ëœ í•™ìŠµ ì„±ëŠ¥ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ê³¼ë§¤ê°œë³€ìˆ˜í™”ëœ í˜„ëŒ€ì  ì²´ì œì—ì„œ ê³¼ì í•© ì—†ì´ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–‘ì ê¸°ê³„ í•™ìŠµì—ì„œì˜ ì´ì¤‘ í•˜ê°•(double descent) í˜„ìƒì„ ë¶„ì„í•©ë‹ˆë‹¤. ê¸°ì¡´ í†µê³„ í•™ìŠµ ì´ë¡ ì—ì„œëŠ” ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ê³  ë³´ì•˜ìœ¼ë‚˜, ì´ì¤‘ í•˜ê°•ì€ ì´ë¥¼ ë°˜ë°•í•˜ë©° í° ëª¨ë¸ì´ í•­ìƒ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì €ìë“¤ì€ ì–‘ì íŠ¹ì§• ê³µê°„ì—ì„œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ì´ì¤‘ í•˜ê°•ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ ì‹œìŠ¤í…œ í¬ê¸°ë¥¼ í™œìš©í•œ ì–‘ì ì»¤ë„ ë°©ë²•ì˜ ì‹¤í—˜ì„ í†µí•´ ì´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–‘ì ëª¨ë¸ì´ ê³¼ì í•© ì—†ì´ ê³¼ë§¤ê°œë³€ìˆ˜í™”ëœ ìƒíƒœì—ì„œë„ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•˜ë©°, ì „í†µì  í†µê³„ í•™ìŠµ ì´ë¡ ì„ ë„˜ì–´ì„  í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ë”ë¸” ë””ì„¼íŠ¸ í˜„ìƒì€ ë” í° ëª¨ë¸ì´ ë°˜ë“œì‹œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•ŠëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì— ë„ì „í•©ë‹ˆë‹¤.

- 2. ì–‘ì ë¨¸ì‹  ëŸ¬ë‹ì—ì„œëŠ” ì´ í˜„ìƒì´ ì˜ ì´í•´ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ë³¸ ì—°êµ¬ëŠ” ì–‘ì íŠ¹ì§• ê³µê°„ì—ì„œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ë”ë¸” ë””ì„¼íŠ¸ í˜„ìƒì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ë¶„ì„ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤.

- 3. ì–‘ì ì»¤ë„ ë°©ë²•ì„ ì‚¬ìš©í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ í”¼í¬ê°€ ì¡´ì¬í•¨ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

- 4. ì—°êµ¬ ê²°ê³¼ëŠ” ì–‘ì ëª¨ë¸ì´ ê³¼ì í•© ì—†ì´ í˜„ëŒ€ì ì¸ ê³¼ë§¤ê°œë³€ìˆ˜í™”ëœ í™˜ê²½ì—ì„œ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì„ ë„˜ì–´ì„  í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:10:02*