# Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks

**Korean Title:** ê²½ê³„ì˜ ë§›: ë™ì°¨ ì‹ ê²½ë§ì—ì„œ ê°€ì¥ ê°€íŒŒë¥¸ í•˜ê°•ë²•ì˜ ì•”ë¬µì  í¸í–¥

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Margin Maximization Problem|Margin Maximization Problem]] [[keywords/specific/Steepest Descent Algorithms|Steepest Descent Algorithms]] [[keywords/specific/Adaptive Methods|Adaptive Methods]] [[keywords/broad/Neural Networks|Neural Networks]] [[keywords/unique/Implicit Bias of Homogeneous Networks|Implicit Bias of Homogeneous Networks]] [[categories/cs.LG|cs.LG]] [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (82.0% similar) [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (81.7% similar) [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Steepest Descent Algorithms, Adaptive Methods
**ğŸ”¬ Broad Technical**: Neural Networks
**â­ Unique Technical**: Geometric Margin
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (82.0% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks A Second-Order Optimization Perspective]] (81.7% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.4% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (81.0% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (80.7% similar)


**ArXiv ID**: [2410.22069](https://arxiv.org/abs/2410.22069)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2410.22069.pdf)


**ArXiv ID**: [2410.22069](https://arxiv.org/abs/2410.22069)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2410.22069.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Steepest Descent Algorithms, Adaptive Methods
**â­ Unique Technical**: Geometric Margin
**ğŸ”¬ Broad Technical**: Neural Networks

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Neural Networks` â€¢ 

`Steepest Descent Algorithms` â€¢ 

`Adaptive Methods` â€¢ 

`Geometric Margin`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.22069v3 Announce Type: replace 
Abstract: We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2410.22069v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” ê¹Šì€ ë™ì°¨ ì‹ ê²½ë§ì—ì„œ ë¬´í•œì†Œ í•™ìŠµë¥ ì„ ê°€ì§„ ê°€ì¥ ê°€íŒŒë¥¸ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜ì ì¸ ê³„ì—´ì˜ ì•”ë¬µì  í¸í–¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤: (a) ì•Œê³ ë¦¬ì¦˜ì— ì˜ì¡´í•˜ëŠ” ê¸°í•˜í•™ì  ë§ˆì§„ì€ ë„¤íŠ¸ì›Œí¬ê°€ ì™„ë²½í•œ í›ˆë ¨ ì •í™•ë„ì— ë„ë‹¬í•œ í›„ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©°, (b) í›ˆë ¨ ê²½ë¡œì˜ ëª¨ë“  ê·¹í•œì ì€ í•´ë‹¹ ë§ˆì§„ ìµœëŒ€í™” ë¬¸ì œì˜ KKT ì ì— í•´ë‹¹í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ê°€ì¥ ê°€íŒŒë¥¸ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”ëœ ì‹ ê²½ë§ì˜ ê²½ë¡œë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ í™•ëŒ€í•˜ì—¬, ì¸ê¸° ìˆëŠ” ì ì‘ ë°©ë²•(Adam ë° Shampoo)ì˜ ì•”ë¬µì  í¸í–¥ê³¼ì˜ ì—°ê²°ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¹Šì€ ë™ì§ˆ ì‹ ê²½ë§ì—ì„œ ë¬´í•œì†Œ í•™ìŠµë¥ ì„ ê°€ì§„ ìµœê¸‰ê°•í•˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì•”ë¬µì  í¸í–¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (a) ë„¤íŠ¸ì›Œí¬ê°€ ì™„ë²½í•œ í›ˆë ¨ ì •í™•ë„ì— ë„ë‹¬í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì— ì˜ì¡´í•˜ëŠ” ê¸°í•˜í•™ì  ë§ˆì§„ì´ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©°, (b) í›ˆë ¨ ê²½ë¡œì˜ ëª¨ë“  ê·¹í•œì ì€ í•´ë‹¹ ë§ˆì§„ ìµœëŒ€í™” ë¬¸ì œì˜ KKT ì ì— í•´ë‹¹í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ìµœê¸‰ê°•í•˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”ëœ ì‹ ê²½ë§ì˜ ê²½ë¡œë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ì¸ê¸° ìˆëŠ” ì ì‘í˜• ë°©ë²•(Adamê³¼ Shampoo)ì˜ ì•”ë¬µì  í¸í–¥ê³¼ì˜ ì—°ê²°ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ë¬´í•œì†Œ í•™ìŠµë¥ ì„ ì‚¬ìš©í•˜ëŠ” ê¹Šì€ ë™ì§ˆ ì‹ ê²½ë§ì˜ ê°€ì¥ ê¸‰í•œ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì˜ ì•”ë¬µì  í¸í–¥ì„ ì—°êµ¬í•˜ì˜€ìŠµë‹ˆë‹¤.

- 2. ë„¤íŠ¸ì›Œí¬ê°€ ì™„ë²½í•œ í›ˆë ¨ ì •í™•ë„ì— ë„ë‹¬í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì— ì˜ì¡´í•˜ëŠ” ê¸°í•˜í•™ì  ë§ˆì§„ì´ ì¦ê°€í•˜ê¸° ì‹œì‘í•©ë‹ˆë‹¤.

- 3. í›ˆë ¨ ê²½ë¡œì˜ ëª¨ë“  ê·¹í•œì ì€ í•´ë‹¹ ë§ˆì§„ ìµœëŒ€í™” ë¬¸ì œì˜ KKT ì ì— í•´ë‹¹í•©ë‹ˆë‹¤.

- 4. ë‹¤ì–‘í•œ ê°€ì¥ ê¸‰í•œ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”ëœ ì‹ ê²½ë§ì˜ ê²½ë¡œë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì¸ê¸° ìˆëŠ” ì ì‘í˜• ë°©ë²•(Adam ë° Shampoo)ì˜ ì•”ë¬µì  í¸í–¥ê³¼ì˜ ì—°ê²°ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 15:53:03*