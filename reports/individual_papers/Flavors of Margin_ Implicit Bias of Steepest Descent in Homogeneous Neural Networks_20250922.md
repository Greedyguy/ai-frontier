# Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks

**Korean Title:** 경계의 맛: 동차 신경망에서 가장 가파른 하강법의 암묵적 편향

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Margin Maximization Problem|Margin Maximization Problem]] [[keywords/specific/Steepest Descent Algorithms|Steepest Descent Algorithms]] [[keywords/specific/Adaptive Methods|Adaptive Methods]] [[keywords/broad/Neural Networks|Neural Networks]] [[keywords/unique/Implicit Bias of Homogeneous Networks|Implicit Bias of Homogeneous Networks]] [[categories/cs.LG|cs.LG]] [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (82.0% similar) [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (81.7% similar) [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: Steepest Descent Algorithms, Adaptive Methods
**🔬 Broad Technical**: Neural Networks
**⭐ Unique Technical**: Geometric Margin
## 🔗 유사한 논문
- [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (82.0% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks A Second-Order Optimization Perspective]] (81.7% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.4% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (81.0% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (80.7% similar)


**ArXiv ID**: [2410.22069](https://arxiv.org/abs/2410.22069)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2410.22069.pdf)


**ArXiv ID**: [2410.22069](https://arxiv.org/abs/2410.22069)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2410.22069.pdf)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: Steepest Descent Algorithms, Adaptive Methods
**⭐ Unique Technical**: Geometric Margin
**🔬 Broad Technical**: Neural Networks

## 🏷️ 추출된 키워드



`Neural Networks` • 

`Steepest Descent Algorithms` • 

`Adaptive Methods` • 

`Geometric Margin`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2410.22069v3 Announce Type: replace 
Abstract: We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).

## 🔍 Abstract (한글 번역)

arXiv:2410.22069v3 발표 유형: 교체  
초록: 우리는 깊은 동차 신경망에서 무한소 학습률을 가진 가장 가파른 하강 알고리즘의 일반적인 계열의 암묵적 편향을 연구합니다. 우리는 다음을 보여줍니다: (a) 알고리즘에 의존하는 기하학적 마진은 네트워크가 완벽한 훈련 정확도에 도달한 후 증가하기 시작하며, (b) 훈련 경로의 모든 극한점은 해당 마진 최대화 문제의 KKT 점에 해당합니다. 우리는 다양한 가장 가파른 하강 알고리즘으로 최적화된 신경망의 경로를 실험적으로 확대하여, 인기 있는 적응 방법(Adam 및 Shampoo)의 암묵적 편향과의 연결을 강조합니다.

## 📝 요약

이 논문은 깊은 동질 신경망에서 무한소 학습률을 가진 최급강하 알고리즘의 암묵적 편향을 연구합니다. 주요 기여는 다음과 같습니다: (a) 네트워크가 완벽한 훈련 정확도에 도달하면 알고리즘에 의존하는 기하학적 마진이 증가하기 시작하며, (b) 훈련 경로의 모든 극한점은 해당 마진 최대화 문제의 KKT 점에 해당합니다. 다양한 최급강하 알고리즘으로 최적화된 신경망의 경로를 실험적으로 분석하여, 인기 있는 적응형 방법(Adam과 Shampoo)의 암묵적 편향과의 연결성을 강조합니다.

## 🎯 주요 포인트


- 1. 무한소 학습률을 사용하는 깊은 동질 신경망의 가장 급한 하강 알고리즘의 암묵적 편향을 연구하였습니다.

- 2. 네트워크가 완벽한 훈련 정확도에 도달하면 알고리즘에 의존하는 기하학적 마진이 증가하기 시작합니다.

- 3. 훈련 경로의 모든 극한점은 해당 마진 최대화 문제의 KKT 점에 해당합니다.

- 4. 다양한 가장 급한 하강 알고리즘으로 최적화된 신경망의 경로를 실험적으로 분석하여 인기 있는 적응형 방법(Adam 및 Shampoo)의 암묵적 편향과의 연결을 강조합니다.


---

*Generated on 2025-09-22 15:53:03*