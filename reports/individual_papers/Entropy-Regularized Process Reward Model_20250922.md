# Entropy-Regularized Process Reward Model

**Korean Title:** ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ëœ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Process Rewards|Process Rewards]] [[keywords/specific/KL-regularized Markov Decision Processes|KL-regularized Markov Decision Processes]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Reinforcement Learning|Reinforcement Learning]] [[keywords/unique/Entropy-Regularized Process Reward Model|Entropy-Regularized Process Reward Model]] [[categories/cs.LG|cs.LG]] [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (87.9% similar) [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (87.9% similar) [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (87.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Process Rewards
**ğŸ”— Specific Connectable**: KL-regularized Markov Decision Processes
**ğŸ”¬ Broad Technical**: Large Language Models, Reinforcement Learning
**â­ Unique Technical**: Entropy-Regularized Process Reward Model
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (87.9% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (87.9% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (87.0% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1 Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.0% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (85.8% similar)


**ArXiv ID**: [2412.11006](https://arxiv.org/abs/2412.11006)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2412.11006.pdf)


**ArXiv ID**: [2412.11006](https://arxiv.org/abs/2412.11006)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2412.11006.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Entropy-Regularization
**ğŸ”— Specific Connectable**: Markov Decision Processes
**â­ Unique Technical**: Entropy-Regularized Process Reward Model
**ğŸ”¬ Broad Technical**: Reinforcement Learning, Large Language Models

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Reinforcement Learning` â€¢ 

`KL-regularized Markov Decision Processes` â€¢ 

`Entropy-Regularized Process Reward Model` â€¢ 

`Process Rewards`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2412.11006v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise in performing complex multi-step reasoning, yet they continue to struggle with mathematical reasoning, often making systematic errors. A promising solution is reinforcement learning (RL) guided by reward models, particularly those focusing on process rewards, which score each intermediate step rather than solely evaluating the final outcome. This approach is more effective at guiding policy models towards correct reasoning trajectories. In this work, we propose an entropy-regularized process reward model (ER-PRM) that integrates KL-regularized Markov Decision Processes (MDP) to balance policy optimization with the need to prevent the policy from shifting too far from its initial distribution. We derive a novel reward construction method based on the theoretical results. Our theoretical analysis shows that we could derive the optimal reward model from the initial policy sampling. Our empirical experiments on the MATH and GSM8K benchmarks demonstrate that ER-PRM consistently outperforms existing process reward models, achieving 1% improvement on GSM8K and 2-3% improvement on MATH under best-of-N evaluation, and more than 1% improvement under RLHF. These results highlight the efficacy of entropy-regularization in enhancing LLMs' reasoning capabilities.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2412.11006v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì—¬ì „íˆ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œëŠ” ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¥¼ ìì£¼ ë²”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ ë§í•œ í•´ê²°ì±…ì€ ë³´ìƒ ëª¨ë¸ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ê°•í™” í•™ìŠµ(RL)ì´ë©°, íŠ¹íˆ ìµœì¢… ê²°ê³¼ë§Œ í‰ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê° ì¤‘ê°„ ë‹¨ê³„ë¥¼ ì ìˆ˜í™”í•˜ëŠ” ê³¼ì • ë³´ìƒì— ì¤‘ì ì„ ë‘” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì •ì±… ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì¶”ë¡  ê²½ë¡œë¡œ ë‚˜ì•„ê°€ë„ë¡ ë” íš¨ê³¼ì ìœ¼ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” KL-ì •ê·œí™” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ í†µí•©í•˜ì—¬ ì •ì±… ìµœì í™”ì™€ ì´ˆê¸° ë¶„í¬ì—ì„œ ì •ì±…ì´ ë„ˆë¬´ ë©€ë¦¬ ì´ë™í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” í•„ìš”ì„± ê°„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” ê³¼ì • ë³´ìƒ ëª¨ë¸(ER-PRM)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¡ ì  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë³´ìƒ êµ¬ì„± ë°©ë²•ì„ ë„ì¶œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì´ë¡ ì  ë¶„ì„ì€ ì´ˆê¸° ì •ì±… ìƒ˜í”Œë§ì—ì„œ ìµœì ì˜ ë³´ìƒ ëª¨ë¸ì„ ë„ì¶œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ER-PRMì€ ê¸°ì¡´ì˜ ê³¼ì • ë³´ìƒ ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•˜ë©°, GSM8Kì—ì„œëŠ” 1%, MATHì—ì„œëŠ” ìµœì„ ì˜ N í‰ê°€ì—ì„œ 2-3%ì˜ ê°œì„ ì„ ë‹¬ì„±í•˜ê³ , RLHFì—ì„œëŠ” 1% ì´ìƒì˜ ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ìˆì–´ ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ì˜ íš¨ê³¼ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œ ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¥¼ ë²”í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°•í™” í•™ìŠµ(RL)ê³¼ ë³´ìƒ ëª¨ë¸ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì´ íš¨ê³¼ì ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì €ìë“¤ì€ KL-ì •ê·œí™” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ í†µí•©í•œ ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(ER-PRM)ì„ ì œì•ˆí•˜ì—¬ ì •ì±… ìµœì í™”ì™€ ì´ˆê¸° ë¶„í¬ë¡œë¶€í„°ì˜ ê³¼ë„í•œ ë³€í™” ë°©ì§€ë¥¼ ê· í˜• ìˆê²Œ ì¡°ì ˆí•©ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ì„ í†µí•´ ìµœì ì˜ ë³´ìƒ ëª¨ë¸ì„ ë„ì¶œí•  ìˆ˜ ìˆìŒì„ ë³´ì˜€ìœ¼ë©°, MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ER-PRMì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ê°€ íš¨ê³¼ì ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì—ì„œ ì ì¬ë ¥ì„ ë³´ì´ì§€ë§Œ, ì—¬ì „íˆ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œ ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¥¼ ë²”í•œë‹¤.

- 2. ë³´ìƒ ëª¨ë¸ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ê°•í™” í•™ìŠµ(RL)ì´ ìœ ë§í•œ í•´ê²°ì±…ìœ¼ë¡œ, íŠ¹íˆ ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒì— ì¤‘ì ì„ ë‘”ë‹¤.

- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” KL-ì •ê·œí™” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ í†µí•©í•œ ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(ER-PRM)ì„ ì œì•ˆí•œë‹¤.

- 4. ER-PRMì€ MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ì˜ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, GSM8Kì—ì„œ 1%, MATHì—ì„œ 2-3%ì˜ ê°œì„ ì„ ë‹¬ì„±í–ˆë‹¤.

- 5. ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ê°€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ì‹¤í—˜ ê²°ê³¼ê°€ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-22 15:53:25*