# LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs

**Korean Title:** LiteLong: LLMì„ ìœ„í•œ ìì› íš¨ìœ¨ì ì¸ ì¥ê¸° ë¬¸ë§¥ ë°ì´í„° í•©ì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multi-agent Debate

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (83.1% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (80.8% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (80.8% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (80.6% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony On Multilingual Data Allocation for Large Language Models Pretraining]] (80.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15568v1 Announce Type: cross 
Abstract: High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency. We present LiteLong, a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. Our approach leverages the BISAC book classification system to provide a comprehensive hierarchical topic organization, and then employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. For each topic, we use lightweight BM25 retrieval to obtain relevant documents and concatenate them into 128K-token training samples. Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods. LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15568v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê³ í’ˆì§ˆì˜ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ ê´‘ë²”ìœ„í•œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í›ˆë ¨í•˜ëŠ” ë° í•„ìˆ˜ì ì´ì§€ë§Œ, ê¸°ì¡´ì˜ ê´€ë ¨ì„± ê¸°ë°˜ ì§‘ê³„ë¥¼ ì‚¬ìš©í•˜ëŠ” í•©ì„± ì ‘ê·¼ ë°©ì‹ì€ ê³„ì‚° íš¨ìœ¨ì„± ë¬¸ì œì— ì§ë©´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” êµ¬ì¡°í™”ëœ ì£¼ì œ ì¡°ì§ê³¼ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ í†µí•´ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ í•©ì„±í•˜ëŠ” ìì› íš¨ìœ¨ì ì¸ ë°©ë²•ì¸ LiteLongì„ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ BISAC ë„ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ í¬ê´„ì ì¸ ê³„ì¸µì  ì£¼ì œ ì¡°ì§ì„ ì œê³µí•œ í›„, ì—¬ëŸ¬ LLMì„ ì‚¬ìš©í•œ í† ë¡  ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì´ êµ¬ì¡° ë‚´ì—ì„œ ë‹¤ì–‘í•˜ê³  ê³ í’ˆì§ˆì˜ ì£¼ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê° ì£¼ì œì— ëŒ€í•´ ê²½ëŸ‰ BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì–»ê³  ì´ë¥¼ 128K-í† í° í›ˆë ¨ ìƒ˜í”Œë¡œ ì—°ê²°í•©ë‹ˆë‹¤. HELMET ë° Ruler ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, LiteLongì€ ê²½ìŸë ¥ ìˆëŠ” ê¸´ ë¬¸ë§¥ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©° ë‹¤ë¥¸ ê¸´ ì¢…ì†ì„± ê°•í™” ë°©ë²•ê³¼ ì›í™œí•˜ê²Œ í†µí•©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. LiteLongì€ ê³„ì‚° ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë¹„ìš©ì„ ì¤„ì„ìœ¼ë¡œì¨ ê³ í’ˆì§ˆì˜ ê¸´ ë¬¸ë§¥ ë°ì´í„° í•©ì„±ì„ ë³´ë‹¤ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•˜ì—¬, ê¸´ ë¬¸ë§¥ ì–¸ì–´ í›ˆë ¨ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

LiteLongì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê³ í’ˆì§ˆì˜ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•©ì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. BISAC ë„ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ì£¼ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì§í•˜ê³ , ì—¬ëŸ¬ LLM ê°„ì˜ í† ë¡  ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë‹¤ì–‘í•œ ê³ í’ˆì§ˆ ì£¼ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê° ì£¼ì œì— ëŒ€í•´ BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ 128K í† í°ì˜ í•™ìŠµ ìƒ˜í”Œë¡œ ê²°í•©í•©ë‹ˆë‹¤. HELMET ë° Ruler ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ LiteLongì€ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ë¥¸ ê¸´ ì˜ì¡´ì„± ê°•í™” ë°©ë²•ê³¼ë„ ì›í™œí•˜ê²Œ í†µí•©ë  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³„ì‚° ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë¹„ìš©ì„ ì ˆê°í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì–¸ì–´ í•™ìŠµ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LiteLongì€ êµ¬ì¡°í™”ëœ ì£¼ì œ ì¡°ì§í™”ì™€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ í†µí•´ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•©ì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. BISAC ë„ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ í¬ê´„ì ì¸ ê³„ì¸µì  ì£¼ì œ ì¡°ì§í™”ë¥¼ ì œê³µí•˜ê³ , ì—¬ëŸ¬ LLMì˜ í† ë¡  ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë‹¤ì–‘í•œ ê³ í’ˆì§ˆ ì£¼ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

- 3. ê° ì£¼ì œì— ëŒ€í•´ ê²½ëŸ‰ BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì–»ê³  ì´ë¥¼ 128K-í† í° í›ˆë ¨ ìƒ˜í”Œë¡œ ì—°ê²°í•©ë‹ˆë‹¤.

- 4. HELMET ë° Ruler ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ LiteLongì€ ê²½ìŸë ¥ ìˆëŠ” ê¸´ ë¬¸ë§¥ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ë‹¤ë¥¸ ê¸´ ì˜ì¡´ì„± ê°•í™” ë°©ë²•ê³¼ ì›í™œí•˜ê²Œ í†µí•©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 5. LiteLongì€ ê³„ì‚° ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë¹„ìš©ì„ ì¤„ì—¬ ê³ í’ˆì§ˆ ê¸´ ë¬¸ë§¥ ë°ì´í„° í•©ì„±ì„ ë³´ë‹¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì–¸ì–´ í›ˆë ¨ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:02:56*