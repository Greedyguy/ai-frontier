# Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks

**Korean Title:** í…ì„œ ë¶„í•´ë¥¼ í†µí•œ ê²¬ê³ í•œ ë¹„ì „-ì–¸ì–´ ëª¨ë¸: ì ëŒ€ì  ê³µê²©ì— ëŒ€í•œ ë°©ì–´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Vision-Language Robustness

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (84.6% similar)
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (83.9% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (82.3% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (82.1% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual A Framework for Constructing Unified Vision-Language Datasets]] (82.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16163v1 Announce Type: cross 
Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone to adversarial attacks. Existing defenses often demand costly retraining or significant architecture changes. We introduce a lightweight defense using tensor decomposition suitable for any pre-trained VLM, requiring no retraining. By decomposing and reconstructing vision encoder representations, it filters adversarial noise while preserving meaning. Experiments with CLIP on COCO and Flickr30K show improved robustness. On Flickr30K, it restores 12.3\% performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%. Analysis shows Tensor Train decomposition with low rank (8-32) and low residual strength ($\alpha=0.1-0.2$) is optimal. This method is a practical, plug-and-play solution with minimal overhead for existing VLMs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16163v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ì´í•´ì—ì„œ ë›°ì–´ë‚˜ì§€ë§Œ, ì ëŒ€ì  ê³µê²©ì— ì·¨ì•½í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ì–´ ë°©ë²•ì€ ì¢…ì¢… ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì¬í›ˆë ¨ì´ë‚˜ ìƒë‹¹í•œ êµ¬ì¡° ë³€ê²½ì„ ìš”êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‚¬ì „ í›ˆë ¨ëœ VLMì— ì í•©í•˜ê³  ì¬í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í…ì„œ ë¶„í•´ë¥¼ ì‚¬ìš©í•œ ê²½ëŸ‰ ë°©ì–´ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë¹„ì „ ì¸ì½”ë” í‘œí˜„ì„ ë¶„í•´í•˜ê³  ì¬êµ¬ì„±í•¨ìœ¼ë¡œì¨, ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì ëŒ€ì  ë…¸ì´ì¦ˆë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤. COCOì™€ Flickr30Kì—ì„œ CLIPì„ ì‚¬ìš©í•œ ì‹¤í—˜ì€ í–¥ìƒëœ ê²¬ê³ ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Flickr30Kì—ì„œëŠ” ê³µê²©ìœ¼ë¡œ ì¸í•´ ì†ì‹¤ëœ ì„±ëŠ¥ì˜ 12.3%ë¥¼ ë³µêµ¬í•˜ì—¬ Recall@1 ì •í™•ë„ë¥¼ 7.5%ì—ì„œ 19.8%ë¡œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. COCOì—ì„œëŠ” 8.1%ì˜ ì„±ëŠ¥ì„ íšŒë³µí•˜ì—¬ ì •í™•ë„ë¥¼ 3.8%ì—ì„œ 11.9%ë¡œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, ë‚®ì€ ë­í¬(8-32)ì™€ ë‚®ì€ ì”ì—¬ ê°•ë„($\alpha=0.1-0.2$)ë¥¼ ê°€ì§„ í…ì„œ íŠ¸ë ˆì¸ ë¶„í•´ê°€ ìµœì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ VLMì— ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œë¡œ ì ìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì ì´ê³  í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ì ëŒ€ì  ê³µê²©ì— ëŒ€í•œ ê²½ëŸ‰ ë°©ì–´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ì–´ë²•ì€ ì¬í›ˆë ¨ì´ë‚˜ ì•„í‚¤í…ì²˜ ë³€ê²½ì´ í•„ìš”í•˜ì§€ë§Œ, ì´ ë°©ë²•ì€ í…ì„œ ë¶„í•´ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ VLMì— ì ìš© ê°€ëŠ¥í•˜ë©° ì¬í›ˆë ¨ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ë¹„ì „ ì¸ì½”ë” í‘œí˜„ì„ ë¶„í•´í•˜ê³  ì¬êµ¬ì„±í•˜ì—¬ ì ëŒ€ì  ë…¸ì´ì¦ˆë¥¼ ê±¸ëŸ¬ë‚´ë©´ì„œ ì˜ë¯¸ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤. CLIP ëª¨ë¸ì„ COCOì™€ Flickr30K ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, Flickr30Kì—ì„œ ê³µê²©ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì†ì‹¤ì„ 12.3% íšŒë³µí•˜ì—¬ Recall@1 ì •í™•ë„ë¥¼ 7.5%ì—ì„œ 19.8%ë¡œ, COCOì—ì„œëŠ” 8.1% íšŒë³µí•˜ì—¬ ì •í™•ë„ë¥¼ 3.8%ì—ì„œ 11.9%ë¡œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ìµœì ì˜ ì„±ëŠ¥ì€ ë‚®ì€ ë­í¬(8-32)ì™€ ë‚®ì€ ì”ì—¬ ê°•ë„($\alpha=0.1-0.2$)ì˜ í…ì„œ íŠ¸ë ˆì¸ ë¶„í•´ë¡œ ë‹¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ VLMì— ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œë¡œ ì ìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì€ ë©€í‹°ëª¨ë‹¬ ì´í•´ì—ì„œ ë›°ì–´ë‚˜ì§€ë§Œ ì ëŒ€ì  ê³µê²©ì— ì·¨ì•½í•˜ë‹¤.

- 2. ê¸°ì¡´ ë°©ì–´ ê¸°ë²•ì€ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì¬í›ˆë ¨ì´ë‚˜ ì•„í‚¤í…ì²˜ ë³€ê²½ì„ ìš”êµ¬í•œë‹¤.

- 3. í…ì„œ ë¶„í•´ë¥¼ í™œìš©í•œ ê²½ëŸ‰ ë°©ì–´ ê¸°ë²•ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ì‚¬ì „ í›ˆë ¨ëœ VLMì— ì ìš© ê°€ëŠ¥í•˜ê³  ì¬í›ˆë ¨ì´ í•„ìš” ì—†ë‹¤.

- 4. CLIPì„ ì‚¬ìš©í•œ ì‹¤í—˜ì—ì„œ COCOì™€ Flickr30K ë°ì´í„°ì…‹ì—ì„œì˜ ê°•ì¸ì„±ì´ í–¥ìƒë˜ì—ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ VLMì— ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œë¡œ ì ìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ì†”ë£¨ì…˜ì´ë‹¤.

---

*Generated on 2025-09-22 14:25:47*