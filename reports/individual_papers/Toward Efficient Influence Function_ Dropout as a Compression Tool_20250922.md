# Toward Efficient Influence Function: Dropout as a Compression Tool

**Korean Title:** íš¨ìœ¨ì ì¸ ì˜í–¥ í•¨ìˆ˜ë¡œ ë‚˜ì•„ê°€ê¸°: ì••ì¶• ë„êµ¬ë¡œì„œì˜ ë“œë¡­ì•„ì›ƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Gradient Compression, Influence Function

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (81.9% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (79.7% similar)
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (78.7% similar)
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (78.0% similar)
- [[2025-09-18/CUFG_ Curriculum Unlearning Guided by the Forgetting Gradient_20250918|CUFG Curriculum Unlearning Guided by the Forgetting Gradient]] (77.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15651v1 Announce Type: cross 
Abstract: Assessing the impact the training data on machine learning models is crucial for understanding the behavior of the model, enhancing the transparency, and selecting training data. Influence function provides a theoretical framework for quantifying the effect of training data points on model's performance given a specific test data. However, the computational and memory costs of influence function presents significant challenges, especially for large-scale models, even when using approximation methods, since the gradients involved in computation are as large as the model itself. In this work, we introduce a novel approach that leverages dropout as a gradient compression mechanism to compute the influence function more efficiently. Our method significantly reduces computational and memory overhead, not only during the influence function computation but also in gradient compression process. Through theoretical analysis and empirical validation, we demonstrate that our method could preserves critical components of the data influence and enables its application to modern large-scale models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15651v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ê°€ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•˜ê³ , íˆ¬ëª…ì„±ì„ ë†’ì´ë©°, í•™ìŠµ ë°ì´í„°ë¥¼ ì„ íƒí•˜ëŠ” ë° ìˆì–´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜í–¥ í•¨ìˆ˜ëŠ” íŠ¹ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í•™ìŠµ ë°ì´í„° í¬ì¸íŠ¸ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”í•  ìˆ˜ ìˆëŠ” ì´ë¡ ì  í‹€ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì˜í–¥ í•¨ìˆ˜ì˜ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ì€ íŠ¹íˆ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ê²½ìš° ìƒë‹¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê·¼ì‚¬ ë°©ë²•ì„ ì‚¬ìš©í•˜ë”ë¼ë„ ê³„ì‚°ì— ê´€ë ¨ëœ ê¸°ìš¸ê¸°ê°€ ëª¨ë¸ ìì²´ë§Œí¼ í¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒì„ ê¸°ìš¸ê¸° ì••ì¶• ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì˜í–¥ í•¨ìˆ˜ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ì˜í–¥ í•¨ìˆ˜ ê³„ì‚°ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ìš¸ê¸° ì••ì¶• ê³¼ì •ì—ì„œë„ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤ì¦ì  ê²€ì¦ì„ í†µí•´ ìš°ë¦¬ì˜ ë°©ë²•ì´ ë°ì´í„° ì˜í–¥ì˜ ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ë³´ì¡´í•˜ê³  í˜„ëŒ€ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì˜í–¥ í•¨ìˆ˜ëŠ” ê³„ì‚° ë¹„ìš©ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì»¤ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš©í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒì„ í™œìš©í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ í†µí•´ ì˜í–¥ í•¨ìˆ˜ ê³„ì‚°ì˜ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì´ë©´ì„œë„ ë°ì´í„° ì˜í–¥ì˜ ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ìœ ì§€í•˜ì—¬, ëŒ€ê·œëª¨ ëª¨ë¸ì—ë„ ì ìš© ê°€ëŠ¥í•¨ì„ ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤í—˜ì„ í†µí•´ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•˜ê³  íˆ¬ëª…ì„±ì„ ë†’ì´ë©° í›ˆë ¨ ë°ì´í„°ë¥¼ ì„ íƒí•˜ëŠ” ë° ì¤‘ìš”í•˜ë‹¤.

- 2. ì˜í–¥ í•¨ìˆ˜ëŠ” íŠ¹ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ í›ˆë ¨ ë°ì´í„° í¬ì¸íŠ¸ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”í•˜ëŠ” ì´ë¡ ì  í‹€ì„ ì œê³µí•œë‹¤.

- 3. ë³¸ ì—°êµ¬ëŠ” ë“œë¡­ì•„ì›ƒì„ í™œìš©í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ì—¬ ì˜í–¥ í•¨ìˆ˜ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê³ ì í•œë‹¤.

- 4. ì œì•ˆëœ ë°©ë²•ì€ ì˜í–¥ í•¨ìˆ˜ ê³„ì‚°ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ ì••ì¶• ê³¼ì •ì—ì„œì˜ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ í¬ê²Œ ì¤„ì¸ë‹¤.

- 5. ì´ ë°©ë²•ì€ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ë°ì´í„° ì˜í–¥ì˜ ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ë³´ì¡´í•¨ì„ ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤ì¦ì  ê²€ì¦ì„ í†µí•´ ì…ì¦í•˜ì˜€ë‹¤.

---

*Generated on 2025-09-22 14:06:26*