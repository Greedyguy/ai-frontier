---
keywords:
  - Foundation Models
  - Zero-Shot Learning
  - Vision-Language Model
  - Sparse-Reward Settings
  - Multi-Armed Bandits
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19924
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:53:25.987141",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Foundation Models",
    "Zero-Shot Learning",
    "Vision-Language Model",
    "Sparse-Reward Settings",
    "Multi-Armed Bandits"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Foundation Models": 0.85,
    "Zero-Shot Learning": 0.9,
    "Vision-Language Model": 0.88,
    "Sparse-Reward Settings": 0.78,
    "Multi-Armed Bandits": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "foundation models",
        "canonical": "Foundation Models",
        "aliases": [
          "foundation model"
        ],
        "category": "broad_technical",
        "rationale": "Foundation models are central to the paper's exploration of capabilities and limitations in reinforcement learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "zero-shot exploration",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot exploration is a key concept tested in the paper, linking to broader zero-shot learning discussions.",
        "novelty_score": 0.6,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "vision-language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for understanding the 'knowing-doing gap' discussed in the paper.",
        "novelty_score": 0.58,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.88
      },
      {
        "surface": "sparse-reward settings",
        "canonical": "Sparse-Reward Settings",
        "aliases": [
          "sparse reward"
        ],
        "category": "unique_technical",
        "rationale": "Sparse-reward settings are a unique challenge in reinforcement learning, directly addressed in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "multi-armed bandits",
        "canonical": "Multi-Armed Bandits",
        "aliases": [
          "bandits"
        ],
        "category": "specific_connectable",
        "rationale": "Multi-armed bandits are a classic RL benchmark used in the paper's experiments.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "exploration",
      "control"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "foundation models",
      "resolved_canonical": "Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "zero-shot exploration",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "sparse-reward settings",
      "resolved_canonical": "Sparse-Reward Settings",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multi-armed bandits",
      "resolved_canonical": "Multi-Armed Bandits",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19924.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19924](https://arxiv.org/abs/2509.19924)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Foundation Models as World Models_ A Foundational Study in Text-Based GridWorlds_20250922|Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds]] (86.6% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (85.6% similar)
- [[2025-09-23/Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?_20250923|Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?]] (84.4% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (84.0% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Foundation Models|Foundation Models]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Multi-Armed Bandits|Multi-Armed Bandits]]
**âš¡ Unique Technical**: [[keywords/Sparse-Reward Settings|Sparse-Reward Settings]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19924v1 Announce Type: cross 
Abstract: Exploration in reinforcement learning (RL) remains challenging, particularly in sparse-reward settings. While foundation models possess strong semantic priors, their capabilities as zero-shot exploration agents in classic RL benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our investigation reveals a key limitation: while VLMs can infer high-level objectives from visual input, they consistently fail at precise low-level control: the "knowing-doing gap". To analyze a potential bridge for this gap, we investigate a simple on-policy hybrid framework in a controlled, best-case scenario. Our results in this idealized setting show that VLM guidance can significantly improve early-stage sample efficiency, providing a clear analysis of the potential and constraints of using foundation models to guide exploration rather than for end-to-end control.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì—ì„œ íƒìƒ‰ì˜ ì–´ë ¤ì›€ì„ ë‹¤ë£¨ë©°, íŠ¹íˆ ë³´ìƒì´ ë“œë¬¸ í™˜ê²½ì—ì„œì˜ ë¬¸ì œë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ íƒêµ¬í•©ë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì„ ë‹¤ì–‘í•œ RL ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì œë¡œìƒ· íƒìƒ‰ ëŠ¥ë ¥ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼, VLMì´ ì‹œê°ì  ì…ë ¥ì—ì„œ ê³ ìˆ˜ì¤€ ëª©í‘œë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆì§€ë§Œ, ì„¸ë¶€ì ì¸ ì €ìˆ˜ì¤€ ì œì–´ì—ëŠ” ì‹¤íŒ¨í•˜ëŠ” "ì§€ì‹-ì‹¤í–‰ ê°„ê·¹"ì´ ì¡´ì¬í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ ì˜¨-ì •ì±… í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ê³ , ì´ìƒì ì¸ ìƒí™©ì—ì„œ VLMì˜ ì§€ë„ê°€ ì´ˆê¸° ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ê¸°ì´ˆ ëª¨ë¸ì„ íƒìƒ‰ ê°€ì´ë“œë¡œ í™œìš©í•  ë•Œì˜ ì ì¬ë ¥ê³¼ í•œê³„ë¥¼ ëª…í™•íˆ ë¶„ì„í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµì—ì„œ íƒìƒ‰ì€ íŠ¹íˆ ë³´ìƒì´ í¬ë°•í•œ í™˜ê²½ì—ì„œ ì—¬ì „íˆ ë„ì „ì ì´ë‹¤.
- 2. ê¸°ì´ˆ ëª¨ë¸ë“¤ì€ ê°•ë ¥í•œ ì˜ë¯¸ì  ì„ í—˜ ì§€ì‹ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ê³ ì „ì ì¸ ê°•í™” í•™ìŠµ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì œë¡œìƒ· íƒìƒ‰ ì—ì´ì „íŠ¸ë¡œì„œì˜ ëŠ¥ë ¥ì€ ì˜ ì´í•´ë˜ì§€ ì•Šì•˜ë‹¤.
- 3. VLMì€ ì‹œê°ì  ì…ë ¥ì—ì„œ ê³ ìˆ˜ì¤€ì˜ ëª©í‘œë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆì§€ë§Œ, ì •í™•í•œ ì €ìˆ˜ì¤€ ì œì–´ì—ì„œëŠ” ì¼ê´€ë˜ê²Œ ì‹¤íŒ¨í•˜ëŠ” "ì•Œê³  í–‰í•˜ëŠ” ê°„ê·¹"ì´ ì¡´ì¬í•œë‹¤.
- 4. ê°„ê·¹ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ ë‹¨ìˆœí•œ ì˜¨-ì •ì±… í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ì›Œí¬ë¥¼ ì´ìƒì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¡°ì‚¬í•œ ê²°ê³¼, VLMì˜ ì§€ì¹¨ì´ ì´ˆê¸° ë‹¨ê³„ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.
- 5. ê¸°ì´ˆ ëª¨ë¸ì„ íƒìƒ‰ì„ ì•ˆë‚´í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ê²ƒì˜ ì ì¬ë ¥ê³¼ ì œì•½ì„ ëª…í™•íˆ ë¶„ì„í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-25 15:53:25*