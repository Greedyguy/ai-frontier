---
keywords:
  - Stochastic Gradient Descent with Momentum
  - Neural Network
  - Mini-batch Stochastic Gradient Descent with Momentum
  - Increasing Batch Size
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2501.08883
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:05:40.699219",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Stochastic Gradient Descent with Momentum",
    "Neural Network",
    "Mini-batch Stochastic Gradient Descent with Momentum",
    "Increasing Batch Size"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Stochastic Gradient Descent with Momentum": 0.85,
    "Neural Network": 0.8,
    "Mini-batch Stochastic Gradient Descent with Momentum": 0.8,
    "Increasing Batch Size": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Stochastic Gradient Descent with Momentum",
        "canonical": "Stochastic Gradient Descent with Momentum",
        "aliases": [
          "SGDM"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific optimization technique crucial for training neural networks, enhancing connectivity to related optimization methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.8,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Deep Neural Network",
        "canonical": "Neural Network",
        "aliases": [
          "DNN"
        ],
        "category": "broad_technical",
        "rationale": "Neural networks are foundational to deep learning, linking to a wide range of machine learning topics.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Mini-batch SGDM",
        "canonical": "Mini-batch Stochastic Gradient Descent with Momentum",
        "aliases": [
          "Mini-batch SGDM"
        ],
        "category": "unique_technical",
        "rationale": "This variant of SGDM is specifically used in the context of deep learning, providing a focused link to training methodologies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Increasing Batch Size",
        "canonical": "Increasing Batch Size",
        "aliases": [
          "Variable Batch Size"
        ],
        "category": "unique_technical",
        "rationale": "This technique is highlighted for its impact on convergence, linking to optimization strategies in machine learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Stochastic Gradient Descent with Momentum",
      "resolved_canonical": "Stochastic Gradient Descent with Momentum",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.8,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Deep Neural Network",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Mini-batch SGDM",
      "resolved_canonical": "Mini-batch Stochastic Gradient Descent with Momentum",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Increasing Batch Size",
      "resolved_canonical": "Increasing Batch Size",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Increasing Batch Size Improves Convergence of Stochastic Gradient Descent with Momentum

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2501.08883.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2501.08883](https://arxiv.org/abs/2501.08883)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (91.1% similar)
- [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (87.8% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (85.8% similar)
- [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (82.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**âš¡ Unique Technical**: [[keywords/Stochastic Gradient Descent with Momentum|Stochastic Gradient Descent with Momentum]], [[keywords/Mini-batch Stochastic Gradient Descent with Momentum|Mini-batch Stochastic Gradient Descent with Momentum]], [[keywords/Increasing Batch Size|Increasing Batch Size]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.08883v2 Announce Type: replace 
Abstract: Stochastic gradient descent with momentum (SGDM), in which a momentum term is added to SGD, has been well studied in both theory and practice. The theoretical studies show that the settings of the learning rate and momentum weight affect the convergence of SGDM. Meanwhile, the practical studies have shown that the batch-size setting strongly affects the performance of SGDM. In this paper, we focus on mini-batch SGDM with a constant learning rate and constant momentum weight, which is frequently used to train deep neural networks. We show theoretically that using a constant batch size does not always minimize the expectation of the full gradient norm of the empirical loss in training a deep neural network, whereas using an increasing batch size definitely minimizes it; that is, an increasing batch size improves the convergence of mini-batch SGDM. We also provide numerical results supporting our analyses, indicating specifically that mini-batch SGDM with an increasing batch size converges to stationary points faster than with a constant batch size, while also reducing computational cost. Python implementations of the optimizers used in the numerical experiments are available at https://github.com/iiduka-researches/NSHB_increasing_batchsize_acml25/.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëª¨ë©˜í…€ì„ í¬í•¨í•œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGDM)ì˜ ì„±ëŠ¥ì— ëŒ€í•œ ì—°êµ¬ë¡œ, íŠ¹íˆ ë¯¸ë‹ˆë°°ì¹˜ SGDMì—ì„œ ì¼ì •í•œ í•™ìŠµë¥ ê³¼ ëª¨ë©˜í…€ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•  ë•Œì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ë³´ë‹¤ ê²½í—˜ì  ì†ì‹¤ì˜ ì „ì²´ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ì„ ìµœì†Œí™”í•˜ëŠ” ë° ë” íš¨ê³¼ì ì„ì„ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤. ì¦‰, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ë¯¸ë‹ˆë°°ì¹˜ SGDMì˜ ìˆ˜ë ´ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìˆ˜ì¹˜ ì‹¤í—˜ ê²°ê³¼, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ê²½ìš°ê°€ ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ê²½ìš°ë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©° ê³„ì‚° ë¹„ìš©ë„ ì ˆê°ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê´€ë ¨ ìµœì í™” ê¸°ë²•ì˜ íŒŒì´ì¬ êµ¬í˜„ì€ GitHubì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëª¨ë©˜í…€ì„ í¬í•¨í•œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGDM)ì€ í•™ìŠµë¥ ê³¼ ëª¨ë©˜í…€ ê°€ì¤‘ì¹˜ ì„¤ì •ì´ ìˆ˜ë ´ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
- 2. ì‹¤í—˜ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì„¤ì •ì´ SGDMì˜ ì„±ëŠ¥ì— ê°•í•œ ì˜í–¥ì„ ì¤€ë‹¤.
- 3. ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ë³´ë‹¤ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œ ë¯¸ë‹ˆë°°ì¹˜ SGDMì˜ ìˆ˜ë ´ì´ ê°œì„ ëœë‹¤.
- 4. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ SGDMì€ ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ë©´ì„œ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•œë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ìˆ˜ì¹˜ì  ê²°ê³¼ê°€ ì œê³µë˜ë©°, ê´€ë ¨ ìµœì í™”ê¸°ì˜ íŒŒì´ì¬ êµ¬í˜„ì´ ê³µê°œë˜ì–´ ìˆë‹¤.


---

*Generated on 2025-09-25 17:05:40*