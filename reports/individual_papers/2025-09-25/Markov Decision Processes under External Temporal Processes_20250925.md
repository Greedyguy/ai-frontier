---
keywords:
  - Markov Decision Processes
  - Machine Learning
  - Temporal Processes
  - Policy Iteration
  - Hawkes Processes
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2305.16056
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:13:53.125324",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Markov Decision Processes",
    "Machine Learning",
    "Temporal Processes",
    "Policy Iteration",
    "Hawkes Processes"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Markov Decision Processes": 0.78,
    "Machine Learning": 0.8,
    "Temporal Processes": 0.72,
    "Policy Iteration": 0.75,
    "Hawkes Processes": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Markov Decision Processes",
        "canonical": "Markov Decision Processes",
        "aliases": [
          "MDP"
        ],
        "category": "broad_technical",
        "rationale": "Markov Decision Processes are fundamental in reinforcement learning and connect well with existing literature on decision-making models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a core area of Machine Learning and connects broadly with other learning paradigms.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Temporal Processes",
        "canonical": "Temporal Processes",
        "aliases": [
          "Time-dependent Processes"
        ],
        "category": "unique_technical",
        "rationale": "Temporal Processes are crucial for understanding nonstationary environments in decision-making.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Policy Iteration Algorithm",
        "canonical": "Policy Iteration",
        "aliases": [
          "Policy Improvement"
        ],
        "category": "specific_connectable",
        "rationale": "Policy Iteration is a key method in reinforcement learning for improving decision policies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Hawkes Processes",
        "canonical": "Hawkes Processes",
        "aliases": [
          "Self-exciting Processes"
        ],
        "category": "unique_technical",
        "rationale": "Hawkes Processes are specialized models for temporal data, relevant for modeling external temporal influences.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "stationary environments",
      "finite history",
      "approximation error"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Markov Decision Processes",
      "resolved_canonical": "Markov Decision Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Temporal Processes",
      "resolved_canonical": "Temporal Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Policy Iteration Algorithm",
      "resolved_canonical": "Policy Iteration",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Hawkes Processes",
      "resolved_canonical": "Hawkes Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Markov Decision Processes under External Temporal Processes

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2305.16056.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2305.16056](https://arxiv.org/abs/2305.16056)

## 🔗 유사한 논문
- [[2025-09-19/Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks_20250919|Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks]] (85.6% similar)
- [[2025-09-22/Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses_20250922|Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses]] (83.4% similar)
- [[2025-09-25/Analysis of approximate linear programming solution to Markov decision problem with log barrier function_20250925|Analysis of approximate linear programming solution to Markov decision problem with log barrier function]] (83.3% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (81.7% similar)
- [[2025-09-25/Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning_20250925|Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning]] (81.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Markov Decision Processes|Markov Decision Processes]], [[keywords/Machine Learning|Machine Learning]]
**🔗 Specific Connectable**: [[keywords/Policy Iteration|Policy Iteration]]
**⚡ Unique Technical**: [[keywords/Temporal Processes|Temporal Processes]], [[keywords/Hawkes Processes|Hawkes Processes]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2305.16056v4 Announce Type: replace-cross 
Abstract: Reinforcement Learning Algorithms are predominantly developed for stationary environments, and the limited literature that considers nonstationary environments often involves specific assumptions about changes that can occur in transition probability matrices and reward functions. Considering that real-world applications involve environments that continuously evolve due to various external events, and humans make decisions by discerning patterns in historical events, this study investigates Markov Decision Processes under the influence of an external temporal process. We establish the conditions under which the problem becomes tractable, allowing it to be addressed by considering only a finite history of events, based on the properties of the perturbations introduced by the exogenous process. We propose and theoretically analyze a policy iteration algorithm to tackle this problem, which learns policies contingent upon the current state of the environment, as well as a finite history of prior events of the exogenous process. We show that such an algorithm is not guaranteed to converge. However, we provide a guarantee for policy improvement in regions of the state space determined by the approximation error induced by considering tractable policies and value functions. We also establish the sample complexity of least-squares policy evaluation and policy improvement algorithms that consider approximations due to the incorporation of only a finite history of temporal events. While our results are applicable to general discrete-time processes satisfying certain conditions on the rate of decay of the influence of their events, we further analyze the case of discrete-time Hawkes processes with Gaussian marks. We performed experiments to demonstrate our findings for policy evaluation and deployment in traditional control environments.

## 📝 요약

이 연구는 외부 시간적 과정의 영향을 받는 마르코프 결정 과정(MDP)을 조사하여, 비정상적인 환경에서 강화 학습 알고리즘의 적용 가능성을 탐구합니다. 연구는 외부 과정에 의해 도입된 변동의 속성을 기반으로 유한한 과거 사건만 고려하여 문제를 해결할 수 있는 조건을 설정합니다. 이를 위해 현재 환경 상태와 외부 과정의 유한한 과거 사건에 따라 정책을 학습하는 정책 반복 알고리즘을 제안하고 이론적으로 분석합니다. 알고리즘의 수렴은 보장되지 않지만, 근사 오류에 의해 결정되는 상태 공간의 영역에서 정책 개선을 보장합니다. 또한, 유한한 과거 사건만 고려한 경우의 샘플 복잡성을 규명합니다. 연구는 일반적인 이산 시간 과정에 적용 가능하며, 특히 가우시안 마크를 가진 이산 시간 호크스 과정의 경우를 추가로 분석합니다. 실험을 통해 전통적 제어 환경에서의 정책 평가 및 배포에 대한 연구 결과를 입증했습니다.

## 🎯 주요 포인트

- 1. 본 연구는 외부 시간적 프로세스의 영향을 받는 마르코프 결정 프로세스를 조사하여, 유한한 과거 사건의 기록만 고려해도 문제를 해결할 수 있는 조건을 확립했습니다.
- 2. 현재 환경 상태와 외부 프로세스의 유한한 과거 사건에 따라 정책을 학습하는 정책 반복 알고리즘을 제안하고 이론적으로 분석했습니다.
- 3. 제안된 알고리즘은 수렴을 보장하지 않지만, 근사 오류에 의해 결정된 상태 공간의 영역에서 정책 개선을 보장합니다.
- 4. 유한한 시간적 사건의 기록만을 고려한 정책 평가 및 정책 개선 알고리즘의 샘플 복잡성을 확립했습니다.
- 5. 일반적인 이산 시간 프로세스에 대한 결과를 적용할 수 있으며, 특히 가우시안 마크를 가진 이산 시간 호크스 프로세스의 경우를 추가로 분석했습니다.


---

*Generated on 2025-09-25 16:13:53*