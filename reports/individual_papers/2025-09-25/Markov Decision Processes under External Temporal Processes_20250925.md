---
keywords:
  - Markov Decision Processes
  - Machine Learning
  - Temporal Processes
  - Policy Iteration
  - Hawkes Processes
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2305.16056
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:13:53.125324",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Markov Decision Processes",
    "Machine Learning",
    "Temporal Processes",
    "Policy Iteration",
    "Hawkes Processes"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Markov Decision Processes": 0.78,
    "Machine Learning": 0.8,
    "Temporal Processes": 0.72,
    "Policy Iteration": 0.75,
    "Hawkes Processes": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Markov Decision Processes",
        "canonical": "Markov Decision Processes",
        "aliases": [
          "MDP"
        ],
        "category": "broad_technical",
        "rationale": "Markov Decision Processes are fundamental in reinforcement learning and connect well with existing literature on decision-making models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a core area of Machine Learning and connects broadly with other learning paradigms.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Temporal Processes",
        "canonical": "Temporal Processes",
        "aliases": [
          "Time-dependent Processes"
        ],
        "category": "unique_technical",
        "rationale": "Temporal Processes are crucial for understanding nonstationary environments in decision-making.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Policy Iteration Algorithm",
        "canonical": "Policy Iteration",
        "aliases": [
          "Policy Improvement"
        ],
        "category": "specific_connectable",
        "rationale": "Policy Iteration is a key method in reinforcement learning for improving decision policies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Hawkes Processes",
        "canonical": "Hawkes Processes",
        "aliases": [
          "Self-exciting Processes"
        ],
        "category": "unique_technical",
        "rationale": "Hawkes Processes are specialized models for temporal data, relevant for modeling external temporal influences.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "stationary environments",
      "finite history",
      "approximation error"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Markov Decision Processes",
      "resolved_canonical": "Markov Decision Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Temporal Processes",
      "resolved_canonical": "Temporal Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Policy Iteration Algorithm",
      "resolved_canonical": "Policy Iteration",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Hawkes Processes",
      "resolved_canonical": "Hawkes Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Markov Decision Processes under External Temporal Processes

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2305.16056.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2305.16056](https://arxiv.org/abs/2305.16056)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks_20250919|Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks]] (85.6% similar)
- [[2025-09-22/Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses_20250922|Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses]] (83.4% similar)
- [[2025-09-25/Analysis of approximate linear programming solution to Markov decision problem with log barrier function_20250925|Analysis of approximate linear programming solution to Markov decision problem with log barrier function]] (83.3% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (81.7% similar)
- [[2025-09-25/Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning_20250925|Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Markov Decision Processes|Markov Decision Processes]], [[keywords/Machine Learning|Machine Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Policy Iteration|Policy Iteration]]
**âš¡ Unique Technical**: [[keywords/Temporal Processes|Temporal Processes]], [[keywords/Hawkes Processes|Hawkes Processes]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2305.16056v4 Announce Type: replace-cross 
Abstract: Reinforcement Learning Algorithms are predominantly developed for stationary environments, and the limited literature that considers nonstationary environments often involves specific assumptions about changes that can occur in transition probability matrices and reward functions. Considering that real-world applications involve environments that continuously evolve due to various external events, and humans make decisions by discerning patterns in historical events, this study investigates Markov Decision Processes under the influence of an external temporal process. We establish the conditions under which the problem becomes tractable, allowing it to be addressed by considering only a finite history of events, based on the properties of the perturbations introduced by the exogenous process. We propose and theoretically analyze a policy iteration algorithm to tackle this problem, which learns policies contingent upon the current state of the environment, as well as a finite history of prior events of the exogenous process. We show that such an algorithm is not guaranteed to converge. However, we provide a guarantee for policy improvement in regions of the state space determined by the approximation error induced by considering tractable policies and value functions. We also establish the sample complexity of least-squares policy evaluation and policy improvement algorithms that consider approximations due to the incorporation of only a finite history of temporal events. While our results are applicable to general discrete-time processes satisfying certain conditions on the rate of decay of the influence of their events, we further analyze the case of discrete-time Hawkes processes with Gaussian marks. We performed experiments to demonstrate our findings for policy evaluation and deployment in traditional control environments.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì™¸ë¶€ ì‹œê°„ì  ê³¼ì •ì˜ ì˜í–¥ì„ ë°›ëŠ” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ ì¡°ì‚¬í•˜ì—¬, ë¹„ì •ìƒì ì¸ í™˜ê²½ì—ì„œ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì ìš© ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì™¸ë¶€ ê³¼ì •ì— ì˜í•´ ë„ì…ëœ ë³€ë™ì˜ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ í•œí•œ ê³¼ê±° ì‚¬ê±´ë§Œ ê³ ë ¤í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ í˜„ì¬ í™˜ê²½ ìƒíƒœì™€ ì™¸ë¶€ ê³¼ì •ì˜ ìœ í•œí•œ ê³¼ê±° ì‚¬ê±´ì— ë”°ë¼ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ì •ì±… ë°˜ë³µ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ê³  ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ì€ ë³´ì¥ë˜ì§€ ì•Šì§€ë§Œ, ê·¼ì‚¬ ì˜¤ë¥˜ì— ì˜í•´ ê²°ì •ë˜ëŠ” ìƒíƒœ ê³µê°„ì˜ ì˜ì—­ì—ì„œ ì •ì±… ê°œì„ ì„ ë³´ì¥í•©ë‹ˆë‹¤. ë˜í•œ, ìœ í•œí•œ ê³¼ê±° ì‚¬ê±´ë§Œ ê³ ë ¤í•œ ê²½ìš°ì˜ ìƒ˜í”Œ ë³µì¡ì„±ì„ ê·œëª…í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì¼ë°˜ì ì¸ ì´ì‚° ì‹œê°„ ê³¼ì •ì— ì ìš© ê°€ëŠ¥í•˜ë©°, íŠ¹íˆ ê°€ìš°ì‹œì•ˆ ë§ˆí¬ë¥¼ ê°€ì§„ ì´ì‚° ì‹œê°„ í˜¸í¬ìŠ¤ ê³¼ì •ì˜ ê²½ìš°ë¥¼ ì¶”ê°€ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´ ì „í†µì  ì œì–´ í™˜ê²½ì—ì„œì˜ ì •ì±… í‰ê°€ ë° ë°°í¬ì— ëŒ€í•œ ì—°êµ¬ ê²°ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì™¸ë¶€ ì‹œê°„ì  í”„ë¡œì„¸ìŠ¤ì˜ ì˜í–¥ì„ ë°›ëŠ” ë§ˆë¥´ì½”í”„ ê²°ì • í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ì‚¬í•˜ì—¬, ìœ í•œí•œ ê³¼ê±° ì‚¬ê±´ì˜ ê¸°ë¡ë§Œ ê³ ë ¤í•´ë„ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ í™•ë¦½í–ˆìŠµë‹ˆë‹¤.
- 2. í˜„ì¬ í™˜ê²½ ìƒíƒœì™€ ì™¸ë¶€ í”„ë¡œì„¸ìŠ¤ì˜ ìœ í•œí•œ ê³¼ê±° ì‚¬ê±´ì— ë”°ë¼ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ì •ì±… ë°˜ë³µ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ê³  ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
- 3. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ë ´ì„ ë³´ì¥í•˜ì§€ ì•Šì§€ë§Œ, ê·¼ì‚¬ ì˜¤ë¥˜ì— ì˜í•´ ê²°ì •ëœ ìƒíƒœ ê³µê°„ì˜ ì˜ì—­ì—ì„œ ì •ì±… ê°œì„ ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 4. ìœ í•œí•œ ì‹œê°„ì  ì‚¬ê±´ì˜ ê¸°ë¡ë§Œì„ ê³ ë ¤í•œ ì •ì±… í‰ê°€ ë° ì •ì±… ê°œì„  ì•Œê³ ë¦¬ì¦˜ì˜ ìƒ˜í”Œ ë³µì¡ì„±ì„ í™•ë¦½í–ˆìŠµë‹ˆë‹¤.
- 5. ì¼ë°˜ì ì¸ ì´ì‚° ì‹œê°„ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ê°€ìš°ì‹œì•ˆ ë§ˆí¬ë¥¼ ê°€ì§„ ì´ì‚° ì‹œê°„ í˜¸í¬ìŠ¤ í”„ë¡œì„¸ìŠ¤ì˜ ê²½ìš°ë¥¼ ì¶”ê°€ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:13:53*