---
keywords:
  - Exact Positional Embeddings
  - Transformer
  - Positional Encoding
  - Causal Language Modeling
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.19569
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:43:10.325348",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Exact Positional Embeddings",
    "Transformer",
    "Positional Encoding",
    "Causal Language Modeling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Exact Positional Embeddings": 0.92,
    "Transformer": 0.85,
    "Positional Encoding": 0.8,
    "Causal Language Modeling": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Exact Positional Embeddings",
        "canonical": "Exact Positional Embeddings",
        "aliases": [
          "ExPE"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for positional encoding that enhances transformer models' extrapolation capabilities.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.92
      },
      {
        "surface": "Transformer Models",
        "canonical": "Transformer",
        "aliases": [
          "Transformer Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on enhancing transformer models with new positional embeddings.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Positional Encoding",
        "canonical": "Positional Encoding",
        "aliases": [
          "Position Embeddings"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in improving the model's ability to handle longer sequences.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Causal Language Modeling",
        "canonical": "Causal Language Modeling",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Relevant to the application of the proposed embeddings in reducing perplexity.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "embedding vectors",
      "token positions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Exact Positional Embeddings",
      "resolved_canonical": "Exact Positional Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Transformer Models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Positional Encoding",
      "resolved_canonical": "Positional Encoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Causal Language Modeling",
      "resolved_canonical": "Causal Language Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19569.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.19569](https://arxiv.org/abs/2509.19569)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features_20250923|Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features]] (85.6% similar)
- [[2025-09-23/Context-aware Biases for Length Extrapolation_20250923|Context-aware Biases for Length Extrapolation]] (81.7% similar)
- [[2025-09-22/Positional Encoding in Transformer-Based Time Series Models_ A Survey_20250922|Positional Encoding in Transformer-Based Time Series Models: A Survey]] (80.6% similar)
- [[2025-09-19/DyWPE_ Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers_20250919|DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers]] (80.5% similar)
- [[2025-09-23/Proxy-Embedding as an Adversarial Teacher_ An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models_20250923|Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models]] (78.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Positional Encoding|Positional Encoding]], [[keywords/Causal Language Modeling|Causal Language Modeling]]
**âš¡ Unique Technical**: [[keywords/Exact Positional Embeddings|Exact Positional Embeddings]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19569v1 Announce Type: new 
Abstract: This paper introduces a novel approach to position embeddings in transformer models, named "Exact Positional Embeddings" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ "ì •í™•í•œ ìœ„ì¹˜ ì„ë² ë”©(ExPE)"ì´ë¼ëŠ” ìƒˆë¡œìš´ ìœ„ì¹˜ ì„ë² ë”© ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ExPEëŠ” í›ˆë ¨ëœ ì‹œí€€ìŠ¤ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ì™¸ì‚½í•  ìˆ˜ ìˆëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”© ë°©ë²•ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì ˆëŒ€ ë˜ëŠ” ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ìœ„ì¹˜ ì •ë³´ë¥¼ í†µí•©í•˜ì§€ë§Œ, ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì„ë² ë”© ë²¡í„°ì˜ íŠ¹ì • ì°¨ì›ì„ ì¬ì •ì˜í•˜ì—¬ ì •í™•í•œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•¨ìœ¼ë¡œì¨ í† í° ìœ„ì¹˜ë¥¼ ë” ì •ë°€í•˜ê²Œ í‘œí˜„í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì›ë˜ ì„ë² ë”©ì˜ ë¬´ê²°ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ë” ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ExPE ì„ë² ë”©ì€ í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ì‹œí€€ìŠ¤ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ íšŒì „ ë° ì‚¬ì¸ ì„ë² ë”©ë³´ë‹¤ ë‹¹í˜¹ë„ë¥¼ í¬ê²Œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. "Exact Positional Embeddings" (ExPE)ë¼ëŠ” ìƒˆë¡œìš´ ìœ„ì¹˜ ì„ë² ë”© ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ExPEëŠ” í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ì‹œí€€ìŠ¤ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ì™¸ì‚½í•  ìˆ˜ ìˆëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”© ë°©ë²•ì…ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ì„ë² ë”© ë²¡í„°ì˜ íŠ¹ì • ì°¨ì›ì„ ì¬ì •ì˜í•˜ì—¬ ì •í™•í•œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
- 4. ExPEëŠ” ì›ë˜ ì„ë² ë”©ì˜ ë¬´ê²°ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë” ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ExPE ì„ë² ë”©ì€ í›ˆë ¨ì— ì‚¬ìš©ëœ ì‹œí€€ìŠ¤ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ íšŒì „ ë° ì‚¬ì¸íŒŒ ì„ë² ë”©ë³´ë‹¤ ë‹¹í˜¹ë„ë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:43:10*