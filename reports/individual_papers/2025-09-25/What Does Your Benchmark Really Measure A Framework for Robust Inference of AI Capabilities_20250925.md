---
keywords:
  - AI Capabilities
  - Benchmark Evaluation
  - Inference Framework
  - Sensitivity to Perturbations
  - Adaptive Algorithm
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19590
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:15:43.973228",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AI Capabilities",
    "Benchmark Evaluation",
    "Inference Framework",
    "Sensitivity to Perturbations",
    "Adaptive Algorithm"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AI Capabilities": 0.75,
    "Benchmark Evaluation": 0.77,
    "Inference Framework": 0.8,
    "Sensitivity to Perturbations": 0.78,
    "Adaptive Algorithm": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AI Capabilities",
        "canonical": "AI Capabilities",
        "aliases": [
          "Artificial Intelligence Capabilities"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's theme, linking AI capabilities with evaluation methods enhances understanding of AI performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Benchmark Evaluation",
        "canonical": "Benchmark Evaluation",
        "aliases": [
          "Benchmark Testing",
          "Performance Benchmarking"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the evaluation process, which is crucial for assessing AI models and linking to performance metrics.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Inference Framework",
        "canonical": "Inference Framework",
        "aliases": [
          "Evaluation as Inference"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to AI evaluation, providing a basis for linking theoretical and practical evaluation methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "Sensitivity to Perturbations",
        "canonical": "Sensitivity to Perturbations",
        "aliases": [
          "Perturbation Sensitivity"
        ],
        "category": "unique_technical",
        "rationale": "Addresses a key challenge in AI evaluation, linking robustness and reliability of AI models.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.79,
        "link_intent_score": 0.78
      },
      {
        "surface": "Adaptive Algorithm",
        "canonical": "Adaptive Algorithm",
        "aliases": [
          "Adaptive Method"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights a method that improves evaluation efficiency, linking to advancements in AI testing methodologies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.76,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "Evaluation",
      "Performance",
      "Model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AI Capabilities",
      "resolved_canonical": "AI Capabilities",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Benchmark Evaluation",
      "resolved_canonical": "Benchmark Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Inference Framework",
      "resolved_canonical": "Inference Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Sensitivity to Perturbations",
      "resolved_canonical": "Sensitivity to Perturbations",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.79,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Adaptive Algorithm",
      "resolved_canonical": "Adaptive Algorithm",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.76,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19590.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19590](https://arxiv.org/abs/2509.19590)

## 🔗 유사한 논문
- [[2025-09-22/Where Fact Ends and Fairness Begins_ Redefining AI Bias Evaluation through Cognitive Biases_20250922|Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases]] (85.8% similar)
- [[2025-09-24/The Illusion of Readiness_ Stress Testing Large Frontier Models on Multimodal Medical Benchmarks_20250924|The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks]] (85.4% similar)
- [[2025-09-22/PRISM_ Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images_20250922|PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images]] (83.0% similar)
- [[2025-09-22/Understanding AI Evaluation Patterns_ How Different GPT Models Assess Vision-Language Descriptions_20250922|Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions]] (82.9% similar)
- [[2025-09-24/Unlearning as Ablation_ Toward a Falsifiable Benchmark for Generative Scientific Discovery_20250924|Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery]] (82.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Adaptive Algorithm|Adaptive Algorithm]]
**⚡ Unique Technical**: [[keywords/AI Capabilities|AI Capabilities]], [[keywords/Benchmark Evaluation|Benchmark Evaluation]], [[keywords/Inference Framework|Inference Framework]], [[keywords/Sensitivity to Perturbations|Sensitivity to Perturbations]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19590v1 Announce Type: new 
Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and their outcomes critically shape public and scientific expectations of AI's capabilities. Yet growing skepticism surrounds their reliability. How can we know that a reported accuracy genuinely reflects a model's true performance? Evaluations are often presented as simple measurements, but in reality they are inferences: to treat benchmark scores as evidence of capability is already to assume a theory of what capability is and how it manifests in a test. We make this step explicit by proposing a principled framework for evaluation as inference: begin from a theory of capability, and then derive methods for estimating it. This perspective, familiar in fields such as psychometrics, has not yet become commonplace in AI evaluation. As a proof of concept, we address a central challenge that undermines reliability: sensitivity to perturbations. After formulating a model of ability, we introduce methods that infer ability while accounting for uncertainty from sensitivity and finite samples, including an adaptive algorithm that significantly reduces sample complexity. Together, these contributions lay the groundwork for more reliable and trustworthy estimates of AI capabilities as measured through benchmarks.

## 📝 요약

이 논문은 생성 모델의 벤치마크 평가의 신뢰성 문제를 다룹니다. 저자들은 평가를 단순한 측정이 아닌 추론으로 보고, 능력에 대한 이론을 바탕으로 평가 방법을 제안합니다. 특히, 평가의 신뢰성을 저해하는 요인인 민감성 문제를 해결하기 위해, 민감성과 유한 샘플로 인한 불확실성을 고려한 능력 추론 방법을 제시합니다. 또한, 샘플 복잡성을 크게 줄이는 적응 알고리즘을 도입하여 AI 능력의 신뢰할 수 있는 평가를 위한 기초를 마련합니다.

## 🎯 주요 포인트

- 1. 생성 모델의 벤치마크 데이터 평가 결과는 AI 능력에 대한 기대를 형성하지만, 그 신뢰성에 대한 회의가 증가하고 있다.
- 2. 벤치마크 점수를 능력의 증거로 간주하는 것은 이미 능력에 대한 이론을 가정하는 것이며, 이를 명시적으로 평가 프레임워크로 제안한다.
- 3. 심리측정학 등 다른 분야에서 익숙한 이 관점은 AI 평가에서는 아직 일반적이지 않다.
- 4. 평가의 신뢰성을 저해하는 주요 문제인 변동성에 대한 민감성을 해결하기 위한 방법을 제안한다.
- 5. 제안된 방법은 샘플 복잡성을 크게 줄이는 적응 알고리즘을 포함하여, 불확실성을 고려한 능력 추론을 가능하게 한다.


---

*Generated on 2025-09-25 15:15:43*