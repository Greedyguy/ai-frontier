---
keywords:
  - AI Capabilities
  - Benchmark Evaluation
  - Inference Framework
  - Sensitivity to Perturbations
  - Adaptive Algorithm
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19590
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:15:43.973228",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AI Capabilities",
    "Benchmark Evaluation",
    "Inference Framework",
    "Sensitivity to Perturbations",
    "Adaptive Algorithm"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AI Capabilities": 0.75,
    "Benchmark Evaluation": 0.77,
    "Inference Framework": 0.8,
    "Sensitivity to Perturbations": 0.78,
    "Adaptive Algorithm": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AI Capabilities",
        "canonical": "AI Capabilities",
        "aliases": [
          "Artificial Intelligence Capabilities"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's theme, linking AI capabilities with evaluation methods enhances understanding of AI performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Benchmark Evaluation",
        "canonical": "Benchmark Evaluation",
        "aliases": [
          "Benchmark Testing",
          "Performance Benchmarking"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the evaluation process, which is crucial for assessing AI models and linking to performance metrics.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Inference Framework",
        "canonical": "Inference Framework",
        "aliases": [
          "Evaluation as Inference"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to AI evaluation, providing a basis for linking theoretical and practical evaluation methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "Sensitivity to Perturbations",
        "canonical": "Sensitivity to Perturbations",
        "aliases": [
          "Perturbation Sensitivity"
        ],
        "category": "unique_technical",
        "rationale": "Addresses a key challenge in AI evaluation, linking robustness and reliability of AI models.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.79,
        "link_intent_score": 0.78
      },
      {
        "surface": "Adaptive Algorithm",
        "canonical": "Adaptive Algorithm",
        "aliases": [
          "Adaptive Method"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights a method that improves evaluation efficiency, linking to advancements in AI testing methodologies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.76,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "Evaluation",
      "Performance",
      "Model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AI Capabilities",
      "resolved_canonical": "AI Capabilities",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Benchmark Evaluation",
      "resolved_canonical": "Benchmark Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Inference Framework",
      "resolved_canonical": "Inference Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Sensitivity to Perturbations",
      "resolved_canonical": "Sensitivity to Perturbations",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.79,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Adaptive Algorithm",
      "resolved_canonical": "Adaptive Algorithm",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.76,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19590.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19590](https://arxiv.org/abs/2509.19590)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Where Fact Ends and Fairness Begins_ Redefining AI Bias Evaluation through Cognitive Biases_20250922|Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases]] (85.8% similar)
- [[2025-09-24/The Illusion of Readiness_ Stress Testing Large Frontier Models on Multimodal Medical Benchmarks_20250924|The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks]] (85.4% similar)
- [[2025-09-22/PRISM_ Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images_20250922|PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images]] (83.0% similar)
- [[2025-09-22/Understanding AI Evaluation Patterns_ How Different GPT Models Assess Vision-Language Descriptions_20250922|Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions]] (82.9% similar)
- [[2025-09-24/Unlearning as Ablation_ Toward a Falsifiable Benchmark for Generative Scientific Discovery_20250924|Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery]] (82.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Adaptive Algorithm|Adaptive Algorithm]]
**âš¡ Unique Technical**: [[keywords/AI Capabilities|AI Capabilities]], [[keywords/Benchmark Evaluation|Benchmark Evaluation]], [[keywords/Inference Framework|Inference Framework]], [[keywords/Sensitivity to Perturbations|Sensitivity to Perturbations]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19590v1 Announce Type: new 
Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and their outcomes critically shape public and scientific expectations of AI's capabilities. Yet growing skepticism surrounds their reliability. How can we know that a reported accuracy genuinely reflects a model's true performance? Evaluations are often presented as simple measurements, but in reality they are inferences: to treat benchmark scores as evidence of capability is already to assume a theory of what capability is and how it manifests in a test. We make this step explicit by proposing a principled framework for evaluation as inference: begin from a theory of capability, and then derive methods for estimating it. This perspective, familiar in fields such as psychometrics, has not yet become commonplace in AI evaluation. As a proof of concept, we address a central challenge that undermines reliability: sensitivity to perturbations. After formulating a model of ability, we introduce methods that infer ability while accounting for uncertainty from sensitivity and finite samples, including an adaptive algorithm that significantly reduces sample complexity. Together, these contributions lay the groundwork for more reliable and trustworthy estimates of AI capabilities as measured through benchmarks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìƒì„± ëª¨ë¸ì˜ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì˜ ì‹ ë¢°ì„± ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì €ìë“¤ì€ í‰ê°€ë¥¼ ë‹¨ìˆœí•œ ì¸¡ì •ì´ ì•„ë‹Œ ì¶”ë¡ ìœ¼ë¡œ ë³´ê³ , ëŠ¥ë ¥ì— ëŒ€í•œ ì´ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ í‰ê°€ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, í‰ê°€ì˜ ì‹ ë¢°ì„±ì„ ì €í•´í•˜ëŠ” ìš”ì¸ì¸ ë¯¼ê°ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë¯¼ê°ì„±ê³¼ ìœ í•œ ìƒ˜í”Œë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ ëŠ¥ë ¥ ì¶”ë¡  ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ìƒ˜í”Œ ë³µì¡ì„±ì„ í¬ê²Œ ì¤„ì´ëŠ” ì ì‘ ì•Œê³ ë¦¬ì¦˜ì„ ë„ì…í•˜ì—¬ AI ëŠ¥ë ¥ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ë¥¼ ìœ„í•œ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìƒì„± ëª¨ë¸ì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í‰ê°€ ê²°ê³¼ëŠ” AI ëŠ¥ë ¥ì— ëŒ€í•œ ê¸°ëŒ€ë¥¼ í˜•ì„±í•˜ì§€ë§Œ, ê·¸ ì‹ ë¢°ì„±ì— ëŒ€í•œ íšŒì˜ê°€ ì¦ê°€í•˜ê³  ìˆë‹¤.
- 2. ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ë¥¼ ëŠ¥ë ¥ì˜ ì¦ê±°ë¡œ ê°„ì£¼í•˜ëŠ” ê²ƒì€ ì´ë¯¸ ëŠ¥ë ¥ì— ëŒ€í•œ ì´ë¡ ì„ ê°€ì •í•˜ëŠ” ê²ƒì´ë©°, ì´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¡œ ì œì•ˆí•œë‹¤.
- 3. ì‹¬ë¦¬ì¸¡ì •í•™ ë“± ë‹¤ë¥¸ ë¶„ì•¼ì—ì„œ ìµìˆ™í•œ ì´ ê´€ì ì€ AI í‰ê°€ì—ì„œëŠ” ì•„ì§ ì¼ë°˜ì ì´ì§€ ì•Šë‹¤.
- 4. í‰ê°€ì˜ ì‹ ë¢°ì„±ì„ ì €í•´í•˜ëŠ” ì£¼ìš” ë¬¸ì œì¸ ë³€ë™ì„±ì— ëŒ€í•œ ë¯¼ê°ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ìƒ˜í”Œ ë³µì¡ì„±ì„ í¬ê²Œ ì¤„ì´ëŠ” ì ì‘ ì•Œê³ ë¦¬ì¦˜ì„ í¬í•¨í•˜ì—¬, ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ ëŠ¥ë ¥ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.


---

*Generated on 2025-09-25 15:15:43*