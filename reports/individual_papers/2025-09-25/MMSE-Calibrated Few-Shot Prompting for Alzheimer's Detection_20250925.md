---
keywords:
  - Few-Shot Learning
  - Zero-Shot Learning
  - Multimodal Learning
  - MMSE-Proxy Prompting
  - Reasoning-augmented Prompting
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19926
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:42:32.178612",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "Zero-Shot Learning",
    "Multimodal Learning",
    "MMSE-Proxy Prompting",
    "Reasoning-augmented Prompting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.78,
    "Zero-Shot Learning": 0.77,
    "Multimodal Learning": 0.79,
    "MMSE-Proxy Prompting": 0.72,
    "Reasoning-augmented Prompting": 0.71
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Few-Shot Prompting",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Prompting"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is a trending concept that enhances model adaptability with minimal data, relevant to the study's methodology.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Prompting",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot Prompting"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a key concept in leveraging models without task-specific training, aligning with the paper's approach.",
        "novelty_score": 0.58,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multimodal LLM",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Language Model"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is crucial for integrating diverse data types, which is central to the paper's methodology.",
        "novelty_score": 0.6,
        "connectivity_score": 0.87,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      },
      {
        "surface": "MMSE-Proxy Prompting",
        "canonical": "MMSE-Proxy Prompting",
        "aliases": [
          "MMSE Anchored Prompting"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel technique specific to the study, linking cognitive assessment with model prompting.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Reasoning-augmented Prompting",
        "canonical": "Reasoning-augmented Prompting",
        "aliases": [
          "Reasoning Enhanced Prompting"
        ],
        "category": "unique_technical",
        "rationale": "This method enhances interpretability and aligns with the study's focus on reasoning capabilities.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.78,
        "link_intent_score": 0.71
      }
    ],
    "ban_list_suggestions": [
      "Prompting",
      "Accuracy",
      "AUC"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Few-Shot Prompting",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Prompting",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multimodal LLM",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.87,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "MMSE-Proxy Prompting",
      "resolved_canonical": "MMSE-Proxy Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Reasoning-augmented Prompting",
      "resolved_canonical": "Reasoning-augmented Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.78,
        "link_intent": 0.71
      }
    }
  ]
}
-->

# MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19926.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19926](https://arxiv.org/abs/2509.19926)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (83.6% similar)
- [[2025-09-25/PromptCoT 2.0_ Scaling Prompt Synthesis for Large Language Model Reasoning_20250925|PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning]] (81.7% similar)
- [[2025-09-23/QA-prompting_ Improving Summarization with Large Language Models using Question-Answering_20250923|QA-prompting: Improving Summarization with Large Language Models using Question-Answering]] (81.6% similar)
- [[2025-09-22/A Layered Multi-Expert Framework for Long-Context Mental Health Assessments_20250922|A Layered Multi-Expert Framework for Long-Context Mental Health Assessments]] (81.4% similar)
- [[2025-09-19/Calibration-Aware Prompt Learning for Medical Vision-Language Models_20250919|Calibration-Aware Prompt Learning for Medical Vision-Language Models]] (81.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/MMSE-Proxy Prompting|MMSE-Proxy Prompting]], [[keywords/Reasoning-augmented Prompting|Reasoning-augmented Prompting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19926v1 Announce Type: new 
Abstract: Prompting large language models is a training-free method for detecting Alzheimer's disease from speech transcripts. Using the ADReSS dataset, we revisit zero-shot prompting and study few-shot prompting with a class-balanced protocol using nested interleave and a strict schema, sweeping up to 20 examples per class. We evaluate two variants achieving state-of-the-art prompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a probability anchored to Mini-Mental State Examination bands via a deterministic mapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii) Reasoning-augmented Prompting: few-shot examples pool is generated with a multimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript, and MMSE to output a reasoning and MMSE-aligned probability; evaluation remains transcript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this is the first ADReSS study to anchor elicited probabilities to MMSE and to use multimodal construction to improve interpretability.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í›ˆë ¨ ì—†ì´ë„ ìŒì„± ì „ì‚¬ì—ì„œ ì•Œì¸ í•˜ì´ë¨¸ ë³‘ì„ ê°ì§€í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ADReSS ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì œë¡œìƒ· ë° í“¨ìƒ· í”„ë¡¬íŒ…ì„ ì—°êµ¬í•˜ì˜€ìœ¼ë©°, í´ë˜ìŠ¤ ê· í˜• í”„ë¡œí† ì½œì„ í†µí•´ ìµœëŒ€ 20ê°œì˜ ì˜ˆì‹œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ë¡ ì„ í‰ê°€í–ˆìœ¼ë©°, ì²« ë²ˆì§¸ëŠ” MMSE-Proxy í”„ë¡¬íŒ…ìœ¼ë¡œ, MMSE ë°´ë“œì— ê¸°ë°˜í•œ í™•ë¥ ì„ ë¶€ì—¬í•˜ì—¬ AUCë¥¼ ê³„ì‚°í•˜ë©° 0.82ì˜ ì •í™•ë„ì™€ 0.86ì˜ AUCë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” Reasoning-augmented í”„ë¡¬íŒ…ìœ¼ë¡œ, GPT-5ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ëª¨ë‹¬ ì…ë ¥ì„ í†µí•´ ì¶”ë¡ ê³¼ MMSE ì •ë ¬ í™•ë¥ ì„ ìƒì„±í•˜ì—¬ 0.82ì˜ ì •í™•ë„ì™€ 0.83ì˜ AUCë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” MMSEì— ê¸°ë°˜í•œ í™•ë¥ ì„ ë„ì…í•˜ê³  ë‹¤ì¤‘ ëª¨ë‹¬ êµ¬ì„±ì„ í†µí•´ í•´ì„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚¨ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í”„ë¡¬í”„íŠ¸ ê¸°ë²•ì€ í›ˆë ¨ ì—†ì´ë„ ìŒì„± ì „ì‚¬ë³¸ì—ì„œ ì•Œì¸ í•˜ì´ë¨¸ë³‘ì„ ê°ì§€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤.
- 2. ADReSS ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì œë¡œìƒ· í”„ë¡¬í”„íŠ¸ì™€ í´ë˜ìŠ¤ ê· í˜• í”„ë¡œí† ì½œì„ í†µí•œ í“¨ìƒ· í”„ë¡¬í”„íŠ¸ë¥¼ ì—°êµ¬í•˜ì˜€ë‹¤.
- 3. MMSE-Proxy í”„ë¡¬í”„íŠ¸ëŠ” Mini-Mental State Examination ë°´ë“œì— í™•ë¥ ì„ ì—°ê²°í•˜ì—¬ AUCë¥¼ ê³„ì‚°í•˜ë©°, 0.82ì˜ ì •í™•ë„ì™€ 0.86ì˜ AUCë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.
- 4. Reasoning-augmented í”„ë¡¬í”„íŠ¸ëŠ” ë©€í‹°ëª¨ë‹¬ LLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ë° MMSE ì •ë ¬ í™•ë¥ ì„ ìƒì„±í•˜ë©°, 0.82ì˜ ì •í™•ë„ì™€ 0.83ì˜ AUCë¥¼ ê¸°ë¡í•˜ì˜€ë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ADReSS ì—°êµ¬ ì¤‘ ìµœì´ˆë¡œ MMSEì— í™•ë¥ ì„ ì—°ê²°í•˜ê³  ë©€í‹°ëª¨ë‹¬ êµ¬ì„±ì„ í†µí•´ í•´ì„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œì¼°ë‹¤.


---

*Generated on 2025-09-25 16:42:32*