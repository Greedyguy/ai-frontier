---
keywords:
  - Vision-Language Model
  - Monocular Depth Estimation
  - Mirror Embedding Matrix
  - Dense Depth Prediction
  - NYU Depth Dataset
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2402.03251
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:14:50.431223",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Monocular Depth Estimation",
    "Mirror Embedding Matrix",
    "Dense Depth Prediction",
    "NYU Depth Dataset"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "Monocular Depth Estimation": 0.85,
    "Mirror Embedding Matrix": 0.8,
    "Dense Depth Prediction": 0.78,
    "NYU Depth Dataset": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "CLIP",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Contrastive Language–Image Pre-training"
        ],
        "category": "evolved_concepts",
        "rationale": "CLIP is central to the paper's approach and aligns with the trending concept of Vision-Language Models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "monocular depth estimation",
        "canonical": "Monocular Depth Estimation",
        "aliases": [
          "single-image depth estimation"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task addressed in the paper, relevant for linking to depth estimation techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "mirror",
        "canonical": "Mirror Embedding Matrix",
        "aliases": [
          "mirror matrix"
        ],
        "category": "unique_technical",
        "rationale": "Introduced as a novel component in the paper, it is crucial for understanding the proposed method.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.95,
        "link_intent_score": 0.8
      },
      {
        "surface": "dense depth prediction",
        "canonical": "Dense Depth Prediction",
        "aliases": [
          "dense depth estimation"
        ],
        "category": "unique_technical",
        "rationale": "A key output of the proposed method, relevant for linking to depth prediction models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "NYU Depth v2",
        "canonical": "NYU Depth Dataset",
        "aliases": [
          "NYU Depth v2 dataset"
        ],
        "category": "specific_connectable",
        "rationale": "A benchmark dataset used for evaluation, important for linking to related datasets and benchmarks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "depth understanding",
      "semantic prior",
      "conventional depth models"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "CLIP",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "monocular depth estimation",
      "resolved_canonical": "Monocular Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "mirror",
      "resolved_canonical": "Mirror Embedding Matrix",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.95,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "dense depth prediction",
      "resolved_canonical": "Dense Depth Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "NYU Depth v2",
      "resolved_canonical": "NYU Depth Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# CLIP Can Understand Depth

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2402.03251.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2402.03251](https://arxiv.org/abs/2402.03251)

## 🔗 유사한 논문
- [[2025-09-23/CLIP-IN_ Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions_20250923|CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions]] (87.7% similar)
- [[2025-09-23/MoCLIP-Lite_ Efficient Video Recognition by Fusing CLIP with Motion Vectors_20250923|MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors]] (85.3% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (84.8% similar)
- [[2025-09-24/Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment_20250924|Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment]] (84.5% similar)
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (84.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/NYU Depth Dataset|NYU Depth Dataset]]
**⚡ Unique Technical**: [[keywords/Monocular Depth Estimation|Monocular Depth Estimation]], [[keywords/Mirror Embedding Matrix|Mirror Embedding Matrix]], [[keywords/Dense Depth Prediction|Dense Depth Prediction]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2402.03251v2 Announce Type: replace-cross 
Abstract: In this paper, we demonstrate that CLIP can also be adapted to downstream tasks where its vision-language alignment is suboptimally learned during pre-training on web-crawled data, all without requiring fine-tuning. We explore the case of monocular depth estimation, where CLIP's contrastive prior struggles to generalize, compared to its success in domains such as generative modeling and semantic segmentation. Since CLIP fails to consistently capture similarities between image patches and natural language prompts describing distance, we eliminate the use of its pre-trained natural language token embeddings and distill the semantic prior of its frozen text encoder into a single learnable embedding matrix called "mirror". The main design goal of mirror is to derive a non-human language prompt that approximates an optimal natural language prompt: "How far is this location from the camera?" Using this approach, we jointly train two lightweight modules, a mirror and a compact decoder, on top of a frozen CLIP for dense depth prediction. Compared to conventional depth models, our framework is significantly more efficient in terms of parameters and computation. The resulting model exhibits impressive performance, matching several state-of-the-art vision models on the NYU Depth v2 and KITTI benchmark datasets, while outperforming all vision-language depth models based on a frozen CLIP prior. Experiments demonstrate that the suboptimal depth understanding of CLIP in terms of spatial and temporal consistency can be significantly corrected without either fine-tuning it or concatenating mirror with its pre-trained subword token embeddings. Furthermore, an ablation study on the convergence status of mirror shows that it is implicitly trained to capture objects, such as humans and windows, where semantic cues play an important role in detection.

## 📝 요약

이 논문은 CLIP 모델을 웹에서 수집한 데이터로 학습한 후 미세 조정 없이도 하향식 작업에 적응시킬 수 있음을 보여줍니다. 특히, 단안 깊이 추정에서 CLIP의 비전-언어 정렬이 최적이 아니므로, 사전 학습된 자연어 토큰 임베딩을 제거하고, 동결된 텍스트 인코더의 의미적 사전 지식을 "mirror"라는 학습 가능한 임베딩 행렬로 증류합니다. 이 방법은 "이 위치는 카메라로부터 얼마나 떨어져 있는가?"라는 최적의 자연어 프롬프트를 근사화합니다. 이를 통해 동결된 CLIP 위에 mirror와 컴팩트한 디코더를 결합하여 밀집 깊이 예측을 수행합니다. 이 프레임워크는 기존 깊이 모델보다 파라미터와 계산 효율성이 높으며, NYU Depth v2와 KITTI 벤치마크 데이터셋에서 여러 최첨단 비전 모델과 성능을 맞추고, 모든 비전-언어 깊이 모델을 능가합니다. 실험 결과, CLIP의 공간적, 시간적 일관성 문제를 미세 조정 없이도 크게 개선할 수 있음을 보여줍니다. 추가로, mirror의 수렴 상태에 대한 연구는 의미적 단서가 중요한 객체를 캡처하는 데 효과적임을 나타냅니다.

## 🎯 주요 포인트

- 1. CLIP의 비전-언어 정렬이 최적화되지 않은 다운스트림 작업에서도 파인튜닝 없이 적응할 수 있음을 입증했습니다.
- 2. 단안 깊이 추정에서 CLIP의 대조적 사전이 일반화에 어려움을 겪어, 자연어 토큰 임베딩을 제거하고 "mirror"라는 학습 가능한 임베딩 행렬을 도입했습니다.
- 3. "mirror"와 컴팩트한 디코더를 동결된 CLIP 위에 공동 훈련하여, 파라미터와 계산 측면에서 효율적인 깊이 예측 모델을 개발했습니다.
- 4. NYU Depth v2와 KITTI 벤치마크 데이터셋에서 최첨단 비전 모델과 성능을 맞추고, 모든 비전-언어 깊이 모델을 능가했습니다.
- 5. 실험 결과, CLIP의 공간적 및 시간적 일관성 부족을 파인튜닝 없이도 크게 개선할 수 있음을 보여주었습니다.


---

*Generated on 2025-09-25 16:14:50*