---
keywords:
  - Vision-Language Model
  - Monocular Depth Estimation
  - Mirror Embedding Matrix
  - Dense Depth Prediction
  - NYU Depth Dataset
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2402.03251
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:14:50.431223",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Monocular Depth Estimation",
    "Mirror Embedding Matrix",
    "Dense Depth Prediction",
    "NYU Depth Dataset"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "Monocular Depth Estimation": 0.85,
    "Mirror Embedding Matrix": 0.8,
    "Dense Depth Prediction": 0.78,
    "NYU Depth Dataset": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "CLIP",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Contrastive Languageâ€“Image Pre-training"
        ],
        "category": "evolved_concepts",
        "rationale": "CLIP is central to the paper's approach and aligns with the trending concept of Vision-Language Models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "monocular depth estimation",
        "canonical": "Monocular Depth Estimation",
        "aliases": [
          "single-image depth estimation"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task addressed in the paper, relevant for linking to depth estimation techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "mirror",
        "canonical": "Mirror Embedding Matrix",
        "aliases": [
          "mirror matrix"
        ],
        "category": "unique_technical",
        "rationale": "Introduced as a novel component in the paper, it is crucial for understanding the proposed method.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.95,
        "link_intent_score": 0.8
      },
      {
        "surface": "dense depth prediction",
        "canonical": "Dense Depth Prediction",
        "aliases": [
          "dense depth estimation"
        ],
        "category": "unique_technical",
        "rationale": "A key output of the proposed method, relevant for linking to depth prediction models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "NYU Depth v2",
        "canonical": "NYU Depth Dataset",
        "aliases": [
          "NYU Depth v2 dataset"
        ],
        "category": "specific_connectable",
        "rationale": "A benchmark dataset used for evaluation, important for linking to related datasets and benchmarks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "depth understanding",
      "semantic prior",
      "conventional depth models"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "CLIP",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "monocular depth estimation",
      "resolved_canonical": "Monocular Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "mirror",
      "resolved_canonical": "Mirror Embedding Matrix",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.95,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "dense depth prediction",
      "resolved_canonical": "Dense Depth Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "NYU Depth v2",
      "resolved_canonical": "NYU Depth Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# CLIP Can Understand Depth

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2402.03251.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2402.03251](https://arxiv.org/abs/2402.03251)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/CLIP-IN_ Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions_20250923|CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions]] (87.7% similar)
- [[2025-09-23/MoCLIP-Lite_ Efficient Video Recognition by Fusing CLIP with Motion Vectors_20250923|MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors]] (85.3% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (84.8% similar)
- [[2025-09-24/Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment_20250924|Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment]] (84.5% similar)
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (84.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/NYU Depth Dataset|NYU Depth Dataset]]
**âš¡ Unique Technical**: [[keywords/Monocular Depth Estimation|Monocular Depth Estimation]], [[keywords/Mirror Embedding Matrix|Mirror Embedding Matrix]], [[keywords/Dense Depth Prediction|Dense Depth Prediction]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2402.03251v2 Announce Type: replace-cross 
Abstract: In this paper, we demonstrate that CLIP can also be adapted to downstream tasks where its vision-language alignment is suboptimally learned during pre-training on web-crawled data, all without requiring fine-tuning. We explore the case of monocular depth estimation, where CLIP's contrastive prior struggles to generalize, compared to its success in domains such as generative modeling and semantic segmentation. Since CLIP fails to consistently capture similarities between image patches and natural language prompts describing distance, we eliminate the use of its pre-trained natural language token embeddings and distill the semantic prior of its frozen text encoder into a single learnable embedding matrix called "mirror". The main design goal of mirror is to derive a non-human language prompt that approximates an optimal natural language prompt: "How far is this location from the camera?" Using this approach, we jointly train two lightweight modules, a mirror and a compact decoder, on top of a frozen CLIP for dense depth prediction. Compared to conventional depth models, our framework is significantly more efficient in terms of parameters and computation. The resulting model exhibits impressive performance, matching several state-of-the-art vision models on the NYU Depth v2 and KITTI benchmark datasets, while outperforming all vision-language depth models based on a frozen CLIP prior. Experiments demonstrate that the suboptimal depth understanding of CLIP in terms of spatial and temporal consistency can be significantly corrected without either fine-tuning it or concatenating mirror with its pre-trained subword token embeddings. Furthermore, an ablation study on the convergence status of mirror shows that it is implicitly trained to capture objects, such as humans and windows, where semantic cues play an important role in detection.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ CLIP ëª¨ë¸ì„ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ í•™ìŠµí•œ í›„ ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ í•˜í–¥ì‹ ì‘ì—…ì— ì ì‘ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì—ì„œ CLIPì˜ ë¹„ì „-ì–¸ì–´ ì •ë ¬ì´ ìµœì ì´ ì•„ë‹ˆë¯€ë¡œ, ì‚¬ì „ í•™ìŠµëœ ìì—°ì–´ í† í° ì„ë² ë”©ì„ ì œê±°í•˜ê³ , ë™ê²°ëœ í…ìŠ¤íŠ¸ ì¸ì½”ë”ì˜ ì˜ë¯¸ì  ì‚¬ì „ ì§€ì‹ì„ "mirror"ë¼ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© í–‰ë ¬ë¡œ ì¦ë¥˜í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ "ì´ ìœ„ì¹˜ëŠ” ì¹´ë©”ë¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ê°€?"ë¼ëŠ” ìµœì ì˜ ìì—°ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¼ì‚¬í™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë™ê²°ëœ CLIP ìœ„ì— mirrorì™€ ì»´íŒ©íŠ¸í•œ ë””ì½”ë”ë¥¼ ê²°í•©í•˜ì—¬ ë°€ì§‘ ê¹Šì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ê¸°ì¡´ ê¹Šì´ ëª¨ë¸ë³´ë‹¤ íŒŒë¼ë¯¸í„°ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ìœ¼ë©°, NYU Depth v2ì™€ KITTI ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ì—¬ëŸ¬ ìµœì²¨ë‹¨ ë¹„ì „ ëª¨ë¸ê³¼ ì„±ëŠ¥ì„ ë§ì¶”ê³ , ëª¨ë“  ë¹„ì „-ì–¸ì–´ ê¹Šì´ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CLIPì˜ ê³µê°„ì , ì‹œê°„ì  ì¼ê´€ì„± ë¬¸ì œë¥¼ ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¶”ê°€ë¡œ, mirrorì˜ ìˆ˜ë ´ ìƒíƒœì— ëŒ€í•œ ì—°êµ¬ëŠ” ì˜ë¯¸ì  ë‹¨ì„œê°€ ì¤‘ìš”í•œ ê°ì²´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CLIPì˜ ë¹„ì „-ì–¸ì–´ ì •ë ¬ì´ ìµœì í™”ë˜ì§€ ì•Šì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œë„ íŒŒì¸íŠœë‹ ì—†ì´ ì ì‘í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 2. ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì—ì„œ CLIPì˜ ëŒ€ì¡°ì  ì‚¬ì „ì´ ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªì–´, ìì—°ì–´ í† í° ì„ë² ë”©ì„ ì œê±°í•˜ê³  "mirror"ë¼ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© í–‰ë ¬ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 3. "mirror"ì™€ ì»´íŒ©íŠ¸í•œ ë””ì½”ë”ë¥¼ ë™ê²°ëœ CLIP ìœ„ì— ê³µë™ í›ˆë ¨í•˜ì—¬, íŒŒë¼ë¯¸í„°ì™€ ê³„ì‚° ì¸¡ë©´ì—ì„œ íš¨ìœ¨ì ì¸ ê¹Šì´ ì˜ˆì¸¡ ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- 4. NYU Depth v2ì™€ KITTI ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ë¹„ì „ ëª¨ë¸ê³¼ ì„±ëŠ¥ì„ ë§ì¶”ê³ , ëª¨ë“  ë¹„ì „-ì–¸ì–´ ê¹Šì´ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, CLIPì˜ ê³µê°„ì  ë° ì‹œê°„ì  ì¼ê´€ì„± ë¶€ì¡±ì„ íŒŒì¸íŠœë‹ ì—†ì´ë„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:14:50*