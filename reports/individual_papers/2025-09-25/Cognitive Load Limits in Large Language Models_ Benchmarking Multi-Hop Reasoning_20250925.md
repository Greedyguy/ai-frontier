---
keywords:
  - Large Language Model
  - Cognitive Load
  - Multi-Hop Reasoning
  - Context Saturation
  - Attentional Residue
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19517
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:14:42.162693",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cognitive Load",
    "Multi-Hop Reasoning",
    "Context Saturation",
    "Attentional Residue"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cognitive Load": 0.78,
    "Multi-Hop Reasoning": 0.8,
    "Context Saturation": 0.77,
    "Attentional Residue": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to existing knowledge on LLMs is crucial for understanding cognitive load impacts.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cognitive Load",
        "canonical": "Cognitive Load",
        "aliases": [
          "Cognitive Burden"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new theoretical framework for understanding model performance under stress.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multi-Hop Reasoning",
        "canonical": "Multi-Hop Reasoning",
        "aliases": [
          "Complex Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Key task in evaluating LLMs' reasoning capabilities under cognitive load.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Context Saturation",
        "canonical": "Context Saturation",
        "aliases": [
          "Information Overload"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific mechanism affecting LLM performance, useful for linking to cognitive theories.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Attentional Residue",
        "canonical": "Attentional Residue",
        "aliases": [
          "Task Switching Interference"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept affecting reasoning, relevant for cognitive load discussions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cognitive Load",
      "resolved_canonical": "Cognitive Load",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multi-Hop Reasoning",
      "resolved_canonical": "Multi-Hop Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Context Saturation",
      "resolved_canonical": "Context Saturation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Attentional Residue",
      "resolved_canonical": "Attentional Residue",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19517.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19517](https://arxiv.org/abs/2509.19517)

## 🔗 유사한 논문
- [[2025-09-24/CogniLoad_ A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density_20250924|CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density]] (89.2% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (87.6% similar)
- [[2025-09-23/How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark_20250923|How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark]] (87.0% similar)
- [[2025-09-23/Can Language Models Follow Multiple Turns of Entangled Instructions?_20250923|Can Language Models Follow Multiple Turns of Entangled Instructions?]] (86.9% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (86.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multi-Hop Reasoning|Multi-Hop Reasoning]]
**⚡ Unique Technical**: [[keywords/Cognitive Load|Cognitive Load]], [[keywords/Context Saturation|Context Saturation]], [[keywords/Attentional Residue|Attentional Residue]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19517v1 Announce Type: new 
Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 성능이 정적 벤치마크에서는 뛰어나지만, 동적이고 정보가 풍부한 환경에서는 취약함을 지적합니다. 연구는 '맥락 포화'와 '주의 잔여'가 성능 저하의 주요 원인임을 제안하며, 이를 평가하기 위한 'Interleaved Cognitive Evaluation (ICE)' 벤치마크를 개발했습니다. 200개의 질문에 대한 실험에서, 작은 오픈소스 모델들은 모든 조건에서 0%의 정확도를 보이며 취약성을 드러냈고, Gemini-2.0-Flash-001 모델은 부분적으로 85%의 정확도를 유지했으나, 맥락 포화 상태에서는 성능이 저하되었습니다. 연구는 인지 부하가 추론 실패의 주요 원인임을 시사하며, AI 시스템의 진정한 회복력과 안전성을 평가하기 위해 동적이고 인지적인 스트레스 테스트가 필요함을 강조합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)의 성능은 정적 벤치마크에서는 뛰어나지만, 동적이고 정보가 풍부한 환경에서는 취약성을 드러낸다.
- 2. 본 연구는 인지 부하의 공식 이론을 도입하여, 과도한 비과제 정보(맥락 포화)와 과제 전환 시 간섭(주의 잔여)이 성능 저하의 주요 메커니즘임을 제안한다.
- 3. Interleaved Cognitive Evaluation (ICE) 벤치마크를 설계하여, 복잡한 다중 단계 추론 과제에서 인지 부하 요인을 체계적으로 조작하였다.
- 4. 소규모 오픈소스 모델들은 높은 내재적 부하 과제에서 0% 정확도를 기록하며 취약성을 보였으나, Gemini-2.0-Flash-001 모델은 부분적으로 회복력을 나타냈다.
- 5. 연구 결과는 인지 부하가 추론 실패의 주요 원인임을 시사하며, 고급 AI 시스템의 진정한 회복력과 안전성을 평가하기 위해 인지 인식 스트레스 테스트가 필수적임을 강조한다.


---

*Generated on 2025-09-25 15:14:42*