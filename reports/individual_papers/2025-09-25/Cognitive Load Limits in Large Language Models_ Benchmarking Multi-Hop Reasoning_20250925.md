---
keywords:
  - Large Language Model
  - Cognitive Load
  - Multi-Hop Reasoning
  - Context Saturation
  - Attentional Residue
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19517
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:14:42.162693",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cognitive Load",
    "Multi-Hop Reasoning",
    "Context Saturation",
    "Attentional Residue"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cognitive Load": 0.78,
    "Multi-Hop Reasoning": 0.8,
    "Context Saturation": 0.77,
    "Attentional Residue": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to existing knowledge on LLMs is crucial for understanding cognitive load impacts.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cognitive Load",
        "canonical": "Cognitive Load",
        "aliases": [
          "Cognitive Burden"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new theoretical framework for understanding model performance under stress.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multi-Hop Reasoning",
        "canonical": "Multi-Hop Reasoning",
        "aliases": [
          "Complex Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Key task in evaluating LLMs' reasoning capabilities under cognitive load.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Context Saturation",
        "canonical": "Context Saturation",
        "aliases": [
          "Information Overload"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific mechanism affecting LLM performance, useful for linking to cognitive theories.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Attentional Residue",
        "canonical": "Attentional Residue",
        "aliases": [
          "Task Switching Interference"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept affecting reasoning, relevant for cognitive load discussions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cognitive Load",
      "resolved_canonical": "Cognitive Load",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multi-Hop Reasoning",
      "resolved_canonical": "Multi-Hop Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Context Saturation",
      "resolved_canonical": "Context Saturation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Attentional Residue",
      "resolved_canonical": "Attentional Residue",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19517.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19517](https://arxiv.org/abs/2509.19517)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/CogniLoad_ A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density_20250924|CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density]] (89.2% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (87.6% similar)
- [[2025-09-23/How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark_20250923|How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark]] (87.0% similar)
- [[2025-09-23/Can Language Models Follow Multiple Turns of Entangled Instructions?_20250923|Can Language Models Follow Multiple Turns of Entangled Instructions?]] (86.9% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (86.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multi-Hop Reasoning|Multi-Hop Reasoning]]
**âš¡ Unique Technical**: [[keywords/Cognitive Load|Cognitive Load]], [[keywords/Context Saturation|Context Saturation]], [[keywords/Attentional Residue|Attentional Residue]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19517v1 Announce Type: new 
Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì´ ì •ì  ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ë›°ì–´ë‚˜ì§€ë§Œ, ë™ì ì´ê³  ì •ë³´ê°€ í’ë¶€í•œ í™˜ê²½ì—ì„œëŠ” ì·¨ì•½í•¨ì„ ì§€ì í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” 'ë§¥ë½ í¬í™”'ì™€ 'ì£¼ì˜ ì”ì—¬'ê°€ ì„±ëŠ¥ ì €í•˜ì˜ ì£¼ìš” ì›ì¸ì„ì„ ì œì•ˆí•˜ë©°, ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ 'Interleaved Cognitive Evaluation (ICE)' ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. 200ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì‹¤í—˜ì—ì„œ, ì‘ì€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤ì€ ëª¨ë“  ì¡°ê±´ì—ì„œ 0%ì˜ ì •í™•ë„ë¥¼ ë³´ì´ë©° ì·¨ì•½ì„±ì„ ë“œëŸ¬ëƒˆê³ , Gemini-2.0-Flash-001 ëª¨ë¸ì€ ë¶€ë¶„ì ìœ¼ë¡œ 85%ì˜ ì •í™•ë„ë¥¼ ìœ ì§€í–ˆìœ¼ë‚˜, ë§¥ë½ í¬í™” ìƒíƒœì—ì„œëŠ” ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” ì¸ì§€ ë¶€í•˜ê°€ ì¶”ë¡  ì‹¤íŒ¨ì˜ ì£¼ìš” ì›ì¸ì„ì„ ì‹œì‚¬í•˜ë©°, AI ì‹œìŠ¤í…œì˜ ì§„ì •í•œ íšŒë³µë ¥ê³¼ ì•ˆì „ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë™ì ì´ê³  ì¸ì§€ì ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì€ ì •ì  ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ë›°ì–´ë‚˜ì§€ë§Œ, ë™ì ì´ê³  ì •ë³´ê°€ í’ë¶€í•œ í™˜ê²½ì—ì„œëŠ” ì·¨ì•½ì„±ì„ ë“œëŸ¬ë‚¸ë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ì¸ì§€ ë¶€í•˜ì˜ ê³µì‹ ì´ë¡ ì„ ë„ì…í•˜ì—¬, ê³¼ë„í•œ ë¹„ê³¼ì œ ì •ë³´(ë§¥ë½ í¬í™”)ì™€ ê³¼ì œ ì „í™˜ ì‹œ ê°„ì„­(ì£¼ì˜ ì”ì—¬)ì´ ì„±ëŠ¥ ì €í•˜ì˜ ì£¼ìš” ë©”ì»¤ë‹ˆì¦˜ì„ì„ ì œì•ˆí•œë‹¤.
- 3. Interleaved Cognitive Evaluation (ICE) ë²¤ì¹˜ë§ˆí¬ë¥¼ ì„¤ê³„í•˜ì—¬, ë³µì¡í•œ ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡  ê³¼ì œì—ì„œ ì¸ì§€ ë¶€í•˜ ìš”ì¸ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‘í•˜ì˜€ë‹¤.
- 4. ì†Œê·œëª¨ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤ì€ ë†’ì€ ë‚´ì¬ì  ë¶€í•˜ ê³¼ì œì—ì„œ 0% ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©° ì·¨ì•½ì„±ì„ ë³´ì˜€ìœ¼ë‚˜, Gemini-2.0-Flash-001 ëª¨ë¸ì€ ë¶€ë¶„ì ìœ¼ë¡œ íšŒë³µë ¥ì„ ë‚˜íƒ€ëƒˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì¸ì§€ ë¶€í•˜ê°€ ì¶”ë¡  ì‹¤íŒ¨ì˜ ì£¼ìš” ì›ì¸ì„ì„ ì‹œì‚¬í•˜ë©°, ê³ ê¸‰ AI ì‹œìŠ¤í…œì˜ ì§„ì •í•œ íšŒë³µë ¥ê³¼ ì•ˆì „ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì¸ì§€ ì¸ì‹ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ê°€ í•„ìˆ˜ì ì„ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-25 15:14:42*