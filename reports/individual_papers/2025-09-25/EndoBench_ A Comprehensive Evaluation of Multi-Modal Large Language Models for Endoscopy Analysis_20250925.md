---
keywords:
  - Large Language Model
  - Endoscopic Procedures
  - Vision-Language Model
  - Visual Question Answering
  - Medical-Specialized Models
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2505.23601
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:25:37.337133",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Endoscopic Procedures",
    "Vision-Language Model",
    "Visual Question Answering",
    "Medical-Specialized Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.82,
    "Endoscopic Procedures": 0.78,
    "Vision-Language Model": 0.8,
    "Visual Question Answering": 0.79,
    "Medical-Specialized Models": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-Modal Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "MLLM",
          "Multi-Modal LLM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader concept of language models used in various technical domains.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Endoscopic Procedures",
        "canonical": "Endoscopic Procedures",
        "aliases": [
          "Endoscopy"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on medical applications and specific to the domain of endoscopy.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents a key area of research in combining visual and textual data processing.",
        "novelty_score": 0.68,
        "connectivity_score": 0.83,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual Question Answering",
        "canonical": "Visual Question Answering",
        "aliases": [
          "VQA"
        ],
        "category": "specific_connectable",
        "rationale": "A specific task that bridges computer vision and natural language processing.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "Medical-Specialized Models",
        "canonical": "Medical-Specialized Models",
        "aliases": [
          "Medical ML Models"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the specialization of models for medical applications, crucial for domain-specific insights.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-Modal Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Endoscopic Procedures",
      "resolved_canonical": "Endoscopic Procedures",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.83,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual Question Answering",
      "resolved_canonical": "Visual Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Medical-Specialized Models",
      "resolved_canonical": "Medical-Specialized Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.23601.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2505.23601](https://arxiv.org/abs/2505.23601)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/EyePCR_ A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery_20250922|EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery]] (87.1% similar)
- [[2025-09-25/EchoBench_ Benchmarking Sycophancy in Medical Large Vision-Language Models_20250925|EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models]] (86.2% similar)
- [[2025-09-24/Zero-shot Monocular Metric Depth for Endoscopic Images_20250924|Zero-shot Monocular Metric Depth for Endoscopic Images]] (84.8% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (84.6% similar)
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (84.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Visual Question Answering|Visual Question Answering]]
**âš¡ Unique Technical**: [[keywords/Endoscopic Procedures|Endoscopic Procedures]], [[keywords/Medical-Specialized Models|Medical-Specialized Models]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.23601v2 Announce Type: replace 
Abstract: Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis. However, current benchmarks are limited, as they typically cover specific endoscopic scenarios and a small set of clinical tasks, failing to capture the real-world diversity of endoscopic scenarios and the full range of skills needed in clinical workflows. To address these issues, we introduce EndoBench, the first comprehensive benchmark specifically designed to assess MLLMs across the full spectrum of endoscopic practice with multi-dimensional capacities. EndoBench encompasses 4 distinct endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks, and 5 levels of visual prompting granularities, resulting in 6,832 rigorously validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation framework mirrors the clinical workflow--spanning anatomical recognition, lesion analysis, spatial localization, and surgical operations--to holistically gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios. We benchmark 23 state-of-the-art models, including general-purpose, medical-specialized, and proprietary MLLMs, and establish human clinician performance as a reference standard. Our extensive experiments reveal: (1) proprietary MLLMs outperform open-source and medical-specialized models overall, but still trail human experts; (2) medical-domain supervised fine-tuning substantially boosts task-specific accuracy; and (3) model performance remains sensitive to prompt format and clinical task complexity. EndoBench establishes a new standard for evaluating and advancing MLLMs in endoscopy, highlighting both progress and persistent gaps between current models and expert clinical reasoning. We publicly release our benchmark and code.

## ğŸ“ ìš”ì•½

EndoBenchëŠ” ë‚´ì‹œê²½ ë¶„ì„ì„ ë•ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ í‰ê°€ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¡œ, ë‚´ì‹œê²½ ì‹¤ë¬´ ì „ë°˜ì„ ì•„ìš°ë¥´ëŠ” ì²« ë²ˆì§¸ í‰ê°€ ë„êµ¬ì…ë‹ˆë‹¤. 4ê°€ì§€ ë‚´ì‹œê²½ ì‹œë‚˜ë¦¬ì˜¤ì™€ 12ê°œì˜ ì„ìƒ ê³¼ì œ, 5ë‹¨ê³„ì˜ ì‹œê°ì  í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ 6,832ê°œì˜ VQA ìŒì„ ì œê³µí•©ë‹ˆë‹¤. 23ê°œì˜ ìµœì‹  ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë…ì  MLLMsê°€ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•˜ì§€ë§Œ ì—¬ì „íˆ ì¸ê°„ ì „ë¬¸ê°€ì— ë¯¸ì¹˜ì§€ ëª»í•˜ë©°, ì˜ë£Œ ë¶„ì•¼ì˜ ê°ë… í•™ìŠµì´ ì •í™•ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. EndoBenchëŠ” ë‚´ì‹œê²½ ë¶„ì•¼ì˜ MLLMs ë°œì „ì„ ìœ„í•œ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. EndoBenchëŠ” ë‹¤ì°¨ì›ì  ì—­ëŸ‰ì„ í†µí•´ ë‚´ì‹œê²½ ì‹¤ìŠµì˜ ì „ì²´ ìŠ¤í™íŠ¸ëŸ¼ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ìµœì´ˆì˜ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- 2. EndoBenchëŠ” 4ê°œì˜ ë‚´ì‹œê²½ ì‹œë‚˜ë¦¬ì˜¤, 12ê°œì˜ ì „ë¬¸ ì„ìƒ ì‘ì—…ê³¼ 12ê°œì˜ ë¶€ì°¨ì  ì‘ì—…, 5ë‹¨ê³„ì˜ ì‹œê°ì  í”„ë¡¬í”„íŠ¸ ì„¸ë¶„í™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, ë…ì  MLLMì´ ì˜¤í”ˆì†ŒìŠ¤ ë° ì˜ë£Œ ì „ë¬¸ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ ì—¬ì „íˆ ì¸ê°„ ì „ë¬¸ê°€ì— ë¹„í•´ ë’¤ì²˜ì§€ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
- 4. ì˜ë£Œ ë„ë©”ì¸ì—ì„œì˜ ê°ë…ëœ ë¯¸ì„¸ ì¡°ì •ì€ ì‘ì—…ë³„ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. ëª¨ë¸ ì„±ëŠ¥ì€ í”„ë¡¬í”„íŠ¸ í˜•ì‹ê³¼ ì„ìƒ ì‘ì—…ì˜ ë³µì¡ì„±ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:25:37*