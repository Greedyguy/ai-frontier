---
keywords:
  - Large Language Model
  - Knowledge Graph
  - Cognitive Complexity Framework
  - Benchmark Evaluation Task
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.19347
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:42:17.221722",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Knowledge Graph",
    "Cognitive Complexity Framework",
    "Benchmark Evaluation Task"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "Knowledge Graph": 0.85,
    "Cognitive Complexity Framework": 0.7,
    "Benchmark Evaluation Task": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion and connect well with existing broad technical concepts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Knowledge Graphs",
        "canonical": "Knowledge Graph",
        "aliases": [
          "KGs"
        ],
        "category": "specific_connectable",
        "rationale": "Knowledge Graphs are a specific focus of the paper, providing strong connectivity to related research areas.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cognitive Complexity Frameworks",
        "canonical": "Cognitive Complexity Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is unique to the paper's approach, offering novel insights into task characterization.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Benchmark Evaluation Tasks",
        "canonical": "Benchmark Evaluation Task",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term is specific to the paper's proposed methodology, highlighting its unique contribution.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "accuracy",
      "output correctness",
      "value distributions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Knowledge Graphs",
      "resolved_canonical": "Knowledge Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cognitive Complexity Frameworks",
      "resolved_canonical": "Cognitive Complexity Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Benchmark Evaluation Tasks",
      "resolved_canonical": "Benchmark Evaluation Task",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19347.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.19347](https://arxiv.org/abs/2509.19347)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/AECBench_ A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field_20250924|AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field]] (87.0% similar)
- [[2025-09-23/Large Language Models Meet Knowledge Graphs for Question Answering_ Synthesis and Opportunities_20250923|Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities]] (86.2% similar)
- [[2025-09-23/LLMsPark_ A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts_20250923|LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts]] (85.8% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (85.6% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Graph|Knowledge Graph]]
**âš¡ Unique Technical**: [[keywords/Cognitive Complexity Framework|Cognitive Complexity Framework]], [[keywords/Benchmark Evaluation Task|Benchmark Evaluation Task]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19347v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used for tasks involving Knowledge Graphs (KGs), whose evaluation typically focuses on accuracy and output correctness. We propose a complementary task characterization approach using three complexity frameworks from cognitive psychology. Applying this to the LLM-KG-Bench framework, we highlight value distributions, identify underrepresented demands and motivate richer interpretation and diversity for benchmark evaluation tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ì§€ì‹ ê·¸ë˜í”„(KGs)ì™€ ê´€ë ¨ëœ ì‘ì—…ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°©ì‹ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ í‰ê°€ê°€ ì£¼ë¡œ ì •í™•ì„±ê³¼ ì¶œë ¥ì˜ ì˜¬ë°”ë¦„ì— ì¤‘ì ì„ ë‘ëŠ” ë°˜ë©´, ì €ìë“¤ì€ ì¸ì§€ ì‹¬ë¦¬í•™ì˜ ì„¸ ê°€ì§€ ë³µì¡ì„± í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ë³´ì™„ì ì¸ ì‘ì—… íŠ¹ì„±í™” ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ LLM-KG-Bench í”„ë ˆì„ì›Œí¬ì— ì ìš©í•˜ì—¬ ê°€ì¹˜ ë¶„í¬ë¥¼ ê°•ì¡°í•˜ê³ , ê³¼ì†Œ ëŒ€í‘œëœ ìš”êµ¬ë¥¼ ì‹ë³„í•˜ë©°, ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‘ì—…ì˜ í•´ì„ê³¼ ë‹¤ì–‘ì„±ì„ í’ë¶€í•˜ê²Œ í•  í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì§€ì‹ ê·¸ë˜í”„(KGs)ì™€ ê´€ë ¨ëœ ì‘ì—…ì—ì„œ ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ í‰ê°€ ë°©ì‹ì€ ì£¼ë¡œ ì •í™•ì„±ê³¼ ì¶œë ¥ì˜ ì˜¬ë°”ë¦„ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤.
- 3. ìš°ë¦¬ëŠ” ì¸ì§€ ì‹¬ë¦¬í•™ì˜ ì„¸ ê°€ì§€ ë³µì¡ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì™„ì ì¸ ì‘ì—… íŠ¹ì„±í™” ì ‘ê·¼ë²•ì„ ì œì•ˆí•œë‹¤.
- 4. LLM-KG-Bench í”„ë ˆì„ì›Œí¬ì— ì´ë¥¼ ì ìš©í•˜ì—¬ ê°€ì¹˜ ë¶„í¬ë¥¼ ê°•ì¡°í•˜ê³ , ê³¼ì†Œ ëŒ€í‘œëœ ìš”êµ¬ë¥¼ ì‹ë³„í•˜ë©°, ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‘ì—…ì— ëŒ€í•œ í’ë¶€í•œ í•´ì„ê³¼ ë‹¤ì–‘ì„±ì„ ì´‰ì§„í•œë‹¤.


---

*Generated on 2025-09-26 08:42:17*