---
keywords:
  - Multi-Task Learning
  - Parameter-Efficient Fine-Tuning
  - Task Interference
  - Adapter Modules
  - Transformer
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19602
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:00:34.389842",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multi-Task Learning",
    "Parameter-Efficient Fine-Tuning",
    "Task Interference",
    "Adapter Modules",
    "Transformer"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multi-Task Learning": 0.78,
    "Parameter-Efficient Fine-Tuning": 0.81,
    "Task Interference": 0.79,
    "Adapter Modules": 0.77,
    "Transformer": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "multi-task learning",
        "canonical": "Multi-Task Learning",
        "aliases": [
          "MTL"
        ],
        "category": "broad_technical",
        "rationale": "Multi-task learning is a central theme of the paper, connecting various tasks and methods discussed.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "parameter-efficient fine-tuning",
        "canonical": "Parameter-Efficient Fine-Tuning",
        "aliases": [
          "efficient fine-tuning"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.81
      },
      {
        "surface": "task interference",
        "canonical": "Task Interference",
        "aliases": [
          "negative transfer"
        ],
        "category": "specific_connectable",
        "rationale": "Task interference is a key challenge addressed by the paper, linking to broader discussions on task optimization.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "adapter modules",
        "canonical": "Adapter Modules",
        "aliases": [
          "adapters"
        ],
        "category": "unique_technical",
        "rationale": "Adapter modules are a specific innovation in the paper, essential for understanding the proposed adaptation strategy.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Swin Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Swin"
        ],
        "category": "broad_technical",
        "rationale": "The Swin Transformer is a specific model used in the experiments, linking to the broader category of Transformers.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "multi-task learning",
      "resolved_canonical": "Multi-Task Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "parameter-efficient fine-tuning",
      "resolved_canonical": "Parameter-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "task interference",
      "resolved_canonical": "Task Interference",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "adapter modules",
      "resolved_canonical": "Adapter Modules",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Swin Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19602.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19602](https://arxiv.org/abs/2509.19602)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/HyperAdapt_ Simple High-Rank Adaptation_20250924|HyperAdapt: Simple High-Rank Adaptation]] (85.9% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (85.3% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (84.8% similar)
- [[2025-09-23/A non-smooth regularization framework for learning over multitask graphs_20250923|A non-smooth regularization framework for learning over multitask graphs]] (84.7% similar)
- [[2025-09-24/Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning_20250924|Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Multi-Task Learning|Multi-Task Learning]], [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Task Interference|Task Interference]]
**âš¡ Unique Technical**: [[keywords/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]], [[keywords/Adapter Modules|Adapter Modules]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19602v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution for adapting pre-trained models to various downstream tasks. While these methods perform well in single-task learning, extending them to multi-task learning exacerbates common challenges, such as task interference and negative transfer, due to the limited number of trainable parameters. To address these issues, we introduce progressive task-specific multi-task adaptation, a novel parameter-efficient approach for multi-task learning. This approach introduces adapter modules in a pre-trained model such that these modules are shared across all tasks in the initial layers and become progressively more task-specific in the later layers. The motivation is to reduce the conflicts among tasks by allowing transfer learning across all tasks in the initial layers and enabling task-specific learning toward the prediction heads. Additionally, we propose a gradient-based approach for computing task similarity and use this measure to allocate similar tasks to the shared adapter modules. Our task similarity method introduces minimal overhead in the pipeline. We evaluate our approach by adapting the Swin Transformer for dense prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate that our approach outperforms a fully fine-tuned multi-task model while requiring only one-fifth of the trainable parameters. This approach achieves better relative improvement to single-task fine-tuning while reducing the number of trainable parameters and surpasses the current state-of-the-art methods for parameter-efficient multi-task learning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµì—ì„œì˜ ê³¼ì œ ê°„ ê°„ì„­ê³¼ ë¶€ì •ì  ì „ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì ì§„ì  ê³¼ì œ íŠ¹í™” ë©€í‹°íƒœìŠ¤í¬ ì ì‘ ë°©ì‹ì„ ë„ì…í•©ë‹ˆë‹¤. ì´ ë°©ì‹ì€ ì´ˆê¸° ë ˆì´ì–´ì—ì„œëŠ” ëª¨ë“  ì‘ì—…ì— ê³µìœ ë˜ëŠ” ì–´ëŒ‘í„° ëª¨ë“ˆì„ ì‚¬ìš©í•˜ê³ , í›„ë°˜ë¶€ ë ˆì´ì–´ì—ì„œëŠ” ì ì°¨ ê³¼ì œ íŠ¹í™”ëœ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ê³¼ì œ ê°„ ì¶©ëŒì„ ì¤„ì…ë‹ˆë‹¤. ë˜í•œ, ê³¼ì œ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ì—¬ ìœ ì‚¬í•œ ê³¼ì œë¥¼ ê³µìœ  ëª¨ë“ˆì— í• ë‹¹í•©ë‹ˆë‹¤. Swin Transformerë¥¼ í™œìš©í•œ ì‹¤í—˜ ê²°ê³¼, PASCAL ë° NYUD-v2 ë°ì´í„°ì…‹ì—ì„œ ì´ ë°©ë²•ì´ ê¸°ì¡´ ë©€í‹°íƒœìŠ¤í¬ ëª¨ë¸ë³´ë‹¤ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ìµœì‹  ê¸°ë²•ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘ì‹œí‚¤ëŠ” ìœ ë§í•œ ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì—ì„œì˜ ê³¼ì œ ê°„ ê°„ì„­ê³¼ ë¶€ì •ì  ì „ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì ì§„ì  ì‘ì—… íŠ¹í™” ë‹¤ì¤‘ ì‘ì—… ì ì‘ì´ë¼ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì´ˆê¸° ë ˆì´ì–´ì—ì„œëŠ” ëª¨ë“  ì‘ì—…ì— ê±¸ì³ ê³µìœ ë˜ê³ , í›„ë°˜ë¶€ ë ˆì´ì–´ì—ì„œëŠ” ì ì  ë” ì‘ì—… íŠ¹í™”ë˜ëŠ” ì–´ëŒ‘í„° ëª¨ë“ˆì„ ë„ì…í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ Swin Transformerë¥¼ ë°€ë„ ì˜ˆì¸¡ ì‘ì—…ì— ì ì‘ì‹œì¼œ PASCAL ë° NYUD-v2 ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ì „ì²´ ë¯¸ì„¸ ì¡°ì •ëœ ë‹¤ì¤‘ ì‘ì—… ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ë‹¨ì¼ ì‘ì—… ë¯¸ì„¸ ì¡°ì •ì— ë¹„í•´ ë” ë‚˜ì€ ìƒëŒ€ì  ê°œì„ ì„ ë‹¬ì„±í•˜ë©°, í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ë©´ì„œë„ í˜„ì¬ ìµœì²¨ë‹¨ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:00:34*