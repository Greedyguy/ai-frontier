---
keywords:
  - Large Language Model
  - Information Gain
  - Bayesian Method
  - Entropy-Based Method
  - ConceptNet
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19593
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:39:40.355498",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Information Gain",
    "Bayesian Method",
    "Entropy-Based Method",
    "ConceptNet"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Information Gain": 0.78,
    "Bayesian Method": 0.72,
    "Entropy-Based Method": 0.7,
    "ConceptNet": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating question-asking capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Information Gain",
        "canonical": "Information Gain",
        "aliases": [
          "IG"
        ],
        "category": "unique_technical",
        "rationale": "Key metric used for evaluating question quality in the study.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Bayesian Method",
        "canonical": "Bayesian Method",
        "aliases": [
          "Bayesian Approach"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific technique for tracking belief updates in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Entropy-Based Method",
        "canonical": "Entropy-Based Method",
        "aliases": [
          "Entropy Method"
        ],
        "category": "unique_technical",
        "rationale": "Represents a distinct approach for filtering candidates in the research.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.7
      },
      {
        "surface": "ConceptNet",
        "canonical": "ConceptNet",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Used as a resource for filtering candidates, relevant for linking to semantic networks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "protocol",
      "Oracle",
      "Guesser",
      "game length",
      "efficiency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Information Gain",
      "resolved_canonical": "Information Gain",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Bayesian Method",
      "resolved_canonical": "Bayesian Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Entropy-Based Method",
      "resolved_canonical": "Entropy-Based Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "ConceptNet",
      "resolved_canonical": "ConceptNet",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19593.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19593](https://arxiv.org/abs/2509.19593)

## 🔗 유사한 논문
- [[2025-09-23/LLMsPark_ A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts_20250923|LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts]] (87.1% similar)
- [[2025-09-24/Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests_20250924|Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests]] (85.1% similar)
- [[2025-09-23/Probing LLM World Models_ Enhancing Guesstimation with Wisdom of Crowds Decoding_20250923|Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding]] (84.9% similar)
- [[2025-09-25/Cognitive Load Limits in Large Language Models_ Benchmarking Multi-Hop Reasoning_20250925|Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning]] (84.6% similar)
- [[2025-09-22/IGD_ Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation_20250922|IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation]] (84.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/ConceptNet|ConceptNet]]
**⚡ Unique Technical**: [[keywords/Information Gain|Information Gain]], [[keywords/Bayesian Method|Bayesian Method]], [[keywords/Entropy-Based Method|Entropy-Based Method]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19593v1 Announce Type: cross 
Abstract: We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 전략적 질문 능력을 평가하기 위한 GuessingGame 프로토콜을 소개합니다. Guesser LLM은 Oracle에게 자유 형식의 질문을 통해 숨겨진 객체를 식별합니다. 질문의 질을 측정하기 위해 두 가지 정보 이득(IG) 지표를 제안합니다: LLM이 평가한 관련성을 기반으로 한 베이지안 방법과 ConceptNet을 활용한 엔트로피 기반 방법입니다. 두 지표 모두 모델에 구애받지 않으며 사후 분석을 지원합니다. 858개의 게임 실험에서 IG가 높을수록 효율성이 증가했으며, IG가 한 표준편차 증가할 때 예상 게임 길이가 43% 감소했습니다. IG에 기반한 질문 다양성 등의 제약은 약한 모델의 성능을 크게 향상시켰습니다. 이 결과는 LLM의 질문 능력이 측정 가능하고 개선 가능하며, 상호작용적 추론에 중요함을 보여줍니다.

## 🎯 주요 포인트

- 1. GuessingGame은 대형 언어 모델(LLM)의 전략적 질문 능력을 평가하기 위한 프로토콜로, 자유형 질문을 통해 숨겨진 객체를 식별합니다.
- 2. 질문의 질을 측정하기 위해 두 가지 정보 이득(IG) 지표를 제안하며, 이는 베이지안 방법과 엔트로피 기반 방법을 포함합니다.
- 3. 정보 이득(IG)이 높을수록 게임의 효율성이 증가하며, 표준 편차가 1 증가할 때 예상 게임 길이가 43% 감소합니다.
- 4. IG에 의해 안내되는 프롬프트 제약, 예를 들어 질문 다양성 강제는 약한 모델의 성능을 크게 향상시킵니다.
- 5. LLM의 질문 능력은 측정 가능하고 개선 가능하며, 상호작용적 추론에 있어 중요합니다.


---

*Generated on 2025-09-25 15:39:40*