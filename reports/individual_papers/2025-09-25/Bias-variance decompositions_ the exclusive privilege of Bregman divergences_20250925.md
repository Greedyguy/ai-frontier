---
keywords:
  - Bregman Divergence
  - Bias-Variance Decomposition
  - Cross-Entropy Loss
  - Mahalanobis Distance
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2501.18581
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:05:57.532251",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Bregman Divergence",
    "Bias-Variance Decomposition",
    "Cross-Entropy Loss",
    "Mahalanobis Distance"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Bregman Divergence": 0.78,
    "Bias-Variance Decomposition": 0.8,
    "Cross-Entropy Loss": 0.82,
    "Mahalanobis Distance": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Bregman divergences",
        "canonical": "Bregman Divergence",
        "aliases": [
          "Bregman divergence",
          "Bregman"
        ],
        "category": "unique_technical",
        "rationale": "Bregman divergences are central to the paper's discussion on bias-variance decomposition, offering a unique perspective in machine learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "bias-variance decomposition",
        "canonical": "Bias-Variance Decomposition",
        "aliases": [
          "bias variance decomposition"
        ],
        "category": "broad_technical",
        "rationale": "This concept is fundamental to understanding model generalization and is directly addressed in the paper.",
        "novelty_score": 0.55,
        "connectivity_score": 0.72,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "cross-entropy loss",
        "canonical": "Cross-Entropy Loss",
        "aliases": [
          "cross entropy",
          "cross-entropy"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-entropy loss is a special case of Bregman divergences, making it relevant for linking with other loss function discussions.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "squared Mahalanobis distance",
        "canonical": "Mahalanobis Distance",
        "aliases": [
          "squared Mahalanobis",
          "Mahalanobis"
        ],
        "category": "specific_connectable",
        "rationale": "The squared Mahalanobis distance is highlighted as the only symmetric loss function with a clean bias-variance decomposition.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "loss function",
      "generalization performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Bregman divergences",
      "resolved_canonical": "Bregman Divergence",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "bias-variance decomposition",
      "resolved_canonical": "Bias-Variance Decomposition",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.72,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "cross-entropy loss",
      "resolved_canonical": "Cross-Entropy Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "squared Mahalanobis distance",
      "resolved_canonical": "Mahalanobis Distance",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Bias-variance decompositions: the exclusive privilege of Bregman divergences

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2501.18581.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2501.18581](https://arxiv.org/abs/2501.18581)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (81.4% similar)
- [[2025-09-23/Loss-Complexity Landscape and Model Structure Functions_20250923|Loss-Complexity Landscape and Model Structure Functions]] (80.2% similar)
- [[2025-09-23/Convergence analysis of equilibrium methods for inverse problems_20250923|Convergence analysis of equilibrium methods for inverse problems]] (80.2% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (80.1% similar)
- [[2025-09-24/Global Minimizers of Sigmoid Contrastive Loss_20250924|Global Minimizers of Sigmoid Contrastive Loss]] (80.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Bias-Variance Decomposition|Bias-Variance Decomposition]]
**ğŸ”— Specific Connectable**: [[keywords/Cross-Entropy Loss|Cross-Entropy Loss]], [[keywords/Mahalanobis Distance|Mahalanobis Distance]]
**âš¡ Unique Technical**: [[keywords/Bregman Divergence|Bregman Divergence]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.18581v2 Announce Type: replace 
Abstract: Bias-variance decompositions are widely used to understand the generalization performance of machine learning models. While the squared error loss permits a straightforward decomposition, other loss functions - such as zero-one loss or $L_1$ loss - either fail to sum bias and variance to the expected loss or rely on definitions that lack the essential properties of meaningful bias and variance. Recent research has shown that clean decompositions can be achieved for the broader class of Bregman divergences, with the cross-entropy loss as a special case. However, the necessary and sufficient conditions for these decompositions remain an open question.
  In this paper, we address this question by studying continuous, nonnegative loss functions that satisfy the identity of indiscernibles (zero loss if and only if the two arguments are identical), under mild regularity conditions. We prove that so-called $g$-Bregman divergences are the only such loss functions that have a clean bias-variance decomposition. A $g$-Bregman divergence can be transformed into a standard Bregman divergence through an invertible change of variables. This makes the squared Mahalanobis distance, up to such a variable transformation, the only symmetric loss function with a clean bias-variance decomposition. Consequently, common metrics such as $0$-$1$ and $L_1$ losses cannot admit a clean bias-variance decomposition, explaining why previous attempts have failed. We also examine the impact of relaxing the restrictions on the loss functions and how this affects our results.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì´í•´í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” í¸í–¥-ë¶„ì‚° ë¶„í•´ì— ê´€í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì œê³± ì˜¤ì°¨ ì†ì‹¤ì€ ê°„ë‹¨íˆ ë¶„í•´ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ë“¤ì€ ì˜ë¯¸ ìˆëŠ” í¸í–¥ê³¼ ë¶„ì‚°ì˜ ì†ì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ê±°ë‚˜ ê¸°ëŒ€ ì†ì‹¤ë¡œ í•©ì‚°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” Bregman ë°œì‚°ì˜ ë” ë„“ì€ ë²”ì£¼ì—ì„œ ëª…í™•í•œ ë¶„í•´ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì„ ë°íˆê³ , êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì´ ê·¸ íŠ¹ë³„í•œ ê²½ìš°ì„ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì—°ì†ì ì´ê³  ë¹„ìŒìˆ˜ì¸ ì†ì‹¤ í•¨ìˆ˜ê°€ indiscerniblesì˜ ì •ì²´ì„±ì„ ë§Œì¡±í•  ë•Œ, $g$-Bregman ë°œì‚°ë§Œì´ ëª…í™•í•œ í¸í–¥-ë¶„ì‚° ë¶„í•´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ $0$-$1$ ë° $L_1$ ì†ì‹¤ê³¼ ê°™ì€ ì¼ë°˜ì ì¸ ì§€í‘œëŠ” ëª…í™•í•œ í¸í–¥-ë¶„ì‚° ë¶„í•´ë¥¼ ê°€ì§ˆ ìˆ˜ ì—†ìŒì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í¸í–¥-ë¶„ì‚° ë¶„í•´ëŠ” ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì´í•´í•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ë˜ë©°, Bregman ë°œì‚°ì˜ ê²½ìš° ëª…í™•í•œ ë¶„í•´ê°€ ê°€ëŠ¥í•˜ë‹¤.
- 2. $g$-Bregman ë°œì‚°ì€ í¸í–¥-ë¶„ì‚° ë¶„í•´ê°€ ê°€ëŠ¥í•œ ìœ ì¼í•œ ì†ì‹¤ í•¨ìˆ˜ë¡œ, í‘œì¤€ Bregman ë°œì‚°ìœ¼ë¡œ ë³€í™˜ë  ìˆ˜ ìˆë‹¤.
- 3. $0$-$1$ ì†ì‹¤ì´ë‚˜ $L_1$ ì†ì‹¤ì€ ëª…í™•í•œ í¸í–¥-ë¶„ì‚° ë¶„í•´ë¥¼ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” ì´ì „ ì‹œë„ê°€ ì‹¤íŒ¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•œë‹¤.
- 4. ì†ì‹¤ í•¨ìˆ˜ì— ëŒ€í•œ ì œì•½ì„ ì™„í™”í•  ê²½ìš° ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-25 17:05:57*