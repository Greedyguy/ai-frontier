---
keywords:
  - Transformer
  - Word Embeddings
  - Compositionality
  - Canonical Correlation Analysis
  - Graph Neural Network
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19332
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:25:16.601499",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Word Embeddings",
    "Compositionality",
    "Canonical Correlation Analysis",
    "Graph Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Word Embeddings": 0.8,
    "Compositionality": 0.78,
    "Canonical Correlation Analysis": 0.75,
    "Graph Neural Network": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer models",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model architecture in NLP, crucial for linking to related works.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Word embeddings",
        "canonical": "Word Embeddings",
        "aliases": [
          "static word embeddings",
          "Word2vec"
        ],
        "category": "specific_connectable",
        "rationale": "Word embeddings are key to understanding compositionality in language models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Compositionality",
        "canonical": "Compositionality",
        "aliases": [
          "additive compositionality"
        ],
        "category": "unique_technical",
        "rationale": "Compositionality is a unique concept central to the paper's evaluation of language models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Canonical correlation analysis",
        "canonical": "Canonical Correlation Analysis",
        "aliases": [
          "CCA"
        ],
        "category": "unique_technical",
        "rationale": "This statistical method is critical for evaluating the linearity of embeddings.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Graph models",
        "canonical": "Graph Neural Network",
        "aliases": [
          "graph models"
        ],
        "category": "specific_connectable",
        "rationale": "Graph models are significant for understanding shifts in meaning within embeddings.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "metrics",
      "stages"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Word embeddings",
      "resolved_canonical": "Word Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Compositionality",
      "resolved_canonical": "Compositionality",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Canonical correlation analysis",
      "resolved_canonical": "Canonical Correlation Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Graph models",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Quantifying Compositionality of Classic and State-of-the-Art Embeddings

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19332.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19332](https://arxiv.org/abs/2509.19332)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Breaking Token Into Concepts_ Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics_20250923|Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics]] (83.3% similar)
- [[2025-09-25/Magnitude Matters_ a Superior Class of Similarity Metrics for Holistic Semantic Understanding_20250925|Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding]] (82.3% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (82.3% similar)
- [[2025-09-22/BBScoreV2_ Learning Time-Evolution and Latent Alignment from Stochastic Representation_20250922|BBScoreV2: Learning Time-Evolution and Latent Alignment from Stochastic Representation]] (82.2% similar)
- [[2025-09-24/Long Story Short_ Disentangling Compositionality and Long-Caption Understanding in VLMs_20250924|Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Word Embeddings|Word Embeddings]], [[keywords/Graph Neural Network|Graph Neural Network]]
**âš¡ Unique Technical**: [[keywords/Compositionality|Compositionality]], [[keywords/Canonical Correlation Analysis|Canonical Correlation Analysis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19332v1 Announce Type: cross 
Abstract: For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a "pelp" is, we can use our knowledge of numbers to understand that "ten pelps" makes more pelps than "two pelps". Static word embeddings such as Word2vec made strong, indeed excessive, claims about compositionality. The SOTA generative, transformer models and graph models, however, go too far in the other direction by providing no real limits on shifts in meaning due to context. To quantify the additive compositionality, we formalize a two-step, generalized evaluation that (i) measures the linearity between known entity attributes and their embeddings via canonical correlation analysis, and (ii) evaluates additive generalization by reconstructing embeddings for unseen attribute combinations and checking reconstruction metrics such as L2 loss, cosine similarity, and retrieval accuracy. These metrics also capture failure cases where linear composition breaks down. Sentences, knowledge graphs, and word embeddings are evaluated and tracked the compositionality across all layers and training stages. Stronger compositional signals are observed in later training stages across data modalities, and in deeper layers of the transformer-based model before a decline at the top layer. Code is available at https://github.com/Zhijin-Guo1/quantifying-compositionality.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì´ ìƒˆë¡œìš´ í‘œí˜„ì— ëŒ€í•´ ì˜¬ë°”ë¥´ê²Œ ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ ì¡°í•©ì  ì˜ë¯¸ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ Word2vec ê°™ì€ ì •ì  ë‹¨ì–´ ì„ë² ë”©ì€ ì¡°í•©ì„±ì„ ê³¼ë„í•˜ê²Œ ì£¼ì¥í–ˆìœ¼ë‚˜, ìµœì‹  ìƒì„±ì  íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ê³¼ ê·¸ë˜í”„ ëª¨ë¸ì€ ë¬¸ë§¥ì— ë”°ë¥¸ ì˜ë¯¸ ë³€í™”ì— ì œí•œì„ ë‘ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ë‘ ë‹¨ê³„ì˜ í‰ê°€ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì²«ì§¸, ì •ì¤€ ìƒê´€ ë¶„ì„ì„ í†µí•´ ì•Œë ¤ì§„ ì—”í‹°í‹° ì†ì„±ê³¼ ì„ë² ë”© ê°„ì˜ ì„ í˜•ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ë‘˜ì§¸, ë³´ì§€ ëª»í•œ ì†ì„± ì¡°í•©ì— ëŒ€í•œ ì„ë² ë”©ì„ ì¬êµ¬ì„±í•˜ì—¬ L2 ì†ì‹¤, ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ê²€ìƒ‰ ì •í™•ë„ ë“±ì˜ ì§€í‘œë¡œ í‰ê°€í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì¡°í•©ì  ì‹¤íŒ¨ ì‚¬ë¡€ë„ í¬ì°©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ê¹Šì€ ì¸µê³¼ í•™ìŠµ í›„ë°˜ ë‹¨ê³„ì—ì„œ ê°•í•œ ì¡°í•© ì‹ í˜¸ê°€ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œì™€ ë°ì´í„°ëŠ” GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸ì´ ìƒˆë¡œìš´ í‘œí˜„ì— ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ì„œëŠ” ì¡°í•©ì  ì˜ë¯¸ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì •ì  ë‹¨ì–´ ì„ë² ë”©ì€ ì¡°í•©ì„±ì— ëŒ€í•´ ê³¼ë„í•œ ì£¼ì¥ì„ í–ˆìœ¼ë‚˜, ìµœì‹  ìƒì„±ì  íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ê³¼ ê·¸ë˜í”„ ëª¨ë¸ì€ ë¬¸ë§¥ì— ë”°ë¥¸ ì˜ë¯¸ ë³€í™”ì— ì œí•œì´ ì—†ìŠµë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” ë‘ ë‹¨ê³„ì˜ ì¼ë°˜í™”ëœ í‰ê°€ë¥¼ í†µí•´ ì¶”ê°€ì  ì¡°í•©ì„±ì„ ì •ëŸ‰í™”í•˜ë©°, ì´ëŠ” ì•Œë ¤ì§„ ì—”í‹°í‹° ì†ì„±ê³¼ ì„ë² ë”© ê°„ì˜ ì„ í˜•ì„±ì„ ì¸¡ì •í•˜ê³ , ë³´ì´ì§€ ì•ŠëŠ” ì†ì„± ì¡°í•©ì— ëŒ€í•œ ì„ë² ë”©ì„ ì¬êµ¬ì„±í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
- 4. í‰ê°€ ì§€í‘œëŠ” L2 ì†ì‹¤, ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ê²€ìƒ‰ ì •í™•ë„ ë“±ì„ í¬í•¨í•˜ë©°, ì„ í˜• ì¡°í•©ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ë„ í¬ì°©í•©ë‹ˆë‹¤.
- 5. ë°ì´í„° ëª¨ë‹¬ë¦¬í‹° ì „ë°˜ì— ê±¸ì³ í›ˆë ¨ í›„ë°˜ ë‹¨ê³„ì—ì„œ ë” ê°•í•œ ì¡°í•© ì‹ í˜¸ê°€ ê´€ì°°ë˜ë©°, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ ê¹Šì€ ì¸µì—ì„œ ì´ëŸ¬í•œ ì‹ í˜¸ê°€ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:25:16*