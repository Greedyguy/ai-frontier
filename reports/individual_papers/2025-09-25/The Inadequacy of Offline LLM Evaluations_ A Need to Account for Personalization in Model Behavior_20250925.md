---
keywords:
  - Large Language Model
  - Offline Evaluation
  - Personalization in AI
  - Field Evaluation
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19364
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:30:28.249805",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Offline Evaluation",
    "Personalization in AI",
    "Field Evaluation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Offline Evaluation": 0.75,
    "Personalization in AI": 0.8,
    "Field Evaluation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "language model"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on model behavior and personalization.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "offline evaluations",
        "canonical": "Offline Evaluation",
        "aliases": [
          "offline assessment",
          "static evaluation"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific evaluation method critiqued in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "personalization",
        "canonical": "Personalization in AI",
        "aliases": [
          "user-specific adaptation",
          "customization"
        ],
        "category": "evolved_concepts",
        "rationale": "Key concept affecting model behavior as discussed in the study.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "field evaluations",
        "canonical": "Field Evaluation",
        "aliases": [
          "real-world testing",
          "practical evaluation"
        ],
        "category": "unique_technical",
        "rationale": "Contrasts with offline evaluations to show real-world model performance.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "state-less",
      "benchmark questions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "offline evaluations",
      "resolved_canonical": "Offline Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "personalization",
      "resolved_canonical": "Personalization in AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "field evaluations",
      "resolved_canonical": "Field Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19364.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19364](https://arxiv.org/abs/2509.19364)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Benchmarking and Improving LLM Robustness for Personalized Generation_20250925|Benchmarking and Improving LLM Robustness for Personalized Generation]] (84.8% similar)
- [[2025-09-23/Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues_20250923|Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues]] (83.9% similar)
- [[2025-09-23/Challenging the Evaluator_ LLM Sycophancy Under User Rebuttal_20250923|Challenging the Evaluator: LLM Sycophancy Under User Rebuttal]] (83.9% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (83.0% similar)
- [[2025-09-23/Latent Inter-User Difference Modeling for LLM Personalization_20250923|Latent Inter-User Difference Modeling for LLM Personalization]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Offline Evaluation|Offline Evaluation]], [[keywords/Field Evaluation|Field Evaluation]]
**ğŸš€ Evolved Concepts**: [[keywords/Personalization in AI|Personalization in AI]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19364v1 Announce Type: cross 
Abstract: Standard offline evaluations for language models -- a series of independent, state-less inferences made by models -- fail to capture how language models actually behave in practice, where personalization fundamentally alters model behavior. For instance, identical benchmark questions to the same language model can produce markedly different responses when prompted to a state-less system, in one user's chat session, or in a different user's chat session. In this work, we provide empirical evidence showcasing this phenomenon by comparing offline evaluations to field evaluations conducted by having 800 real users of ChatGPT and Gemini pose benchmark and other provided questions to their chat interfaces.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì˜ í‘œì¤€ ì˜¤í”„ë¼ì¸ í‰ê°€ê°€ ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì—ì„œì˜ ëª¨ë¸ í–‰ë™ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤ëŠ” ì ì„ ì§€ì í•©ë‹ˆë‹¤. ê°œì¸í™”ëœ ì‚¬ìš© í™˜ê²½ì—ì„œëŠ” ë™ì¼í•œ ì§ˆë¬¸ì´ë¼ë„ ì‚¬ìš©ìì— ë”°ë¼ ë‹¤ë¥¸ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ì§„ì€ ChatGPTì™€ Geminië¥¼ ì‚¬ìš©í•˜ëŠ” 800ëª…ì˜ ì‹¤ì œ ì‚¬ìš©ìë¡œë¶€í„° ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ í†µí•´ ì˜¤í”„ë¼ì¸ í‰ê°€ì™€ í˜„ì¥ í‰ê°€ë¥¼ ë¹„êµí•˜ì—¬ ì´ëŸ¬í•œ í˜„ìƒì„ ì‹¤ì¦ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì—ì„œì˜ í–‰ë™ì„ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•œ í‰ê°€ ë°©ë²•ë¡ ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì „í†µì ì¸ ì˜¤í”„ë¼ì¸ í‰ê°€ ë°©ì‹ì€ ì–¸ì–´ ëª¨ë¸ì˜ ì‹¤ì œ ë™ì‘ ë°©ì‹ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
- 2. ê°œì¸í™”ê°€ ì–¸ì–´ ëª¨ë¸ì˜ í–‰ë™ì„ ê·¼ë³¸ì ìœ¼ë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤.
- 3. ë™ì¼í•œ ì§ˆë¬¸ì´ë¼ë„ ì‚¬ìš©ìì— ë”°ë¼ ì–¸ì–´ ëª¨ë¸ì˜ ì‘ë‹µì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. 800ëª…ì˜ ì‹¤ì œ ì‚¬ìš©ìì™€ í•¨ê»˜í•œ í˜„ì¥ í‰ê°€ë¥¼ í†µí•´ ì´ëŸ¬í•œ í˜„ìƒì„ ì‹¤ì¦ì ìœ¼ë¡œ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:30:28*