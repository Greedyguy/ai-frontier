---
keywords:
  - Vision-Language Model
  - Zero-Shot Learning
  - TrustVLM
  - Multimodal Learning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2505.23745
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:26:05.783183",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Zero-Shot Learning",
    "TrustVLM",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.9,
    "Zero-Shot Learning": 0.85,
    "TrustVLM": 0.8,
    "Multimodal Learning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models represent a key concept in the paper and are central to understanding the multimodal capabilities discussed.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.9
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "ZSL"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a critical capability of Vision-Language Models that enhances their applicability across various tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "TrustVLM",
        "canonical": "TrustVLM",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "TrustVLM is a novel framework introduced in the paper, specifically designed to improve the trustworthiness of VLM predictions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Understanding",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Understanding is a core aspect of Vision-Language Models, crucial for their ability to process and integrate visual and textual data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.76,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "misclassification",
      "safety-critical domains"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "TrustVLM",
      "resolved_canonical": "TrustVLM",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Understanding",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.76,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# To Trust Or Not To Trust Your Vision-Language Model's Prediction

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.23745.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2505.23745](https://arxiv.org/abs/2505.23745)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Bi-VLM_ Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models_20250924|Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models]] (85.6% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (85.5% similar)
- [[2025-09-22/ViLU_ Learning Vision-Language Uncertainties for Failure Prediction_20250922|ViLU: Learning Vision-Language Uncertainties for Failure Prediction]] (85.5% similar)
- [[2025-09-24/No Labels Needed_ Zero-Shot Image Classification with Collaborative Self-Learning_20250924|No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning]] (84.8% similar)
- [[2025-09-22/Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study_20250922|Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/TrustVLM|TrustVLM]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.23745v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code is available at https://github.com/EPFL-IMOS/TrustVLM.

## ğŸ“ ìš”ì•½

Vision-Language Models(VLMs)ëŠ” ì‹œê° ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ë° ìƒì„± ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë¸ë“¤ì€ ì˜¤ë¶„ë¥˜ ë¬¸ì œë¡œ ì¸í•´ ì•ˆì „ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì—ì„œ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” VLMì˜ ì˜ˆì¸¡ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í”„ë ˆì„ì›Œí¬ì¸ TrustVLMì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì„ë² ë”© ê³µê°„ì—ì„œì˜ íŠ¹ì • ê°œë…ì˜ ëª…í™•í•œ í‘œí˜„ì„ í™œìš©í•œ ìƒˆë¡œìš´ ì‹ ë¢°ë„ ì ìˆ˜ í•¨ìˆ˜ë¥¼ í†µí•´ ì˜¤ë¶„ë¥˜ë¥¼ ê°ì§€í•©ë‹ˆë‹¤. 17ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ê³¼ 4ê°œì˜ ì•„í‚¤í…ì²˜, 2ê°œì˜ VLMì„ ì‚¬ìš©í•œ í‰ê°€ì—ì„œ ê¸°ì¡´ ê¸°ì¤€ë³´ë‹¤ ìµœëŒ€ 51.87%ì˜ AURC, 9.14%ì˜ AUROC, 32.42%ì˜ FPR95 í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. TrustVLMì€ ì¬í›ˆë ¨ ì—†ì´ ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ ë†’ì—¬ ì‹¤ì œ ì‘ìš©ì—ì„œì˜ ì•ˆì „í•œ VLM ì‚¬ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision-Language Models(VLMs)ëŠ” ì‹œê° ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì •ë ¬í•˜ëŠ” ë° ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì´ë©°, ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ë° ìƒì„± ì‘ìš©ì— í™œìš©ëœë‹¤.
- 2. VLMsëŠ” ì œë¡œìƒ· ë° ì „ì´ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì˜ëª»ëœ ì˜ˆì¸¡ì„ ìì‹  ìˆê²Œ ë‚´ë†“ëŠ” ê²½ìš°ê°€ ìˆì–´ ì•ˆì „ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì—ì„œëŠ” í° ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
- 3. TrustVLMì€ VLMì˜ ì˜ˆì¸¡ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ì´ë¯¸ì§€ ì„ë² ë”© ê³µê°„ì„ í™œìš©í•œ ìƒˆë¡œìš´ ì‹ ë¢° ì ìˆ˜ ê¸°ëŠ¥ì„ ì œì•ˆí•œë‹¤.
- 4. TrustVLMì€ 17ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ê³¼ 4ê°œì˜ ì•„í‚¤í…ì²˜, 2ê°œì˜ VLMì„ í†µí•´ í‰ê°€ë˜ì—ˆìœ¼ë©°, ê¸°ì¡´ ê¸°ì¤€ ëŒ€ë¹„ AURC 51.87%, AUROC 9.14%, FPR95 32.42%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ë‹¤.
- 5. TrustVLMì€ ì¬í›ˆë ¨ ì—†ì´ ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œì¼œ VLMì˜ ì‹¤ì œ ì‘ìš©ì—ì„œì˜ ì•ˆì „í•œ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.


---

*Generated on 2025-09-25 16:26:05*