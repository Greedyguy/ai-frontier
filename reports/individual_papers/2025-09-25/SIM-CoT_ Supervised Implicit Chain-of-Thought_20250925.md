---
keywords:
  - Implicit Chain-of-Thought
  - Large Language Model
  - Step-level Supervision
  - Auxiliary Decoder
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20317
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:06:09.573249",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Implicit Chain-of-Thought",
    "Large Language Model",
    "Step-level Supervision",
    "Auxiliary Decoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Implicit Chain-of-Thought": 0.8,
    "Large Language Model": 0.85,
    "Step-level Supervision": 0.78,
    "Auxiliary Decoder": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Implicit Chain-of-Thought",
        "canonical": "Implicit Chain-of-Thought",
        "aliases": [
          "Implicit CoT"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a novel approach to reasoning in LLMs, which is not yet widely recognized.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This is a fundamental concept in the paper, linking it to a wide range of research in language processing.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Step-level Supervision",
        "canonical": "Step-level Supervision",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This term is crucial for understanding the proposed method's novelty in stabilizing implicit CoT.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Auxiliary Decoder",
        "canonical": "Auxiliary Decoder",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The auxiliary decoder is a key component of the proposed method, enhancing interpretability and efficiency.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Implicit Chain-of-Thought",
      "resolved_canonical": "Implicit Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Step-level Supervision",
      "resolved_canonical": "Step-level Supervision",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Auxiliary Decoder",
      "resolved_canonical": "Auxiliary Decoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# SIM-CoT: Supervised Implicit Chain-of-Thought

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20317.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20317](https://arxiv.org/abs/2509.20317)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/CODI_ Compressing Chain-of-Thought into Continuous Space via Self-Distillation_20250924|CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation]] (88.6% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (87.4% similar)
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (85.7% similar)
- [[2025-09-24/LaV-CoT_ Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA_20250924|LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA]] (85.4% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (85.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Step-level Supervision|Step-level Supervision]], [[keywords/Auxiliary Decoder|Auxiliary Decoder]]
**âš¡ Unique Technical**: [[keywords/Implicit Chain-of-Thought|Implicit Chain-of-Thought]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20317v1 Announce Type: cross 
Abstract: Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.

## ğŸ“ ìš”ì•½

Implicit Chain-of-Thought (CoT) ë°©ë²•ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ëª…ì‹œì  CoT ì¶”ë¡ ë³´ë‹¤ íš¨ìœ¨ì ì¸ ëŒ€ì•ˆì´ì§€ë§Œ, ì„±ëŠ¥ ê²©ì°¨ë¡œ ì¸í•´ ì œí•œì ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì•”ë¬µì  CoT ì ‘ê·¼ë²•ì˜ ê³„ì‚° ì˜ˆì‚°ì„ í™•ì¥í•  ë•Œ ë°œìƒí•˜ëŠ” ë¶ˆì•ˆì •ì„± ë¬¸ì œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œëŠ” ì ì¬ í‘œí˜„ì˜ ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•´ì§€ëŠ” ë°ì„œ ë¹„ë¡¯ë˜ë©°, ì´ëŠ” ê¸°ì¡´ ë°©ë²•ì˜ ë‹¨ê³„ë³„ ê°ë… ë¶€ì¡± ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ SIM-CoTë¼ëŠ” ëª¨ë“ˆì„ ì œì•ˆí•˜ì—¬ ë‹¨ê³„ë³„ ê°ë…ì„ ë„ì…, ì ì¬ ì¶”ë¡  ê³µê°„ì„ ì•ˆì •í™”í•˜ê³  í’ë¶€í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. SIM-CoTëŠ” ë³´ì¡° ë””ì½”ë”ë¥¼ ì‚¬ìš©í•´ ê° ì•”ë¬µì  í† í°ì„ ëª…ì‹œì  ì¶”ë¡  ë‹¨ê³„ì™€ ì •ë ¬ì‹œì¼œ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ í¬ì°©í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ ëª¨ë“ˆì€ ì¶”ë¡  ì‹œ ì œê±°ë˜ì–´ íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©°, ì•”ë¬µì  ì¶”ë¡ ì˜ í•´ì„ ê°€ëŠ¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. SIM-CoTëŠ” ë‹¤ì–‘í•œ ì•”ë¬µì  CoT ë°©ë²•ì˜ ì •í™•ì„±ê³¼ ì•ˆì •ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìœ¼ë©°, GPT-2ì™€ LLaMA-3.1 8Bì—ì„œ ê°ê° 8.2%ì™€ 3.0%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ëª…ì‹œì  CoTë³´ë‹¤ 2.3ë°° ë†’ì€ í† í° íš¨ìœ¨ì„±ì„ ë³´ì´ë©° ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì•”ì‹œì  ì—°ì‡„ ì‚¬ê³ (CoT) ë°©ë²•ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ëª…ì‹œì  CoT ì¶”ë¡ ì— ëŒ€í•œ ìœ ë§í•œ ëŒ€ì•ˆì´ì§€ë§Œ, ì„±ëŠ¥ ê²©ì°¨ë¡œ ì¸í•´ ê·¸ ì ìš©ì´ ì œí•œë©ë‹ˆë‹¤.
- 2. ì•”ì‹œì  CoT ì ‘ê·¼ë²•ì˜ ê³„ì‚° ì˜ˆì‚°ì„ í™•ì¥í•  ë•Œ, ì•”ì‹œì  ì¶”ë¡  í† í° ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ í›ˆë ¨ ê³¼ì •ì´ ë¶ˆì•ˆì •í•´ì§€ê³  ë¶•ê´´ë˜ëŠ” ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 3. SIM-CoTëŠ” ë‹¨ê³„ë³„ ê°ë…ì„ ë„ì…í•˜ì—¬ ì ì¬ì  ì¶”ë¡  ê³µê°„ì„ ì•ˆì •í™”í•˜ê³  í’ë¶€í•˜ê²Œ ë§Œë“œëŠ” í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ í›ˆë ¨ ëª¨ë“ˆë¡œ, ë³´ì¡° ë””ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì•”ì‹œì  í† í°ì„ ëª…ì‹œì  ì¶”ë¡  ë‹¨ê³„ì™€ ì •ë ¬ì‹œí‚µë‹ˆë‹¤.
- 4. SIM-CoTëŠ” ë‹¤ì–‘í•œ ì•”ì‹œì  CoT ë°©ë²•ì˜ ì •í™•ì„±ê³¼ ì•ˆì •ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ì˜ˆë¥¼ ë“¤ì–´ GPT-2ì—ì„œ Coconutì˜ ì„±ëŠ¥ì„ 8.2% í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. SIM-CoTëŠ” ëª…ì‹œì  CoT ê¸°ì¤€ì„ GPT-2ì—ì„œ 2.1% ì´ˆê³¼í•˜ë©°, LLaMA-3.1 8Bì™€ ê°™ì€ ë” í° ëª¨ë¸ì—ì„œë„ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ìƒë‹¹íˆ ì¤„ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:06:09*