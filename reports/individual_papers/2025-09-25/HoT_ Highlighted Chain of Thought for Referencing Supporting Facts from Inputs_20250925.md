---
keywords:
  - Large Language Model
  - Highlighted Chain-of-Thought Prompting
  - Few-Shot Learning
  - Chain of Thought Prompting
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2503.02003
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:52:17.892660",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Highlighted Chain-of-Thought Prompting",
    "Few-Shot Learning",
    "Chain of Thought Prompting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Highlighted Chain-of-Thought Prompting": 0.8,
    "Few-Shot Learning": 0.78,
    "Chain of Thought Prompting": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion, providing a basis for linking with other works on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Highlighted Chain-of-Thought Prompting",
        "canonical": "Highlighted Chain-of-Thought Prompting",
        "aliases": [
          "HoT"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel technique specific to this paper, crucial for understanding its unique contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Few-Shot Settings",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the broader topic of learning paradigms, relevant for linking with other few-shot learning research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain of Thought Prompting",
        "canonical": "Chain of Thought Prompting",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "A specific prompting technique that is central to the paper's methodology, useful for linking to related prompting strategies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "arithmetic",
      "reading comprehension",
      "logical reasoning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Highlighted Chain-of-Thought Prompting",
      "resolved_canonical": "Highlighted Chain-of-Thought Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Few-Shot Settings",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain of Thought Prompting",
      "resolved_canonical": "Chain of Thought Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.02003.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2503.02003](https://arxiv.org/abs/2503.02003)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (85.6% similar)
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (85.4% similar)
- [[2025-09-24/Please Translate Again_ Two Simple Experiments on Whether Human-Like Reasoning Helps Translation_20250924|Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation]] (84.7% similar)
- [[2025-09-25/Understanding Before Reasoning_ Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting_20250925|Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting]] (84.6% similar)
- [[2025-09-24/Pathways of Thoughts_ Multi-Directional Thinking for Long-form Personalized Question Answering_20250924|Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Highlighted Chain-of-Thought Prompting|Highlighted Chain-of-Thought Prompting]], [[keywords/Chain of Thought Prompting|Chain of Thought Prompting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.02003v4 Announce Type: replace 
Abstract: An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë¹„ì‚¬ì‹¤ì  ì§„ìˆ  ìƒì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 'ê°•ì¡°ëœ ì‚¬ê³  ì‚¬ìŠ¬ í”„ë¡¬í”„íŠ¸(HoT)' ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì…ë ¥ ì§ˆë¬¸ì— XML íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì‹¤ì„ ê°•ì¡°í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤. HoTëŠ” ì†Œìˆ˜ì˜ ì˜ˆì‹œë§Œìœ¼ë¡œë„ ì‚°ìˆ , ë…í•´, ë…¼ë¦¬ì  ì¶”ë¡  ë“± 17ê°€ì§€ ê³¼ì œì—ì„œ ê¸°ì¡´ì˜ ì‚¬ê³  ì‚¬ìŠ¬ í”„ë¡¬í”„íŠ¸(CoT)ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì¸ê°„ì´ LLMì˜ ì‘ë‹µì„ ê²€ì¦í•  ë•Œ, ê°•ì¡° í‘œì‹œê°€ ì‹œê°„ ì œí•œì´ ìˆëŠ” ì°¸ê°€ìë“¤ì´ LLMì˜ ì •í™•ì„±ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì‹í•˜ë„ë¡ ë•ì§€ë§Œ, LLMì´ í‹€ë¦° ê²½ìš°ì—ë„ ì •ë‹µìœ¼ë¡œ ë¯¿ê²Œ ë§Œë“œëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì•½ì ì€ ë¹„ì‚¬ì‹¤ì  ì§„ìˆ ì„ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ì ì´ë‹¤.
- 2. Highlighted Chain-of-Thought Prompting(HoT)ëŠ” XML íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì‹¤ì„ ê°•ì¡°í•¨ìœ¼ë¡œì¨ LLMì˜ ì‘ë‹µì„ ê°œì„ í•˜ëŠ” ê¸°ë²•ì´ë‹¤.
- 3. HoTëŠ” ì†Œìˆ˜ ìƒ· ì„¤ì •ì—ì„œ 17ê°œì˜ ë‹¤ì–‘í•œ ê³¼ì œì—ì„œ ê¸°ì¡´ì˜ ì²´ì¸ ì˜¤ë¸Œ ì˜íŠ¸ í”„ë¡¬í”„íŒ…(CoT)ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
- 4. ì¸ê°„ì´ LLMì˜ ì‘ë‹µì„ ê²€ì¦í•  ë•Œ, ê°•ì¡° í‘œì‹œê°€ ì œí•œëœ ì‹œê°„ ë‚´ì— ë” ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤.
- 5. ê·¸ëŸ¬ë‚˜ LLMì´ í‹€ë ¸ì„ ë•Œ, HoTëŠ” ì‚¬ìš©ìê°€ ë‹µë³€ì´ ë§ë‹¤ê³  ë¯¿ê²Œ ë§Œë“œëŠ” ê²½í–¥ì´ ìˆë‹¤.


---

*Generated on 2025-09-26 08:52:17*