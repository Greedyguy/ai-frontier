---
keywords:
  - Multimodal Learning
  - Model Compression
  - Model Pruning
  - Model Quantization
  - Fine-tuned Models
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2507.21976
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:12:59.835199",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Model Compression",
    "Model Pruning",
    "Model Quantization",
    "Fine-tuned Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Model Compression": 0.79,
    "Model Pruning": 0.78,
    "Model Quantization": 0.77,
    "Fine-tuned Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that enhances connectivity by linking language and vision tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "compression techniques",
        "canonical": "Model Compression",
        "aliases": [
          "compression strategies"
        ],
        "category": "unique_technical",
        "rationale": "Model Compression is crucial for deploying large models efficiently, especially in resource-constrained environments.",
        "novelty_score": 0.68,
        "connectivity_score": 0.73,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "pruning",
        "canonical": "Model Pruning",
        "aliases": [
          "structural pruning"
        ],
        "category": "specific_connectable",
        "rationale": "Model Pruning is a specific technique that reduces model size and is widely applicable across various models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.76,
        "link_intent_score": 0.78
      },
      {
        "surface": "quantization",
        "canonical": "Model Quantization",
        "aliases": [
          "activation-aware quantization"
        ],
        "category": "specific_connectable",
        "rationale": "Model Quantization is a key technique for reducing computational load, relevant for efficient model deployment.",
        "novelty_score": 0.52,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "fine-tuned LLAVA model",
        "canonical": "Fine-tuned Models",
        "aliases": [
          "LLAVA"
        ],
        "category": "unique_technical",
        "rationale": "Fine-tuning is a common practice for adapting models to specific tasks, enhancing their relevance and performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.79,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "performance trade-offs",
      "medical applications",
      "memory usage"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "compression techniques",
      "resolved_canonical": "Model Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.73,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "pruning",
      "resolved_canonical": "Model Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.76,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "quantization",
      "resolved_canonical": "Model Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "fine-tuned LLAVA model",
      "resolved_canonical": "Fine-tuned Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.79,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Compression Strategies for Efficient Multimodal LLMs in Medical Contexts

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.21976.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2507.21976](https://arxiv.org/abs/2507.21976)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (87.9% similar)
- [[2025-09-24/Sparse Training Scheme for Multimodal LLM_20250924|Sparse Training Scheme for Multimodal LLM]] (87.3% similar)
- [[2025-09-24/Advances in Large Language Models for Medicine_20250924|Advances in Large Language Models for Medicine]] (86.0% similar)
- [[2025-09-24/LongLLaVA_ Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture_20250924|LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture]] (85.9% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (85.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Model Pruning|Model Pruning]], [[keywords/Model Quantization|Model Quantization]]
**âš¡ Unique Technical**: [[keywords/Model Compression|Model Compression]], [[keywords/Fine-tuned Models|Fine-tuned Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.21976v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the medical domain, but their computational costs necessitate efficient compression techniques. This paper evaluates the impact of structural pruning and activation-aware quantization on a fine-tuned LLAVA model for medical applications. We propose a novel layer selection method for pruning, analyze different quantization techniques, and assess the performance trade-offs in a prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B parameters to run within 4 GB of VRAM, reducing memory usage by 70% while achieving 4% higher model performance compared to traditional pruning and quantization techniques in the same compression ratio.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ íš¨ìœ¨ì ì¸ ì••ì¶• ê¸°ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì €ìë“¤ì€ êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸°ì™€ í™œì„±í™” ì¸ì‹ ì–‘ìí™”ë¥¼ LLAVA ëª¨ë¸ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë ˆì´ì–´ ì„ íƒ ë°©ë²•ì„ ì œì•ˆí•˜ê³ , ë‹¤ì–‘í•œ ì–‘ìí™” ê¸°ë²•ì„ ë¶„ì„í•˜ë©°, ê°€ì§€ì¹˜ê¸°-ì„¸ë°€ ì¡°ì •-ì–‘ìí™” íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ì„±ëŠ¥ ì ˆì¶©ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ 70%ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ 4% ë” ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì—¬, 7B íŒŒë¼ë¯¸í„°ì˜ MLLMì„ 4GB VRAM ë‚´ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ í° ì ì¬ë ¥ì„ ì§€ë‹ˆê³  ìˆì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ì¸í•´ íš¨ìœ¨ì ì¸ ì••ì¶• ê¸°ë²•ì´ í•„ìš”í•˜ë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” LLAVA ëª¨ë¸ì˜ êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸°ì™€ í™œì„±í™” ì¸ì‹ ì–‘ìí™”ê°€ ì˜ë£Œ ì‘ìš© ë¶„ì•¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•œë‹¤.
- 3. ê°€ì§€ì¹˜ê¸°ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë ˆì´ì–´ ì„ íƒ ë°©ë²•ì„ ì œì•ˆí•˜ê³ , ë‹¤ì–‘í•œ ì–‘ìí™” ê¸°ë²•ì„ ë¶„ì„í•˜ë©°, ê°€ì§€ì¹˜ê¸°-ì„¸ë°€ ì¡°ì •-ì–‘ìí™” íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ì„±ëŠ¥ ì ˆì¶©ì„ í‰ê°€í•œë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ 7B ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ MLLMsë¥¼ 4GB VRAM ë‚´ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 70% ê°ì†Œì‹œí‚¤ë©´ì„œë„ ë™ì¼í•œ ì••ì¶• ë¹„ìœ¨ì—ì„œ ê¸°ì¡´ ê°€ì§€ì¹˜ê¸° ë° ì–‘ìí™” ê¸°ë²• ëŒ€ë¹„ ëª¨ë¸ ì„±ëŠ¥ì„ 4% í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-25 16:12:59*