---
keywords:
  - Multimodal Learning
  - Model Compression
  - Model Pruning
  - Model Quantization
  - Fine-tuned Models
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2507.21976
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:12:59.835199",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Model Compression",
    "Model Pruning",
    "Model Quantization",
    "Fine-tuned Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Model Compression": 0.79,
    "Model Pruning": 0.78,
    "Model Quantization": 0.77,
    "Fine-tuned Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that enhances connectivity by linking language and vision tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "compression techniques",
        "canonical": "Model Compression",
        "aliases": [
          "compression strategies"
        ],
        "category": "unique_technical",
        "rationale": "Model Compression is crucial for deploying large models efficiently, especially in resource-constrained environments.",
        "novelty_score": 0.68,
        "connectivity_score": 0.73,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "pruning",
        "canonical": "Model Pruning",
        "aliases": [
          "structural pruning"
        ],
        "category": "specific_connectable",
        "rationale": "Model Pruning is a specific technique that reduces model size and is widely applicable across various models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.76,
        "link_intent_score": 0.78
      },
      {
        "surface": "quantization",
        "canonical": "Model Quantization",
        "aliases": [
          "activation-aware quantization"
        ],
        "category": "specific_connectable",
        "rationale": "Model Quantization is a key technique for reducing computational load, relevant for efficient model deployment.",
        "novelty_score": 0.52,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "fine-tuned LLAVA model",
        "canonical": "Fine-tuned Models",
        "aliases": [
          "LLAVA"
        ],
        "category": "unique_technical",
        "rationale": "Fine-tuning is a common practice for adapting models to specific tasks, enhancing their relevance and performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.79,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "performance trade-offs",
      "medical applications",
      "memory usage"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "compression techniques",
      "resolved_canonical": "Model Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.73,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "pruning",
      "resolved_canonical": "Model Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.76,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "quantization",
      "resolved_canonical": "Model Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "fine-tuned LLAVA model",
      "resolved_canonical": "Fine-tuned Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.79,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Compression Strategies for Efficient Multimodal LLMs in Medical Contexts

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.21976.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2507.21976](https://arxiv.org/abs/2507.21976)

## 🔗 유사한 논문
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (87.9% similar)
- [[2025-09-24/Sparse Training Scheme for Multimodal LLM_20250924|Sparse Training Scheme for Multimodal LLM]] (87.3% similar)
- [[2025-09-24/Advances in Large Language Models for Medicine_20250924|Advances in Large Language Models for Medicine]] (86.0% similar)
- [[2025-09-24/LongLLaVA_ Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture_20250924|LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture]] (85.9% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (85.7% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Model Pruning|Model Pruning]], [[keywords/Model Quantization|Model Quantization]]
**⚡ Unique Technical**: [[keywords/Model Compression|Model Compression]], [[keywords/Fine-tuned Models|Fine-tuned Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.21976v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the medical domain, but their computational costs necessitate efficient compression techniques. This paper evaluates the impact of structural pruning and activation-aware quantization on a fine-tuned LLAVA model for medical applications. We propose a novel layer selection method for pruning, analyze different quantization techniques, and assess the performance trade-offs in a prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B parameters to run within 4 GB of VRAM, reducing memory usage by 70% while achieving 4% higher model performance compared to traditional pruning and quantization techniques in the same compression ratio.

## 📝 요약

이 논문은 의료 분야에서 활용 가능한 다중 모달 대형 언어 모델(MLLM)의 효율적인 압축 기법을 연구합니다. 저자들은 구조적 가지치기와 활성화 인식 양자화를 LLAVA 모델에 적용하여 성능을 평가했습니다. 새로운 레이어 선택 방법을 제안하고, 다양한 양자화 기법을 분석하며, 가지치기-세밀 조정-양자화 파이프라인에서의 성능 절충을 평가했습니다. 제안된 방법은 70%의 메모리 사용량을 줄이면서도 4% 더 높은 성능을 달성하여, 7B 파라미터의 MLLM을 4GB VRAM 내에서 실행할 수 있게 합니다.

## 🎯 주요 포인트

- 1. 다중모달 대형 언어 모델(MLLMs)은 의료 분야에서 큰 잠재력을 지니고 있지만, 높은 계산 비용으로 인해 효율적인 압축 기법이 필요하다.
- 2. 본 연구에서는 LLAVA 모델의 구조적 가지치기와 활성화 인식 양자화가 의료 응용 분야에 미치는 영향을 평가한다.
- 3. 가지치기를 위한 새로운 레이어 선택 방법을 제안하고, 다양한 양자화 기법을 분석하며, 가지치기-세밀 조정-양자화 파이프라인에서의 성능 절충을 평가한다.
- 4. 제안된 방법은 7B 매개변수를 가진 MLLMs를 4GB VRAM 내에서 실행 가능하게 하여 메모리 사용량을 70% 감소시키면서도 동일한 압축 비율에서 기존 가지치기 및 양자화 기법 대비 모델 성능을 4% 향상시킨다.


---

*Generated on 2025-09-25 16:12:59*