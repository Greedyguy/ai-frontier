---
keywords:
  - Multimodal Reference Visual Grounding
  - Vision-Language Model
  - Few-Shot Learning
  - Large Language Model
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2504.02876
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:38:13.302995",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Reference Visual Grounding",
    "Vision-Language Model",
    "Few-Shot Learning",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Reference Visual Grounding": 0.88,
    "Vision-Language Model": 0.8,
    "Few-Shot Learning": 0.82,
    "Large Language Model": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Reference Visual Grounding",
        "canonical": "Multimodal Reference Visual Grounding",
        "aliases": [
          "MRVG"
        ],
        "category": "unique_technical",
        "rationale": "This is a newly introduced task that combines multimodal learning and visual grounding, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Large Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLM",
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's methodology and are a trending concept in AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Few-Shot Object Detection",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Detection"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is crucial for the proposed method and connects to broader machine learning strategies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are used for object matching, linking to a broad technical category.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "method",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Reference Visual Grounding",
      "resolved_canonical": "Multimodal Reference Visual Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Large Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Few-Shot Object Detection",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Multimodal Reference Visual Grounding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2504.02876.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2504.02876](https://arxiv.org/abs/2504.02876)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval_20250922|Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval]] (85.3% similar)
- [[2025-09-18/Improving Generalized Visual Grounding with Instance-aware Joint Learning_20250918|Improving Generalized Visual Grounding with Instance-aware Joint Learning]] (84.8% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (84.6% similar)
- [[2025-09-25/Towards Visual Text Grounding of Multimodal Large Language Model_20250925|Towards Visual Text Grounding of Multimodal Large Language Model]] (84.3% similar)
- [[2025-09-24/RSVG-ZeroOV_ Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images_20250924|RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Multimodal Reference Visual Grounding|Multimodal Reference Visual Grounding]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.02876v2 Announce Type: replace-cross 
Abstract: Visual grounding focuses on detecting objects from images based on language expressions. Recent Large Vision-Language Models (LVLMs) have significantly advanced visual grounding performance by training large models with large-scale datasets. However, the problem remains challenging, especially when similar objects appear in the input image. For example, an LVLM may not be able to differentiate Diet Coke and regular Coke in an image. In this case, if additional reference images of Diet Coke and regular Coke are available, it can help the visual grounding of similar objects.
  In this work, we introduce a new task named Multimodal Reference Visual Grounding (MRVG). In this task, a model has access to a set of reference images of objects in a database. Based on these reference images and a language expression, the model is required to detect a target object from a query image. We first introduce a new dataset to study the MRVG problem. Then we introduce a novel method, named MRVG-Net, to solve this visual grounding problem. We show that by efficiently using reference images with few-shot object detection and using Large Language Models (LLMs) for object matching, our method achieves superior visual grounding performance compared to the state-of-the-art LVLMs such as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection and visual grounding, unlocking new capabilities for visual understanding, which has wide applications in robotics. Project page with our video, code, and dataset: https://irvlutd.github.io/MultiGrounding

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìœ ì‚¬í•œ ê°ì²´ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ì—ì„œ ì–¸ì–´ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ë¥¼ ì‹ë³„í•˜ëŠ” 'ë©€í‹°ëª¨ë‹¬ ì°¸ì¡° ì‹œê°ì  ê·¸ë¼ìš´ë”©(MRVG)'ì´ë¼ëŠ” ìƒˆë¡œìš´ ê³¼ì œë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. MRVGëŠ” ë°ì´í„°ë² ì´ìŠ¤ì˜ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ ëª©í‘œ ê°ì²´ë¥¼ ì‹ë³„í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ê³¼ MRVG-Netì´ë¼ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°, ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ í™œìš©í•œ ì†Œìˆ˜ ìƒ· ê°ì²´ íƒì§€ì™€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í†µí•œ ê°ì²´ ë§¤ì¹­ì„ í†µí•´ ê¸°ì¡´ì˜ ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLM)ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë¡œë´‡ê³µí•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì‘ìš©ë  ìˆ˜ ìˆëŠ” ì‹œê°ì  ì´í•´ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ìœ ì‚¬í•œ ê°ì²´ê°€ ë“±ì¥í•˜ëŠ” ì´ë¯¸ì§€ì—ì„œ ì‹œê°ì  ê·¸ë¼ìš´ë”© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì‘ì—…ì¸ ë‹¤ì¤‘ ëª¨ë‹¬ ì°¸ì¡° ì‹œê°ì  ê·¸ë¼ìš´ë”©(MRVG)ì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 2. MRVG ì‘ì—…ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ìˆëŠ” ê°ì²´ì˜ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ ì´ë¯¸ì§€ì—ì„œ ëª©í‘œ ê°ì²´ë¥¼ íƒì§€í•´ì•¼ í•©ë‹ˆë‹¤.
- 3. MRVG-Netì´ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ì—¬, ì†Œìˆ˜ ìƒ· ê°ì²´ íƒì§€ì™€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ê°ì²´ ë§¤ì¹­ì„ í†µí•´ ì‹œê°ì  ê·¸ë¼ìš´ë”© ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ìµœì²¨ë‹¨ ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLM)ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë¡œë´‡ ê³µí•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œì˜ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ í™•ì¥í•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ì™€ ê´€ë ¨ëœ ë¹„ë””ì˜¤, ì½”ë“œ, ë°ì´í„°ì…‹ì€ í”„ë¡œì íŠ¸ í˜ì´ì§€(https://irvlutd.github.io/MultiGrounding)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:38:13*