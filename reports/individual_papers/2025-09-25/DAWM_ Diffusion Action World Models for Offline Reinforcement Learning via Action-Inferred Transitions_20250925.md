---
keywords:
  - Diffusion Models
  - Offline Reinforcement Learning
  - Inverse Dynamics Model
  - Temporal Difference Learning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19538
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:38:27.478468",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Models",
    "Offline Reinforcement Learning",
    "Inverse Dynamics Model",
    "Temporal Difference Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Models": 0.82,
    "Offline Reinforcement Learning": 0.79,
    "Inverse Dynamics Model": 0.75,
    "Temporal Difference Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion-based world models",
        "canonical": "Diffusion Models",
        "aliases": [
          "Diffusion-based models",
          "Diffusion world models"
        ],
        "category": "specific_connectable",
        "rationale": "Diffusion models are a key component in the proposed method, linking to advancements in model-based RL.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Offline Reinforcement Learning",
        "canonical": "Offline Reinforcement Learning",
        "aliases": [
          "Offline RL"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus, connecting to a broad range of RL research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.68,
        "link_intent_score": 0.79
      },
      {
        "surface": "Inverse Dynamics Model",
        "canonical": "Inverse Dynamics Model",
        "aliases": [
          "IDM"
        ],
        "category": "unique_technical",
        "rationale": "A unique component of the proposed method, facilitating action inference.",
        "novelty_score": 0.71,
        "connectivity_score": 0.67,
        "specificity_score": 0.81,
        "link_intent_score": 0.75
      },
      {
        "surface": "Temporal Difference Learning",
        "canonical": "Temporal Difference Learning",
        "aliases": [
          "TD Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding the compatibility with standard RL algorithms.",
        "novelty_score": 0.52,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion-based world models",
      "resolved_canonical": "Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Offline Reinforcement Learning",
      "resolved_canonical": "Offline Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.68,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Inverse Dynamics Model",
      "resolved_canonical": "Inverse Dynamics Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.71,
        "connectivity": 0.67,
        "specificity": 0.81,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Temporal Difference Learning",
      "resolved_canonical": "Temporal Difference Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19538.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19538](https://arxiv.org/abs/2509.19538)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Wavelet Fourier Diffuser_ Frequency-Aware Diffusion Model for Reinforcement Learning_20250925|Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning]] (86.5% similar)
- [[2025-09-22/DiffusionNFT_ Online Diffusion Reinforcement with Forward Process_20250922|DiffusionNFT: Online Diffusion Reinforcement with Forward Process]] (86.1% similar)
- [[2025-09-24/World4RL_ Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation_20250924|World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation]] (85.1% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.0% similar)
- [[2025-09-24/Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors_20250924|Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Offline Reinforcement Learning|Offline Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Diffusion Models|Diffusion Models]], [[keywords/Temporal Difference Learning|Temporal Difference Learning]]
**âš¡ Unique Technical**: [[keywords/Inverse Dynamics Model|Inverse Dynamics Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19538v1 Announce Type: cross 
Abstract: Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í™•ì‚° ê¸°ë°˜ ì„¸ê³„ ëª¨ë¸ì¸ DAWMì„ ì œì•ˆí•˜ì—¬ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì—ì„œ í˜„ì‹¤ì ì¸ ì¥ê¸° ê¶¤ì ì„ ìƒì„±í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ìƒíƒœì™€ ë³´ìƒë§Œì„ ìƒì„±í•˜ì—¬ ì¼ë³´ì°¨ ì‹œê°„ì°¨ í•™ìŠµì„ ì‚¬ìš©í•˜ëŠ” ê°€ì¹˜ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ í˜¸í™˜ì„±ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. DAWMì€ í˜„ì¬ ìƒíƒœ, í–‰ë™, ëª©í‘œ ë³´ìƒì— ë”°ë¼ ë¯¸ë˜ ìƒíƒœ-ë³´ìƒ ê¶¤ì ì„ ìƒì„±í•˜ê³ , ì—­ë™í•™ ëª¨ë¸(IDM)ì„ í†µí•´ íš¨ìœ¨ì ì¸ í–‰ë™ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ ëª¨ë“ˆì‹ ì„¤ê³„ëŠ” ì¼ë³´ì°¨ TD ê¸°ë°˜ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì— ì í•©í•œ ì™„ì „í•œ í•©ì„± ì „ì´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë³´ìˆ˜ì ì¸ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ TD3BCì™€ IQLì´ ì´ëŸ¬í•œ í™•ì¥ëœ ê¶¤ì ì„ í™œìš©í•˜ì—¬ D4RL ë²¤ì¹˜ë§ˆí¬ì˜ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ê¸°ì¡´ í™•ì‚° ê¸°ë°˜ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Diffusion ê¸°ë°˜ì˜ ì„¸ê³„ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì—ì„œ í˜„ì‹¤ì ì¸ ì¥ê¸° ê¶¤ì ì„ í•©ì„±í•˜ëŠ” ë° ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ìƒíƒœì™€ ë³´ìƒê³¼ í•¨ê»˜ í–‰ë™ì„ ì§ì ‘ ìƒì„±í•˜ì§€ ì•Šì•„, í‘œì¤€ ê°€ì¹˜ ê¸°ë°˜ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ í˜¸í™˜ì„±ì´ ì œí•œëœë‹¤.
- 3. DAWMì€ í˜„ì¬ ìƒíƒœ, í–‰ë™, ê·¸ë¦¬ê³  ë°˜í™˜ ê°’ì— ì¡°ê±´í™”ëœ ë¯¸ë˜ ìƒíƒœ-ë³´ìƒ ê¶¤ì ì„ ìƒì„±í•˜ë©°, íš¨ìœ¨ì ì¸ í–‰ë™ ì¶”ë¡ ì„ ìœ„í•œ ì—­ë™í•™ ëª¨ë¸(IDM)ê³¼ ê²°í•©ëœë‹¤.
- 4. ì´ ëª¨ë“ˆí˜• ì„¤ê³„ëŠ” 1ë‹¨ê³„ TD ê¸°ë°˜ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì— ì í•©í•œ ì™„ì „í•œ í•©ì„± ì „ì´ë¥¼ ìƒì„±í•˜ì—¬ íš¨ê³¼ì ì´ê³  ê³„ì‚° íš¨ìœ¨ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 5. ë³´ìˆ˜ì ì¸ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ TD3BCì™€ IQLì€ ì´ëŸ¬í•œ í™•ì¥ëœ ê¶¤ì ì„ í™œìš©í•œ í›ˆë ¨ì—ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ë©°, D4RL ë²¤ì¹˜ë§ˆí¬ì˜ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ê¸°ì¡´ì˜ í™•ì‚° ê¸°ë°˜ ê¸°ì¤€ì„ ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•œë‹¤.


---

*Generated on 2025-09-25 15:38:27*