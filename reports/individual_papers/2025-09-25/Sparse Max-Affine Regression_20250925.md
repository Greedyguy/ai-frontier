---
keywords:
  - Sparse Gradient Descent
  - Max-Affine Regression
  - Sparse Principal Component Analysis
  - Real Maslov Dequantization
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2411.02225
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:37:01.148509",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Sparse Gradient Descent",
    "Max-Affine Regression",
    "Sparse Principal Component Analysis",
    "Real Maslov Dequantization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Sparse Gradient Descent": 0.8,
    "Max-Affine Regression": 0.78,
    "Sparse Principal Component Analysis": 0.72,
    "Real Maslov Dequantization": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Sparse Gradient Descent",
        "canonical": "Sparse Gradient Descent",
        "aliases": [
          "Sp-GD"
        ],
        "category": "unique_technical",
        "rationale": "Sparse Gradient Descent is a novel method proposed in the paper, crucial for variable selection in max-affine regression.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Max-Affine Regression",
        "canonical": "Max-Affine Regression",
        "aliases": [
          "max-affine model"
        ],
        "category": "unique_technical",
        "rationale": "Max-Affine Regression is the central focus of the paper, offering a specific regression approach that can be linked to other affine transformation studies.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sparse Principal Component Analysis",
        "canonical": "Sparse Principal Component Analysis",
        "aliases": [
          "Sparse PCA"
        ],
        "category": "broad_technical",
        "rationale": "Sparse PCA is a well-known technique used in the initialization scheme of the proposed method, linking it to dimensionality reduction topics.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Real Maslov Dequantization",
        "canonical": "Real Maslov Dequantization",
        "aliases": [
          "RMD"
        ],
        "category": "unique_technical",
        "rationale": "Real Maslov Dequantization is a new transformation introduced in the paper, providing a novel approach to transform polynomials into max-affine models.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.92,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "convex piecewise linear regression",
      "non-asymptotic analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Sparse Gradient Descent",
      "resolved_canonical": "Sparse Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Max-Affine Regression",
      "resolved_canonical": "Max-Affine Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sparse Principal Component Analysis",
      "resolved_canonical": "Sparse Principal Component Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Real Maslov Dequantization",
      "resolved_canonical": "Real Maslov Dequantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.92,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Sparse Max-Affine Regression

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2411.02225.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2411.02225](https://arxiv.org/abs/2411.02225)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (81.7% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (80.6% similar)
- [[2025-09-24/DWTGS_ Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting_20250924|DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting]] (80.4% similar)
- [[2025-09-23/Risk Comparisons in Linear Regression_ Implicit Regularization Dominates Explicit Regularization_20250923|Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization]] (80.3% similar)
- [[2025-09-24/Diffusion Bridge Variational Inference for Deep Gaussian Processes_20250924|Diffusion Bridge Variational Inference for Deep Gaussian Processes]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Sparse Principal Component Analysis|Sparse Principal Component Analysis]]
**âš¡ Unique Technical**: [[keywords/Sparse Gradient Descent|Sparse Gradient Descent]], [[keywords/Max-Affine Regression|Max-Affine Regression]], [[keywords/Real Maslov Dequantization|Real Maslov Dequantization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2411.02225v2 Announce Type: replace-cross 
Abstract: This paper presents Sparse Gradient Descent as a solution for variable selection in convex piecewise linear regression, where the model is given as the maximum of $k$-affine functions $ x \mapsto \max_{j \in [k]} \langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$. Here, $\{ a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth weight vectors and intercepts. A non-asymptotic local convergence analysis is provided for Sp-GD under sub-Gaussian noise when the covariate distribution satisfies the sub-Gaussianity and anti-concentration properties. When the model order and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given $\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where $\sigma_z^2$ denotes the noise variance. This also implies the exact parameter recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. The proposed initialization scheme uses sparse principal component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$, then applies an $r$-covering search to estimate the model parameters. A non-asymptotic analysis is presented for this initialization scheme when the covariates and noise samples follow Gaussian distributions. When the model order and parameters are fixed, this initialization scheme provides an $\epsilon$-accurate estimate given $\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$ observations. A new transformation named Real Maslov Dequantization (RMD) is proposed to transform sparse generalized polynomials into sparse max-affine models. The error decay rate of RMD is shown to be exponentially small in its temperature parameter. Furthermore, theoretical guarantees for Sp-GD are extended to the bounded noise model induced by RMD. Numerical Monte Carlo results corroborate theoretical findings for Sp-GD and the initialization scheme.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë³¼ë¡ ì¡°ê°ë³„ ì„ í˜• íšŒê·€ì—ì„œ ë³€ìˆ˜ ì„ íƒì„ ìœ„í•œ Sparse Gradient Descent(Sp-GD)ì„ ì œì•ˆí•©ë‹ˆë‹¤. Sp-GDëŠ” ì„œë¸Œ ê°€ìš°ì‹œì•ˆ ì¡ìŒ í•˜ì—ì„œ ë¹„ëŒ€ì¹­ì  ì§€ì—­ ìˆ˜ë ´ ë¶„ì„ì„ ì œê³µí•˜ë©°, ëª¨ë¸ ìˆœì„œì™€ ë§¤ê°œë³€ìˆ˜ê°€ ê³ ì •ëœ ê²½ìš° $\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ ê´€ì¸¡ì¹˜ë¡œ $\epsilon$ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ˆê¸°í™”ëŠ” í¬ì†Œ ì£¼ì„±ë¶„ ë¶„ì„ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ë©°, ì´ëŠ” $\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$ ê´€ì¸¡ì¹˜ë¡œ $\epsilon$ ì •í™•ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, í¬ì†Œ ì¼ë°˜í™” ë‹¤í•­ì‹ì„ í¬ì†Œ ìµœëŒ€-ì•„í•€ ëª¨ë¸ë¡œ ë³€í™˜í•˜ëŠ” Real Maslov Dequantization(RMD)ì„ ì œì•ˆí•˜ë©°, RMDì˜ ì˜¤ë¥˜ ê°ì†Œìœ¨ì€ ì˜¨ë„ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•´ ì§€ìˆ˜ì ìœ¼ë¡œ ì‘ìŠµë‹ˆë‹¤. Sp-GDì˜ ì´ë¡ ì  ë³´ì¥ì€ RMDì— ì˜í•´ ìœ ë„ëœ ì¡ìŒ ëª¨ë¸ë¡œ í™•ì¥ë˜ë©°, ìˆ˜ì¹˜ì  ëª¬í…Œì¹´ë¥¼ë¡œ ê²°ê³¼ëŠ” ì´ë¡ ì  ë°œê²¬ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Sparse Gradient Descent(SGD)ëŠ” ë³¼ë¡ ì¡°ê°ë³„ ì„ í˜• íšŒê·€ì—ì„œ ë³€ìˆ˜ ì„ íƒì„ ìœ„í•œ ì†”ë£¨ì…˜ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 2. Sp-GDëŠ” ì„œë¸Œ ê°€ìš°ì‹œì•ˆ ì¡ìŒ í•˜ì—ì„œ ë¹„ëŒ€ì¹­ì  ì§€ì—­ ìˆ˜ë ´ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ì´ˆê¸°í™” ë°©ì‹ì€ í¬ì†Œ ì£¼ì„±ë¶„ ë¶„ì„ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
- 4. ìƒˆë¡œìš´ ë³€í™˜ì¸ Real Maslov Dequantization(RMD)ëŠ” í¬ì†Œ ì¼ë°˜í™” ë‹¤í•­ì‹ì„ í¬ì†Œ ìµœëŒ€-ì•„í•€ ëª¨ë¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- 5. Sp-GDì™€ ì´ˆê¸°í™” ë°©ì‹ì— ëŒ€í•œ ì´ë¡ ì  ë³´ì¥ì€ ìˆ˜ì¹˜ì  ëª¬í…Œì¹´ë¥¼ë¡œ ê²°ê³¼ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:37:01*