---
keywords:
  - Legal Case Retrieval
  - LegalSearchLM
  - Legal Elements Generation
  - Large-Scale Retrieval Benchmark
  - Constrained Decoding
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2505.23832
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:55:33.286171",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Legal Case Retrieval",
    "LegalSearchLM",
    "Legal Elements Generation",
    "Large-Scale Retrieval Benchmark",
    "Constrained Decoding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Legal Case Retrieval": 0.8,
    "LegalSearchLM": 0.9,
    "Legal Elements Generation": 0.85,
    "Large-Scale Retrieval Benchmark": 0.8,
    "Constrained Decoding": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Legal Case Retrieval",
        "canonical": "Legal Case Retrieval",
        "aliases": [
          "LCR"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized task within legal informatics that connects to broader legal and AI research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "LegalSearchLM",
        "canonical": "LegalSearchLM",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel retrieval model that enhances legal case retrieval, offering a new link to legal AI advancements.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "Legal Elements Generation",
        "canonical": "Legal Elements Generation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a new approach in legal AI that could connect to similar generative models in other domains.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large-Scale Retrieval Benchmark",
        "canonical": "Large-Scale Retrieval Benchmark",
        "aliases": [
          "LEGAR BENCH"
        ],
        "category": "unique_technical",
        "rationale": "A significant dataset that could link to other large-scale benchmarks in AI research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Constrained Decoding",
        "canonical": "Constrained Decoding",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A method used in generative models that can link to broader discussions on decoding strategies in NLP.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Legal Case Retrieval",
      "resolved_canonical": "Legal Case Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "LegalSearchLM",
      "resolved_canonical": "LegalSearchLM",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Legal Elements Generation",
      "resolved_canonical": "Legal Elements Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large-Scale Retrieval Benchmark",
      "resolved_canonical": "Large-Scale Retrieval Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Constrained Decoding",
      "resolved_canonical": "Constrained Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.23832.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2505.23832](https://arxiv.org/abs/2505.23832)

## 🔗 유사한 논문
- [[2025-09-23/LightRetriever_ A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference_20250923|LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference]] (84.5% similar)
- [[2025-09-24/SoK_ Large Language Model Copyright Auditing via Fingerprinting_20250924|SoK: Large Language Model Copyright Auditing via Fingerprinting]] (83.4% similar)
- [[2025-09-25/Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation_20250925|Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation]] (83.0% similar)
- [[2025-09-23/seqBench_ A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs_20250923|seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs]] (82.8% similar)
- [[2025-09-24/Benchmarking Critical Questions Generation_ A Challenging Reasoning Task for Large Language Models_20250924|Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models]] (82.6% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Constrained Decoding|Constrained Decoding]]
**⚡ Unique Technical**: [[keywords/Legal Case Retrieval|Legal Case Retrieval]], [[keywords/LegalSearchLM|LegalSearchLM]], [[keywords/Legal Elements Generation|Legal Elements Generation]], [[keywords/Large-Scale Retrieval Benchmark|Large-Scale Retrieval Benchmark]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.23832v2 Announce Type: replace 
Abstract: Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M candidate cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content containing those elements, grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.

## 📝 요약

이 논문은 법률 사례 검색(LCR)의 한계를 극복하기 위해 두 가지 주요 기여를 제시합니다. 첫째, 411개의 다양한 범죄 유형을 포함한 120만 개 이상의 사례를 다루는 대규모 한국어 LCR 벤치마크인 LEGAR BENCH를 소개합니다. 둘째, LegalSearchLM이라는 검색 모델을 개발하여 쿼리 사례의 법적 요소를 추론하고, 이를 기반으로 제한된 디코딩을 통해 관련 내용을 생성합니다. 실험 결과, LegalSearchLM은 LEGAR BENCH에서 기존 모델보다 6-20% 성능을 향상시켰으며, 도메인 외 사례에서도 15% 더 나은 일반화 성능을 보였습니다.

## 🎯 주요 포인트

- 1. 기존의 법률 사례 검색 연구는 소규모 데이터와 제한된 범위의 범죄 유형에 의존하여 현실적인 법률 검색 시나리오의 복잡성을 충분히 반영하지 못한다.
- 2. LEGAR BENCH는 411개의 다양한 범죄 유형을 포함하여 120만 개 이상의 후보 사례를 다루는 최초의 대규모 한국어 법률 사례 검색 벤치마크이다.
- 3. LegalSearchLM은 쿼리 사례에 대한 법적 요소 추론을 수행하고, 제한된 디코딩을 통해 목표 사례에 기반한 내용을 생성하는 검색 모델이다.
- 4. LegalSearchLM은 LEGAR BENCH에서 기존 모델들보다 6-20% 더 높은 성능을 보이며, 최첨단 성능을 달성하였다.
- 5. LegalSearchLM은 도메인 외 사례에서도 강력한 일반화 능력을 보여주며, 도메인 내 데이터로 훈련된 단순 생성 모델보다 15% 더 높은 성능을 발휘한다.


---

*Generated on 2025-09-26 08:55:33*