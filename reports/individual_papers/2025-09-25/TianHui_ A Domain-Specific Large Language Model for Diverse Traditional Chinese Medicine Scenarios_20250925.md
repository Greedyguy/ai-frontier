---
keywords:
  - Large Language Model
  - Traditional Chinese Medicine
  - Domain Knowledge Fusion
  - Attention Mechanism
  - QLoRA
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19834
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:50:45.744837",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Traditional Chinese Medicine",
    "Domain Knowledge Fusion",
    "Attention Mechanism",
    "QLoRA"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Traditional Chinese Medicine": 0.8,
    "Domain Knowledge Fusion": 0.7,
    "Attention Mechanism": 0.75,
    "QLoRA": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect to a wide range of NLP and AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Traditional Chinese Medicine",
        "canonical": "Traditional Chinese Medicine",
        "aliases": [
          "TCM"
        ],
        "category": "unique_technical",
        "rationale": "The focus on TCM makes it a unique domain-specific application, enhancing specificity and novelty.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Domain Knowledge Fusion",
        "canonical": "Domain Knowledge Fusion",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is crucial for integrating specialized knowledge into LLMs, offering high novelty.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Flash Attention 2",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Flash Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms like Flash Attention are key to improving model efficiency and performance.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "QLoRA",
        "canonical": "QLoRA",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "QLoRA is a specific technique used in the study, contributing to the novelty and technical depth.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Traditional Chinese Medicine",
      "resolved_canonical": "Traditional Chinese Medicine",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Domain Knowledge Fusion",
      "resolved_canonical": "Domain Knowledge Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Flash Attention 2",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "QLoRA",
      "resolved_canonical": "QLoRA",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19834.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19834](https://arxiv.org/abs/2509.19834)

## 🔗 유사한 논문
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (84.5% similar)
- [[2025-09-23/CUTE_ A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages_20250923|CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages]] (84.0% similar)
- [[2025-09-24/Qianfan-VL_ Domain-Enhanced Universal Vision-Language Models_20250924|Qianfan-VL: Domain-Enhanced Universal Vision-Language Models]] (82.7% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (82.6% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (82.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Traditional Chinese Medicine|Traditional Chinese Medicine]], [[keywords/Domain Knowledge Fusion|Domain Knowledge Fusion]], [[keywords/QLoRA|QLoRA]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19834v1 Announce Type: cross 
Abstract: Domain-specific LLMs in TCM face limitations in research settings due to constrained adaptability, insufficient evaluation datasets, and limited computational resources. This study presents TianHui, a specialized TCM LLM built through contextual data integration and domain knowledge fusion. We constructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA pairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage 2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked top-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW) and achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC, ADTG). Optimal configuration was identified as LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation and scalable application of TCM knowledge. All resources are open-sourced.

## 📝 요약

이 연구는 전통 중국 의학(TCM) 분야의 도메인 특화 대형 언어 모델(LLM)인 TianHui를 소개합니다. 기존 TCM LLM의 적응성 제한, 평가 데이터셋 부족, 컴퓨팅 자원 한계를 극복하기 위해, 대규모 TCM 코퍼스와 이단계 훈련 전략을 활용했습니다. TianHui는 12개의 벤치마크에서 상위 성과를 기록했으며, 최적의 설정은 LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048로 확인되었습니다. 이 모델은 TCM 지식의 체계적 보존과 확장 가능한 응용을 가능하게 하며, 모든 자원은 오픈 소스로 제공됩니다.

## 🎯 주요 포인트

- 1. TianHui는 한의학(TCM) 분야에 특화된 대규모 언어 모델(LLM)로, 맥락적 데이터 통합과 도메인 지식 융합을 통해 구축되었습니다.
- 2. 본 연구에서는 0.97GB의 비지도 데이터와 611,312개의 QA 쌍으로 구성된 대규모 한의학 말뭉치를 구축하고, QLoRA, DeepSpeed Stage 2, Flash Attention 2를 활용한 2단계 훈련 전략을 사용했습니다.
- 3. TianHui는 12개의 벤치마크 평가에서 6개 데이터셋(APQ, TCMCD, HFR, HCCA, DHPE, TLAW)에서 상위 3위 내에 들었으며, 나머지 6개 데이터셋(TCMEE, APR, GCPMI, TCMKQA, TCMRC, ADTG)에서는 최고 성과를 기록했습니다.
- 4. 최적의 구성은 LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048로 확인되었습니다.
- 5. TianHui는 한의학 지식의 체계적인 보존과 확장 가능한 응용을 가능하게 하며, 모든 자원은 오픈 소스로 제공됩니다.


---

*Generated on 2025-09-25 15:50:45*