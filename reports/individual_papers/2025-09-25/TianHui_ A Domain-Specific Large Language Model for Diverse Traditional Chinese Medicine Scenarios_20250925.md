---
keywords:
  - Large Language Model
  - Traditional Chinese Medicine
  - Domain Knowledge Fusion
  - Attention Mechanism
  - QLoRA
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19834
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:50:45.744837",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Traditional Chinese Medicine",
    "Domain Knowledge Fusion",
    "Attention Mechanism",
    "QLoRA"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Traditional Chinese Medicine": 0.8,
    "Domain Knowledge Fusion": 0.7,
    "Attention Mechanism": 0.75,
    "QLoRA": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect to a wide range of NLP and AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Traditional Chinese Medicine",
        "canonical": "Traditional Chinese Medicine",
        "aliases": [
          "TCM"
        ],
        "category": "unique_technical",
        "rationale": "The focus on TCM makes it a unique domain-specific application, enhancing specificity and novelty.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Domain Knowledge Fusion",
        "canonical": "Domain Knowledge Fusion",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is crucial for integrating specialized knowledge into LLMs, offering high novelty.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Flash Attention 2",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Flash Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms like Flash Attention are key to improving model efficiency and performance.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "QLoRA",
        "canonical": "QLoRA",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "QLoRA is a specific technique used in the study, contributing to the novelty and technical depth.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Traditional Chinese Medicine",
      "resolved_canonical": "Traditional Chinese Medicine",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Domain Knowledge Fusion",
      "resolved_canonical": "Domain Knowledge Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Flash Attention 2",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "QLoRA",
      "resolved_canonical": "QLoRA",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19834.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19834](https://arxiv.org/abs/2509.19834)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (84.5% similar)
- [[2025-09-23/CUTE_ A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages_20250923|CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages]] (84.0% similar)
- [[2025-09-24/Qianfan-VL_ Domain-Enhanced Universal Vision-Language Models_20250924|Qianfan-VL: Domain-Enhanced Universal Vision-Language Models]] (82.7% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (82.6% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (82.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Traditional Chinese Medicine|Traditional Chinese Medicine]], [[keywords/Domain Knowledge Fusion|Domain Knowledge Fusion]], [[keywords/QLoRA|QLoRA]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19834v1 Announce Type: cross 
Abstract: Domain-specific LLMs in TCM face limitations in research settings due to constrained adaptability, insufficient evaluation datasets, and limited computational resources. This study presents TianHui, a specialized TCM LLM built through contextual data integration and domain knowledge fusion. We constructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA pairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage 2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked top-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW) and achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC, ADTG). Optimal configuration was identified as LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation and scalable application of TCM knowledge. All resources are open-sourced.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì „í†µ ì¤‘êµ­ ì˜í•™(TCM) ë¶„ì•¼ì˜ ë„ë©”ì¸ íŠ¹í™” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì¸ TianHuië¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ TCM LLMì˜ ì ì‘ì„± ì œí•œ, í‰ê°€ ë°ì´í„°ì…‹ ë¶€ì¡±, ì»´í“¨íŒ… ìì› í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ëŒ€ê·œëª¨ TCM ì½”í¼ìŠ¤ì™€ ì´ë‹¨ê³„ í›ˆë ¨ ì „ëµì„ í™œìš©í–ˆìŠµë‹ˆë‹¤. TianHuiëŠ” 12ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒìœ„ ì„±ê³¼ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ìµœì ì˜ ì„¤ì •ì€ LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ TCM ì§€ì‹ì˜ ì²´ê³„ì  ë³´ì¡´ê³¼ í™•ì¥ ê°€ëŠ¥í•œ ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ëª¨ë“  ìì›ì€ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TianHuiëŠ” í•œì˜í•™(TCM) ë¶„ì•¼ì— íŠ¹í™”ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ë¡œ, ë§¥ë½ì  ë°ì´í„° í†µí•©ê³¼ ë„ë©”ì¸ ì§€ì‹ ìœµí•©ì„ í†µí•´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” 0.97GBì˜ ë¹„ì§€ë„ ë°ì´í„°ì™€ 611,312ê°œì˜ QA ìŒìœ¼ë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ í•œì˜í•™ ë§ë­‰ì¹˜ë¥¼ êµ¬ì¶•í•˜ê³ , QLoRA, DeepSpeed Stage 2, Flash Attention 2ë¥¼ í™œìš©í•œ 2ë‹¨ê³„ í›ˆë ¨ ì „ëµì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- 3. TianHuiëŠ” 12ê°œì˜ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ì„œ 6ê°œ ë°ì´í„°ì…‹(APQ, TCMCD, HFR, HCCA, DHPE, TLAW)ì—ì„œ ìƒìœ„ 3ìœ„ ë‚´ì— ë“¤ì—ˆìœ¼ë©°, ë‚˜ë¨¸ì§€ 6ê°œ ë°ì´í„°ì…‹(TCMEE, APR, GCPMI, TCMKQA, TCMRC, ADTG)ì—ì„œëŠ” ìµœê³  ì„±ê³¼ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
- 4. ìµœì ì˜ êµ¬ì„±ì€ LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. TianHuiëŠ” í•œì˜í•™ ì§€ì‹ì˜ ì²´ê³„ì ì¸ ë³´ì¡´ê³¼ í™•ì¥ ê°€ëŠ¥í•œ ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ëª¨ë“  ìì›ì€ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:50:45*