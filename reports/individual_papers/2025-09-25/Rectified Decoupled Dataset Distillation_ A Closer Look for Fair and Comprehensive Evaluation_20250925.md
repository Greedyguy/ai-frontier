---
keywords:
  - Dataset Distillation
  - Synthetic Datasets
  - Decoupled Dataset Distillation
  - Evaluation Protocols
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19743
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:05:05.911843",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Dataset Distillation",
    "Synthetic Datasets",
    "Decoupled Dataset Distillation",
    "Evaluation Protocols"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Dataset Distillation": 0.78,
    "Synthetic Datasets": 0.75,
    "Decoupled Dataset Distillation": 0.77,
    "Evaluation Protocols": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Dataset Distillation",
        "canonical": "Dataset Distillation",
        "aliases": [
          "Data Distillation"
        ],
        "category": "unique_technical",
        "rationale": "Dataset Distillation is a core concept of the paper, focusing on generating compact datasets, which is crucial for linking to related work in data efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Synthetic Datasets",
        "canonical": "Synthetic Datasets",
        "aliases": [
          "Artificial Datasets"
        ],
        "category": "specific_connectable",
        "rationale": "Synthetic Datasets are central to the paper's methodology, providing a link to discussions on data generation techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Decoupled Dataset Distillation",
        "canonical": "Decoupled Dataset Distillation",
        "aliases": [
          "Decoupled Distillation"
        ],
        "category": "unique_technical",
        "rationale": "This approach is a novel method discussed in the paper, highlighting a specific technique for dataset distillation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Post-Evaluation Protocols",
        "canonical": "Evaluation Protocols",
        "aliases": [
          "Post-Evaluation Methods"
        ],
        "category": "specific_connectable",
        "rationale": "Evaluation protocols are critical for assessing the effectiveness of dataset distillation methods, linking to broader discussions on evaluation standards.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "test accuracy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Dataset Distillation",
      "resolved_canonical": "Dataset Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Synthetic Datasets",
      "resolved_canonical": "Synthetic Datasets",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Decoupled Dataset Distillation",
      "resolved_canonical": "Decoupled Dataset Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Post-Evaluation Protocols",
      "resolved_canonical": "Evaluation Protocols",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19743.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19743](https://arxiv.org/abs/2509.19743)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DD-Ranking_ Rethinking the Evaluation of Dataset Distillation_20250923|DD-Ranking: Rethinking the Evaluation of Dataset Distillation]] (90.2% similar)
- [[2025-09-22/Efficient Multimodal Dataset Distillation via Generative Models_20250922|Efficient Multimodal Dataset Distillation via Generative Models]] (83.8% similar)
- [[2025-09-22/RMT-KD_ Random Matrix Theoretic Causal Knowledge Distillation_20250922|RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation]] (82.4% similar)
- [[2025-09-23/RCTDistill_ Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion_20250923|RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion]] (81.7% similar)
- [[2025-09-24/"What is Different Between These Datasets?" A Framework for Explaining Data Distribution Shifts_20250924|"What is Different Between These Datasets?" A Framework for Explaining Data Distribution Shifts]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Synthetic Datasets|Synthetic Datasets]], [[keywords/Evaluation Protocols|Evaluation Protocols]]
**âš¡ Unique Technical**: [[keywords/Dataset Distillation|Dataset Distillation]], [[keywords/Decoupled Dataset Distillation|Decoupled Dataset Distillation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19743v1 Announce Type: new 
Abstract: Dataset distillation aims to generate compact synthetic datasets that enable models trained on them to achieve performance comparable to those trained on full real datasets, while substantially reducing storage and computational costs. Early bi-level optimization methods (e.g., MTT) have shown promising results on small-scale datasets, but their scalability is limited by high computational overhead. To address this limitation, recent decoupled dataset distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training from the synthetic data generation process. These methods also introduce random data augmentation and epoch-wise soft labels during the post-evaluation phase to improve performance and generalization. However, existing decoupled distillation methods suffer from inconsistent post-evaluation protocols, which hinders progress in the field. In this work, we propose Rectified Decoupled Dataset Distillation (RD$^3$), and systematically investigate how different post-evaluation settings affect test accuracy. We further examine whether the reported performance differences across existing methods reflect true methodological advances or stem from discrepancies in evaluation procedures. Our analysis reveals that much of the performance variation can be attributed to inconsistent evaluation rather than differences in the intrinsic quality of the synthetic data. In addition, we identify general strategies that improve the effectiveness of distilled datasets across settings. By establishing a standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a foundation for fair and reproducible comparisons in future dataset distillation research.

## ğŸ“ ìš”ì•½

ë°ì´í„°ì…‹ ì¦ë¥˜ëŠ” ëª¨ë¸ì´ ì „ì²´ ì‹¤ì œ ë°ì´í„°ì…‹ì´ ì•„ë‹Œ ì••ì¶•ëœ í•©ì„± ë°ì´í„°ì…‹ìœ¼ë¡œë„ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‚´ë„ë¡ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ˆê¸° ì´ì¤‘ ìµœì í™” ë°©ë²•ì€ ì†Œê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ë†’ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ í™•ì¥ì„±ì— í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìµœê·¼ ë¶„ë¦¬ëœ ë°ì´í„°ì…‹ ì¦ë¥˜ ë°©ë²•ì€ êµì‚¬ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ê³¼ í•©ì„± ë°ì´í„° ìƒì„± ê³¼ì •ì„ ë¶„ë¦¬í•˜ê³ , ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë¬´ì‘ìœ„ ë°ì´í„° ì¦ê°•ê³¼ ì—í¬í¬ë³„ ì†Œí”„íŠ¸ ë¼ë²¨ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¼ê´€ë˜ì§€ ì•Šì€ í‰ê°€ í”„ë¡œí† ì½œë¡œ ì¸í•´ ë°œì „ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” Rectified Decoupled Dataset Distillation (RD$^3$)ì„ ì œì•ˆí•˜ê³ , ë‹¤ì–‘í•œ í‰ê°€ ì„¤ì •ì´ í…ŒìŠ¤íŠ¸ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, ì„±ëŠ¥ ì°¨ì´ì˜ ëŒ€ë¶€ë¶„ì€ í‰ê°€ ì ˆì°¨ì˜ ë¶ˆì¼ì¹˜ì—ì„œ ë¹„ë¡¯ë˜ì—ˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì¦ë¥˜ëœ ë°ì´í„°ì…‹ì˜ íš¨ê³¼ë¥¼ ë†’ì´ëŠ” ì¼ë°˜ì ì¸ ì „ëµì„ ì œì‹œí•˜ë©°, RD$^3$ì€ ê³µì •í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ ë¹„êµë¥¼ ìœ„í•œ í‘œì¤€í™”ëœ ë²¤ì¹˜ë§ˆí¬ì™€ í‰ê°€ í”„ë¡œí† ì½œì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë°ì´í„°ì…‹ ì¦ë¥˜ëŠ” ì €ì¥ ë° ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ë©´ì„œë„ ì „ì²´ ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ì••ì¶•ëœ í•©ì„± ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
- 2. ì´ˆê¸° ì´ì¤‘ ìµœì í™” ë°©ë²•ì€ ì†Œê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ë†’ì€ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¡œ ì¸í•´ í™•ì¥ì„±ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
- 3. ìµœê·¼ì˜ ë¶„ë¦¬ëœ ë°ì´í„°ì…‹ ì¦ë¥˜ ë°©ë²•ì€ êµì‚¬ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ê³¼ í•©ì„± ë°ì´í„° ìƒì„± ê³¼ì •ì„ ë¶„ë¦¬í•˜ì—¬ ì„±ëŠ¥ê³¼ ì¼ë°˜í™”ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 4. ê¸°ì¡´ì˜ ë¶„ë¦¬ëœ ì¦ë¥˜ ë°©ë²•ì€ ì¼ê´€ë˜ì§€ ì•Šì€ ì‚¬í›„ í‰ê°€ í”„ë¡œí† ì½œë¡œ ì¸í•´ ë°œì „ì— ì¥ì• ê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 5. RD$^3$ëŠ” í‘œì¤€í™”ëœ ë²¤ì¹˜ë§ˆí¬ì™€ ì—„ê²©í•œ í‰ê°€ í”„ë¡œí† ì½œì„ ìˆ˜ë¦½í•˜ì—¬ ë°ì´í„°ì…‹ ì¦ë¥˜ ì—°êµ¬ì—ì„œ ê³µì •í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ ë¹„êµë¥¼ ìœ„í•œ ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:05:05*