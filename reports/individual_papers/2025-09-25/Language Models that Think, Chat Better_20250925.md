---
keywords:
  - Reinforcement Learning with Verifiable Rewards
  - Model-rewarded Thinking
  - Chain of Thought reasoning
  - Preference-based Reward Model
  - Llama-3.1-8B
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.20357
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:49:24.680324",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning with Verifiable Rewards",
    "Model-rewarded Thinking",
    "Chain of Thought reasoning",
    "Preference-based Reward Model",
    "Llama-3.1-8B"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning with Verifiable Rewards": 0.78,
    "Model-rewarded Thinking": 0.82,
    "Chain of Thought reasoning": 0.8,
    "Preference-based Reward Model": 0.77,
    "Llama-3.1-8B": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning with Verifiable Rewards",
        "canonical": "Reinforcement Learning with Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's novel approach and offers a unique perspective on reinforcement learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Model-rewarded Thinking",
        "canonical": "Model-rewarded Thinking",
        "aliases": [
          "RLMT"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new paradigm that extends beyond verifiable domains, crucial for understanding the paper's contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "Chain of Thought reasoning",
        "canonical": "Chain of Thought reasoning",
        "aliases": [
          "CoT reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "A key technique in enhancing language model reasoning, relevant to the broader NLP community.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Preference-based Reward Model",
        "canonical": "Preference-based Reward Model",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for understanding the optimization process in RLMT, connecting to reinforcement learning frameworks.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Llama-3.1-8B",
        "canonical": "Llama-3.1-8B",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific model used in experiments, significant for contextualizing results.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "chat benchmarks",
      "training runs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning with Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning with Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Model-rewarded Thinking",
      "resolved_canonical": "Model-rewarded Thinking",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Chain of Thought reasoning",
      "resolved_canonical": "Chain of Thought reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Preference-based Reward Model",
      "resolved_canonical": "Preference-based Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Llama-3.1-8B",
      "resolved_canonical": "Llama-3.1-8B",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Language Models that Think, Chat Better

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20357.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.20357](https://arxiv.org/abs/2509.20357)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (88.5% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (87.0% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (86.8% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (86.4% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (86.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Chain of Thought reasoning|Chain of Thought reasoning]], [[keywords/Preference-based Reward Model|Preference-based Reward Model]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning with Verifiable Rewards|Reinforcement Learning with Verifiable Rewards]], [[keywords/Model-rewarded Thinking|Model-rewarded Thinking]], [[keywords/Llama-3.1-8B|Llama-3.1-8B]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20357v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ê¸°ë°˜ ê°•í™” í•™ìŠµ(RLVR)ì„ í™•ì¥í•˜ì—¬ ì¼ë°˜ì ì¸ ëŒ€í™” ê¸°ëŠ¥ì„ ê°œì„ í•˜ëŠ” RLMT(RL with Model-rewarded Thinking)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. RLMTëŠ” ë‹¤ì–‘í•œ ì‹¤ì œ ìƒí™©ì—ì„œ ê¸´ ì—°ì‡„ì  ì¶”ë¡ (CoT)ì„ ìƒì„±í•˜ë„ë¡ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ë©°, RLHFì—ì„œ ì‚¬ìš©ë˜ëŠ” ì„ í˜¸ ê¸°ë°˜ ë³´ìƒ ëª¨ë¸ì„ í†µí•´ ìµœì í™”í•©ë‹ˆë‹¤. Llama-3.1-8Bì™€ Qwen-2.5-7B ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ 40íšŒì˜ í›ˆë ¨ì—ì„œ RLMTëŠ” ê¸°ì¡´ RLHF íŒŒì´í”„ë¼ì¸ì„ ëŠ¥ê°€í•˜ë©°, íŠ¹íˆ ì„¸ ê°€ì§€ ëŒ€í™” ë²¤ì¹˜ë§ˆí¬ì—ì„œ 3-7ì ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ì°½ì˜ì  ê¸€ì“°ê¸°ì™€ ì¼ë°˜ ì§€ì‹ì—ì„œë„ 1-3ì ì˜ ê°œì„ ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. RLMTëŠ” ì‚¬ì „ í›ˆë ¨ ì—†ì´ë„ ê¸°ë³¸ ëª¨ë¸ì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•˜ë©°, 7ì²œ ê°œì˜ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œë„ ë³µì¡í•œ ë‹¤ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ ê±°ì¹œ ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” í›ˆë ¨ëœ ëª¨ë¸ì˜ ì‘ë‹µ ê³„íšì— ëŒ€í•œ ì •ì„±ì , ì •ëŸ‰ì  ë¶„ì„ì„ í†µí•´ í›„ì† ì—°êµ¬ì˜ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RLVRëŠ” ê²€ì¦ ê°€ëŠ¥í•œ ë„ë©”ì¸ì—ì„œ ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡ ì„ ê°œì„ í•˜ì§€ë§Œ, ê°œë°©í˜• ì‘ì—…ì—ì„œëŠ” ì¼ë°˜í™”ê°€ ì œí•œì ì…ë‹ˆë‹¤.
- 2. RLMTëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” ê¸°ëŠ¥ì„ ìœ„í•´ ëª¨ë¸ ë³´ìƒ ì¶”ë¡ ì„ ë„ì…í•˜ì—¬, RLHFì—ì„œ ì‚¬ìš©ë˜ëŠ” ì„ í˜¸ ê¸°ë°˜ ë³´ìƒ ëª¨ë¸ì„ í†µí•´ ìµœì í™”í•©ë‹ˆë‹¤.
- 3. RLMTëŠ” ë‹¤ì–‘í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì±„íŒ… ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‘œì¤€ RLHF íŒŒì´í”„ë¼ì¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. RLMTëŠ” SFT ë‹¨ê³„ ì—†ì´ ê¸°ë³¸ ëª¨ë¸ì— ì§ì ‘ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì ì€ ìˆ˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œë„ ë³µì¡í•œ ë‹¤ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì‚¬í›„ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì„ ì¬ê³ í•˜ê²Œ í•˜ë©°, ë¯¸ë˜ ì—°êµ¬ì—ì„œ ë” ë„“ì€ ë²”ìœ„ì˜ ì‚¬ê³ ë¥¼ ì´í•´í•˜ê³  í™œìš©í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:49:24*