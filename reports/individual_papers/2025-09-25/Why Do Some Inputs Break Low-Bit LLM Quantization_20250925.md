---
keywords:
  - Low-bit Quantization
  - Large Language Model
  - Residual Stream
  - Activation Patching
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2506.12044
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:28:10.329170",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Low-bit Quantization",
    "Large Language Model",
    "Residual Stream",
    "Activation Patching"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Low-bit Quantization": 0.78,
    "Large Language Model": 0.85,
    "Residual Stream": 0.77,
    "Activation Patching": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Low-bit weight-only quantization",
        "canonical": "Low-bit Quantization",
        "aliases": [
          "Low-bit Quantization",
          "Weight-only Quantization"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's analysis of memory reduction in large language models and is not covered by existing canonical terms.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This is a fundamental concept in the paper, linking to the broader field of language model research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Residual stream magnitudes",
        "canonical": "Residual Stream",
        "aliases": [
          "Residual Magnitudes",
          "Residual Stream Magnitudes"
        ],
        "category": "unique_technical",
        "rationale": "This term is crucial for understanding the error analysis and model behavior discussed in the paper.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Activation patching",
        "canonical": "Activation Patching",
        "aliases": [
          "Activation Patching"
        ],
        "category": "unique_technical",
        "rationale": "This technique is highlighted as a key method for analyzing quantization errors, offering a unique perspective.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "examples"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Low-bit weight-only quantization",
      "resolved_canonical": "Low-bit Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Residual stream magnitudes",
      "resolved_canonical": "Residual Stream",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Activation patching",
      "resolved_canonical": "Activation Patching",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Why Do Some Inputs Break Low-Bit LLM Quantization?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.12044.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2506.12044](https://arxiv.org/abs/2506.12044)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Does quantization affect models' performance on long-context tasks?_20250923|Does quantization affect models' performance on long-context tasks?]] (88.0% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (86.3% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (86.0% similar)
- [[2025-09-24/Speculate Deep and Accurate_ Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding_20250924|Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding]] (85.8% similar)
- [[2025-09-24/SBVR_ Summation of BitVector Representation for Efficient LLM Quantization_20250924|SBVR: Summation of BitVector Representation for Efficient LLM Quantization]] (85.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Low-bit Quantization|Low-bit Quantization]], [[keywords/Residual Stream|Residual Stream]], [[keywords/Activation Patching|Activation Patching]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.12044v2 Announce Type: replace-cross 
Abstract: Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ëŠ” ì €ë¹„íŠ¸ ê°€ì¤‘ì¹˜ ì–‘ìí™”ê°€ íŠ¹ì • ì˜ˆì œì— ë¶ˆê· í˜•ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì ì„ ë¶„ì„í•©ë‹ˆë‹¤. 7Bì—ì„œ 70B í¬ê¸°ì˜ LLMì— ëŒ€í•´ ë‹¤ì–‘í•œ 3-4ë¹„íŠ¸ ì–‘ìí™” ë°©ë²•ì„ ë¶„ì„í•œ ê²°ê³¼, 50ìŒì˜ ë°©ë²•ì—ì„œ ì–‘ìí™” ì˜¤ë¥˜ê°€ ê°•í•˜ê²Œ ìƒê´€ë˜ì–´ ìˆìŒ(í‰ê·  0.82)ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì™„ì „ ì •ë°€ ëª¨ë¸ì˜ ì”ì—¬ ìŠ¤íŠ¸ë¦¼ í¬ê¸°ê°€ ë¯¸ë˜ì˜ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒì„ ë°í˜”ìŠµë‹ˆë‹¤. ì”ì—¬ ìŠ¤íŠ¸ë¦¼ í¬ê¸°ì™€ ì˜¤ë¥˜ ì¦í­ ë° ëˆ„ì  ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ê°€ì„¤ì„ ì„¸ì› ìœ¼ë©°, LLM ë¡œì»¬ë¼ì´ì œì´ì…˜ ê¸°ë²•, ì¡°ê¸° ì¢…ë£Œ, í™œì„±í™” íŒ¨ì¹­ì„ í†µí•´ í° ì˜¤ë¥˜ë¥¼ ê°€ì§„ ì˜ˆì œëŠ” í›„ë°˜ ë ˆì´ì–´ì—ì„œ ì •í™•í•œ ì”ì—¬ í™œì„±í™”ì— ì˜ì¡´í•˜ë©°, MLP ê²Œì´íŠ¸ì˜ ì¶œë ¥ì´ í˜¼ë€ë„ë¥¼ ìœ ì§€í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” íŠ¹ì • ì˜ˆì œê°€ í° ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ì´ìœ ì™€ ì„±ëŠ¥ ìœ ì§€ì— ê°€ì¥ ì¤‘ìš”í•œ ëª¨ë¸ êµ¬ì„± ìš”ì†Œë¥¼ ë°í˜€ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì €ë¹„íŠ¸ ê°€ì¤‘ì¹˜ ì „ìš© ì–‘ìí™”ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ì§€ë§Œ íŠ¹ì • ì˜ˆì œì— ë¶ˆê· í˜•ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.
- 2. 50ìŒì˜ 3-4ë¹„íŠ¸ ì–‘ìí™” ë°©ë²•ì˜ ì˜¤ë¥˜ê°€ FineWeb ì˜ˆì œì—ì„œ ê°•í•˜ê²Œ ìƒê´€ë˜ì–´ ìˆìŒ(avg. 0.82)ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 3. ì „ì²´ ì •ë°€ë„ ëª¨ë¸ì˜ ì”ì—¬ ìŠ¤íŠ¸ë¦¼ í¬ê¸°ëŠ” ë¯¸ë˜ì˜ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì§€í‘œê°€ ë©ë‹ˆë‹¤.
- 4. ì”ì—¬ ìŠ¤íŠ¸ë¦¼ í¬ê¸°ê°€ ì˜¤ë¥˜ ì¦í­ ë° ëˆ„ì ê³¼ ê´€ë ¨ì´ ìˆë‹¤ëŠ” ê°€ì„¤ì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤.
- 5. MLP ê²Œì´íŠ¸ì˜ ì¶œë ¥ì´ ë‹¹í˜¹ë„ë¥¼ ìœ ì§€í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, í° ì˜¤ë¥˜ë¥¼ ê°€ì§„ ì˜ˆì œëŠ” í›„ë°˜ ë ˆì´ì–´ì—ì„œ ì •í™•í•œ ì”ì—¬ í™œì„±í™”ì— ì˜ì¡´í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:28:10*