---
keywords:
  - Large Language Model
  - Graph Neural Network
  - Circuit Tracing
  - Token Merging
  - Structural Memorization
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20336
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:07:19.254986",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Graph Neural Network",
    "Circuit Tracing",
    "Token Merging",
    "Structural Memorization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Graph Neural Network": 0.8,
    "Circuit Tracing": 0.78,
    "Token Merging": 0.77,
    "Structural Memorization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based LLMs",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Transformer-based LLM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader concept of language models used in various reasoning tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.78
      },
      {
        "surface": "graph reasoning",
        "canonical": "Graph Neural Network",
        "aliases": [
          "graph reasoning tasks"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the study of graph structures and their processing in neural networks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "circuit-tracer framework",
        "canonical": "Circuit Tracing",
        "aliases": [
          "circuit-tracer"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a unique method for visualizing reasoning processes in transformers.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "token merging",
        "canonical": "Token Merging",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific mechanism identified in graph reasoning within transformers.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "structural memorization",
        "canonical": "Structural Memorization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a core mechanism in graph reasoning that aids in understanding transformer processes.",
        "novelty_score": 0.68,
        "connectivity_score": 0.74,
        "specificity_score": 0.83,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "reasoning process",
      "model size",
      "graph density"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based LLMs",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "graph reasoning",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "circuit-tracer framework",
      "resolved_canonical": "Circuit Tracing",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "token merging",
      "resolved_canonical": "Token Merging",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "structural memorization",
      "resolved_canonical": "Structural Memorization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.74,
        "specificity": 0.83,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20336.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20336](https://arxiv.org/abs/2509.20336)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Modeling Transformers as complex networks to analyze learning dynamics_20250922|Modeling Transformers as complex networks to analyze learning dynamics]] (84.1% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory]] (82.6% similar)
- [[2025-09-25/The Conductor and the Engine_ A Path Towards Co-Designed Reasoning_20250925|The Conductor and the Engine: A Path Towards Co-Designed Reasoning]] (82.1% similar)
- [[2025-09-23/The Transfer Neurons Hypothesis_ An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs_20250923|The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs]] (81.9% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]]
**âš¡ Unique Technical**: [[keywords/Circuit Tracing|Circuit Tracing]], [[keywords/Token Merging|Token Merging]], [[keywords/Structural Memorization|Structural Memorization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20336v1 Announce Type: cross 
Abstract: Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Transformer ê¸°ë°˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê·¸ë˜í”„ ì¶”ë¡  ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ê·¸ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì€ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì ì„ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ë³¸ ë””ì½”ë” ì „ìš© Transformerë¥¼ ì„¤ì •í•˜ê³ , íšŒë¡œ ì¶”ì  í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¶”ë¡  ê³¼ì •ì„ ì‹œê°í™”í•˜ê³ , ê·¸ë˜í”„ ì¶”ë¡ ì—ì„œ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì¸ í† í° ë³‘í•©ê³¼ êµ¬ì¡°ì  ê¸°ì–µì„ ì‹ë³„í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì€ ê²½ë¡œ ì¶”ë¡ ê³¼ í•˜ìœ„ êµ¬ì¡° ì¶”ì¶œ ì‘ì—…ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤. ë˜í•œ, ê·¸ë˜í”„ ë°€ë„ì™€ ëª¨ë¸ í¬ê¸°ê°€ ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì—¬, ë””ì½”ë” ì „ìš© Transformerì˜ êµ¬ì¡°ì  ì¶”ë¡ ì„ ì´í•´í•˜ê¸° ìœ„í•œ í†µí•© í•´ì„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transformer ê¸°ë°˜ì˜ LLMì€ ê·¸ë˜í”„ ì¶”ë¡  ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 2. ê¸°ë³¸ ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒë¡œ ì¶”ì  í”„ë ˆì„ì›Œí¬ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
- 3. ê·¸ë˜í”„ ì¶”ë¡ ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í† í° ë³‘í•©ê³¼ êµ¬ì¡°ì  ê¸°ì–µì„ ì‹ë³„í•©ë‹ˆë‹¤.
- 4. ê·¸ë˜í”„ ë°€ë„ì™€ ëª¨ë¸ í¬ê¸°ê°€ ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- 5. ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ì  ì¶”ë¡ ì„ ì´í•´í•˜ê¸° ìœ„í•œ í†µí•© í•´ì„ ê°€ëŠ¥ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:07:19*