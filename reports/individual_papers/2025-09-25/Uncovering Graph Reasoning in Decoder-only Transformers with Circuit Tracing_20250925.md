---
keywords:
  - Large Language Model
  - Graph Neural Network
  - Circuit Tracing
  - Token Merging
  - Structural Memorization
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20336
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:07:19.254986",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Graph Neural Network",
    "Circuit Tracing",
    "Token Merging",
    "Structural Memorization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Graph Neural Network": 0.8,
    "Circuit Tracing": 0.78,
    "Token Merging": 0.77,
    "Structural Memorization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based LLMs",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Transformer-based LLM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader concept of language models used in various reasoning tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.78
      },
      {
        "surface": "graph reasoning",
        "canonical": "Graph Neural Network",
        "aliases": [
          "graph reasoning tasks"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the study of graph structures and their processing in neural networks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "circuit-tracer framework",
        "canonical": "Circuit Tracing",
        "aliases": [
          "circuit-tracer"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a unique method for visualizing reasoning processes in transformers.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "token merging",
        "canonical": "Token Merging",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific mechanism identified in graph reasoning within transformers.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "structural memorization",
        "canonical": "Structural Memorization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a core mechanism in graph reasoning that aids in understanding transformer processes.",
        "novelty_score": 0.68,
        "connectivity_score": 0.74,
        "specificity_score": 0.83,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "reasoning process",
      "model size",
      "graph density"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based LLMs",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "graph reasoning",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "circuit-tracer framework",
      "resolved_canonical": "Circuit Tracing",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "token merging",
      "resolved_canonical": "Token Merging",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "structural memorization",
      "resolved_canonical": "Structural Memorization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.74,
        "specificity": 0.83,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20336.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20336](https://arxiv.org/abs/2509.20336)

## 🔗 유사한 논문
- [[2025-09-22/Modeling Transformers as complex networks to analyze learning dynamics_20250922|Modeling Transformers as complex networks to analyze learning dynamics]] (84.1% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory]] (82.6% similar)
- [[2025-09-25/The Conductor and the Engine_ A Path Towards Co-Designed Reasoning_20250925|The Conductor and the Engine: A Path Towards Co-Designed Reasoning]] (82.1% similar)
- [[2025-09-23/The Transfer Neurons Hypothesis_ An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs_20250923|The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs]] (81.9% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]]
**⚡ Unique Technical**: [[keywords/Circuit Tracing|Circuit Tracing]], [[keywords/Token Merging|Token Merging]], [[keywords/Structural Memorization|Structural Memorization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20336v1 Announce Type: cross 
Abstract: Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.

## 📝 요약

이 논문은 Transformer 기반 대형 언어 모델(LLM)이 그래프 추론 작업에서 뛰어난 성능을 보이지만, 그 내부 메커니즘은 충분히 탐구되지 않았다는 점을 지적합니다. 이를 해결하기 위해 기본 디코더 전용 Transformer를 설정하고, 회로 추적 프레임워크를 사용해 설명합니다. 이를 통해 추론 과정을 시각화하고, 그래프 추론에서 핵심 메커니즘인 토큰 병합과 구조적 기억을 식별합니다. 이러한 메커니즘은 경로 추론과 하위 구조 추출 작업의 기초가 됩니다. 또한, 그래프 밀도와 모델 크기가 이러한 메커니즘에 미치는 영향을 분석하여, 디코더 전용 Transformer의 구조적 추론을 이해하기 위한 통합 해석 프레임워크를 제공합니다.

## 🎯 주요 포인트

- 1. Transformer 기반의 LLM은 그래프 추론 작업에서 강력한 성능을 보입니다.
- 2. 기본 디코더 전용 트랜스포머를 사용하여 회로 추적 프레임워크로 설명합니다.
- 3. 그래프 추론의 핵심 메커니즘으로 토큰 병합과 구조적 기억을 식별합니다.
- 4. 그래프 밀도와 모델 크기가 이러한 메커니즘에 미치는 영향을 분석합니다.
- 5. 디코더 전용 트랜스포머의 구조적 추론을 이해하기 위한 통합 해석 가능성 프레임워크를 제공합니다.


---

*Generated on 2025-09-25 16:07:19*