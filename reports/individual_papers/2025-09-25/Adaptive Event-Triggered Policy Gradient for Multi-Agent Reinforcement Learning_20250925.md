---
keywords:
  - Multi-Agent Reinforcement Learning
  - Event-Triggered Policy Gradient
  - Attention Mechanism
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20338
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:07:37.975960",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multi-Agent Reinforcement Learning",
    "Event-Triggered Policy Gradient",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multi-Agent Reinforcement Learning": 0.78,
    "Event-Triggered Policy Gradient": 0.82,
    "Attention Mechanism": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-Agent Reinforcement Learning",
        "canonical": "Multi-Agent Reinforcement Learning",
        "aliases": [
          "MARL"
        ],
        "category": "broad_technical",
        "rationale": "This is a fundamental concept in the paper, linking to a broad area of research in machine learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Event-Triggered Policy Gradient",
        "canonical": "Event-Triggered Policy Gradient",
        "aliases": [
          "ET-MAPG"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach specific to the paper, enhancing connectivity with related technical concepts.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Attention-Based Variant",
        "canonical": "Attention Mechanism",
        "aliases": [
          "AET-MAPG"
        ],
        "category": "specific_connectable",
        "rationale": "Links to existing research on attention mechanisms, which is crucial for understanding communication patterns in the paper.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "time-triggered execution",
      "state-of-the-art",
      "computational load"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-Agent Reinforcement Learning",
      "resolved_canonical": "Multi-Agent Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Event-Triggered Policy Gradient",
      "resolved_canonical": "Event-Triggered Policy Gradient",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Attention-Based Variant",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20338.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20338](https://arxiv.org/abs/2509.20338)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning_20250923|Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning]] (88.0% similar)
- [[2025-09-19/LEED_ A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning_20250919|LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning]] (87.1% similar)
- [[2025-09-19/Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control_20250919|Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control]] (85.3% similar)
- [[2025-09-23/Strategic Coordination for Evolving Multi-agent Systems_ A Hierarchical Reinforcement and Collective Learning Approach_20250923|Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach]] (85.2% similar)
- [[2025-09-23/HypeMARL_ Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems_20250923|HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Multi-Agent Reinforcement Learning|Multi-Agent Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Event-Triggered Policy Gradient|Event-Triggered Policy Gradient]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20338v1 Announce Type: cross 
Abstract: Conventional multi-agent reinforcement learning (MARL) methods rely on time-triggered execution, where agents sample and communicate actions at fixed intervals. This approach is often computationally expensive and communication-intensive. To address this limitation, we propose ET-MAPG (Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a framework that jointly learns an agent's control policy and its event-triggering policy. Unlike prior work that decouples these mechanisms, ET-MAPG integrates them into a unified learning process, enabling agents to learn not only what action to take but also when to execute it. For scenarios with inter-agent communication, we introduce AET-MAPG, an attention-based variant that leverages a self-attention mechanism to learn selective communication patterns. AET-MAPG empowers agents to determine not only when to trigger an action but also with whom to communicate and what information to exchange, thereby optimizing coordination. Both methods can be integrated with any policy gradient MARL algorithm. Extensive experiments across diverse MARL benchmarks demonstrate that our approaches achieve performance comparable to state-of-the-art, time-triggered baselines while significantly reducing both computational load and communication overhead.

## ğŸ“ ìš”ì•½

ê¸°ì¡´ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµ(MARL) ë°©ë²•ì€ ê³ ì •ëœ ê°„ê²©ìœ¼ë¡œ í–‰ë™ì„ ìƒ˜í”Œë§í•˜ê³  í†µì‹ í•˜ëŠ” ì‹œê°„ ê¸°ë°˜ ì‹¤í–‰ì— ì˜ì¡´í•˜ì—¬ ê³„ì‚° ë¹„ìš©ê³¼ í†µì‹  ë¶€ë‹´ì´ í½ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ET-MAPG(ì´ë²¤íŠ¸ ê¸°ë°˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì •ì±… ê²½ì‚¬ ê°•í™” í•™ìŠµ)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì—ì´ì „íŠ¸ì˜ ì œì–´ ì •ì±…ê³¼ ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±° ì •ì±…ì„ í†µí•©í•˜ì—¬ í•™ìŠµí•˜ë©°, ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ í–‰ë™ì„ ì·¨í• ì§€ë¿ë§Œ ì•„ë‹ˆë¼ ì–¸ì œ ì‹¤í–‰í• ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ì—ì´ì „íŠ¸ ê°„ í†µì‹ ì´ í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•´ AET-MAPGë¼ëŠ” ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ ê¸°ë°˜ ë³€í˜•ì„ ë„ì…í•˜ì—¬ ì„ íƒì  í†µì‹  íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. AET-MAPGëŠ” ì—ì´ì „íŠ¸ê°€ í–‰ë™ì„ ì–¸ì œ ì‹¤í–‰í• ì§€ë¿ë§Œ ì•„ë‹ˆë¼ ëˆ„êµ¬ì™€ í†µì‹ í• ì§€, ì–´ë–¤ ì •ë³´ë¥¼ êµí™˜í• ì§€ë¥¼ ê²°ì •í•˜ì—¬ í˜‘ì—…ì„ ìµœì í™”í•©ë‹ˆë‹¤. ì´ ë‘ ë°©ë²•ì€ ëª¨ë“  ì •ì±… ê²½ì‚¬ MARL ì•Œê³ ë¦¬ì¦˜ê³¼ í†µí•©ë  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ MARL ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ìµœì‹  ì‹œê°„ ê¸°ë°˜ ë°©ë²•ë“¤ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œë„ ê³„ì‚° ë° í†µì‹  ë¶€ë‹´ì„ í¬ê²Œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµ(MARL) ë°©ë²•ì€ ê³ ì •ëœ ê°„ê²©ìœ¼ë¡œ í–‰ë™ì„ ìƒ˜í”Œë§í•˜ê³  í†µì‹ í•˜ëŠ” ì‹œê°„ ê¸°ë°˜ ì‹¤í–‰ì— ì˜ì¡´í•©ë‹ˆë‹¤.
- 2. ET-MAPGëŠ” ì—ì´ì „íŠ¸ì˜ ì œì–´ ì •ì±…ê³¼ ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±° ì •ì±…ì„ í†µí•©í•˜ì—¬ í•™ìŠµí•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ í–‰ë™ì„ ì·¨í• ì§€ë¿ë§Œ ì•„ë‹ˆë¼ ì–¸ì œ ì‹¤í–‰í• ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
- 3. AET-MAPGëŠ” ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ì„ íƒì  í†µì‹  íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì£¼ì˜ ê¸°ë°˜ ë³€í˜•ìœ¼ë¡œ, ì—ì´ì „íŠ¸ê°€ í–‰ë™ì„ ì–¸ì œ íŠ¸ë¦¬ê±°í• ì§€ë¿ë§Œ ì•„ë‹ˆë¼ ëˆ„êµ¬ì™€ í†µì‹ í• ì§€, ì–´ë–¤ ì •ë³´ë¥¼ êµí™˜í• ì§€ë¥¼ ê²°ì •í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ë“¤ì€ ëª¨ë“  ì •ì±… ê²½ì‚¬ MARL ì•Œê³ ë¦¬ì¦˜ê³¼ í†µí•©ë  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ MARL ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ìµœì‹ ì˜ ì‹œê°„ ê¸°ë°˜ ë°©ë²•ë“¤ê³¼ ë¹„êµí•´ ì„±ëŠ¥ì€ ìœ ì‚¬í•˜ë©´ì„œë„ ê³„ì‚° ë¶€í•˜ì™€ í†µì‹  ì˜¤ë²„í—¤ë“œë¥¼ í¬ê²Œ ì¤„ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:07:37*