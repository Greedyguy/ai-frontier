---
keywords:
  - Sparse Neural Network
  - Neural Network
  - Iterative Hard Thresholding
  - Sparse Recovery
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.20323
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:48:41.888202",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Sparse Neural Network",
    "Neural Network",
    "Iterative Hard Thresholding",
    "Sparse Recovery"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Sparse Neural Network": 0.88,
    "Neural Network": 0.7,
    "Iterative Hard Thresholding": 0.78,
    "Sparse Recovery": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Sparse Neural Networks",
        "canonical": "Sparse Neural Network",
        "aliases": [
          "Sparse Networks"
        ],
        "category": "specific_connectable",
        "rationale": "Sparse Neural Networks are a specific type of neural network architecture that can be linked to research on efficient model design.",
        "novelty_score": 0.75,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "ReLU Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "ReLU Networks"
        ],
        "category": "broad_technical",
        "rationale": "ReLU Neural Networks are a fundamental type of neural network that connects to a wide range of deep learning research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Iterative Hard Thresholding",
        "canonical": "Iterative Hard Thresholding",
        "aliases": [
          "IHT"
        ],
        "category": "unique_technical",
        "rationale": "Iterative Hard Thresholding is a specific algorithm relevant to sparse recovery, offering unique insights into optimization techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sparse Recovery",
        "canonical": "Sparse Recovery",
        "aliases": [
          "Sparse Signal Recovery"
        ],
        "category": "specific_connectable",
        "rationale": "Sparse Recovery is a key concept in signal processing and neural network optimization, linking to efficient model training.",
        "novelty_score": 0.68,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "simple experiments",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Sparse Neural Networks",
      "resolved_canonical": "Sparse Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "ReLU Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Iterative Hard Thresholding",
      "resolved_canonical": "Iterative Hard Thresholding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sparse Recovery",
      "resolved_canonical": "Sparse Recovery",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# A Recovery Guarantee for Sparse Neural Networks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20323.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.20323](https://arxiv.org/abs/2509.20323)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Pulling Back the Curtain on ReLU Networks_20250923|Pulling Back the Curtain on ReLU Networks]] (83.3% similar)
- [[2025-09-25/Sobolev acceleration for neural networks_20250925|Sobolev acceleration for neural networks]] (83.3% similar)
- [[2025-09-25/Robust Training of Neural Networks at Arbitrary Precision and Sparsity_20250925|Robust Training of Neural Networks at Arbitrary Precision and Sparsity]] (80.6% similar)
- [[2025-09-23/Checking extracted rules in Neural Networks_20250923|Checking extracted rules in Neural Networks]] (80.3% similar)
- [[2025-09-24/Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions_20250924|Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Sparse Neural Network|Sparse Neural Network]], [[keywords/Sparse Recovery|Sparse Recovery]]
**âš¡ Unique Technical**: [[keywords/Iterative Hard Thresholding|Iterative Hard Thresholding]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20323v1 Announce Type: new 
Abstract: We prove the first guarantees of sparse recovery for ReLU neural networks, where the sparse network weights constitute the signal to be recovered. Specifically, we study structural properties of the sparse network weights for two-layer, scalar-output networks under which a simple iterative hard thresholding algorithm recovers these weights exactly, using memory that grows linearly in the number of nonzero weights. We validate this theoretical result with simple experiments on recovery of sparse planted MLPs, MNIST classification, and implicit neural representations. Experimentally, we find performance that is competitive with, and often exceeds, a high-performing but memory-inefficient baseline based on iterative magnitude pruning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ReLU ì‹ ê²½ë§ì˜ í¬ì†Œ íšŒë³µì— ëŒ€í•œ ìµœì´ˆì˜ ë³´ì¥ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, 2ì¸µ ìŠ¤ì¹¼ë¼ ì¶œë ¥ ë„¤íŠ¸ì›Œí¬ì˜ í¬ì†Œ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ì—°êµ¬í•˜ì—¬, ê°„ë‹¨í•œ ë°˜ë³µì  í•˜ë“œ ìŠ¤ë ˆìˆ„ë”© ì•Œê³ ë¦¬ì¦˜ì´ ë©”ëª¨ë¦¬ë¥¼ ë¹„ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ì§€ ì•Šê³  ì •í™•í•˜ê²Œ ê°€ì¤‘ì¹˜ë¥¼ íšŒë³µí•  ìˆ˜ ìˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤. ì´ ì´ë¡ ì  ê²°ê³¼ëŠ” í¬ì†Œí•˜ê²Œ ì‹¬ì–´ì§„ MLP, MNIST ë¶„ë¥˜, ì•”ë¬µì  ì‹ ê²½ í‘œí˜„ì˜ íšŒë³µ ì‹¤í—˜ì„ í†µí•´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§€ëŠ” ë°˜ë³µì  í¬ê¸° ê°€ì§€ì¹˜ê¸° ê¸°ë°˜ì˜ ê³ ì„±ëŠ¥ ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì¢…ì¢… ì´ë¥¼ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ReLU ì‹ ê²½ë§ì˜ í¬ì†Œ ë³µêµ¬ì— ëŒ€í•œ ìµœì´ˆì˜ ë³´ì¥ì„ ì¦ëª…í•˜ì˜€ìŠµë‹ˆë‹¤.
- 2. ì´ ì—°êµ¬ëŠ” 2ì¸µ, ìŠ¤ì¹¼ë¼ ì¶œë ¥ ë„¤íŠ¸ì›Œí¬ì˜ í¬ì†Œ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- 3. ê°„ë‹¨í•œ ë°˜ë³µì  í•˜ë“œ ìŠ¤ë ˆìˆ„ë”© ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ê°€ ë¹„ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ì§€ ì•Šê³  ì •í™•í•˜ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.
- 4. ì‹¤í—˜ì ìœ¼ë¡œ í¬ì†Œí•œ MLP, MNIST ë¶„ë¥˜, ì•”ë¬µì  ì‹ ê²½ í‘œí˜„ì˜ ë³µêµ¬ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì ì¸ ë°˜ë³µì  í¬ê¸° ê°€ì§€ì¹˜ê¸° ê¸°ë°˜ì˜ ê³ ì„±ëŠ¥ ê¸°ì¤€ê³¼ ê²½ìŸí•˜ë©° ì¢…ì¢… ì´ë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:48:41*