---
keywords:
  - Kalman Filter Enhanced GRPO
  - Reinforcement Learning
  - Advantage Function
  - Large Language Model
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2505.07527
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:06:59.302077",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Kalman Filter Enhanced GRPO",
    "Reinforcement Learning",
    "Advantage Function",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Kalman Filter Enhanced GRPO": 0.78,
    "Reinforcement Learning": 0.85,
    "Advantage Function": 0.72,
    "Large Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Kalman Filter Enhanced Group Relative Policy Optimization",
        "canonical": "Kalman Filter Enhanced GRPO",
        "aliases": [
          "KRPO"
        ],
        "category": "unique_technical",
        "rationale": "KRPO represents a novel integration of Kalman filtering with GRPO, offering a unique approach to advantage estimation in reinforcement learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational concept in the paper, connecting it to a wide array of related research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Advantage Function",
        "canonical": "Advantage Function",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The advantage function is central to the paper's methodology, linking it to broader discussions on policy optimization.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Language models are critical to the paper's context, connecting it to ongoing developments in NLP.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "policy optimization",
      "reward signals"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Kalman Filter Enhanced Group Relative Policy Optimization",
      "resolved_canonical": "Kalman Filter Enhanced GRPO",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Advantage Function",
      "resolved_canonical": "Advantage Function",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.07527.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2505.07527](https://arxiv.org/abs/2505.07527)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/NGRPO_ Negative-enhanced Group Relative Policy Optimization_20250924|NGRPO: Negative-enhanced Group Relative Policy Optimization]] (90.6% similar)
- [[2025-09-25/Stepwise Guided Policy Optimization_ Coloring your Incorrect Reasoning in GRPO_20250925|Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO]] (89.6% similar)
- [[2025-09-24/Single-stream Policy Optimization_20250924|Single-stream Policy Optimization]] (88.8% similar)
- [[2025-09-23/GRPO-LEAD_ A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models_20250923|GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models]] (88.8% similar)
- [[2025-09-24/MAPO_ Mixed Advantage Policy Optimization_20250924|MAPO: Mixed Advantage Policy Optimization]] (87.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Advantage Function|Advantage Function]]
**âš¡ Unique Technical**: [[keywords/Kalman Filter Enhanced GRPO|Kalman Filter Enhanced GRPO]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.07527v3 Announce Type: replace 
Abstract: The advantage function is a central concept in RL that helps reduce variance in policy gradient estimates. Recently, for language modeling, Group Relative Policy Optimization (GRPO) was proposed to compute the advantage for each output by subtracting the mean reward, as the baseline, for all outputs in the group. However, it can lead to high variance when the reward advantage is inaccurately predicted. In this work, we propose Kalman Filter Enhanced Group Relative Policy Optimization (KRPO) model, by using lightweight Kalman filtering to dynamically estimate the latent reward baseline and uncertainty. This filtering technique replaces the naive group mean, enabling more adaptive advantage normalization. Our method does not require additional learned parameters over GRPO. This approach offers a simple yet effective way to incorporate multiple outputs of GRPO into advantage estimation, improving policy optimization in settings where highly dynamic reward signals are difficult to model for language models. Through the accuracies and rewards obtained from math question answering and reasoning, we show that using a more adaptive advantage estimation model, KRPO can improve the stability and performance of GRPO. The code is available at https://github.com/billhhh/KRPO_LLMs_RL.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL)ì—ì„œ ì •ì±… ê¸°ìš¸ê¸° ì¶”ì •ì˜ ë¶„ì‚°ì„ ì¤„ì´ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ì´ì  í•¨ìˆ˜ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ GRPOëŠ” ê·¸ë£¹ ë‚´ ëª¨ë“  ì¶œë ¥ì˜ í‰ê·  ë³´ìƒì„ ê¸°ì¤€ìœ¼ë¡œ ì´ì ì„ ê³„ì‚°í•˜ì§€ë§Œ, ë³´ìƒ ì´ì ì´ ë¶€ì •í™•í•˜ê²Œ ì˜ˆì¸¡ë  ê²½ìš° ë†’ì€ ë¶„ì‚°ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ëŠ” ê²½ëŸ‰ ì¹¼ë§Œ í•„í„°ë¥¼ í™œìš©í•œ KRPO ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì¹¼ë§Œ í•„í„°ë¥¼ í†µí•´ ì ì¬ ë³´ìƒ ê¸°ì¤€ì„ ê³¼ ë¶ˆí™•ì‹¤ì„±ì„ ë™ì ìœ¼ë¡œ ì¶”ì •í•˜ì—¬, ë³´ë‹¤ ì ì‘ì ì¸ ì´ì  ì •ê·œí™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. KRPOëŠ” ì¶”ê°€ í•™ìŠµ íŒŒë¼ë¯¸í„° ì—†ì´ GRPOì˜ ë‹¤ì¤‘ ì¶œë ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ì—¬ ì •ì±… ìµœì í™”ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ìˆ˜í•™ ë¬¸ì œ í•´ê²° ë° ì¶”ë¡ ì—ì„œì˜ ì •í™•ë„ì™€ ë³´ìƒ ê²°ê³¼ë¥¼ í†µí•´ KRPOê°€ GRPOì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GRPOëŠ” ê·¸ë£¹ ë‚´ ëª¨ë“  ì¶œë ¥ì˜ í‰ê·  ë³´ìƒì„ ê¸°ì¤€ìœ¼ë¡œ ì´ì ì„ ê³„ì‚°í•˜ì§€ë§Œ, ë³´ìƒ ì´ì ì´ ë¶€ì •í™•í•˜ê²Œ ì˜ˆì¸¡ë˜ë©´ ë†’ì€ ë¶„ì‚°ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
- 2. KRPO ëª¨ë¸ì€ ê²½ëŸ‰ ì¹¼ë§Œ í•„í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ ì ì¬ ë³´ìƒ ê¸°ì¤€ì„ ê³¼ ë¶ˆí™•ì‹¤ì„±ì„ ë™ì ìœ¼ë¡œ ì¶”ì •í•˜ì—¬ ë” ì ì‘ì ì¸ ì´ì  ì •ê·œí™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 3. KRPOëŠ” GRPOì— ë¹„í•´ ì¶”ê°€ì ì¸ í•™ìŠµ ë§¤ê°œë³€ìˆ˜ë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•Šìœ¼ë©°, GRPOì˜ ì—¬ëŸ¬ ì¶œë ¥ì„ ì´ì  ì¶”ì •ì— í†µí•©í•˜ëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì œê³µí•œë‹¤.
- 4. KRPOëŠ” ìˆ˜í•™ ë¬¸ì œ í•´ê²° ë° ì¶”ë¡ ì—ì„œì˜ ì •í™•ë„ì™€ ë³´ìƒì„ í†µí•´ GRPOì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-25 17:06:59*