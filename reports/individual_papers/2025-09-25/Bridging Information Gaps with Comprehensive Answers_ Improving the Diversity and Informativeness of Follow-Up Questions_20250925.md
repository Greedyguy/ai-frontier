---
keywords:
  - Large Language Model
  - Information Gap Knowledge Distillation
  - FollowupQG Dataset
  - Conversational Agents
  - Teacher-Student Models
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2502.17715
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:19:54.887891",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Information Gap Knowledge Distillation",
    "FollowupQG Dataset",
    "Conversational Agents",
    "Teacher-Student Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Information Gap Knowledge Distillation": 0.78,
    "FollowupQG Dataset": 0.77,
    "Conversational Agents": 0.8,
    "Teacher-Student Models": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and are a well-established concept in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "information-gap-driven knowledge distillation",
        "canonical": "Information Gap Knowledge Distillation",
        "aliases": [
          "gap-driven distillation"
        ],
        "category": "unique_technical",
        "rationale": "This novel method is a key contribution of the paper, enhancing model training by focusing on information gaps.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "FollowupQG dataset",
        "canonical": "FollowupQG Dataset",
        "aliases": [
          "Followup Question Generation Dataset"
        ],
        "category": "unique_technical",
        "rationale": "The dataset is crucial for training models in the study and is specific to the paper's domain.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "conversational agents",
        "canonical": "Conversational Agents",
        "aliases": [
          "chatbots",
          "dialogue systems"
        ],
        "category": "specific_connectable",
        "rationale": "Conversational agents are the application focus of the paper, linking it to broader AI and NLP fields.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "teacher-student model pairs",
        "canonical": "Teacher-Student Models",
        "aliases": [
          "teacher-student architecture"
        ],
        "category": "specific_connectable",
        "rationale": "This architecture is a key mechanism in the paper for knowledge transfer and model training.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "comprehensive answers",
      "small models",
      "resource-constrained"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "information-gap-driven knowledge distillation",
      "resolved_canonical": "Information Gap Knowledge Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "FollowupQG dataset",
      "resolved_canonical": "FollowupQG Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "conversational agents",
      "resolved_canonical": "Conversational Agents",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "teacher-student model pairs",
      "resolved_canonical": "Teacher-Student Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Bridging Information Gaps with Comprehensive Answers: Improving the Diversity and Informativeness of Follow-Up Questions

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.17715.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2502.17715](https://arxiv.org/abs/2502.17715)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue_20250923|A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue]] (85.1% similar)
- [[2025-09-22/Search and Refine During Think_ Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning_20250922|Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning]] (83.2% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (83.1% similar)
- [[2025-09-25/Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation_20250925|Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation]] (83.0% similar)
- [[2025-09-25/The Knowledge-Behaviour Disconnect in LLM-based Chatbots_20250925|The Knowledge-Behaviour Disconnect in LLM-based Chatbots]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Conversational Agents|Conversational Agents]], [[keywords/Teacher-Student Models|Teacher-Student Models]]
**âš¡ Unique Technical**: [[keywords/Information Gap Knowledge Distillation|Information Gap Knowledge Distillation]], [[keywords/FollowupQG Dataset|FollowupQG Dataset]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.17715v2 Announce Type: replace-cross 
Abstract: Generating diverse follow-up questions that uncover missing information remains challenging for conversational agents, particularly when they run on small, locally hosted models. To address this, we develop an information-gap-driven knowledge distillation pipeline in which a teacher LLM generates a comprehensive answer, contrasts it with the initial answer to identify information gaps, and formulates gap-bridging follow-up questions. Using this pipeline, we augment the existing FollowupQG dataset tenfold. We then fine-tune smaller student models on the augmented dataset to distill the teacher's knowledge. Experiments with selected teacher-student model pairs show that fine-tuned students achieve significantly higher informativeness and diversity than variations trained on the original dataset. These findings indicate that our pipeline, which mirrors the human cognitive process of information seeking, provides an efficient distillation channel from state-of-the-art LLMs to smaller models, enabling resource-constrained conversational systems to generate more diverse and informative follow-up questions.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í™”í˜• ì—ì´ì „íŠ¸ê°€ ë‹¤ì–‘í•œ í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì •ë³´ ê²©ì°¨ ê¸°ë°˜ì˜ ì§€ì‹ ì¦ë¥˜ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ íŒŒì´í”„ë¼ì¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì´ˆê¸° ë‹µë³€ê³¼ ë¹„êµí•˜ì—¬ ì •ë³´ ê²©ì°¨ë¥¼ ì‹ë³„í•œ í›„, ê·¸ ê²©ì°¨ë¥¼ ë©”ìš°ëŠ” í›„ì† ì§ˆë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì¡´ FollowupQG ë°ì´í„°ì…‹ì„ 10ë°°ë¡œ í™•ì¥í•˜ê³ , ì†Œí˜• ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ëŒ€í˜• ëª¨ë¸ì˜ ì§€ì‹ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë¯¸ì„¸ ì¡°ì •ëœ ì†Œí˜• ëª¨ë¸ì´ ì›ë˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ë³´ë‹¤ ì •ë³´ì„±ê³¼ ë‹¤ì–‘ì„±ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ìì›ì´ ì œí•œëœ ëŒ€í™” ì‹œìŠ¤í…œì´ ë” ë‹¤ì–‘í•˜ê³  ì •ë³´ì„± ìˆëŠ” í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì •ë³´ ê²©ì°¨ë¥¼ ë©”ìš°ê¸° ìœ„í•œ ì§€ì‹ ì¦ë¥˜ íŒŒì´í”„ë¼ì¸ì„ ê°œë°œí•˜ì—¬ ëŒ€í™”í˜• ì—ì´ì „íŠ¸ì˜ í›„ì† ì§ˆë¬¸ ìƒì„±ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
- 2. êµì‚¬ LLMì´ í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì´ˆê¸° ë‹µë³€ê³¼ ë¹„êµí•˜ì—¬ ì •ë³´ ê²©ì°¨ë¥¼ ì‹ë³„í•œ í›„ ì´ë¥¼ ë©”ìš°ëŠ” í›„ì† ì§ˆë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤.
- 3. ê°œë°œí•œ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ê¸°ì¡´ FollowupQG ë°ì´í„°ì…‹ì„ 10ë°°ë¡œ ì¦ê°•í–ˆìŠµë‹ˆë‹¤.
- 4. ì¦ê°•ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ì†Œí˜• í•™ìƒ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ êµì‚¬ì˜ ì§€ì‹ì„ ì¦ë¥˜í•˜ì˜€ìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ë¯¸ì„¸ ì¡°ì •ëœ í•™ìƒ ëª¨ë¸ì´ ì›ë³¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ëœ ë³€í˜•ë³´ë‹¤ ì •ë³´ì„±ê³¼ ë‹¤ì–‘ì„±ì—ì„œ í˜„ì €íˆ ë†’ì€ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:19:54*