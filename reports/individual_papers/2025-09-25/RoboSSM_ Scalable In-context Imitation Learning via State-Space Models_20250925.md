---
keywords:
  - Few-Shot Learning
  - State-Space Models
  - Longhorn
  - Transformer
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19658
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:42:47.414034",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "State-Space Models",
    "Longhorn",
    "Transformer"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.82,
    "State-Space Models": 0.79,
    "Longhorn": 0.77,
    "Transformer": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "In-context Imitation Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "ICIL"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of learning from few examples, which is central to the paper's approach.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "State-Space Models",
        "canonical": "State-Space Models",
        "aliases": [
          "SSM"
        ],
        "category": "unique_technical",
        "rationale": "Key technical component of the proposed method, distinguishing it from Transformer-based approaches.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "Longhorn",
        "canonical": "Longhorn",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific implementation of SSM that enhances scalability and efficiency, central to the paper's contribution.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Serves as a comparative baseline in the paper, highlighting the advantages of the proposed method.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "LIBERO benchmark",
      "deployment time",
      "parameter updates"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "In-context Imitation Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "State-Space Models",
      "resolved_canonical": "State-Space Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Longhorn",
      "resolved_canonical": "Longhorn",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# RoboSSM: Scalable In-context Imitation Learning via State-Space Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19658.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19658](https://arxiv.org/abs/2509.19658)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/CodeSSM_ Towards State Space Models for Code Understanding_20250923|CodeSSM: Towards State Space Models for Code Understanding]] (84.7% similar)
- [[2025-09-25/Self-evolved Imitation Learning in Simulated World_20250925|Self-evolved Imitation Learning in Simulated World]] (83.1% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (81.8% similar)
- [[2025-09-23/TACO_ Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration_20250923|TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration]] (81.8% similar)
- [[2025-09-22/Disentangling Latent Shifts of In-Context Learning with Weak Supervision_20250922|Disentangling Latent Shifts of In-Context Learning with Weak Supervision]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/State-Space Models|State-Space Models]], [[keywords/Longhorn|Longhorn]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19658v1 Announce Type: cross 
Abstract: In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn -- a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at https://github.com/youngjuY/RoboSSM.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ì¸ì»¨í…ìŠ¤íŠ¸ ëª¨ë°© í•™ìŠµ(ICIL) ë°©ë²•ì¸ RoboSSMì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ICIL ë°©ë²•ì€ Transformerë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. RoboSSMì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Longhornì´ë¼ëŠ” ìµœì‹  ìƒíƒœ ê³µê°„ ëª¨ë¸(SSM)ì„ ë„ì…í•˜ì—¬ ì„ í˜• ì‹œê°„ ì¶”ë¡ ê³¼ ê°•ë ¥í•œ ì™¸ì‚½ ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤. LIBERO ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, RoboSSMì€ ë‹¤ì–‘í•œ ì¸ì»¨í…ìŠ¤íŠ¸ ë°ëª¨ì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©°, ìƒˆë¡œìš´ ì‘ì—…ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ê¸´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œë„ ê°•ê±´í•¨ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŠ” SSMì´ ICILì˜ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ê¸°ë°˜ì´ ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. In-context imitation learning(ICIL)ì€ ì†Œìˆ˜ì˜ ì‹œì—°ìœ¼ë¡œ ë¡œë´‡ì´ ì‘ì—…ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ë©°, ìƒˆë¡œìš´ ì‘ì—…ì— ëŒ€í•œ few-shot ì ì‘ì„ ì§€ì›í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ICIL ë°©ë²•ì€ Transformerì— ì˜ì¡´í•˜ì§€ë§Œ, ì´ëŠ” ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 3. RoboSSMì€ Transformer ëŒ€ì‹  Longhornì´ë¼ëŠ” ìµœì‹  ìƒíƒœ ê³µê°„ ëª¨ë¸(SSM)ì„ ì‚¬ìš©í•˜ì—¬ ê¸´ í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ ì„ í˜• ì‹œê°„ ì¶”ë¡ ê³¼ ê°•ë ¥í•œ ì™¸ì‚½ ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. LIBERO ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, RoboSSMì€ ë‹¤ì–‘í•œ ìˆ˜ì˜ ì‹œì—°ì— íš¨ê³¼ì ìœ¼ë¡œ ì™¸ì‚½í•˜ê³ , ë³´ì§€ ëª»í•œ ì‘ì—…ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ê¸´ ìˆ˜í‰ì„  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œë„ ê°•ê±´í•¨ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 5. SSMì´ ICILì˜ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°±ë³¸ìœ¼ë¡œì„œì˜ ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŒì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:42:47*