---
keywords:
  - Large Language Model Evaluation
  - Self-Reference Evaluation
  - Large Language Model
  - Model Abilities Correlation
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19880
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:52:26.009322",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model Evaluation",
    "Self-Reference Evaluation",
    "Large Language Model",
    "Model Abilities Correlation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model Evaluation": 0.78,
    "Self-Reference Evaluation": 0.82,
    "Large Language Model": 0.85,
    "Model Abilities Correlation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LLM-as-Judge",
        "canonical": "Large Language Model Evaluation",
        "aliases": [
          "LLM Evaluation",
          "LLM Judge"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a specific framework for evaluating AI models, which is central to the paper's contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "self-reference-guided evaluation",
        "canonical": "Self-Reference Evaluation",
        "aliases": [
          "Self-Reference Method"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel evaluation strategy introduced in the paper, enhancing the linkage between generation and judgment abilities.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper, this term ties the evaluation framework to a well-established concept in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "generation and judgment abilities",
        "canonical": "Model Abilities Correlation",
        "aliases": [
          "Generation-Judgment Correlation"
        ],
        "category": "unique_technical",
        "rationale": "The paper focuses on the correlation between these abilities, which is a key aspect of the proposed evaluation strategy.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "task",
      "sensitivity"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LLM-as-Judge",
      "resolved_canonical": "Large Language Model Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "self-reference-guided evaluation",
      "resolved_canonical": "Self-Reference Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "generation and judgment abilities",
      "resolved_canonical": "Model Abilities Correlation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19880.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19880](https://arxiv.org/abs/2509.19880)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues_20250923|Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues]] (86.5% similar)
- [[2025-09-25/Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers_20250925|Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers]] (86.2% similar)
- [[2025-09-24/Analyzing Uncertainty of LLM-as-a-Judge_ Interval Evaluations with Conformal Prediction_20250924|Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction]] (85.4% similar)
- [[2025-09-24/AECBench_ A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field_20250924|AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field]] (85.3% similar)
- [[2025-09-23/Improving User Interface Generation Models from Designer Feedback_20250923|Improving User Interface Generation Models from Designer Feedback]] (84.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Large Language Model Evaluation|Large Language Model Evaluation]], [[keywords/Self-Reference Evaluation|Self-Reference Evaluation]], [[keywords/Model Abilities Correlation|Model Abilities Correlation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19880v1 Announce Type: cross 
Abstract: LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet research findings on the relationship between models' generation and judgment abilities remain inconsistent. We investigate this relationship through systematic dataset- and instance-level analyses across 11 models and 21 diverse tasks. Despite both capabilities relying on the same underlying knowledge, our analyses reveal they are only weakly correlated, primarily due to LLMs' sensitivity to the responses being judged. To address this, we propose a self-reference-guided evaluation strategy that leverages a model's own answers as references. This approach significantly strengthens the correlation between generation and judgment abilities, offering a practical path to align these skills and providing a reliable proxy for model selection in evaluation tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ AI í‰ê°€ì—ì„œ ì‚¬ìš©ë˜ëŠ” LLM-as-Judge í”„ë ˆì„ì›Œí¬ì˜ ìƒì„± ëŠ¥ë ¥ê³¼ íŒë‹¨ ëŠ¥ë ¥ ê°„ì˜ ê´€ê³„ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. 11ê°œ ëª¨ë¸ê³¼ 21ê°œ ë‹¤ì–‘í•œ ì‘ì—…ì„ í†µí•´ ë¶„ì„í•œ ê²°ê³¼, ë‘ ëŠ¥ë ¥ì€ ë™ì¼í•œ ì§€ì‹ì— ê¸°ë°˜í•˜ì§€ë§Œ ì•½í•˜ê²Œ ìƒê´€ë˜ì–´ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” LLMì´ í‰ê°€í•˜ëŠ” ì‘ë‹µì— ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ëª¨ë¸ì˜ ìì²´ ì‘ë‹µì„ ì°¸ì¡°ë¡œ ì‚¬ìš©í•˜ëŠ” ìê¸° ì°¸ì¡° ê¸°ë°˜ í‰ê°€ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ìƒì„±ê³¼ íŒë‹¨ ëŠ¥ë ¥ ê°„ì˜ ìƒê´€ì„±ì„ ê°•í™”í•˜ì—¬, í‰ê°€ ì‘ì—…ì—ì„œ ëª¨ë¸ ì„ íƒì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLM-as-Judge í”„ë ˆì„ì›Œí¬ëŠ” AI í‰ê°€ì—ì„œ ì¸ê¸°ë¥¼ ëŒê³  ìˆì§€ë§Œ, ëª¨ë¸ì˜ ìƒì„± ëŠ¥ë ¥ê³¼ íŒë‹¨ ëŠ¥ë ¥ ê°„ì˜ ê´€ê³„ì— ëŒ€í•œ ì—°êµ¬ ê²°ê³¼ëŠ” ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 2. 11ê°œì˜ ëª¨ë¸ê³¼ 21ê°œì˜ ë‹¤ì–‘í•œ ì‘ì—…ì„ í†µí•´ ë°ì´í„°ì…‹ ë° ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ì—ì„œ ì´ ê´€ê³„ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.
- 3. ë¶„ì„ ê²°ê³¼, ë‘ ëŠ¥ë ¥ì´ ë™ì¼í•œ ì§€ì‹ì— ê¸°ë°˜ì„ ë‘ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , LLMì˜ ì‘ë‹µì— ëŒ€í•œ ë¯¼ê°ì„± ë•Œë¬¸ì— ì•½í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ëª¨ë¸ì˜ ìì²´ ë‹µë³€ì„ ì°¸ì¡°ë¡œ í™œìš©í•˜ëŠ” ìê¸° ì°¸ì¡° ê¸°ë°˜ í‰ê°€ ì „ëµì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.
- 5. ì´ ì ‘ê·¼ë²•ì€ ìƒì„± ë° íŒë‹¨ ëŠ¥ë ¥ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ í¬ê²Œ ê°•í™”í•˜ì—¬, í‰ê°€ ì‘ì—…ì—ì„œ ëª¨ë¸ ì„ íƒì„ ìœ„í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:52:26*