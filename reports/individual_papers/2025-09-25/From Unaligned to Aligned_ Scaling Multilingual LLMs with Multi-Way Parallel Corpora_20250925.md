---
keywords:
  - Large Language Model
  - Multi-Way Parallel Data
  - Instruction Tuning
  - Cross-Lingual Semantics
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2505.14045
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:25:00.010993",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multi-Way Parallel Data",
    "Instruction Tuning",
    "Cross-Lingual Semantics"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multi-Way Parallel Data": 0.78,
    "Instruction Tuning": 0.82,
    "Cross-Lingual Semantics": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion on multilingual scaling and are a key concept in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "multi-way parallel data",
        "canonical": "Multi-Way Parallel Data",
        "aliases": [
          "parallel corpora",
          "aligned data"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique concept introduced in the paper that enhances cross-lingual consistency, crucial for multilingual LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "instruction tuning",
        "canonical": "Instruction Tuning",
        "aliases": [
          "instruction-based tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Instruction tuning is a significant method for improving model performance, connecting to broader themes in model training.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "cross-lingual semantics",
        "canonical": "Cross-Lingual Semantics",
        "aliases": [
          "cross-language semantics"
        ],
        "category": "specific_connectable",
        "rationale": "Understanding cross-lingual semantics is vital for multilingual model performance, linking to semantic analysis.",
        "novelty_score": 0.58,
        "connectivity_score": 0.78,
        "specificity_score": 0.76,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "low-resource languages",
      "multilingual benchmarks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "multi-way parallel data",
      "resolved_canonical": "Multi-Way Parallel Data",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "instruction tuning",
      "resolved_canonical": "Instruction Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "cross-lingual semantics",
      "resolved_canonical": "Cross-Lingual Semantics",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.78,
        "specificity": 0.76,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.14045.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2505.14045](https://arxiv.org/abs/2505.14045)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (85.4% similar)
- [[2025-09-23/CUTE_ A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages_20250923|CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages]] (85.3% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (85.3% similar)
- [[2025-09-22/A method for improving multilingual quality and diversity of instruction fine-tuning datasets_20250922|A method for improving multilingual quality and diversity of instruction fine-tuning datasets]] (84.0% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Instruction Tuning|Instruction Tuning]], [[keywords/Cross-Lingual Semantics|Cross-Lingual Semantics]]
**âš¡ Unique Technical**: [[keywords/Multi-Way Parallel Data|Multi-Way Parallel Data]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.14045v3 Announce Type: replace-cross 
Abstract: Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤êµ­ì–´ ëŒ€ê·œëª¨ ë³‘ë ¬ ì½”í¼ìŠ¤ TED2025ë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” 113ê°œ ì–¸ì–´ë¥¼ í¬í•¨í•˜ê³  ìµœëŒ€ 50ê°œ ì–¸ì–´ê°€ ë³‘ë ¬ë¡œ ì •ë ¬ëœ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìµœì ì˜ ë°©ë²•ë¡ ì„ ì—°êµ¬í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¤êµ­ì–´ ë³‘ë ¬ ë°ì´í„°ê°€ ë¹„ì •ë ¬ ë‹¤êµ­ì–´ ë°ì´í„°ë³´ë‹¤ ì¼ê´€ëœ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ì œê³µí•¨ì„ ì‹¤í—˜ì„ í†µí•´ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¤êµ­ì–´ ë³‘ë ¬ ë°ì´í„°ì˜ í™œìš© ì „ëµê³¼ ì£¼ìš” ì˜í–¥ ìš”ì¸ ë¶„ì„ì„ í†µí•´ LLMì˜ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ê°œì„ í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ë‹¤êµ­ì–´ ë°ì´í„°ì˜ ì§€ì†ì ì¸ ì‚¬ì „ í•™ìŠµê³¼ ëª…ë ¹ì–´ íŠœë‹ì€ ì €ìì› ì–¸ì–´ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì´ë‹¤.
- 2. ë‹¤ì¤‘ ë³‘ë ¬ ë°ì´í„°ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ë™ì¼í•œ ì½˜í…ì¸ ê°€ ì •ë ¬ë˜ì–´ ìˆì–´ ê°•ë ¥í•œ êµì°¨ ì–¸ì–´ ì¼ê´€ì„±ì„ ì œê³µí•˜ë©°, ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ê°œì„ í•  ì ì¬ë ¥ì´ í¬ë‹¤.
- 3. TED Talksë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë‹¤ì¤‘ ë³‘ë ¬ ì½”í¼ìŠ¤ TED2025ë¥¼ ì†Œê°œí•˜ë©°, 113ê°œ ì–¸ì–´ë¥¼ í¬í•¨í•˜ê³  ìµœëŒ€ 50ê°œ ì–¸ì–´ê°€ ë³‘ë ¬ë¡œ ì •ë ¬ë˜ì–´ ìˆë‹¤.
- 4. ë‹¤ì¤‘ ë³‘ë ¬ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ LLMì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìµœì„ ì˜ ë°©ë²•ì„ ì—°êµ¬í•˜ë©°, ì§€ì†ì ì¸ ì‚¬ì „ í•™ìŠµ, ëª…ë ¹ì–´ íŠœë‹, ì£¼ìš” ì˜í–¥ ìš”ì¸ ë¶„ì„ì„ í¬í•¨í•œë‹¤.
- 5. ì—¬ì„¯ ê°œì˜ ë‹¤êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ë‹¤ì¤‘ ë³‘ë ¬ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ë¹„ì •ë ¬ ë‹¤êµ­ì–´ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-25 16:25:00*