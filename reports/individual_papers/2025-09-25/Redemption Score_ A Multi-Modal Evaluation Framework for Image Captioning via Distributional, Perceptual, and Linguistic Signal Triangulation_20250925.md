---
keywords:
  - Redemption Score
  - Mutual Information Divergence
  - DINO-based Perceptual Similarity
  - Large Language Model
  - Vision-Language Model
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2505.16180
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:58:29.622578",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Redemption Score",
    "Mutual Information Divergence",
    "DINO-based Perceptual Similarity",
    "Large Language Model",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Redemption Score": 0.7,
    "Mutual Information Divergence": 0.65,
    "DINO-based Perceptual Similarity": 0.68,
    "Large Language Model": 0.75,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Redemption Score",
        "canonical": "Redemption Score",
        "aliases": [
          "RS"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel evaluation framework specific to this research, enhancing understanding of image captioning assessment.",
        "novelty_score": 0.85,
        "connectivity_score": 0.45,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      },
      {
        "surface": "Mutual Information Divergence",
        "canonical": "Mutual Information Divergence",
        "aliases": [
          "MID"
        ],
        "category": "unique_technical",
        "rationale": "A key component of the proposed framework, contributing to the distributional alignment evaluation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.85,
        "link_intent_score": 0.65
      },
      {
        "surface": "DINO-based perceptual similarity",
        "canonical": "DINO-based Perceptual Similarity",
        "aliases": [
          "DINO similarity"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel perceptual similarity measure using DINO, relevant for visual grounding in image captioning.",
        "novelty_score": 0.78,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.68
      },
      {
        "surface": "LLM Text Embeddings",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM embeddings"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader concept of language models, relevant for contextual text similarity.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents the intersection of vision and language, crucial for understanding multimodal evaluation.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "assessment",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Redemption Score",
      "resolved_canonical": "Redemption Score",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.45,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Mutual Information Divergence",
      "resolved_canonical": "Mutual Information Divergence",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.85,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "DINO-based perceptual similarity",
      "resolved_canonical": "DINO-based Perceptual Similarity",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.68
      }
    },
    {
      "candidate_surface": "LLM Text Embeddings",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Redemption Score: A Multi-Modal Evaluation Framework for Image Captioning via Distributional, Perceptual, and Linguistic Signal Triangulation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.16180.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2505.16180](https://arxiv.org/abs/2505.16180)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion_20250924|Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion]] (83.8% similar)
- [[2025-09-23/Describe-to-Score_ Text-Guided Efficient Image Complexity Assessment_20250923|Describe-to-Score: Text-Guided Efficient Image Complexity Assessment]] (83.5% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.7% similar)
- [[2025-09-24/RS3DBench_ A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing_20250924|RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing]] (81.1% similar)
- [[2025-09-22/PCSR_ Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning_20250922|PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Redemption Score|Redemption Score]], [[keywords/Mutual Information Divergence|Mutual Information Divergence]], [[keywords/DINO-based Perceptual Similarity|DINO-based Perceptual Similarity]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.16180v2 Announce Type: replace-cross 
Abstract: Evaluating image captions requires cohesive assessment of both visual semantics and language pragmatics, which is often not entirely captured by most metrics. We introduce Redemption Score(RS), a novel hybrid framework that ranks image captions by triangulating three complementary signals: (1) Mutual Information Divergence (MID) for global image-text distributional alignment, (2) DINO-based perceptual similarity of cycle-generated images for visual grounding, and (3) LLM Text Embeddings for contextual text similarity against human references. A calibrated fusion of these signals allows RS to offer a more holistic assessment. On the Flickr8k benchmark, RS achieves a Kendall-$\tau$ of 58.42, outperforming most prior methods and demonstrating superior correlation with human judgments without requiring task-specific training. Our framework provides a more robust and nuanced evaluation by thoroughly examining both the visual accuracy and text quality together, with consistent performance across Conceptual Captions and MS COCO.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì´ë¯¸ì§€ ìº¡ì…˜ í‰ê°€ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ Redemption Score(RS)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. RSëŠ” ì„¸ ê°€ì§€ ì‹ í˜¸ë¥¼ ê²°í•©í•˜ì—¬ ì´ë¯¸ì§€ ìº¡ì…˜ì„ í‰ê°€í•©ë‹ˆë‹¤: (1) ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì „ë°˜ì ì¸ ë¶„í¬ ì •ë ¬ì„ ìœ„í•œ ìƒí˜¸ ì •ë³´ ë°œì‚°(MID), (2) ì‹œê°ì  ê¸°ë°˜ì„ ìœ„í•œ DINO ê¸°ë°˜ì˜ ì´ë¯¸ì§€ ìœ ì‚¬ì„±, (3) ì¸ê°„ ì°¸ì¡°ì™€ì˜ ë¬¸ë§¥ì  í…ìŠ¤íŠ¸ ìœ ì‚¬ì„±ì„ ìœ„í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) í…ìŠ¤íŠ¸ ì„ë² ë”©. ì´ëŸ¬í•œ ì‹ í˜¸ì˜ ì¡°í™”ë¡œìš´ ìœµí•©ì„ í†µí•´ RSëŠ” ë³´ë‹¤ í¬ê´„ì ì¸ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Flickr8k ë²¤ì¹˜ë§ˆí¬ì—ì„œ RSëŠ” Kendall-$\tau$ 58.42ë¥¼ ê¸°ë¡í•˜ë©°, ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ë°©ë²•ì„ ëŠ¥ê°€í•˜ê³  ì¸ê°„ íŒë‹¨ê³¼ì˜ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” Conceptual Captionsì™€ MS COCOì—ì„œë„ ì¼ê´€ëœ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‹œê°ì  ì •í™•ì„±ê³¼ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ í•¨ê»˜ í‰ê°€í•˜ëŠ” ë° ìˆì–´ ê°•ë ¥í•˜ê³  ì„¸ë°€í•œ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Redemption Score(RS)ëŠ” ì´ë¯¸ì§€ ìº¡ì…˜ í‰ê°€ë¥¼ ìœ„í•´ ì„¸ ê°€ì§€ ì‹ í˜¸ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. RSëŠ” Mutual Information Divergence, DINO ê¸°ë°˜ì˜ ì‹œê°ì  ìœ ì‚¬ì„±, LLM í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 3. Flickr8k ë²¤ì¹˜ë§ˆí¬ì—ì„œ RSëŠ” Kendall-$\tau$ 58.42ë¥¼ ê¸°ë¡í•˜ë©°, ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 4. RSëŠ” ê°œë…ì  ìº¡ì…˜ê³¼ MS COCO ë°ì´í„°ì…‹ì—ì„œë„ ì¼ê´€ëœ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©°, ì‹œê°ì  ì •í™•ì„±ê³¼ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ í•¨ê»˜ í‰ê°€í•©ë‹ˆë‹¤.
- 5. RSëŠ” íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ í›ˆë ¨ ì—†ì´ë„ ì¸ê°„ íŒë‹¨ê³¼ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:58:29*