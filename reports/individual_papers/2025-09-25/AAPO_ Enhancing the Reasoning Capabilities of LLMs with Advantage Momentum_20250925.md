---
keywords:
  - Large Language Model
  - Reinforcement Learning
  - Advantage-Augmented Policy Optimization
  - Group Relative Policy Optimization
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2505.14264
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:07:40.848835",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning",
    "Advantage-Augmented Policy Optimization",
    "Group Relative Policy Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning": 0.8,
    "Advantage-Augmented Policy Optimization": 0.78,
    "Group Relative Policy Optimization": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on enhancing reasoning capabilities, providing a strong link to related works in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a key method used in the paper, connecting it to a broad range of machine learning literature.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Advantage-Augmented Policy Optimization",
        "canonical": "Advantage-Augmented Policy Optimization",
        "aliases": [
          "AAPO"
        ],
        "category": "unique_technical",
        "rationale": "AAPO is a novel algorithm introduced in the paper, providing a unique contribution to the field of RL-based optimization methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "GRPO is discussed as a comparative method, linking the paper to existing RL optimization techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "chain-of-thought",
      "cross-entropy loss"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Advantage-Augmented Policy Optimization",
      "resolved_canonical": "Advantage-Augmented Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.14264.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2505.14264](https://arxiv.org/abs/2505.14264)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning_20250925|Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning]] (90.0% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (89.6% similar)
- [[2025-09-24/NGRPO_ Negative-enhanced Group Relative Policy Optimization_20250924|NGRPO: Negative-enhanced Group Relative Policy Optimization]] (89.4% similar)
- [[2025-09-23/GRPO-LEAD_ A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models_20250923|GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models]] (88.9% similar)
- [[2025-09-25/Stepwise Guided Policy Optimization_ Coloring your Incorrect Reasoning in GRPO_20250925|Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO]] (88.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Advantage-Augmented Policy Optimization|Advantage-Augmented Policy Optimization]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.14264v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL)ì„ í†µí•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°ë… í•™ìŠµ(SFT)ì´ ì œí•œëœ ì‚¬ê³  ê³¼ì • ë°ì´í„°ë¡œ ì¸í•´ í•œê³„ë¥¼ ë³´ì¼ ë•Œ, RL ê¸°ë°˜ì˜ í›„ì† í•™ìŠµ ë°©ë²•ì´ íš¨ê³¼ì ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. íŠ¹íˆ, ê·¸ë£¹ ìƒëŒ€ ì´ì  ì¶”ì • ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” Group Relative Policy Optimization (GRPO)ì´ ê°€ì¹˜ ëª¨ë¸ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì œê±°í•˜ì—¬ ì „í†µì ì¸ ë°©ë²•ë³´ë‹¤ í›ˆë ¨ì„ ë‹¨ìˆœí™”í•˜ì§€ë§Œ, ì´ì  ì¶”ì •ì¹˜ê°€ 0ì— ê°€ê¹Œì›Œì§ˆ ë•Œ í›ˆë ¨ ë¹„íš¨ìœ¨ì„±ì´ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ ë…¼ë¬¸ì€ ëª¨ë©˜í…€ ê¸°ë°˜ ì¶”ì • ë°©ì‹ì„ í†µí•´ ì´ì ì„ ê°•í™”í•˜ì—¬ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ RL ì•Œê³ ë¦¬ì¦˜ì¸ Advantage-Augmented Policy Optimization (AAPO)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, AAPOê°€ ì—¬ëŸ¬ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµ(RL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. Group Relative Policy Optimization (GRPO)ì€ ê°€ì¹˜ ëª¨ë¸ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì œê±°í•˜ì—¬ ì „í†µì ì¸ ë°©ë²•ë³´ë‹¤ í›ˆë ¨ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.
- 3. ê¸°ì¡´ì˜ ê·¸ë£¹ ìƒëŒ€ì  ì´ì  ì¶”ì • ë°©ë²•ì€ í›ˆë ¨ ë¹„íš¨ìœ¨ì„± ë¬¸ì œë¥¼ ê²ªê³  ìˆìœ¼ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Advantage-Augmented Policy Optimization (AAPO)ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. AAPOëŠ” ëª¨ë©˜í…€ ê¸°ë°˜ ì¶”ì • ë°©ì‹ì„ í†µí•´ ì´ì ì„ ê°•í™”í•˜ì—¬ êµì°¨ ì—”íŠ¸ë¡œí”¼(CE) ì†ì‹¤ì„ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ RL ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 5. ì—¬ëŸ¬ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ AAPOì˜ ìš°ìˆ˜í•œ ì„±ëŠ¥ì´ ì‹¤í—˜ ê²°ê³¼ë¡œ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 17:07:40*