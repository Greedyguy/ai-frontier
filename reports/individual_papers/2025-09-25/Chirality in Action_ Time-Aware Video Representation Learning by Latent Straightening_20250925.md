---
keywords:
  - Chiral Action Recognition
  - Time-Sensitive Video Representation
  - Self-supervised Learning
  - Latent Straightening
  - Autoencoder
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.08502
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:27:49.912805",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Chiral Action Recognition",
    "Time-Sensitive Video Representation",
    "Self-supervised Learning",
    "Latent Straightening",
    "Autoencoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Chiral Action Recognition": 0.78,
    "Time-Sensitive Video Representation": 0.8,
    "Self-supervised Learning": 0.82,
    "Latent Straightening": 0.77,
    "Autoencoder": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "chiral action recognition",
        "canonical": "Chiral Action Recognition",
        "aliases": [
          "temporal action distinction"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's contribution and offers a unique perspective on video representation learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "time-sensitive video representation",
        "canonical": "Time-Sensitive Video Representation",
        "aliases": [
          "temporal video embedding"
        ],
        "category": "unique_technical",
        "rationale": "This term captures the paper's focus on improving video embeddings to account for temporal changes.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "self-supervised adaptation",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "self-supervised method"
        ],
        "category": "specific_connectable",
        "rationale": "The paper employs self-supervised techniques, which are relevant for linking with existing research in this area.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "latent straightening",
        "canonical": "Latent Straightening",
        "aliases": [
          "perceptual straightening"
        ],
        "category": "unique_technical",
        "rationale": "This novel approach is a key component of the proposed model and offers a unique contribution to the field.",
        "novelty_score": 0.88,
        "connectivity_score": 0.6,
        "specificity_score": 0.92,
        "link_intent_score": 0.77
      },
      {
        "surface": "auto-encoder",
        "canonical": "Autoencoder",
        "aliases": [
          "autoencoder model"
        ],
        "category": "broad_technical",
        "rationale": "Autoencoders are a fundamental part of the model architecture discussed in the paper.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "video models",
      "large-scale video datasets",
      "classification performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "chiral action recognition",
      "resolved_canonical": "Chiral Action Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "time-sensitive video representation",
      "resolved_canonical": "Time-Sensitive Video Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "self-supervised adaptation",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "latent straightening",
      "resolved_canonical": "Latent Straightening",
      "decision": "linked",
      "scores": {
        "novelty": 0.88,
        "connectivity": 0.6,
        "specificity": 0.92,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "auto-encoder",
      "resolved_canonical": "Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.08502.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.08502](https://arxiv.org/abs/2509.08502)

## 🔗 유사한 논문
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (83.1% similar)
- [[2025-09-23/Stable Video-Driven Portraits_20250923|Stable Video-Driven Portraits]] (82.7% similar)
- [[2025-09-25/From Prompt to Progression_ Taming Video Diffusion Models for Seamless Attribute Transition_20250925|From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition]] (82.4% similar)
- [[2025-09-22/Simulated Cortical Magnification Supports Self-Supervised Object Learning_20250922|Simulated Cortical Magnification Supports Self-Supervised Object Learning]] (82.2% similar)
- [[2025-09-18/LSTC-MDA_ A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition_20250918|LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Autoencoder|Autoencoder]]
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**⚡ Unique Technical**: [[keywords/Chiral Action Recognition|Chiral Action Recognition]], [[keywords/Time-Sensitive Video Representation|Time-Sensitive Video Representation]], [[keywords/Latent Straightening|Latent Straightening]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.08502v2 Announce Type: replace 
Abstract: Our objective is to develop compact video representations that are sensitive to visual change over time. To measure such time-sensitivity, we introduce a new task: chiral action recognition, where one needs to distinguish between a pair of temporally opposite actions, such as "opening vs. closing a door", "approaching vs. moving away from something", "folding vs. unfolding paper", etc. Such actions (i) occur frequently in everyday life, (ii) require understanding of simple visual change over time (in object state, size, spatial position, count . . . ), and (iii) are known to be poorly represented by many video embeddings. Our goal is to build time aware video representations which offer linear separability between these chiral pairs. To that end, we propose a self-supervised adaptation recipe to inject time-sensitivity into a sequence of frozen image features. Our model is based on an auto-encoder with a latent space with inductive bias inspired by perceptual straightening. We show that this results in a compact but time-sensitive video representation for the proposed task across three datasets: Something-Something, EPIC-Kitchens, and Charade. Our method (i) outperforms much larger video models pre-trained on large-scale video datasets, and (ii) leads to an improvement in classification performance on standard benchmarks when combined with these existing models.

## 📝 요약

이 논문은 시간에 따른 시각적 변화를 민감하게 감지할 수 있는 압축된 비디오 표현을 개발하는 것을 목표로 합니다. 이를 위해 "열기 vs. 닫기", "접근하기 vs. 멀어지기"와 같은 시간적으로 반대되는 행동을 구분하는 '키랄 행동 인식'이라는 새로운 과제를 제안합니다. 이러한 행동은 일상에서 자주 발생하며, 시간에 따른 간단한 시각적 변화를 이해해야 하며, 기존의 많은 비디오 임베딩이 이를 잘 표현하지 못합니다. 우리는 이러한 키랄 쌍을 선형적으로 구분할 수 있는 시간 인식 비디오 표현을 구축하고자 합니다. 이를 위해, 고정된 이미지 특징에 시간 민감성을 주입하는 자가 지도 학습 방법을 제안합니다. 제안된 모델은 지각적 직선화를 기반으로 한 오토인코더를 사용하며, Something-Something, EPIC-Kitchens, Charade 세 가지 데이터셋에서 제안된 과제에 대해 시간 민감성이 높은 압축된 비디오 표현을 제공합니다. 우리의 방법은 대규모 비디오 데이터셋으로 사전 학습된 더 큰 비디오 모델을 능가하며, 기존 모델과 결합 시 표준 벤치마크에서 분류 성능이 향상됩니다.

## 🎯 주요 포인트

- 1. 본 연구는 시간에 따른 시각적 변화를 민감하게 반영하는 컴팩트한 비디오 표현을 개발하는 것을 목표로 한다.
- 2. 시간 민감성을 측정하기 위해 "열기 vs. 닫기", "접근하기 vs. 멀어지기"와 같은 시간적으로 반대되는 행동을 구분하는 새로운 과제인 키랄 행동 인식을 도입하였다.
- 3. 제안된 모델은 단순한 시각적 변화를 이해해야 하는 일상적인 행동을 효과적으로 표현하며, 많은 비디오 임베딩이 이러한 행동을 잘 표현하지 못한다는 문제를 해결한다.
- 4. 자기 지도 학습을 통해 고정된 이미지 특징에 시간 민감성을 주입하는 방법을 제안하였으며, 이는 세 가지 데이터셋에서 시간 민감한 비디오 표현을 가능하게 한다.
- 5. 제안된 방법은 대규모 비디오 데이터셋에서 사전 학습된 더 큰 비디오 모델을 능가하며, 기존 모델과 결합 시 표준 벤치마크에서 분류 성능을 향상시킨다.


---

*Generated on 2025-09-26 09:27:49*