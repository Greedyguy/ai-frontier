---
keywords:
  - Hallucination Detection
  - RAGTruth Dataset
  - Out-of-Distribution Generalization
  - Supervised Linear Probes
  - Hyperparameter Tuning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19372
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:32:12.163845",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hallucination Detection",
    "RAGTruth Dataset",
    "Out-of-Distribution Generalization",
    "Supervised Linear Probes",
    "Hyperparameter Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hallucination Detection": 0.78,
    "RAGTruth Dataset": 0.7,
    "Out-of-Distribution Generalization": 0.82,
    "Supervised Linear Probes": 0.72,
    "Hyperparameter Tuning": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "hallucination detection",
        "canonical": "Hallucination Detection",
        "aliases": [
          "hallucination identification",
          "hallucination recognition"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on improving detection methods and evaluation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "RAGTruth dataset",
        "canonical": "RAGTruth Dataset",
        "aliases": [
          "RAGTruth",
          "RAG dataset"
        ],
        "category": "unique_technical",
        "rationale": "Specific dataset used in the study, relevant for linking to data-driven discussions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      },
      {
        "surface": "out-of-distribution generalization",
        "canonical": "Out-of-Distribution Generalization",
        "aliases": [
          "OOD generalization",
          "distribution generalization"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for linking discussions on model robustness and adaptability.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "supervised linear probes",
        "canonical": "Supervised Linear Probes",
        "aliases": [
          "linear probes",
          "supervised probes"
        ],
        "category": "unique_technical",
        "rationale": "Used as a baseline in the study, relevant for linking to model evaluation techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "hyperparameter tuning",
        "canonical": "Hyperparameter Tuning",
        "aliases": [
          "parameter tuning",
          "hyperparameter optimization"
        ],
        "category": "broad_technical",
        "rationale": "Common technique in machine learning, relevant for linking optimization discussions.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "state-of-the-art",
      "spurious correlation",
      "random performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "hallucination detection",
      "resolved_canonical": "Hallucination Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "RAGTruth dataset",
      "resolved_canonical": "RAGTruth Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "out-of-distribution generalization",
      "resolved_canonical": "Out-of-Distribution Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "supervised linear probes",
      "resolved_canonical": "Supervised Linear Probes",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "hyperparameter tuning",
      "resolved_canonical": "Hyperparameter Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19372.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19372](https://arxiv.org/abs/2509.19372)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (80.9% similar)
- [[2025-09-23/GLSim_ Detecting Object Hallucinations in LVLMs via Global-Local Similarity_20250923|GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity]] (80.8% similar)
- [[2025-09-23/Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks_20250923|Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks]] (80.2% similar)
- [[2025-09-23/How Large Language Models are Designed to Hallucinate_20250923|How Large Language Models are Designed to Hallucinate]] (79.4% similar)
- [[2025-09-24/Mitigating Hallucination in Large Vision-Language Models through Aligning Attention Distribution to Information Flow_20250924|Mitigating Hallucination in Large Vision-Language Models through Aligning Attention Distribution to Information Flow]] (79.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Hyperparameter Tuning|Hyperparameter Tuning]]
**ğŸ”— Specific Connectable**: [[keywords/Out-of-Distribution Generalization|Out-of-Distribution Generalization]]
**âš¡ Unique Technical**: [[keywords/Hallucination Detection|Hallucination Detection]], [[keywords/RAGTruth Dataset|RAGTruth Dataset]], [[keywords/Supervised Linear Probes|Supervised Linear Probes]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19372v1 Announce Type: cross 
Abstract: We critically assess the efficacy of the current SOTA in hallucination detection and find that its performance on the RAGTruth dataset is largely driven by a spurious correlation with data. Controlling for this effect, state-of-the-art performs no better than supervised linear probes, while requiring extensive hyperparameter tuning across datasets. Out-of-distribution generalization is currently out of reach, with all of the analyzed methods performing close to random. We propose a set of guidelines for hallucination detection and its evaluation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í˜„ì¬ ìµœì²¨ë‹¨ í™˜ê° íƒì§€ ê¸°ë²•ì˜ íš¨ìœ¨ì„±ì„ í‰ê°€í•˜ë©°, RAGTruth ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ì´ ë°ì´í„°ì™€ì˜ ì˜ëª»ëœ ìƒê´€ê´€ê³„ì— ì˜í•´ ì£¼ë¡œ ì¢Œìš°ëœë‹¤ëŠ” ì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íš¨ê³¼ë¥¼ í†µì œí•˜ë©´, ìµœì²¨ë‹¨ ê¸°ë²•ì€ ì§€ë„ í•™ìŠµëœ ì„ í˜• í”„ë¡œë¸Œë³´ë‹¤ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•Šìœ¼ë©°, ë°ì´í„°ì…‹ ì „ë°˜ì— ê±¸ì³ ê´‘ë²”ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ë˜í•œ, ë¶„ì„ëœ ëª¨ë“  ë°©ë²•ì´ ë¬´ì‘ìœ„ì— ê°€ê¹Œìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ë¶„í¬ ì™¸ ì¼ë°˜í™”ëŠ” í˜„ì¬ ë¶ˆê°€ëŠ¥í•œ ìƒíƒœì…ë‹ˆë‹¤. ì €ìë“¤ì€ í™˜ê° íƒì§€ ë° í‰ê°€ë¥¼ ìœ„í•œ ì§€ì¹¨ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í˜„ì¬ ìµœì²¨ë‹¨ í™˜ê° íƒì§€ ê¸°ìˆ ì˜ íš¨ëŠ¥ì€ ë°ì´í„°ì™€ì˜ ì˜ëª»ëœ ìƒê´€ê´€ê³„ì— í¬ê²Œ ì˜ì¡´í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ì´ëŸ¬í•œ íš¨ê³¼ë¥¼ í†µì œí•˜ë©´, ìµœì²¨ë‹¨ ê¸°ìˆ ì€ ê°ë…ëœ ì„ í˜• íƒì§€ê¸°ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•Šìœ¼ë©°, ë°ì´í„°ì…‹ ì „ë°˜ì— ê±¸ì³ ê´‘ë²”ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.
- 3. ë¶„ì„ëœ ëª¨ë“  ë°©ë²•ì´ ë¬´ì‘ìœ„ì— ê°€ê¹Œìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ë¶„í¬ ë°– ì¼ë°˜í™”ëŠ” í˜„ì¬ ë„ë‹¬í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤.
- 4. ìš°ë¦¬ëŠ” í™˜ê° íƒì§€ ë° ê·¸ í‰ê°€ë¥¼ ìœ„í•œ ì¼ë ¨ì˜ ì§€ì¹¨ì„ ì œì•ˆí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:32:12*