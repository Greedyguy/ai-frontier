---
keywords:
  - Semi-online Reinforcement Learning
  - Graphical User Interface
  - Discounted Future Returns
  - Multi-turn Reasoning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.11543
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:34:46.015124",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Semi-online Reinforcement Learning",
    "Graphical User Interface",
    "Discounted Future Returns",
    "Multi-turn Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Semi-online Reinforcement Learning": 0.78,
    "Graphical User Interface": 0.72,
    "Discounted Future Returns": 0.65,
    "Multi-turn Reasoning": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Semi-online Reinforcement Learning",
        "canonical": "Semi-online Reinforcement Learning",
        "aliases": [
          "Semi-online RL"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel paradigm that bridges offline and online RL, enhancing connectivity with reinforcement learning topics.",
        "novelty_score": 0.85,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Graphical User Interface agents",
        "canonical": "Graphical User Interface",
        "aliases": [
          "GUI agents"
        ],
        "category": "specific_connectable",
        "rationale": "Connects with existing GUI automation and agent-based interaction frameworks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Discounted future returns",
        "canonical": "Discounted Future Returns",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Essential for understanding reward computation in the proposed RL paradigm.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.65
      },
      {
        "surface": "Multi-turn reasoning",
        "canonical": "Multi-turn Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Links to broader discussions on dialogue systems and reasoning tasks in AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "offline RL",
      "online RL",
      "SOTA performance",
      "real-world evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Semi-online Reinforcement Learning",
      "resolved_canonical": "Semi-online Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Graphical User Interface agents",
      "resolved_canonical": "Graphical User Interface",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Discounted future returns",
      "resolved_canonical": "Discounted Future Returns",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Multi-turn reasoning",
      "resolved_canonical": "Multi-turn Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.11543.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.11543](https://arxiv.org/abs/2509.11543)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (87.2% similar)
- [[2025-09-24/MobileRL_ Online Agentic Reinforcement Learning for Mobile GUI Agents_20250924|MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents]] (86.2% similar)
- [[2025-09-23/Mano Report_20250923|Mano Report]] (85.3% similar)
- [[2025-09-18/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250918|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (85.1% similar)
- [[2025-09-24/Online Process Reward Leanring for Agentic Reinforcement Learning_20250924|Online Process Reward Leanring for Agentic Reinforcement Learning]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Graphical User Interface|Graphical User Interface]], [[keywords/Multi-turn Reasoning|Multi-turn Reasoning]]
**âš¡ Unique Technical**: [[keywords/Semi-online Reinforcement Learning|Semi-online Reinforcement Learning]], [[keywords/Discounted Future Returns|Discounted Future Returns]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.11543v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ GUI ì—ì´ì „íŠ¸ì˜ ë³µì¡í•œ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ìƒí˜¸ì‘ìš© ìë™í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ê°•í™” í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì¸ ì„¸ë¯¸-ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì€ ì•ˆì •ì ì¸ í›ˆë ¨ì„ ì œê³µí•˜ì§€ë§Œ ë‹¤ë‹¨ê³„ ì‘ì—… ì‹¤í–‰ì— ì–´ë ¤ì›€ì„ ê²ªê³ , ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì€ í™˜ê²½ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì‹ í˜¸ë¥¼ í¬ì°©í•˜ì§€ë§Œ ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¸ë¯¸-ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì€ ì˜¤í”„ë¼ì¸ ê¶¤ì ì—ì„œ ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì„ ì‹œë®¬ë ˆì´ì…˜í•˜ë©°, ë¡¤ì•„ì›ƒ ê³¼ì •ì—ì„œ íŒ¨ì¹˜ ëª¨ë“ˆì„ í†µí•´ ê¶¤ì  ê°„ì˜ ì°¨ì´ë¥¼ ë³´ì •í•©ë‹ˆë‹¤. ë˜í•œ, ì¥ê¸°ì ì¸ í›ˆë ¨ ì‹ í˜¸ë¥¼ í¬ì°©í•˜ê¸° ìœ„í•´ í• ì¸ëœ ë¯¸ë˜ ìˆ˜ìµì„ ë„ì…í•˜ê³ , ì •ì±… ìµœì í™”ë¥¼ ìœ„í•´ ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ëœ ë‹¨ê³„ë³„ ë° ì—í”¼ì†Œë“œë³„ ì´ì ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ 7B ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì˜¤í”„ë¼ì¸ í›ˆë ¨ íš¨ìœ¨ì„±ê³¼ ì˜¨ë¼ì¸ ë‹¤ì¤‘ í„´ ì¶”ë¡  ê°„ì˜ ê²©ì°¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GUI ì—ì´ì „íŠ¸ì˜ ë³µì¡í•œ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ìƒí˜¸ì‘ìš© ìë™í™”ì—ì„œ ê°•í™” í•™ìŠµì˜ ì§„ì „ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 2. ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì€ ì•ˆì •ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, ë‹¤ë‹¨ê³„ ì‘ì—… ì‹¤í–‰ì—ì„œ ë³´ìƒ ì‹ í˜¸ ë¶€ì¡± ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤.
- 3. ì„¸ë¯¸-ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì€ ì˜¤í”„ë¼ì¸ ê¶¤ì ì—ì„œ ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 4. ì„¸ë¯¸-ì˜¨ë¼ì¸ ì„±ëŠ¥(SOP)ì´ë¼ëŠ” ìƒˆë¡œìš´ ì§€í‘œë¥¼ ë„ì…í•˜ì—¬ ì‹¤ì œ ì˜¨ë¼ì¸ ì„±ëŠ¥ê³¼ ë” ì˜ ë§ì¶”ê³  í‰ê°€ì˜ ì‹¤ìš©ì  ëŒ€ë¦¬ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì„¸ë¯¸-ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµì´ 7B ëª¨ë¸ì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì˜¤í”„ë¼ì¸ í›ˆë ¨ íš¨ìœ¨ì„±ê³¼ ì˜¨ë¼ì¸ ë‹¤ì¤‘ í„´ ì¶”ë¡  ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° í° ì§„ì „ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:34:46*