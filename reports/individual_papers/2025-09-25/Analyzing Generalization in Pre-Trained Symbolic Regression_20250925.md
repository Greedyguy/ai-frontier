---
keywords:
  - Transformer
  - Symbolic Regression
  - Pre-training Data Distribution
  - Out-of-Distribution Generalization
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19849
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:51:03.038500",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Symbolic Regression",
    "Pre-training Data Distribution",
    "Out-of-Distribution Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Symbolic Regression": 0.8,
    "Pre-training Data Distribution": 0.75,
    "Out-of-Distribution Generalization": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based models",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models",
          "Transformer architecture"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational technology in machine learning, linking well with a wide range of topics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Symbolic regression",
        "canonical": "Symbolic Regression",
        "aliases": [
          "Symbolic formula discovery"
        ],
        "category": "unique_technical",
        "rationale": "Symbolic regression is a specialized area of study that connects to mathematical modeling and data fitting.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Pre-training distribution",
        "canonical": "Pre-training Data Distribution",
        "aliases": [
          "Training data distribution"
        ],
        "category": "specific_connectable",
        "rationale": "Understanding data distribution is crucial for evaluating model generalization and performance.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Out-of-distribution challenges",
        "canonical": "Out-of-Distribution Generalization",
        "aliases": [
          "OOD challenges",
          "Generalization beyond training"
        ],
        "category": "specific_connectable",
        "rationale": "Out-of-distribution generalization is a critical aspect of model robustness and applicability.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "generalization",
      "performance",
      "practitioners"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Symbolic regression",
      "resolved_canonical": "Symbolic Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Pre-training distribution",
      "resolved_canonical": "Pre-training Data Distribution",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Out-of-distribution challenges",
      "resolved_canonical": "Out-of-Distribution Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Analyzing Generalization in Pre-Trained Symbolic Regression

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19849.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19849](https://arxiv.org/abs/2509.19849)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Improving Monte Carlo Tree Search for Symbolic Regression_20250922|Improving Monte Carlo Tree Search for Symbolic Regression]] (81.6% similar)
- [[2025-09-23/Evolution of Concepts in Language Model Pre-Training_20250923|Evolution of Concepts in Language Model Pre-Training]] (80.2% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (80.1% similar)
- [[2025-09-24/Is Pre-training Truly Better Than Meta-Learning?_20250924|Is Pre-training Truly Better Than Meta-Learning?]] (79.9% similar)
- [[2025-09-25/Linear Transformers Implicitly Discover Unified Numerical Algorithms_20250925|Linear Transformers Implicitly Discover Unified Numerical Algorithms]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Pre-training Data Distribution|Pre-training Data Distribution]], [[keywords/Out-of-Distribution Generalization|Out-of-Distribution Generalization]]
**âš¡ Unique Technical**: [[keywords/Symbolic Regression|Symbolic Regression]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19849v1 Announce Type: cross 
Abstract: Symbolic regression algorithms search a space of mathematical expressions for formulas that explain given data. Transformer-based models have emerged as a promising, scalable approach shifting the expensive combinatorial search to a large-scale pre-training phase. However, the success of these models is critically dependent on their pre-training data. Their ability to generalize to problems outside of this pre-training distribution remains largely unexplored. In this work, we conduct a systematic empirical study to evaluate the generalization capabilities of pre-trained, transformer-based symbolic regression. We rigorously test performance both within the pre-training distribution and on a series of out-of-distribution challenges for several state of the art approaches. Our findings reveal a significant dichotomy: while pre-trained models perform well in-distribution, the performance consistently degrades in out-of-distribution scenarios. We conclude that this generalization gap is a critical barrier for practitioners, as it severely limits the practical use of pre-trained approaches for real-world applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìˆ˜ì‹ íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë³€í™˜ê¸° ê¸°ë°˜ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ë³€í™˜ê¸° ê¸°ë°˜ ëª¨ë¸ì€ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµì„ í†µí•´ ìˆ˜ì‹ íƒìƒ‰ì„ íš¨ìœ¨í™”í•˜ì§€ë§Œ, ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì‚¬ì „ í•™ìŠµ ë¶„í¬ ë‚´ì—ì„œëŠ” ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ì§€ë§Œ, ë¶„í¬ ì™¸ ë¬¸ì œì—ì„œëŠ” ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê²½í–¥ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ì‘ìš©ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ í™œìš©ì— ì¤‘ìš”í•œ ì œì•½ì´ ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°í˜¸ íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ëŠ” ìˆ˜í•™ì  í‘œí˜„ì‹ì„ íƒìƒ‰í•˜ëŠ” ë°©ë²•ì´ë‹¤.
- 2. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµì„ í†µí•´ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì¡°í•©ì  íƒìƒ‰ì„ ëŒ€ì²´í•˜ëŠ” ìœ ë§í•œ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ë¶€ìƒí–ˆë‹¤.
- 3. ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ê¸°í˜¸ íšŒê·€ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì²´ê³„ì ì¸ ì‹¤ì¦ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆë‹¤.
- 4. ì—°êµ¬ ê²°ê³¼, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ ì‚¬ì „ í•™ìŠµ ë¶„í¬ ë‚´ì—ì„œëŠ” ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ì§€ë§Œ, ë¶„í¬ ì™¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ì„±ëŠ¥ì´ ì¼ê´€ë˜ê²Œ ì €í•˜ë˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.
- 5. ì´ëŸ¬í•œ ì¼ë°˜í™” ê²©ì°¨ëŠ” ì‹¤ì œ ì‘ìš©ì—ì„œ ì‚¬ì „ í•™ìŠµ ì ‘ê·¼ ë°©ì‹ì˜ ì‹¤ìš©ì„±ì„ ì‹¬ê°í•˜ê²Œ ì œí•œí•˜ëŠ” ì¤‘ìš”í•œ ì¥ì• ë¬¼ë¡œ ê²°ë¡ ì§€ì—ˆë‹¤.


---

*Generated on 2025-09-25 15:51:03*