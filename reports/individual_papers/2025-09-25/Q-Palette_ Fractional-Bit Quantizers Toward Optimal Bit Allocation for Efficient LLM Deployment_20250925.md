---
keywords:
  - Large Language Model
  - Post-Training Quantization
  - Fractional-Bit Quantizers
  - Gaussian Distortion-Rate Bound
  - CUDA Kernels
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20214
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:02:42.263993",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Post-Training Quantization",
    "Fractional-Bit Quantizers",
    "Gaussian Distortion-Rate Bound",
    "CUDA Kernels"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Post-Training Quantization": 0.7,
    "Fractional-Bit Quantizers": 0.78,
    "Gaussian Distortion-Rate Bound": 0.72,
    "CUDA Kernels": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on quantization techniques for efficient deployment.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Post-Training Quantization",
        "canonical": "Post-Training Quantization",
        "aliases": [
          "PTQ"
        ],
        "category": "unique_technical",
        "rationale": "A specific technique discussed extensively in the paper for optimizing LLM deployment.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Fractional-Bit Quantizers",
        "canonical": "Fractional-Bit Quantizers",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to quantization that is central to the paper's contributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Gaussian Distortion-Rate Bound",
        "canonical": "Gaussian Distortion-Rate Bound",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key theoretical concept that underpins the quantization strategy proposed in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "CUDA Kernels",
        "canonical": "CUDA Kernels",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Relevant for implementation and optimization of the proposed quantization methods.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Post-Training Quantization",
      "resolved_canonical": "Post-Training Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Fractional-Bit Quantizers",
      "resolved_canonical": "Fractional-Bit Quantizers",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Gaussian Distortion-Rate Bound",
      "resolved_canonical": "Gaussian Distortion-Rate Bound",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "CUDA Kernels",
      "resolved_canonical": "CUDA Kernels",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20214.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20214](https://arxiv.org/abs/2509.20214)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (88.4% similar)
- [[2025-09-24/SBVR_ Summation of BitVector Representation for Efficient LLM Quantization_20250924|SBVR: Summation of BitVector Representation for Efficient LLM Quantization]] (87.1% similar)
- [[2025-09-23/QWHA_ Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models_20250923|QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models]] (87.0% similar)
- [[2025-09-23/GuidedQuant_ Large Language Model Quantization via Exploiting End Loss Guidance_20250923|GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance]] (86.3% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (86.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/CUDA Kernels|CUDA Kernels]]
**âš¡ Unique Technical**: [[keywords/Post-Training Quantization|Post-Training Quantization]], [[keywords/Fractional-Bit Quantizers|Fractional-Bit Quantizers]], [[keywords/Gaussian Distortion-Rate Bound|Gaussian Distortion-Rate Bound]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20214v1 Announce Type: cross 
Abstract: We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¬í•™ìŠµ ì—†ì´ ì–‘ìí™”í•˜ëŠ” 'ê°€ì¤‘ì¹˜ ì „ìš© ì‚¬í›„ í›ˆë ¨ ì–‘ìí™”(PTQ)'ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ê°œì¸í™”ëœ ì¶”ë¡ ê³¼ ê°™ì€ ë©”ëª¨ë¦¬ ì œì•½ ìƒí™©ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤. LLMì˜ ë¶ˆê·œì¹™í•œ ê°€ì¤‘ì¹˜ ë¶„í¬ëŠ” ì–‘ìí™”ë¥¼ ë³µì¡í•˜ê²Œ ë§Œë“¤ë©°, ìµœê·¼ì—ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ê±°ì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ë³€í™˜í•˜ëŠ” íšŒì „ ê¸°ë°˜ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê°€ìš°ì‹œì•ˆí™”ëœ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ì •ë³´ ì´ë¡ ì ìœ¼ë¡œ ìµœì ì˜ ë¹„íŠ¸ í• ë‹¹ì„ ë„ì¶œí•˜ê³ , ì´ë¥¼ í†µí•´ ê·¼ì ‘ ìµœì ì˜ ì–‘ìí™” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë¯¸ì„¸í•œ ë¶„ìˆ˜ ë¹„íŠ¸ ì–‘ìí™”ê¸°ê°€ í•„ìš”í•¨ì„ ë°í˜”ìŠµë‹ˆë‹¤. ì´ë¥¼ ì‹¤ìš©ì ìœ¼ë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ ë¹„íŠ¸í­ì— ìµœì í™”ëœ CUDA ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” 'Q-Palette'ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, Q-Paletteë¥¼ í™œìš©í•˜ì—¬ ìì› ì œì•½ì„ ê³ ë ¤í•œ í˜¼í•© ìŠ¤í‚´ ì–‘ìí™” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì½”ë“œì™€ ê´€ë ¨ ìë£ŒëŠ” GitHubì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì¬í›ˆë ¨ ì—†ì´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•˜ëŠ” weight-only post-training quantization (PTQ)ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤.
- 2. LLMì˜ ë¶ˆê·œì¹™í•œ ê°€ì¤‘ì¹˜ ë¶„í¬ì™€ heavy-tailed outliersëŠ” ì–‘ìí™”ë¥¼ ë³µì¡í•˜ê²Œ í•˜ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íšŒì „ ê¸°ë°˜ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. Gaussianized weightsì— ëŒ€í•œ ì •ë³´ ì´ë¡ ì ìœ¼ë¡œ ìµœì ì˜ ë¹„íŠ¸ í• ë‹¹ì„ ë„ì¶œí•˜ì—¬, ì„¸ë¶„í™”ëœ fractional-bit quantizerê°€ ê±°ì˜ ìµœì ì˜ ì–‘ìí™” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° í•„ìˆ˜ì ì„ì„ ë°í˜”ìŠµë‹ˆë‹¤.
- 4. Q-PaletteëŠ” ë‹¤ì–‘í•œ ë¹„íŠ¸ ë„ˆë¹„ì— ê±¸ì³ ìµœì í™”ëœ CUDA ì»¤ë„ë¡œ êµ¬í˜„ëœ trellis-coded quantizerë¶€í„° ê°„ë‹¨í•œ ë²¡í„° ë° ìŠ¤ì¹¼ë¼ ì–‘ìí™”ê¸°ê¹Œì§€ í¬í•¨í•˜ëŠ” ë²”ìš©ì ì¸ fractional-bit quantizer ëª¨ìŒì…ë‹ˆë‹¤.
- 5. Q-Paletteë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ìì› ì œì•½ì„ ê³ ë ¤í•˜ì—¬ ì–‘ìí™”ê¸° ì„ íƒê³¼ ë ˆì´ì–´ ìœµí•© ê²°ì •ì„ ê³µë™ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ í˜¼í•© ìŠ¤í‚´ ì–‘ìí™” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:02:42*