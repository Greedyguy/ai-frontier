---
keywords:
  - Large Language Model
  - BERTopic
  - Transformer
  - Hierarchical Clustering
  - Topic Diversity
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19365
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:51:21.681977",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "BERTopic",
    "Transformer",
    "Hierarchical Clustering",
    "Topic Diversity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "BERTopic": 0.78,
    "Transformer": 0.8,
    "Hierarchical Clustering": 0.77,
    "Topic Diversity": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the proposed framework, enhancing topic reduction capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "BERTopic",
        "canonical": "BERTopic",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "BERTopic is a specific method used for topic generation, crucial for understanding the framework's approach.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer Embeddings",
        "canonical": "Transformer",
        "aliases": [
          "Transformer Models"
        ],
        "category": "broad_technical",
        "rationale": "Transformer embeddings are foundational to the BERTopic framework, linking to broader NLP techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Hierarchical Clustering",
        "canonical": "Hierarchical Clustering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Hierarchical clustering is a key technique in the topic extraction process, facilitating connections with clustering methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Topic Diversity",
        "canonical": "Topic Diversity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Topic diversity is an outcome measure of the framework, important for evaluating topic modeling effectiveness.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "BERTopic",
      "resolved_canonical": "BERTopic",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer Embeddings",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Hierarchical Clustering",
      "resolved_canonical": "Hierarchical Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Topic Diversity",
      "resolved_canonical": "Topic Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# LLM-Assisted Topic Reduction for BERTopic on Social Media Data

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19365.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19365](https://arxiv.org/abs/2509.19365)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming_20250923|Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming]] (82.0% similar)
- [[2025-09-19/Position_ Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models_20250919|Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models]] (81.0% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (80.9% similar)
- [[2025-09-24/Multi-Hierarchical Feature Detection for Large Language Model Generated Text_20250924|Multi-Hierarchical Feature Detection for Large Language Model Generated Text]] (80.7% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Hierarchical Clustering|Hierarchical Clustering]]
**âš¡ Unique Technical**: [[keywords/BERTopic|BERTopic]], [[keywords/Topic Diversity|Topic Diversity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19365v1 Announce Type: cross 
Abstract: The BERTopic framework leverages transformer embeddings and hierarchical clustering to extract latent topics from unstructured text corpora. While effective, it often struggles with social media data, which tends to be noisy and sparse, resulting in an excessive number of overlapping topics. Recent work explored the use of large language models for end-to-end topic modelling. However, these approaches typically require significant computational overhead, limiting their scalability in big data contexts. In this work, we propose a framework that combines BERTopic for topic generation with large language models for topic reduction. The method first generates an initial set of topics and constructs a representation for each. These representations are then provided as input to the language model, which iteratively identifies and merges semantically similar topics. We evaluate the approach across three Twitter/X datasets and four different language models. Our method outperforms the baseline approach in enhancing topic diversity and, in many cases, coherence, with some sensitivity to dataset characteristics and initial parameter selection.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ BERTopicê³¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„°ì˜ ì£¼ì œ ëª¨ë¸ë§ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ BERTopicì€ ì†Œì…œ ë¯¸ë””ì–´ì˜ ì¡ìŒê³¼ í¬ì†Œì„±ìœ¼ë¡œ ì¸í•´ ì¤‘ë³µëœ ì£¼ì œë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ BERTopicìœ¼ë¡œ ì´ˆê¸° ì£¼ì œë¥¼ ìƒì„±í•˜ê³ , ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬í•œ ì£¼ì œë¥¼ í†µí•©í•˜ì—¬ ì£¼ì œ ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤. ì„¸ ê°œì˜ Twitter/X ë°ì´í„°ì…‹ê³¼ ë„¤ ê°€ì§€ ì–¸ì–´ ëª¨ë¸ì„ í†µí•´ í‰ê°€í•œ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì£¼ì œ ë‹¤ì–‘ì„±ê³¼ ì¼ê´€ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. BERTopic í”„ë ˆì„ì›Œí¬ëŠ” ë³€í™˜ê¸° ì„ë² ë”©ê³¼ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì„ í™œìš©í•˜ì—¬ ë¹„êµ¬ì¡°ì  í…ìŠ¤íŠ¸ì—ì„œ ì ì¬ ì£¼ì œë¥¼ ì¶”ì¶œí•˜ì§€ë§Œ, ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„°ì—ì„œëŠ” ì¤‘ë³µëœ ì£¼ì œê°€ ë§ì•„ì§€ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
- 2. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¢…ë‹¨ ê°„ ì£¼ì œ ëª¨ë¸ë§ì€ ë†’ì€ ê³„ì‚° ë¹„ìš©ì´ í•„ìš”í•˜ì—¬ ëŒ€ê·œëª¨ ë°ì´í„° í™˜ê²½ì—ì„œ í™•ì¥ì„±ì´ ì œí•œëœë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” BERTopicì„ ì‚¬ìš©í•œ ì£¼ì œ ìƒì„±ê³¼ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í†µí•œ ì£¼ì œ ì¶•ì†Œë¥¼ ê²°í•©í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ì´ˆê¸° ì£¼ì œ ì§‘í•©ì„ ìƒì„±í•˜ê³  ê° ì£¼ì œì˜ í‘œí˜„ì„ êµ¬ì¶•í•œ í›„, ì–¸ì–´ ëª¨ë¸ì„ í†µí•´ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì£¼ì œë¥¼ ì‹ë³„í•˜ê³  ë³‘í•©í•œë‹¤.
- 5. ì„¸ ê°œì˜ Twitter/X ë°ì´í„°ì…‹ê³¼ ë„¤ ê°€ì§€ ì–¸ì–´ ëª¨ë¸ì„ í†µí•´ í‰ê°€í•œ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì£¼ì œ ë‹¤ì–‘ì„±ê³¼ ì¼ê´€ì„±ì„ ê°œì„ í•˜ëŠ” ë° ìˆì–´ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-25 16:51:21*