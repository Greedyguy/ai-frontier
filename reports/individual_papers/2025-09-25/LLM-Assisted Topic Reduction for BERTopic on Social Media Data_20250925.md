---
keywords:
  - Large Language Model
  - BERTopic
  - Transformer
  - Hierarchical Clustering
  - Topic Diversity
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19365
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:51:21.681977",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "BERTopic",
    "Transformer",
    "Hierarchical Clustering",
    "Topic Diversity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "BERTopic": 0.78,
    "Transformer": 0.8,
    "Hierarchical Clustering": 0.77,
    "Topic Diversity": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the proposed framework, enhancing topic reduction capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "BERTopic",
        "canonical": "BERTopic",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "BERTopic is a specific method used for topic generation, crucial for understanding the framework's approach.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer Embeddings",
        "canonical": "Transformer",
        "aliases": [
          "Transformer Models"
        ],
        "category": "broad_technical",
        "rationale": "Transformer embeddings are foundational to the BERTopic framework, linking to broader NLP techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Hierarchical Clustering",
        "canonical": "Hierarchical Clustering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Hierarchical clustering is a key technique in the topic extraction process, facilitating connections with clustering methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Topic Diversity",
        "canonical": "Topic Diversity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Topic diversity is an outcome measure of the framework, important for evaluating topic modeling effectiveness.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "BERTopic",
      "resolved_canonical": "BERTopic",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer Embeddings",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Hierarchical Clustering",
      "resolved_canonical": "Hierarchical Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Topic Diversity",
      "resolved_canonical": "Topic Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# LLM-Assisted Topic Reduction for BERTopic on Social Media Data

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19365.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19365](https://arxiv.org/abs/2509.19365)

## 🔗 유사한 논문
- [[2025-09-23/Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming_20250923|Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming]] (82.0% similar)
- [[2025-09-19/Position_ Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models_20250919|Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models]] (81.0% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (80.9% similar)
- [[2025-09-24/Multi-Hierarchical Feature Detection for Large Language Model Generated Text_20250924|Multi-Hierarchical Feature Detection for Large Language Model Generated Text]] (80.7% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (80.7% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Hierarchical Clustering|Hierarchical Clustering]]
**⚡ Unique Technical**: [[keywords/BERTopic|BERTopic]], [[keywords/Topic Diversity|Topic Diversity]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19365v1 Announce Type: cross 
Abstract: The BERTopic framework leverages transformer embeddings and hierarchical clustering to extract latent topics from unstructured text corpora. While effective, it often struggles with social media data, which tends to be noisy and sparse, resulting in an excessive number of overlapping topics. Recent work explored the use of large language models for end-to-end topic modelling. However, these approaches typically require significant computational overhead, limiting their scalability in big data contexts. In this work, we propose a framework that combines BERTopic for topic generation with large language models for topic reduction. The method first generates an initial set of topics and constructs a representation for each. These representations are then provided as input to the language model, which iteratively identifies and merges semantically similar topics. We evaluate the approach across three Twitter/X datasets and four different language models. Our method outperforms the baseline approach in enhancing topic diversity and, in many cases, coherence, with some sensitivity to dataset characteristics and initial parameter selection.

## 📝 요약

이 논문은 BERTopic과 대형 언어 모델을 결합하여 소셜 미디어 데이터의 주제 모델링을 개선하는 방법을 제안합니다. 기존 BERTopic은 소셜 미디어의 잡음과 희소성으로 인해 중복된 주제를 생성하는 문제가 있었습니다. 제안된 방법은 BERTopic으로 초기 주제를 생성하고, 대형 언어 모델을 통해 유사한 주제를 통합하여 주제 수를 줄입니다. 세 개의 Twitter/X 데이터셋과 네 가지 언어 모델을 통해 평가한 결과, 제안된 방법은 주제 다양성과 일관성을 향상시키는 데 효과적임을 보였습니다.

## 🎯 주요 포인트

- 1. BERTopic 프레임워크는 변환기 임베딩과 계층적 클러스터링을 활용하여 비구조적 텍스트에서 잠재 주제를 추출하지만, 소셜 미디어 데이터에서는 중복된 주제가 많아지는 문제가 있다.
- 2. 대규모 언어 모델을 사용한 종단 간 주제 모델링은 높은 계산 비용이 필요하여 대규모 데이터 환경에서 확장성이 제한된다.
- 3. 본 연구에서는 BERTopic을 사용한 주제 생성과 대규모 언어 모델을 통한 주제 축소를 결합한 프레임워크를 제안한다.
- 4. 제안된 방법은 초기 주제 집합을 생성하고 각 주제의 표현을 구축한 후, 언어 모델을 통해 의미적으로 유사한 주제를 식별하고 병합한다.
- 5. 세 개의 Twitter/X 데이터셋과 네 가지 언어 모델을 통해 평가한 결과, 제안된 방법은 주제 다양성과 일관성을 개선하는 데 있어 기존 방법보다 우수한 성능을 보였다.


---

*Generated on 2025-09-25 16:51:21*