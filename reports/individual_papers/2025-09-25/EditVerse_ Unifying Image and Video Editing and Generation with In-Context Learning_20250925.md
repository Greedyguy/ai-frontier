---
keywords:
  - EditVerse
  - In-Context Learning
  - Attention Mechanism
  - Multimodal Learning
  - EditVerseBench
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.20360
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:17:05.189157",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "EditVerse",
    "In-Context Learning",
    "Attention Mechanism",
    "Multimodal Learning",
    "EditVerseBench"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "EditVerse": 0.8,
    "In-Context Learning": 0.78,
    "Attention Mechanism": 0.85,
    "Multimodal Learning": 0.82,
    "EditVerseBench": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "EditVerse",
        "canonical": "EditVerse",
        "aliases": [
          "EditVerse Framework"
        ],
        "category": "unique_technical",
        "rationale": "EditVerse is a novel framework that unifies image and video editing and generation, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "in-context learning",
        "canonical": "In-Context Learning",
        "aliases": [
          "contextual learning"
        ],
        "category": "specific_connectable",
        "rationale": "In-Context Learning is critical for understanding the model's ability to adapt to new tasks without explicit retraining.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "self-attention mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Self-attention is a key component of the model architecture, enabling cross-modal knowledge transfer.",
        "novelty_score": 0.6,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "cross-modal knowledge transfer",
        "canonical": "Multimodal Learning",
        "aliases": [
          "cross-modal transfer"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-modal knowledge transfer is essential for the framework's ability to handle diverse inputs and outputs.",
        "novelty_score": 0.65,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "video editing benchmark",
        "canonical": "EditVerseBench",
        "aliases": [
          "video editing benchmark"
        ],
        "category": "unique_technical",
        "rationale": "EditVerseBench is a unique benchmark for evaluating instruction-based video editing capabilities.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "foundation models",
      "scalable data pipeline",
      "state-of-the-art performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "EditVerse",
      "resolved_canonical": "EditVerse",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "in-context learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "cross-modal knowledge transfer",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "video editing benchmark",
      "resolved_canonical": "EditVerseBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20360.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.20360](https://arxiv.org/abs/2509.20360)

## 🔗 유사한 논문
- [[2025-09-18/EdiVal-Agent_ An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing_20250918|EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing]] (83.7% similar)
- [[2025-09-23/In-Context Edit_ Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer_20250923|In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer]] (82.3% similar)
- [[2025-09-23/VidCLearn_ A Continual Learning Approach for Text-to-Video Generation_20250923|VidCLearn: A Continual Learning Approach for Text-to-Video Generation]] (82.2% similar)
- [[2025-09-23/FOCUS_ Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation_20250923|FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation]] (82.0% similar)
- [[2025-09-25/Video models are zero-shot learners and reasoners_20250925|Video models are zero-shot learners and reasoners]] (81.2% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/In-Context Learning|In-Context Learning]], [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/EditVerse|EditVerse]], [[keywords/EditVerseBench|EditVerseBench]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20360v1 Announce Type: new 
Abstract: Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.

## 📝 요약

최근 기초 모델의 발전은 다양한 분야에서 통합과 확장의 경향을 보이고 있습니다. 그러나 비디오 생성 및 편집은 여전히 단편화되어 있습니다. 본 연구에서는 이미지와 비디오 생성 및 편집을 하나의 모델로 통합한 EditVerse를 소개합니다. EditVerse는 텍스트, 이미지, 비디오를 통합된 토큰 시퀀스로 표현하여 강력한 학습과 자연스러운 지식 전이를 가능하게 합니다. 또한, 비디오 편집 데이터 부족 문제를 해결하기 위해 232,000개의 비디오 편집 샘플을 포함한 대규모 데이터 파이프라인을 설계했습니다. EditVerseBench라는 최초의 비디오 편집 벤치마크를 통해 다양한 작업과 해상도를 다루며, 실험 결과 EditVerse는 기존 모델들을 뛰어넘는 성능을 보여주었습니다.

## 🎯 주요 포인트

- 1. EditVerse는 이미지와 비디오 생성 및 편집을 하나의 모델로 통합한 프레임워크입니다.
- 2. 텍스트, 이미지, 비디오를 통합된 토큰 시퀀스로 표현하여 강력한 맥락 내 학습과 자연스러운 교차 모달 지식 전이를 달성합니다.
- 3. 비디오 편집 훈련 데이터 부족 문제를 해결하기 위해 232K 비디오 편집 샘플을 포함하는 확장 가능한 데이터 파이프라인을 설계했습니다.
- 4. EditVerseBench는 다양한 작업과 해상도를 다루는 최초의 지시 기반 비디오 편집 벤치마크입니다.
- 5. EditVerse는 기존의 오픈 소스 및 상업용 모델을 능가하는 성능을 보여주며, 모달리티 전반에 걸쳐 새로운 편집 및 생성 능력을 발휘합니다.


---

*Generated on 2025-09-26 09:17:05*