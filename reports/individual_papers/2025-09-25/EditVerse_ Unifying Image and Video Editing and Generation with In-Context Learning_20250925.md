---
keywords:
  - EditVerse
  - In-Context Learning
  - Attention Mechanism
  - Multimodal Learning
  - EditVerseBench
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.20360
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:17:05.189157",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "EditVerse",
    "In-Context Learning",
    "Attention Mechanism",
    "Multimodal Learning",
    "EditVerseBench"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "EditVerse": 0.8,
    "In-Context Learning": 0.78,
    "Attention Mechanism": 0.85,
    "Multimodal Learning": 0.82,
    "EditVerseBench": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "EditVerse",
        "canonical": "EditVerse",
        "aliases": [
          "EditVerse Framework"
        ],
        "category": "unique_technical",
        "rationale": "EditVerse is a novel framework that unifies image and video editing and generation, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "in-context learning",
        "canonical": "In-Context Learning",
        "aliases": [
          "contextual learning"
        ],
        "category": "specific_connectable",
        "rationale": "In-Context Learning is critical for understanding the model's ability to adapt to new tasks without explicit retraining.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "self-attention mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Self-attention is a key component of the model architecture, enabling cross-modal knowledge transfer.",
        "novelty_score": 0.6,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "cross-modal knowledge transfer",
        "canonical": "Multimodal Learning",
        "aliases": [
          "cross-modal transfer"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-modal knowledge transfer is essential for the framework's ability to handle diverse inputs and outputs.",
        "novelty_score": 0.65,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "video editing benchmark",
        "canonical": "EditVerseBench",
        "aliases": [
          "video editing benchmark"
        ],
        "category": "unique_technical",
        "rationale": "EditVerseBench is a unique benchmark for evaluating instruction-based video editing capabilities.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "foundation models",
      "scalable data pipeline",
      "state-of-the-art performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "EditVerse",
      "resolved_canonical": "EditVerse",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "in-context learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "cross-modal knowledge transfer",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "video editing benchmark",
      "resolved_canonical": "EditVerseBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20360.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.20360](https://arxiv.org/abs/2509.20360)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/EdiVal-Agent_ An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing_20250918|EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing]] (83.7% similar)
- [[2025-09-23/In-Context Edit_ Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer_20250923|In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer]] (82.3% similar)
- [[2025-09-23/VidCLearn_ A Continual Learning Approach for Text-to-Video Generation_20250923|VidCLearn: A Continual Learning Approach for Text-to-Video Generation]] (82.2% similar)
- [[2025-09-23/FOCUS_ Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation_20250923|FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation]] (82.0% similar)
- [[2025-09-25/Video models are zero-shot learners and reasoners_20250925|Video models are zero-shot learners and reasoners]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/In-Context Learning|In-Context Learning]], [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/EditVerse|EditVerse]], [[keywords/EditVerseBench|EditVerseBench]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20360v1 Announce Type: new 
Abstract: Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.

## ğŸ“ ìš”ì•½

ìµœê·¼ ê¸°ì´ˆ ëª¨ë¸ì˜ ë°œì „ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í†µí•©ê³¼ í™•ì¥ì˜ ê²½í–¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¹„ë””ì˜¤ ìƒì„± ë° í¸ì§‘ì€ ì—¬ì „íˆ ë‹¨í¸í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ìƒì„± ë° í¸ì§‘ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í†µí•©í•œ EditVerseë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. EditVerseëŠ” í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤ë¥¼ í†µí•©ëœ í† í° ì‹œí€€ìŠ¤ë¡œ í‘œí˜„í•˜ì—¬ ê°•ë ¥í•œ í•™ìŠµê³¼ ìì—°ìŠ¤ëŸ¬ìš´ ì§€ì‹ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ë¹„ë””ì˜¤ í¸ì§‘ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 232,000ê°œì˜ ë¹„ë””ì˜¤ í¸ì§‘ ìƒ˜í”Œì„ í¬í•¨í•œ ëŒ€ê·œëª¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. EditVerseBenchë¼ëŠ” ìµœì´ˆì˜ ë¹„ë””ì˜¤ í¸ì§‘ ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í•´ìƒë„ë¥¼ ë‹¤ë£¨ë©°, ì‹¤í—˜ ê²°ê³¼ EditVerseëŠ” ê¸°ì¡´ ëª¨ë¸ë“¤ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. EditVerseëŠ” ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ìƒì„± ë° í¸ì§‘ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í†µí•©í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤ë¥¼ í†µí•©ëœ í† í° ì‹œí€€ìŠ¤ë¡œ í‘œí˜„í•˜ì—¬ ê°•ë ¥í•œ ë§¥ë½ ë‚´ í•™ìŠµê³¼ ìì—°ìŠ¤ëŸ¬ìš´ êµì°¨ ëª¨ë‹¬ ì§€ì‹ ì „ì´ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 3. ë¹„ë””ì˜¤ í¸ì§‘ í›ˆë ¨ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 232K ë¹„ë””ì˜¤ í¸ì§‘ ìƒ˜í”Œì„ í¬í•¨í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.
- 4. EditVerseBenchëŠ” ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í•´ìƒë„ë¥¼ ë‹¤ë£¨ëŠ” ìµœì´ˆì˜ ì§€ì‹œ ê¸°ë°˜ ë¹„ë””ì˜¤ í¸ì§‘ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- 5. EditVerseëŠ” ê¸°ì¡´ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ë° ìƒì—…ìš© ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ëª¨ë‹¬ë¦¬í‹° ì „ë°˜ì— ê±¸ì³ ìƒˆë¡œìš´ í¸ì§‘ ë° ìƒì„± ëŠ¥ë ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:17:05*