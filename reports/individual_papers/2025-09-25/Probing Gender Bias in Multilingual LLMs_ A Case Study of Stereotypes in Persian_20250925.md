---
keywords:
  - Large Language Model
  - Gender Bias
  - Low-Resource Languages
  - Domain-Specific Gender Skew Index
  - Inclusive NLP Practices
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.20168
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:48:36.762430",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Gender Bias",
    "Low-Resource Languages",
    "Domain-Specific Gender Skew Index",
    "Inclusive NLP Practices"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Gender Bias": 0.9,
    "Low-Resource Languages": 0.8,
    "Domain-Specific Gender Skew Index": 0.75,
    "Inclusive NLP Practices": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multilingual Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "Multilingual LLMs"
        ],
        "category": "broad_technical",
        "rationale": "This term connects to the broader concept of LLMs, which is central to the study of bias in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "gender bias",
        "canonical": "Gender Bias",
        "aliases": [
          "gender stereotypes"
        ],
        "category": "specific_connectable",
        "rationale": "Understanding and mitigating gender bias is crucial for ethical AI, making it a key concept for linking.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "low-resource languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "understudied languages"
        ],
        "category": "unique_technical",
        "rationale": "This highlights a significant gap in AI research, offering a unique angle for further exploration.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Domain-Specific Gender Skew Index",
        "canonical": "Domain-Specific Gender Skew Index",
        "aliases": [
          "DS-GSI"
        ],
        "category": "unique_technical",
        "rationale": "This new metric provides a specific tool for measuring gender bias, enhancing the study's depth.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "inclusive NLP practices",
        "canonical": "Inclusive NLP Practices",
        "aliases": [
          "inclusive natural language processing"
        ],
        "category": "evolved_concepts",
        "rationale": "Promoting inclusivity in NLP is a growing trend, linking to ethical considerations in AI.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "stereotypes",
      "semantic domains",
      "real-world data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multilingual Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "gender bias",
      "resolved_canonical": "Gender Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "low-resource languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Domain-Specific Gender Skew Index",
      "resolved_canonical": "Domain-Specific Gender Skew Index",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "inclusive NLP practices",
      "resolved_canonical": "Inclusive NLP Practices",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20168.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.20168](https://arxiv.org/abs/2509.20168)

## 🔗 유사한 논문
- [[2025-09-23/EuroGEST_ Investigating gender stereotypes in multilingual language models_20250923|EuroGEST: Investigating gender stereotypes in multilingual language models]] (87.8% similar)
- [[2025-09-25/Blind Men and the Elephant_ Diverse Perspectives on Gender Stereotypes in Benchmark Datasets_20250925|Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets]] (86.4% similar)
- [[2025-09-23/Auto-Search and Refinement_ An Automated Framework for Gender Bias Mitigation in Large Language Models_20250923|Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models]] (85.0% similar)
- [[2025-09-23/Justice in Judgment_ Unveiling (Hidden) Bias in LLM-assisted Peer Reviews_20250923|Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews]] (84.7% similar)
- [[2025-09-23/Intrinsic Meets Extrinsic Fairness_ Assessing the Downstream Impact of Bias Mitigation in Large Language Models_20250923|Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models]] (84.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Gender Bias|Gender Bias]]
**⚡ Unique Technical**: [[keywords/Low-Resource Languages|Low-Resource Languages]], [[keywords/Domain-Specific Gender Skew Index|Domain-Specific Gender Skew Index]]
**🚀 Evolved Concepts**: [[keywords/Inclusive NLP Practices|Inclusive NLP Practices]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20168v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) are increasingly used worldwide, making it essential to ensure they are free from gender bias to prevent representational harm. While prior studies have examined such biases in high-resource languages, low-resource languages remain understudied. In this paper, we propose a template-based probing methodology, validated against real-world data, to uncover gender stereotypes in LLMs. As part of this framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a metric that quantifies deviations from gender parity. We evaluate four prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B, across four semantic domains, focusing on Persian, a low-resource language with distinct linguistic features. Our results show that all models exhibit gender stereotypes, with greater disparities in Persian than in English across all domains. Among these, sports reflect the most rigid gender biases. This study underscores the need for inclusive NLP practices and provides a framework for assessing bias in other low-resource languages.

## 📝 요약

이 논문은 다국어 대형 언어 모델(LLM)에서 성별 편향을 탐구하며, 특히 저자원 언어에 대한 연구가 부족한 점을 지적합니다. 저자들은 템플릿 기반의 탐색 방법론을 제안하고, 성별 불균형을 정량화하는 DS-GSI 지표를 도입하여 이를 검증했습니다. 페르시아어를 중심으로 GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, Qwen QwQ 32B 모델을 평가한 결과, 모든 모델이 성별 편향을 보였으며, 특히 스포츠 분야에서 가장 강한 편향이 나타났습니다. 이 연구는 포괄적인 자연어 처리(NLP) 관행의 필요성을 강조하며, 다른 저자원 언어의 편향 평가를 위한 틀을 제공합니다.

## 🎯 주요 포인트

- 1. 다국어 대형 언어 모델(LLM)에서 성별 편향을 제거하는 것이 중요하며, 특히 저자원 언어에서의 연구가 필요하다.
- 2. 본 논문은 성별 고정관념을 발견하기 위한 템플릿 기반 탐색 방법론을 제안하며, 실제 데이터와의 검증을 통해 이를 입증한다.
- 3. 도메인별 성별 편향 지수(DS-GSI)를 도입하여 성별 불균형을 정량화하는 새로운 지표를 제시한다.
- 4. 페르시아어를 포함한 저자원 언어에서 네 가지 주요 모델을 평가한 결과, 모든 모델이 성별 고정관념을 나타내며, 특히 스포츠 분야에서 가장 큰 편향이 발견되었다.
- 5. 본 연구는 포괄적인 자연어 처리(NLP) 관행의 필요성을 강조하며, 다른 저자원 언어에서의 편향 평가를 위한 프레임워크를 제공한다.


---

*Generated on 2025-09-26 08:48:36*