---
keywords:
  - Vision-Language Model
  - Human Demonstration Videos
  - Task Planning
  - Pick-and-Place Tasks
  - Simulation Environment
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2410.08792
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:17:23.544693",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Human Demonstration Videos",
    "Task Planning",
    "Pick-and-Place Tasks",
    "Simulation Environment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Human Demonstration Videos": 0.72,
    "Task Planning": 0.8,
    "Pick-and-Place Tasks": 0.7,
    "Simulation Environment": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision Language Model"
        ],
        "category": "evolved_concepts",
        "rationale": "Links to the concept of integrating vision and language for robotics, a trending area of research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.67,
        "link_intent_score": 0.85
      },
      {
        "surface": "human demonstration videos",
        "canonical": "Human Demonstration Videos",
        "aliases": [
          "demonstration videos",
          "human demos"
        ],
        "category": "unique_technical",
        "rationale": "Specific to the method of using videos for robot task planning, which is novel in this context.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "task planning",
        "canonical": "Task Planning",
        "aliases": [
          "robot task planning"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's contribution in translating human actions to robot tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "pick-and-place tasks",
        "canonical": "Pick-and-Place Tasks",
        "aliases": [
          "pick and place"
        ],
        "category": "unique_technical",
        "rationale": "A specific type of task used for benchmarking in robotics, relevant for linking to task-specific research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "simulation environment",
        "canonical": "Simulation Environment",
        "aliases": [
          "simulated environment"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the testing and deployment context of the robot tasks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.77,
        "specificity_score": 0.6,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.67,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "human demonstration videos",
      "resolved_canonical": "Human Demonstration Videos",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "task planning",
      "resolved_canonical": "Task Planning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "pick-and-place tasks",
      "resolved_canonical": "Pick-and-Place Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "simulation environment",
      "resolved_canonical": "Simulation Environment",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.77,
        "specificity": 0.6,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2410.08792.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2410.08792](https://arxiv.org/abs/2410.08792)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/KRAST_ Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models_20250923|KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models]] (83.2% similar)
- [[2025-09-18/FSR-VLN_ Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph_20250918|FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph]] (83.1% similar)
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (82.9% similar)
- [[2025-09-24/Reading Images Like Texts_ Sequential Image Understanding in Vision-Language Models_20250924|Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models]] (82.9% similar)
- [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Task Planning|Task Planning]], [[keywords/Simulation Environment|Simulation Environment]]
**âš¡ Unique Technical**: [[keywords/Human Demonstration Videos|Human Demonstration Videos]], [[keywords/Pick-and-Place Tasks|Pick-and-Place Tasks]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.08792v2 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) have recently been adopted in robotics for their capability in common sense reasoning and generalizability. Existing work has applied VLMs to generate task and motion planning from natural language instructions and simulate training data for robot learning. In this work, we explore using VLM to interpret human demonstration videos and generate robot task planning. Our method integrates keyframe selection, visual perception, and VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to ''see'' human demonstrations and explain the corresponding plans to the robot for it to ''do''. To validate our approach, we collected a set of long-horizon human videos demonstrating pick-and-place tasks in three diverse categories and designed a set of metrics to comprehensively benchmark SeeDo against several baselines, including state-of-the-art video-input VLMs. The experiments demonstrate SeeDo's superior performance. We further deployed the generated task plans in both a simulation environment and on a real robot arm.

## ğŸ“ ìš”ì•½

Vision Language Models(VLMs)ì€ ë¡œë´‡ ê³µí•™ì—ì„œ ì¼ë°˜í™”ì™€ ìƒì‹ì  ì¶”ë¡  ëŠ¥ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” VLMì„ í™œìš©í•˜ì—¬ ì¸ê°„ ì‹œë²” ì˜ìƒì„ í•´ì„í•˜ê³  ë¡œë´‡ ì‘ì—… ê³„íšì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. 'SeeDo'ë¼ ëª…ëª…ëœ ì´ ë°©ë²•ì€ í‚¤í”„ë ˆì„ ì„ íƒ, ì‹œê°ì  ì¸ì‹, VLM ì¶”ë¡ ì„ í†µí•©í•˜ì—¬ ë¡œë´‡ì´ ì¸ê°„ ì‹œë²”ì„ ë³´ê³  ì´í•´í•œ ê³„íšì„ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë²”ì£¼ì˜ ë¬¼ì²´ ì§‘ê¸° ë° ë†“ê¸° ì‘ì—…ì„ ì‹œì—°í•œ ì¸ê°„ ì˜ìƒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , SeeDoì˜ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SeeDoëŠ” ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ VLMì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ìƒì„±ëœ ì‘ì—… ê³„íšì„ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ê³¼ ì‹¤ì œ ë¡œë´‡ íŒ”ì— ì„±ê³µì ìœ¼ë¡œ ì ìš©í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision Language Models(VLMs)ëŠ” ë¡œë´‡ ê³µí•™ì—ì„œ ìƒì‹ì  ì¶”ë¡ ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ìœ¼ë¡œ ì±„íƒë˜ê³  ìˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” VLMì„ í™œìš©í•˜ì—¬ ì¸ê°„ ì‹œë²” ì˜ìƒì„ í•´ì„í•˜ê³  ë¡œë´‡ ì‘ì—… ê³„íšì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•œë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ í‚¤í”„ë ˆì„ ì„ íƒ, ì‹œê°ì  ì¸ì‹, VLM ì¶”ë¡ ì„ í†µí•©í•˜ì—¬ 'SeeDo'ë¼ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•œë‹¤.
- 4. SeeDoëŠ” ì¸ê°„ ì‹œë²”ì„ 'ë³´ê³ ' í•´ë‹¹ ê³„íšì„ ë¡œë´‡ì—ê²Œ 'ì„¤ëª…'í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, SeeDoëŠ” ìµœì²¨ë‹¨ ë¹„ë””ì˜¤ ì…ë ¥ VLMì„ í¬í•¨í•œ ì—¬ëŸ¬ ê¸°ì¤€ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-25 16:17:23*