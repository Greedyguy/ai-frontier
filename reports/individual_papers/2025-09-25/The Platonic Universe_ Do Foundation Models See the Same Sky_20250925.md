---
keywords:
  - Transformer
  - Self-supervised Learning
  - Astronomy-specific Architectures
  - Representational Convergence
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19453
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:54:15.317812",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Self-supervised Learning",
    "Astronomy-specific Architectures",
    "Representational Convergence"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.8,
    "Self-supervised Learning": 0.82,
    "Astronomy-specific Architectures": 0.75,
    "Representational Convergence": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "vision transformers",
        "canonical": "Transformer",
        "aliases": [
          "Vision Transformer",
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational architecture in machine learning, crucial for linking various model types.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "self-supervised models",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "self-supervised"
        ],
        "category": "specific_connectable",
        "rationale": "Self-supervised learning is a key method for training models without labeled data, relevant across domains.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "astronomy-specific architectures",
        "canonical": "Astronomy-specific Architectures",
        "aliases": [
          "astro architectures"
        ],
        "category": "unique_technical",
        "rationale": "These architectures are tailored for astronomical data, providing unique insights into domain-specific model design.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "representational convergence",
        "canonical": "Representational Convergence",
        "aliases": [
          "convergence of representations"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to understanding how different models align in their representations, crucial for cross-model analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "JWST",
      "HSC",
      "Legacy Survey",
      "DESI"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "vision transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "self-supervised models",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "astronomy-specific architectures",
      "resolved_canonical": "Astronomy-specific Architectures",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "representational convergence",
      "resolved_canonical": "Representational Convergence",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# The Platonic Universe: Do Foundation Models See the Same Sky?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19453.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19453](https://arxiv.org/abs/2509.19453)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Comparing Computational Pathology Foundation Models using Representational Similarity Analysis_20250922|Comparing Computational Pathology Foundation Models using Representational Similarity Analysis]] (80.6% similar)
- [[2025-09-17/Towards a Physics Foundation Model_20250917|Towards a Physics Foundation Model]] (79.3% similar)
- [[2025-09-23/From Prediction to Understanding_ Will AI Foundation Models Transform Brain Science?_20250923|From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?]] (78.9% similar)
- [[2025-09-23/$\boldsymbol{\lambda}$-Orthogonality Regularization for Compatible Representation Learning_20250923|$\boldsymbol{\lambda}$-Orthogonality Regularization for Compatible Representation Learning]] (78.3% similar)
- [[2025-09-24/The Transparent Earth_ A Multimodal Foundation Model for the Earth's Subsurface_20250924|The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface]] (77.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Astronomy-specific Architectures|Astronomy-specific Architectures]], [[keywords/Representational Convergence|Representational Convergence]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19453v1 Announce Type: cross 
Abstract: We test the Platonic Representation Hypothesis (PRH) in astronomy by measuring representational convergence across a range of foundation models trained on different data types. Using spectroscopic and imaging observations from JWST, HSC, Legacy Survey, and DESI, we compare representations from vision transformers, self-supervised models, and astronomy-specific architectures via mutual $k$-nearest neighbour analysis. We observe consistent scaling: representational alignment generally increases with model capacity across our tested architectures, supporting convergence toward a shared representation of galaxy astrophysics. Our results suggest that astronomical foundation models can use pre-trained general-purpose architectures, allowing us to capitalise on the broader machine learning community's already-spent computational investment.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì²œë¬¸í•™ì—ì„œ í”Œë¼í†¤ì  í‘œí˜„ ê°€ì„¤(PRH)ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ìœ¼ë¡œ í›ˆë ¨ëœ ê¸°ì´ˆ ëª¨ë¸ ê°„ì˜ í‘œí˜„ ìˆ˜ë ´ì„ ì¸¡ì •í•©ë‹ˆë‹¤. JWST, HSC, Legacy Survey, DESIì˜ ë¶„ê´‘ ë° ì´ë¯¸ì§€ ê´€ì¸¡ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸, ìê°€ ì§€ë„ í•™ìŠµ ëª¨ë¸, ì²œë¬¸í•™ ì „ìš© ì•„í‚¤í…ì²˜ì˜ í‘œí˜„ì„ ìƒí˜¸ $k$-ìµœê·¼ì ‘ ì´ì›ƒ ë¶„ì„ì„ í†µí•´ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ëª¨ë¸ ìš©ëŸ‰ì´ ì¦ê°€í• ìˆ˜ë¡ í‘œí˜„ ì •ë ¬ì´ ì¼ë°˜ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ì€í•˜ ì²œì²´ë¬¼ë¦¬í•™ì˜ ê³µí†µ í‘œí˜„ìœ¼ë¡œ ìˆ˜ë ´í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì²œë¬¸í•™ ê¸°ì´ˆ ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ëœ ë²”ìš© ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•˜ë©°, ê¸°ê³„ í•™ìŠµ ì»¤ë®¤ë‹ˆí‹°ì˜ ê¸°ì¡´ ì»´í“¨íŒ… íˆ¬ìë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í”Œë¼í†¤ì  í‘œí˜„ ê°€ì„¤(PRH)ì„ ì²œë¬¸í•™ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ìœ¼ë¡œ í›ˆë ¨ëœ ê¸°ì´ˆ ëª¨ë¸ ê°„ì˜ í‘œí˜„ ìˆ˜ë ´ì„ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.
- 2. JWST, HSC, Legacy Survey, DESIì˜ ë¶„ê´‘ ë° ì´ë¯¸ì§€ ê´€ì¸¡ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸, ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸, ì²œë¬¸í•™ ì „ìš© ì•„í‚¤í…ì²˜ì˜ í‘œí˜„ì„ ìƒí˜¸ $k$-ìµœê·¼ì ‘ ì´ì›ƒ ë¶„ì„ì„ í†µí•´ ë¹„êµí–ˆìŠµë‹ˆë‹¤.
- 3. ëª¨ë¸ ìš©ëŸ‰ì´ ì¦ê°€í•¨ì— ë”°ë¼ í‘œí˜„ ì •ë ¬ì´ ì¼ë°˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì¼ê´€ëœ ìŠ¤ì¼€ì¼ë§ì„ ê´€ì°°í–ˆìœ¼ë©°, ì´ëŠ” ì€í•˜ ì²œì²´ë¬¼ë¦¬í•™ì˜ ê³µìœ  í‘œí˜„ìœ¼ë¡œì˜ ìˆ˜ë ´ì„ ì§€ì§€í•©ë‹ˆë‹¤.
- 4. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ì²œë¬¸í•™ ê¸°ì´ˆ ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ëœ ë²”ìš© ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•˜ë©°, ì´ë¥¼ í†µí•´ ê¸°ê³„ í•™ìŠµ ì»¤ë®¤ë‹ˆí‹°ì˜ ì´ë¯¸ íˆ¬ìëœ ê³„ì‚° ìì›ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:54:15*