---
keywords:
  - Large Language Model
  - Knowledge Infusion Scaling Law
  - Memory Collapse
  - Domain Knowledge
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19371
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:31:55.368538",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Knowledge Infusion Scaling Law",
    "Memory Collapse",
    "Domain Knowledge"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Knowledge Infusion Scaling Law": 0.8,
    "Memory Collapse": 0.78,
    "Domain Knowledge": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "This is a central concept in the paper, linking to many discussions on language model architectures and capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "knowledge infusion scaling law",
        "canonical": "Knowledge Infusion Scaling Law",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel concept introduced by the paper, crucial for understanding the proposed methodology.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "memory collapse",
        "canonical": "Memory Collapse",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term describes a specific failure mode in LLMs, which is key to the paper's analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "domain knowledge",
        "canonical": "Domain Knowledge",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Understanding how domain-specific data impacts LLMs is crucial for linking to broader discussions on knowledge representation.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "pretraining",
      "downstream performance",
      "catastrophic forgetting"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "knowledge infusion scaling law",
      "resolved_canonical": "Knowledge Infusion Scaling Law",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "memory collapse",
      "resolved_canonical": "Memory Collapse",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "domain knowledge",
      "resolved_canonical": "Domain Knowledge",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19371.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19371](https://arxiv.org/abs/2509.19371)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Experience Scaling_ Post-Deployment Evolution For Large Language Models_20250924|Experience Scaling: Post-Deployment Evolution For Large Language Models]] (85.3% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (85.2% similar)
- [[2025-09-25/Cognitive Load Limits in Large Language Models_ Benchmarking Multi-Hop Reasoning_20250925|Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning]] (84.1% similar)
- [[2025-09-23/Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels_20250923|Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels]] (83.8% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (83.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Domain Knowledge|Domain Knowledge]]
**âš¡ Unique Technical**: [[keywords/Knowledge Infusion Scaling Law|Knowledge Infusion Scaling Law]], [[keywords/Memory Collapse|Memory Collapse]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19371v1 Announce Type: cross 
Abstract: Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, íŠ¹ì • ë¶„ì•¼ì˜ ì§€ì‹ì´ ë¶€ì¡±í•  ê²½ìš° ì „ë¬¸ ì§€ì‹ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ê³  í™˜ê°ì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì „ëµì ìœ¼ë¡œ ë¶„ì•¼ ì§€ì‹ì„ ì£¼ì…í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ì ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³¼ë„í•œ ì§€ì‹ ì£¼ì…ì€ ê¸°ì¡´ ì§€ì‹ì˜ ë§ê°ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê³¼ë„í•œ ì£¼ì…ìœ¼ë¡œ ì¸í•œ ê¸°ì–µ ë¶•ê´´ í˜„ìƒì— ì£¼ëª©í•˜ì—¬, 1) ê° ëª¨ë¸ì—ëŠ” ì§€ì‹ ìœ ì§€ ëŠ¥ë ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ” ì„ê³„ì ì´ ì¡´ì¬í•˜ë©°, 2) ì´ëŸ¬í•œ ë¶•ê´´ì ì€ ëª¨ë¸ í¬ê¸°ì™€ ë¹„ë¡€í•œë‹¤ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°œê²¬ì„ í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‘ì€ ëª¨ë¸ì„ ë¶„ì„í•˜ì—¬ ëŒ€í˜• LLMì— ìµœì ì˜ ë¶„ì•¼ ì§€ì‹ ì£¼ì…ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ì§€ì‹ ì£¼ì… ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì™€ í† í° ì˜ˆì‚°ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´ ì´ ë²•ì¹™ì˜ íš¨ê³¼ì„±ê³¼ ì¼ë°˜ì„±ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, íŠ¹ì • ë¶„ì•¼ì˜ ìµœì í™”ê°€ ì—†ìœ¼ë©´ ì „ë¬¸ ì§€ì‹ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤.
- 2. ë„ë©”ì¸ ì§€ì‹ì„ ì „ëµì ìœ¼ë¡œ ì£¼ì…í•˜ë©´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ê³¼ë„í•œ ì£¼ì…ì€ ê¸°ì¡´ ì§€ì‹ì˜ ë§ê°ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
- 3. ë©”ëª¨ë¦¬ ë¶•ê´´ í˜„ìƒì€ ë„ë©”ì¸ ì§€ì‹ì˜ ê³¼ë„í•œ ì£¼ì…ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ë©°, ëª¨ë¸ì˜ ì§€ì‹ ìœ ì§€ ëŠ¥ë ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ” ì„ê³„ ë¶•ê´´ì ì´ ì¡´ì¬í•œë‹¤.
- 4. ì„ê³„ ë¶•ê´´ì ì€ ëª¨ë¸ì˜ í¬ê¸°ì™€ ì¼ê´€ë˜ê²Œ ë¹„ë¡€í•˜ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ë„ë©”ì¸ ì§€ì‹ ì£¼ì…ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ì§€ì‹ ì£¼ì… ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì œì•ˆí•œë‹¤.
- 5. ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì™€ í† í° ì˜ˆì‚°ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì˜ íš¨ê³¼ì„±ê³¼ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ì…ì¦í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-25 15:31:55*