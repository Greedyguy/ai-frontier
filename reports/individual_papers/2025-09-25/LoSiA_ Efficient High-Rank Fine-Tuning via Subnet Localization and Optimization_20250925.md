---
keywords:
  - Parameter-Efficient Fine-Tuning
  - Low-Rank Adaptation
  - Gradient Sparsity Analysis
  - Domain Specialization
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2507.04487
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:30:11.143550",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Parameter-Efficient Fine-Tuning",
    "Low-Rank Adaptation",
    "Gradient Sparsity Analysis",
    "Domain Specialization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Parameter-Efficient Fine-Tuning": 0.78,
    "Low-Rank Adaptation": 0.82,
    "Gradient Sparsity Analysis": 0.77,
    "Domain Specialization": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Parameter-Efficient Fine-Tuning",
        "canonical": "Parameter-Efficient Fine-Tuning",
        "aliases": [
          "PEFT"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's innovation and distinguishes it from traditional fine-tuning methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Low-Rank Adaptation",
        "canonical": "Low-Rank Adaptation",
        "aliases": [
          "LoRA"
        ],
        "category": "specific_connectable",
        "rationale": "LoRA is a well-known method in the context of efficient fine-tuning, providing strong connectivity to related works.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Gradient Sparsity Analysis",
        "canonical": "Gradient Sparsity Analysis",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is a novel approach within the paper for optimizing sub-networks, enhancing its specificity.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Domain Specialization",
        "canonical": "Domain Specialization",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Domain specialization is a common application area for fine-tuning methods, linking it to broader technical discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "training process"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Parameter-Efficient Fine-Tuning",
      "resolved_canonical": "Parameter-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Low-Rank Adaptation",
      "resolved_canonical": "Low-Rank Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Gradient Sparsity Analysis",
      "resolved_canonical": "Gradient Sparsity Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Domain Specialization",
      "resolved_canonical": "Domain Specialization",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.04487.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2507.04487](https://arxiv.org/abs/2507.04487)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Localized LoRA_ A Structured Low-Rank Approximation for Efficient Fine-Tuning_20250925|Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning]] (89.9% similar)
- [[2025-09-23/Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA_20250923|Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA]] (87.7% similar)
- [[2025-09-23/TASO_ Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation_20250923|TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation]] (86.6% similar)
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (86.2% similar)
- [[2025-09-23/RefLoRA_ Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models_20250923|RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models]] (85.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Domain Specialization|Domain Specialization]]
**ğŸ”— Specific Connectable**: [[keywords/Low-Rank Adaptation|Low-Rank Adaptation]]
**âš¡ Unique Technical**: [[keywords/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]], [[keywords/Gradient Sparsity Analysis|Gradient Sparsity Analysis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.04487v4 Announce Type: replace-cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training. The source code is available at https://github.com/KlozeWang/LoSiA.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì˜ ë¹„íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ LoSiAë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ PEFT ë°©ë²•ì€ í–‰ë ¬ ê³±ì…ˆì´ ë§ì•„ ë¹„íš¨ìœ¨ì ì´ì—ˆìœ¼ë‚˜, LoSiAëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ìµœì í™”í•˜ì—¬ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ê¸°ìš¸ê¸° í¬ì†Œì„± ë¶„ì„ì„ í†µí•´ ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‹ë³„í•˜ê³  ì´ë¥¼ ìµœì í™” ëŒ€ìƒìœ¼ë¡œ ì‚¼ì•„ ê³ ì°¨ì› ì ì‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. LoSiA-ProëŠ” LoSiAì˜ ë¹ ë¥¸ êµ¬í˜„ ë²„ì „ìœ¼ë¡œ, LoRA ëŒ€ë¹„ í›ˆë ¨ ì‹œê°„ì„ ì•½ 27% ë‹¨ì¶•í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, LoSiAëŠ” ì „ì²´ ë¯¸ì„¸ ì¡°ì • ëŒ€ë¹„ ì„±ëŠ¥ ì €í•˜ê°€ ê±°ì˜ ì—†ìœ¼ë©°, ë„ë©”ì¸ íŠ¹í™” ë° ìƒì‹ ì¶”ë¡  ì‘ì—…ì—ì„œ ê°€ì¥ ì ì€ í›ˆë ¨ ì‹œê°„ì„ ìš”êµ¬í•©ë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì—ì„œëŠ” LoSiAê°€ ì§€ì†ì ì¸ í›ˆë ¨ ì¤‘ ë§ê°ì„ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LoSiAëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ì§€ì—­í™”í•˜ê³  ìµœì í™”í•˜ì—¬ ê³ íš¨ìœ¨ì˜ íŒŒë¼ë¯¸í„° ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 2. LoSiA-ProëŠ” LoRAì— ë¹„í•´ ì•½ 27%ì˜ í›ˆë ¨ ì§€ì—° ì‹œê°„ì„ ì¤„ì—¬ì£¼ëŠ” ë¹ ë¥¸ êµ¬í˜„ ë°©ë²•ì…ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ë„ë©”ì¸ íŠ¹í™” ë° ìƒì‹ ì¶”ë¡  ì‘ì—…ì—ì„œ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ê³¼ ë¹„êµí•˜ì—¬ ìµœì†Œí•œì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì…ë‹ˆë‹¤.
- 4. LoSiAëŠ” í›ˆë ¨ ì¤‘ ë§ê°ì„ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.
- 5. ì†ŒìŠ¤ ì½”ë“œëŠ” https://github.com/KlozeWang/LoSiAì—ì„œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:30:11*