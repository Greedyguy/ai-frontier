---
keywords:
  - Responsible AI
  - AI Risk Taxonomy
  - AI Governance
  - SafetyGuard
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20057
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:57:51.245378",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Responsible AI",
    "AI Risk Taxonomy",
    "AI Governance",
    "SafetyGuard"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Responsible AI": 0.78,
    "AI Risk Taxonomy": 0.7,
    "AI Governance": 0.72,
    "SafetyGuard": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Responsible AI",
        "canonical": "Responsible AI",
        "aliases": [
          "RAI"
        ],
        "category": "unique_technical",
        "rationale": "Responsible AI is a unique and emerging concept that focuses on ethical and safe AI practices, which is central to the paper's theme.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "AI risk taxonomy",
        "canonical": "AI Risk Taxonomy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The AI risk taxonomy is a specific framework mentioned in the paper that helps in assessing AI safety and robustness.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "AI governance",
        "canonical": "AI Governance",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "AI governance is a critical area for linking discussions on regulatory compliance and ethical AI practices.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Guardrail : SafetyGuard",
        "canonical": "SafetyGuard",
        "aliases": [
          "Guardrail"
        ],
        "category": "unique_technical",
        "rationale": "SafetyGuard is a proprietary tool introduced in the paper, highlighting its importance in real-time AI safety.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.9,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "assessment methodology",
      "risk mitigation technologies",
      "regulatory compliance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Responsible AI",
      "resolved_canonical": "Responsible AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "AI risk taxonomy",
      "resolved_canonical": "AI Risk Taxonomy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "AI governance",
      "resolved_canonical": "AI Governance",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Guardrail : SafetyGuard",
      "resolved_canonical": "SafetyGuard",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.9,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Responsible AI Technical Report

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20057.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20057](https://arxiv.org/abs/2509.20057)

## 🔗 유사한 논문
- [[2025-09-22/Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents_20250922|Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents]] (85.0% similar)
- [[2025-09-24/An Artificial Intelligence Value at Risk Approach_ Metrics and Models_20250924|An Artificial Intelligence Value at Risk Approach: Metrics and Models]] (83.8% similar)
- [[2025-09-18/TAI Scan Tool_ A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment_20250918|TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment]] (83.7% similar)
- [[2025-09-24/Perceptions of AI Across Sectors_ A Comparative Review of Public Attitudes_20250924|Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes]] (81.7% similar)
- [[2025-09-23/A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents_20250923|A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents]] (81.6% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/AI Governance|AI Governance]]
**⚡ Unique Technical**: [[keywords/Responsible AI|Responsible AI]], [[keywords/AI Risk Taxonomy|AI Risk Taxonomy]], [[keywords/SafetyGuard|SafetyGuard]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20057v1 Announce Type: cross 
Abstract: KT developed a Responsible AI (RAI) assessment methodology and risk mitigation technologies to ensure the safety and reliability of AI services. By analyzing the Basic Act on AI implementation and global AI governance trends, we established a unique approach for regulatory compliance and systematically identify and manage all potential risk factors from AI development to operation. We present a reliable assessment methodology that systematically verifies model safety and robustness based on KT's AI risk taxonomy tailored to the domestic environment. We also provide practical tools for managing and mitigating identified AI risks. With the release of this report, we also release proprietary Guardrail : SafetyGuard that blocks harmful responses from AI models in real-time, supporting the enhancement of safety in the domestic AI development ecosystem. We also believe these research outcomes provide valuable insights for organizations seeking to develop Responsible AI.

## 📝 요약

KT는 AI 서비스의 안전성과 신뢰성을 보장하기 위해 책임 있는 AI(RAI) 평가 방법론과 위험 완화 기술을 개발했습니다. AI 기본법 시행과 글로벌 AI 거버넌스 동향을 분석하여 규제 준수를 위한 독창적인 접근 방식을 수립하고, AI 개발부터 운영까지의 모든 잠재적 위험 요소를 체계적으로 식별하고 관리합니다. KT의 AI 위험 분류 체계를 기반으로 모델의 안전성과 견고성을 검증하는 신뢰할 수 있는 평가 방법론을 제시하며, 식별된 AI 위험을 관리하고 완화하기 위한 실용적인 도구도 제공합니다. 이 보고서와 함께 AI 모델의 유해한 응답을 실시간으로 차단하는 SafetyGuard라는 독점적인 도구를 출시하여 국내 AI 개발 생태계의 안전성을 강화합니다. 이러한 연구 결과는 책임 있는 AI를 개발하려는 조직에 유용한 통찰력을 제공합니다.

## 🎯 주요 포인트

- 1. KT는 AI 서비스의 안전성과 신뢰성을 보장하기 위해 책임 있는 AI 평가 방법론과 위험 완화 기술을 개발했습니다.
- 2. AI 개발부터 운영까지의 모든 잠재적 위험 요소를 체계적으로 식별하고 관리하기 위한 독창적인 규제 준수 접근 방식을 확립했습니다.
- 3. KT의 AI 위험 분류를 기반으로 모델의 안전성과 견고성을 체계적으로 검증하는 신뢰할 수 있는 평가 방법론을 제시합니다.
- 4. 실시간으로 AI 모델의 유해한 응답을 차단하는 SafetyGuard를 출시하여 국내 AI 개발 생태계의 안전성을 강화합니다.
- 5. 이 연구 결과는 책임 있는 AI를 개발하려는 조직에 귀중한 통찰력을 제공합니다.


---

*Generated on 2025-09-25 15:57:51*