---
keywords:
  - Responsible AI
  - AI Risk Taxonomy
  - AI Governance
  - SafetyGuard
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20057
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:57:51.245378",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Responsible AI",
    "AI Risk Taxonomy",
    "AI Governance",
    "SafetyGuard"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Responsible AI": 0.78,
    "AI Risk Taxonomy": 0.7,
    "AI Governance": 0.72,
    "SafetyGuard": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Responsible AI",
        "canonical": "Responsible AI",
        "aliases": [
          "RAI"
        ],
        "category": "unique_technical",
        "rationale": "Responsible AI is a unique and emerging concept that focuses on ethical and safe AI practices, which is central to the paper's theme.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "AI risk taxonomy",
        "canonical": "AI Risk Taxonomy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The AI risk taxonomy is a specific framework mentioned in the paper that helps in assessing AI safety and robustness.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "AI governance",
        "canonical": "AI Governance",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "AI governance is a critical area for linking discussions on regulatory compliance and ethical AI practices.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Guardrail : SafetyGuard",
        "canonical": "SafetyGuard",
        "aliases": [
          "Guardrail"
        ],
        "category": "unique_technical",
        "rationale": "SafetyGuard is a proprietary tool introduced in the paper, highlighting its importance in real-time AI safety.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.9,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "assessment methodology",
      "risk mitigation technologies",
      "regulatory compliance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Responsible AI",
      "resolved_canonical": "Responsible AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "AI risk taxonomy",
      "resolved_canonical": "AI Risk Taxonomy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "AI governance",
      "resolved_canonical": "AI Governance",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Guardrail : SafetyGuard",
      "resolved_canonical": "SafetyGuard",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.9,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Responsible AI Technical Report

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20057.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20057](https://arxiv.org/abs/2509.20057)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents_20250922|Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents]] (85.0% similar)
- [[2025-09-24/An Artificial Intelligence Value at Risk Approach_ Metrics and Models_20250924|An Artificial Intelligence Value at Risk Approach: Metrics and Models]] (83.8% similar)
- [[2025-09-18/TAI Scan Tool_ A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment_20250918|TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment]] (83.7% similar)
- [[2025-09-24/Perceptions of AI Across Sectors_ A Comparative Review of Public Attitudes_20250924|Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes]] (81.7% similar)
- [[2025-09-23/A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents_20250923|A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/AI Governance|AI Governance]]
**âš¡ Unique Technical**: [[keywords/Responsible AI|Responsible AI]], [[keywords/AI Risk Taxonomy|AI Risk Taxonomy]], [[keywords/SafetyGuard|SafetyGuard]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20057v1 Announce Type: cross 
Abstract: KT developed a Responsible AI (RAI) assessment methodology and risk mitigation technologies to ensure the safety and reliability of AI services. By analyzing the Basic Act on AI implementation and global AI governance trends, we established a unique approach for regulatory compliance and systematically identify and manage all potential risk factors from AI development to operation. We present a reliable assessment methodology that systematically verifies model safety and robustness based on KT's AI risk taxonomy tailored to the domestic environment. We also provide practical tools for managing and mitigating identified AI risks. With the release of this report, we also release proprietary Guardrail : SafetyGuard that blocks harmful responses from AI models in real-time, supporting the enhancement of safety in the domestic AI development ecosystem. We also believe these research outcomes provide valuable insights for organizations seeking to develop Responsible AI.

## ğŸ“ ìš”ì•½

KTëŠ” AI ì„œë¹„ìŠ¤ì˜ ì•ˆì „ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì±…ì„ ìˆëŠ” AI(RAI) í‰ê°€ ë°©ë²•ë¡ ê³¼ ìœ„í—˜ ì™„í™” ê¸°ìˆ ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. AI ê¸°ë³¸ë²• ì‹œí–‰ê³¼ ê¸€ë¡œë²Œ AI ê±°ë²„ë„ŒìŠ¤ ë™í–¥ì„ ë¶„ì„í•˜ì—¬ ê·œì œ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ë…ì°½ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ìˆ˜ë¦½í•˜ê³ , AI ê°œë°œë¶€í„° ìš´ì˜ê¹Œì§€ì˜ ëª¨ë“  ì ì¬ì  ìœ„í—˜ ìš”ì†Œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤. KTì˜ AI ìœ„í—˜ ë¶„ë¥˜ ì²´ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì˜ ì•ˆì „ì„±ê³¼ ê²¬ê³ ì„±ì„ ê²€ì¦í•˜ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ ë°©ë²•ë¡ ì„ ì œì‹œí•˜ë©°, ì‹ë³„ëœ AI ìœ„í—˜ì„ ê´€ë¦¬í•˜ê³  ì™„í™”í•˜ê¸° ìœ„í•œ ì‹¤ìš©ì ì¸ ë„êµ¬ë„ ì œê³µí•©ë‹ˆë‹¤. ì´ ë³´ê³ ì„œì™€ í•¨ê»˜ AI ëª¨ë¸ì˜ ìœ í•´í•œ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì°¨ë‹¨í•˜ëŠ” SafetyGuardë¼ëŠ” ë…ì ì ì¸ ë„êµ¬ë¥¼ ì¶œì‹œí•˜ì—¬ êµ­ë‚´ AI ê°œë°œ ìƒíƒœê³„ì˜ ì•ˆì „ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ ê²°ê³¼ëŠ” ì±…ì„ ìˆëŠ” AIë¥¼ ê°œë°œí•˜ë ¤ëŠ” ì¡°ì§ì— ìœ ìš©í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. KTëŠ” AI ì„œë¹„ìŠ¤ì˜ ì•ˆì „ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì±…ì„ ìˆëŠ” AI í‰ê°€ ë°©ë²•ë¡ ê³¼ ìœ„í—˜ ì™„í™” ê¸°ìˆ ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- 2. AI ê°œë°œë¶€í„° ìš´ì˜ê¹Œì§€ì˜ ëª¨ë“  ì ì¬ì  ìœ„í—˜ ìš”ì†Œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë…ì°½ì ì¸ ê·œì œ ì¤€ìˆ˜ ì ‘ê·¼ ë°©ì‹ì„ í™•ë¦½í–ˆìŠµë‹ˆë‹¤.
- 3. KTì˜ AI ìœ„í—˜ ë¶„ë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì˜ ì•ˆì „ì„±ê³¼ ê²¬ê³ ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ ë°©ë²•ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 4. ì‹¤ì‹œê°„ìœ¼ë¡œ AI ëª¨ë¸ì˜ ìœ í•´í•œ ì‘ë‹µì„ ì°¨ë‹¨í•˜ëŠ” SafetyGuardë¥¼ ì¶œì‹œí•˜ì—¬ êµ­ë‚´ AI ê°œë°œ ìƒíƒœê³„ì˜ ì•ˆì „ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ ê²°ê³¼ëŠ” ì±…ì„ ìˆëŠ” AIë¥¼ ê°œë°œí•˜ë ¤ëŠ” ì¡°ì§ì— ê·€ì¤‘í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:57:51*