---
keywords:
  - Vision-Language Model
  - Intermediate Projector
  - Transformer
  - Residual Query Alignment
  - Multimodal Learning
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2508.13739
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:26:59.430875",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Intermediate Projector",
    "Transformer",
    "Residual Query Alignment",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Intermediate Projector": 0.79,
    "Transformer": 0.82,
    "Residual Query Alignment": 0.77,
    "Multimodal Learning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on adversarial attacks, providing strong linkage to multimodal learning discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Intermediate Projector",
        "canonical": "Intermediate Projector",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The Intermediate Projector is a novel component proposed in the paper, crucial for enhancing attack granularity and transferability.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Querying Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Q-Former"
        ],
        "category": "broad_technical",
        "rationale": "The Querying Transformer is a key mechanism for transforming embeddings, linking to broader Transformer discussions.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Residual Query Alignment",
        "canonical": "Residual Query Alignment",
        "aliases": [
          "RQA"
        ],
        "category": "unique_technical",
        "rationale": "Residual Query Alignment is a specialized module introduced in the paper, enhancing the specificity of fine-grained attacks.",
        "novelty_score": 0.81,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multimodal Alignment",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Alignment is essential for understanding the interaction between visual and language components in VLMs.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.68,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "black-box",
      "attack framework",
      "baseline"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Intermediate Projector",
      "resolved_canonical": "Intermediate Projector",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Querying Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Residual Query Alignment",
      "resolved_canonical": "Residual Query Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.81,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multimodal Alignment",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.68,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2508.13739.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2508.13739](https://arxiv.org/abs/2508.13739)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/ADVEDM_Fine-grained Adversarial Attack against VLM-based Embodied Agents_20250923|ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents]] (86.2% similar)
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (85.8% similar)
- [[2025-09-25/FreezeVLA_ Action-Freezing Attacks against Vision-Language-Action Models_20250925|FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models]] (85.4% similar)
- [[2025-09-25/Semantic Representation Attack against Aligned Large Language Models_20250925|Semantic Representation Attack against Aligned Large Language Models]] (85.3% similar)
- [[2025-09-23/Sugar-Coated Poison_ Benign Generation Unlocks LLM Jailbreaking_20250923|Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Intermediate Projector|Intermediate Projector]], [[keywords/Residual Query Alignment|Residual Query Alignment]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.13739v2 Announce Type: replace 
Abstract: The growing deployment of Large Vision-Language Models (VLMs) raises safety concerns, as adversaries may exploit model vulnerabilities to induce harmful outputs, with targeted black-box adversarial attacks posing a particularly severe threat. However, existing methods primarily maximize encoder-level global similarity, which lacks the granularity for stealthy and practical fine-grained attacks, where only specific target should be altered (e.g., modifying a car while preserving its background). Moreover, they largely neglect the projector, a key semantic bridge in VLMs for multimodal alignment. To address these limitations, we propose a novel black-box targeted attack framework that leverages the projector. Specifically, we utilize the widely adopted Querying Transformer (Q-Former) which transforms global image embeddings into fine-grained query outputs, to enhance attack effectiveness and granularity. For standard global targeted attack scenarios, we propose the Intermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained query outputs with the target to enhance attack strength and exploits the intermediate pretrained Q-Former that is not fine-tuned for any specific Large Language Model (LLM) to improve attack transferability. For fine-grained attack scenarios, we augment IPGA with the Residual Query Alignment (RQA) module, which preserves unrelated content by constraining non-target query outputs to enhance attack granularity. Extensive experiments demonstrate that IPGA significantly outperforms baselines in global targeted attacks, and IPGA with RQA (IPGA-R) attains superior success rates and unrelated content preservation over baselines in fine-grained attacks. Our method also transfers effectively to commercial VLMs such as Google Gemini and OpenAI GPT.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ì•ˆì „ì„± ë¬¸ì œë¥¼ ë‹¤ë£¨ë©°, íŠ¹íˆ ë¸”ë™ë°•ìŠ¤ ê³µê²©ì˜ ìœ„í˜‘ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì£¼ë¡œ ì¸ì½”ë” ìˆ˜ì¤€ì˜ ì „ì—­ ìœ ì‚¬ì„±ì„ ê·¹ëŒ€í™”í•˜ì§€ë§Œ, ì´ëŠ” ì„¸ë°€í•œ ê³µê²©ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•´, ìš°ë¦¬ëŠ” í”„ë¡œì í„°ë¥¼ í™œìš©í•œ ìƒˆë¡œìš´ ë¸”ë™ë°•ìŠ¤ ê³µê²© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ Q-Formerë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ì„¸ë°€í•œ ì¿¼ë¦¬ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³µê²©ì˜ íš¨ê³¼ì™€ ì„¸ë°€ì„±ì„ ë†’ì…ë‹ˆë‹¤. ë˜í•œ, IPGAì™€ RQA ëª¨ë“ˆì„ í†µí•´ ë¹„í‘œì  ì¿¼ë¦¬ ì¶œë ¥ì„ ì œì•½í•˜ì—¬ ê³µê²©ì˜ ì„¸ë°€ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ê³¼ ë¹„ê´€ë ¨ ì½˜í…ì¸  ë³´ì¡´ ëŠ¥ë ¥ì„ ë³´ì´ë©°, ìƒì—…ì  VLMì—ë„ íš¨ê³¼ì ìœ¼ë¡œ ì „ì´ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ë³´ê¸‰ì´ ì¦ê°€í•¨ì— ë”°ë¼, ëª¨ë¸ì˜ ì·¨ì•½ì ì„ ì•…ìš©í•˜ì—¬ ìœ í•´í•œ ì¶œë ¥ì„ ìœ ë„í•˜ëŠ” ê³µê²©ì— ëŒ€í•œ ì•ˆì „ì„± ìš°ë ¤ê°€ ì œê¸°ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì£¼ë¡œ ì¸ì½”ë” ìˆ˜ì¤€ì˜ ì „ì—­ ìœ ì‚¬ì„±ì„ ê·¹ëŒ€í™”í•˜ì§€ë§Œ, ì„¸ë°€í•œ í‘œì  ê³µê²©ì—ëŠ” ì í•©í•˜ì§€ ì•Šìœ¼ë©°, í”„ë¡œì í„°ì˜ ì—­í• ì„ ê°„ê³¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” Q-Formerë¥¼ í™œìš©í•˜ì—¬ ê³µê²©ì˜ íš¨ê³¼ì„±ê³¼ ì„¸ë°€í•¨ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ë¸”ë™ë°•ìŠ¤ í‘œì  ê³µê²© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. IPGAëŠ” Q-Formerì˜ ì„¸ë°€í•œ ì¿¼ë¦¬ ì¶œë ¥ì„ í‘œì ê³¼ ì •ë ¬í•˜ì—¬ ê³µê²© ê°•ë„ë¥¼ ë†’ì´ê³ , íŠ¹ì • ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ë§ì¶° ì¡°ì •ë˜ì§€ ì•Šì€ ì¤‘ê°„ ì‚¬ì „ í›ˆë ¨ëœ Q-Formerë¥¼ í™œìš©í•˜ì—¬ ê³µê²© ì „ì´ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, IPGAëŠ” ì „ì—­ í‘œì  ê³µê²©ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, IPGA-Rì€ ì„¸ë°€í•œ ê³µê²©ì—ì„œ ë†’ì€ ì„±ê³µë¥ ê³¼ ë¹„ê´€ë ¨ ì½˜í…ì¸  ë³´ì¡´ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:26:59*