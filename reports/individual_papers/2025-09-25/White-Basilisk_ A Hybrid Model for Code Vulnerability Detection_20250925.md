---
keywords:
  - White-Basilisk Model
  - Vulnerability Detection
  - Mixture of Experts
  - Attention Mechanism
  - Large Language Model
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2507.08540
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:31:10.821044",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "White-Basilisk Model",
    "Vulnerability Detection",
    "Mixture of Experts",
    "Attention Mechanism",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "White-Basilisk Model": 0.78,
    "Vulnerability Detection": 0.85,
    "Mixture of Experts": 0.8,
    "Attention Mechanism": 0.82,
    "Large Language Model": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "White-Basilisk",
        "canonical": "White-Basilisk Model",
        "aliases": [
          "White-Basilisk"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel model architecture specifically designed for code vulnerability detection, offering a unique point of reference.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "vulnerability detection",
        "canonical": "Vulnerability Detection",
        "aliases": [
          "code vulnerability detection"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's focus, linking to broader cybersecurity and AI applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Mixture of Experts",
        "canonical": "Mixture of Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "A key component of the model's architecture, relevant to discussions on model efficiency and scalability.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "linear self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "linear self-attention"
        ],
        "category": "specific_connectable",
        "rationale": "A variant of attention mechanisms, crucial for understanding the model's ability to handle long sequences.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Provides context for the model's performance comparison and relevance in AI development.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "methodologies",
      "performance",
      "tasks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "White-Basilisk",
      "resolved_canonical": "White-Basilisk Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "vulnerability detection",
      "resolved_canonical": "Vulnerability Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Mixture of Experts",
      "resolved_canonical": "Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "linear self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# White-Basilisk: A Hybrid Model for Code Vulnerability Detection

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.08540.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2507.08540](https://arxiv.org/abs/2507.08540)

## 🔗 유사한 논문
- [[2025-09-25/CyberSOCEval_ Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning_20250925|CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning]] (84.5% similar)
- [[2025-09-22/Activation Space Interventions Can Be Transferred Between Large Language Models_20250922|Activation Space Interventions Can Be Transferred Between Large Language Models]] (83.8% similar)
- [[2025-09-23/LLaVul_ A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code_20250923|LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code]] (83.5% similar)
- [[2025-09-25/Cognitive Load Limits in Large Language Models_ Benchmarking Multi-Hop Reasoning_20250925|Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning]] (83.1% similar)
- [[2025-09-22/SeCodePLT_ A Unified Platform for Evaluating the Security of Code GenAI_20250922|SeCodePLT: A Unified Platform for Evaluating the Security of Code GenAI]] (83.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Vulnerability Detection|Vulnerability Detection]], [[keywords/Mixture of Experts|Mixture of Experts]], [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/White-Basilisk Model|White-Basilisk Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.08540v3 Announce Type: replace-cross 
Abstract: The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The model's capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications.

## 📝 요약

소프트웨어 취약점의 증가로 인해 효과적인 탐지 방법이 필요합니다. 본 연구에서는 White-Basilisk라는 새로운 취약점 탐지 접근법을 소개합니다. 이 모델은 Mamba 레이어, 선형 자기 주의 메커니즘, 전문가 혼합 프레임워크를 통합한 혁신적인 아키텍처를 사용하여, 200M 파라미터로 최첨단 성능을 달성합니다. 특히, 긴 시퀀스를 처리할 수 있어 대규모 코드베이스를 한 번에 분석할 수 있으며, 기존 대형 언어 모델의 한계를 극복합니다. White-Basilisk는 불균형한 실제 데이터셋에서도 뛰어난 성능을 보이며, 다양한 조직 규모에 맞춰 효율적으로 배포될 수 있습니다. 이 연구는 코드 보안 분야에서 새로운 기준을 세우고, 작고 효율적인 모델이 더 큰 모델보다 특정 작업에서 더 나은 성능을 발휘할 수 있음을 입증하여 AI 개발의 최적화 전략을 재정의할 가능성을 제시합니다.

## 🎯 주요 포인트

- 1. White-Basilisk는 소프트웨어 취약점 탐지에서 뛰어난 성능을 보이며, AI 모델 확장에 대한 기존 가정을 도전합니다.
- 2. Mamba 레이어, 선형 자기 주의, 전문가 혼합 프레임워크를 통합한 혁신적인 아키텍처를 사용하여 200M 파라미터로 최첨단 결과를 달성합니다.
- 3. 긴 시퀀스를 처리할 수 있는 모델의 능력은 대규모 코드베이스를 단일 패스로 분석할 수 있게 하여, 현재 대형 언어 모델의 문맥 제한을 초월합니다.
- 4. White-Basilisk는 불균형한 실제 데이터셋에서도 강력한 성능을 발휘하며, 다양한 조직 규모에 배포할 수 있는 계산 효율성을 유지합니다.
- 5. 이 연구는 코드 보안의 새로운 기준을 세우고, 효율적으로 설계된 소형 모델이 특화된 작업에서 더 큰 모델을 능가할 수 있음을 입증하여 AI 개발의 최적화 전략을 재정의할 가능성을 제시합니다.


---

*Generated on 2025-09-25 16:31:10*