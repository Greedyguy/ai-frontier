---
keywords:
  - Penetration Testing
  - Partial Observability
  - Proximal Policy Optimization
  - Machine Learning
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.20008
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:44:29.163203",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Penetration Testing",
    "Partial Observability",
    "Proximal Policy Optimization",
    "Machine Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Penetration Testing": 0.78,
    "Partial Observability": 0.77,
    "Proximal Policy Optimization": 0.8,
    "Machine Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Penetration Testing",
        "canonical": "Penetration Testing",
        "aliases": [
          "Pentesting",
          "Security Testing"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's focus on cybersecurity and is specific to the domain of security assessments.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Partial Observability",
        "canonical": "Partial Observability",
        "aliases": [
          "Partial Observation"
        ],
        "category": "unique_technical",
        "rationale": "Partial observability is a key challenge in the paper, affecting the design of reinforcement learning algorithms.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "broad_technical",
        "rationale": "PPO is a widely used reinforcement learning algorithm, relevant for linking discussions on RL techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement learning is a core aspect of the study, linking to broader discussions in machine learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "sequential decision-making",
      "real-world environments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Penetration Testing",
      "resolved_canonical": "Penetration Testing",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Partial Observability",
      "resolved_canonical": "Partial Observability",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20008.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.20008](https://arxiv.org/abs/2509.20008)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (83.3% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (82.9% similar)
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (82.9% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.3% similar)
- [[2025-09-24/Online Process Reward Leanring for Agentic Reinforcement Learning_20250924|Online Process Reward Leanring for Agentic Reinforcement Learning]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]], [[keywords/Machine Learning|Machine Learning]]
**âš¡ Unique Technical**: [[keywords/Penetration Testing|Penetration Testing]], [[keywords/Partial Observability|Partial Observability]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20008v1 Announce Type: new 
Abstract: Penetration testing, the simulation of cyberattacks to identify security vulnerabilities, presents a sequential decision-making problem well-suited for reinforcement learning (RL) automation. Like many applications of RL to real-world problems, partial observability presents a major challenge, as it invalidates the Markov property present in Markov Decision Processes (MDPs). Partially Observable MDPs require history aggregation or belief state estimation to learn successful policies. We investigate stochastic, partially observable penetration testing scenarios over host networks of varying size, aiming to better reflect real-world complexity through more challenging and representative benchmarks. This approach leads to the development of more robust and transferable policies, which are crucial for ensuring reliable performance across diverse and unpredictable real-world environments. Using vanilla Proximal Policy Optimization (PPO) as a baseline, we compare a selection of PPO variants designed to mitigate partial observability, including frame-stacking, augmenting observations with historical information, and employing recurrent or transformer-based architectures. We conduct a systematic empirical analysis of these algorithms across different host network sizes. We find that this task greatly benefits from history aggregation. Converging three times faster than other approaches. Manual inspection of the learned policies by the algorithms reveals clear distinctions and provides insights that go beyond quantitative results.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¹¨íˆ¬ í…ŒìŠ¤íŠ¸ì—ì„œ ê°•í™” í•™ìŠµ(RL)ì„ í™œìš©í•˜ì—¬ ë³´ì•ˆ ì·¨ì•½ì ì„ ì‹ë³„í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ë¶€ë¶„ ê´€ì°° ê°€ëŠ¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ PPO(ê·¼ì ‘ ì •ì±… ìµœì í™”) ë³€í˜•ì„ ë¹„êµí•˜ë©°, íŠ¹íˆ ì—­ì‚¬ ì •ë³´ì˜ ì§‘ê³„ê°€ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì—­ì‚¬ ì •ë³´ ì§‘ê³„ë¥¼ í†µí•œ ë°©ë²•ì´ ë‹¤ë¥¸ ë°©ë²•ë“¤ë³´ë‹¤ ì„¸ ë°° ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©°, í•™ìŠµëœ ì •ì±…ì˜ ìˆ˜ë™ ê²€í† ë¥¼ í†µí•´ ì •ëŸ‰ì  ê²°ê³¼ë¥¼ ë„˜ì–´ì„  í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ì—ì„œì˜ ì‹¤í—˜ì„ í†µí•´ í˜„ì‹¤ ì„¸ê³„ì˜ ë³µì¡ì„±ì„ ë” ì˜ ë°˜ì˜í•˜ëŠ” ê°•ë ¥í•˜ê³  ì „ì´ ê°€ëŠ¥í•œ ì •ì±… ê°œë°œì— ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¹¨íˆ¬ í…ŒìŠ¤íŠ¸ëŠ” ê°•í™” í•™ìŠµ ìë™í™”ì— ì í•©í•œ ìˆœì°¨ì  ì˜ì‚¬ ê²°ì • ë¬¸ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- 2. ë¶€ë¶„ ê´€ì°° ê°€ëŠ¥ì„±ì€ MDPì˜ ë§ˆë¥´ì½”í”„ ì†ì„±ì„ ë¬´íš¨í™”í•˜ì—¬ ê°•í™” í•™ìŠµì— ì£¼ìš” ë„ì „ ê³¼ì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ í¬ê¸°ì˜ í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ì—ì„œ ë¶€ë¶„ ê´€ì°° ê°€ëŠ¥í•œ ì¹¨íˆ¬ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì¡°ì‚¬í•˜ì—¬ í˜„ì‹¤ ì„¸ê³„ì˜ ë³µì¡ì„±ì„ ë” ì˜ ë°˜ì˜í•˜ë ¤ê³  í•©ë‹ˆë‹¤.
- 4. ì—­ì‚¬ ì§‘ê³„ ë˜ëŠ” ì‹ ë… ìƒíƒœ ì¶”ì •ì€ ì„±ê³µì ì¸ ì •ì±… í•™ìŠµì— í•„ìˆ˜ì ì´ë©°, ì´ëŠ” ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì¥í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.
- 5. Vanilla PPOì™€ ë‹¤ì–‘í•œ ë³€í˜•ì„ ë¹„êµí•œ ê²°ê³¼, ì—­ì‚¬ ì§‘ê³„ê°€ í•™ìŠµ ì†ë„ë¥¼ ì„¸ ë°° ë¹ ë¥´ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:44:29*