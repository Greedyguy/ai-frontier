---
keywords:
  - Zero-Shot Learning
  - Model Finetuning
  - Domain-Specific Pretraining
  - Small Language Models
  - Large Language Model
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2504.21191
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:23:11.777047",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Zero-Shot Learning",
    "Model Finetuning",
    "Domain-Specific Pretraining",
    "Small Language Models",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Zero-Shot Learning": 0.85,
    "Model Finetuning": 0.82,
    "Domain-Specific Pretraining": 0.78,
    "Small Language Models": 0.77,
    "Large Language Model": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Zero-Shot",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept relevant to the model evaluation discussed in the paper.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Finetuning",
        "canonical": "Model Finetuning",
        "aliases": [
          "Finetuning",
          "Fine-tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Model Finetuning is a critical process in enhancing model performance, as emphasized in the study.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Domain-Specific Pretraining",
        "canonical": "Domain-Specific Pretraining",
        "aliases": [
          "Domain Pretraining"
        ],
        "category": "unique_technical",
        "rationale": "This concept is key for improving model performance on complex tasks with limited data.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Small Language Models",
        "canonical": "Small Language Models",
        "aliases": [
          "SLMs"
        ],
        "category": "unique_technical",
        "rationale": "SLMs are compared to LLMs, highlighting their relevance and trade-offs in specialized applications.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are a central focus of the study, providing a baseline for performance comparison.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "electronic pathology reports",
      "British Columbia Cancer Registry"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Zero-Shot",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Finetuning",
      "resolved_canonical": "Model Finetuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Domain-Specific Pretraining",
      "resolved_canonical": "Domain-Specific Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Small Language Models",
      "resolved_canonical": "Small Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2504.21191.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2504.21191](https://arxiv.org/abs/2504.21191)

## 🔗 유사한 논문
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (86.2% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (85.3% similar)
- [[2025-09-23/Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning_20250923|Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning]] (85.2% similar)
- [[2025-09-23/The Sound of Syntax_ Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology_20250923|The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology]] (85.0% similar)
- [[2025-09-24/Large Language Models Do Multi-Label Classification Differently_20250924|Large Language Models Do Multi-Label Classification Differently]] (84.7% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Model Finetuning|Model Finetuning]]
**⚡ Unique Technical**: [[keywords/Domain-Specific Pretraining|Domain-Specific Pretraining]], [[keywords/Small Language Models|Small Language Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2504.21191v2 Announce Type: replace-cross 
Abstract: This study aims to guide language model selection by investigating: 1) the necessity of finetuning versus zero-shot usage, 2) the benefits of domain-adjacent versus generic pretrained models, 3) the value of further domain-specific pretraining, and 4) the continued relevance of Small Language Models (SLMs) compared to Large Language Models (LLMs) for specific tasks. Using electronic pathology reports from the British Columbia Cancer Registry (BCCR), three classification scenarios with varying difficulty and data size are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning significantly improved SLM performance across all scenarios compared to their zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally performed better than the generic SLM after finetuning, especially on harder tasks. Further domain-specific pretraining yielded modest gains on easier tasks but significant improvements on the complex, data-scarce task. The results highlight the critical role of finetuning for SLMs in specialized domains, enabling them to surpass zero-shot LLM performance on targeted classification tasks. Pretraining on domain-adjacent or domain-specific data provides further advantages, particularly for complex problems or limited finetuning data. While LLMs offer strong zero-shot capabilities, their performance on these specific tasks did not match that of appropriately finetuned SLMs. In the era of LLMs, SLMs remain relevant and effective, offering a potentially superior performance-resource trade-off compared to LLMs.

## 📝 요약

이 연구는 언어 모델 선택을 위한 지침을 제공하기 위해 다양한 측면을 조사합니다. 주요 기여는 다음과 같습니다: 1) 파인튜닝의 필요성, 2) 도메인 인접 모델의 이점, 3) 도메인 특화 사전 학습의 가치, 4) 특정 작업에서 소형 언어 모델(SLM)의 지속적 중요성. 연구는 British Columbia Cancer Registry의 병리 보고서를 사용하여 세 가지 분류 시나리오를 평가했습니다. SLM은 제로샷과 파인튜닝 방식으로 평가되었고, 대형 언어 모델(LLM)은 제로샷으로만 평가되었습니다. 결과적으로, 파인튜닝된 SLM은 모든 시나리오에서 제로샷 SLM과 LLM을 능가했습니다. 도메인 인접 SLM은 파인튜닝 후 일반 SLM보다 더 나은 성능을 보였으며, 복잡한 작업에서 도메인 특화 사전 학습이 큰 개선을 가져왔습니다. 연구는 SLM의 파인튜닝이 전문 분야에서 중요하며, LLM의 제로샷 성능을 초과할 수 있음을 강조합니다. LLM은 강력한 제로샷 기능을 제공하지만, 적절히 파인튜닝된 SLM의 성능을 따라잡지 못했습니다. LLM 시대에도 SLM은 여전히 관련성과 효율성을 유지하며, 성능과 자원 효율성에서 우위를 점할 수 있습니다.

## 🎯 주요 포인트

- 1. SLM의 미세 조정은 모든 시나리오에서 성능을 크게 향상시켜, 제로샷 LLM을 능가했습니다.
- 2. 도메인 인접 SLM은 특히 어려운 작업에서 일반 SLM보다 미세 조정 후 더 나은 성능을 보였습니다.
- 3. 도메인 특화 사전 학습은 복잡하고 데이터가 부족한 작업에서 상당한 개선을 가져왔습니다.
- 4. LLM은 강력한 제로샷 성능을 제공하지만, 적절히 미세 조정된 SLM의 성능에는 미치지 못했습니다.
- 5. LLM 시대에도 SLM은 여전히 관련성이 있으며, 성능-자원 균형에서 LLM보다 우수할 수 있습니다.


---

*Generated on 2025-09-25 16:23:11*