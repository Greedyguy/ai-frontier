---
keywords:
  - Large Language Model
  - Clinical Trial Matching
  - Zero-Shot Learning
  - Synthetic Data
  - GPT-4
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19327
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:24:14.947721",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Clinical Trial Matching",
    "Zero-Shot Learning",
    "Synthetic Data",
    "GPT-4"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Clinical Trial Matching": 0.78,
    "Zero-Shot Learning": 0.8,
    "Synthetic Data": 0.7,
    "GPT-4": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large language model"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on trial-matching pipelines, providing a direct link to existing research in NLP and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "clinical trial matching",
        "canonical": "Clinical Trial Matching",
        "aliases": [
          "trial-matching"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique application of LLMs, crucial for understanding the paper's contribution to healthcare technology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "zero-shot prompting",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot prompting is a trending technique in LLMs, relevant for linking to broader discussions on model capabilities.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "synthetic data",
        "canonical": "Synthetic Data",
        "aliases": [
          "artificial data"
        ],
        "category": "unique_technical",
        "rationale": "The use of synthetic data is significant for understanding data privacy and model training aspects in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "GPT-4 model",
        "canonical": "GPT-4",
        "aliases": [
          "GPT-4o"
        ],
        "category": "specific_connectable",
        "rationale": "GPT-4 is a prominent model in the paper, offering a direct link to discussions on model performance and capabilities.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "manual matching",
      "recruitment delays",
      "evaluation metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "clinical trial matching",
      "resolved_canonical": "Clinical Trial Matching",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "zero-shot prompting",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "synthetic data",
      "resolved_canonical": "Synthetic Data",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "GPT-4 model",
      "resolved_canonical": "GPT-4",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# A systematic review of trial-matching pipelines using large language models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19327.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19327](https://arxiv.org/abs/2509.19327)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (85.8% similar)
- [[2025-09-19/Position_ Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models_20250919|Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models]] (85.4% similar)
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (85.1% similar)
- [[2025-09-24/Advances in Large Language Models for Medicine_20250924|Advances in Large Language Models for Medicine]] (84.9% similar)
- [[2025-09-23/Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation_20250923|Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation]] (84.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/GPT-4|GPT-4]]
**âš¡ Unique Technical**: [[keywords/Clinical Trial Matching|Clinical Trial Matching]], [[keywords/Synthetic Data|Synthetic Data]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19327v1 Announce Type: cross 
Abstract: Matching patients to clinical trial options is critical for identifying novel treatments, especially in oncology. However, manual matching is labor-intensive and error-prone, leading to recruitment delays. Pipelines incorporating large language models (LLMs) offer a promising solution. We conducted a systematic review of studies published between 2020 and 2025 from three academic databases and one preprint server, identifying LLM-based approaches to clinical trial matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies focused on matching patient-to-criterion only (n=4), patient-to-trial only (n=10), trial-to-patient only (n=2), binary eligibility classification only (n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real patient data; one used both. Variability in datasets and evaluation metrics limited cross-study comparability. In studies with direct comparisons, the GPT-4 model consistently outperformed other models, even finely-tuned ones, in matching and eligibility extraction, albeit at higher cost. Promising strategies included zero-shot prompting with proprietary LLMs like the GPT-4o model, advanced retrieval methods, and fine-tuning smaller, open-source models for data privacy when incorporation of large models into hospital infrastructure is infeasible. Key challenges include accessing sufficiently large real-world data sets, and deployment-associated challenges such as reducing cost, mitigating risk of hallucinations, data leakage, and bias. This review synthesizes progress in applying LLMs to clinical trial matching, highlighting promising directions and key limitations. Standardized metrics, more realistic test sets, and attention to cost-efficiency and fairness will be critical for broader deployment.

## ğŸ“ ìš”ì•½

ì„ìƒ ì‹œí—˜ ì˜µì…˜ê³¼ í™˜ì ë§¤ì¹­ì€ íŠ¹íˆ ì¢…ì–‘í•™ ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ì¹˜ë£Œë²•ì„ ì°¾ëŠ” ë° ì¤‘ìš”í•˜ì§€ë§Œ, ìˆ˜ì‘ì—… ë§¤ì¹­ì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³  ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ íŒŒì´í”„ë¼ì¸ì€ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìœ ë§í•œ ë°©ë²•ìœ¼ë¡œ, 2020ë…„ë¶€í„° 2025ë…„ê¹Œì§€ì˜ ì—°êµ¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê²€í† í•œ ê²°ê³¼ 31ê°œì˜ ê´€ë ¨ ì—°êµ¬ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ë“¤ì€ ì£¼ë¡œ í™˜ì-ê¸°ì¤€, í™˜ì-ì‹œí—˜, ì‹œí—˜-í™˜ì ë§¤ì¹­ ë° ì´ì§„ ì í•©ì„± ë¶„ë¥˜ì— ì¤‘ì ì„ ë‘ì—ˆìœ¼ë©°, GPT-4 ëª¨ë¸ì´ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì£¼ìš” ë„ì „ ê³¼ì œë¡œëŠ” ì¶©ë¶„í•œ ì‹¤ì œ ë°ì´í„° í™•ë³´, ë¹„ìš© ì ˆê°, í™˜ê° ìœ„í—˜ ì™„í™”, ë°ì´í„° ìœ ì¶œ ë° í¸í–¥ ë¬¸ì œ ë“±ì´ ìˆìœ¼ë©°, í‘œì¤€í™”ëœ í‰ê°€ ì§€í‘œì™€ í˜„ì‹¤ì ì¸ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì„ìƒ ì‹œí—˜ ë§¤ì¹­ì— ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ëŠ” ê²ƒì€ íŠ¹íˆ ì¢…ì–‘í•™ ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ì¹˜ë£Œë²•ì„ ì‹ë³„í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- 2. 2020ë…„ë¶€í„° 2025ë…„ê¹Œì§€ ë°œí‘œëœ ì—°êµ¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê²€í† í•œ ê²°ê³¼, LLM ê¸°ë°˜ì˜ ì„ìƒ ì‹œí—˜ ë§¤ì¹­ ì ‘ê·¼ë²•ì´ ì‹ë³„ë˜ì—ˆë‹¤.
- 3. GPT-4 ëª¨ë¸ì€ ë§¤ì¹­ ë° ì ê²©ì„± ì¶”ì¶œì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ë¹„ìš©ì´ ë” ë†’ì•˜ë‹¤.
- 4. ì£¼ìš” ê³¼ì œë¡œëŠ” ì¶©ë¶„íˆ í° ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ì ‘ê·¼ê³¼ ë¹„ìš© ì ˆê°, í™˜ê° ìœ„í—˜ ì™„í™”, ë°ì´í„° ìœ ì¶œ ë° í¸í–¥ ë¬¸ì œ í•´ê²°ì´ ìˆë‹¤.
- 5. í‘œì¤€í™”ëœ ì§€í‘œ, ë³´ë‹¤ í˜„ì‹¤ì ì¸ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸, ë¹„ìš© íš¨ìœ¨ì„±ê³¼ ê³µì •ì„±ì— ëŒ€í•œ ì£¼ì˜ê°€ ë” ë„“ì€ ë°°í¬ë¥¼ ìœ„í•´ ì¤‘ìš”í•˜ë‹¤.


---

*Generated on 2025-09-25 15:24:14*