---
keywords:
  - Convex Regression
  - Subgradient
  - Queueing Theory
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19788
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:58:17.783542",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Convex Regression",
    "Subgradient",
    "Queueing Theory"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Convex Regression": 0.75,
    "Subgradient": 0.78,
    "Queueing Theory": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Convex Regression",
        "canonical": "Convex Regression",
        "aliases": [
          "Convex Function Estimation"
        ],
        "category": "unique_technical",
        "rationale": "Convex regression is a specific statistical method that is central to the paper's contribution.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Subgradient",
        "canonical": "Subgradient",
        "aliases": [
          "Subgradient Method"
        ],
        "category": "unique_technical",
        "rationale": "The subgradient is a key mathematical concept used in the proposed estimator, linking it to optimization techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Single-Server Queue",
        "canonical": "Queueing Theory",
        "aliases": [
          "Queueing Model",
          "Single-Server Model"
        ],
        "category": "specific_connectable",
        "rationale": "Queueing theory is relevant for applications of the estimator, providing a bridge to operational research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Penalty",
      "Estimator",
      "Sum of Squared Errors"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Convex Regression",
      "resolved_canonical": "Convex Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Subgradient",
      "resolved_canonical": "Subgradient",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Single-Server Queue",
      "resolved_canonical": "Queueing Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Convex Regression with a Penalty

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19788.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19788](https://arxiv.org/abs/2509.19788)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Pareto-optimal Tradeoffs Between Communication and Computation with Flexible Gradient Tracking_20250924|Pareto-optimal Tradeoffs Between Communication and Computation with Flexible Gradient Tracking]] (82.1% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (81.6% similar)
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (81.2% similar)
- [[2025-09-23/Regularizing Extrapolation in Causal Inference_20250923|Regularizing Extrapolation in Causal Inference]] (81.1% similar)
- [[2025-09-23/Conformal Prediction with Upper and Lower Bound Models_20250923|Conformal Prediction with Upper and Lower Bound Models]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Queueing Theory|Queueing Theory]]
**âš¡ Unique Technical**: [[keywords/Convex Regression|Convex Regression]], [[keywords/Subgradient|Subgradient]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19788v1 Announce Type: cross 
Abstract: A common way to estimate an unknown convex regression function $f_0: \Omega \subset \mathbb{R}^d \rightarrow \mathbb{R}$ from a set of $n$ noisy observations is to fit a convex function that minimizes the sum of squared errors. However, this estimator is known for its tendency to overfit near the boundary of $\Omega$, posing significant challenges in real-world applications. In this paper, we introduce a new estimator of $f_0$ that avoids this overfitting by minimizing a penalty on the subgradient while enforcing an upper bound $s_n$ on the sum of squared errors. The key advantage of this method is that $s_n$ can be directly estimated from the data. We establish the uniform almost sure consistency of the proposed estimator and its subgradient over $\Omega$ as $n \rightarrow \infty$ and derive convergence rates. The effectiveness of our estimator is illustrated through its application to estimating waiting times in a single-server queue.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê²½ê³„ ê·¼ì²˜ì—ì„œ ê³¼ì í•© ë¬¸ì œê°€ ìˆëŠ” ê¸°ì¡´ì˜ ë³¼ë¡ íšŒê·€ í•¨ìˆ˜ ì¶”ì • ë°©ë²•ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì¶”ì • ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì œê³± ì˜¤ì°¨ í•©ì— ìƒí•œì„ ë‘ê³ , ì„œë¸Œê·¸ë˜ë””ì–¸íŠ¸ì— íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤. ì´ ìƒí•œì€ ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ì¶”ì •ê¸°ì˜ ê· ì¼ ê±°ì˜ í™•ì‹¤í•œ ì¼ì¹˜ì„±ê³¼ ìˆ˜ë ´ ì†ë„ë¥¼ ì¦ëª…í•˜ì˜€ìœ¼ë©°, ë‹¨ì¼ ì„œë²„ ëŒ€ê¸°ì—´ì˜ ëŒ€ê¸° ì‹œê°„ ì¶”ì •ì— ì ìš©í•˜ì—¬ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ë³¼ë¡ íšŒê·€ í•¨ìˆ˜ ì¶”ì • ë°©ë²•ì€ ê²½ê³„ ê·¼ì²˜ì—ì„œ ê³¼ì í•© ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
- 2. ìƒˆë¡œìš´ ì¶”ì • ë°©ë²•ì€ ì„œë¸Œê·¸ë˜ë””ì–¸íŠ¸ì— ëŒ€í•œ íŒ¨ë„í‹°ë¥¼ ìµœì†Œí™”í•˜ê³ , ì˜¤ì°¨ ì œê³±í•©ì— ìƒí•œì„ ë‘ì–´ ê³¼ì í•©ì„ ë°©ì§€í•œë‹¤.
- 3. ì œì•ˆëœ ì¶”ì • ë°©ë²•ì€ ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ ìƒí•œì„ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆë‹¤.
- 4. ì œì•ˆëœ ì¶”ì •ê¸°ì˜ ì¼ê´€ì„±ê³¼ ìˆ˜ë ´ ì†ë„ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…í•˜ì˜€ë‹¤.
- 5. ë‹¨ì¼ ì„œë²„ ëŒ€ê¸°ì—´ì˜ ëŒ€ê¸° ì‹œê°„ ì¶”ì •ì— ì ìš©í•˜ì—¬ ì¶”ì •ê¸°ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-25 16:58:17*