---
keywords:
  - Discrete Diffusion Models
  - Learnable Sampler Distillation
  - Natural Language Processing
  - High-Fidelity Samplers
  - Adaptive Time Scheduling
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19962
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:43:03.116512",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Discrete Diffusion Models",
    "Learnable Sampler Distillation",
    "Natural Language Processing",
    "High-Fidelity Samplers",
    "Adaptive Time Scheduling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Discrete Diffusion Models": 0.78,
    "Learnable Sampler Distillation": 0.8,
    "Natural Language Processing": 0.65,
    "High-Fidelity Samplers": 0.7,
    "Adaptive Time Scheduling": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Discrete Diffusion Models",
        "canonical": "Discrete Diffusion Models",
        "aliases": [
          "DDMs"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific technique central to the paper, offering a unique approach to discrete data generation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Learnable Sampler Distillation",
        "canonical": "Learnable Sampler Distillation",
        "aliases": [
          "LSD"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for improving sampling efficiency in DDMs, which is the core contribution of the paper.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.8
      },
      {
        "surface": "Text Generation",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "Text generation is a key application area for the proposed method, linking it to broader NLP research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.65
      },
      {
        "surface": "High-Fidelity Samplers",
        "canonical": "High-Fidelity Samplers",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific advancement in sampler design, crucial for the paper's proposed improvements.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Time Schedules",
        "canonical": "Adaptive Time Scheduling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Refers to the innovative scheduling method proposed to enhance the efficiency of sampling processes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.5,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "approach",
      "quality"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Discrete Diffusion Models",
      "resolved_canonical": "Discrete Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Learnable Sampler Distillation",
      "resolved_canonical": "Learnable Sampler Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Text Generation",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "High-Fidelity Samplers",
      "resolved_canonical": "High-Fidelity Samplers",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Time Schedules",
      "resolved_canonical": "Adaptive Time Scheduling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.5,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Learnable Sampler Distillation for Discrete Diffusion Models

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19962.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19962](https://arxiv.org/abs/2509.19962)

## 🔗 유사한 논문
- [[2025-09-25/Diffusion Curriculum_ Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion_20250925|Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion]] (83.2% similar)
- [[2025-09-24/SparseDiT_ Token Sparsification for Efficient Diffusion Transformer_20250924|SparseDiT: Token Sparsification for Efficient Diffusion Transformer]] (83.1% similar)
- [[2025-09-23/Reward-Weighted Sampling_ Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs_20250923|Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs]] (83.0% similar)
- [[2025-09-22/SAGE_ Semantic-Aware Shared Sampling for Efficient Diffusion_20250922|SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion]] (82.9% similar)
- [[2025-09-23/Absorb and Converge_ Provable Convergence Guarantee for Absorbing Discrete Diffusion Models_20250923|Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models]] (82.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**⚡ Unique Technical**: [[keywords/Discrete Diffusion Models|Discrete Diffusion Models]], [[keywords/Learnable Sampler Distillation|Learnable Sampler Distillation]], [[keywords/High-Fidelity Samplers|High-Fidelity Samplers]], [[keywords/Adaptive Time Scheduling|Adaptive Time Scheduling]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19962v1 Announce Type: new 
Abstract: Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}.

## 📝 요약

이 논문은 텍스트와 분자와 같은 이산 데이터 생성에 강력한 능력을 가진 이산 확산 모델(DDM)의 비효율적인 샘플링 문제를 해결하기 위해 학습 가능한 샘플러 증류(LSD) 방법을 제안합니다. LSD는 적은 단계로 고품질 샘플러의 중간 점수 궤적을 학습하여 샘플링 품질을 향상시킵니다. 또한, LSD+는 비균일한 시간 스케줄을 학습하여 샘플링 단계를 최적화합니다. 실험 결과, 제안된 방법은 기존 샘플러보다 적은 단계로 더 높은 샘플링 품질을 달성했습니다.

## 🎯 주요 포인트

- 1. 이산 확산 모델(DDMs)은 텍스트와 분자와 같은 이산 데이터 생성에서 강력한 성능을 보이지만, 비효율적인 샘플링으로 인해 실제 적용에 어려움이 있다.
- 2. 기존의 샘플링 가속화 방법은 큰 스텝 크기를 사용하여 생성 품질에 문제를 일으키며, 이는 예측 오류와 수치 근사화로 인한 오류를 증폭시킨다.
- 3. 제안된 학습 가능한 샘플러 증류(LSD)는 고품질의 교사 샘플러의 중간 점수 궤적과 학생 샘플러의 궤적을 정렬하여 빠르고 높은 충실도의 샘플러를 훈련한다.
- 4. LSD+는 비균일하게 스텝을 할당하는 시간 스케줄을 학습하여 샘플링 품질을 더욱 향상시킨다.
- 5. 실험 결과, 제안된 방법이 기존의 샘플러보다 적은 샘플링 스텝으로도 높은 샘플링 품질을 달성함을 보여준다.


---

*Generated on 2025-09-25 16:43:03*