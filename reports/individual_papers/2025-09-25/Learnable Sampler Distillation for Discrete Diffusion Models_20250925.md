---
keywords:
  - Discrete Diffusion Models
  - Learnable Sampler Distillation
  - Natural Language Processing
  - High-Fidelity Samplers
  - Adaptive Time Scheduling
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19962
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:43:03.116512",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Discrete Diffusion Models",
    "Learnable Sampler Distillation",
    "Natural Language Processing",
    "High-Fidelity Samplers",
    "Adaptive Time Scheduling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Discrete Diffusion Models": 0.78,
    "Learnable Sampler Distillation": 0.8,
    "Natural Language Processing": 0.65,
    "High-Fidelity Samplers": 0.7,
    "Adaptive Time Scheduling": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Discrete Diffusion Models",
        "canonical": "Discrete Diffusion Models",
        "aliases": [
          "DDMs"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific technique central to the paper, offering a unique approach to discrete data generation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Learnable Sampler Distillation",
        "canonical": "Learnable Sampler Distillation",
        "aliases": [
          "LSD"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for improving sampling efficiency in DDMs, which is the core contribution of the paper.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.8
      },
      {
        "surface": "Text Generation",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "Text generation is a key application area for the proposed method, linking it to broader NLP research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.65
      },
      {
        "surface": "High-Fidelity Samplers",
        "canonical": "High-Fidelity Samplers",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific advancement in sampler design, crucial for the paper's proposed improvements.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Time Schedules",
        "canonical": "Adaptive Time Scheduling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Refers to the innovative scheduling method proposed to enhance the efficiency of sampling processes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.5,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "approach",
      "quality"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Discrete Diffusion Models",
      "resolved_canonical": "Discrete Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Learnable Sampler Distillation",
      "resolved_canonical": "Learnable Sampler Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Text Generation",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "High-Fidelity Samplers",
      "resolved_canonical": "High-Fidelity Samplers",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Time Schedules",
      "resolved_canonical": "Adaptive Time Scheduling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.5,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Learnable Sampler Distillation for Discrete Diffusion Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19962.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19962](https://arxiv.org/abs/2509.19962)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Diffusion Curriculum_ Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion_20250925|Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion]] (83.2% similar)
- [[2025-09-24/SparseDiT_ Token Sparsification for Efficient Diffusion Transformer_20250924|SparseDiT: Token Sparsification for Efficient Diffusion Transformer]] (83.1% similar)
- [[2025-09-23/Reward-Weighted Sampling_ Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs_20250923|Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs]] (83.0% similar)
- [[2025-09-22/SAGE_ Semantic-Aware Shared Sampling for Efficient Diffusion_20250922|SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion]] (82.9% similar)
- [[2025-09-23/Absorb and Converge_ Provable Convergence Guarantee for Absorbing Discrete Diffusion Models_20250923|Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**âš¡ Unique Technical**: [[keywords/Discrete Diffusion Models|Discrete Diffusion Models]], [[keywords/Learnable Sampler Distillation|Learnable Sampler Distillation]], [[keywords/High-Fidelity Samplers|High-Fidelity Samplers]], [[keywords/Adaptive Time Scheduling|Adaptive Time Scheduling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19962v1 Announce Type: new 
Abstract: Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í…ìŠ¤íŠ¸ì™€ ë¶„ìì™€ ê°™ì€ ì´ì‚° ë°ì´í„° ìƒì„±ì— ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ê°€ì§„ ì´ì‚° í™•ì‚° ëª¨ë¸(DDM)ì˜ ë¹„íš¨ìœ¨ì ì¸ ìƒ˜í”Œë§ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•™ìŠµ ê°€ëŠ¥í•œ ìƒ˜í”ŒëŸ¬ ì¦ë¥˜(LSD) ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. LSDëŠ” ì ì€ ë‹¨ê³„ë¡œ ê³ í’ˆì§ˆ ìƒ˜í”ŒëŸ¬ì˜ ì¤‘ê°„ ì ìˆ˜ ê¶¤ì ì„ í•™ìŠµí•˜ì—¬ ìƒ˜í”Œë§ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë˜í•œ, LSD+ëŠ” ë¹„ê· ì¼í•œ ì‹œê°„ ìŠ¤ì¼€ì¤„ì„ í•™ìŠµí•˜ì—¬ ìƒ˜í”Œë§ ë‹¨ê³„ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ ìƒ˜í”ŒëŸ¬ë³´ë‹¤ ì ì€ ë‹¨ê³„ë¡œ ë” ë†’ì€ ìƒ˜í”Œë§ í’ˆì§ˆì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ì‚° í™•ì‚° ëª¨ë¸(DDMs)ì€ í…ìŠ¤íŠ¸ì™€ ë¶„ìì™€ ê°™ì€ ì´ì‚° ë°ì´í„° ìƒì„±ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë¹„íš¨ìœ¨ì ì¸ ìƒ˜í”Œë§ìœ¼ë¡œ ì¸í•´ ì‹¤ì œ ì ìš©ì— ì–´ë ¤ì›€ì´ ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ ìƒ˜í”Œë§ ê°€ì†í™” ë°©ë²•ì€ í° ìŠ¤í… í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„± í’ˆì§ˆì— ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ë©°, ì´ëŠ” ì˜ˆì¸¡ ì˜¤ë¥˜ì™€ ìˆ˜ì¹˜ ê·¼ì‚¬í™”ë¡œ ì¸í•œ ì˜¤ë¥˜ë¥¼ ì¦í­ì‹œí‚¨ë‹¤.
- 3. ì œì•ˆëœ í•™ìŠµ ê°€ëŠ¥í•œ ìƒ˜í”ŒëŸ¬ ì¦ë¥˜(LSD)ëŠ” ê³ í’ˆì§ˆì˜ êµì‚¬ ìƒ˜í”ŒëŸ¬ì˜ ì¤‘ê°„ ì ìˆ˜ ê¶¤ì ê³¼ í•™ìƒ ìƒ˜í”ŒëŸ¬ì˜ ê¶¤ì ì„ ì •ë ¬í•˜ì—¬ ë¹ ë¥´ê³  ë†’ì€ ì¶©ì‹¤ë„ì˜ ìƒ˜í”ŒëŸ¬ë¥¼ í›ˆë ¨í•œë‹¤.
- 4. LSD+ëŠ” ë¹„ê· ì¼í•˜ê²Œ ìŠ¤í…ì„ í• ë‹¹í•˜ëŠ” ì‹œê°„ ìŠ¤ì¼€ì¤„ì„ í•™ìŠµí•˜ì—¬ ìƒ˜í”Œë§ í’ˆì§ˆì„ ë”ìš± í–¥ìƒì‹œí‚¨ë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ì˜ ìƒ˜í”ŒëŸ¬ë³´ë‹¤ ì ì€ ìƒ˜í”Œë§ ìŠ¤í…ìœ¼ë¡œë„ ë†’ì€ ìƒ˜í”Œë§ í’ˆì§ˆì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-25 16:43:03*