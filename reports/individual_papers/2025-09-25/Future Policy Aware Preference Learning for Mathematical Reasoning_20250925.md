---
keywords:
  - Large Language Model
  - Preference Learning
  - Mathematical Reasoning
  - Future Policy Aware Preference Learning
  - SimPER
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.19893
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:46:36.552481",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Preference Learning",
    "Mathematical Reasoning",
    "Future Policy Aware Preference Learning",
    "SimPER"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Preference Learning": 0.8,
    "Mathematical Reasoning": 0.75,
    "Future Policy Aware Preference Learning": 0.8,
    "SimPER": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Essential for understanding the context of preference learning in the paper.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Preference Learning",
        "canonical": "Preference Learning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Central to the paper's contribution, focusing on improving mathematical reasoning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Mathematical Reasoning",
        "canonical": "Mathematical Reasoning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key application area where the proposed method is evaluated.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Future Policy Aware",
        "canonical": "Future Policy Aware Preference Learning",
        "aliases": [
          "FPA"
        ],
        "category": "unique_technical",
        "rationale": "The novel method proposed by the authors to enhance preference learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.55,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "SimPER",
        "canonical": "SimPER",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "One of the methods evaluated with significant performance gains using FPA.",
        "novelty_score": 0.7,
        "connectivity_score": 0.5,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "trajectory",
      "probability",
      "regularization"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Preference Learning",
      "resolved_canonical": "Preference Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Mathematical Reasoning",
      "resolved_canonical": "Mathematical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Future Policy Aware",
      "resolved_canonical": "Future Policy Aware Preference Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.55,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "SimPER",
      "resolved_canonical": "SimPER",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.5,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Future Policy Aware Preference Learning for Mathematical Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19893.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.19893](https://arxiv.org/abs/2509.19893)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/AAPO_ Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum_20250925|AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum]] (85.7% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (85.3% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (84.9% similar)
- [[2025-09-24/Exploiting Tree Structure for Credit Assignment in RL Training of LLMs_20250924|Exploiting Tree Structure for Credit Assignment in RL Training of LLMs]] (84.7% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Preference Learning|Preference Learning]], [[keywords/Mathematical Reasoning|Mathematical Reasoning]], [[keywords/Future Policy Aware Preference Learning|Future Policy Aware Preference Learning]], [[keywords/SimPER|SimPER]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19893v1 Announce Type: new 
Abstract: Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ Future Policy Aware (FPA) ì„ í˜¸ í•™ìŠµ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ì„ í˜¸ë˜ì§€ ì•ŠëŠ” ê²½ë¡œì˜ í™•ë¥ ì„ ë‚®ì¶”ëŠ” ê³¼ì •ì—ì„œ ìœ ìš©í•œ í† í°ë„ ê³¼ë„í•˜ê²Œ íŒ¨ë„í‹°ë¥¼ ë°›ì•„ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. FPAëŠ” í˜„ì¬ ì •ì±… ëŒ€ì‹  ë¯¸ë˜ ì •ì±…ì„ ì •ê·œí™” í•­ì— ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì ì¬ì ìœ¼ë¡œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ì „ì— ì •ê·œí™”í•˜ê³ , ìœ ìš©í•œ ìˆ˜í•™ì  í† í°ì˜ í™•ë¥ ì„ ìœ ì§€í•˜ë©´ì„œ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤. MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ì—ì„œ FPAë¥¼ ì ìš©í•œ ê²°ê³¼, íŠ¹íˆ SimPERì—ì„œ ìµœëŒ€ 5.75%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. FPAëŠ” ìµœì†Œí•œì˜ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ë” ê¸´ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì½”ë“œ ê³µê°œë¥¼ ì˜ˆì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Direct Preference Optimization(DPO)ì™€ ê°™ì€ ì„ í˜¸ í•™ìŠµ ë°©ë²•ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) í›„ì† í›ˆë ¨ì— í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ì•˜ì§€ë§Œ, ìˆ˜í•™ì  ì¶”ë¡ ì—ëŠ” ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.
- 2. ì„ í˜¸ ë° ë¹„ì„ í˜¸ ê²½ë¡œ ê°„ì˜ í° í† í° ì¤‘ë³µì´ ë¬¸ì œë¡œ, ë¹„ì„ í˜¸ ê²½ë¡œì˜ í™•ë¥ ì„ ë‚®ì¶”ë©´ ìœ ìš©í•œ í† í°ì˜ í™•ë¥ ë„ ê°ì†Œí•˜ì—¬ ê³¼ë„í•œ íŒ¨ë„í‹°ì™€ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤.
- 3. Future Policy Aware(FPA) ì„ í˜¸ í•™ìŠµì€ í˜„ì¬ ì •ì±… ëŒ€ì‹  ë¯¸ë˜ ì •ì±…ì„ ì •ê·œí™” í•­ì— ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 4. FPAëŠ” DPO, RPO, SimPERì— ì ìš©ë˜ì–´ MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ SimPERì—ì„œ ìµœëŒ€ 5.75%ì˜ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 5. FPAëŠ” ìœ ìš©í•œ ìˆ˜í•™ì  í† í°ì˜ í™•ë¥ ì„ ë³´ì¡´í•˜ë©´ì„œë„, ì ì¬ì ìœ¼ë¡œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ì „ì— ì •ê·œí™”í•˜ì—¬ ì•ˆì „í•œ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:46:36*