---
keywords:
  - Large Language Model Benchmarks
  - Psychometric Validity
  - ELO Aggregation
  - Schema Incoherence
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20293
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:05:51.430521",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model Benchmarks",
    "Psychometric Validity",
    "ELO Aggregation",
    "Schema Incoherence"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model Benchmarks": 0.78,
    "Psychometric Validity": 0.74,
    "ELO Aggregation": 0.71,
    "Schema Incoherence": 0.69
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LLM-judged benchmarks",
        "canonical": "Large Language Model Benchmarks",
        "aliases": [
          "LLM Benchmarks"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to discussions on evaluation frameworks for language models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "psychometric validity",
        "canonical": "Psychometric Validity",
        "aliases": [
          "validity signals"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel evaluation metric for benchmarking reliability.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.81,
        "link_intent_score": 0.74
      },
      {
        "surface": "ELO-style aggregation",
        "canonical": "ELO Aggregation",
        "aliases": [
          "ELO ranking"
        ],
        "category": "unique_technical",
        "rationale": "Links to ranking systems used in competitive settings.",
        "novelty_score": 0.68,
        "connectivity_score": 0.73,
        "specificity_score": 0.77,
        "link_intent_score": 0.71
      },
      {
        "surface": "schema incoherence",
        "canonical": "Schema Incoherence",
        "aliases": [
          "incoherent schema"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific design flaw impacting benchmark validity.",
        "novelty_score": 0.7,
        "connectivity_score": 0.66,
        "specificity_score": 0.75,
        "link_intent_score": 0.69
      }
    ],
    "ban_list_suggestions": [
      "benchmark rankings",
      "high-confidence rankings",
      "evaluation schema"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LLM-judged benchmarks",
      "resolved_canonical": "Large Language Model Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "psychometric validity",
      "resolved_canonical": "Psychometric Validity",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.81,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "ELO-style aggregation",
      "resolved_canonical": "ELO Aggregation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.73,
        "specificity": 0.77,
        "link_intent": 0.71
      }
    },
    {
      "candidate_surface": "schema incoherence",
      "resolved_canonical": "Schema Incoherence",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.66,
        "specificity": 0.75,
        "link_intent": 0.69
      }
    }
  ]
}
-->

# When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20293.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20293](https://arxiv.org/abs/2509.20293)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (84.8% similar)
- [[2025-09-23/LLMsPark_ A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts_20250923|LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts]] (84.3% similar)
- [[2025-09-25/Do Before You Judge_ Self-Reference as a Pathway to Better LLM Evaluation_20250925|Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation]] (84.3% similar)
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (83.7% similar)
- [[2025-09-23/SATBench_ Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas_20250923|SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Large Language Model Benchmarks|Large Language Model Benchmarks]]
**âš¡ Unique Technical**: [[keywords/Psychometric Validity|Psychometric Validity]], [[keywords/ELO Aggregation|ELO Aggregation]], [[keywords/Schema Incoherence|Schema Incoherence]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20293v1 Announce Type: cross 
Abstract: LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) ê¸°ë°˜ í‰ê°€ ê¸°ì¤€ì˜ ì„¤ê³„ ë¬¸ì œë¥¼ ì§€ì í•˜ë©°, ê¸°ì¡´ì˜ ëª…í™•í•œ ì •ë‹µ ê¸°ë°˜ í‰ê°€ì™€ ë‹¬ë¦¬ ë†’ì€ ë¶ˆí™•ì‹¤ì„±ì„ ë‚´í¬í•  ìˆ˜ ìˆìŒì„ ì£¼ì¥í•©ë‹ˆë‹¤. ì €ìë“¤ì€ í‰ê°€ ê¸°ì¤€ì˜ ì¼ê´€ì„±ì„ ì¸¡ì •í•˜ëŠ” 'ë„ì‹ì  ì¤€ìˆ˜'ì™€ ì‹¬ë¦¬ì¸¡ì •í•™ì  íƒ€ë‹¹ì„±ì„ í™œìš©í•˜ì—¬ í‰ê°€ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì§„ë‹¨í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ Arena-Hard Autoì— ì ìš©í•œ ê²°ê³¼, í‰ê°€ ê¸°ì¤€ì˜ ì¼ê´€ì„± ë¶€ì¡±ê³¼ ë†’ì€ ìƒê´€ê´€ê³„ë¡œ ì¸í•œ í‰ê°€ ì‹ ë¢°ì„± ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•˜ë©°, ê´€ë ¨ ì½”ë“œë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLM-judged ë²¤ì¹˜ë§ˆí¬ëŠ” ë³µì¡í•œ ëª¨ë¸ í–‰ë™ í‰ê°€ì— ì‚¬ìš©ë˜ì§€ë§Œ, ê¸°ì¡´ì˜ ì§„ì‹¤ ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ì™€ëŠ” ë‹¤ë¥¸ ì‹¤íŒ¨ ëª¨ë“œë¥¼ ê°€ì§€ê³  ìˆë‹¤.
- 2. ëª…í™•í•œ ëª©í‘œì™€ ê²€ì¦ ê°€ëŠ¥í•œ êµ¬ì¡°ê°€ ì—†ìœ¼ë©´ ë²¤ì¹˜ë§ˆí¬ ìˆœìœ„ëŠ” ì‹¤ì œë¡œëŠ” ë…¸ì´ì¦ˆì— ë¶ˆê³¼í•œ ë†’ì€ ì‹ ë¢°ë„ì˜ ìˆœìœ„ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.
- 3. ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„±ì€ íŒì‚¬ì˜ í‰ê°€ê°€ ëª…ì‹œëœ í‰ê°€ ìŠ¤í‚¤ë§ˆì— ì˜í•´ ì–¼ë§ˆë‚˜ ì„¤ëª…ë˜ëŠ”ì§€ë¥¼ ì •ëŸ‰í™”í•˜ì—¬, íŒì‚¬ê°€ ìì‹ ì˜ ê¸°ì¤€ì—ì„œ ë²—ì–´ë‚  ë•Œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ë³€ë™ì„±ì„ ë“œëŸ¬ë‚¸ë‹¤.
- 4. ì‹¬ë¦¬ì¸¡ì •í•™ì  íƒ€ë‹¹ì„±ì€ ë‚´ë¶€ ì¼ê´€ì„±ê³¼ ë³€ë³„ íƒ€ë‹¹ì„± ì‹ í˜¸ë¥¼ ì§‘ê³„í•˜ì—¬ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰ì—ì„œ ì¤„ì¼ ìˆ˜ ì—†ëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•œë‹¤.
- 5. Arena-Hard Autoì— ì´ ë„êµ¬ë“¤ì„ ì ìš©í•œ ê²°ê³¼, ì¸ê¸° ìˆëŠ” íŒì‚¬ë“¤ ì‚¬ì´ì—ì„œ ì‹¬ê°í•œ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ì™€ ìš”ì†Œ ë¶•ê´´ê°€ ë°œê²¬ë˜ì—ˆìœ¼ë©°, ELO ìŠ¤íƒ€ì¼ì˜ ì§‘ê³„ ë°©ì‹ì´ ì§„ì •í•œ ìˆœìœ„ ë¶ˆí™•ì‹¤ì„±ì„ ì€íí•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-25 16:05:51*