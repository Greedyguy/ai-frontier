---
keywords:
  - Large Language Model
  - Reinforcement Learning from Human Feedback
  - Group Relative Policy Optimization
  - Domain-Informed Self-Consistency Policy Optimization
  - Domain-aware reward scaling
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2505.15074
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:25:19.161174",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning from Human Feedback",
    "Group Relative Policy Optimization",
    "Domain-Informed Self-Consistency Policy Optimization",
    "Domain-aware reward scaling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning from Human Feedback": 0.78,
    "Group Relative Policy Optimization": 0.8,
    "Domain-Informed Self-Consistency Policy Optimization": 0.82,
    "Domain-aware reward scaling": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus and aligns with existing vocabulary, facilitating connections to related works in language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.92,
        "specificity_score": 0.68,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning from Human Feedback",
        "canonical": "Reinforcement Learning from Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "unique_technical",
        "rationale": "This concept is a unique approach discussed in the paper, offering a specific method for aligning models with human preferences.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "GRPO is a specific method critiqued and expanded upon in the paper, making it a key term for understanding the proposed improvements.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Domain-Informed Self-Consistency Policy Optimization",
        "canonical": "Domain-Informed Self-Consistency Policy Optimization",
        "aliases": [
          "DISCO"
        ],
        "category": "unique_technical",
        "rationale": "DISCO is the novel method introduced in the paper, central to its contributions and findings.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "Domain-aware reward scaling",
        "canonical": "Domain-aware reward scaling",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This technique is a key innovation in the paper, addressing domain imbalance and enhancing policy learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.92,
        "specificity": 0.68,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Human Feedback",
      "resolved_canonical": "Reinforcement Learning from Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Domain-Informed Self-Consistency Policy Optimization",
      "resolved_canonical": "Domain-Informed Self-Consistency Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Domain-aware reward scaling",
      "resolved_canonical": "Domain-aware reward scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.15074.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2505.15074](https://arxiv.org/abs/2505.15074)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Single-stream Policy Optimization_20250924|Single-stream Policy Optimization]] (85.4% similar)
- [[2025-09-25/Stepwise Guided Policy Optimization_ Coloring your Incorrect Reasoning in GRPO_20250925|Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO]] (85.2% similar)
- [[2025-09-24/NGRPO_ Negative-enhanced Group Relative Policy Optimization_20250924|NGRPO: Negative-enhanced Group Relative Policy Optimization]] (85.2% similar)
- [[2025-09-25/Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment_20250925|Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment]] (84.8% similar)
- [[2025-09-24/DRO-REBEL_ Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment_20250924|DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment]] (84.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Domain-aware reward scaling|Domain-aware reward scaling]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning from Human Feedback|Reinforcement Learning from Human Feedback]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]], [[keywords/Domain-Informed Self-Consistency Policy Optimization|Domain-Informed Self-Consistency Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.15074v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups, assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks. Our code and data are available at https://github.com/Tonyzhou98/disco_grpo.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ(RLHF)ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ GRPO ë°©ë²•ì€ ê°„ë‹¨í•˜ê³  ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ì§€ë§Œ, ë„ë©”ì¸ ë¶ˆê· í˜• ë¬¸ì œë¡œ ì¸í•´ íŠ¹ì • ë„ë©”ì¸ì— ì¹˜ìš°ì¹˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ DISCOëŠ” ë„ë©”ì¸ ì¸ì‹ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ê³¼ ë‚œì´ë„ ì¸ì‹ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ë¶ˆê· í˜•ì„ ê°œì„ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DISCOëŠ” ì—¬ëŸ¬ LLMì—ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, íŠ¹íˆ Qwen3 ëª¨ë¸ì—ì„œ ê¸°ì¡´ GRPO ë³€í˜•ë³´ë‹¤ 5% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ì¤‘ ë„ë©”ì¸ ì •ë ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GRPOëŠ” í•™ìŠµëœ ê°€ì¹˜ í•¨ìˆ˜ ì—†ì´ë„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë„ë©”ì¸ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ ëª»í•´ ì¼ë°˜í™”ì™€ ê³µì •ì„±ì—ì„œ í•œê³„ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.
- 2. DISCOëŠ” GRPOì˜ ë„ë©”ì¸ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë„ë©”ì¸ ì¸ì‹ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ê³¼ ë‚œì´ë„ ì¸ì‹ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 3. ë„ë©”ì¸ ì¸ì‹ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ì€ ë„ë©”ì¸ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”ë¥¼ ì¬ì¡°ì •í•˜ì—¬ ë¹ˆë„ í¸í–¥ì„ ìƒì‡„í•©ë‹ˆë‹¤.
- 4. ë‚œì´ë„ ì¸ì‹ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ì€ í”„ë¡¬í”„íŠ¸ ìˆ˜ì¤€ì˜ ìê¸° ì¼ê´€ì„±ì„ í™œìš©í•´ ë¶ˆí™•ì‹¤í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹ë³„í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
- 5. DISCOëŠ” ì—¬ëŸ¬ LLMê³¼ ë¶ˆê· í˜•í•œ í•™ìŠµ ë¶„í¬ì—ì„œ ì¼ë°˜í™”ë¥¼ ê°œì„ í•˜ê³ , Qwen3 ëª¨ë¸ì—ì„œ ê¸°ì¡´ GRPO ë³€í˜•ë³´ë‹¤ 5% ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:25:19*