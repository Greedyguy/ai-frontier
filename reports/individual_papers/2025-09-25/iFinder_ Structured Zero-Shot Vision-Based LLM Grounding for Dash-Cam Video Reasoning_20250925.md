---
keywords:
  - Zero-Shot Learning
  - Vision-Language Model
  - Dash-Cam Video Analysis
  - Semantic Grounding Framework
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19552
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:59:38.740258",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Zero-Shot Learning",
    "Vision-Language Model",
    "Dash-Cam Video Analysis",
    "Semantic Grounding Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Zero-Shot Learning": 0.82,
    "Vision-Language Model": 0.79,
    "Dash-Cam Video Analysis": 0.78,
    "Semantic Grounding Framework": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Zero-Shot",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending approach relevant to the paper's focus on domain-specific video analysis without additional training.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "V-VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's discussion on integrating vision and language for video reasoning.",
        "novelty_score": 0.58,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "Dash-Cam Video Analysis",
        "canonical": "Dash-Cam Video Analysis",
        "aliases": [
          "Dash-Cam Analysis"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique application area for the proposed framework, highlighting its specificity and novelty.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Semantic Grounding Framework",
        "canonical": "Semantic Grounding Framework",
        "aliases": [
          "Grounding Framework"
        ],
        "category": "unique_technical",
        "rationale": "The framework is a novel contribution of the paper, offering a structured approach to video reasoning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Zero-Shot",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Dash-Cam Video Analysis",
      "resolved_canonical": "Dash-Cam Video Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Semantic Grounding Framework",
      "resolved_canonical": "Semantic Grounding Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19552.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19552](https://arxiv.org/abs/2509.19552)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/VideoRFT_ Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning_20250923|VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning]] (84.6% similar)
- [[2025-09-23/Retrieval Enhanced Feedback via In-context Neural Error-book_20250923|Retrieval Enhanced Feedback via In-context Neural Error-book]] (84.4% similar)
- [[2025-09-24/Knowledge Transfer from Interaction Learning_20250924|Knowledge Transfer from Interaction Learning]] (83.9% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (83.3% similar)
- [[2025-09-23/Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?_20250923|Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Dash-Cam Video Analysis|Dash-Cam Video Analysis]], [[keywords/Semantic Grounding Framework|Semantic Grounding Framework]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19552v1 Announce Type: new 
Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ëŒ€ì‹œìº  ìš´ì „ ì˜ìƒ ë¶„ì„ê³¼ ê°™ì€ íŠ¹ì • ë„ë©”ì¸ ì‘ì—…ì— ì ìš©í•˜ëŠ” ì–´ë ¤ì›€ì„ í•´ê²°í•˜ê¸° ìœ„í•´ iFinderë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. iFinderëŠ” ëŒ€ì‹œìº  ì˜ìƒì„ ê³„ì¸µì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ ë°ì´í„° êµ¬ì¡°ë¡œ ë³€í™˜í•˜ì—¬ ì¸ì‹ê³¼ ì¶”ë¡ ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ëœ ë¹„ì „ ëª¨ë¸ì„ ì‚¬ìš©í•´ ê°ì²´ì˜ ìì„¸, ì°¨ì„  ìœ„ì¹˜, ê°ì²´ ê¶¤ì  ë“±ì˜ ì¤‘ìš”í•œ ë‹¨ì„œë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì¡°ì§í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ì „ëµì„ í†µí•´ LLMì´ V-VLMì˜ ì¶œë ¥ì„ ê°œì„ í•˜ê³  ì •í™•í•œ ì¶”ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤. ë„¤ ê°€ì§€ ëŒ€ì‹œìº  ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ì„œ iFinderëŠ” ê°ì²´ ë°©í–¥ê³¼ ì „ì—­ ì»¨í…ìŠ¤íŠ¸ì™€ ê°™ì€ ë„ë©”ì¸ íŠ¹í™” ë‹¨ì„œë¥¼ í™œìš©í•˜ì—¬ ì‚¬ê³  ì¶”ë¡  ì •í™•ë„ë¥¼ ìµœëŒ€ 39% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” ìš´ì „ ë„ë©”ì¸ íŠ¹í™” í‘œí˜„ì„ í†µí•´ LLMì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í•´ì„ ê°€ëŠ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëŒ€ì•ˆì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ì‹œìº  ìš´ì „ ë¹„ë””ì˜¤ ë¶„ì„ì„ ìœ„í•œ iFinderëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì§€ê°ê³¼ ì¶”ë¡ ì„ ë¶„ë¦¬í•˜ì—¬ êµ¬ì¡°í™”ëœ ì˜ë¯¸ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 2. iFinderëŠ” ì‚¬ì „ í•™ìŠµëœ ë¹„ì „ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê°ì²´ ìì„¸, ì°¨ì„  ìœ„ì¹˜, ê°ì²´ ê¶¤ì  ë“±ì˜ ì¤‘ìš”í•œ ë‹¨ì„œë¥¼ ì¶”ì¶œí•˜ê³  ì´ë¥¼ ê³„ì¸µì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
- 3. ì„¸ ê°€ì§€ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ì „ëµì„ í†µí•´ LLMì´ ë‹¨ê³„ë³„ë¡œ ê·¼ê±° ìˆëŠ” ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì—¬ V-VLMì˜ ì¶œë ¥ì„ ê°œì„ í•˜ê³  ì •í™•í•œ ì¶”ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. iFinderëŠ” ë„¤ ê°€ì§€ ëŒ€ì‹œìº  ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê°ì²´ ë°©í–¥ì„±ê³¼ ê¸€ë¡œë²Œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ê³  ì¶”ë¡  ì •í™•ë„ë¥¼ ìµœëŒ€ 39% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 5. iFinderëŠ” ìš´ì „ ë„ë©”ì¸ì— íŠ¹í™”ëœ í‘œí˜„ì„ í†µí•´ LLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ í•´ì„ ê°€ëŠ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì œë¡œìƒ· ëŒ€ì•ˆ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:59:38*