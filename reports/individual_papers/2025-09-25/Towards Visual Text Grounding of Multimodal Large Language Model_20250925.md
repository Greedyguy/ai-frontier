---
keywords:
  - Multimodal Learning
  - Text-Rich Image Grounding
  - Document Question-Answering
  - Visual Text Grounding
  - Spatial Reasoning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2504.04974
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:22:56.087880",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Text-Rich Image Grounding",
    "Document Question-Answering",
    "Visual Text Grounding",
    "Spatial Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Text-Rich Image Grounding": 0.79,
    "Document Question-Answering": 0.77,
    "Visual Text Grounding": 0.75,
    "Spatial Reasoning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects with existing work on integrating multiple data modalities in language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Text-Rich Image Grounding",
        "canonical": "Text-Rich Image Grounding",
        "aliases": [
          "TRIG"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel task that addresses a specific gap in current multimodal research.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Document Question-Answering",
        "canonical": "Document Question-Answering",
        "aliases": [
          "DocQA"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specialized application of language models in processing document images.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.77
      },
      {
        "surface": "Visual Text Grounding",
        "canonical": "Visual Text Grounding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Addresses the challenge of aligning visual and textual data, crucial for multimodal models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.75
      },
      {
        "surface": "Spatial Reasoning",
        "canonical": "Spatial Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for understanding and processing the layout of text-rich images.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "benchmark",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Text-Rich Image Grounding",
      "resolved_canonical": "Text-Rich Image Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Document Question-Answering",
      "resolved_canonical": "Document Question-Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Visual Text Grounding",
      "resolved_canonical": "Visual Text Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Spatial Reasoning",
      "resolved_canonical": "Spatial Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Towards Visual Text Grounding of Multimodal Large Language Model

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2504.04974.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2504.04974](https://arxiv.org/abs/2504.04974)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (87.9% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.5% similar)
- [[2025-09-23/From Easy to Hard_ The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning_20250923|From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning]] (86.4% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (85.9% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Spatial Reasoning|Spatial Reasoning]]
**âš¡ Unique Technical**: [[keywords/Text-Rich Image Grounding|Text-Rich Image Grounding]], [[keywords/Document Question-Answering|Document Question-Answering]], [[keywords/Visual Text Grounding|Visual Text Grounding]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.04974v2 Announce Type: replace-cross 
Abstract: Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì´ ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œì˜ ì‹œê°ì  í…ìŠ¤íŠ¸ ì—°ê²°ì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ëŠ” ìì—° ì´ë¯¸ì§€ì— ì¤‘ì ì„ ë‘ê³  ìˆì–´, í…ìŠ¤íŠ¸ê°€ ë§ì€ ë¬¸ì„œ ì´ë¯¸ì§€ì˜ ë³µì¡í•œ ë ˆì´ì•„ì›ƒê³¼ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶©ë¶„íˆ ë‹¤ë£¨ì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ë¬¸ì„œ ì§ˆë¬¸-ì‘ë‹µì—ì„œ MLLMsì˜ í…ìŠ¤íŠ¸-ë¦¬ì¹˜ ì´ë¯¸ì§€ ì—°ê²° ëŠ¥ë ¥ì„ í‰ê°€í•˜ê³  ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê³¼ì œì¸ TRIGë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. OCR-LLM-ì¸ê°„ ìƒí˜¸ì‘ìš© íŒŒì´í”„ë¼ì¸ì„ í†µí•´ 800ê°œì˜ ìˆ˜ë™ ì£¼ì„ ì§ˆë¬¸-ì‘ë‹µ ìŒê³¼ 90ê°œì˜ í•©ì„± ë°ì´í„°ë¥¼ í¬í•¨í•œ ëŒ€ê·œëª¨ í•™ìŠµ ì„¸íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ MLLMsë¥¼ í‰ê°€í•œ ê²°ê³¼, í…ìŠ¤íŠ¸-ë¦¬ì¹˜ ì´ë¯¸ì§€ì—ì„œì˜ ì—°ê²° ëŠ¥ë ¥ì— ìƒë‹¹í•œ í•œê³„ê°€ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ì¸ ì§€ì‹œ ì¡°ì •ê³¼ í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ íš¨ìœ¨ì  ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‘ ê°€ì§€ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ TRIG ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. í•©ì„± ë°ì´í„°ì…‹ìœ¼ë¡œ MLLMsë¥¼ ë¯¸ì„¸ ì¡°ì •í•¨ìœ¼ë¡œì¨ ê³µê°„ì  ì¶”ë¡  ë° ì—°ê²° ëŠ¥ë ¥ì´ ìœ ë§í•˜ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ë¬¸ì„œ ì´ë¯¸ì§€ì˜ ë³µì¡í•œ ë ˆì´ì•„ì›ƒê³¼ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¡œ ì¸í•´ ì‹œê°ì  í…ìŠ¤íŠ¸ ê·¸ë¼ìš´ë”©ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤.
- 2. ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ëŠ” ìì—° ì´ë¯¸ì§€ì— ì¤‘ì ì„ ë‘ê³  ìˆì–´ í…ìŠ¤íŠ¸ê°€ í’ë¶€í•œ ë¬¸ì„œ ì´ë¯¸ì§€ì˜ ë¬¸ì œë¥¼ ì¶©ë¶„íˆ ë‹¤ë£¨ì§€ ëª»í•˜ê³  ìˆë‹¤.
- 3. TRIGë¼ëŠ” ìƒˆë¡œìš´ ê³¼ì œë¥¼ ë„ì…í•˜ì—¬ ë¬¸ì„œ ì§ˆë¬¸-ì‘ë‹µì—ì„œ MLLMsì˜ í…ìŠ¤íŠ¸-ë¦¬ì¹˜ ì´ë¯¸ì§€ ê·¸ë¼ìš´ë”© ëŠ¥ë ¥ì„ í‰ê°€í•˜ê³  ê°œì„ í•˜ê³ ì í•œë‹¤.
- 4. OCR-LLM-ì¸ê°„ ìƒí˜¸ì‘ìš© íŒŒì´í”„ë¼ì¸ì„ í†µí•´ 800ê°œì˜ ìˆ˜ë™ ì£¼ì„ì´ ë‹¬ë¦° ì§ˆë¬¸-ì‘ë‹µ ìŒê³¼ 90ê°œì˜ í•©ì„± ë°ì´í„°ë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ í•™ìŠµ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ì˜€ë‹¤.
- 5. ì œì•ˆëœ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ì–‘í•œ MLLMsì˜ í‰ê°€ ê²°ê³¼, í…ìŠ¤íŠ¸ê°€ í’ë¶€í•œ ì´ë¯¸ì§€ì—ì„œì˜ ê·¸ë¼ìš´ë”© ëŠ¥ë ¥ì— ìƒë‹¹í•œ í•œê³„ê°€ ìˆìŒì„ ë“œëŸ¬ëƒˆë‹¤.


---

*Generated on 2025-09-25 16:22:56*