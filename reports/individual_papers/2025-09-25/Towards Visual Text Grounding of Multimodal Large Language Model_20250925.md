---
keywords:
  - Multimodal Learning
  - Text-Rich Image Grounding
  - Document Question-Answering
  - Visual Text Grounding
  - Spatial Reasoning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2504.04974
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:22:56.087880",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Text-Rich Image Grounding",
    "Document Question-Answering",
    "Visual Text Grounding",
    "Spatial Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Text-Rich Image Grounding": 0.79,
    "Document Question-Answering": 0.77,
    "Visual Text Grounding": 0.75,
    "Spatial Reasoning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects with existing work on integrating multiple data modalities in language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Text-Rich Image Grounding",
        "canonical": "Text-Rich Image Grounding",
        "aliases": [
          "TRIG"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel task that addresses a specific gap in current multimodal research.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Document Question-Answering",
        "canonical": "Document Question-Answering",
        "aliases": [
          "DocQA"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specialized application of language models in processing document images.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.77
      },
      {
        "surface": "Visual Text Grounding",
        "canonical": "Visual Text Grounding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Addresses the challenge of aligning visual and textual data, crucial for multimodal models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.75
      },
      {
        "surface": "Spatial Reasoning",
        "canonical": "Spatial Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for understanding and processing the layout of text-rich images.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "benchmark",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Text-Rich Image Grounding",
      "resolved_canonical": "Text-Rich Image Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Document Question-Answering",
      "resolved_canonical": "Document Question-Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Visual Text Grounding",
      "resolved_canonical": "Visual Text Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Spatial Reasoning",
      "resolved_canonical": "Spatial Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Towards Visual Text Grounding of Multimodal Large Language Model

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2504.04974.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2504.04974](https://arxiv.org/abs/2504.04974)

## 🔗 유사한 논문
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (87.9% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.5% similar)
- [[2025-09-23/From Easy to Hard_ The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning_20250923|From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning]] (86.4% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (85.9% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (85.5% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Spatial Reasoning|Spatial Reasoning]]
**⚡ Unique Technical**: [[keywords/Text-Rich Image Grounding|Text-Rich Image Grounding]], [[keywords/Document Question-Answering|Document Question-Answering]], [[keywords/Visual Text Grounding|Visual Text Grounding]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2504.04974v2 Announce Type: replace-cross 
Abstract: Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.

## 📝 요약

이 논문은 다중 모달 대형 언어 모델(MLLMs)이 문서 이미지에서의 시각적 텍스트 연결에 어려움을 겪는 문제를 해결하고자 합니다. 기존 벤치마크는 자연 이미지에 중점을 두고 있어, 텍스트가 많은 문서 이미지의 복잡한 레이아웃과 텍스트 콘텐츠를 충분히 다루지 못합니다. 이를 해결하기 위해, 저자들은 문서 질문-응답에서 MLLMs의 텍스트-리치 이미지 연결 능력을 평가하고 개선하기 위한 새로운 과제인 TRIG를 소개합니다. OCR-LLM-인간 상호작용 파이프라인을 통해 800개의 수동 주석 질문-응답 쌍과 90개의 합성 데이터를 포함한 대규모 학습 세트를 생성했습니다. 다양한 MLLMs를 평가한 결과, 텍스트-리치 이미지에서의 연결 능력에 상당한 한계가 있음을 발견했습니다. 이를 개선하기 위해 일반적인 지시 조정과 플러그 앤 플레이 효율적 임베딩을 기반으로 한 두 가지 간단하고 효과적인 TRIG 방법을 제안했습니다. 합성 데이터셋으로 MLLMs를 미세 조정함으로써 공간적 추론 및 연결 능력이 유망하게 개선되었습니다.

## 🎯 주요 포인트

- 1. 멀티모달 대형 언어 모델(MLLMs)은 문서 이미지의 복잡한 레이아웃과 텍스트 콘텐츠로 인해 시각적 텍스트 그라운딩에 어려움을 겪고 있다.
- 2. 기존 벤치마크는 자연 이미지에 중점을 두고 있어 텍스트가 풍부한 문서 이미지의 문제를 충분히 다루지 못하고 있다.
- 3. TRIG라는 새로운 과제를 도입하여 문서 질문-응답에서 MLLMs의 텍스트-리치 이미지 그라운딩 능력을 평가하고 개선하고자 한다.
- 4. OCR-LLM-인간 상호작용 파이프라인을 통해 800개의 수동 주석이 달린 질문-응답 쌍과 90개의 합성 데이터로 구성된 대규모 학습 세트를 생성하였다.
- 5. 제안된 벤치마크에서 다양한 MLLMs의 평가 결과, 텍스트가 풍부한 이미지에서의 그라운딩 능력에 상당한 한계가 있음을 드러냈다.


---

*Generated on 2025-09-25 16:22:56*