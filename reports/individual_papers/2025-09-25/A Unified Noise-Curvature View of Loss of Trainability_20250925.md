---
keywords:
  - Loss of Trainability
  - Gradient Noise
  - Curvature Volatility
  - Wasserstein Regularization
  - L2 Weight Decay
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19698
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:45:21.742803",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Loss of Trainability",
    "Gradient Noise",
    "Curvature Volatility",
    "Wasserstein Regularization",
    "L2 Weight Decay"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Loss of Trainability": 0.78,
    "Gradient Noise": 0.8,
    "Curvature Volatility": 0.75,
    "Wasserstein Regularization": 0.79,
    "L2 Weight Decay": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Loss of Trainability",
        "canonical": "Loss of Trainability",
        "aliases": [
          "LoT"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's analysis and represents a specific challenge in continual learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Gradient Noise",
        "canonical": "Gradient Noise",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Gradient noise is a key factor in the proposed predictive threshold for trainability.",
        "novelty_score": 0.55,
        "connectivity_score": 0.72,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Curvature Volatility",
        "canonical": "Curvature Volatility",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel concept introduced in the paper to control trainability through a predictive threshold.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "Wasserstein Regularization",
        "canonical": "Wasserstein Regularization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Wasserstein regularization is used in the paper's experiments to stabilize training.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      },
      {
        "surface": "L2 Weight Decay",
        "canonical": "L2 Weight Decay",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "L2 weight decay is a common technique referenced in the paper to improve accuracy.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "optimization lens",
      "adequate capacity",
      "supervision"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Loss of Trainability",
      "resolved_canonical": "Loss of Trainability",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Gradient Noise",
      "resolved_canonical": "Gradient Noise",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.72,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Curvature Volatility",
      "resolved_canonical": "Curvature Volatility",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Wasserstein Regularization",
      "resolved_canonical": "Wasserstein Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "L2 Weight Decay",
      "resolved_canonical": "L2 Weight Decay",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# A Unified Noise-Curvature View of Loss of Trainability

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19698.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19698](https://arxiv.org/abs/2509.19698)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (82.6% similar)
- [[2025-09-23/Stabilizing Information Flow Entropy_ Regularization for Safe and Interpretable Autonomous Driving Perception_20250923|Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception]] (81.6% similar)
- [[2025-09-24/Stability and Generalization of Adversarial Diffusion Training_20250924|Stability and Generalization of Adversarial Diffusion Training]] (81.5% similar)
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM: Hierarchical Adapter Merging for Scalable Continual Learning]] (81.5% similar)
- [[2025-09-23/On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm_20250923|On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Gradient Noise|Gradient Noise]], [[keywords/Wasserstein Regularization|Wasserstein Regularization]], [[keywords/L2 Weight Decay|L2 Weight Decay]]
**âš¡ Unique Technical**: [[keywords/Loss of Trainability|Loss of Trainability]], [[keywords/Curvature Volatility|Curvature Volatility]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19698v1 Announce Type: cross 
Abstract: Loss of trainability (LoT) in continual learning occurs when gradient steps no longer yield improvement as tasks evolve, so accuracy stalls or degrades despite adequate capacity and supervision. We analyze LoT incurred with Adam through an optimization lens and find that single indicators such as Hessian rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios, and unit-sign entropy are not reliable predictors. Instead we introduce two complementary criteria: a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound that combine into a per-layer predictive threshold that anticipates trainability behavior. Using this threshold, we build a simple per-layer scheduler that keeps each layers effective step below a safe limit, stabilizing training and improving accuracy across concatenated ReLU (CReLU), Wasserstein regularization, and L2 weight decay, with learned learning-rate trajectories that mirror canonical decay.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì§€ì† í•™ìŠµì—ì„œ ë°œìƒí•˜ëŠ” í•™ìŠµ ê°€ëŠ¥ì„± ìƒì‹¤(LoT) ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. Adam ìµœì í™” ê´€ì ì—ì„œ LoTë¥¼ ë¶„ì„í•œ ê²°ê³¼, í—¤ì‹œì•ˆ ë­í¬, ìƒ¤í”„ë‹ˆìŠ¤ ìˆ˜ì¤€, ê°€ì¤‘ì¹˜ ë˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„, ê·¸ë˜ë””ì–¸íŠ¸-ë§¤ê°œë³€ìˆ˜ ë¹„ìœ¨, ìœ ë‹›-ì‚¬ì¸ ì—”íŠ¸ë¡œí”¼ì™€ ê°™ì€ ë‹¨ì¼ ì§€í‘œëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì˜ˆì¸¡ìì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ëŒ€ì‹ , ë°°ì¹˜ í¬ê¸°ë¥¼ ê³ ë ¤í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ì´ì¦ˆ ê²½ê³„ì™€ ê³¡ë¥  ë³€ë™ ì œì–´ ê²½ê³„ë¥¼ ê²°í•©í•œ ë ˆì´ì–´ë³„ ì˜ˆì¸¡ ì„ê³„ê°’ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° ë ˆì´ì–´ì˜ íš¨ê³¼ì ì¸ ìŠ¤í…ì„ ì•ˆì „í•œ í•œë„ ë‚´ë¡œ ìœ ì§€í•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ êµ¬ì¶•í•˜ì—¬, CReLU, Wasserstein ì •ê·œí™”, L2 ê°€ì¤‘ì¹˜ ê°ì†Œì—ì„œì˜ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì§€ì† í•™ìŠµì—ì„œì˜ í•™ìŠµ ê°€ëŠ¥ì„± ìƒì‹¤(LoT)ì€ ê³¼ì œì˜ ì§„í™”ë¡œ ì¸í•´ ê¸°ìš¸ê¸° ë‹¨ê³„ê°€ ë” ì´ìƒ ê°œì„ ì„ ê°€ì ¸ì˜¤ì§€ ì•Šì„ ë•Œ ë°œìƒí•˜ë©°, ì´ëŠ” ì¶©ë¶„í•œ ìš©ëŸ‰ê³¼ ê°ë…ì—ë„ ë¶ˆêµ¬í•˜ê³  ì •í™•ë„ê°€ ì •ì²´ë˜ê±°ë‚˜ ì €í•˜ë˜ëŠ” í˜„ìƒì´ë‹¤.
- 2. Adam ìµœì í™” ê´€ì ì—ì„œ LoTë¥¼ ë¶„ì„í•œ ê²°ê³¼, í—¤ì‹œì•ˆ ë­í¬, ìƒ¤í”„ë‹ˆìŠ¤ ìˆ˜ì¤€, ê°€ì¤‘ì¹˜ ë˜ëŠ” ê¸°ìš¸ê¸° ë…¸ë¦„, ê¸°ìš¸ê¸° ëŒ€ íŒŒë¼ë¯¸í„° ë¹„ìœ¨, ë‹¨ìœ„ ë¶€í˜¸ ì—”íŠ¸ë¡œí”¼ì™€ ê°™ì€ ë‹¨ì¼ ì§€í‘œëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ìê°€ ì•„ë‹˜ì„ ë°œê²¬í–ˆë‹¤.
- 3. ëŒ€ì‹ , ë°°ì¹˜ í¬ê¸° ì¸ì‹ ê¸°ìš¸ê¸°-ë…¸ì´ì¦ˆ ê²½ê³„ì™€ ê³¡ë¥  ë³€ë™ ì œì–´ ê²½ê³„ë¥¼ ë„ì…í•˜ì—¬ ê³„ì¸µë³„ ì˜ˆì¸¡ ì„ê³„ê°’ì„ ì„¤ì •í•˜ê³ , ì´ë¥¼ í†µí•´ í•™ìŠµ ê°€ëŠ¥ì„± í–‰ë™ì„ ì˜ˆì¸¡í•œë‹¤.
- 4. ì´ ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê° ê³„ì¸µì˜ íš¨ê³¼ì ì¸ ë‹¨ê³„ë¥¼ ì•ˆì „í•œ í•œë„ ë‚´ë¡œ ìœ ì§€í•˜ëŠ” ê°„ë‹¨í•œ ê³„ì¸µë³„ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ êµ¬ì¶•í•˜ì—¬, CReLU, Wasserstein ì •ê·œí™”, L2 ê°€ì¤‘ì¹˜ ê°ì‡ ì—ì„œ í•™ìŠµë¥  ê¶¤ì ì„ í•™ìŠµí•˜ì—¬ í›ˆë ¨ì„ ì•ˆì •í™”í•˜ê³  ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-25 15:45:21*