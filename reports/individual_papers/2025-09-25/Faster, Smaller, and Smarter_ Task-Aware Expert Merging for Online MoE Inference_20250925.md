---
keywords:
  - Sparse Mixture of Experts
  - Transformer
  - Neural Bandit
  - Task-Aware Expert Merging
  - Online MoE Inference
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19781
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:39:55.413996",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Sparse Mixture of Experts",
    "Transformer",
    "Neural Bandit",
    "Task-Aware Expert Merging",
    "Online MoE Inference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Sparse Mixture of Experts": 0.82,
    "Transformer": 0.88,
    "Neural Bandit": 0.83,
    "Task-Aware Expert Merging": 0.79,
    "Online MoE Inference": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Sparse Mixture of Experts",
        "canonical": "Sparse Mixture of Experts",
        "aliases": [
          "SMoE"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific architecture relevant to scaling Transformer capacity efficiently.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model architecture in deep learning, relevant to the paper's context.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.88
      },
      {
        "surface": "Neural Bandit",
        "canonical": "Neural Bandit",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Neural Bandit is a novel approach in the paper for optimizing expert merging decisions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.83
      },
      {
        "surface": "Task-Aware Expert Merging",
        "canonical": "Task-Aware Expert Merging",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique technique introduced in the paper for improving online inference.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "Online MoE Inference",
        "canonical": "Online MoE Inference",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The paper focuses on improving this specific type of inference, making it a key concept.",
        "novelty_score": 0.68,
        "connectivity_score": 0.64,
        "specificity_score": 0.76,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Sparse Mixture of Experts",
      "resolved_canonical": "Sparse Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Neural Bandit",
      "resolved_canonical": "Neural Bandit",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Task-Aware Expert Merging",
      "resolved_canonical": "Task-Aware Expert Merging",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Online MoE Inference",
      "resolved_canonical": "Online MoE Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.64,
        "specificity": 0.76,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19781.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19781](https://arxiv.org/abs/2509.19781)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (85.2% similar)
- [[2025-09-24/Union of Experts_ Adapting Hierarchical Routing to Equivalently Decomposed Transformer_20250924|Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer]] (83.9% similar)
- [[2025-09-24/Symphony-MoE_ Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts_20250924|Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts]] (83.0% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (82.9% similar)
- [[2025-09-24/LongCat-Flash-Thinking Technical Report_20250924|LongCat-Flash-Thinking Technical Report]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**âš¡ Unique Technical**: [[keywords/Sparse Mixture of Experts|Sparse Mixture of Experts]], [[keywords/Neural Bandit|Neural Bandit]], [[keywords/Task-Aware Expert Merging|Task-Aware Expert Merging]], [[keywords/Online MoE Inference|Online MoE Inference]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19781v1 Announce Type: new 
Abstract: Sparse Mixture of Experts (SMoE) has become a preferred architecture for scaling Transformer capacity without increasing computational cost, as it activates only a small subset of experts for each input. However, deploying such an approach for \textit{online inference} remains challenging due to the large size of a full SMoE model and the complexity of expert routing, especially in resource-constrained edge networks. Moreover, during the online inference, task information is often unavailable, making the task-level routing error-prone. In this work, we propose a novel tree-structured adaptive neural bandit router, \texttt{Tanbr}, to enable efficient and reliable online MoE inference. Instead of relying on explicit task tags, \texttt{Tanbr} estimates the task distribution over time from historical data and uses it to guide task-aware expert merging within a given pre-trained MoE. To handle the large continuous space of merging weights, \texttt{Tanbr} employs a binary tree to progressively partition the space and generate finer candidate weights. It then applies a neural bandit to learn the non-linear mapping from merging weight to model performance and decides optimal expert merging. We prove that \texttt{Tanbr} achieves a sublinear regret bound of {\small $\mathcal{O}(\sqrt{T} \log(T))$} over {\small $T$} rounds, despite operating over a continuous decision space, matching regret bounds compared to existing methods. Extensive experiments show that \texttt{Tanbr} reduces inference latency by at least {\small $45\%$} and memory usage by up to {\small $25\%$}, while maintaining a high accuracy compared to many state-of-the-art methods.

## ğŸ“ ìš”ì•½

Sparse Mixture of Experts(SMoE)ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ìš©ëŸ‰ì„ í™•ì¥í•˜ë©´ì„œë„ ê³„ì‚° ë¹„ìš©ì„ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠëŠ” ì•„í‚¤í…ì²˜ë¡œ ì£¼ëª©ë°›ê³  ìˆì§€ë§Œ, ì˜¨ë¼ì¸ ì¶”ë¡ ì—ì„œëŠ” ëª¨ë¸ì˜ í¬ê¸°ì™€ ì „ë¬¸ê°€ ë¼ìš°íŒ…ì˜ ë³µì¡ì„± ë•Œë¬¸ì— ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ì—°êµ¬ì—ì„œëŠ” \texttt{Tanbr}ë¼ëŠ” ìƒˆë¡œìš´ íŠ¸ë¦¬ êµ¬ì¡°ì˜ ì ì‘í˜• ì‹ ê²½ ë°´ë”§ ë¼ìš°í„°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. \texttt{Tanbr}ëŠ” ëª…ì‹œì ì¸ íƒœìŠ¤í¬ íƒœê·¸ ì—†ì´ ê³¼ê±° ë°ì´í„°ë¥¼ í†µí•´ íƒœìŠ¤í¬ ë¶„í¬ë¥¼ ì¶”ì •í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ê°€ ë³‘í•©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë˜í•œ, ì´ì§„ íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³‘í•© ê°€ì¤‘ì¹˜ ê³µê°„ì„ ì„¸ë¶„í™”í•˜ê³ , ì‹ ê²½ ë°´ë”§ì„ í†µí•´ ìµœì ì˜ ì „ë¬¸ê°€ ë³‘í•©ì„ ê²°ì •í•©ë‹ˆë‹¤. \texttt{Tanbr}ëŠ” ì—°ì†ì ì¸ ê²°ì • ê³µê°„ì—ì„œë„ ì„œë¸Œë¦¬ë‹ˆì–´ í›„íšŒ ê²½ê³„ë¥¼ ë‹¬ì„±í•˜ë©°, ì‹¤í—˜ ê²°ê³¼ ìµœì†Œ 45%ì˜ ì¶”ë¡  ì§€ì—° ê°ì†Œì™€ ìµœëŒ€ 25%ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš© ì ˆê°ì„ ì´ë£¨ë©´ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Sparse Mixture of Experts (SMoE)ëŠ” ì…ë ¥ë§ˆë‹¤ ì†Œìˆ˜ì˜ ì „ë¬¸ê°€ë§Œ í™œì„±í™”í•˜ì—¬ ê³„ì‚° ë¹„ìš©ì„ ì¦ê°€ì‹œí‚¤ì§€ ì•Šê³  Transformerì˜ ìš©ëŸ‰ì„ í™•ì¥í•˜ëŠ” ë° ì„ í˜¸ë˜ëŠ” ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.
- 2. \texttt{Tanbr}ëŠ” ì˜¨ë¼ì¸ ì¶”ë¡  ì‹œ íš¨ìœ¨ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” MoE ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìƒˆë¡œìš´ íŠ¸ë¦¬ êµ¬ì¡°ì˜ ì ì‘í˜• ì‹ ê²½ ë°´ë”§ ë¼ìš°í„°ì…ë‹ˆë‹¤.
- 3. \texttt{Tanbr}ëŠ” ëª…ì‹œì ì¸ íƒœìŠ¤í¬ íƒœê·¸ ëŒ€ì‹  ì—­ì‚¬ì  ë°ì´í„°ë¥¼ í†µí•´ íƒœìŠ¤í¬ ë¶„í¬ë¥¼ ì¶”ì •í•˜ê³  ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë¬¸ê°€ ë³‘í•©ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
- 4. \texttt{Tanbr}ëŠ” ì—°ì†ì ì¸ ë³‘í•© ê°€ì¤‘ì¹˜ ê³µê°„ì„ ì´ì§„ íŠ¸ë¦¬ë¡œ ë¶„í• í•˜ê³  ì‹ ê²½ ë°´ë”§ì„ ì ìš©í•˜ì—¬ ìµœì ì˜ ì „ë¬¸ê°€ ë³‘í•©ì„ ê²°ì •í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, \texttt{Tanbr}ëŠ” ì¶”ë¡  ì§€ì—°ì„ ìµœì†Œ 45% ì¤„ì´ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœëŒ€ 25% ì¤„ì´ë©´ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:39:55*