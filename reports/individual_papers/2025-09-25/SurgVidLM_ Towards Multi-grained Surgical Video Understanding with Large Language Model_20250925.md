---
keywords:
  - Large Language Model
  - Multimodal Learning
  - Surgical Scene Understanding
  - Attention Mechanism
  - Robot-Assisted Surgery
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2506.17873
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:28:26.830298",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multimodal Learning",
    "Surgical Scene Understanding",
    "Attention Mechanism",
    "Robot-Assisted Surgery"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multimodal Learning": 0.82,
    "Surgical Scene Understanding": 0.8,
    "Attention Mechanism": 0.78,
    "Robot-Assisted Surgery": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "It is a foundational technology in the paper, connecting to various applications in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal LLMs"
        ],
        "category": "specific_connectable",
        "rationale": "This connects the paper to the growing field of integrating multiple data types in AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Surgical scene understanding",
        "canonical": "Surgical Scene Understanding",
        "aliases": [
          "Surgical Video Analysis"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique application area that the paper addresses, linking to medical AI research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multi-frequency Fusion Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Fusion Attention"
        ],
        "category": "specific_connectable",
        "rationale": "This is a specific technique that enhances understanding of video content, linking to advanced AI methods.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "robot-assisted surgery",
        "canonical": "Robot-Assisted Surgery",
        "aliases": [
          "Robotic Surgery"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific application domain that benefits from the proposed model, linking to robotics in healthcare.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Surgical scene understanding",
      "resolved_canonical": "Surgical Scene Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multi-frequency Fusion Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "robot-assisted surgery",
      "resolved_canonical": "Robot-Assisted Surgery",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.17873.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2506.17873](https://arxiv.org/abs/2506.17873)

## 🔗 유사한 논문
- [[2025-09-23/Surgical-MambaLLM_ Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery_20250923|Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery]] (88.4% similar)
- [[2025-09-22/Data-Efficient Learning for Generalizable Surgical Video Understanding_20250922|Data-Efficient Learning for Generalizable Surgical Video Understanding]] (86.2% similar)
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (83.6% similar)
- [[2025-09-24/LongLLaVA_ Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture_20250924|LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture]] (83.2% similar)
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (83.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Surgical Scene Understanding|Surgical Scene Understanding]], [[keywords/Robot-Assisted Surgery|Robot-Assisted Surgery]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.17873v2 Announce Type: replace-cross 
Abstract: Surgical scene understanding is critical for surgical training and robotic decision-making in robot-assisted surgery. Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated great potential for advancing scene perception in the medical domain, facilitating surgeons to understand surgical scenes and procedures. However, these methods are primarily oriented towards image-based analysis or global video understanding, overlooking the fine-grained video reasoning that is crucial for analyzing specific processes and capturing detailed task execution within a surgical procedure. To bridge this gap, we propose SurgVidLM, the first video language model designed to address both full and fine-grained surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K that is a large-scale dataset with over 31K video-instruction pairs, enabling both holistic understanding and detailed analysis of surgical procedures. Building on this resource, SurgVidLM incorporates a two-stage StageFocus mechanism: the first stage extracts global procedural context, while the second stage performs high-frequency local analysis guided by temporal cues. We also develop the Multi-frequency Fusion Attention to effectively integrate low- and high-frequency visual tokens, ensuring the preservation of critical task-specific details. Experimental results demonstrate that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs of comparable parameter scale in both full and fine-grained video understanding tasks, showcasing its superior capability in capturing the context of complex robot-assisted surgeries. Our code and dataset will be publicly accessible soon.

## 📝 요약

이 논문은 로봇 보조 수술에서 중요한 수술 장면 이해를 개선하기 위해 SurgVidLM이라는 비디오 언어 모델을 제안합니다. 기존의 다중모드 대형 언어 모델은 이미지 기반 분석에 치중했으나, SurgVidLM은 전체적인 이해와 세부적인 분석을 모두 가능하게 합니다. 이를 위해 31,000개 이상의 비디오-설명 쌍으로 구성된 대규모 데이터셋 SVU-31K를 구축하였고, 두 단계의 StageFocus 메커니즘을 통해 전반적인 절차 맥락과 세부적인 작업을 분석합니다. 또한, Multi-frequency Fusion Attention을 통해 시각적 정보를 효과적으로 통합합니다. 실험 결과, SurgVidLM은 기존 모델보다 뛰어난 성능을 보이며, 복잡한 로봇 수술의 맥락을 잘 포착합니다. 코드와 데이터셋은 곧 공개될 예정입니다.

## 🎯 주요 포인트

- 1. SurgVidLM은 로봇 보조 수술에서 전체 및 세부적인 수술 비디오 이해를 목표로 하는 최초의 비디오 언어 모델입니다.
- 2. SurgVidLM은 31K 이상의 비디오-설명 쌍을 포함한 대규모 데이터셋 SVU-31K를 활용하여 수술 절차의 전체적 이해와 세부 분석을 가능하게 합니다.
- 3. StageFocus 메커니즘을 통해 첫 번째 단계에서 전반적인 절차적 맥락을 추출하고, 두 번째 단계에서 시간적 단서를 기반으로 세부적인 지역 분석을 수행합니다.
- 4. Multi-frequency Fusion Attention을 개발하여 저주파 및 고주파 시각 토큰을 효과적으로 통합하고, 중요한 작업 세부 사항을 보존합니다.
- 5. 실험 결과, SurgVidLM은 유사한 파라미터 규모의 최신 Vid-LLMs보다 전체 및 세부적인 비디오 이해 작업에서 뛰어난 성능을 보입니다.


---

*Generated on 2025-09-25 16:28:26*