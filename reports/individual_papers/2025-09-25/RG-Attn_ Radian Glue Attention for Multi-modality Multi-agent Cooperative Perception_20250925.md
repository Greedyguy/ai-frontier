---
keywords:
  - Radian Glue Attention
  - Cooperative Perception
  - Multi-agent Sensor Fusion
  - Vehicle-to-Everything
  - Cross-modal Fusion
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2501.16803
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:29:19.052814",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Radian Glue Attention",
    "Cooperative Perception",
    "Multi-agent Sensor Fusion",
    "Vehicle-to-Everything",
    "Cross-modal Fusion"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Radian Glue Attention": 0.8,
    "Cooperative Perception": 0.82,
    "Multi-agent Sensor Fusion": 0.85,
    "Vehicle-to-Everything": 0.78,
    "Cross-modal Fusion": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Radian Glue Attention",
        "canonical": "Radian Glue Attention",
        "aliases": [
          "RG-Attn"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel attention mechanism specifically designed for multi-modal sensor fusion in cooperative perception.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cooperative Perception",
        "canonical": "Cooperative Perception",
        "aliases": [
          "Collaborative Perception"
        ],
        "category": "specific_connectable",
        "rationale": "A key concept in autonomous driving that involves the integration of data from multiple agents to enhance perception capabilities.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multi-agent Sensor Fusion",
        "canonical": "Multi-agent Sensor Fusion",
        "aliases": [
          "Sensor Fusion"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's approach, this concept involves combining data from multiple sensors across different agents for improved perception.",
        "novelty_score": 0.6,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vehicle-to-Everything",
        "canonical": "Vehicle-to-Everything",
        "aliases": [
          "V2X"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for enabling communication between vehicles and other entities, facilitating cooperative perception.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cross-modal Fusion",
        "canonical": "Cross-modal Fusion",
        "aliases": [
          "Multimodal Fusion"
        ],
        "category": "specific_connectable",
        "rationale": "A critical process for integrating different sensor modalities, enhancing the robustness of perception systems.",
        "novelty_score": 0.58,
        "connectivity_score": 0.83,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Radian Glue Attention",
      "resolved_canonical": "Radian Glue Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cooperative Perception",
      "resolved_canonical": "Cooperative Perception",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multi-agent Sensor Fusion",
      "resolved_canonical": "Multi-agent Sensor Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vehicle-to-Everything",
      "resolved_canonical": "Vehicle-to-Everything",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cross-modal Fusion",
      "resolved_canonical": "Cross-modal Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.83,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2501.16803.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2501.16803](https://arxiv.org/abs/2501.16803)

## 🔗 유사한 논문
- [[2025-09-24/MLF-4DRCNet_ Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving_20250924|MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving]] (85.0% similar)
- [[2025-09-19/Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention_20250919|Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention]] (83.6% similar)
- [[2025-09-24/TriFusion-AE_ Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing_20250924|TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing]] (83.5% similar)
- [[2025-09-22/A re-calibration method for object detection with multi-modal alignment bias in autonomous driving_20250922|A re-calibration method for object detection with multi-modal alignment bias in autonomous driving]] (83.4% similar)
- [[2025-09-23/A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion_20250923|A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion]] (83.3% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Cooperative Perception|Cooperative Perception]], [[keywords/Multi-agent Sensor Fusion|Multi-agent Sensor Fusion]], [[keywords/Vehicle-to-Everything|Vehicle-to-Everything]], [[keywords/Cross-modal Fusion|Cross-modal Fusion]]
**⚡ Unique Technical**: [[keywords/Radian Glue Attention|Radian Glue Attention]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2501.16803v3 Announce Type: replace-cross 
Abstract: Cooperative perception enhances autonomous driving by leveraging Vehicle-to-Everything (V2X) communication for multi-agent sensor fusion. However, most existing methods rely on single-modal data sharing, limiting fusion performance, particularly in heterogeneous sensor settings involving both LiDAR and cameras across vehicles and roadside units (RSUs). To address this, we propose Radian Glue Attention (RG-Attn), a lightweight and generalizable cross-modal fusion module that unifies intra-agent and inter-agent fusion via transformation-based coordinate alignment and a unified sampling/inversion strategy. RG-Attn efficiently aligns features through a radian-based attention constraint, operating column-wise on geometrically consistent regions to reduce overhead and preserve spatial coherence, thereby enabling accurate and robust fusion. Building upon RG-Attn, we propose three cooperative architectures. The first, Paint-To-Puzzle (PTP), prioritizes communication efficiency but assumes all agents have LiDAR, optionally paired with cameras. The second, Co-Sketching-Co-Coloring (CoS-CoCo), offers maximal flexibility, supporting any sensor setup (e.g., LiDAR-only, camera-only, or both) and enabling strong cross-modal generalization for real-world deployment. The third, Pyramid-RG-Attn Fusion (PRGAF), aims for peak detection accuracy with the highest computational overhead. Extensive evaluations on simulated and real-world datasets show our framework delivers state-of-the-art detection accuracy with high flexibility and efficiency. GitHub Link: https://github.com/LantaoLi/RG-Attn

## 📝 요약

이 논문은 자율주행을 위한 협력적 인식을 개선하기 위해 V2X 통신을 활용한 다중 에이전트 센서 융합을 다룹니다. 기존 방법들이 단일 모달 데이터 공유에 의존하여 융합 성능이 제한되는 문제를 해결하기 위해, 저자들은 Radian Glue Attention (RG-Attn)이라는 경량의 일반화 가능한 교차 모달 융합 모듈을 제안합니다. RG-Attn은 변환 기반 좌표 정렬과 통합 샘플링/반전 전략을 통해 에이전트 간 및 에이전트 내 융합을 통합합니다. 이를 기반으로 세 가지 협력 아키텍처를 제안하며, 각각 통신 효율성, 유연성, 최고 탐지 정확도를 목표로 합니다. 시뮬레이션 및 실제 데이터셋 평가 결과, 이 프레임워크는 높은 유연성과 효율성을 유지하면서 최첨단 탐지 정확도를 달성합니다.

## 🎯 주요 포인트

- 1. Radian Glue Attention (RG-Attn)은 경량화된 모듈로, 변환 기반 좌표 정렬과 통합 샘플링/반전 전략을 통해 에이전트 간 및 에이전트 내의 교차 모달 융합을 통합합니다.
- 2. RG-Attn은 라디안 기반의 주의 제약을 통해 특징을 정렬하여 오버헤드를 줄이고 공간적 일관성을 유지하면서 정확하고 견고한 융합을 가능하게 합니다.
- 3. 세 가지 협력 아키텍처를 제안하며, Paint-To-Puzzle (PTP)은 통신 효율성을 우선시하고 Co-Sketching-Co-Coloring (CoS-CoCo)은 센서 설정에 대한 최대 유연성을 제공합니다.
- 4. Pyramid-RG-Attn Fusion (PRGAF)은 최고 수준의 탐지 정확도를 목표로 하며, 가장 높은 계산 오버헤드를 가집니다.
- 5. 제안된 프레임워크는 시뮬레이션 및 실제 데이터셋에서 최첨단 탐지 정확도를 달성하며 높은 유연성과 효율성을 보여줍니다.


---

*Generated on 2025-09-26 09:29:19*