---
keywords:
  - Monocular Visual-Inertial Motion and Depth Estimation
  - Dense Metric Depth Estimation
  - MSCKF-based Motion Tracking
  - Zero-Shot Learning
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19713
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:03:40.786195",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Monocular Visual-Inertial Motion and Depth Estimation",
    "Dense Metric Depth Estimation",
    "MSCKF-based Motion Tracking",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Monocular Visual-Inertial Motion and Depth Estimation": 0.92,
    "Dense Metric Depth Estimation": 0.85,
    "MSCKF-based Motion Tracking": 0.8,
    "Zero-Shot Learning": 0.88
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "monocular visual-inertial motion and depth",
        "canonical": "Monocular Visual-Inertial Motion and Depth Estimation",
        "aliases": [
          "VIMD"
        ],
        "category": "unique_technical",
        "rationale": "This is the core focus of the paper, representing a novel framework that integrates visual and inertial data for depth estimation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.92
      },
      {
        "surface": "dense metric depth estimation",
        "canonical": "Dense Metric Depth Estimation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This is a key technical term that connects to the broader field of 3D visual perception in robotics.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "MSCKF-based monocular visual-inertial motion tracking",
        "canonical": "MSCKF-based Motion Tracking",
        "aliases": [
          "MSCKF"
        ],
        "category": "unique_technical",
        "rationale": "This specific method is central to the paper's approach and is a unique application of MSCKF in this context.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "zero-shot generalization",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "This term connects to the trending concept of zero-shot learning, highlighting the framework's adaptability.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.88
      }
    ],
    "ban_list_suggestions": [
      "3D visual perception",
      "resource constrained settings"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "monocular visual-inertial motion and depth",
      "resolved_canonical": "Monocular Visual-Inertial Motion and Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "dense metric depth estimation",
      "resolved_canonical": "Dense Metric Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "MSCKF-based monocular visual-inertial motion tracking",
      "resolved_canonical": "MSCKF-based Motion Tracking",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "zero-shot generalization",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.88
      }
    }
  ]
}
-->

# VIMD: Monocular Visual-Inertial Motion and Depth Estimation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19713.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19713](https://arxiv.org/abs/2509.19713)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Depth AnyEvent_ A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation_20250919|Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation]] (87.3% similar)
- [[2025-09-24/Zero-shot Monocular Metric Depth for Endoscopic Images_20250924|Zero-shot Monocular Metric Depth for Endoscopic Images]] (83.7% similar)
- [[2025-09-23/BaseBoostDepth_ Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation_20250923|BaseBoostDepth: Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation]] (83.7% similar)
- [[2025-09-18/Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model_20250918|Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model]] (83.6% similar)
- [[2025-09-24/RS3DBench_ A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing_20250924|RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Dense Metric Depth Estimation|Dense Metric Depth Estimation]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Monocular Visual-Inertial Motion and Depth Estimation|Monocular Visual-Inertial Motion and Depth Estimation]], [[keywords/MSCKF-based Motion Tracking|MSCKF-based Motion Tracking]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19713v1 Announce Type: new 
Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR. In this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking. At the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale, instead of globally fitting an invariant affine model as in the prior work. The VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones. We conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset. Our results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points as few as 10-20 metric depth points per image. This makes the proposed VIMD a practical solution for deployment in resource constrained settings, while its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¡œë´‡ ê³µí•™ê³¼ XRì—ì„œ ì¤‘ìš”í•œ 3D ì‹œê° ì¸ì‹ì„ ìœ„í•´ ì •í™•í•˜ê³  íš¨ìœ¨ì ì¸ ë°€ì§‘ ë©”íŠ¸ë¦­ ê¹Šì´ ì¶”ì •ì„ ëª©í‘œë¡œ í•˜ëŠ” ë‹¨ì•ˆ ë¹„ì£¼ì–¼-ê´€ì„± ëª¨ì…˜ ë° ê¹Šì´(VIMD) í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì „ì—­ ë¶ˆë³€ ì•„í•€ ëª¨ë¸ ëŒ€ì‹ , ë‹¤ì¤‘ ë·° ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ í”½ì…€ ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ì„ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë°©ë²•ë¡ ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤. VIMDëŠ” ë‹¤ì–‘í•œ ê¹Šì´ ì¶”ì • ë°±ë³¸ê³¼ í˜¸í™˜ ê°€ëŠ¥í•œ ëª¨ë“ˆì‹ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, TartanAirì™€ VOID ë°ì´í„°ì…‹ì—ì„œì˜ í‰ê°€ì™€ AR Table ë°ì´í„°ì…‹ì—ì„œì˜ ì œë¡œìƒ· ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, VIMDëŠ” ì´ë¯¸ì§€ë‹¹ 10-20ê°œì˜ ë§¤ìš° í¬ì†Œí•œ ë©”íŠ¸ë¦­ ê¹Šì´ ì ì—ì„œë„ ë›°ì–´ë‚œ ì •í™•ì„±ê³¼ ê°•ì¸í•¨ì„ ë³´ì—¬ì£¼ë©°, ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œë„ ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VIMD í•™ìŠµ í”„ë ˆì„ì›Œí¬ëŠ” MSCKF ê¸°ë°˜ì˜ ë‹¨ì•ˆ ì‹œê°-ê´€ì„± ëª¨ì…˜ ì¶”ì ì„ í™œìš©í•˜ì—¬ ë°€ì§‘ ë©”íŠ¸ë¦­ ê¹Šì´ë¥¼ ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì •í•©ë‹ˆë‹¤.
- 2. VIMDëŠ” ë‹¤ì¤‘ ë·° ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ í”½ì…€ ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ì„ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•˜ë©°, ê¸°ì¡´ì˜ ì „ì—­ ë¶ˆë³€ ì•„í•€ ëª¨ë¸ì„ ëŒ€ì²´í•©ë‹ˆë‹¤.
- 3. VIMD í”„ë ˆì„ì›Œí¬ëŠ” ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´ ë‹¤ì–‘í•œ ê¸°ì¡´ ê¹Šì´ ì¶”ì • ë°±ë³¸ê³¼ í˜¸í™˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- 4. TartanAir ë° VOID ë°ì´í„°ì…‹ì—ì„œ ê´‘ë²”ìœ„í•œ í‰ê°€ë¥¼ í†µí•´ VIMDì˜ ì œë¡œìƒ· ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì…ì¦í•˜ì˜€ìœ¼ë©°, AR Table ë°ì´í„°ì…‹ì—ì„œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. VIMDëŠ” ì´ë¯¸ì§€ë‹¹ 10-20ê°œì˜ ë©”íŠ¸ë¦­ ê¹Šì´ í¬ì¸íŠ¸ì™€ ê°™ì€ ê·¹ë„ë¡œ í¬ì†Œí•œ ì ì—ì„œë„ ë›°ì–´ë‚œ ì •í™•ë„ì™€ ê°•ì¸í•¨ì„ ë³´ì—¬ì£¼ë©°, ìì› ì œí•œ í™˜ê²½ì—ì„œì˜ ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:03:40*