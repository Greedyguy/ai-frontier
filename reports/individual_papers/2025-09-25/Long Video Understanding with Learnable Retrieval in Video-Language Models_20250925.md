---
keywords:
  - Large Language Model
  - Vision-Language Model
  - Zero-Shot Learning
  - Learnable Retrieval
  - Soft Matching Loss
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2312.04931
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:20:57.619799",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Vision-Language Model",
    "Zero-Shot Learning",
    "Learnable Retrieval",
    "Soft Matching Loss"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Vision-Language Model": 0.82,
    "Zero-Shot Learning": 0.78,
    "Learnable Retrieval": 0.72,
    "Soft Matching Loss": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's approach and connect to numerous related works in NLP and video understanding.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a key component of the proposed method, linking visual and textual data processing.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Zero-Shot Video Question Answering",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot VQA"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending topic and relevant to the paper's evaluation methodology.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Learnable Retrieval",
        "canonical": "Learnable Retrieval",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Learnable Retrieval is a novel concept introduced in the paper for efficient video processing.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Soft Matching Loss",
        "canonical": "Soft Matching Loss",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Soft Matching Loss is a unique technique proposed in the paper to enhance model training.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "system"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Zero-Shot Video Question Answering",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Learnable Retrieval",
      "resolved_canonical": "Learnable Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Soft Matching Loss",
      "resolved_canonical": "Soft Matching Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Long Video Understanding with Learnable Retrieval in Video-Language Models

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2312.04931.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2312.04931](https://arxiv.org/abs/2312.04931)

## 🔗 유사한 논문
- [[2025-09-24/LongLLaVA_ Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture_20250924|LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture]] (87.2% similar)
- [[2025-09-25/Video models are zero-shot learners and reasoners_20250925|Video models are zero-shot learners and reasoners]] (85.8% similar)
- [[2025-09-23/VideoRFT_ Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning_20250923|VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning]] (85.4% similar)
- [[2025-09-25/SurgVidLM_ Towards Multi-grained Surgical Video Understanding with Large Language Model_20250925|SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model]] (85.0% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (84.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Learnable Retrieval|Learnable Retrieval]], [[keywords/Soft Matching Loss|Soft Matching Loss]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2312.04931v3 Announce Type: replace 
Abstract: The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video understanding, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video reasoning process. To address these issues, we introduce a simple yet effective learnable retrieval-based video-language model (R-VLM) for efficient long video understanding. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant K video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. We achieve this by incorporating a learnable lightweight MLP block to facilitate the efficient retrieval of question-relevant chunks, through the end-to-end training of our video-language model with a proposed soft matching loss. Our experimental results on multiple zero-shot video question answering datasets validate the effectiveness of our framework for comprehending long videos.

## 📝 요약

대형 언어 모델(LLM)의 자연어 이해 및 생성 능력을 비디오 이해에 활용하는 것은 매력적이지만, 긴 비디오를 처리하는 데는 어려움이 있습니다. 많은 비디오 토큰은 계산 비용을 증가시키고, 불필요한 토큰은 잡음을 유발합니다. 이를 해결하기 위해, 우리는 효율적인 긴 비디오 이해를 위한 학습 가능한 검색 기반 비디오-언어 모델(R-VLM)을 제안합니다. 이 모델은 질문과 긴 비디오를 입력받아 가장 관련성 높은 K개의 비디오 청크를 선택하여 LLM의 문맥으로 사용합니다. 이를 통해 비디오 토큰 수를 줄이고 잡음을 제거하며 성능을 향상시킵니다. 실험 결과, 제안된 프레임워크가 긴 비디오 이해에 효과적임을 입증했습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)을 활용한 긴 비디오 이해는 높은 계산 비용과 시각적 세부 정보 손실 문제를 야기합니다.
- 2. 비디오 추론 과정에서 질문과 관련 없는 토큰의 존재는 잡음을 유발합니다.
- 3. R-VLM은 질문과 관련된 비디오 청크를 선택하여 LLM 추론의 맥락으로 사용함으로써 효율적인 긴 비디오 이해를 가능하게 합니다.
- 4. 학습 가능한 경량 MLP 블록을 통해 질문과 관련된 청크를 효율적으로 검색하고, 제안된 소프트 매칭 손실을 통해 모델을 훈련합니다.
- 5. 여러 제로샷 비디오 질문 응답 데이터셋에서의 실험 결과는 긴 비디오 이해에 대한 프레임워크의 효과성을 입증합니다.


---

*Generated on 2025-09-26 09:20:57*