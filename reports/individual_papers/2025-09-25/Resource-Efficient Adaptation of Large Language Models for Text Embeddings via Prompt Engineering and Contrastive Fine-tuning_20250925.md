---
keywords:
  - Large Language Model
  - Natural Language Processing
  - Prompt Engineering
  - Contrastive Fine-tuning
  - Attention Mechanism
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2507.22729
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:56:48.565187",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Natural Language Processing",
    "Prompt Engineering",
    "Contrastive Fine-tuning",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Natural Language Processing": 0.8,
    "Prompt Engineering": 0.78,
    "Contrastive Fine-tuning": 0.77,
    "Attention Mechanism": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on adapting models for text embeddings.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Natural Language Processing",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "Provides context for the domain of application for the discussed techniques.",
        "novelty_score": 0.2,
        "connectivity_score": 0.88,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      },
      {
        "surface": "Prompt Engineering",
        "canonical": "Prompt Engineering",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A key technique explored in the paper for adapting language models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Contrastive Fine-tuning",
        "canonical": "Contrastive Fine-tuning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a novel adaptation strategy for improving text embeddings.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Attention Map",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Map"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant for understanding how focus shifts during fine-tuning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "text embedding",
      "aggregation techniques",
      "task-specific"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Natural Language Processing",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.88,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Prompt Engineering",
      "resolved_canonical": "Prompt Engineering",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Contrastive Fine-tuning",
      "resolved_canonical": "Contrastive Fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Attention Map",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2507.22729.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2507.22729](https://arxiv.org/abs/2507.22729)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (88.7% similar)
- [[2025-09-23/L-MTP_ Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models_20250923|L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models]] (86.4% similar)
- [[2025-09-25/Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning_20250925|Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning]] (86.1% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (86.0% similar)
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (85.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Natural Language Processing|Natural Language Processing]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Prompt Engineering|Prompt Engineering]], [[keywords/Contrastive Fine-tuning|Contrastive Fine-tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.22729v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields competitive performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ì ì‘ì‹œí‚¤ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” í† í° ì„ë² ë”©ì˜ ë‹¤ì–‘í•œ ì§‘ê³„ ê¸°ë²•, ì‘ì—…ë³„ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, ëŒ€ì¡°ì  ë¯¸ì„¸ ì¡°ì •ì„ í†µí•œ í…ìŠ¤íŠ¸ ìˆ˜ì¤€ì˜ ì¦ê°•ì„ ì œì•ˆí•œ ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì„ ê²°í•©í•˜ì—¬ MTEBì˜ ì˜ì–´ í´ëŸ¬ìŠ¤í„°ë§ íŠ¸ë™ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì£¼ì˜ ë§µ ë¶„ì„ ê²°ê³¼, ë¯¸ì„¸ ì¡°ì •ì´ í”„ë¡¬í”„íŠ¸ í† í°ì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ ìˆëŠ” ë‹¨ì–´ë¡œ ì´ˆì ì„ ì´ë™ì‹œì¼œ ì˜ë¯¸ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ê³¼ ìì› íš¨ìœ¨ì ì¸ ëŒ€ì¡°ì  ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ LLMì„ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œ ì„±ê³µì ìœ¼ë¡œ ì ì‘ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 2. í† í° ìˆ˜ì¤€ì˜ í‘œí˜„ì„ í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ í’€ë§í•˜ë©´ ì¤‘ìš”í•œ ì •ë³´ê°€ ì†ì‹¤ëœë‹¤.
- 3. ë¹„ìƒì„±ì  ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì€ ì—¬ì „íˆ ì •í™•í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì„œ ìˆ˜ì¤€ì˜ ì„ë² ë”©ì— ì˜ì¡´í•œë‹¤.
- 4. ë‹¤ì–‘í•œ ì ì‘ ì „ëµì„ í†µí•´ LLMì„ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ê³¼ ëŒ€ì¡°ì  ë¯¸ì„¸ ì¡°ì •ì„ ê²°í•©í•˜ì—¬ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-26 08:56:48*