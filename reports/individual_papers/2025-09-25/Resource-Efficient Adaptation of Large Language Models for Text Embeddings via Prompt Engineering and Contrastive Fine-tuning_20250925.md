---
keywords:
  - Large Language Model
  - Natural Language Processing
  - Prompt Engineering
  - Contrastive Fine-tuning
  - Attention Mechanism
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2507.22729
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:56:48.565187",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Natural Language Processing",
    "Prompt Engineering",
    "Contrastive Fine-tuning",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Natural Language Processing": 0.8,
    "Prompt Engineering": 0.78,
    "Contrastive Fine-tuning": 0.77,
    "Attention Mechanism": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on adapting models for text embeddings.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Natural Language Processing",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "Provides context for the domain of application for the discussed techniques.",
        "novelty_score": 0.2,
        "connectivity_score": 0.88,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      },
      {
        "surface": "Prompt Engineering",
        "canonical": "Prompt Engineering",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A key technique explored in the paper for adapting language models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Contrastive Fine-tuning",
        "canonical": "Contrastive Fine-tuning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a novel adaptation strategy for improving text embeddings.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Attention Map",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Map"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant for understanding how focus shifts during fine-tuning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "text embedding",
      "aggregation techniques",
      "task-specific"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Natural Language Processing",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.88,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Prompt Engineering",
      "resolved_canonical": "Prompt Engineering",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Contrastive Fine-tuning",
      "resolved_canonical": "Contrastive Fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Attention Map",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2507.22729.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2507.22729](https://arxiv.org/abs/2507.22729)

## 🔗 유사한 논문
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (88.7% similar)
- [[2025-09-23/L-MTP_ Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models_20250923|L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models]] (86.4% similar)
- [[2025-09-25/Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning_20250925|Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning]] (86.1% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (86.0% similar)
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (85.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Natural Language Processing|Natural Language Processing]]
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Prompt Engineering|Prompt Engineering]], [[keywords/Contrastive Fine-tuning|Contrastive Fine-tuning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.22729v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields competitive performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.

## 📝 요약

이 논문은 대규모 언어 모델(LLM)을 텍스트 임베딩 모델로 효과적으로 적응시키는 방법을 탐구합니다. 주요 기여는 토큰 임베딩의 다양한 집계 기법, 작업별 프롬프트 엔지니어링, 대조적 미세 조정을 통한 텍스트 수준의 증강을 제안한 것입니다. 이러한 접근법을 결합하여 MTEB의 영어 클러스터링 트랙에서 경쟁력 있는 성능을 달성했습니다. 주의 맵 분석 결과, 미세 조정이 프롬프트 토큰에서 의미적으로 관련 있는 단어로 초점을 이동시켜 의미를 더 효과적으로 압축함을 보여줍니다. 실험 결과, 프롬프트 엔지니어링과 자원 효율적인 대조적 미세 조정을 통해 LLM을 텍스트 임베딩 모델로 성공적으로 적응시킬 수 있음을 입증했습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 자연어 처리(NLP)에서 중요한 역할을 하며, 텍스트 생성에서 뛰어난 성능을 보인다.
- 2. 토큰 수준의 표현을 텍스트 임베딩으로 풀링하면 중요한 정보가 손실된다.
- 3. 비생성적 다운스트림 작업은 여전히 정확하고 제어 가능한 문장 또는 문서 수준의 임베딩에 의존한다.
- 4. 다양한 적응 전략을 통해 LLM을 텍스트 임베딩 모델로 효과적으로 변환할 수 있다.
- 5. 실험 결과, 프롬프트 엔지니어링과 대조적 미세 조정을 결합하여 LLM의 성능을 향상시킬 수 있음을 보여준다.


---

*Generated on 2025-09-26 08:56:48*