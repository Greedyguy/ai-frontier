---
keywords:
  - Large Language Model
  - Semantic Similarity Analysis
  - Knowledge Graph Metrics
  - Automated Review Generation
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19326
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:23:58.059366",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Semantic Similarity Analysis",
    "Knowledge Graph Metrics",
    "Automated Review Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Semantic Similarity Analysis": 0.78,
    "Knowledge Graph Metrics": 0.8,
    "Automated Review Generation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on automated review generation, linking to existing work on LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Semantic Similarity Analysis",
        "canonical": "Semantic Similarity Analysis",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key component of the proposed evaluation framework, offering a unique angle for linking.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Structured Knowledge Graph Metrics",
        "canonical": "Knowledge Graph Metrics",
        "aliases": [
          "Structured Knowledge Graph"
        ],
        "category": "specific_connectable",
        "rationale": "Provides a structured approach to evaluate LLMs, connecting to broader knowledge graph research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Automated Review Generation",
        "canonical": "Automated Review Generation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Core application of LLMs in the paper, facilitating connections to similar automation studies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "peer-review process",
      "human-written counterparts",
      "ICLR",
      "NeurIPS"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Semantic Similarity Analysis",
      "resolved_canonical": "Semantic Similarity Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Structured Knowledge Graph Metrics",
      "resolved_canonical": "Knowledge Graph Metrics",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Automated Review Generation",
      "resolved_canonical": "Automated Review Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19326.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19326](https://arxiv.org/abs/2509.19326)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (90.7% similar)
- [[2025-09-23/Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues_20250923|Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues]] (88.9% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (88.4% similar)
- [[2025-09-23/Challenging the Evaluator_ LLM Sycophancy Under User Rebuttal_20250923|Challenging the Evaluator: LLM Sycophancy Under User Rebuttal]] (87.7% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (87.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Graph Metrics|Knowledge Graph Metrics]]
**âš¡ Unique Technical**: [[keywords/Semantic Similarity Analysis|Semantic Similarity Analysis]], [[keywords/Automated Review Generation|Automated Review Generation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19326v1 Announce Type: cross 
Abstract: The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structured and coherent feedback, their capacity for critical reasoning, contextual grounding, and quality sensitivity remains limited. To systematically evaluate these aspects, we propose a comprehensive evaluation framework that integrates semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews against human-written counterparts. We construct a large-scale benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and generate reviews using five LLMs. Our findings show that LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies of the original work, with GPT-4o highlighted as an illustrative example, generating 15.74% more entities than human reviewers in the strengths section of good papers in ICLR 2025. However, they consistently underperform in identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality. GPT-4o produces 59.42% fewer entities than real reviewers in the weaknesses and increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews. Similar trends are observed across all conferences, years, and models, providing empirical foundations for understanding the merits and defects of LLM-generated reviews and informing the development of future LLM-assisted reviewing tools. Data, code, and more detailed results are publicly available at https://github.com/RichardLRC/Peer-Review.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ìë™ ë¦¬ë·° ìƒì„±ì˜ ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì „í†µì ì¸ ë™ë£Œ í‰ê°€ ê³¼ì •ì˜ ë¶€ë‹´ì„ ëœê¸° ìœ„í•´ LLMì˜ í™œìš©ì´ ì œì•ˆë˜ì—ˆì§€ë§Œ, ë¹„íŒì  ì‚¬ê³ ì™€ ë§¥ë½ì  ì´í•´, í’ˆì§ˆ ê°ìˆ˜ì„±ì—ì„œ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ LLMì´ ìƒì„±í•œ ë¦¬ë·°ì™€ ì¸ê°„ ë¦¬ë·°ë¥¼ ë¹„êµí•˜ëŠ” í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ICLRê³¼ NeurIPSì˜ 1,683ê°œ ë…¼ë¬¸ê³¼ 6,495ê°œì˜ ì „ë¬¸ê°€ ë¦¬ë·°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM ë¦¬ë·°ë¥¼ ìƒì„±í•œ ê²°ê³¼, LLMì€ ì£¼ë¡œ ê¸°ìˆ ì  ë‚´ìš©ê³¼ ê¸ì •ì  í”¼ë“œë°±ì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ì•½ì  ì‹ë³„ê³¼ ë¹„íŒì  ì§ˆë¬¸ ì œê¸°ì—ì„œëŠ” ë¶€ì¡±í•¨ì„ ë“œëŸ¬ëƒˆìŠµë‹ˆë‹¤. íŠ¹íˆ, GPT-4oëŠ” ê°•ì  ë¶€ë¶„ì—ì„œ ì¸ê°„ ë¦¬ë·°ì–´ë³´ë‹¤ ë” ë§ì€ ì—”í‹°í‹°ë¥¼ ìƒì„±í–ˆì§€ë§Œ, ì•½ì  ì‹ë³„ì—ì„œëŠ” ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLM ê¸°ë°˜ ë¦¬ë·° ë„êµ¬ ê°œë°œì— ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ë°ì´í„°ì™€ ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³¼í•™ ë…¼ë¬¸ ì œì¶œ ì¦ê°€ë¡œ ì „í†µì ì¸ ë™ë£Œ í‰ê°€ ê³¼ì •ì— ë¶€ë‹´ì´ ê°€ì¤‘ë˜ë©´ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í™œìš©í•œ ìë™ ë¦¬ë·° ìƒì„±ì´ íƒêµ¬ë˜ê³  ìˆë‹¤.
- 2. LLMsëŠ” êµ¬ì¡°ì ì´ê³  ì¼ê´€ëœ í”¼ë“œë°±ì„ ìƒì„±í•˜ëŠ” ë° ëŠ¥ìˆ™í•˜ì§€ë§Œ, ë¹„íŒì  ì‚¬ê³ , ë§¥ë½ì  ê¸°ë°˜, í’ˆì§ˆ ë¯¼ê°ë„ì—ì„œëŠ” í•œê³„ê°€ ìˆë‹¤.
- 3. LLMsëŠ” ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ì™€ ë°©ë²•ë¡ ì„ ì˜ í¬ì°©í•˜ì§€ë§Œ, ì•½ì ì„ ì‹ë³„í•˜ê³  ì‹¤ì§ˆì ì¸ ì§ˆë¬¸ì„ ì œê¸°í•˜ë©° ë…¼ë¬¸ í’ˆì§ˆì— ë”°ë¼ í”¼ë“œë°±ì„ ì¡°ì •í•˜ëŠ” ë°ëŠ” ë¯¸í¡í•˜ë‹¤.
- 4. ì—°êµ¬ ê²°ê³¼, LLMsëŠ” ICLR 2025ì—ì„œ ì¢‹ì€ ë…¼ë¬¸ì˜ ê°•ì  ë¶€ë¶„ì—ì„œ ì¸ê°„ ë¦¬ë·°ì–´ë³´ë‹¤ 15.74% ë” ë§ì€ ì—”í‹°í‹°ë¥¼ ìƒì„±í–ˆì§€ë§Œ, ì•½ì  ì‹ë³„ì—ì„œëŠ” 59.42% ì ì€ ì—”í‹°í‹°ë¥¼ ìƒì„±í–ˆë‹¤.
- 5. ëª¨ë“  íšŒì˜, ì—°ë„, ëª¨ë¸ì—ì„œ ìœ ì‚¬í•œ ê²½í–¥ì´ ê´€ì°°ë˜ë©°, ì´ëŠ” LLM ê¸°ë°˜ ë¦¬ë·°ì˜ ì¥ë‹¨ì ì„ ì´í•´í•˜ê³  ë¯¸ë˜ì˜ LLM ì§€ì› ë¦¬ë·° ë„êµ¬ ê°œë°œì— ì •ë³´ë¥¼ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-25 15:23:58*