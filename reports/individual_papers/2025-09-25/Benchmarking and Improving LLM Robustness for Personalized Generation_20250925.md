---
keywords:
  - Large Language Model
  - PERG Framework
  - Personalized Generation
  - Preference Aligner
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19358
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:29:13.264353",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "PERG Framework",
    "Personalized Generation",
    "Preference Aligner"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "PERG Framework": 0.8,
    "Personalized Generation": 0.78,
    "Preference Aligner": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "This is a fundamental concept in the paper, linking to a broad technical category relevant to many discussions.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "PERG",
        "canonical": "PERG Framework",
        "aliases": [
          "PERGData"
        ],
        "category": "unique_technical",
        "rationale": "A unique framework introduced in the paper, essential for understanding the proposed evaluation method.",
        "novelty_score": 0.85,
        "connectivity_score": 0.55,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Personalization",
        "canonical": "Personalized Generation",
        "aliases": [
          "personalized responses"
        ],
        "category": "specific_connectable",
        "rationale": "Central theme of the paper, connecting to the concept of tailoring LLM outputs to user preferences.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Pref-Aligner",
        "canonical": "Preference Aligner",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel approach proposed to enhance robustness, crucial for understanding improvements in personalization.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "factuality",
      "evaluation",
      "responses"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "PERG",
      "resolved_canonical": "PERG Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.55,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Personalization",
      "resolved_canonical": "Personalized Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Pref-Aligner",
      "resolved_canonical": "Preference Aligner",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Benchmarking and Improving LLM Robustness for Personalized Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19358.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19358](https://arxiv.org/abs/2509.19358)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/A Survey of Personalized Large Language Models_ Progress and Future Directions_20250923|A Survey of Personalized Large Language Models: Progress and Future Directions]] (88.2% similar)
- [[2025-09-23/Latent Inter-User Difference Modeling for LLM Personalization_20250923|Latent Inter-User Difference Modeling for LLM Personalization]] (86.8% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (86.8% similar)
- [[2025-09-25/Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers_20250925|Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers]] (86.4% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (86.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Personalized Generation|Personalized Generation]]
**âš¡ Unique Technical**: [[keywords/PERG Framework|PERG Framework]], [[keywords/Preference Aligner|Preference Aligner]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19358v1 Announce Type: cross 
Abstract: Recent years have witnessed a growing interest in personalizing the responses of large language models (LLMs). While existing evaluations primarily focus on whether a response aligns with a user's preferences, we argue that factuality is an equally important yet often overlooked dimension. In the context of personalization, we define a model as robust if its responses are both factually accurate and align with the user preferences. To assess this, we introduce PERG, a scalable framework for evaluating robustness in LLMs, along with a new dataset, PERGData. We evaluate fourteen models from five different model families using different prompting methods. Our findings show that current LLMs struggle with robust personalization: even the strongest models (GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously successful cases without personalization, while smaller models (e.g., 7B-scale) can fail more than 20% of the time. Further analysis reveals that robustness is significantly affected by the nature of the query and the type of user preference. To mitigate these failures, we propose Pref-Aligner, a two-stage approach that improves robustness by an average of 25% across models. Our work highlights critical gaps in current evaluation practices and introduces tools and metrics to support more reliable, user-aligned LLM deployments.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê°œì¸í™”ëœ ì‘ë‹µì— ëŒ€í•œ ê´€ì‹¬ì´ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ í‰ê°€ê°€ ì£¼ë¡œ ì‚¬ìš©ì ì„ í˜¸ë„ì™€ì˜ ì¼ì¹˜ ì—¬ë¶€ì— ì´ˆì ì„ ë§ì¶”ëŠ” ë°˜ë©´, ë³¸ ì—°êµ¬ëŠ” ì‚¬ì‹¤ì„± ë˜í•œ ì¤‘ìš”í•œ ìš”ì†Œë¼ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ê°œì¸í™” ë§¥ë½ì—ì„œ, ëª¨ë¸ì˜ ì‘ë‹µì´ ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ì¼ì¹˜í•  ë•Œ ì´ë¥¼ ê°•ê±´í•˜ë‹¤ê³  ì •ì˜í•©ë‹ˆë‹¤. ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ PERGë¼ëŠ” í™•ì¥ ê°€ëŠ¥í•œ í‰ê°€ í”„ë ˆì„ì›Œí¬ì™€ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ PERGDataë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°€ì§€ ëª¨ë¸ êµ°ì—ì„œ 14ê°œì˜ ëª¨ë¸ì„ ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ë°©ë²•ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼, í˜„ì¬ LLMë“¤ì€ ê°•ê±´í•œ ê°œì¸í™”ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê°•ë ¥í•œ ëª¨ë¸ì¡°ì°¨ë„ ê°œì¸í™” ì—†ì´ ì„±ê³µí–ˆë˜ ì‚¬ë¡€ì˜ 5%ì—ì„œ ì •í™•ì„±ì„ ìœ ì§€í•˜ì§€ ëª»í–ˆìœ¼ë©°, ì‘ì€ ëª¨ë¸ì€ 20% ì´ìƒ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹¤íŒ¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ Pref-Alignerë¼ëŠ” ë‘ ë‹¨ê³„ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ì—¬ ê°•ê±´ì„±ì„ í‰ê·  25% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” í˜„ì¬ í‰ê°€ ê´€í–‰ì˜ ì¤‘ìš”í•œ ê²©ì°¨ë¥¼ ê°•ì¡°í•˜ë©°, ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ìš©ì ë§ì¶¤í˜• LLM ë°°í¬ë¥¼ ì§€ì›í•˜ëŠ” ë„êµ¬ì™€ ì§€í‘œë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°œì¸í™”ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‘ë‹µì—ì„œ ì‚¬ì‹¤ì„±(factuality)ì´ ì¤‘ìš”í•˜ì§€ë§Œ ì¢…ì¢… ê°„ê³¼ë˜ê³  ìˆë‹¤.
- 2. PERGë¼ëŠ” í”„ë ˆì„ì›Œí¬ì™€ PERGDataë¼ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ í†µí•´ LLMì˜ ê²¬ê³ ì„±ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
- 3. í˜„ì¬ì˜ LLMë“¤ì€ ê°œì¸í™”ëœ ìƒí™©ì—ì„œ ê²¬ê³ í•œ ì‘ë‹µì„ ìœ ì§€í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, íŠ¹íˆ ì‘ì€ ëª¨ë¸ì€ 20% ì´ìƒì˜ ì‹¤íŒ¨ìœ¨ì„ ë³´ì¸ë‹¤.
- 4. Pref-Alignerë¼ëŠ” ë‘ ë‹¨ê³„ ì ‘ê·¼ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ê²¬ê³ ì„±ì„ í‰ê·  25% ê°œì„ í•  ìˆ˜ ìˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” í˜„ì¬ í‰ê°€ ê´€í–‰ì˜ ì¤‘ìš”í•œ ê²©ì°¨ë¥¼ ê°•ì¡°í•˜ê³ , ì‚¬ìš©ì ë§ì¶¤í˜• LLM ë°°í¬ë¥¼ ì§€ì›í•˜ëŠ” ë„êµ¬ì™€ ì§€í‘œë¥¼ ë„ì…í•œë‹¤.


---

*Generated on 2025-09-25 15:29:13*