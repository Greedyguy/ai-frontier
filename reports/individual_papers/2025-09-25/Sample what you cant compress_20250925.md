---
keywords:
  - Autoencoder
  - Diffusion Model
  - Latent Diffusion Model
  - Generative Adversarial Network
  - Stochastic Decoder
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2409.02529
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:04:29.816178",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Autoencoder",
    "Diffusion Model",
    "Latent Diffusion Model",
    "Generative Adversarial Network",
    "Stochastic Decoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Autoencoder": 0.72,
    "Diffusion Model": 0.88,
    "Latent Diffusion Model": 0.8,
    "Generative Adversarial Network": 0.75,
    "Stochastic Decoder": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "autoencoder",
        "canonical": "Autoencoder",
        "aliases": [
          "AE"
        ],
        "category": "broad_technical",
        "rationale": "Autoencoders are fundamental in representation learning and relate to the paper's focus on improving image reconstruction.",
        "novelty_score": 0.45,
        "connectivity_score": 0.78,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "diffusion-based loss",
        "canonical": "Diffusion Model",
        "aliases": [
          "Diffusion-based Loss",
          "Diffusion Process"
        ],
        "category": "specific_connectable",
        "rationale": "Diffusion models are central to the paper's methodology, offering a new approach to image generation and compression.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "latent diffusion model",
        "canonical": "Latent Diffusion Model",
        "aliases": [
          "LDM"
        ],
        "category": "unique_technical",
        "rationale": "The latent diffusion model is a novel concept introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "GAN-based autoencoders",
        "canonical": "Generative Adversarial Network",
        "aliases": [
          "GAN",
          "GAN-based Autoencoder"
        ],
        "category": "specific_connectable",
        "rationale": "GANs are compared against the proposed method, highlighting differences in reconstruction quality and tuning ease.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "stochastic decoder",
        "canonical": "Stochastic Decoder",
        "aliases": [
          "Randomized Decoder"
        ],
        "category": "unique_technical",
        "rationale": "The stochastic decoder is a unique feature of the proposed method, enabling generation of unencoded details.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "blurry results",
      "reconstruction quality",
      "additional penalties"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "autoencoder",
      "resolved_canonical": "Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.78,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "diffusion-based loss",
      "resolved_canonical": "Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "latent diffusion model",
      "resolved_canonical": "Latent Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "GAN-based autoencoders",
      "resolved_canonical": "Generative Adversarial Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "stochastic decoder",
      "resolved_canonical": "Stochastic Decoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Sample what you cant compress

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2409.02529.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2409.02529](https://arxiv.org/abs/2409.02529)

## 🔗 유사한 논문
- [[2025-09-23/Single-step Diffusion for Image Compression at Ultra-Low Bitrates_20250923|Single-step Diffusion for Image Compression at Ultra-Low Bitrates]] (84.0% similar)
- [[2025-09-22/G2D2_ Gradient-Guided Discrete Diffusion for Inverse Problem Solving_20250922|G2D2: Gradient-Guided Discrete Diffusion for Inverse Problem Solving]] (83.5% similar)
- [[2025-09-23/Penalizing Boundary Activation for Object Completeness in Diffusion Models_20250923|Penalizing Boundary Activation for Object Completeness in Diffusion Models]] (83.3% similar)
- [[2025-09-18/Generative Image Coding with Diffusion Prior_20250918|Generative Image Coding with Diffusion Prior]] (83.3% similar)
- [[2025-09-22/Causal Fingerprints of AI Generative Models_20250922|Causal Fingerprints of AI Generative Models]] (82.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Autoencoder|Autoencoder]]
**🔗 Specific Connectable**: [[keywords/Diffusion Model|Diffusion Model]], [[keywords/Generative Adversarial Network|Generative Adversarial Network]]
**⚡ Unique Technical**: [[keywords/Latent Diffusion Model|Latent Diffusion Model]], [[keywords/Stochastic Decoder|Stochastic Decoder]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2409.02529v4 Announce Type: replace 
Abstract: For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate jointly learning a continuous encoder and decoder under a diffusion-based loss and showing that it can lead to higher compression and better generation. We demonstrate that this approach yields better reconstruction quality as compared to GAN-based autoencoders while being easier to tune. We also show that the resulting representation is easier to model with a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss. Since our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach "Sample what you can't compress", or SWYCC for short.

## 📝 요약

이 논문은 오토인코더와 확산 모델을 결합하여 이미지 재구성 품질을 향상시키는 새로운 방법론을 제안합니다. 기존의 오토인코더는 흐릿한 이미지를 생성하는 경향이 있으며, 이를 개선하기 위해 적대적 손실(GAN)과 지각 손실을 추가하지만, 이러한 접근법은 명확한 해석이 부족합니다. 본 연구는 확산 기반 손실을 통해 연속적인 인코더와 디코더를 공동 학습하는 최초의 시도로, 더 높은 압축률과 우수한 이미지 생성을 달성합니다. 제안된 방법은 GAN 기반 오토인코더보다 재구성 품질이 뛰어나고 조정이 용이하며, 확산 모델을 사용한 잠재 표현 모델링이 더 쉬워집니다. 확률적 디코더를 통해 기존의 결정론적 잠재 표현에 포함되지 않은 세부 사항을 생성할 수 있어, 이를 "압축할 수 없는 것을 샘플링하라(SWYCC)"라고 명명했습니다.

## 🎯 주요 포인트

- 1. 기본 오토인코더는 종종 흐릿한 이미지를 생성하지만, 추가적인 페널티를 통해 재구성 품질을 향상시킬 수 있다.
- 2. 본 연구는 오토인코더 표현 학습과 확산을 결합하여, 연속적인 인코더와 디코더를 확산 기반 손실 하에 공동 학습하는 첫 번째 사례를 제시한다.
- 3. 제안된 접근법은 GAN 기반 오토인코더와 비교하여 더 나은 재구성 품질을 제공하며, 튜닝이 더 용이하다.
- 4. 확산 모델을 활용한 잠재 표현은 최첨단 GAN 기반 손실로부터 얻은 표현보다 모델링이 더 용이하다.
- 5. 확률적 디코더를 통해 결정론적 잠재 표현에 인코딩되지 않은 세부 사항을 생성할 수 있어, 이 접근법을 "압축할 수 없는 것을 샘플링하라" (SWYCC)라고 명명하였다.


---

*Generated on 2025-09-25 17:04:29*