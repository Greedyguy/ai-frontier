---
keywords:
  - Neural Machine Translation
  - Low-Resource Languages
  - Transfer Learning
  - Custom Tokenizer
  - Evaluation Benchmarks
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20209
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:02:21.268457",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Machine Translation",
    "Low-Resource Languages",
    "Transfer Learning",
    "Custom Tokenizer",
    "Evaluation Benchmarks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Machine Translation": 0.8,
    "Low-Resource Languages": 0.82,
    "Transfer Learning": 0.85,
    "Custom Tokenizer": 0.78,
    "Evaluation Benchmarks": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Machine Translation",
        "canonical": "Neural Machine Translation",
        "aliases": [
          "NMT"
        ],
        "category": "broad_technical",
        "rationale": "Neural Machine Translation is a fundamental concept in the paper, linking it to broader machine learning and language processing discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Low-Resource Languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "Underrepresented Languages"
        ],
        "category": "unique_technical",
        "rationale": "The focus on low-resource languages is central to the paper's contribution, offering unique insights into language-specific challenges.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Transfer Learning",
        "canonical": "Transfer Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Transfer Learning is a key technique employed in the study, connecting it to existing models and strategies in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Custom Tokenizer",
        "canonical": "Custom Tokenizer",
        "aliases": [
          "Tokenization Strategy"
        ],
        "category": "unique_technical",
        "rationale": "Custom Tokenizers are crucial for handling language-specific nuances, enhancing the paper's technical depth.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Evaluation Benchmarks",
        "canonical": "Evaluation Benchmarks",
        "aliases": [
          "Standardized Benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Evaluation Benchmarks are essential for assessing model performance, providing a standardized measure for comparison.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Machine Translation",
      "resolved_canonical": "Neural Machine Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Low-Resource Languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Transfer Learning",
      "resolved_canonical": "Transfer Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Custom Tokenizer",
      "resolved_canonical": "Custom Tokenizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Evaluation Benchmarks",
      "resolved_canonical": "Evaluation Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20209.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20209](https://arxiv.org/abs/2509.20209)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Leveraging Multilingual Training for Authorship Representation_ Enhancing Generalization across Languages and Domains_20250923|Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains]] (85.1% similar)
- [[2025-09-23/Scaling, Simplification, and Adaptation_ Lessons from Pretraining on Machine-Translated Text_20250923|Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text]] (85.0% similar)
- [[2025-09-23/Enhancing Cross-Lingual Transfer through Reversible Transliteration_ A Huffman-Based Approach for Low-Resource Languages_20250923|Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages]] (83.8% similar)
- [[2025-09-24/Investigating Test-Time Scaling with Reranking for Machine Translation_20250924|Investigating Test-Time Scaling with Reranking for Machine Translation]] (83.5% similar)
- [[2025-09-25/Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks_20250925|Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Machine Translation|Neural Machine Translation]]
**ğŸ”— Specific Connectable**: [[keywords/Transfer Learning|Transfer Learning]]
**âš¡ Unique Technical**: [[keywords/Low-Resource Languages|Low-Resource Languages]], [[keywords/Custom Tokenizer|Custom Tokenizer]], [[keywords/Evaluation Benchmarks|Evaluation Benchmarks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20209v1 Announce Type: cross 
Abstract: Despite advances in Neural Machine Translation (NMT), low-resource languages like Tigrinya remain underserved due to persistent challenges, including limited corpora, inadequate tokenization strategies, and the lack of standardized evaluation benchmarks. This paper investigates transfer learning techniques using multilingual pretrained models to enhance translation quality for morphologically rich, low-resource languages. We propose a refined approach that integrates language-specific tokenization, informed embedding initialization, and domain-adaptive fine-tuning. To enable rigorous assessment, we construct a high-quality, human-aligned English-Tigrinya evaluation dataset covering diverse domains. Experimental results demonstrate that transfer learning with a custom tokenizer substantially outperforms zero-shot baselines, with gains validated by BLEU, chrF, and qualitative human evaluation. Bonferroni correction is applied to ensure statistical significance across configurations. Error analysis reveals key limitations and informs targeted refinements. This study underscores the importance of linguistically aware modeling and reproducible benchmarks in bridging the performance gap for underrepresented languages. Resources are available at https://github.com/hailaykidu/MachineT_TigEng
  and https://huggingface.co/Hailay/MachineT_TigEng

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì €ìë“¤ì´ ë‹¤êµ­ì–´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ì „ì´ í•™ìŠµ ê¸°ë²•ì„ í†µí•´ í‹°ê·¸ë¦¬ëƒì–´ì™€ ê°™ì€ ìì› ë¶€ì¡± ì–¸ì–´ì˜ ë²ˆì—­ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì—°êµ¬í•œ ê²°ê³¼ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì €ìë“¤ì€ ì–¸ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì œì´ì…˜, ì„ë² ë”© ì´ˆê¸°í™”, ë„ë©”ì¸ ì ì‘ ë¯¸ì„¸ ì¡°ì •ì„ í†µí•©í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë„ë©”ì¸ì„ ì•„ìš°ë¥´ëŠ” ê³ í’ˆì§ˆì˜ ì˜ì–´-í‹°ê·¸ë¦¬ëƒ í‰ê°€ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë§ì¶¤í˜• í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ ì „ì´ í•™ìŠµì´ ì œë¡œìƒ· ê¸°ì¤€ì„ í¬ê²Œ ëŠ¥ê°€í–ˆìœ¼ë©°, BLEU, chrF ì ìˆ˜ ë° ì¸ê°„ í‰ê°€ë¥¼ í†µí•´ ê·¸ ì„±ëŠ¥ì´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸í˜ë¡œë‹ˆ ë³´ì •ì„ ì ìš©í•˜ì—¬ í†µê³„ì  ìœ ì˜ì„±ì„ í™•ë³´í–ˆìœ¼ë©°, ì˜¤ë¥˜ ë¶„ì„ì„ í†µí•´ ì£¼ìš” í•œê³„ë¥¼ íŒŒì•…í•˜ê³  ê°œì„  ë°©í–¥ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–¸ì–´í•™ì ìœ¼ë¡œ ì¸ì‹ëœ ëª¨ë¸ë§ê³¼ ì¬í˜„ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ìì› ë¶€ì¡± ì–¸ì–´ì˜ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ê´€ë ¨ ìì›ì€ GitHub ë° Hugging Faceì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì €ìë“¤ì€ ë‹¤êµ­ì–´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ì „ì´ í•™ìŠµ ê¸°ë²•ì„ í†µí•´ í˜•íƒœì ìœ¼ë¡œ ë³µì¡í•œ ì €ìì› ì–¸ì–´ì˜ ë²ˆì—­ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì—°êµ¬í–ˆìŠµë‹ˆë‹¤.
- 2. ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì œì´ì…˜, ì„ë² ë”© ì´ˆê¸°í™”, ë„ë©”ì¸ ì ì‘í˜• ë¯¸ì„¸ ì¡°ì •ì„ í†µí•©í•œ ê°œì„ ëœ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ ë„ë©”ì¸ì„ í¬ê´„í•˜ëŠ” ê³ í’ˆì§ˆì˜ ì¸ê°„ ì •ë ¬ ì˜ì–´-í‹°ê·¸ë¦¬ëƒ í‰ê°€ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ ì—„ê²©í•œ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ë§ì¶¤í˜• í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ ì „ì´ í•™ìŠµì´ ì œë¡œìƒ· ë² ì´ìŠ¤ë¼ì¸ì„ í¬ê²Œ ëŠ¥ê°€í–ˆìœ¼ë©°, BLEU, chrF, ì§ˆì  ì¸ê°„ í‰ê°€ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ì–¸ì–´ì ìœ¼ë¡œ ì¸ì‹ëœ ëª¨ë¸ë§ê³¼ ì¬í˜„ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ì €ëŒ€í‘œ ì–¸ì–´ì˜ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:02:21*