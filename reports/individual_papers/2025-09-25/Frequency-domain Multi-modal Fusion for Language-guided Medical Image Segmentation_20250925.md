---
keywords:
  - Frequency-domain Multi-modal Interaction
  - Language-guided Frequency-domain Feature Interaction
  - Vision-Language Model
  - Frequency-domain Feature Bidirectional Interaction
  - Multimodal Learning
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19719
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:03:54.231523",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Frequency-domain Multi-modal Interaction",
    "Language-guided Frequency-domain Feature Interaction",
    "Vision-Language Model",
    "Frequency-domain Feature Bidirectional Interaction",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Frequency-domain Multi-modal Interaction": 0.8,
    "Language-guided Frequency-domain Feature Interaction": 0.78,
    "Vision-Language Model": 0.82,
    "Frequency-domain Feature Bidirectional Interaction": 0.77,
    "Multimodal Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Frequency-domain Multi-modal Interaction model",
        "canonical": "Frequency-domain Multi-modal Interaction",
        "aliases": [
          "FMISeg"
        ],
        "category": "unique_technical",
        "rationale": "This specific model is central to the paper's proposed solution and is unique to the research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Language-guided Frequency-domain Feature Interaction",
        "canonical": "Language-guided Frequency-domain Feature Interaction",
        "aliases": [
          "LFFI"
        ],
        "category": "unique_technical",
        "rationale": "This module is a novel component of the proposed method, enhancing the interaction between language and visual features.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-language modalities",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-language integration"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is crucial for understanding the interaction between visual and linguistic data in the paper.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Frequency-domain Feature Bidirectional Interaction",
        "canonical": "Frequency-domain Feature Bidirectional Interaction",
        "aliases": [
          "FFBI"
        ],
        "category": "unique_technical",
        "rationale": "This module is a key innovation in the paper, facilitating effective feature fusion.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multimodal Fusion",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal integration"
        ],
        "category": "specific_connectable",
        "rationale": "This term connects to the broader concept of integrating multiple data modalities, which is central to the paper's methodology.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "semantic gap",
      "suboptimal segmentation performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Frequency-domain Multi-modal Interaction model",
      "resolved_canonical": "Frequency-domain Multi-modal Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Language-guided Frequency-domain Feature Interaction",
      "resolved_canonical": "Language-guided Frequency-domain Feature Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-language modalities",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Frequency-domain Feature Bidirectional Interaction",
      "resolved_canonical": "Frequency-domain Feature Bidirectional Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multimodal Fusion",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19719.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19719](https://arxiv.org/abs/2509.19719)

## 🔗 유사한 논문
- [[2025-09-23/Multimodal Medical Image Classification via Synergistic Learning Pre-training_20250923|Multimodal Medical Image Classification via Synergistic Learning Pre-training]] (84.4% similar)
- [[2025-09-22/pFedSAM_ Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation_20250922|pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation]] (84.3% similar)
- [[2025-09-24/MOIS-SAM2_ Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI_20250924|MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI]] (83.7% similar)
- [[2025-09-23/A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet_20250923|A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet]] (82.8% similar)
- [[2025-09-24/Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction_20250924|Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction]] (82.8% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Frequency-domain Multi-modal Interaction|Frequency-domain Multi-modal Interaction]], [[keywords/Language-guided Frequency-domain Feature Interaction|Language-guided Frequency-domain Feature Interaction]], [[keywords/Frequency-domain Feature Bidirectional Interaction|Frequency-domain Feature Bidirectional Interaction]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19719v1 Announce Type: new 
Abstract: Automatically segmenting infected areas in radiological images is essential for diagnosing pulmonary infectious diseases. Recent studies have demonstrated that the accuracy of the medical image segmentation can be improved by incorporating clinical text reports as semantic guidance. However, the complex morphological changes of lesions and the inherent semantic gap between vision-language modalities prevent existing methods from effectively enhancing the representation of visual features and eliminating semantically irrelevant information, ultimately resulting in suboptimal segmentation performance. To address these problems, we propose a Frequency-domain Multi-modal Interaction model (FMISeg) for language-guided medical image segmentation. FMISeg is a late fusion model that establishes interaction between linguistic features and frequency-domain visual features in the decoder. Specifically, to enhance the visual representation, our method introduces a Frequency-domain Feature Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain features. Furthermore, a Language-guided Frequency-domain Feature Interaction (LFFI) module is incorporated within the decoder to suppress semantically irrelevant visual features under the guidance of linguistic information. Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method outperforms the state-of-the-art methods qualitatively and quantitatively.

## 📝 요약

이 논문은 방사선 이미지에서 감염 부위를 자동으로 분할하는 방법을 제안합니다. 기존 방법들은 시각-언어 간의 의미적 차이로 인해 최적의 성능을 내지 못했습니다. 이를 해결하기 위해, 저자들은 주파수 도메인 다중 모달 상호작용 모델(FMISeg)을 제안했습니다. 이 모델은 주파수 도메인에서 시각적 특징과 언어적 특징을 결합하여 시각적 표현을 강화하고, 언어 정보를 통해 의미적으로 관련 없는 시각적 특징을 억제합니다. QaTa-COV19와 MosMedData+ 데이터셋 실험 결과, 제안된 방법이 기존 최첨단 방법들보다 뛰어난 성능을 보였습니다.

## 🎯 주요 포인트

- 1. 방사선 이미지에서 감염 부위를 자동으로 분할하는 것은 폐 감염 질환 진단에 필수적입니다.
- 2. 임상 텍스트 보고서를 의미적 지침으로 활용하면 의료 이미지 분할의 정확성을 향상시킬 수 있습니다.
- 3. 기존 방법들은 시각-언어 모달리티 간의 의미적 차이로 인해 시각적 특징의 표현을 효과적으로 개선하지 못합니다.
- 4. FMISeg 모델은 주파수 도메인 시각 특징과 언어적 특징 간의 상호작용을 통해 의미적으로 관련 없는 시각적 특징을 억제합니다.
- 5. QaTa-COV19와 MosMedData+ 실험에서 FMISeg는 최첨단 방법들보다 우수한 성능을 보였습니다.


---

*Generated on 2025-09-26 09:03:54*