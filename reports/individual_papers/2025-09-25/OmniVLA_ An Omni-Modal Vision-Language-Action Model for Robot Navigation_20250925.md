---
keywords:
  - OmniVLA
  - Vision-Language Model
  - Multimodal Learning
  - Robotic Navigation
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19480
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:55:15.669055",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "OmniVLA",
    "Vision-Language Model",
    "Multimodal Learning",
    "Robotic Navigation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "OmniVLA": 0.8,
    "Vision-Language Model": 0.88,
    "Multimodal Learning": 0.82,
    "Robotic Navigation": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "OmniVLA",
        "canonical": "OmniVLA",
        "aliases": [
          "Omni-Modal Vision-Language-Action Model"
        ],
        "category": "unique_technical",
        "rationale": "OmniVLA is a novel model introduced in the paper, representing a unique approach to omni-modal robotic navigation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-Language-Action",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLA"
        ],
        "category": "evolved_concepts",
        "rationale": "The integration of vision, language, and action in a single model is a significant advancement in multimodal learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Omni-modal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is central to the paper's approach, enhancing connectivity with related research on integrating multiple data types.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Robotic Navigation",
        "canonical": "Robotic Navigation",
        "aliases": [
          "Robot Navigation"
        ],
        "category": "broad_technical",
        "rationale": "Robotic navigation is a fundamental aspect of the study, linking to a wide range of robotics research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "OmniVLA",
      "resolved_canonical": "OmniVLA",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-Language-Action",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Robotic Navigation",
      "resolved_canonical": "Robotic Navigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19480.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19480](https://arxiv.org/abs/2509.19480)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/GeoAware-VLA_ Implicit Geometry Aware Vision-Language-Action Model_20250918|GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model]] (86.3% similar)
- [[2025-09-24/Pure Vision Language Action (VLA) Models_ A Comprehensive Survey_20250924|Pure Vision Language Action (VLA) Models: A Comprehensive Survey]] (86.0% similar)
- [[2025-09-18/Embodied Navigation Foundation Model_20250918|Embodied Navigation Foundation Model]] (85.9% similar)
- [[2025-09-24/OmniBridge_ Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment_20250924|OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment]] (85.7% similar)
- [[2025-09-23/Evo-0_ Vision-Language-Action Model with Implicit Spatial Understanding_20250923|Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding]] (85.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Robotic Navigation|Robotic Navigation]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/OmniVLA|OmniVLA]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19480v1 Announce Type: cross 
Abstract: Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜ì—ì„œ ë‹¤ì–‘í•œ ëª©í‘œ ëª…ì„¸ë¥¼ ìœ ì—°í•˜ê²Œ í•´ì„í•˜ê³  ì¡°í•©í•  ìˆ˜ ìˆëŠ” í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜ ì •ì±…ì€ ë‹¨ì¼ ëª¨ë‹¬ë¦¬í‹°ì— ì˜ì¡´í•˜ì§€ë§Œ, ì œì•ˆëœ OmniVLA ëª¨ë¸ì€ 2D ìœ„ì¹˜, ìì•„ ì¤‘ì‹¬ ì´ë¯¸ì§€, ìì—°ì–´ ë“± ì„¸ ê°€ì§€ ì£¼ìš” ëª©í‘œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê²°í•©í•˜ì—¬ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë°ì´í„°ì…‹ì˜ í™œìš© ë²”ìœ„ë¥¼ í™•ì¥í•˜ê³ , ì •ì±…ì´ í’ë¶€í•œ ê¸°í•˜í•™ì , ì˜ë¯¸ì , ì‹œê°ì  í‘œí˜„ì„ ê°œë°œí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ OmniVLAëŠ” ìƒˆë¡œìš´ í™˜ê²½ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ í¬ì†Œí•œ ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ë³´ì—¬ì£¼ë©°, ìƒˆë¡œìš´ ìì—°ì–´ ì§€ì‹œë¥¼ ë”°ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. OmniVLAëŠ” ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ì „ë¬¸ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë©°, ìƒˆë¡œìš´ ëª¨ë‹¬ë¦¬í‹°ì™€ ì‘ì—…ì— ë§ì¶° ì¡°ì •í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë²”ìš©ì ì´ê³  ìœ ì—°í•œ ë‚´ë¹„ê²Œì´ì…˜ ì •ì±… ê°œë°œì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê°„ì€ ë‹¤ì–‘í•œ ëª©í‘œ ì‚¬ì–‘ì„ ìœ ì—°í•˜ê²Œ í•´ì„í•˜ê³  ì¡°í•©í•  ìˆ˜ ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜ ì •ì±…ì€ ë‹¨ì¼ ëª¨ë‹¬ë¦¬í‹°ì— í›ˆë ¨ë˜ì–´ ìˆì–´ ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì ì‘ë ¥ì´ ì œí•œì ì…ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¹„ì „ ê¸°ë°˜ ë‚´ë¹„ê²Œì´ì…˜ì„ ìœ„í•œ ë¡œë´‡ ê¸°ì´ˆ ëª¨ë¸ì˜ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬, ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ì˜ ëª©í‘œ ì¡°ê±´ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ OmniVLA ëª¨ë¸ì€ ë³´ì§€ ëª»í•œ í™˜ê²½ì— ëŒ€í•œ ì¼ë°˜í™”, ëª¨ë‹¬ë¦¬í‹° ë¶€ì¡±ì— ëŒ€í•œ ê°•ê±´ì„±, ìƒˆë¡œìš´ ìì—°ì–´ ì§€ì‹œë¥¼ ë”°ë¥´ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. OmniVLAëŠ” ëª¨ë‹¬ë¦¬í‹° ì „ë°˜ì— ê±¸ì³ ì „ë¬¸í™”ëœ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, ìƒˆë¡œìš´ ëª¨ë‹¬ë¦¬í‹°ì™€ ì‘ì—…ì— ëŒ€í•œ ìœ ì—°í•œ ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” OmniVLAê°€ ê´‘ë²”ìœ„í•˜ê²Œ ì¼ë°˜í™” ê°€ëŠ¥í•˜ê³  ìœ ì—°í•œ ë‚´ë¹„ê²Œì´ì…˜ ì •ì±…ì„ ìœ„í•œ ì¤‘ìš”í•œ ì§„ì „ì„ ì œê³µí•˜ë©°, ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì•„ìš°ë¥´ëŠ” ë¡œë´‡ ê¸°ì´ˆ ëª¨ë¸ êµ¬ì¶•ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ê²½ë¡œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:55:15*