---
keywords:
  - Preference-based Reinforcement Learning
  - Diffusion Classifier
  - Conditional Diffusion Reward
  - Reinforcement Learning
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2503.01143
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:06:26.676580",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Preference-based Reinforcement Learning",
    "Diffusion Classifier",
    "Conditional Diffusion Reward",
    "Reinforcement Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Preference-based Reinforcement Learning": 0.75,
    "Diffusion Classifier": 0.7,
    "Conditional Diffusion Reward": 0.65,
    "Reinforcement Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Offline Preference-based Reinforcement Learning",
        "canonical": "Preference-based Reinforcement Learning",
        "aliases": [
          "PbRL"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a specific approach within reinforcement learning, facilitating connections to related works.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Diffusion Classifier",
        "canonical": "Diffusion Classifier",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel classification method that is pivotal to the paper's methodology, offering potential links to diffusion models.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Conditional Diffusion Preference-based Reward",
        "canonical": "Conditional Diffusion Reward",
        "aliases": [
          "C-DPR"
        ],
        "category": "unique_technical",
        "rationale": "Represents an advanced method proposed in the paper, enhancing the diffusion-based approach with conditional elements.",
        "novelty_score": 0.75,
        "connectivity_score": 0.55,
        "specificity_score": 0.9,
        "link_intent_score": 0.65
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A foundational concept that underpins the entire study, essential for linking to the broader field of machine learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "trajectory-wise preferences",
      "step-wise reward"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Offline Preference-based Reinforcement Learning",
      "resolved_canonical": "Preference-based Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Diffusion Classifier",
      "resolved_canonical": "Diffusion Classifier",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Conditional Diffusion Preference-based Reward",
      "resolved_canonical": "Conditional Diffusion Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.55,
        "specificity": 0.9,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Diffusion Classifier-Driven Reward for Offline Preference-based Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2503.01143.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2503.01143](https://arxiv.org/abs/2503.01143)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.2% similar)
- [[2025-09-25/DAWM_ Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions_20250925|DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions]] (84.9% similar)
- [[2025-09-24/Online Process Reward Leanring for Agentic Reinforcement Learning_20250924|Online Process Reward Leanring for Agentic Reinforcement Learning]] (84.8% similar)
- [[2025-09-24/MiCRo_ Mixture Modeling and Context-aware Routing for Personalized Preference Learning_20250924|MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning]] (84.3% similar)
- [[2025-09-25/Wavelet Fourier Diffuser_ Frequency-Aware Diffusion Model for Reinforcement Learning_20250925|Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning]] (84.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Preference-based Reinforcement Learning|Preference-based Reinforcement Learning]], [[keywords/Diffusion Classifier|Diffusion Classifier]], [[keywords/Conditional Diffusion Reward|Conditional Diffusion Reward]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.01143v3 Announce Type: replace 
Abstract: Offline preference-based reinforcement learning (PbRL) mitigates the need for reward definition, aligning with human preferences via preference-driven reward feedback without interacting with the environment. However, trajectory-wise preference labels are difficult to meet the precise learning of step-wise reward, thereby affecting the performance of downstream algorithms. To alleviate the insufficient step-wise reward caused by trajectory-wise preferences, we propose a novel preference-based reward acquisition method: Diffusion Preference-based Reward (DPR). DPR directly treats step-wise preference-based reward acquisition as a binary classification and utilizes the robustness of diffusion classifiers to infer step-wise rewards discriminatively. In addition, to further utilize trajectory-wise preference information, we propose Conditional Diffusion Preference-based Reward (C-DPR), which conditions on trajectory-wise preference labels to enhance reward inference. We apply the above methods to existing offline RL algorithms, and a series of experimental results demonstrate that the diffusion classifier-driven reward outperforms the previous reward acquisition method with the Bradley-Terry model.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜¤í”„ë¼ì¸ ì„ í˜¸ ê¸°ë°˜ ê°•í™” í•™ìŠµ(PbRL)ì—ì„œ ê²½ë¡œ ë‹¨ìœ„ì˜ ì„ í˜¸ ë ˆì´ë¸”ì´ ë‹¨ê³„ë³„ ë³´ìƒ í•™ìŠµì— ì–´ë ¤ì›€ì„ ì£¼ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ë³´ìƒ íšë“ ë°©ë²•ì¸ í™•ì‚° ì„ í˜¸ ê¸°ë°˜ ë³´ìƒ(DPR)ì„ ì œì•ˆí•©ë‹ˆë‹¤. DPRì€ ë‹¨ê³„ë³„ ë³´ìƒ íšë“ì„ ì´ì§„ ë¶„ë¥˜ë¡œ ì²˜ë¦¬í•˜ì—¬ í™•ì‚° ë¶„ë¥˜ê¸°ì˜ ê°•ë ¥í•¨ì„ í™œìš©í•´ ë³´ìƒì„ ì¶”ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, ê²½ë¡œ ë‹¨ìœ„ì˜ ì„ í˜¸ ì •ë³´ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ ì¡°ê±´ë¶€ í™•ì‚° ì„ í˜¸ ê¸°ë°˜ ë³´ìƒ(C-DPR)ì„ ì œì•ˆí•˜ì—¬ ë³´ìƒ ì¶”ë¡ ì„ ê°•í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, í™•ì‚° ë¶„ë¥˜ê¸°ë¥¼ í™œìš©í•œ ë³´ìƒì´ ê¸°ì¡´ì˜ Bradley-Terry ëª¨ë¸ ê¸°ë°˜ ë³´ìƒ íšë“ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì˜¤í”„ë¼ì¸ ì„ í˜¸ ê¸°ë°˜ ê°•í™” í•™ìŠµ(PbRL)ì€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš© ì—†ì´ ì„ í˜¸ ê¸°ë°˜ ë³´ìƒ í”¼ë“œë°±ì„ í†µí•´ ì¸ê°„ì˜ ì„ í˜¸ì™€ ì¼ì¹˜í•˜ëŠ” ë³´ìƒì„ ì •ì˜í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 2. ê¶¤ì  ê¸°ë°˜ ì„ í˜¸ ë ˆì´ë¸”ì€ ë‹¨ê³„ë³„ ë³´ìƒì˜ ì •í™•í•œ í•™ìŠµì„ ì¶©ì¡±ì‹œí‚¤ê¸° ì–´ë ¤ì›Œ, ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.
- 3. Diffusion Preference-based Reward (DPR)ì€ ë‹¨ê³„ë³„ ì„ í˜¸ ê¸°ë°˜ ë³´ìƒ íšë“ì„ ì´ì§„ ë¶„ë¥˜ë¡œ ì²˜ë¦¬í•˜ì—¬, í™•ì‚° ë¶„ë¥˜ê¸°ì˜ ê°•ê±´ì„±ì„ í™œìš©í•´ ë‹¨ê³„ë³„ ë³´ìƒì„ íŒë³„ì ìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
- 4. Conditional Diffusion Preference-based Reward (C-DPR)ì€ ê¶¤ì  ê¸°ë°˜ ì„ í˜¸ ë ˆì´ë¸”ì„ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬ ë³´ìƒ ì¶”ë¡ ì„ ê°•í™”í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, í™•ì‚° ë¶„ë¥˜ê¸° ê¸°ë°˜ ë³´ìƒì´ Bradley-Terry ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ì „ ë³´ìƒ íšë“ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 17:06:26*