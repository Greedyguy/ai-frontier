---
keywords:
  - Complaint Description from Videos
  - ComVID Dataset
  - VideoLLaMA2-7b Model
  - Retrieval Augmented Generation
  - Multimodal Learning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19952
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:54:56.484048",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Complaint Description from Videos",
    "ComVID Dataset",
    "VideoLLaMA2-7b Model",
    "Retrieval Augmented Generation",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Complaint Description from Videos": 0.8,
    "ComVID Dataset": 0.75,
    "VideoLLaMA2-7b Model": 0.78,
    "Retrieval Augmented Generation": 0.77,
    "Multimodal Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Complaint Description from Videos",
        "canonical": "Complaint Description from Videos",
        "aliases": [
          "CoD-V"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel task in complaint mining, linking video content to textual complaints.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "ComVID",
        "canonical": "ComVID Dataset",
        "aliases": [
          "Video Complaint Dataset"
        ],
        "category": "unique_technical",
        "rationale": "A specific dataset that supports research in multimodal complaint generation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "VideoLLaMA2-7b model",
        "canonical": "VideoLLaMA2-7b Model",
        "aliases": [
          "VideoLLaMA2"
        ],
        "category": "unique_technical",
        "rationale": "A specialized model for generating complaints from video, linking multimodal data with user emotions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "Enhances connectivity by linking retrieval methods with generative models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Facilitates connections between different data types like text and video in complaint generation.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "explainable complaint mining",
      "video summary generation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Complaint Description from Videos",
      "resolved_canonical": "Complaint Description from Videos",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "ComVID",
      "resolved_canonical": "ComVID Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "VideoLLaMA2-7b model",
      "resolved_canonical": "VideoLLaMA2-7b Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19952.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19952](https://arxiv.org/abs/2509.19952)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/VideoRFT_ Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning_20250923|VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning]] (81.4% similar)
- [[2025-09-24/VIBE_ Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR_20250924|VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR]] (80.8% similar)
- [[2025-09-23/SD-VSum_ A Method and Dataset for Script-Driven Video Summarization_20250923|SD-VSum: A Method and Dataset for Script-Driven Video Summarization]] (80.7% similar)
- [[2025-09-19/MovieCORE_ COgnitive REasoning in Movies_20250919|MovieCORE: COgnitive REasoning in Movies]] (80.6% similar)
- [[2025-09-24/LD-ViCE_ Latent Diffusion Model for Video Counterfactual Explanations_20250924|LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Complaint Description from Videos|Complaint Description from Videos]], [[keywords/ComVID Dataset|ComVID Dataset]], [[keywords/VideoLLaMA2-7b Model|VideoLLaMA2-7b Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19952v1 Announce Type: cross 
Abstract: While there exists a lot of work on explainable complaint mining, articulating user concerns through text or video remains a significant challenge, often leaving issues unresolved. Users frequently struggle to express their complaints clearly in text but can easily upload videos depicting product defects (e.g., vague text such as `worst product' paired with a 5-second video depicting a broken headphone with the right earcup). This paper formulates a new task in the field of complaint mining to aid the common users' need to write an expressive complaint, which is Complaint Description from Videos (CoD-V) (e.g., to help the above user articulate her complaint about the defective right earcup). To this end, we introduce ComVID, a video complaint dataset containing 1,175 complaint videos and the corresponding descriptions, also annotated with the emotional state of the complainer. Additionally, we present a new complaint retention (CR) evaluation metric that discriminates the proposed (CoD-V) task against standard video summary generation and description tasks. To strengthen this initiative, we introduce a multimodal Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to generate complaints while accounting for the user's emotional state. We conduct a comprehensive evaluation of several Video Language Models on several tasks (pre-trained and fine-tuned versions) with a range of established evaluation metrics, including METEOR, perplexity, and the Coleman-Liau readability score, among others. Our study lays the foundation for a new research direction to provide a platform for users to express complaints through video. Dataset and resources are available at: https://github.com/sarmistha-D/CoD-V.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ìš©ìë“¤ì´ ì œí’ˆ ë¶ˆë§Œì„ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ ë•ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê³¼ì œì¸ ë¹„ë””ì˜¤ë¥¼ í†µí•œ ë¶ˆë§Œ ì„¤ëª…(CoD-V)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ 1,175ê°œì˜ ë¶ˆë§Œ ë¹„ë””ì˜¤ì™€ ì„¤ëª…ì´ í¬í•¨ëœ ComVID ë°ì´í„°ì…‹ì„ ì†Œê°œí•˜ê³ , ë¶ˆë§Œ ìœ ì§€(CR) í‰ê°€ ì§€í‘œë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ê³ ë ¤í•˜ì—¬ ë¶ˆë§Œì„ ìƒì„±í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ RAG ì„ë² ë””ë“œ VideoLLaMA2-7b ëª¨ë¸ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì–¸ì–´ ëª¨ë¸ì„ METEOR, perplexity, Coleman-Liau ê°€ë…ì„± ì ìˆ˜ ë“±ì˜ í‰ê°€ ì§€í‘œë¡œ í‰ê°€í•˜ì—¬ ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ê³¼ ìì›ì€ GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‚¬ìš©ìë“¤ì´ í…ìŠ¤íŠ¸ë¡œ ë¶ˆë§Œì„ ëª…í™•íˆ í‘œí˜„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë¹„ë””ì˜¤ë¥¼ í†µí•œ ë¶ˆë§Œ ì„¤ëª…(CoD-V)ì´ë¼ëŠ” ìƒˆë¡œìš´ ê³¼ì œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ComVIDë¼ëŠ” ë¹„ë””ì˜¤ ë¶ˆë§Œ ë°ì´í„°ì…‹ì„ ì†Œê°œí•˜ë©°, 1,175ê°œì˜ ë¶ˆë§Œ ë¹„ë””ì˜¤ì™€ í•´ë‹¹ ì„¤ëª…, ë¶ˆë§Œìì˜ ê°ì • ìƒíƒœë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ìƒˆë¡œìš´ ë¶ˆë§Œ ìœ ì§€(CR) í‰ê°€ ì§€í‘œë¥¼ ì œì‹œí•˜ì—¬ CoD-V ê³¼ì œë¥¼ ê¸°ì¡´ì˜ ë¹„ë””ì˜¤ ìš”ì•½ ìƒì„± ë° ì„¤ëª… ê³¼ì œì™€ êµ¬ë³„í•©ë‹ˆë‹¤.
- 4. ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ê³ ë ¤í•˜ì—¬ ë¶ˆë§Œì„ ìƒì„±í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ RAG ì„ë² ë””ë“œ VideoLLaMA2-7b ëª¨ë¸ì„ ë„ì…í•©ë‹ˆë‹¤.
- 5. ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì–¸ì–´ ëª¨ë¸ì„ ì—¬ëŸ¬ ê³¼ì œì—ì„œ í‰ê°€í•˜ë©°, METEOR, perplexity, Coleman-Liau ê°€ë…ì„± ì ìˆ˜ ë“± ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:54:56*