---
keywords:
  - Mixture-of-Experts
  - Large Language Model
  - Orthogonality Loss
  - Variance Loss
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2505.22323
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:55:05.224407",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Large Language Model",
    "Orthogonality Loss",
    "Variance Loss"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.78,
    "Large Language Model": 0.72,
    "Orthogonality Loss": 0.7,
    "Variance Loss": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "Mixture-of-Experts is a distinct model architecture that enhances expert specialization, crucial for linking specialized research in neural networks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are foundational in NLP, providing a broad technical context for the study of expert specialization.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Orthogonality Loss",
        "canonical": "Orthogonality Loss",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Orthogonality Loss is a novel technique proposed to enhance expert specialization, offering a unique technical insight.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Variance Loss",
        "canonical": "Variance Loss",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Variance Loss is introduced as a complementary objective to improve routing decisions, providing a unique technical contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.58,
        "specificity_score": 0.78,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "auxiliary loss",
      "load balancing"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Orthogonality Loss",
      "resolved_canonical": "Orthogonality Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Variance Loss",
      "resolved_canonical": "Variance Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.58,
        "specificity": 0.78,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Advancing Expert Specialization for Better MoE

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.22323.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2505.22323](https://arxiv.org/abs/2505.22323)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Symphony-MoE_ Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts_20250924|Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts]] (91.4% similar)
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (90.2% similar)
- [[2025-09-24/Union of Experts_ Adapting Hierarchical Routing to Equivalently Decomposed Transformer_20250924|Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer]] (89.8% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (89.2% similar)
- [[2025-09-23/GuiLoMo_ Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors_20250923|GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors]] (88.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Orthogonality Loss|Orthogonality Loss]], [[keywords/Variance Loss|Variance Loss]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.22323v2 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.

## ğŸ“ ìš”ì•½

Mixture-of-Experts (MoE) ëª¨ë¸ì€ ì…ë ¥ë‹¹ ì¼ë¶€ ì „ë¬¸ê°€ë§Œ í™œì„±í™”í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì  í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ë¶€ê°€ì ì¸ ë¶€í•˜ ê· í˜• ì†ì‹¤ì€ ì „ë¬¸ê°€ ê°„ì˜ ì¤‘ë³µê³¼ ì§€ë‚˜ì¹˜ê²Œ ê· ì¼í•œ ë¼ìš°íŒ…ì„ ì´ˆë˜í•˜ì—¬ ì „ë¬¸ê°€ì˜ ì „ë¬¸í™”ì™€ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚µë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ëª©í‘œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤: (1) ì „ë¬¸ê°€ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ í† í°ì„ ì²˜ë¦¬í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì§êµì„± ì†ì‹¤ê³¼ (2) ë³´ë‹¤ ì°¨ë³„í™”ëœ ë¼ìš°íŒ… ê²°ì •ì„ ìœ ë„í•˜ëŠ” ë¶„ì‚° ì†ì‹¤ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ìš°ë¦¬ì˜ ë°©ë²•ì€ ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì „ë¬¸ê°€ì˜ ì „ë¬¸í™”ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìœ¼ë©°, ê¸°ì¡´ MoE ëª¨ë¸ ëŒ€ë¹„ ìµœëŒ€ 23.79% ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì•„í‚¤í…ì²˜ ìˆ˜ì • ì—†ì´ë„ ë¶€í•˜ ê· í˜•ì„ ìœ ì§€í•˜ë©°, ì½”ë“œë¥¼ ê³µê°œí•˜ì—¬ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— ê¸°ì—¬í•  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Mixture-of-Experts (MoE) ëª¨ë¸ì€ ì…ë ¥ë‹¹ ì¼ë¶€ ì „ë¬¸ê°€ë§Œ í™œì„±í™”í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì  í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 2. ê¸°ì¡´ì˜ ë¶€ê°€ì ì¸ ë¶€í•˜ ê· í˜• ì†ì‹¤ì€ ì „ë¬¸ê°€ì˜ ì¤‘ë³µê³¼ ì§€ë‚˜ì¹˜ê²Œ ê· ì¼í•œ ë¼ìš°íŒ…ì„ ìœ ë°œí•˜ì—¬ ì „ë¬¸ê°€ì˜ ì „ë¬¸í™”ë¥¼ ë°©í•´í•˜ê³  ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¨ë‹¤.
- 3. ì œì•ˆëœ í•´ê²°ì±…ì€ ì „ë¬¸ê°€ê°€ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ í† í°ì„ ì²˜ë¦¬í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì§êµ ì†ì‹¤ê³¼ ë” ë¶„ë³„ë ¥ ìˆëŠ” ë¼ìš°íŒ… ê²°ì •ì„ ìœ ë„í•˜ëŠ” ë¶„ì‚° ì†ì‹¤ì„ í¬í•¨í•œë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì „ë¬¸ê°€ì˜ ì „ë¬¸í™”ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ë¶€ê°€ì ì¸ ë¶€í•˜ ê· í˜• ì†ì‹¤ì„ ì‚¬ìš©í•˜ëŠ” ê¸°ì¡´ MoE ê¸°ì¤€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 23.79%ê¹Œì§€ ê°œì„ í•œë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ì•„í‚¤í…ì²˜ ìˆ˜ì •ì´ë‚˜ ì¶”ê°€ êµ¬ì„± ìš”ì†Œ ì—†ì´ë„ í•˜ìœ„ ì‘ì—…ì—ì„œ ë¶€í•˜ ê· í˜•ì„ ìœ ì§€í•˜ë©°, ì½”ë“œê°€ ê³µê°œë  ì˜ˆì •ì´ë‹¤.


---

*Generated on 2025-09-26 08:55:05*