---
keywords:
  - Large Language Model
  - Speech Large Models
  - Multilingual Speech Modality Alignment
  - Progressive Alignment Representation Training
  - Multilingual Understanding
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.19745
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:44:51.483821",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Speech Large Models",
    "Multilingual Speech Modality Alignment",
    "Progressive Alignment Representation Training",
    "Multilingual Understanding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Speech Large Models": 0.7,
    "Multilingual Speech Modality Alignment": 0.75,
    "Progressive Alignment Representation Training": 0.8,
    "Multilingual Understanding": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and connect to a wide range of NLP tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Speech Large Models",
        "canonical": "Speech Large Models",
        "aliases": [
          "SLMs"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new concept of large models specifically for speech, which is pivotal for the paper's focus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Multilingual Speech Modality Alignment",
        "canonical": "Multilingual Speech Modality Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique technical term that encapsulates the paper's main contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Progressive Alignment Representation Training",
        "canonical": "Progressive Alignment Representation Training",
        "aliases": [
          "PART"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel method introduced by the paper, essential for understanding its contribution.",
        "novelty_score": 0.9,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multilingual Understanding",
        "canonical": "Multilingual Understanding",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key to the paper's exploration of cross-language tasks, enhancing connectivity with multilingual NLP research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Speech Large Models",
      "resolved_canonical": "Speech Large Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Multilingual Speech Modality Alignment",
      "resolved_canonical": "Multilingual Speech Modality Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Progressive Alignment Representation Training",
      "resolved_canonical": "Progressive Alignment Representation Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multilingual Understanding",
      "resolved_canonical": "Multilingual Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19745.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.19745](https://arxiv.org/abs/2509.19745)

## 🔗 유사한 논문
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (87.3% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (86.0% similar)
- [[2025-09-25/Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning_20250925|Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning]] (85.7% similar)
- [[2025-09-23/Probabilistic Token Alignment for Large Language Model Fusion_20250923|Probabilistic Token Alignment for Large Language Model Fusion]] (85.2% similar)
- [[2025-09-24/Explore the Reinforcement Learning for the LLM based ASR and TTS system_20250924|Explore the Reinforcement Learning for the LLM based ASR and TTS system]] (85.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multilingual Understanding|Multilingual Understanding]]
**⚡ Unique Technical**: [[keywords/Speech Large Models|Speech Large Models]], [[keywords/Multilingual Speech Modality Alignment|Multilingual Speech Modality Alignment]], [[keywords/Progressive Alignment Representation Training|Progressive Alignment Representation Training]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19745v1 Announce Type: new 
Abstract: Large language models (LLMs) have expanded from text to speech, giving rise to Speech Large Models (SLMs) that support recognition, translation, and synthesis. A key challenge is aligning speech and text representations, which becomes harder in multilingual settings. Existing methods often freeze LLM parameters and train encoders on multilingual data, but this forces cross-language convergence and limits performance. We introduce Progressive Alignment Representation Training (PART), a multi-stage and multi-task framework that separates within-language from cross-language alignment. During cross-language training, LLM parameters are dynamically activated, and text-based tasks are later introduced to enhance multilingual understanding. Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART surpasses conventional approaches, with analysis confirming its ability to balance language-specific distinctions and cross-language generalization. These results demonstrate PART's effectiveness and generality for multilingual speech modality alignment.

## 📝 요약

대형 언어 모델(LLM)이 텍스트에서 음성으로 확장되면서 음성 인식, 번역, 합성을 지원하는 대형 음성 모델(SLM)이 등장했습니다. 그러나 다국어 환경에서 음성과 텍스트 표현을 정렬하는 것은 어려운 과제입니다. 기존 방법은 LLM의 매개변수를 고정하고 다국어 데이터를 기반으로 인코더를 훈련하지만, 이는 언어 간 수렴을 강요하여 성능을 제한합니다. 본 연구에서는 언어 내 정렬과 언어 간 정렬을 분리하는 다단계, 다과제 프레임워크인 Progressive Alignment Representation Training (PART)을 제안합니다. 언어 간 훈련 중에는 LLM 매개변수가 동적으로 활성화되며, 이후 텍스트 기반 과제를 도입하여 다국어 이해를 강화합니다. CommonVoice 15, Fleurs, Wenetspeech, CoVoST2에서의 실험 결과, PART는 기존 방법을 능가하며, 언어별 구별과 언어 간 일반화를 균형 있게 달성함을 확인했습니다. 이러한 결과는 PART의 다국어 음성 정렬에 대한 효과성과 일반성을 입증합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)이 음성 인식, 번역, 합성을 지원하는 대형 음성 모델(SLM)로 확장되었습니다.
- 2. 다국어 환경에서 음성과 텍스트 표현을 정렬하는 것이 주요 과제로 대두되었습니다.
- 3. 기존 방법은 LLM 매개변수를 고정하고 다국어 데이터로 인코더를 훈련하지만, 이는 언어 간 수렴을 강요하여 성능을 제한합니다.
- 4. 우리는 언어 내 정렬과 언어 간 정렬을 분리하는 Progressive Alignment Representation Training (PART)을 도입했습니다.
- 5. 실험 결과, PART는 기존 접근 방식을 능가하며, 언어별 차별성과 언어 간 일반화를 균형 있게 유지하는 능력을 확인했습니다.


---

*Generated on 2025-09-26 08:44:51*