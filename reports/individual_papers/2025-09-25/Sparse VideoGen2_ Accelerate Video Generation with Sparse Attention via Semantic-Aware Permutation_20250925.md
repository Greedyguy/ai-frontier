---
keywords:
  - Transformer
  - Attention Mechanism
  - Semantic Clustering
  - Resource Management
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2505.18875
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:25:19.065187",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Attention Mechanism",
    "Semantic Clustering",
    "Resource Management"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Attention Mechanism": 0.88,
    "Semantic Clustering": 0.8,
    "Resource Management": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformers",
        "canonical": "Transformer",
        "aliases": [
          "DiTs"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in machine learning, and linking to them provides context for understanding the architecture used in video generation.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sparse Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Sparse Attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are critical in optimizing computational efficiency, and sparse attention is a specific variant that enhances understanding of the method's efficiency.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Semantic-Aware Permutation",
        "canonical": "Semantic Clustering",
        "aliases": [
          "Semantic Reordering"
        ],
        "category": "unique_technical",
        "rationale": "This technique is unique to the paper and provides insights into how semantic similarity is leveraged for efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dynamic Budget Control",
        "canonical": "Resource Management",
        "aliases": [
          "Budget Control"
        ],
        "category": "unique_technical",
        "rationale": "Understanding resource management techniques is crucial for optimizing computational processes in machine learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "framework",
      "efficiency",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sparse Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Semantic-Aware Permutation",
      "resolved_canonical": "Semantic Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dynamic Budget Control",
      "resolved_canonical": "Resource Management",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.18875.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2505.18875](https://arxiv.org/abs/2505.18875)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/SparseDiT_ Token Sparsification for Efficient Diffusion Transformer_20250924|SparseDiT: Token Sparsification for Efficient Diffusion Transformer]] (87.7% similar)
- [[2025-09-23/FG-Attn_ Leveraging Fine-Grained Sparsity In Diffusion Transformers_20250923|FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers]] (87.4% similar)
- [[2025-09-17/BWCache_ Accelerating Video Diffusion Transformers through Block-Wise Caching_20250917|BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching]] (86.3% similar)
- [[2025-09-25/From Slow Bidirectional to Fast Autoregressive Video Diffusion Models_20250925|From Slow Bidirectional to Fast Autoregressive Video Diffusion Models]] (86.1% similar)
- [[2025-09-23/QVGen_ Pushing the Limit of Quantized Video Generative Models_20250923|QVGen: Pushing the Limit of Quantized Video Generative Models]] (85.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Semantic Clustering|Semantic Clustering]], [[keywords/Resource Management|Resource Management]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.18875v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ë””ì˜¤ ìƒì„±ì— í•„ìˆ˜ì ì¸ Diffusion Transformers(DiTs)ì˜ ì£¼ì˜ë ¥ ê³„ì‚° ë³µì¡ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ SVG2ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ í† í°ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì •í™•í•œ í† í° ì‹ë³„ì— ì‹¤íŒ¨í•˜ê³ , GPU ìµœì í™”ì— ë¹„íš¨ìœ¨ì ì¸ ê³„ì‚° ë‚­ë¹„ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. SVG2ëŠ” ì˜ë¯¸ ê¸°ë°˜ì˜ í† í° ì¬ë°°ì¹˜ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, k-í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ í† í°ì„ ì˜ë¯¸ì ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•˜ê³  ì¬ë°°ì—´í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •í™•í•œ í´ëŸ¬ìŠ¤í„° í‘œí˜„ê³¼ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, SVG2ëŠ” ë™ì  ì˜ˆì‚° ì œì–´ì™€ ë§ì¶¤í˜• ì»¤ë„ êµ¬í˜„ì„ í†µí•©í•˜ì—¬ HunyuanVideoì™€ Wan 2.1ì—ì„œ ê°ê° ìµœëŒ€ 2.30ë°°, 1.89ë°°ì˜ ì†ë„ í–¥ìƒì„ ì´ë£¨ë©´ì„œ PSNRì„ 30ê³¼ 26ê¹Œì§€ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DiTsëŠ” ë¹„ë””ì˜¤ ìƒì„±ì— í•„ìˆ˜ì ì´ì§€ë§Œ, ì£¼ì˜ë ¥ì˜ ì´ì°¨ì  ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ì§€ì—°ì´ ë°œìƒí•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ì€ ìœ„ì¹˜ ê¸°ë°˜ í† í° í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì¸í•´ ì •í™•í•œ í† í° ì‹ë³„ì— ì‹¤íŒ¨í•©ë‹ˆë‹¤.
- 3. SVG2ëŠ” ì˜ë¯¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ í† í° ì‹ë³„ ì •í™•ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ê³„ì‚° ë‚­ë¹„ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.
- 4. SVG2ëŠ” top-p ë™ì  ì˜ˆì‚° ì œì–´ì™€ ë§ì¶¤í˜• ì»¤ë„ êµ¬í˜„ì„ í†µí•©í•˜ì—¬ ìµœëŒ€ 2.30ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 5. HunyuanVideoì™€ Wan 2.1ì—ì„œ ê°ê° ìµœëŒ€ PSNR 30ê³¼ 26ì„ ìœ ì§€í•˜ë©° íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:25:19*