---
keywords:
  - Large Language Model
  - Few-Shot Learning
  - In-context Learning
  - Evaluation Criteria
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2407.10999
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:15:28.272940",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Few-Shot Learning",
    "In-context Learning",
    "Evaluation Criteria"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Few-Shot Learning": 0.8,
    "In-context Learning": 0.82,
    "Evaluation Criteria": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Model"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluation methods for language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Zero-shot and Few-shot",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Zero-shot Learning",
          "Few-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the combination of learning paradigms used in the evaluation approach.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "In-context Learning",
        "canonical": "In-context Learning",
        "aliases": [
          "ICL"
        ],
        "category": "unique_technical",
        "rationale": "Key technique used for teaching models in the proposed evaluation method.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Evaluation Criteria",
        "canonical": "Evaluation Criteria",
        "aliases": [
          "In-house Criteria",
          "Criteria Division"
        ],
        "category": "unique_technical",
        "rationale": "Specific focus on tailoring evaluation metrics to domain-specific needs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "model-based evaluation",
      "prompt paradigm",
      "engineering approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Zero-shot and Few-shot",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "In-context Learning",
      "resolved_canonical": "In-context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Evaluation Criteria",
      "resolved_canonical": "Evaluation Criteria",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2407.10999.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2407.10999](https://arxiv.org/abs/2407.10999)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Integrated Framework for LLM Evaluation with Answer Generation_20250925|Integrated Framework for LLM Evaluation with Answer Generation]] (85.6% similar)
- [[2025-09-25/Do Before You Judge_ Self-Reference as a Pathway to Better LLM Evaluation_20250925|Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation]] (85.4% similar)
- [[2025-09-25/Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers_20250925|Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers]] (85.2% similar)
- [[2025-09-22/Beyond Pointwise Scores_ Decomposed Criteria-Based Evaluation of LLM Responses_20250922|Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses]] (85.2% similar)
- [[2025-09-23/Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues_20250923|Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/In-context Learning|In-context Learning]], [[keywords/Evaluation Criteria|Evaluation Criteria]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2407.10999v2 Announce Type: replace-cross 
Abstract: With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important. Measuring text generation tasks such as summarization and article creation is very difficult. Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult. So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming. In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria. In addition, we try combining zero-shot and few-shot to make the judge model focus on more information. We also propose a prompt paradigm and an engineering approach to adjust and iterate the shots ,helping judge model to better understand the complex criteria. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks. The code is released in https://github.com/zlkqz/auto_eval

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í‰ê°€ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ LLM í‰ê°€ê°€ ìˆ˜ì‘ì—…ì— ì˜ì¡´í•˜ì—¬ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì‚¬ìš©ì ë§ì¶¤í˜• í‰ê°€ ê¸°ì¤€ì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ê¸°ë°˜ í‰ê°€ ë°©ë²•ì¸ TALECë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. TALECëŠ” in-context learning(ICL)ì„ í™œìš©í•˜ì—¬ í‰ê°€ ëª¨ë¸ì´ ë‚´ë¶€ ê¸°ì¤€ì„ í•™ìŠµí•˜ë„ë¡ í•˜ë©°, ì œë¡œìƒ·ê³¼ í“¨ìƒ·ì„ ê²°í•©í•˜ì—¬ ë” ë§ì€ ì •ë³´ë¥¼ ë°˜ì˜í•˜ë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, í”„ë¡¬í”„íŠ¸ íŒ¨ëŸ¬ë‹¤ì„ê³¼ ì—”ì§€ë‹ˆì–´ë§ ì ‘ê·¼ë²•ì„ í†µí•´ í‰ê°€ ëª¨ë¸ì´ ë³µì¡í•œ ê¸°ì¤€ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, TALECëŠ” ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ë©°, ì¸ê°„ í‰ê°€ì™€ 80% ì´ìƒì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì¼ë¶€ ì‘ì—…ì—ì„œ ì¸ê°„ ê°„ ìƒê´€ê´€ê³„ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ê³¼ì…ë‹ˆë‹¤. ì½”ë“œë„ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í‰ê°€ ì¤‘ìš”ì„±ì´ ì¦ê°€í•˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ íŠ¹ì • ì‘ìš© ë¶„ì•¼ì—ì„œ í‰ê°€ ê¸°ì¤€ì´ ë”ìš± ë³µì¡í•´ì§€ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ LLM í‰ê°€ëŠ” ì£¼ë¡œ ìˆ˜ë™ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì‹œê°„ì´ ì†Œìš”ëœë‹¤.
- 3. TALECë¼ëŠ” ëª¨ë¸ ê¸°ë°˜ í‰ê°€ ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ì‚¬ìš©ìë“¤ì´ ìœ ì—°í•˜ê²Œ í‰ê°€ ê¸°ì¤€ì„ ì„¤ì •í•˜ê³ , in-context learning(ICL)ì„ í†µí•´ ëª¨ë¸ì´ ë‚´ë¶€ ê¸°ì¤€ì„ í•™ìŠµí•˜ë„ë¡ í•œë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ zero-shotê³¼ few-shotì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ì´ ë” ë§ì€ ì •ë³´ë¥¼ ì§‘ì¤‘í•˜ë„ë¡ í•˜ë©°, í”„ë¡¬í”„íŠ¸ íŒ¨ëŸ¬ë‹¤ì„ê³¼ ì—”ì§€ë‹ˆì–´ë§ ì ‘ê·¼ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ì´í•´ë„ë¥¼ ë†’ì¸ë‹¤.
- 5. TALECëŠ” ì¸ê°„ì˜ ì„ í˜¸ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ë©°, ì¸ê°„ í‰ê°€ì™€ì˜ ìƒê´€ê´€ê³„ê°€ 80% ì´ìƒìœ¼ë¡œ, ì¼ë¶€ ì‘ì—…ì—ì„œëŠ” ì¸ê°„ ê°„ ìƒê´€ê´€ê³„ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.


---

*Generated on 2025-09-25 16:15:28*