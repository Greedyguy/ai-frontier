---
keywords:
  - Large Language Model
  - Activation Sparsity
  - Optimal Brain Compression
  - Structured Pruning
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2506.20194
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:08:41.208850",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Activation Sparsity",
    "Optimal Brain Compression",
    "Structured Pruning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Activation Sparsity": 0.78,
    "Optimal Brain Compression": 0.77,
    "Structured Pruning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on pruning and optimization, providing a strong link to existing research on model efficiency.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Activation Sparsity",
        "canonical": "Activation Sparsity",
        "aliases": [
          "Dynamic Structured Weight Sparsity"
        ],
        "category": "unique_technical",
        "rationale": "Activation Sparsity is a novel concept in the paper, crucial for understanding the proposed dual-sparsity framework.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Optimal Brain Compression",
        "canonical": "Optimal Brain Compression",
        "aliases": [
          "OBC"
        ],
        "category": "unique_technical",
        "rationale": "The Optimal Brain Compression framework is extended in the paper, making it a key technical contribution.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Structured Pruning",
        "canonical": "Structured Pruning",
        "aliases": [
          "Structured Sparsity"
        ],
        "category": "specific_connectable",
        "rationale": "Structured Pruning is compared against the proposed method, providing a basis for evaluating the paper's contributions.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "accuracy",
      "baseline"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Activation Sparsity",
      "resolved_canonical": "Activation Sparsity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Optimal Brain Compression",
      "resolved_canonical": "Optimal Brain Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Structured Pruning",
      "resolved_canonical": "Structured Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.20194.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2506.20194](https://arxiv.org/abs/2506.20194)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Speculate Deep and Accurate_ Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding_20250924|Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding]] (85.1% similar)
- [[2025-09-23/GALLa_ Graph Aligned Large Language Models for Improved Source Code Understanding_20250923|GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding]] (84.9% similar)
- [[2025-09-24/Sparse Training Scheme for Multimodal LLM_20250924|Sparse Training Scheme for Multimodal LLM]] (84.3% similar)
- [[2025-09-17/NIRVANA_ Structured pruning reimagined for large language models compression_20250917|NIRVANA: Structured pruning reimagined for large language models compression]] (83.3% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (83.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Structured Pruning|Structured Pruning]]
**âš¡ Unique Technical**: [[keywords/Activation Sparsity|Activation Sparsity]], [[keywords/Optimal Brain Compression|Optimal Brain Compression]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.20194v2 Announce Type: replace 
Abstract: Large language models (LLMs) deliver strong performance but are difficult to deploy due to high memory and compute costs. While pruning reduces these demands, most methods ignore activation sparsity observed at runtime. We reinterpret activation sparsity as dynamic structured weight sparsity and propose DuoGPT, a unified framework that constructs dual-sparse (spMspV) workloads by combining unstructured weight pruning with activation sparsity. To preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with activation-aware calibration and introduce output residuals from the dense model as correction terms. We further optimize the solution for efficient GPU execution, enabling scalability to billion-parameter LLMs. Evaluations on LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$ compared to the baseline dense model. Code is available at Github.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë†’ì€ ë©”ëª¨ë¦¬ì™€ ì—°ì‚° ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DuoGPTë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. DuoGPTëŠ” ì‹¤í–‰ ì‹œ ë°œìƒí•˜ëŠ” í™œì„±í™” í¬ì†Œì„±ì„ ë™ì  êµ¬ì¡°ì  ê°€ì¤‘ì¹˜ í¬ì†Œì„±ìœ¼ë¡œ ì¬í•´ì„í•˜ì—¬, ë¹„êµ¬ì¡°ì  ê°€ì¤‘ì¹˜ ê°€ì§€ì¹˜ê¸°ì™€ í™œì„±í™” í¬ì†Œì„±ì„ ê²°í•©í•œ ì´ì¤‘ í¬ì†Œ ì‘ì—…ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ìµœì ì˜ ë‡Œ ì••ì¶•(OBC) í”„ë ˆì„ì›Œí¬ë¥¼ í™œì„±í™” ì¸ì‹ ë³´ì •ìœ¼ë¡œ í™•ì¥í•˜ê³ , ë°€ì§‘ ëª¨ë¸ì˜ ì¶œë ¥ ì”ì°¨ë¥¼ ë³´ì • í•­ìœ¼ë¡œ ë„ì…í•©ë‹ˆë‹¤. DuoGPTëŠ” GPU ì‹¤í–‰ íš¨ìœ¨ì„±ì„ ìµœì í™”í•˜ì—¬ ìˆ˜ì‹­ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ LLMì— í™•ì¥ ê°€ëŠ¥í•˜ë©°, LLaMA-2 ë° LLaMA-3 í‰ê°€ì—ì„œ ê¸°ì¡´ êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸° ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 9.17% ë†’ì€ ì •í™•ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œëŠ” Githubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DuoGPTëŠ” ì‹¤í–‰ ì‹œ ê´€ì°°ë˜ëŠ” í™œì„±í™” í¬ì†Œì„±ì„ ë™ì  êµ¬ì¡°ì  ê°€ì¤‘ì¹˜ í¬ì†Œì„±ìœ¼ë¡œ ì¬í•´ì„í•˜ì—¬, ë¹„êµ¬ì¡°ì  ê°€ì¤‘ì¹˜ ê°€ì§€ì¹˜ê¸°ì™€ í™œì„±í™” í¬ì†Œì„±ì„ ê²°í•©í•œ ì´ì¤‘ í¬ì†Œ ì‘ì—… ë¶€í•˜ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
- 2. ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´, í™œì„±í™” ì¸ì‹ ë³´ì •ì„ í†µí•´ ìµœì ì˜ ë‡Œ ì••ì¶•(OBC) í”„ë ˆì„ì›Œí¬ë¥¼ í™•ì¥í•˜ê³ , ë°€ì§‘ ëª¨ë¸ì˜ ì¶œë ¥ ì”ì°¨ë¥¼ ë³´ì • í•­ìœ¼ë¡œ ë„ì…í•©ë‹ˆë‹¤.
- 3. DuoGPTëŠ” íš¨ìœ¨ì ì¸ GPU ì‹¤í–‰ì„ ìœ„í•´ ì†”ë£¨ì…˜ì„ ìµœì í™”í•˜ì—¬, ìˆ˜ì‹­ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ë¡œì˜ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. LLaMA-2 ë° LLaMA-3ì— ëŒ€í•œ í‰ê°€ì—ì„œ DuoGPTëŠ” ìµœì²¨ë‹¨ êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸° ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 9.17% ë” ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•˜ë©°, ê¸°ì¤€ ë°€ì§‘ ëª¨ë¸ ëŒ€ë¹„ 1.39ë°°ì˜ ë™ì¼ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 5. DuoGPTì˜ ì½”ë“œëŠ” Githubì—ì„œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 17:08:41*