---
keywords:
  - Transformer
  - Attention Mechanism
  - LEDiT
  - Locality Enhancement Module
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2503.04344
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:23:38.338940",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Attention Mechanism",
    "LEDiT",
    "Locality Enhancement Module"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Attention Mechanism": 0.9,
    "LEDiT": 0.8,
    "Locality Enhancement Module": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformer",
        "canonical": "Transformer",
        "aliases": [
          "DiT",
          "Diffusion Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in deep learning, and linking to them provides a broad technical context.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "causal attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "causal attention mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Causal attention is a specific type of attention mechanism, crucial for understanding the paper's approach.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Length-Extrapolatable Diffusion Transformer",
        "canonical": "LEDiT",
        "aliases": [
          "Length-Extrapolatable DiT"
        ],
        "category": "unique_technical",
        "rationale": "LEDiT is a novel concept introduced in the paper, representing a significant technical innovation.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.95,
        "link_intent_score": 0.8
      },
      {
        "surface": "locality enhancement module",
        "canonical": "Locality Enhancement Module",
        "aliases": [
          "locality module"
        ],
        "category": "unique_technical",
        "rationale": "This module is a unique component of the proposed model, enhancing its technical specificity.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "positional encoding",
      "resolution scaling"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "causal attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Length-Extrapolatable Diffusion Transformer",
      "resolved_canonical": "LEDiT",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.95,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "locality enhancement module",
      "resolved_canonical": "Locality Enhancement Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# LEDiT: Your Length-Extrapolatable Diffusion Transformer without Positional Encoding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2503.04344.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2503.04344](https://arxiv.org/abs/2503.04344)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/In-Context Edit_ Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer_20250923|In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer]] (83.0% similar)
- [[2025-09-23/LRQ-DiT_ Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation_20250923|LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation]] (82.6% similar)
- [[2025-09-23/Context-aware Biases for Length Extrapolation_20250923|Context-aware Biases for Length Extrapolation]] (82.5% similar)
- [[2025-09-24/Foresight_ Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation_20250924|Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation]] (82.1% similar)
- [[2025-09-24/SparseDiT_ Token Sparsification for Efficient Diffusion Transformer_20250924|SparseDiT: Token Sparsification for Efficient Diffusion Transformer]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/LEDiT|LEDiT]], [[keywords/Locality Enhancement Module|Locality Enhancement Module]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.04344v3 Announce Type: replace 
Abstract: Diffusion transformers (DiTs) struggle to generate images at resolutions higher than their training resolutions. The primary obstacle is that the explicit positional encodings(PE), such as RoPE, need extrapolating to unseen positions which degrades performance when the inference resolution differs from training. In this paper, We propose a Length-Extrapolatable Diffusion Transformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs, thereby avoiding PE extrapolation. The key innovation of LEDiT lies in the use of causal attention. We demonstrate that causal attention can implicitly encode global positional information and show that such information facilitates extrapolation. We further introduce a locality enhancement module, which captures fine-grained local information to complement the global coarse-grained position information encoded by causal attention. Experimental results on both conditional and text-to-image generation tasks demonstrate that LEDiT supports up to 4x resolution scaling (e.g., from 256x256 to 512x512), achieving better image quality compared to the state-of-the-art length extrapolation methods. We believe that LEDiT marks a departure from the standard RoPE-based methods and offers a promising insight into length extrapolation. Project page: https://shenzhang2145.github.io/ledit/

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ Length-Extrapolatable Diffusion Transformer(LEDiT)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ Diffusion TransformersëŠ” ëª…ì‹œì  ìœ„ì¹˜ ì¸ì½”ë”©(PE) ë¬¸ì œë¡œ ì¸í•´ í›ˆë ¨ í•´ìƒë„ë¥¼ ì´ˆê³¼í•˜ëŠ” ì´ë¯¸ì§€ ìƒì„±ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. LEDiTëŠ” ëª…ì‹œì  PEë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì¸ê³¼ì  ì£¼ì˜ë¥¼ í†µí•´ ì „ì—­ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•”ë¬µì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ë˜í•œ, ì„¸ë°€í•œ ì§€ì—­ ì •ë³´ë¥¼ í¬ì°©í•˜ëŠ” locality enhancement ëª¨ë“ˆì„ ë„ì…í•˜ì—¬ ì¸ê³¼ì  ì£¼ì˜ë¡œ ì¸ì½”ë”©ëœ ì „ì—­ ì •ë³´ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, LEDiTëŠ” ìµœëŒ€ 4ë°° í•´ìƒë„ í™•ì¥ì„ ì§€ì›í•˜ë©°, ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì´ë¯¸ì§€ í’ˆì§ˆì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” RoPE ê¸°ë°˜ ë°©ë²•ì—ì„œ ë²—ì–´ë‚˜ ìƒˆë¡œìš´ ê¸¸ì„ ì œì‹œí•˜ëŠ” ì¤‘ìš”í•œ ê¸°ì—¬ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Diffusion transformersëŠ” í›ˆë ¨ í•´ìƒë„ë³´ë‹¤ ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
- 2. LEDiTëŠ” ëª…ì‹œì  ìœ„ì¹˜ ì¸ì½”ë”© ì—†ì´ë„ ì‘ë™í•˜ì—¬ ìœ„ì¹˜ ì¸ì½”ë”©ì˜ ì™¸ì‚½ ë¬¸ì œë¥¼ í”¼í•©ë‹ˆë‹¤.
- 3. LEDiTì˜ í•µì‹¬ í˜ì‹ ì€ ì¸ê³¼ì  ì£¼ì˜(attention)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•”ë¬µì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, LEDiTëŠ” ìµœëŒ€ 4ë°° í•´ìƒë„ í™•ì¥ì„ ì§€ì›í•˜ë©°, ê¸°ì¡´ì˜ ê¸¸ì´ ì™¸ì‚½ ë°©ë²•ë“¤ë³´ë‹¤ ë” ë‚˜ì€ ì´ë¯¸ì§€ í’ˆì§ˆì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. LEDiTëŠ” RoPE ê¸°ë°˜ ë°©ë²•ì—ì„œ ë²—ì–´ë‚˜ ê¸¸ì´ ì™¸ì‚½ì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:23:38*