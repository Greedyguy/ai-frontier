---
keywords:
  - Transformer
  - Continual Learning
  - Low-Rank Adaptation
  - Dynamic Memory
  - Parameter-Efficient Fine-Tuning
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2411.00623
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:36:42.578642",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Continual Learning",
    "Low-Rank Adaptation",
    "Dynamic Memory",
    "Parameter-Efficient Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Continual Learning": 0.88,
    "Low-Rank Adaptation": 0.82,
    "Dynamic Memory": 0.8,
    "Parameter-Efficient Fine-Tuning": 0.84
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the discussed continual learning approach, linking to broader transformer-based research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "CL"
        ],
        "category": "specific_connectable",
        "rationale": "Continual learning is the core focus of the paper, connecting to ongoing research in adaptive learning systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Low-Rank Adaptation",
        "canonical": "Low-Rank Adaptation",
        "aliases": [
          "LoRA"
        ],
        "category": "unique_technical",
        "rationale": "Low-Rank Adaptation is a key technique explored in the paper, offering a unique approach to parameter-efficient tuning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Dynamic Memory",
        "canonical": "Dynamic Memory",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Dynamic memory is crucial for balancing stability and plasticity in the proposed method, linking to memory mechanisms in AI.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Parameter-Efficient Fine-Tuning",
        "canonical": "Parameter-Efficient Fine-Tuning",
        "aliases": [
          "PEFT"
        ],
        "category": "specific_connectable",
        "rationale": "PEFT is a significant aspect of the paper's approach, connecting to broader discussions on efficient model adaptation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.83,
        "specificity_score": 0.77,
        "link_intent_score": 0.84
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Low-Rank Adaptation",
      "resolved_canonical": "Low-Rank Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Dynamic Memory",
      "resolved_canonical": "Dynamic Memory",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Parameter-Efficient Fine-Tuning",
      "resolved_canonical": "Parameter-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.83,
        "specificity": 0.77,
        "link_intent": 0.84
      }
    }
  ]
}
-->

# Replay-Free Continual Low-Rank Adaptation with Dynamic Memory

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2411.00623.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2411.00623](https://arxiv.org/abs/2411.00623)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM: Hierarchical Adapter Merging for Scalable Continual Learning]] (86.2% similar)
- [[2025-09-23/Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA_20250923|Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA]] (85.8% similar)
- [[2025-09-25/Faster Than SVD, Smarter Than SGD_ The OPLoRA Alternating Update_20250925|Faster Than SVD, Smarter Than SGD: The OPLoRA Alternating Update]] (85.8% similar)
- [[2025-09-25/Localized LoRA_ A Structured Low-Rank Approximation for Efficient Fine-Tuning_20250925|Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning]] (85.6% similar)
- [[2025-09-25/TensLoRA_ Tensor Alternatives for Low-Rank Adaptation_20250925|TensLoRA: Tensor Alternatives for Low-Rank Adaptation]] (85.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Continual Learning|Continual Learning]], [[keywords/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Low-Rank Adaptation|Low-Rank Adaptation]], [[keywords/Dynamic Memory|Dynamic Memory]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2411.00623v3 Announce Type: replace-cross 
Abstract: We revisit continual learning~(CL), which enables pre-trained vision transformers (ViTs) to sequentially fine-tune on new downstream tasks over time. However, as the scale of these models increases, catastrophic forgetting remains a more serious challenge. Recent studies highlight a crossover between CL techniques and parameter-efficient fine-tuning (PEFT), which focuses on fine-tuning only a small set of trainable parameters to adapt to downstream tasks, such as low-rank adaptation (LoRA). While LoRA achieves faster convergence and requires fewer trainable parameters, it has seldom been explored in the context of continual learning. To address this gap, we propose a novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which introduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel to pre-trained weights in each layer. These components are orchestrated by a dynamic memory mechanism to strike a balance between stability and plasticity. Additionally, we propose a scheme to predict task identity with confidence and calibrate the model's outputs accordingly. On ViT-based models, we demonstrate that DualLoRA offers significant advantages in accuracy, inference speed, and computation efficiency in training over existing CL methods across multiple benchmarks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ì „ í•™ìŠµëœ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViTs)ì˜ ì—°ì† í•™ìŠµ(CL) ë¬¸ì œë¥¼ ë‹¤ë£¨ë©°, íŠ¹íˆ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ê¸°ë²•ê³¼ì˜ êµì°¨ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ìƒˆë¡œìš´ PEFT-CL ë°©ë²•ì¸ Dual Low-Rank Adaptation(DualLoRA)ì„ ì œì•ˆí•˜ì—¬, ê° ì¸µì— ì§êµ LoRA ì–´ëŒ‘í„°ì™€ ì”ì—¬ LoRA ì–´ëŒ‘í„°ë¥¼ ë³‘ë ¬ë¡œ ë„ì…í•©ë‹ˆë‹¤. ì´ëŠ” ë™ì  ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì•ˆì •ì„±ê³¼ ê°€ë³€ì„±ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤. ë˜í•œ, ì‘ì—… ì‹ë³„ì„ ì˜ˆì¸¡í•˜ê³  ëª¨ë¸ ì¶œë ¥ì„ ë³´ì •í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DualLoRAëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ CL ë°©ë²•ë“¤ë³´ë‹¤ ì •í™•ì„±, ì¶”ë¡  ì†ë„, ê³„ì‚° íš¨ìœ¨ì„±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViTs)ì˜ ì—°ì† í•™ìŠµ(CL)ì—ì„œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ê¸°ë²•ê³¼ì˜ êµì°¨ê°€ ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 2. DualLoRAëŠ” ì •ê·œ LoRA ì–´ëŒ‘í„°ì™€ ì”ì—¬ LoRA ì–´ëŒ‘í„°ë¥¼ ë„ì…í•˜ì—¬ ê° ì¸µì˜ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ì™€ ë³‘ë ¬ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
- 3. ë™ì  ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì•ˆì •ì„±ê³¼ ê°€ì†Œì„± ê°„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” DualLoRA ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. DualLoRAëŠ” ì‘ì—… ì •ì²´ì„±ì„ ì˜ˆì¸¡í•˜ê³  ëª¨ë¸ì˜ ì¶œë ¥ì„ ë³´ì •í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. ViT ê¸°ë°˜ ëª¨ë¸ì—ì„œ DualLoRAëŠ” ê¸°ì¡´ CL ë°©ë²•ì— ë¹„í•´ ì •í™•ë„, ì¶”ë¡  ì†ë„, í›ˆë ¨ ì‹œ ê³„ì‚° íš¨ìœ¨ì„±ì—ì„œ í° ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:36:42*