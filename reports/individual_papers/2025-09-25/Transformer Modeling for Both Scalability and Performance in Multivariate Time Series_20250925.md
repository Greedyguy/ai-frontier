---
keywords:
  - Transformer
  - Delegate Token Attention
  - Multivariate Time Series
  - Attention Mechanism
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19471
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:36:12.169480",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Delegate Token Attention",
    "Multivariate Time Series",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Delegate Token Attention": 0.8,
    "Multivariate Time Series": 0.78,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in deep learning, crucial for linking various advancements in the field.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Delegate Token Attention",
        "canonical": "Delegate Token Attention",
        "aliases": [
          "DELTAformer"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel method proposed in the paper, offering a unique approach to improving transformer scalability and performance.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multivariate Time Series",
        "canonical": "Multivariate Time Series",
        "aliases": [
          "MTS"
        ],
        "category": "specific_connectable",
        "rationale": "A key application area for the proposed method, connecting to other works in time series analysis.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the functioning of transformers and their variants, facilitating connections to a wide range of related research.",
        "novelty_score": 0.35,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "scalability",
      "performance",
      "noise-resilience"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Delegate Token Attention",
      "resolved_canonical": "Delegate Token Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multivariate Time Series",
      "resolved_canonical": "Multivariate Time Series",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.35,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Transformer Modeling for Both Scalability and Performance in Multivariate Time Series

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19471.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19471](https://arxiv.org/abs/2509.19471)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MTM_ A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification_20250923|MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification]] (84.1% similar)
- [[2025-09-23/TSGym_ Design Choices for Deep Multivariate Time-Series Forecasting_20250923|TSGym: Design Choices for Deep Multivariate Time-Series Forecasting]] (83.0% similar)
- [[2025-09-23/Conv-like Scale-Fusion Time Series Transformer_ A Multi-Scale Representation for Variable-Length Long Time Series_20250923|Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series]] (83.0% similar)
- [[2025-09-25/Mamba Modulation_ On the Length Generalization of Mamba_20250925|Mamba Modulation: On the Length Generalization of Mamba]] (82.9% similar)
- [[2025-09-23/Measure-to-measure interpolation using Transformers_20250923|Measure-to-measure interpolation using Transformers]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multivariate Time Series|Multivariate Time Series]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Delegate Token Attention|Delegate Token Attention]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19471v1 Announce Type: new 
Abstract: Variable count is among the main scalability bottlenecks for transformer modeling in multivariate time series (MTS) data. On top of this, a growing consensus in the field points to indiscriminate inter-variable mixing as a potential source of noise-accumulation and performance degradation. This is likely exacerbated by sparsity of informative signals characteristic of many MTS systems coupled with representational misalignment stemming from indiscriminate information mixing between (heterogeneous) variables. While scalability and performance are often seen as competing interests in transformer design, we show that both can be improved simultaneously in MTS by strategically constraining the representational capacity of inter-variable mixing. Our proposed method, transformer with Delegate Token Attention (DELTAformer), constrains inter-variable modeling through what we call delegate tokens which are then used to perform full, unconstrained, inter-temporal modeling. Delegate tokens act as an implicit regularizer that forces the model to be highly selective about what inter-variable information is allowed to propagate through the network. Our results show that DELTAformer scales linearly with variable-count while actually outperforming standard transformers, achieving state-of-the-art performance across benchmarks and baselines. In addition, DELTAformer can focus on relevant signals better than standard transformers in noisy MTS environments and overall exhibit superior noise-resilience. Overall, results across various experiments confirm that by aligning our model design to leverage domain-specific challenges in MTS to our advantage, DELTAformer can simultaneously achieve linear scaling while actually improving its performance against standard, quadratic transformers.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´(MTS) ë°ì´í„°ì—ì„œ ë³€ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DELTAformerë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. DELTAformerëŠ” ëŒ€í‘œ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ ê°„ ì •ë³´ í˜¼í•©ì„ ì „ëµì ìœ¼ë¡œ ì œí•œí•¨ìœ¼ë¡œì¨, ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ í™•ì¥ì„±ì„ ë™ì‹œì— ê°œì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë³€ìˆ˜ ìˆ˜ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ í™•ì¥ë˜ë©°, ê¸°ì¡´ì˜ í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. íŠ¹íˆ, ì¡ìŒì´ ë§ì€ í™˜ê²½ì—ì„œë„ ê´€ë ¨ ì‹ í˜¸ë¥¼ ë” ì˜ í¬ì°©í•˜ì—¬ ë›°ì–´ë‚œ ì¡ìŒ ì €í•­ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‹¤í—˜ ê²°ê³¼, DELTAformerëŠ” MTSì˜ ë„ë©”ì¸ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³€ìˆ˜ ìˆ˜ëŠ” ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´(MTS) ë°ì´í„°ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë§ì˜ ì£¼ìš” í™•ì¥ì„± ë³‘ëª© í˜„ìƒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
- 2. ë¬´ì°¨ë³„ì ì¸ ë³€ìˆ˜ ê°„ í˜¼í•©ì€ ë…¸ì´ì¦ˆ ì¶•ì ê³¼ ì„±ëŠ¥ ì €í•˜ì˜ ì ì¬ì  ì›ì¸ìœ¼ë¡œ ì§€ëª©ë©ë‹ˆë‹¤.
- 3. DELTAformerëŠ” ëŒ€í‘œ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ ê°„ ëª¨ë¸ë§ì„ ì œì•½í•¨ìœ¼ë¡œì¨ í™•ì¥ì„±ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— ê°œì„ í•©ë‹ˆë‹¤.
- 4. DELTAformerëŠ” ë³€ìˆ˜ ìˆ˜ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ í™•ì¥ë˜ë©°, í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. DELTAformerëŠ” ë…¸ì´ì¦ˆê°€ ë§ì€ MTS í™˜ê²½ì—ì„œë„ ê´€ë ¨ ì‹ í˜¸ì— ë” ì§‘ì¤‘í•  ìˆ˜ ìˆì–´ ìš°ìˆ˜í•œ ë…¸ì´ì¦ˆ ì €í•­ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:36:12*