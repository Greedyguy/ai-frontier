---
keywords:
  - EmbeddingGemma
  - Text Embedding Model
  - Geometric Embedding Distillation
  - Massive Text Embedding Benchmark
  - Low-Latency Applications
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20354
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:08:32.144688",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "EmbeddingGemma",
    "Text Embedding Model",
    "Geometric Embedding Distillation",
    "Massive Text Embedding Benchmark",
    "Low-Latency Applications"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "EmbeddingGemma": 0.8,
    "Text Embedding Model": 0.7,
    "Geometric Embedding Distillation": 0.75,
    "Massive Text Embedding Benchmark": 0.7,
    "Low-Latency Applications": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "EmbeddingGemma",
        "canonical": "EmbeddingGemma",
        "aliases": [
          "Gemma 3"
        ],
        "category": "unique_technical",
        "rationale": "EmbeddingGemma is a novel model introduced in the paper, representing a unique contribution to text embedding techniques.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "text embedding model",
        "canonical": "Text Embedding Model",
        "aliases": [
          "text embeddings"
        ],
        "category": "broad_technical",
        "rationale": "Text embedding models are a fundamental concept in NLP, facilitating connections with various language processing tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      },
      {
        "surface": "geometric embedding distillation",
        "canonical": "Geometric Embedding Distillation",
        "aliases": [
          "embedding distillation"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a specific innovation in the paper, enhancing model performance and efficiency.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Massive Text Embedding Benchmark",
        "canonical": "Massive Text Embedding Benchmark",
        "aliases": [
          "MTEB"
        ],
        "category": "specific_connectable",
        "rationale": "MTEB is a key evaluation metric used in the paper, relevant for benchmarking text embedding models.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "low-latency applications",
        "canonical": "Low-Latency Applications",
        "aliases": [
          "real-time applications"
        ],
        "category": "specific_connectable",
        "rationale": "Low-latency applications are crucial for deploying models in real-time scenarios, linking to performance optimization discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "model robustness",
      "performance-to-cost ratio"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "EmbeddingGemma",
      "resolved_canonical": "EmbeddingGemma",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "text embedding model",
      "resolved_canonical": "Text Embedding Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "geometric embedding distillation",
      "resolved_canonical": "Geometric Embedding Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Massive Text Embedding Benchmark",
      "resolved_canonical": "Massive Text Embedding Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "low-latency applications",
      "resolved_canonical": "Low-Latency Applications",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# EmbeddingGemma: Powerful and Lightweight Text Representations

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20354.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20354](https://arxiv.org/abs/2509.20354)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MetaEmbed_ Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction_20250923|MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction]] (82.4% similar)
- [[2025-09-24/EMMA_ End-to-End Multimodal Model for Autonomous Driving_20250924|EMMA: End-to-End Multimodal Model for Autonomous Driving]] (82.2% similar)
- [[2025-09-24/Hyper-Bagel_ A Unified Acceleration Framework for Multimodal Understanding and Generation_20250924|Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation]] (82.0% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (81.5% similar)
- [[2025-09-23/EAMET_ Robust Massive Model Editing via Embedding Alignment Optimization_20250923|EAMET: Robust Massive Model Editing via Embedding Alignment Optimization]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Text Embedding Model|Text Embedding Model]]
**ğŸ”— Specific Connectable**: [[keywords/Massive Text Embedding Benchmark|Massive Text Embedding Benchmark]], [[keywords/Low-Latency Applications|Low-Latency Applications]]
**âš¡ Unique Technical**: [[keywords/EmbeddingGemma|EmbeddingGemma]], [[keywords/Geometric Embedding Distillation|Geometric Embedding Distillation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20354v1 Announce Type: cross 
Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research.

## ğŸ“ ìš”ì•½

EmbeddingGemmaëŠ” Gemma 3 ì–¸ì–´ ëª¨ë¸ ê³„ì—´ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²½ëŸ‰ì˜ ì˜¤í”ˆ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œ, ëŒ€í˜• ëª¨ë¸ì˜ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ìº¡ì²˜í•˜ëŠ” í˜ì‹ ì ì¸ í•™ìŠµ ë°©ì‹ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì¸ì½”ë”-ë””ì½”ë” ì´ˆê¸°í™”ì™€ ê¸°í•˜í•™ì  ì„ë² ë”© ì¦ë¥˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤. Massive Text Embedding Benchmark(MTEB)ì—ì„œ ë‹¤êµ­ì–´, ì˜ì–´, ì½”ë“œ ë„ë©”ì¸ì— ëŒ€í•´ í‰ê°€í•œ ê²°ê³¼, EmbeddingGemma(300M)ëŠ” 500M ë¯¸ë§Œì˜ íŒŒë¼ë¯¸í„°ë¡œ ê¸°ì¡´ì˜ ìƒìœ„ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ë©°, ë‘ ë°° í¬ê¸°ì˜ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•˜ê±°ë‚˜ ì„ë² ë”© ì¶œë ¥ì„ ì˜ë¼ë‚¼ ë•Œë„ ì„±ëŠ¥ ìš°ìœ„ë¥¼ ìœ ì§€í•˜ì—¬, ì €ì§€ì—° ë° ê³ ì²˜ë¦¬ëŸ‰ì´ ìš”êµ¬ë˜ëŠ” ì˜¨ë””ë°”ì´ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•©ë‹ˆë‹¤. ì£¼ìš” ì„¤ê³„ ì„ íƒì„ íƒêµ¬í•˜ëŠ” ì†Œê±° ì—°êµ¬ë¥¼ í¬í•¨í•˜ì—¬, EmbeddingGemmaë¥¼ ì»¤ë®¤ë‹ˆí‹°ì— ê³µê°œí•˜ì—¬ ì¶”ê°€ ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê³ ì í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. EmbeddingGemmaëŠ” Gemma 3 ì–¸ì–´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²½ëŸ‰ì˜ ì˜¤í”ˆ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œ, ëŒ€í˜• ëª¨ë¸ì˜ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ìº¡ì²˜í•©ë‹ˆë‹¤.
- 2. ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ spread-out regularizerë¥¼ ì‚¬ìš©í•˜ê³ , ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ì¼ë°˜í™”ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
- 3. Massive Text Embedding Benchmark (MTEB)ì—ì„œ ë‹¤êµ­ì–´, ì˜ì–´, ì½”ë“œ ë„ë©”ì¸ì— ëŒ€í•´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©°, 500M ë¯¸ë§Œì˜ íŒŒë¼ë¯¸í„°ë¡œ ì´ì „ì˜ ìµœê³  ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 4. ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•˜ê±°ë‚˜ ì„ë² ë”© ì¶œë ¥ì„ ì˜ë¼ë‚´ë„ ì„±ëŠ¥ì´ ìœ ì§€ë˜ì–´, EmbeddingGemmaëŠ” ì €ì§€ì—° ë° ê³ ì²˜ë¦¬ëŸ‰ì„ ìš”êµ¬í•˜ëŠ” ì˜¨ë””ë°”ì´ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•©ë‹ˆë‹¤.
- 5. ì£¼ìš” ì„¤ê³„ ì„ íƒì„ íƒêµ¬í•˜ëŠ” ì†Œê±° ì—°êµ¬ë¥¼ ì œê³µí•˜ë©°, ì»¤ë®¤ë‹ˆí‹°ì— EmbeddingGemmaë¥¼ ê³µê°œí•˜ì—¬ ì¶”ê°€ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:08:32*