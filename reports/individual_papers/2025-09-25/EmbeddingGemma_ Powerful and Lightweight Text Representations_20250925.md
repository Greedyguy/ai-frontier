---
keywords:
  - EmbeddingGemma
  - Text Embedding Model
  - Geometric Embedding Distillation
  - Massive Text Embedding Benchmark
  - Low-Latency Applications
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20354
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:08:32.144688",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "EmbeddingGemma",
    "Text Embedding Model",
    "Geometric Embedding Distillation",
    "Massive Text Embedding Benchmark",
    "Low-Latency Applications"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "EmbeddingGemma": 0.8,
    "Text Embedding Model": 0.7,
    "Geometric Embedding Distillation": 0.75,
    "Massive Text Embedding Benchmark": 0.7,
    "Low-Latency Applications": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "EmbeddingGemma",
        "canonical": "EmbeddingGemma",
        "aliases": [
          "Gemma 3"
        ],
        "category": "unique_technical",
        "rationale": "EmbeddingGemma is a novel model introduced in the paper, representing a unique contribution to text embedding techniques.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "text embedding model",
        "canonical": "Text Embedding Model",
        "aliases": [
          "text embeddings"
        ],
        "category": "broad_technical",
        "rationale": "Text embedding models are a fundamental concept in NLP, facilitating connections with various language processing tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      },
      {
        "surface": "geometric embedding distillation",
        "canonical": "Geometric Embedding Distillation",
        "aliases": [
          "embedding distillation"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a specific innovation in the paper, enhancing model performance and efficiency.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Massive Text Embedding Benchmark",
        "canonical": "Massive Text Embedding Benchmark",
        "aliases": [
          "MTEB"
        ],
        "category": "specific_connectable",
        "rationale": "MTEB is a key evaluation metric used in the paper, relevant for benchmarking text embedding models.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "low-latency applications",
        "canonical": "Low-Latency Applications",
        "aliases": [
          "real-time applications"
        ],
        "category": "specific_connectable",
        "rationale": "Low-latency applications are crucial for deploying models in real-time scenarios, linking to performance optimization discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "model robustness",
      "performance-to-cost ratio"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "EmbeddingGemma",
      "resolved_canonical": "EmbeddingGemma",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "text embedding model",
      "resolved_canonical": "Text Embedding Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "geometric embedding distillation",
      "resolved_canonical": "Geometric Embedding Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Massive Text Embedding Benchmark",
      "resolved_canonical": "Massive Text Embedding Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "low-latency applications",
      "resolved_canonical": "Low-Latency Applications",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# EmbeddingGemma: Powerful and Lightweight Text Representations

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20354.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20354](https://arxiv.org/abs/2509.20354)

## 🔗 유사한 논문
- [[2025-09-23/MetaEmbed_ Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction_20250923|MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction]] (82.4% similar)
- [[2025-09-24/EMMA_ End-to-End Multimodal Model for Autonomous Driving_20250924|EMMA: End-to-End Multimodal Model for Autonomous Driving]] (82.2% similar)
- [[2025-09-24/Hyper-Bagel_ A Unified Acceleration Framework for Multimodal Understanding and Generation_20250924|Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation]] (82.0% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (81.5% similar)
- [[2025-09-23/EAMET_ Robust Massive Model Editing via Embedding Alignment Optimization_20250923|EAMET: Robust Massive Model Editing via Embedding Alignment Optimization]] (80.7% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Text Embedding Model|Text Embedding Model]]
**🔗 Specific Connectable**: [[keywords/Massive Text Embedding Benchmark|Massive Text Embedding Benchmark]], [[keywords/Low-Latency Applications|Low-Latency Applications]]
**⚡ Unique Technical**: [[keywords/EmbeddingGemma|EmbeddingGemma]], [[keywords/Geometric Embedding Distillation|Geometric Embedding Distillation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20354v1 Announce Type: cross 
Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research.

## 📝 요약

EmbeddingGemma는 Gemma 3 언어 모델 계열을 기반으로 한 경량의 오픈 텍스트 임베딩 모델로, 대형 모델의 지식을 효과적으로 캡처하는 혁신적인 학습 방식을 도입했습니다. 인코더-디코더 초기화와 기하학적 임베딩 증류를 통해 모델의 견고성과 표현력을 향상시키고, 다양한 체크포인트를 결합하여 일반화 능력을 확보했습니다. Massive Text Embedding Benchmark(MTEB)에서 다국어, 영어, 코드 도메인에 대해 평가한 결과, EmbeddingGemma(300M)는 500M 미만의 파라미터로 기존의 상위 모델들을 능가하며, 두 배 크기의 모델과 유사한 성능을 제공합니다. 특히, 모델 가중치를 양자화하거나 임베딩 출력을 잘라낼 때도 성능 우위를 유지하여, 저지연 및 고처리량이 요구되는 온디바이스 애플리케이션에 적합합니다. 주요 설계 선택을 탐구하는 소거 연구를 포함하여, EmbeddingGemma를 커뮤니티에 공개하여 추가 연구를 촉진하고자 합니다.

## 🎯 주요 포인트

- 1. EmbeddingGemma는 Gemma 3 언어 모델을 기반으로 한 경량의 오픈 텍스트 임베딩 모델로, 대형 모델의 지식을 효과적으로 캡처합니다.
- 2. 모델의 견고성과 표현력을 향상시키기 위해 spread-out regularizer를 사용하고, 다양한 체크포인트를 결합하여 일반화를 보장합니다.
- 3. Massive Text Embedding Benchmark (MTEB)에서 다국어, 영어, 코드 도메인에 대해 최첨단 성능을 기록하며, 500M 미만의 파라미터로 이전의 최고 모델들을 능가합니다.
- 4. 모델 가중치를 양자화하거나 임베딩 출력을 잘라내도 성능이 유지되어, EmbeddingGemma는 저지연 및 고처리량을 요구하는 온디바이스 애플리케이션에 적합합니다.
- 5. 주요 설계 선택을 탐구하는 소거 연구를 제공하며, 커뮤니티에 EmbeddingGemma를 공개하여 추가 연구를 촉진합니다.


---

*Generated on 2025-09-25 16:08:32*