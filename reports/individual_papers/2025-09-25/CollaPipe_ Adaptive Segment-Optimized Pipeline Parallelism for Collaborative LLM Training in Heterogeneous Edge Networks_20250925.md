---
keywords:
  - Large Language Model
  - Pipeline Parallelism
  - Federated Learning
  - Dynamic Resource Allocation
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19855
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:51:42.443876",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Pipeline Parallelism",
    "Federated Learning",
    "Dynamic Resource Allocation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Pipeline Parallelism": 0.78,
    "Federated Learning": 0.8,
    "Dynamic Resource Allocation": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Transformer LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on collaborative training in edge networks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "pipeline parallelism",
        "canonical": "Pipeline Parallelism",
        "aliases": [
          "pipeline training",
          "parallel training"
        ],
        "category": "specific_connectable",
        "rationale": "Pipeline parallelism is a key technique discussed for optimizing model training in distributed environments.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "federated aggregation",
        "canonical": "Federated Learning",
        "aliases": [
          "federated model aggregation"
        ],
        "category": "specific_connectable",
        "rationale": "Federated aggregation is crucial for the collaborative aspect of the training framework described.",
        "novelty_score": 0.7,
        "connectivity_score": 0.82,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dynamic Segment Scheduling and Resource Allocation",
        "canonical": "Dynamic Resource Allocation",
        "aliases": [
          "DSSDA algorithm"
        ],
        "category": "unique_technical",
        "rationale": "The DSSDA algorithm is a novel contribution of the paper, optimizing resource allocation for training.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "mobile edge computing",
      "self-evolving intelligent networks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "pipeline parallelism",
      "resolved_canonical": "Pipeline Parallelism",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "federated aggregation",
      "resolved_canonical": "Federated Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.82,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dynamic Segment Scheduling and Resource Allocation",
      "resolved_canonical": "Dynamic Resource Allocation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# CollaPipe: Adaptive Segment-Optimized Pipeline Parallelism for Collaborative LLM Training in Heterogeneous Edge Networks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19855.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19855](https://arxiv.org/abs/2509.19855)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/PipelineRL_ Faster On-policy Reinforcement Learning for Long Sequence Generatio_20250924|PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio]] (83.9% similar)
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (83.8% similar)
- [[2025-09-19/{\lambda}Scale_ Enabling Fast Scaling for Serverless Large Language Model Inference_20250919|{\lambda}Scale: Enabling Fast Scaling for Serverless Large Language Model Inference]] (83.7% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (83.2% similar)
- [[2025-09-23/MobiZO_ Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines_20250923|MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Pipeline Parallelism|Pipeline Parallelism]], [[keywords/Federated Learning|Federated Learning]]
**âš¡ Unique Technical**: [[keywords/Dynamic Resource Allocation|Dynamic Resource Allocation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19855v1 Announce Type: cross 
Abstract: The increasing demand for intelligent mobile applications has made multi-agent collaboration with Transformer-based large language models (LLMs) essential in mobile edge computing (MEC) networks. However, training LLMs in such environments remains challenging due to heavy computation, high end-to-end latency, and limited model generalization. We introduce CollaPipe, a hybrid distributed learning framework that integrates collaborative pipeline parallelism with federated aggregation to support self-evolving intelligent networks. In CollaPipe, the encoder part is adaptively partitioned into variable-sized segments and deployed across mobile devices for pipeline-parallel training, while the decoder is deployed on edge servers to handle generative tasks. Then we perform global model update via federated aggregation. To enhance training efficiency, we formulate a joint optimization problem that adaptively allocates model segments, micro-batches, bandwidth, and transmission power. We derive and use a closed-form convergence bound to design an Dynamic Segment Scheduling and Resource Allocation (DSSDA) algorithm based on Lyapunov optimization, ensuring system stability under long-term constraints. Extensive experiments on downstream tasks with Transformer and BERT models show that CollaPipe improves computation efficiency by up to 15.09%, reduces end-to-end latency by at least 48.98%, and cuts single device memory usage by more than half, enabling online learning in heterogeneous and dynamic communication environments.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë°”ì¼ ì—£ì§€ ì»´í“¨íŒ… ë„¤íŠ¸ì›Œí¬ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—…ì„ ì§€ì›í•˜ê¸° ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ CollaPipeë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CollaPipeëŠ” í˜‘ì—… íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬ì™€ ì—°í•© ì§‘ê³„ë¥¼ ê²°í•©í•˜ì—¬ ëª¨ë¸ì˜ ìê°€ ì§„í™”ë¥¼ ë•ìŠµë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ê°€ë³€ í¬ê¸°ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ë‰˜ì–´ ëª¨ë°”ì¼ ì¥ì¹˜ì— ë°°í¬ë˜ê³ , ë””ì½”ë”ëŠ” ì—£ì§€ ì„œë²„ì— ë°°í¬ë˜ì–´ ìƒì„± ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ ëª¨ë¸ ì„¸ê·¸ë¨¼íŠ¸, ë§ˆì´í¬ë¡œ ë°°ì¹˜, ëŒ€ì—­í­, ì „ì†¡ ì „ë ¥ì„ ìµœì í™”í•˜ëŠ” ë¬¸ì œë¥¼ ì„¤ì •í•˜ê³ , Lyapunov ìµœì í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë™ì  ì„¸ê·¸ë¨¼íŠ¸ ìŠ¤ì¼€ì¤„ë§ ë° ìì› í• ë‹¹ ì•Œê³ ë¦¬ì¦˜(DSSDA)ì„ ì„¤ê³„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CollaPipeëŠ” ê³„ì‚° íš¨ìœ¨ì„±ì„ ìµœëŒ€ 15.09% í–¥ìƒì‹œí‚¤ê³ , ì§€ì—° ì‹œê°„ì„ ìµœì†Œ 48.98% ì¤„ì´ë©°, ë‹¨ì¼ ì¥ì¹˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ ì´ìƒ ê°ì†Œì‹œì¼œ ì´ê¸°ì¢… ë° ë™ì  í†µì‹  í™˜ê²½ì—ì„œì˜ ì˜¨ë¼ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CollaPipeëŠ” í˜‘ì—… íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬ì™€ ì—°í•© ì§‘ê³„ë¥¼ í†µí•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì¸ì½”ë”ëŠ” ê°€ë³€ í¬ê¸°ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• ë˜ì–´ ëª¨ë°”ì¼ ì¥ì¹˜ì— ë°°í¬ë˜ê³ , ë””ì½”ë”ëŠ” ì—£ì§€ ì„œë²„ì— ë°°í¬ë˜ì–´ ìƒì„± ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- 3. ëª¨ë¸ ì„¸ê·¸ë¨¼íŠ¸, ë§ˆì´í¬ë¡œ ë°°ì¹˜, ëŒ€ì—­í­ ë° ì „ì†¡ ì „ë ¥ì„ ì ì‘ì ìœ¼ë¡œ í• ë‹¹í•˜ëŠ” ê³µë™ ìµœì í™” ë¬¸ì œë¥¼ ê³µì‹í™”í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. Lyapunov ìµœì í™”ì— ê¸°ë°˜í•œ ë™ì  ì„¸ê·¸ë¨¼íŠ¸ ìŠ¤ì¼€ì¤„ë§ ë° ìì› í• ë‹¹ ì•Œê³ ë¦¬ì¦˜(DSSDA)ì„ ì„¤ê³„í•˜ì—¬ ì¥ê¸° ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, CollaPipeëŠ” ê³„ì‚° íš¨ìœ¨ì„±ì„ ìµœëŒ€ 15.09% í–¥ìƒì‹œí‚¤ê³ , ì¢…ë‹¨ ê°„ ì§€ì—°ì„ ìµœì†Œ 48.98% ì¤„ì´ë©°, ë‹¨ì¼ ì¥ì¹˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ ì´ìƒ ê°ì†Œì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:51:42*