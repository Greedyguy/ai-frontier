---
keywords:
  - Retrieval Augmented Generation
  - Adaptive Context Compression
  - Large Language Model
  - Hierarchical Compressor
  - Context Selector
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2507.22931
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:32:11.040710",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval Augmented Generation",
    "Adaptive Context Compression",
    "Large Language Model",
    "Hierarchical Compressor",
    "Context Selector"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Retrieval Augmented Generation": 0.9,
    "Adaptive Context Compression": 0.85,
    "Large Language Model": 0.8,
    "Hierarchical Compressor": 0.78,
    "Context Selector": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "RAG",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "RAG is a trending concept that connects retrieval and generation in LLMs, crucial for understanding the paper's context.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "Adaptive Context Compression",
        "canonical": "Adaptive Context Compression",
        "aliases": [
          "ACC"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel method introduced in the paper, essential for linking to specific advancements in RAG efficiency.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus on enhancing model efficiency with external knowledge.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Hierarchical Compressor",
        "canonical": "Hierarchical Compressor",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This component is key to the proposed method, enabling multi-granular embeddings and efficient context management.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Context Selector",
        "canonical": "Context Selector",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The context selector is crucial for retaining essential information, enhancing the understanding of the paper's methodology.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "efficiency",
      "accuracy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "RAG",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Adaptive Context Compression",
      "resolved_canonical": "Adaptive Context Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Hierarchical Compressor",
      "resolved_canonical": "Hierarchical Compressor",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Context Selector",
      "resolved_canonical": "Context Selector",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Enhancing RAG Efficiency with Adaptive Context Compression

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.22931.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2507.22931](https://arxiv.org/abs/2507.22931)

## 🔗 유사한 논문
- [[2025-09-22/CORE-RAG_ Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning_20250922|CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning]] (90.2% similar)
- [[2025-09-24/RAG+_ Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning_20250924|RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning]] (89.2% similar)
- [[2025-09-19/Enhancing Retrieval Augmentation via Adversarial Collaboration_20250919|Enhancing Retrieval Augmentation via Adversarial Collaboration]] (88.4% similar)
- [[2025-09-24/SIRAG_ Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework_20250924|SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework]] (88.2% similar)
- [[2025-09-23/AttnComp_ Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation_20250923|AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation]] (87.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**⚡ Unique Technical**: [[keywords/Adaptive Context Compression|Adaptive Context Compression]], [[keywords/Hierarchical Compressor|Hierarchical Compressor]], [[keywords/Context Selector|Context Selector]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.22931v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.

## 📝 요약

이 논문은 외부 지식을 활용하여 대형 언어 모델(LLM)의 성능을 향상시키는 검색 증강 생성(RAG)의 효율성을 높이기 위한 새로운 방법론을 제안합니다. 기존의 고정 압축률 방식은 간단한 쿼리를 과도하게 압축하거나 복잡한 쿼리를 충분히 압축하지 못하는 문제를 가지고 있습니다. 이를 해결하기 위해, 입력 복잡도에 따라 압축률을 동적으로 조정하는 적응형 컨텍스트 압축(ACC-RAG) 프레임워크를 개발했습니다. ACC-RAG는 계층적 압축기와 컨텍스트 선택기를 결합하여 최소한의 충분한 정보를 유지하며, 인간의 스키밍 방식과 유사하게 작동합니다. Wikipedia와 5개의 QA 데이터셋에서 평가한 결과, ACC-RAG는 고정 압축률 방법보다 우수한 성능을 보였으며, 표준 RAG 대비 4배 이상 빠른 추론 속도를 유지하거나 개선된 정확도를 제공했습니다.

## 🎯 주요 포인트

- 1. RAG는 외부 지식을 활용하여 LLM의 성능을 향상시키지만, 긴 검색 컨텍스트로 인해 추론 비용이 증가합니다.
- 2. 기존의 고정 압축률 방법은 간단한 쿼리를 과도하게 압축하거나 복잡한 쿼리를 충분히 압축하지 못합니다.
- 3. ACC-RAG는 입력 복잡성에 따라 압축률을 동적으로 조정하여 효율성을 최적화하면서 정확성을 유지합니다.
- 4. ACC-RAG는 계층적 압축기와 컨텍스트 선택기를 결합하여 최소한의 충분한 정보를 유지합니다.
- 5. ACC-RAG는 Wikipedia와 5개의 QA 데이터셋에서 평가되었으며, 고정 압축률 방법보다 우수한 성능을 보이며, 표준 RAG 대비 4배 이상 빠른 추론 속도를 달성합니다.


---

*Generated on 2025-09-25 16:32:11*