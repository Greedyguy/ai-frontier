---
keywords:
  - 3D Queryable Scene Representation
  - Vision-Language Model
  - 3D Scene Graph
  - Robotic Task Planning
  - Multimodal Object Embedding
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.20077
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:19:06.446380",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "3D Queryable Scene Representation",
    "Vision-Language Model",
    "3D Scene Graph",
    "Robotic Task Planning",
    "Multimodal Object Embedding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "3D Queryable Scene Representation": 0.8,
    "Vision-Language Model": 0.9,
    "3D Scene Graph": 0.85,
    "Robotic Task Planning": 0.8,
    "Multimodal Object Embedding": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "3D Queryable Scene Representation",
        "canonical": "3D Queryable Scene Representation",
        "aliases": [
          "3D QSR"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel framework introduced in the paper, crucial for linking to specific methodologies in 3D scene understanding.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "These models are integral to the framework's ability to link multimodal object embeddings, enhancing semantic queryability.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.9
      },
      {
        "surface": "3D Scene Graphs",
        "canonical": "3D Scene Graph",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "3D Scene Graphs are essential for structured and scalable organization within the framework, facilitating semantic reasoning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Robotic Task Planning",
        "canonical": "Robotic Task Planning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This is a key application of the framework, linking high-level instructions to precise execution in robotics.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Object Embeddings",
        "canonical": "Multimodal Object Embedding",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "These embeddings are crucial for linking geometric, visual, and semantic information, enhancing the framework's queryability.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "scene understanding",
      "semantic reasoning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "3D Queryable Scene Representation",
      "resolved_canonical": "3D Queryable Scene Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "3D Scene Graphs",
      "resolved_canonical": "3D Scene Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Robotic Task Planning",
      "resolved_canonical": "Robotic Task Planning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Object Embeddings",
      "resolved_canonical": "Multimodal Object Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20077.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.20077](https://arxiv.org/abs/2509.20077)

## 🔗 유사한 논문
- [[2025-09-23/Text-Scene_ A Scene-to-Language Parsing Framework for 3D Scene Understanding_20250923|Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding]] (85.3% similar)
- [[2025-09-22/GP3_ A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation_20250922|GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation]] (85.2% similar)
- [[2025-09-23/No Need for Real 3D_ Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning_20250923|No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning]] (84.7% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (83.8% similar)
- [[2025-09-24/Query-Centric Diffusion Policy for Generalizable Robotic Assembly_20250924|Query-Centric Diffusion Policy for Generalizable Robotic Assembly]] (83.8% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/3D Scene Graph|3D Scene Graph]], [[keywords/Robotic Task Planning|Robotic Task Planning]], [[keywords/Multimodal Object Embedding|Multimodal Object Embedding]]
**⚡ Unique Technical**: [[keywords/3D Queryable Scene Representation|3D Queryable Scene Representation]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20077v1 Announce Type: cross 
Abstract: To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics. To address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution. We evaluate our approach through simulated robotic task planning scenarios in Unity, guided by abstract language instructions and using the indoor public dataset Replica. Furthermore, we apply it in a digital duplicate of a real wet lab environment to test QSR-supported robotic task planning for emergency response. The results demonstrate the framework's ability to facilitate scene understanding and integrate spatial and semantic reasoning, effectively translating high-level human instructions into precise robotic task planning in complex 3D environments.

## 📝 요약

이 논문은 로봇이 고차원적인 인간의 지시를 이해하고 복잡한 작업을 수행할 수 있도록 돕는 3D Queryable Scene Representation (3D QSR)이라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 3D 환경을 의미 있게 해석하고 상호작용하기 위해 세 가지 3D 표현을 통합합니다: 팬옵틱 재구성을 통한 3D 일관성 있는 새로운 뷰 렌더링 및 세분화, 3D 포인트 클라우드로부터의 정밀한 기하학, 그리고 3D 장면 그래프를 통한 구조적이고 확장 가능한 조직입니다. 이 시스템은 대형 비전-언어 모델과 통합되어 다중 모달 객체 임베딩을 연결하고, 객체 수준에서 기하학적, 시각적, 의미적 정보를 검색할 수 있도록 지원합니다. 이를 통해 로봇 작업 계획에 필요한 데이터를 제공합니다. Unity 시뮬레이션과 실제 실험실 환경에서의 테스트 결과, 이 프레임워크는 복잡한 3D 환경에서 고차원적인 인간 지시를 로봇 작업 계획으로 효과적으로 변환할 수 있음을 보여주었습니다.

## 🎯 주요 포인트

- 1. 3D Queryable Scene Representation (3D QSR) 프레임워크는 3D 환경에서의 포괄적인 장면 이해를 위해 개발되었습니다.
- 2. 이 프레임워크는 3D 일관성을 유지하는 새로운 뷰 렌더링 및 세분화, 3D 포인트 클라우드에서의 정밀한 기하학, 3D 장면 그래프를 통한 구조적, 확장 가능한 조직을 통합합니다.
- 3. 객체 중심 설계를 기반으로 하여 대형 비전-언어 모델과 통합되어 다중 모달 객체 임베딩을 연결하고, 기하학적, 시각적, 의미적 정보의 객체 수준 검색을 지원합니다.
- 4. Unity에서의 시뮬레이션 로봇 작업 계획 시나리오와 실제 실험실 환경의 디지털 복제본에서의 응급 대응 로봇 작업 계획을 통해 평가되었습니다.
- 5. 이 연구는 복잡한 3D 환경에서 고수준의 인간 지시를 정밀한 로봇 작업 계획으로 효과적으로 변환할 수 있음을 보여줍니다.


---

*Generated on 2025-09-26 09:19:06*