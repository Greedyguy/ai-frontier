---
keywords:
  - Large Language Model
  - Chain-of-Thought Prompting
  - Self-Correction Methodologies
  - Feedback Mechanism
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2504.21625
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:53:50.778096",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Chain-of-Thought Prompting",
    "Self-Correction Methodologies",
    "Feedback Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Chain-of-Thought Prompting": 0.78,
    "Self-Correction Methodologies": 0.8,
    "Feedback Mechanism": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on instruction-following capabilities and feedback mechanisms.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Chain-of-Thought prompting",
        "canonical": "Chain-of-Thought Prompting",
        "aliases": [
          "CoT prompting"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights a specific technique relevant to the iterative self-correction process discussed.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "self-correction methodologies",
        "canonical": "Self-Correction Methodologies",
        "aliases": [
          "self-correction methods"
        ],
        "category": "unique_technical",
        "rationale": "Describes a unique approach central to the benchmark's functionality.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "feedback mechanism",
        "canonical": "Feedback Mechanism",
        "aliases": [
          "feedback systems"
        ],
        "category": "unique_technical",
        "rationale": "Integral to the iterative improvement process of LLMs as described in the paper.",
        "novelty_score": 0.6,
        "connectivity_score": 0.68,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "instruction",
      "benchmark",
      "performance",
      "analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Chain-of-Thought prompting",
      "resolved_canonical": "Chain-of-Thought Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "self-correction methodologies",
      "resolved_canonical": "Self-Correction Methodologies",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "feedback mechanism",
      "resolved_canonical": "Feedback Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.68,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark evaluating LLMs' Instruction Following Capability

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2504.21625.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2504.21625](https://arxiv.org/abs/2504.21625)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Language Models Fail to Introspect About Their Knowledge of Language_20250925|Language Models Fail to Introspect About Their Knowledge of Language]] (87.1% similar)
- [[2025-09-23/seqBench_ A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs_20250923|seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs]] (87.1% similar)
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (86.6% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (86.6% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (86.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Chain-of-Thought Prompting|Chain-of-Thought Prompting]]
**âš¡ Unique Technical**: [[keywords/Self-Correction Methodologies|Self-Correction Methodologies]], [[keywords/Feedback Mechanism|Feedback Mechanism]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.21625v5 Announce Type: replace 
Abstract: The capability to precisely adhere to instructions is a cornerstone for Large Language Models (LLMs) to function as dependable agents in real-world scenarios. However, confronted with complex prompts, LLMs frequently encounter difficulties in fulfilling all specified requirements within a single response. Drawing inspiration from recent advancements in Chain-of-Thought (CoT) prompting and self-correction methodologies, we introduce Meeseeks (The name is inspired by Mr. Meeseeks from "Rick and Morty," a character renowned for efficiently accomplishing assigned tasks. See: https://en.wikipedia.org/wiki/Mr._Meeseeks), a fully automated iterative instruction-following benchmark equipped with an integrated feedback mechanism. Meeseeks identifies erroneous components in model responses and provides corresponding feedback accurately, thereby iteratively guiding the model toward self-correction. The dataset contains over 700 curated instances annotated by 32 distinct capability tags in Chinese and English. Extensive experimental results reveal that different state-of-the-art commercial and open-source LLMs exhibit vastly disparate performance, and even after 20 turns of iterative feedback-driven self-correction, nearly all models demonstrate suboptimal performance. We conducted comprehensive analysis from both macro and instance levels, uncovering numerous common issues prevalent in current state-of-the-art models, as well as several counterintuitive phenomena. We've open-sourced our work on https://github.com/ADoublLEN/Meeseeks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë³µì¡í•œ ì§€ì‹œë¥¼ ì •í™•íˆ ë”°ë¥´ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ Meeseeksë¼ëŠ” ìë™í™”ëœ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. MeeseeksëŠ” ëª¨ë¸ì˜ ì‘ë‹µì—ì„œ ì˜¤ë¥˜ë¥¼ ì‹ë³„í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. 700ê°œ ì´ìƒì˜ ì‚¬ë¡€ê°€ í¬í•¨ëœ ë°ì´í„°ì…‹ì„ í†µí•´ ë‹¤ì–‘í•œ ìƒìš© ë° ì˜¤í”ˆ ì†ŒìŠ¤ LLMì˜ ì„±ëŠ¥ì„ í‰ê°€í•œ ê²°ê³¼, ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ 20ë²ˆì˜ í”¼ë“œë°± í›„ì—ë„ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” í˜„ì¬ ìµœì²¨ë‹¨ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ ëª‡ ê°€ì§€ ì˜ˆê¸°ì¹˜ ì•Šì€ í˜„ìƒì„ ë°í˜€ëƒˆìœ¼ë©°, ì—°êµ¬ ê²°ê³¼ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MeeseeksëŠ” ë³µì¡í•œ ì§€ì‹œ ì‚¬í•­ì„ ì¶©ì‹¤íˆ ë”°ë¥´ê¸° ìœ„í•´ ì„¤ê³„ëœ ìë™í™”ëœ ë°˜ë³µì  ì§€ì‹œ ìˆ˜í–‰ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- 2. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ëª¨ë¸ ì‘ë‹µì˜ ì˜¤ë¥˜ë¥¼ ì‹ë³„í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ìì²´ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
- 3. ë°ì´í„°ì…‹ì€ ì¤‘êµ­ì–´ì™€ ì˜ì–´ë¡œ ëœ 700ê°œ ì´ìƒì˜ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, 32ê°œì˜ ëŠ¥ë ¥ íƒœê·¸ë¡œ ì£¼ì„ì´ ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ìƒì—… ë° ì˜¤í”ˆ ì†ŒìŠ¤ LLMì˜ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¤ë¥´ë©°, 20ë²ˆì˜ ë°˜ë³µì ì¸ í”¼ë“œë°± í›„ì—ë„ ê±°ì˜ ëª¨ë“  ëª¨ë¸ì´ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
- 5. ì—°êµ¬ëŠ” ë§¤í¬ë¡œ ë° ê°œë³„ ì‚¬ë¡€ ìˆ˜ì¤€ì—ì„œì˜ ë¶„ì„ì„ í†µí•´ í˜„ì¬ ìµœì²¨ë‹¨ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ ëª‡ ê°€ì§€ ì§ê´€ì— ë°˜í•˜ëŠ” í˜„ìƒì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:53:50*