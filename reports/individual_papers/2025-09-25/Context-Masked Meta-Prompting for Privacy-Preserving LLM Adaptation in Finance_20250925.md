---
keywords:
  - Large Language Model
  - Privacy-Preserving
  - Meta-Prompting
  - Regeneration Process
  - Financial Applications
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2407.18920
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:50:40.975839",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Privacy-Preserving",
    "Meta-Prompting",
    "Regeneration Process",
    "Financial Applications"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Privacy-Preserving": 0.78,
    "Meta-Prompting": 0.82,
    "Regeneration Process": 0.77,
    "Financial Applications": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Essential for linking discussions on AI models in finance.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Privacy-Preserving",
        "canonical": "Privacy-Preserving",
        "aliases": [
          "Data Privacy"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for compliance in sensitive domains like finance.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Meta-Prompting",
        "canonical": "Meta-Prompting",
        "aliases": [
          "Prompt Optimization"
        ],
        "category": "unique_technical",
        "rationale": "Novel approach for enhancing prompt efficacy in LLMs.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Regeneration Process",
        "canonical": "Regeneration Process",
        "aliases": [
          "Prompt Regeneration"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific method for improving prompt outcomes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Financial Applications",
        "canonical": "Financial Applications",
        "aliases": [
          "Finance Use Cases"
        ],
        "category": "specific_connectable",
        "rationale": "Connects LLM adaptation to practical finance scenarios.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "methodology",
      "improvements",
      "strategy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Privacy-Preserving",
      "resolved_canonical": "Privacy-Preserving",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Meta-Prompting",
      "resolved_canonical": "Meta-Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Regeneration Process",
      "resolved_canonical": "Regeneration Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Financial Applications",
      "resolved_canonical": "Financial Applications",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in Finance

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2407.18920.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2407.18920](https://arxiv.org/abs/2407.18920)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Benchmarking and Improving LLM Robustness for Personalized Generation_20250925|Benchmarking and Improving LLM Robustness for Personalized Generation]] (86.2% similar)
- [[2025-09-23/Advanced Financial Reasoning at Scale_ A Comprehensive Evaluation of Large Language Models on CFA Level III_20250923|Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III]] (85.6% similar)
- [[2025-09-23/Privacy-Aware In-Context Learning for Large Language Models_20250923|Privacy-Aware In-Context Learning for Large Language Models]] (85.1% similar)
- [[2025-09-24/LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection_20250924|LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection]] (84.8% similar)
- [[2025-09-23/QA-prompting_ Improving Summarization with Large Language Models using Question-Answering_20250923|QA-prompting: Improving Summarization with Large Language Models using Question-Answering]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Financial Applications|Financial Applications]]
**âš¡ Unique Technical**: [[keywords/Privacy-Preserving|Privacy-Preserving]], [[keywords/Meta-Prompting|Meta-Prompting]], [[keywords/Regeneration Process|Regeneration Process]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2407.18920v2 Announce Type: replace 
Abstract: The increasing reliance on Large Language Models (LLMs) in sensitive domains like finance necessitates robust methods for privacy preservation and regulatory compliance. This paper presents an iterative meta-prompting methodology designed to optimise hard prompts without exposing proprietary or confidential context to the LLM. Through a novel regeneration process involving feeder and propagation methods, we demonstrate significant improvements in prompt efficacy. Evaluated on public datasets serving as proxies for financial tasks such as SQuAD for extractive financial Q&amp;A, CNN/DailyMail for news summarisation, and SAMSum for client interaction summarisation, our approach, utilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for question answering. This work highlights a practical, low-cost strategy for adapting LLMs to financial applications while upholding critical privacy and auditability standards, offering a compelling case for its relevance in the evolving landscape of generative AI in finance.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸ˆìœµ ë¶„ì•¼ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ì™€ ê·œì œ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìµœì í™” ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì €ìëŠ” í”¼ë” ë° ì „íŒŒ ë°©ë²•ì„ í¬í•¨í•œ ìƒˆë¡œìš´ ì¬ìƒì„± ê³¼ì •ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ íš¨ê³¼ì„±ì„ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. SQuAD, CNN/DailyMail, SAMSum ë“±ì˜ ê³µê°œ ë°ì´í„°ì…‹ì„ í™œìš©í•œ í‰ê°€ì—ì„œ, ì œì•ˆëœ ë°©ë²•ì€ ì§ˆë¬¸ ì‘ë‹µì—ì„œ ROUGE-L F1 ì ìˆ˜ë¥¼ 103.87% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” ê¸ˆìœµ ì• í”Œë¦¬ì¼€ì´ì…˜ì— LLMì„ ì €ë¹„ìš©ìœ¼ë¡œ ì ì‘ì‹œí‚¤ë©´ì„œ í”„ë¼ì´ë²„ì‹œì™€ ê°ì‚¬ ê¸°ì¤€ì„ ìœ ì§€í•˜ëŠ” ì‹¤ìš©ì ì¸ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¯¼ê°í•œ ê¸ˆìœµ ë¶„ì•¼ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ìš© ì¦ê°€ì— ë”°ë¼ í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ì™€ ê·œì œ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ê°•ë ¥í•œ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì€ LLMì— ê¸°ë°€ ì •ë³´ë¥¼ ë…¸ì¶œí•˜ì§€ ì•Šê³  í•˜ë“œ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•œ ë°˜ë³µì ì¸ ë©”íƒ€ í”„ë¡¬í”„íŒ… ë°©ë²•ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 3. í”¼ë” ë° ì „íŒŒ ë°©ë²•ì„ í¬í•¨í•œ ìƒˆë¡œìš´ ì¬ìƒì„± ê³¼ì •ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ íš¨ìœ¨ì„±ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ë³¸ ì ‘ê·¼ë²•ì€ ê¸ˆìœµ ì‘ì—…ì˜ ëŒ€ë¦¬ë¡œ ì‚¬ìš©ë˜ëŠ” ê³µê°œ ë°ì´í„°ì…‹ì„ í‰ê°€í•˜ì—¬, ì§ˆë¬¸ ì‘ë‹µì—ì„œ ROUGE-L F1 ì ìˆ˜ê°€ 103.87% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ê¸ˆìœµ ì• í”Œë¦¬ì¼€ì´ì…˜ì— LLMì„ ì ì‘ì‹œí‚¤ëŠ” ì‹¤ìš©ì ì´ê³  ì €ë¹„ìš©ì˜ ì „ëµì„ ê°•ì¡°í•˜ë©°, í”„ë¼ì´ë²„ì‹œì™€ ê°ì‚¬ ê°€ëŠ¥ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:50:40*