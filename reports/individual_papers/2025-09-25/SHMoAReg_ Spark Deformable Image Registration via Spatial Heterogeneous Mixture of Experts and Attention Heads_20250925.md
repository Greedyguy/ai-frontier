---
keywords:
  - Deformable Image Registration
  - Mixture of Experts
  - Attention Mechanism
  - Spatial Heterogeneous Mixture of Experts
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.20073
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:12:23.273250",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deformable Image Registration",
    "Mixture of Experts",
    "Attention Mechanism",
    "Spatial Heterogeneous Mixture of Experts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deformable Image Registration": 0.78,
    "Mixture of Experts": 0.82,
    "Attention Mechanism": 0.79,
    "Spatial Heterogeneous Mixture of Experts": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deformable Image Registration",
        "canonical": "Deformable Image Registration",
        "aliases": [
          "DIR"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's contribution and connects to specialized fields in medical imaging.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mixture of Experts",
        "canonical": "Mixture of Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "A key mechanism introduced in the paper, linking to advanced neural network architectures.",
        "novelty_score": 0.68,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Attention Heads",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Heads"
        ],
        "category": "specific_connectable",
        "rationale": "Integral to the encoder's function, enhancing feature extraction, and connects to known attention models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      },
      {
        "surface": "Spatial Heterogeneous Mixture of Experts",
        "canonical": "Spatial Heterogeneous Mixture of Experts",
        "aliases": [
          "SHMoE"
        ],
        "category": "unique_technical",
        "rationale": "A novel approach for predicting deformation fields, specific to the paper's methodology.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "Encoder-Decoder architectures",
      "Dice score",
      "abdominal CT dataset"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deformable Image Registration",
      "resolved_canonical": "Deformable Image Registration",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mixture of Experts",
      "resolved_canonical": "Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Attention Heads",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Spatial Heterogeneous Mixture of Experts",
      "resolved_canonical": "Spatial Heterogeneous Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20073.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.20073](https://arxiv.org/abs/2509.20073)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/TrueMoE_ Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection_20250922|TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection]] (84.6% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (83.6% similar)
- [[2025-09-23/GM-MoE_ Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts_20250923|GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts]] (83.6% similar)
- [[2025-09-22/Ideal Registration? Segmentation is All You Need_20250922|Ideal Registration? Segmentation is All You Need]] (83.4% similar)
- [[2025-09-23/CoBEVMoE_ Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception_20250923|CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception]] (82.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Mixture of Experts|Mixture of Experts]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Deformable Image Registration|Deformable Image Registration]], [[keywords/Spatial Heterogeneous Mixture of Experts|Spatial Heterogeneous Mixture of Experts]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20073v1 Announce Type: new 
Abstract: Encoder-Decoder architectures are widely used in deep learning-based Deformable Image Registration (DIR), where the encoder extracts multi-scale features and the decoder predicts deformation fields by recovering spatial locations. However, current methods lack specialized extraction of features (that are useful for registration) and predict deformation jointly and homogeneously in all three directions. In this paper, we propose a novel expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the specialization of feature extraction by dynamically selecting the optimal combination of attention heads for each image token. Meanwhile, the SHMoE predicts deformation fields heterogeneously in three directions for each voxel using experts with varying kernel sizes. Extensive experiments conducted on two publicly available datasets show consistent improvements over various methods, with a notable increase from 60.58% to 65.58% in Dice score for the abdominal CT dataset. Furthermore, SHMoAReg enhances model interpretability by differentiating experts' utilities across/within different resolution layers. To the best of our knowledge, we are the first to introduce MoE mechanism into DIR tasks. The code will be released soon.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ í•™ìŠµ ê¸°ë°˜ì˜ ë³€í˜• ì´ë¯¸ì§€ ì •í•©(Deformable Image Registration, DIR)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì „ë¬¸ê°€ ìœ ë„ DIR ë„¤íŠ¸ì›Œí¬ì¸ SHMoARegë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë„¤íŠ¸ì›Œí¬ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ì „ë¬¸ê°€ í˜¼í•©(Mixture of Experts, MoE) ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•˜ì—¬, ì¸ì½”ë”ì—ì„œëŠ” ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜(Mixture of Attention heads, MoA)ì„ í†µí•´ ì´ë¯¸ì§€ í† í°ì— ìµœì í™”ëœ ì£¼ì˜ í—¤ë“œë¥¼ ì„ íƒí•˜ê³ , ë””ì½”ë”ì—ì„œëŠ” ê³µê°„ì ìœ¼ë¡œ ì´ì§ˆì ì¸ ì „ë¬¸ê°€ í˜¼í•©(SHMoE)ì„ í†µí•´ ê° ë°©í–¥ìœ¼ë¡œ ì´ì§ˆì ì¸ ë³€í˜• í•„ë“œë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ë‘ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, ë³µë¶€ CT ë°ì´í„°ì…‹ì—ì„œ Dice ì ìˆ˜ê°€ 60.58%ì—ì„œ 65.58%ë¡œ í–¥ìƒë˜ëŠ” ë“± ë‹¤ì–‘í•œ ë°©ë²•ì— ë¹„í•´ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, SHMoARegëŠ” ë‹¤ì–‘í•œ í•´ìƒë„ ì¸µì—ì„œ ì „ë¬¸ê°€ì˜ ìœ ìš©ì„±ì„ êµ¬ë³„í•˜ì—¬ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤. MoE ë©”ì»¤ë‹ˆì¦˜ì„ DIR ì‘ì—…ì— ë„ì…í•œ ê²ƒì€ ì´ë²ˆì´ ì²˜ìŒì…ë‹ˆë‹¤. ì½”ë“œë„ ê³§ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SHMoARegëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ì „ë¬¸ê°€ í˜¼í•©(MoE) ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•œ ìƒˆë¡œìš´ DIR ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì¸ì½”ë” ë ˆì´ì–´ì— ì£¼ì˜ë ¥ í˜¼í•©(MoA)ì„ ë„ì…í•˜ì—¬ ì´ë¯¸ì§€ í† í°ì— ëŒ€í•œ ìµœì ì˜ ì£¼ì˜ë ¥ ì¡°í•©ì„ ë™ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
- 3. ë””ì½”ë” ë ˆì´ì–´ì— ê³µê°„ ì´ì§ˆì  ì „ë¬¸ê°€ í˜¼í•©(SHMoE)ì„ ì ìš©í•˜ì—¬ ê° ë°©í–¥ìœ¼ë¡œ ë³€í˜• í•„ë“œë¥¼ ì´ì§ˆì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
- 4. ë‘ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, SHMoARegëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë©°, ë³µë¶€ CT ë°ì´í„°ì…‹ì—ì„œ Dice ì ìˆ˜ê°€ 60.58%ì—ì„œ 65.58%ë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.
- 5. SHMoARegëŠ” ì „ë¬¸ê°€ì˜ ìœ ìš©ì„±ì„ êµ¬ë¶„í•˜ì—¬ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:12:23*