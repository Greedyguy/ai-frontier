---
keywords:
  - CLIP-ResNet
  - Neuron-Attention Decomposition
  - Attention Mechanism
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19943
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:54:26.900856",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "CLIP-ResNet",
    "Neuron-Attention Decomposition",
    "Attention Mechanism",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "CLIP-ResNet": 0.72,
    "Neuron-Attention Decomposition": 0.8,
    "Attention Mechanism": 0.78,
    "Vision-Language Model": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "CLIP-ResNet",
        "canonical": "CLIP-ResNet",
        "aliases": [
          "CLIP ResNet"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific architecture being analyzed, providing a unique technical focus for linking.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      },
      {
        "surface": "Neuron-Attention Decomposition",
        "canonical": "Neuron-Attention Decomposition",
        "aliases": [
          "Neuron Attention Decomposition"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel technique introduced in the paper, offering a unique perspective for linking.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention"
        ],
        "category": "specific_connectable",
        "rationale": "A core concept in the paper, facilitating connections to other works on attention mechanisms.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision Language Model"
        ],
        "category": "evolved_concepts",
        "rationale": "The paper's focus on image-text embedding aligns with this trending concept, enhancing connectivity.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "CLIP-ResNet",
      "resolved_canonical": "CLIP-ResNet",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Neuron-Attention Decomposition",
      "resolved_canonical": "Neuron-Attention Decomposition",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Interpreting ResNet-based CLIP via Neuron-Attention Decomposition

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19943.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19943](https://arxiv.org/abs/2509.19943)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/CLIP-IN_ Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions_20250923|CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions]] (83.5% similar)
- [[2025-09-23/MoCLIP-Lite_ Efficient Video Recognition by Fusing CLIP with Motion Vectors_20250923|MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors]] (81.6% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (81.3% similar)
- [[2025-09-23/Pulling Back the Curtain on ReLU Networks_20250923|Pulling Back the Curtain on ReLU Networks]] (81.1% similar)
- [[2025-09-23/Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models_20250923|Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/CLIP-ResNet|CLIP-ResNet]], [[keywords/Neuron-Attention Decomposition|Neuron-Attention Decomposition]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19943v1 Announce Type: cross 
Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by decomposing their contributions to the output into individual computation paths. More specifically, we analyze all pairwise combinations of neurons and the following attention heads of CLIP's attention-pooling layer. We find that these neuron-head pairs can be approximated by a single direction in CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret each neuron-head pair by associating it with text. Additionally, we find that only a sparse set of the neuron-head pairs have a significant contribution to the output value, and that some neuron-head pairs, while polysemantic, represent sub-concepts of their corresponding neurons. We use these observations for two applications. First, we employ the pairs for training-free semantic segmentation, outperforming previous methods for CLIP-ResNet. Second, we utilize the contributions of neuron-head pairs to monitor dataset distribution shifts. Our results demonstrate that examining individual computation paths in neural networks uncovers interpretable units, and that such units can be utilized for downstream tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ CLIP-ResNetì˜ ë‰´ëŸ°ì„ í•´ì„í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë‰´ëŸ°ê³¼ ì£¼ì˜ì§‘ì¤‘ ë ˆì´ì–´ì˜ í—¤ë“œ ìŒì„ ë¶„ì„í•˜ì—¬, ì´ë“¤ì´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì„ë² ë”© ê³µê°„ì—ì„œ ë‹¨ì¼ ë°©í–¥ìœ¼ë¡œ ê·¼ì‚¬ë  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° ë‰´ëŸ°-í—¤ë“œ ìŒì„ í…ìŠ¤íŠ¸ì™€ ì—°ê´€ì‹œì¼œ í•´ì„í•  ìˆ˜ ìˆìœ¼ë©°, ì†Œìˆ˜ì˜ ë‰´ëŸ°-í—¤ë“œ ìŒë§Œì´ ì¶œë ¥ì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•œë‹¤ëŠ” ê²ƒì„ ë°í˜”ìŠµë‹ˆë‹¤. ë˜í•œ, ì¼ë¶€ ìŒì€ ë‹¤ì˜ì ì´ì§€ë§Œ ë‰´ëŸ°ì˜ í•˜ìœ„ ê°œë…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ì°°ì„ í†µí•´, í•™ìŠµ ì—†ì´ë„ CLIP-ResNetì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì˜ë¯¸ì  ë¶„í• ê³¼ ë°ì´í„°ì…‹ ë¶„í¬ ë³€í™”ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë‘ ê°€ì§€ ì‘ìš©ì„ ì œì‹œí•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì‹ ê²½ë§ì˜ ê°œë³„ ê²½ë¡œë¥¼ ë¶„ì„í•˜ë©´ í•´ì„ ê°€ëŠ¥í•œ ë‹¨ìœ„ê°€ ë“œëŸ¬ë‚˜ë©°, ì´ë¥¼ í™œìš©í•œ ë‹¤ì–‘í•œ ì‘ì—…ì´ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CLIP-ResNetì˜ ë‰´ëŸ°ì„ í•´ì„í•˜ê¸° ìœ„í•´ ê°œë³„ ê³„ì‚° ê²½ë¡œë¡œ ê¸°ì—¬ë„ë¥¼ ë¶„í•´í•˜ëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.
- 2. ë‰´ëŸ°ê³¼ CLIPì˜ ì–´í…ì…˜ í’€ë§ ë ˆì´ì–´ì˜ ì–´í…ì…˜ í—¤ë“œì˜ ìŒì„ ë¶„ì„í•˜ì—¬, ì´ë“¤ì´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì„ë² ë”© ê³µê°„ì—ì„œ ë‹¨ì¼ ë°©í–¥ìœ¼ë¡œ ê·¼ì‚¬ë  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 3. ë‰´ëŸ°-í—¤ë“œ ìŒ ì¤‘ ì¼ë¶€ë§Œì´ ì¶œë ¥ ê°’ì— ìœ ì˜ë¯¸í•œ ê¸°ì—¬ë¥¼ í•˜ë©°, ì¼ë¶€ ë‹¤ì˜ì  ìŒì€ í•´ë‹¹ ë‰´ëŸ°ì˜ í•˜ìœ„ ê°œë…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- 4. ë‰´ëŸ°-í—¤ë“œ ìŒì„ í™œìš©í•˜ì—¬ í•™ìŠµ ì—†ì´ë„ CLIP-ResNetì˜ ì˜ë¯¸ ë¶„í•  ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 5. ë‰´ëŸ°-í—¤ë“œ ìŒì˜ ê¸°ì—¬ë„ë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë¶„í¬ ë³€í™” ëª¨ë‹ˆí„°ë§ì— í™œìš©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:54:26*