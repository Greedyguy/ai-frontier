---
keywords:
  - Low-Rank Adaptation
  - Transformer
  - Attention Mechanism
  - Tensor-based Adaptations
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19391
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:33:30.538940",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Low-Rank Adaptation",
    "Transformer",
    "Attention Mechanism",
    "Tensor-based Adaptations",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Low-Rank Adaptation": 0.8,
    "Transformer": 0.85,
    "Attention Mechanism": 0.82,
    "Tensor-based Adaptations": 0.75,
    "Vision-Language Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Low-Rank Adaptation",
        "canonical": "Low-Rank Adaptation",
        "aliases": [
          "LoRA"
        ],
        "category": "unique_technical",
        "rationale": "Low-Rank Adaptation is central to the paper's contribution and is a unique technical concept.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "The paper discusses adaptations for Transformers, a fundamental concept in the field.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Attention Projections",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Projections"
        ],
        "category": "specific_connectable",
        "rationale": "Attention Mechanism is a key component in the discussed adaptations, enhancing connectivity.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Tensor-based Adaptations",
        "canonical": "Tensor-based Adaptations",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The paper introduces a new framework for tensor-based adaptations, which is a novel contribution.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision and Language Benchmarks",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision and Language Benchmarks"
        ],
        "category": "evolved_concepts",
        "rationale": "The evaluation on vision and language benchmarks aligns with the concept of Vision-Language Models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Low-Rank Adaptation",
      "resolved_canonical": "Low-Rank Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Attention Projections",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Tensor-based Adaptations",
      "resolved_canonical": "Tensor-based Adaptations",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision and Language Benchmarks",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# TensLoRA: Tensor Alternatives for Low-Rank Adaptation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19391.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19391](https://arxiv.org/abs/2509.19391)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA_20250923|Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA]] (88.0% similar)
- [[2025-09-23/RefLoRA_ Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models_20250923|RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models]] (87.2% similar)
- [[2025-09-23/TASO_ Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation_20250923|TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation]] (86.5% similar)
- [[2025-09-23/Accurate and Efficient Low-Rank Model Merging in Core Space_20250923|Accurate and Efficient Low-Rank Model Merging in Core Space]] (85.5% similar)
- [[2025-09-24/TsqLoRA_ Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning_20250924|TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning]] (85.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Low-Rank Adaptation|Low-Rank Adaptation]], [[keywords/Tensor-based Adaptations|Tensor-based Adaptations]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19391v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” Transformer ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ì ì‘ì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì €ë­í¬ ì ì‘(LoRA)ì˜ í™•ì¥ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ LoRAëŠ” ê° ì£¼ì˜ í”„ë¡œì ì…˜ì— ë…ë¦½ì ìœ¼ë¡œ ì €ë­í¬ í–‰ë ¬ì„ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì´ì—ˆì§€ë§Œ, ì´ ì—°êµ¬ì—ì„œëŠ” ì´ë¥¼ ê³ ì°¨ì› í…ì„œë¡œ í†µí•©í•˜ëŠ” TensLoRAë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. TensLoRAëŠ” ê¸°ì¡´ì˜ í…ì„œ ê¸°ë°˜ ë°©ë²•ë“¤ì„ ì¼ë°˜í™”í•˜ë©°, ëª¨ë“œë³„ ì••ì¶•ë¥ ì„ ì¡°ì ˆí•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì™€ ì‘ì—…ì— ë§ì¶˜ íŒŒë¼ë¯¸í„° ì˜ˆì‚° ì„¤ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TensLoRAëŠ” ë¹„ìŠ·í•œ íŒŒë¼ë¯¸í„° ìˆ˜ì—ì„œë„ ê¸°ì¡´ LoRAë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²½ìš°ê°€ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Low-Rank Adaptation (LoRA)ëŠ” Transformerì˜ íš¨ìœ¨ì ì¸ ì ì‘ì„ ìœ„í•´ ì£¼ë¡œ ì‚¬ìš©ë˜ë©°, ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ê° íˆ¬ì˜ì— ë…ë¦½ì ì¸ ì €ë­í¬ í–‰ë ¬ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
- 2. TensLoRAëŠ” LoRA ì—…ë°ì´íŠ¸ë¥¼ ê³ ì°¨ì› í…ì„œë¡œ ì§‘ê³„í•˜ì—¬ ë‹¤ì–‘í•œ í…ì„œ ê¸°ë°˜ ì €ë­í¬ ì ì‘ì„ ëª¨ë¸ë§í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. TensLoRAëŠ” ê¸°ì¡´ì˜ í…ì„œ ê¸°ë°˜ ë°©ë²•ì„ ì¼ë°˜í™”í•˜ê³ , ëª¨ë“œë³„ ì••ì¶•ë¥ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ì™€ ì‘ì—…ì— ë”°ë¼ ë§¤ê°œë³€ìˆ˜ ì˜ˆì‚°ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, í…ì„œ êµ¬ì„±ì€ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ë•Œë¡œëŠ” ìœ ì‚¬í•œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì—ì„œ í‘œì¤€ LoRAë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:33:30*