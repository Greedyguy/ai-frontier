---
keywords:
  - Reinforcement Learning
  - User-Centric Agent
  - Simulated Users
  - Reward Shaping
  - Trajectory Scoring
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19736
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:16:37.058977",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "User-Centric Agent",
    "Simulated Users",
    "Reward Shaping",
    "Trajectory Scoring"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "User-Centric Agent": 0.78,
    "Simulated Users": 0.77,
    "Reward Shaping": 0.79,
    "Trajectory Scoring": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational concept in the paper, crucial for understanding the training of user-centric agents.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "User-Centric Agent",
        "canonical": "User-Centric Agent",
        "aliases": [
          "User-Centric Model"
        ],
        "category": "unique_technical",
        "rationale": "The focus on user-centric agents is a unique aspect of the study, emphasizing the interaction between agents and users.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Simulated Users",
        "canonical": "Simulated Users",
        "aliases": [
          "User Simulation"
        ],
        "category": "specific_connectable",
        "rationale": "Simulated users are critical for testing and evaluating the agent's performance in a controlled environment.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Reward Shaping",
        "canonical": "Reward Shaping",
        "aliases": [
          "Reward Design"
        ],
        "category": "specific_connectable",
        "rationale": "Reward shaping is a key technique for enhancing the learning process in reinforcement learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "Trajectory Scoring",
        "canonical": "Trajectory Scoring",
        "aliases": [
          "Trajectory Evaluation"
        ],
        "category": "unique_technical",
        "rationale": "Trajectory scoring is a novel concept introduced to improve multi-turn interactions in the study.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.76,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "dynamic interactions",
      "model scale"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "User-Centric Agent",
      "resolved_canonical": "User-Centric Agent",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Simulated Users",
      "resolved_canonical": "Simulated Users",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Reward Shaping",
      "resolved_canonical": "Reward Shaping",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Trajectory Scoring",
      "resolved_canonical": "Trajectory Scoring",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.76,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# UserRL: Training Interactive User-Centric Agent via Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19736.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19736](https://arxiv.org/abs/2509.19736)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/MobileRL_ Online Agentic Reinforcement Learning for Mobile GUI Agents_20250924|MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents]] (85.6% similar)
- [[2025-09-24/APRIL_ Active Partial Rollouts in Reinforcement Learning to tame long-tail generation_20250924|APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation]] (85.0% similar)
- [[2025-09-23/Generalizable End-to-End Tool-Use RL with Synthetic CodeGym_20250923|Generalizable End-to-End Tool-Use RL with Synthetic CodeGym]] (84.7% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (84.4% similar)
- [[2025-09-19/Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents_20250919|Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents]] (84.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Simulated Users|Simulated Users]], [[keywords/Reward Shaping|Reward Shaping]]
**âš¡ Unique Technical**: [[keywords/User-Centric Agent|User-Centric Agent]], [[keywords/Trajectory Scoring|Trajectory Scoring]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19736v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ìš©ì ì¤‘ì‹¬ì˜ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ UserRLì„ ì œì•ˆí•©ë‹ˆë‹¤. UserRLì€ í‘œì¤€í™”ëœ í™˜ê²½ê³¼ ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ìë¥¼ í†µí•´ ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš© ëŠ¥ë ¥ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” GRPO ì•Œê³ ë¦¬ì¦˜ í•˜ì—ì„œ ë‹¤ì–‘í•œ ë³´ìƒ í• ë‹¹ ë° ì ìˆ˜ ê³„ì‚° ë°©ì‹ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (i) ì´ˆê¸° ìƒí˜¸ì‘ìš© ëŠ¥ë ¥ì„ í™•ë³´í•˜ê³  ì§€ì†ì ì¸ ê°•í™” í•™ìŠµ ê°œì„ ì„ ìœ„í•´ SFT ì½œë“œ ìŠ¤íƒ€íŠ¸ê°€ ì¤‘ìš”í•˜ë©°, (ii) ì˜ë„ì ì¸ ê²½ë¡œ ì ìˆ˜ ê³„ì‚°ì´ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , (iii) ê°•ë ¥í•œ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©ì(GPT-4o)ê°€ í›ˆë ¨ì— ë„ì›€ì´ ë˜ì§€ë§Œ, ì˜¤í”ˆì†ŒìŠ¤ ì‹œë®¬ë ˆì´í„°(Qwen3-32B)ê°€ ë¹„ìš© íš¨ìœ¨ì ì´ê³  ì „ì´ ê°€ëŠ¥í•œ ì˜µì…˜ìœ¼ë¡œ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ë³´ìƒ ì„¤ê³„ì™€ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜ ì„ íƒì´ ëª¨ë¸ì˜ ê·œëª¨ë§Œí¼ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ê°•ì¡°í•˜ë©°, UserRLì´ ê°•ë ¥í•œ ì‚¬ìš©ì ì¤‘ì‹¬ ì—ì´ì „íŠ¸ ëª¨ë¸ ê°œë°œì˜ ì‹¤ìš©ì ì¸ ê²½ë¡œì„ì„ ì…ì¦í•©ë‹ˆë‹¤. ëª¨ë“  ì½”ë“œì™€ ë°ì´í„°ëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. UserRLì€ ì‚¬ìš©ì ì¤‘ì‹¬ì˜ ëŠ¥ë ¥ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ê¸° ìœ„í•œ í†µí•© í”„ë ˆì„ì›Œí¬ë¡œ, í‘œì¤€í™”ëœ í™˜ê²½ê³¼ ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ìë¥¼ í™œìš©í•©ë‹ˆë‹¤.
- 2. SFT ì´ˆê¸° ì‹œì‘ì€ ì´ˆê¸° ìƒí˜¸ì‘ìš© ëŠ¥ë ¥ì„ ë°œíœ˜í•˜ê³  ì§€ì†ì ì¸ ê°•í™” í•™ìŠµ ê°œì„ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.
- 3. ì˜ë„ì ì¸ ê²½ë¡œ ì ìˆ˜ ê³„ì‚°ì€ ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš©ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. ê°•ë ¥í•œ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©ì(ì˜ˆ: GPT-4o)ëŠ” í›ˆë ¨ì„ ìš©ì´í•˜ê²Œ í•˜ì§€ë§Œ, ì˜¤í”ˆ ì†ŒìŠ¤ ì‹œë®¬ë ˆì´í„°(ì˜ˆ: Qwen3-32B)ëŠ” ë¹„ìš© íš¨ìœ¨ì ì´ê³  ì „ì´ ê°€ëŠ¥í•œ ì˜µì…˜ì…ë‹ˆë‹¤.
- 5. ë³´ìƒ ì„¤ê³„ì™€ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜ ì„ íƒì˜ ì‹ ì¤‘í•œ ì„¤ê³„ê°€ ëª¨ë¸ ê·œëª¨ë§Œí¼ ì¤‘ìš”í•˜ë©°, UserRLì€ ê²¬ê³ í•œ ì‚¬ìš©ì ì¤‘ì‹¬ ì—ì´ì „íŠ¸ ëª¨ë¸ ê°œë°œì„ ìœ„í•œ ì‹¤ìš©ì ì¸ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:16:37*