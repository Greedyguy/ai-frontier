---
keywords:
  - Concept Bottleneck Model
  - Transformer
  - Neural Network
  - Deep Learning
  - Human-Neural Network Mutual Understanding
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2506.22803
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:39:42.611686",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Concept Bottleneck Model",
    "Transformer",
    "Neural Network",
    "Deep Learning",
    "Human-Neural Network Mutual Understanding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Concept Bottleneck Model": 0.82,
    "Transformer": 0.88,
    "Neural Network": 0.85,
    "Deep Learning": 0.8,
    "Human-Neural Network Mutual Understanding": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Concept Bottleneck Model",
        "canonical": "Concept Bottleneck Model",
        "aliases": [
          "CBM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework for enhancing interpretability in neural networks.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Transformer-based models",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the known category of Transformer models, which are widely used in deep learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      },
      {
        "surface": "Neural Network",
        "canonical": "Neural Network",
        "aliases": [
          "NN"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental concept in deep learning, providing a basis for linking various models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.95,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Deep Learning",
        "canonical": "Deep Learning",
        "aliases": [
          "DL"
        ],
        "category": "broad_technical",
        "rationale": "A core area of study, relevant to the paper's focus on model interpretability.",
        "novelty_score": 0.25,
        "connectivity_score": 0.92,
        "specificity_score": 0.55,
        "link_intent_score": 0.8
      },
      {
        "surface": "Human-Neural Network Mutual Understanding",
        "canonical": "Human-Neural Network Mutual Understanding",
        "aliases": [
          "HNMU"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific goal of the proposed model, enhancing human interaction with AI.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Concept Bottleneck Model",
      "resolved_canonical": "Concept Bottleneck Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Transformer-based models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Neural Network",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.95,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Deep Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.92,
        "specificity": 0.55,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Human-Neural Network Mutual Understanding",
      "resolved_canonical": "Human-Neural Network Mutual Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.22803.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2506.22803](https://arxiv.org/abs/2506.22803)

## 🔗 유사한 논문
- [[2025-09-23/Show and Tell_ Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models_20250923|Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models]] (87.6% similar)
- [[2025-09-23/Chat-CBM_ Towards Interactive Concept Bottleneck Models with Frozen Large Language Models_20250923|Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models]] (86.5% similar)
- [[2025-09-22/Bayesian Concept Bottleneck Models with LLM Priors_20250922|Bayesian Concept Bottleneck Models with LLM Priors]] (86.1% similar)
- [[2025-09-19/EnCoBo_ Energy-Guided Concept Bottlenecks for Interpretable Generation_20250919|EnCoBo: Energy-Guided Concept Bottlenecks for Interpretable Generation]] (84.9% similar)
- [[2025-09-24/ConceptFlow_ Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks_20250924|ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks]] (84.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Neural Network|Neural Network]], [[keywords/Deep Learning|Deep Learning]]
**⚡ Unique Technical**: [[keywords/Concept Bottleneck Model|Concept Bottleneck Model]], [[keywords/Human-Neural Network Mutual Understanding|Human-Neural Network Mutual Understanding]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.22803v3 Announce Type: replace-cross 
Abstract: Recent advances in deep learning have led to increasingly complex models with deeper layers and more parameters, reducing interpretability and making their decisions harder to understand. While many methods explain black-box reasoning, most lack effective interventions or only operate at sample-level without modifying the model itself. To address this, we propose the Concept Bottleneck Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU). CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable framework to approximate black-box reasoning and communicate conceptual understanding. Detrimental concepts are automatically identified and refined (removed/replaced) based on global gradient contributions. The modified CBM then distills corrected knowledge back into the black-box model, enhancing both interpretability and accuracy. We evaluate CBM-HNMU on various CNN and transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft, and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum increase in average accuracy across 1.03%. Source code is available at: https://github.com/XiGuaBo/CBM-HNMU.

## 📝 요약

최근 딥러닝의 발전으로 인해 모델의 복잡성이 증가하면서 해석 가능성이 감소하고 있습니다. 이를 해결하기 위해 본 연구에서는 인간과 신경망 간의 상호 이해를 증진시키기 위한 개념 병목 모델(CBM-HNMU)을 제안합니다. 이 모델은 해석 가능한 프레임워크인 개념 병목 모델(CBM)을 활용하여 블랙박스 모델의 추론을 근사하고 개념적 이해를 전달합니다. 유해한 개념은 전역 기울기 기여도를 기반으로 자동으로 식별 및 수정되며, 수정된 CBM은 블랙박스 모델에 올바른 지식을 전달하여 해석 가능성과 정확성을 향상시킵니다. Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft, CUB-200 등의 데이터셋을 통해 평가한 결과, 최대 2.64%의 정확도 향상과 평균 정확도 최대 1.03% 증가를 달성했습니다. 소스 코드는 https://github.com/XiGuaBo/CBM-HNMU에서 제공됩니다.

## 🎯 주요 포인트

- 1. CBM-HNMU는 Concept Bottleneck Model을 활용하여 블랙박스 모델의 해석 가능성을 높이고 개념적 이해를 전달합니다.
- 2. 유해한 개념은 글로벌 기울기 기여도를 기반으로 자동 식별 및 수정(제거/교체)됩니다.
- 3. 수정된 CBM은 블랙박스 모델에 교정된 지식을 다시 주입하여 해석 가능성과 정확성을 향상시킵니다.
- 4. Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft, CUB-200 데이터셋에서 최대 2.64%의 정확도 향상과 평균 정확도 1.03% 증가를 달성했습니다.
- 5. 소스 코드는 https://github.com/XiGuaBo/CBM-HNMU에서 제공됩니다.


---

*Generated on 2025-09-26 08:39:42*