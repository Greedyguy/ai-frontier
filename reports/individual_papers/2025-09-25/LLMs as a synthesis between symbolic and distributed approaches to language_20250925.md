---
keywords:
  - Large Language Model
  - Deep Learning
  - Symbolic and Distributed Approaches
  - Morphosyntactic Knowledge
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2502.11856
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:51:31.193660",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Deep Learning",
    "Symbolic and Distributed Approaches",
    "Morphosyntactic Knowledge"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Deep Learning": 0.8,
    "Symbolic and Distributed Approaches": 0.78,
    "Morphosyntactic Knowledge": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LLMs",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the discussion of synthesis between symbolic and distributed approaches.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "deep learning models",
        "canonical": "Deep Learning",
        "aliases": [
          "DL models",
          "deep neural networks"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is a foundational technology that underpins the synthesis discussed in the paper.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "symbolic and distributed approaches",
        "canonical": "Symbolic and Distributed Approaches",
        "aliases": [
          "symbolic vs distributed",
          "symbolic-distributed synthesis"
        ],
        "category": "unique_technical",
        "rationale": "This represents the core synthesis theme of the paper, linking symbolic and distributed paradigms.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "morphosyntactic knowledge",
        "canonical": "Morphosyntactic Knowledge",
        "aliases": [
          "morphosyntax",
          "syntactic knowledge"
        ],
        "category": "unique_technical",
        "rationale": "Morphosyntactic knowledge is highlighted as a key area where LLMs demonstrate symbolic-like behavior.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "engineering development",
      "fuzzy representations"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LLMs",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "deep learning models",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "symbolic and distributed approaches",
      "resolved_canonical": "Symbolic and Distributed Approaches",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "morphosyntactic knowledge",
      "resolved_canonical": "Morphosyntactic Knowledge",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# LLMs as a synthesis between symbolic and distributed approaches to language

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.11856.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2502.11856](https://arxiv.org/abs/2502.11856)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (89.6% similar)
- [[2025-09-23/Measuring Scalar Constructs in Social Science with LLMs_20250923|Measuring Scalar Constructs in Social Science with LLMs]] (87.2% similar)
- [[2025-09-25/LLMs4All_ A Review on Large Language Models for Research and Applications in Academic Disciplines_20250925|LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines]] (87.1% similar)
- [[2025-09-25/Language Models Fail to Introspect About Their Knowledge of Language_20250925|Language Models Fail to Introspect About Their Knowledge of Language]] (87.0% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (86.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Deep Learning|Deep Learning]]
**âš¡ Unique Technical**: [[keywords/Symbolic and Distributed Approaches|Symbolic and Distributed Approaches]], [[keywords/Morphosyntactic Knowledge|Morphosyntactic Knowledge]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.11856v2 Announce Type: replace 
Abstract: Since the middle of the 20th century, a fierce battle is being fought between symbolic and distributed approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the distributed camp has won, or dismissed as an irrelevant engineering development. In this position paper, I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both distributed/continuous/fuzzy and symbolic/discrete/categorical-like representations and processing; 2) models trained on language make use of this flexibility. In particular, I review recent research in interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it makes them particularly interesting for the study of language. Is it time for peace?

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ì™€ ì¸ì§€ì— ëŒ€í•œ ìƒì§•ì  ì ‘ê·¼ê³¼ ë¶„ì‚°ì  ì ‘ê·¼ ê°„ì˜ ì˜¤ëœ ë…¼ìŸì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‘ ì „í†µì˜ í†µí•©ì„ ë‚˜íƒ€ë‚¸ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ëŠ” ë¶„ì‚°ì /ì—°ì†ì /ëª¨í˜¸í•œ í‘œí˜„ê³¼ ìƒì§•ì /ì´ì‚°ì /ë²”ì£¼ì  í‘œí˜„ì„ ëª¨ë‘ í—ˆìš©í•˜ë©°, ì–¸ì–´ ëª¨ë¸ì€ ì´ëŸ¬í•œ ìœ ì—°ì„±ì„ í™œìš©í•©ë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´, LLMì€ í˜•íƒœí†µì‚¬ì  ì§€ì‹ì„ ê±°ì˜ ì´ì‚°ì ì¸ ë°©ì‹ìœ¼ë¡œ ì¸ì½”ë”©í•˜ë©°, ì´ëŠ” ë‹¤ì–‘í•œ í–‰ë™ì´ ìë°œì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê³  ëª¨ë¸ì´ í•„ìš”ì— ë”°ë¼ ë‘ ëª¨ë“œ ì‚¬ì´ë¥¼ ìœ ì—°í•˜ê²Œ ì „í™˜í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì„±ì€ LLMì˜ ì„±ê³µ ìš”ì¸ ì¤‘ í•˜ë‚˜ë¡œ, ì–¸ì–´ ì—°êµ¬ì— íŠ¹íˆ í¥ë¯¸ë¡œìš´ ì ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¬ë³¼ë¦­ ì ‘ê·¼ë²•ê³¼ ë¶„ì‚° ì ‘ê·¼ë²• ê°„ì˜ ë…¼ìŸì€ ì—¬ì „íˆ ì§„í–‰ ì¤‘ì´ë©°, ì‹¬ì¸µ í•™ìŠµ ëª¨ë¸ì€ ì´ ë‘ ì „í†µì˜ í†µí•©ì„ ë‚˜íƒ€ë‚¸ë‹¤.
- 2. ì‹¬ì¸µ í•™ìŠµ ì•„í‚¤í…ì²˜ëŠ” ë¶„ì‚°ì /ì—°ì†ì /ëª¨í˜¸í•œ í‘œí˜„ê³¼ ì‹¬ë³¼ë¦­/ì´ì‚°ì /ë²”ì£¼ì  í‘œí˜„ ë° ì²˜ë¦¬ë¥¼ ëª¨ë‘ í—ˆìš©í•œë‹¤.
- 3. ì–¸ì–´ì— ëŒ€í•œ ëª¨ë¸ í•™ìŠµì€ ì´ëŸ¬í•œ ìœ ì—°ì„±ì„ í™œìš©í•˜ë©°, ìµœê·¼ ì—°êµ¬ëŠ” LLMì—ì„œ í˜•íƒœí†µì‚¬ì  ì§€ì‹ì´ ê±°ì˜ ì´ì‚°ì ì¸ ë°©ì‹ìœ¼ë¡œ ì¸ì½”ë”©ë¨ì„ ë³´ì—¬ì¤€ë‹¤.
- 4. ëª¨ë¸ì€ í•„ìš”ì— ë”°ë¼ ë‘ ê°€ì§€ ëª¨ë“œ(ë° ê·¸ ì‚¬ì´ì˜ ëª¨ë“  ê²ƒ) ì‚¬ì´ë¥¼ ìœ ì—°í•˜ê²Œ ì „í™˜í•˜ë©°, ì´ëŠ” ê·¸ë“¤ì˜ ì„±ê³µì˜ ì£¼ìš” ì´ìœ  ì¤‘ í•˜ë‚˜ë¡œ ì–¸ì–´ ì—°êµ¬ì— íŠ¹íˆ í¥ë¯¸ë¡­ë‹¤.


---

*Generated on 2025-09-26 08:51:31*