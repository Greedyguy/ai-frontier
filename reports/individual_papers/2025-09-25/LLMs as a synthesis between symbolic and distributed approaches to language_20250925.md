---
keywords:
  - Large Language Model
  - Deep Learning
  - Symbolic and Distributed Approaches
  - Morphosyntactic Knowledge
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2502.11856
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:51:31.193660",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Deep Learning",
    "Symbolic and Distributed Approaches",
    "Morphosyntactic Knowledge"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Deep Learning": 0.8,
    "Symbolic and Distributed Approaches": 0.78,
    "Morphosyntactic Knowledge": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LLMs",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the discussion of synthesis between symbolic and distributed approaches.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "deep learning models",
        "canonical": "Deep Learning",
        "aliases": [
          "DL models",
          "deep neural networks"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is a foundational technology that underpins the synthesis discussed in the paper.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "symbolic and distributed approaches",
        "canonical": "Symbolic and Distributed Approaches",
        "aliases": [
          "symbolic vs distributed",
          "symbolic-distributed synthesis"
        ],
        "category": "unique_technical",
        "rationale": "This represents the core synthesis theme of the paper, linking symbolic and distributed paradigms.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "morphosyntactic knowledge",
        "canonical": "Morphosyntactic Knowledge",
        "aliases": [
          "morphosyntax",
          "syntactic knowledge"
        ],
        "category": "unique_technical",
        "rationale": "Morphosyntactic knowledge is highlighted as a key area where LLMs demonstrate symbolic-like behavior.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "engineering development",
      "fuzzy representations"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LLMs",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "deep learning models",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "symbolic and distributed approaches",
      "resolved_canonical": "Symbolic and Distributed Approaches",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "morphosyntactic knowledge",
      "resolved_canonical": "Morphosyntactic Knowledge",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# LLMs as a synthesis between symbolic and distributed approaches to language

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.11856.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2502.11856](https://arxiv.org/abs/2502.11856)

## 🔗 유사한 논문
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (89.6% similar)
- [[2025-09-23/Measuring Scalar Constructs in Social Science with LLMs_20250923|Measuring Scalar Constructs in Social Science with LLMs]] (87.2% similar)
- [[2025-09-25/LLMs4All_ A Review on Large Language Models for Research and Applications in Academic Disciplines_20250925|LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines]] (87.1% similar)
- [[2025-09-25/Language Models Fail to Introspect About Their Knowledge of Language_20250925|Language Models Fail to Introspect About Their Knowledge of Language]] (87.0% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (86.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Deep Learning|Deep Learning]]
**⚡ Unique Technical**: [[keywords/Symbolic and Distributed Approaches|Symbolic and Distributed Approaches]], [[keywords/Morphosyntactic Knowledge|Morphosyntactic Knowledge]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.11856v2 Announce Type: replace 
Abstract: Since the middle of the 20th century, a fierce battle is being fought between symbolic and distributed approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the distributed camp has won, or dismissed as an irrelevant engineering development. In this position paper, I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both distributed/continuous/fuzzy and symbolic/discrete/categorical-like representations and processing; 2) models trained on language make use of this flexibility. In particular, I review recent research in interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it makes them particularly interesting for the study of language. Is it time for peace?

## 📝 요약

이 논문은 언어와 인지에 대한 상징적 접근과 분산적 접근 간의 오랜 논쟁에서 딥러닝 모델, 특히 대규모 언어 모델(LLM)이 두 전통의 통합을 나타낸다고 주장합니다. 딥러닝 아키텍처는 분산적/연속적/모호한 표현과 상징적/이산적/범주적 표현을 모두 허용하며, 언어 모델은 이러한 유연성을 활용합니다. 최근 연구에 따르면, LLM은 형태통사적 지식을 거의 이산적인 방식으로 인코딩하며, 이는 다양한 행동이 자발적으로 나타나고 모델이 필요에 따라 두 모드 사이를 유연하게 전환할 수 있음을 시사합니다. 이러한 특성은 LLM의 성공 요인 중 하나로, 언어 연구에 특히 흥미로운 점입니다.

## 🎯 주요 포인트

- 1. 심볼릭 접근법과 분산 접근법 간의 논쟁은 여전히 진행 중이며, 심층 학습 모델은 이 두 전통의 통합을 나타낸다.
- 2. 심층 학습 아키텍처는 분산적/연속적/모호한 표현과 심볼릭/이산적/범주적 표현 및 처리를 모두 허용한다.
- 3. 언어에 대한 모델 학습은 이러한 유연성을 활용하며, 최근 연구는 LLM에서 형태통사적 지식이 거의 이산적인 방식으로 인코딩됨을 보여준다.
- 4. 모델은 필요에 따라 두 가지 모드(및 그 사이의 모든 것) 사이를 유연하게 전환하며, 이는 그들의 성공의 주요 이유 중 하나로 언어 연구에 특히 흥미롭다.


---

*Generated on 2025-09-26 08:51:31*