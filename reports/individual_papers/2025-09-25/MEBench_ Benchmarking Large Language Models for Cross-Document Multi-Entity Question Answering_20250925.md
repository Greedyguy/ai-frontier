---
keywords:
  - Large Language Model
  - Retrieval Augmented Generation
  - Multi-Entity Question Answering
  - Entity-Attributed F1
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2502.18993
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:51:59.050172",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Retrieval Augmented Generation",
    "Multi-Entity Question Answering",
    "Entity-Attributed F1"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Retrieval Augmented Generation": 0.82,
    "Multi-Entity Question Answering": 0.78,
    "Entity-Attributed F1": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on MEQA and are a key technology in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.68,
        "link_intent_score": 0.85
      },
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG",
          "Retrieval-Augmented Generation"
        ],
        "category": "specific_connectable",
        "rationale": "RAG is a trending concept that enhances LLMs by integrating retrieval mechanisms, relevant for MEQA.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multi-Entity Question Answering",
        "canonical": "Multi-Entity Question Answering",
        "aliases": [
          "MEQA"
        ],
        "category": "unique_technical",
        "rationale": "MEQA is a unique challenge addressed by the paper, focusing on cross-document entity reasoning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "Entity-Attributed F1",
        "canonical": "Entity-Attributed F1",
        "aliases": [
          "EA-F1"
        ],
        "category": "unique_technical",
        "rationale": "EA-F1 is a specific metric introduced for evaluating entity-level correctness in MEQA tasks.",
        "novelty_score": 0.68,
        "connectivity_score": 0.54,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "information extraction"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.68,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multi-Entity Question Answering",
      "resolved_canonical": "Multi-Entity Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Entity-Attributed F1",
      "resolved_canonical": "Entity-Attributed F1",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.54,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.18993.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2502.18993](https://arxiv.org/abs/2502.18993)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MSCoRe_ A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents_20250923|MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents]] (86.8% similar)
- [[2025-09-24/AECBench_ A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field_20250924|AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field]] (86.8% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (86.7% similar)
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (86.2% similar)
- [[2025-09-23/Beyond Prompting_ An Efficient Embedding Framework for Open-Domain Question Answering_20250923|Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering]] (86.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**âš¡ Unique Technical**: [[keywords/Multi-Entity Question Answering|Multi-Entity Question Answering]], [[keywords/Entity-Attributed F1|Entity-Attributed F1]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.18993v3 Announce Type: replace 
Abstract: Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.

## ğŸ“ ìš”ì•½

ë‹¤ì¤‘ ì—”í‹°í‹° ì§ˆë¬¸ ì‘ë‹µ(MEQA)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ê²€ìƒ‰ ë³´ê°• ìƒì„±(RAG) ì‹œìŠ¤í…œì— ìƒë‹¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ë‹¨ì¼ ë¬¸ì„œ ì´í•´ì—ëŠ” ë›°ì–´ë‚˜ì§€ë§Œ, ë‹¤ì–‘í•œ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ MEBenchë¼ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. MEBenchëŠ” LLMì´ ë¶„ì‚°ëœ ì •ë³´ë¥¼ ê²€ìƒ‰, í†µí•©, ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ë‹¤ì¤‘ ë¬¸ì„œ, ë‹¤ì¤‘ ì—”í‹°í‹° ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. 4,780ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì‹¤ì œ ë‹¤ì¤‘ ì—”í‹°í‹° ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬ê´„í•©ë‹ˆë‹¤. ìµœì‹  LLMê³¼ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ ê²°ê³¼, MEBenchì—ì„œ 59%ì˜ ì •í™•ë„ì— ê·¸ì³¤ìœ¼ë©°, ì •ë³´ ì¶”ì¶œì˜ ì™„ì „ì„±ê³¼ ì‚¬ì‹¤ì  ì •í™•ì„±ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. MEBenchëŠ” í˜„ì¬ LLMì˜ í•œê³„ë¥¼ ë“œëŸ¬ë‚´ê³ , ì—”í‹°í‹° ì¸ì‹ QA ì•„í‚¤í…ì²˜ ë°œì „ì˜ ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MEQAëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ì‹œìŠ¤í…œì— ìˆì–´ ì—¬ëŸ¬ ë¬¸ì„œì— í©ì–´ì§„ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤.
- 2. MEBenchëŠ” LLMì˜ ì •ë³´ ê²€ìƒ‰, í†µí•© ë° ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë‹¤ì¤‘ ë¬¸ì„œ, ë‹¤ì¤‘ ì—”í‹°í‹° ë²¤ì¹˜ë§ˆí¬ì´ë‹¤.
- 3. MEBenchëŠ” 4,780ê°œì˜ ì§ˆë¬¸ì„ ì„¸ ê°€ì§€ ì£¼ìš” ë²”ì£¼ì™€ ì—¬ëŸ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ì‹¤ì œ ë‹¤ì¤‘ ì—”í‹°í‹° ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬ê´„í•œë‹¤.
- 4. ìµœì‹  LLMê³¼ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ ê²°ê³¼, MEBenchì—ì„œ 59%ì˜ ì •í™•ë„ë§Œì„ ë‹¬ì„±í•˜ì—¬ í•œê³„ë¥¼ ë“œëŸ¬ëƒˆë‹¤.
- 5. MEBenchëŠ” í˜„ì¬ LLM í”„ë ˆì„ì›Œí¬ì˜ ì²´ê³„ì  ì•½ì ì„ ê°•ì¡°í•˜ë©°, ì—”í‹°í‹° ì¤‘ì‹¬ì˜ QA ì•„í‚¤í…ì²˜ ë°œì „ì„ ìœ„í•œ ê¸°ì´ˆë¥¼ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-26 08:51:59*