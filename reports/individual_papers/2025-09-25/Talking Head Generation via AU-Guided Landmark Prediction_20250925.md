---
keywords:
  - Facial Action Units
  - Landmark Prediction
  - Diffusion-based Video Synthesis
  - Talking Head Generation
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19749
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:05:47.966696",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Facial Action Units",
    "Landmark Prediction",
    "Diffusion-based Video Synthesis",
    "Talking Head Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Facial Action Units": 0.78,
    "Landmark Prediction": 0.79,
    "Diffusion-based Video Synthesis": 0.82,
    "Talking Head Generation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Action Units",
        "canonical": "Facial Action Units",
        "aliases": [
          "AUs"
        ],
        "category": "unique_technical",
        "rationale": "Facial Action Units are central to the paper's method for expression control, offering a unique technical approach.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Landmark Prediction",
        "canonical": "Landmark Prediction",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Landmark prediction is a key component in the proposed framework, linking to computer vision tasks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      },
      {
        "surface": "Diffusion-based Synthesizer",
        "canonical": "Diffusion-based Video Synthesis",
        "aliases": [
          "Diffusion-based Synthesizer"
        ],
        "category": "unique_technical",
        "rationale": "This technique is crucial for generating realistic videos, representing a novel application of diffusion models.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Talking Head Generation",
        "canonical": "Talking Head Generation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The core focus of the paper, connecting to advancements in video synthesis and animation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Action Units",
      "resolved_canonical": "Facial Action Units",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Landmark Prediction",
      "resolved_canonical": "Landmark Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Diffusion-based Synthesizer",
      "resolved_canonical": "Diffusion-based Video Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Talking Head Generation",
      "resolved_canonical": "Talking Head Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Talking Head Generation via AU-Guided Landmark Prediction

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19749.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19749](https://arxiv.org/abs/2509.19749)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Audio-Driven Universal Gaussian Head Avatars_20250924|Audio-Driven Universal Gaussian Head Avatars]] (84.4% similar)
- [[2025-09-23/Beat on Gaze_ Learning Stylized Generation of Gaze and Head Dynamics_20250923|Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics]] (84.3% similar)
- [[2025-09-22/Tiny is not small enough_ High-quality, low-resource facial animation models through hybrid knowledge distillation_20250922|Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation]] (82.3% similar)
- [[2025-09-22/FLOAT_ Generative Motion Latent Flow Matching for Audio-driven Talking Portrait_20250922|FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait]] (82.3% similar)
- [[2025-09-23/Follow-Your-Emoji-Faster_ Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation_20250923|Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Landmark Prediction|Landmark Prediction]], [[keywords/Talking Head Generation|Talking Head Generation]]
**âš¡ Unique Technical**: [[keywords/Facial Action Units|Facial Action Units]], [[keywords/Diffusion-based Video Synthesis|Diffusion-based Video Synthesis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19749v1 Announce Type: new 
Abstract: We propose a two-stage framework for audio-driven talking head generation with fine-grained expression control via facial Action Units (AUs). Unlike prior methods relying on emotion labels or implicit AU conditioning, our model explicitly maps AUs to 2D facial landmarks, enabling physically grounded, per-frame expression control. In the first stage, a variational motion generator predicts temporally coherent landmark sequences from audio and AU intensities. In the second stage, a diffusion-based synthesizer generates realistic, lip-synced videos conditioned on these landmarks and a reference image. This separation of motion and appearance improves expression accuracy, temporal stability, and visual realism. Experiments on the MEAD dataset show that our method outperforms state-of-the-art baselines across multiple metrics, demonstrating the effectiveness of explicit AU-to-landmark modeling for expressive talking head generation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¼êµ´ í–‰ë™ ë‹¨ìœ„(AUs)ë¥¼ í™œìš©í•œ ì„¸ë°€í•œ í‘œì • ì œì–´ê°€ ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ê¸°ë°˜ì˜ ë§í•˜ëŠ” ì–¼êµ´ ìƒì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ê³¼ ë‹¬ë¦¬, AUsë¥¼ 2D ì–¼êµ´ ëœë“œë§ˆí¬ë¡œ ëª…ì‹œì ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ë¬¼ë¦¬ì ìœ¼ë¡œ ê¸°ë°˜ì´ ìˆëŠ” í”„ë ˆì„ë³„ í‘œì • ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ë³€ë™ ëª¨ì…˜ ìƒì„±ê¸°ê°€ ì˜¤ë””ì˜¤ì™€ AU ê°•ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ëœë“œë§ˆí¬ ì‹œí€€ìŠ¤ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” í™•ì‚° ê¸°ë°˜ í•©ì„±ê¸°ê°€ ì´ëŸ¬í•œ ëœë“œë§ˆí¬ì™€ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì‚¬ì‹¤ì ì´ê³  ë¦½ì‹±í¬ëœ ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ëª¨ì…˜ê³¼ ì™¸í˜•ì˜ ë¶„ë¦¬ëŠ” í‘œí˜„ ì •í™•ì„±, ì‹œê°„ì  ì•ˆì •ì„±, ì‹œê°ì  ì‚¬ì‹¤ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. MEAD ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì—¬ëŸ¬ ì§€í‘œì—ì„œ ìµœì‹  ê¸°ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©°, ëª…ì‹œì ì¸ AU-ëœë“œë§ˆí¬ ëª¨ë¸ë§ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ì–¼êµ´ í–‰ë™ ë‹¨ìœ„(AUs)ë¥¼ í†µí•´ ì„¸ë°€í•œ í‘œí˜„ ì œì–´ê°€ ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ê¸°ë°˜ ë§í•˜ëŠ” ì–¼êµ´ ìƒì„± ê¸°ìˆ ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 2. ëª¨ë¸ì€ AUsë¥¼ 2D ì–¼êµ´ ëœë“œë§ˆí¬ë¡œ ëª…ì‹œì ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ë¬¼ë¦¬ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” í”„ë ˆì„ë³„ í‘œí˜„ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ë³€ë™ ëª¨ì…˜ ìƒì„±ê¸°ê°€ ì˜¤ë””ì˜¤ì™€ AU ê°•ë„ë¡œë¶€í„° ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ëœë“œë§ˆí¬ ì‹œí€€ìŠ¤ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
- 4. ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” í™•ì‚° ê¸°ë°˜ í•©ì„±ê¸°ê°€ ì´ëŸ¬í•œ ëœë“œë§ˆí¬ì™€ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì‚¬ì‹¤ì ì´ê³  ë¦½ì‹±í¬ëœ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- 5. MEAD ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì—¬ëŸ¬ ì§€í‘œì—ì„œ ìµœì‹  ê¸°ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©°, ëª…ì‹œì ì¸ AU-ëœë“œë§ˆí¬ ëª¨ë¸ë§ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:05:47*