---
keywords:
  - Large Language Model
  - Thinking Augmented Pre-Training
  - Reasoning Benchmarks
  - Data Efficiency
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.20186
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T17:02:01.043586",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Thinking Augmented Pre-Training",
    "Reasoning Benchmarks",
    "Data Efficiency"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Thinking Augmented Pre-Training": 0.92,
    "Reasoning Benchmarks": 0.78,
    "Data Efficiency": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and improvements, making them crucial for linking related research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Thinking Augmented Pre-Training",
        "canonical": "Thinking Augmented Pre-Training",
        "aliases": [
          "TPT"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel methodology introduced by the paper, essential for understanding the specific contributions of the research.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.92
      },
      {
        "surface": "Reasoning Benchmarks",
        "canonical": "Reasoning Benchmarks",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Reasoning benchmarks are used to evaluate the effectiveness of the proposed method, linking it to performance metrics.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Data Efficiency",
        "canonical": "Data Efficiency",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Improving data efficiency is a key outcome of the research, relevant for linking to optimization studies in LLM training.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "methodology",
      "performance",
      "training data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Thinking Augmented Pre-Training",
      "resolved_canonical": "Thinking Augmented Pre-Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Reasoning Benchmarks",
      "resolved_canonical": "Reasoning Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Data Efficiency",
      "resolved_canonical": "Data Efficiency",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Thinking Augmented Pre-training

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20186.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.20186](https://arxiv.org/abs/2509.20186)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (88.5% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (86.5% similar)
- [[2025-09-23/Language Modeling with Learned Meta-Tokens_20250923|Language Modeling with Learned Meta-Tokens]] (85.8% similar)
- [[2025-09-23/L-MTP_ Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models_20250923|L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models]] (84.9% similar)
- [[2025-09-22/Tag&Tab_ Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack_20250922|Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reasoning Benchmarks|Reasoning Benchmarks]], [[keywords/Data Efficiency|Data Efficiency]]
**âš¡ Unique Technical**: [[keywords/Thinking Augmented Pre-Training|Thinking Augmented Pre-Training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20186v1 Announce Type: cross 
Abstract: This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\%$ on several challenging reasoning benchmarks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì˜ ë°ì´í„° íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ê¸°ì¡´ í…ìŠ¤íŠ¸ ë°ì´í„°ì— ì‚¬ê³  ê²½ë¡œë¥¼ ì¶”ê°€í•˜ëŠ” ê°„ë‹¨í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê³ í’ˆì§ˆ ë°ì´í„°ì˜ ì œí•œëœ ê°€ìš©ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 'Thinking augmented Pre-Training (TPT)' ë°©ë²•ë¡ ì„ ë„ì…í•˜ì—¬ ìë™ ìƒì„±ëœ ì‚¬ê³  ê²½ë¡œë¡œ í…ìŠ¤íŠ¸ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í›ˆë ¨ ë°ì´í„°ì˜ ì–‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ê³ , ë³µì¡í•œ ê³ í’ˆì§ˆ í† í°ì„ ë‹¨ê³„ë³„ ì¶”ë¡ ê³¼ ë¶„í•´ë¥¼ í†µí•´ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ë‹¤ì–‘í•œ í›ˆë ¨ ì„¤ì •ì—ì„œ TPTë¥¼ ì ìš©í•œ ê²°ê³¼, LLMì˜ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìœ¼ë©°, ë°ì´í„° íš¨ìœ¨ì„±ì´ 3ë°° ì¦ê°€í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, 3B íŒŒë¼ë¯¸í„° ëª¨ë¸ì—ì„œëŠ” ì—¬ëŸ¬ ì–´ë ¤ìš´ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ í›ˆë ¨ í›„ ì„±ëŠ¥ì´ 10% ì´ìƒ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ê¸°ì¡´ í…ìŠ¤íŠ¸ ë°ì´í„°ì— ì‚¬ê³  ê²½ë¡œë¥¼ ì¶”ê°€í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì˜ ë°ì´í„° íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. Thinking augmented Pre-Training (TPT)ì€ ìë™ ìƒì„±ëœ ì‚¬ê³  ê²½ë¡œë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ë³´ê°•í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì˜ ì–‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ê³ , ê³ í’ˆì§ˆ í† í°ì„ ë‹¨ê³„ë³„ ì¶”ë¡ ê³¼ ë¶„í•´ë¥¼ í†µí•´ ë” ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- 3. TPTëŠ” ë‹¤ì–‘í•œ í›ˆë ¨ êµ¬ì„±ì—ì„œ ì ìš©ë˜ë©°, ì‹¤í—˜ ê²°ê³¼ LLMì˜ ì„±ëŠ¥ì„ ëª¨ë¸ í¬ê¸°ì™€ ê³„ì—´ì— ê±¸ì³ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. TPTëŠ” LLM ì‚¬ì „ í›ˆë ¨ì˜ ë°ì´í„° íš¨ìœ¨ì„±ì„ 3ë°° í–¥ìƒì‹œí‚¤ë©°, 3B íŒŒë¼ë¯¸í„° ëª¨ë¸ì˜ ê²½ìš° ì—¬ëŸ¬ ì–´ë ¤ìš´ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ í›ˆë ¨ í›„ ì„±ëŠ¥ì„ 10% ì´ìƒ ê°œì„ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 17:02:01*