---
keywords:
  - Scientific VQA
  - Vision-Language Model
  - Text-in-Image Format
  - Multimodal Learning
  - Zero-Shot Learning
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.20119
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:12:55.463556",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Scientific VQA",
    "Vision-Language Model",
    "Text-in-Image Format",
    "Multimodal Learning",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Scientific VQA": 0.78,
    "Vision-Language Model": 0.8,
    "Text-in-Image Format": 0.75,
    "Multimodal Learning": 0.77,
    "Zero-Shot Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Scientific Visual Question Answering",
        "canonical": "Scientific VQA",
        "aliases": [
          "Scientific Visual QA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task that combines vision and language processing, crucial for linking specialized research in multimodal AI.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Systems"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper and connects to a broader trend in AI research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Text-in-Image Format",
        "canonical": "Text-in-Image Format",
        "aliases": [
          "Embedded Text Images"
        ],
        "category": "unique_technical",
        "rationale": "This format is a novel approach in the context of the paper, enabling new methods of data representation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is a key aspect of the paper's methodology, linking it to broader AI research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-Shot Settings",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a significant challenge addressed in the paper, relevant for linking to ongoing research in AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "training data",
      "fine-tuning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Scientific Visual Question Answering",
      "resolved_canonical": "Scientific VQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Text-in-Image Format",
      "resolved_canonical": "Text-in-Image Format",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-Shot Settings",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20119.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.20119](https://arxiv.org/abs/2509.20119)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning_20250923|Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning]] (87.0% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (84.2% similar)
- [[2025-09-25/Towards Visual Text Grounding of Multimodal Large Language Model_20250925|Towards Visual Text Grounding of Multimodal Large Language Model]] (84.2% similar)
- [[2025-09-23/ProtoVQA_ An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering_20250923|ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering]] (83.5% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Scientific VQA|Scientific VQA]], [[keywords/Text-in-Image Format|Text-in-Image Format]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20119v1 Announce Type: new 
Abstract: Scientific visual question answering poses significant challenges for vision-language models due to the complexity of scientific figures and their multimodal context. Traditional approaches treat the figure and accompanying text (e.g., questions and answer options) as separate inputs. EXAMS-V introduced a new paradigm by embedding both visual and textual content into a single image. However, even state-of-the-art proprietary models perform poorly on this setup in zero-shot settings, underscoring the need for task-specific fine-tuning. To address the scarcity of training data in this "text-in-image" format, we synthesize a new dataset by converting existing separate image-text pairs into unified images. Fine-tuning a small multilingual multimodal model on a mix of our synthetic data and EXAMS-V yields notable gains across 13 languages, demonstrating strong average improvements and cross-lingual transfer.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³¼í•™ì  ì‹œê° ì§ˆë¬¸ ì‘ë‹µì—ì„œ ì‹œê°-ì–¸ì–´ ëª¨ë¸ì´ ì§ë©´í•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í–ˆìœ¼ë‚˜, EXAMS-VëŠ” ì´ë¥¼ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœì‹  ëª¨ë¸ë„ ì´ ì„¤ì •ì—ì„œ ì„±ëŠ¥ì´ ì €ì¡°í•˜ì—¬, íŠ¹ì • ì‘ì—…ì— ë§ì¶˜ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê¸°ì¡´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì„ í†µí•© ì´ë¯¸ì§€ë¡œ ë³€í™˜í•œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ í•©ì„± ë°ì´í„°ì™€ EXAMS-Vë¥¼ ì‚¬ìš©í•´ ì†Œí˜• ë‹¤êµ­ì–´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•œ ê²°ê³¼, 13ê°œ ì–¸ì–´ì—ì„œ í‰ê·  ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ê³ , ì–¸ì–´ ê°„ ì „ì´ í•™ìŠµì—ì„œë„ ê°•ë ¥í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³¼í•™ì  ì‹œê° ì§ˆë¬¸ ì‘ë‹µì€ ê³¼í•™ì  ë„í‘œì˜ ë³µì¡ì„±ê³¼ ë‹¤ì¤‘ ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ë•Œë¬¸ì— ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì—ê²Œ ìƒë‹¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- 2. EXAMS-VëŠ” ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ë‹¨ì¼ ì´ë¯¸ì§€ë¡œ ì„ë² ë”©í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 3. "í…ìŠ¤íŠ¸-ì¸-ì´ë¯¸ì§€" í˜•ì‹ì˜ í›ˆë ¨ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì„ í†µí•© ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ í•©ì„±í–ˆìŠµë‹ˆë‹¤.
- 4. í•©ì„± ë°ì´í„°ì™€ EXAMS-Vë¥¼ í˜¼í•©í•˜ì—¬ ì†Œí˜• ë‹¤êµ­ì–´ ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•œ ê²°ê³¼, 13ê°œ ì–¸ì–´ì—ì„œ í‰ê· ì ì¸ ì„±ëŠ¥ í–¥ìƒê³¼ êµì°¨ ì–¸ì–´ ì „ì´ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:12:55*