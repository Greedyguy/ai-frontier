---
keywords:
  - Scientific VQA
  - Vision-Language Model
  - Text-in-Image Format
  - Multimodal Learning
  - Zero-Shot Learning
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.20119
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:12:55.463556",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Scientific VQA",
    "Vision-Language Model",
    "Text-in-Image Format",
    "Multimodal Learning",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Scientific VQA": 0.78,
    "Vision-Language Model": 0.8,
    "Text-in-Image Format": 0.75,
    "Multimodal Learning": 0.77,
    "Zero-Shot Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Scientific Visual Question Answering",
        "canonical": "Scientific VQA",
        "aliases": [
          "Scientific Visual QA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task that combines vision and language processing, crucial for linking specialized research in multimodal AI.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Systems"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper and connects to a broader trend in AI research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Text-in-Image Format",
        "canonical": "Text-in-Image Format",
        "aliases": [
          "Embedded Text Images"
        ],
        "category": "unique_technical",
        "rationale": "This format is a novel approach in the context of the paper, enabling new methods of data representation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is a key aspect of the paper's methodology, linking it to broader AI research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-Shot Settings",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a significant challenge addressed in the paper, relevant for linking to ongoing research in AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "training data",
      "fine-tuning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Scientific Visual Question Answering",
      "resolved_canonical": "Scientific VQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Text-in-Image Format",
      "resolved_canonical": "Text-in-Image Format",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-Shot Settings",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20119.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.20119](https://arxiv.org/abs/2509.20119)

## 🔗 유사한 논문
- [[2025-09-23/Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning_20250923|Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning]] (87.0% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (84.2% similar)
- [[2025-09-25/Towards Visual Text Grounding of Multimodal Large Language Model_20250925|Towards Visual Text Grounding of Multimodal Large Language Model]] (84.2% similar)
- [[2025-09-23/ProtoVQA_ An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering_20250923|ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering]] (83.5% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Scientific VQA|Scientific VQA]], [[keywords/Text-in-Image Format|Text-in-Image Format]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20119v1 Announce Type: new 
Abstract: Scientific visual question answering poses significant challenges for vision-language models due to the complexity of scientific figures and their multimodal context. Traditional approaches treat the figure and accompanying text (e.g., questions and answer options) as separate inputs. EXAMS-V introduced a new paradigm by embedding both visual and textual content into a single image. However, even state-of-the-art proprietary models perform poorly on this setup in zero-shot settings, underscoring the need for task-specific fine-tuning. To address the scarcity of training data in this "text-in-image" format, we synthesize a new dataset by converting existing separate image-text pairs into unified images. Fine-tuning a small multilingual multimodal model on a mix of our synthetic data and EXAMS-V yields notable gains across 13 languages, demonstrating strong average improvements and cross-lingual transfer.

## 📝 요약

이 논문은 과학적 시각 질문 응답에서 시각-언어 모델이 직면하는 문제를 다룹니다. 기존 방법은 이미지와 텍스트를 별도로 처리했으나, EXAMS-V는 이를 하나의 이미지로 통합하는 새로운 접근 방식을 제안했습니다. 그러나 최신 모델도 이 설정에서 성능이 저조하여, 특정 작업에 맞춘 미세 조정이 필요함을 보여줍니다. 이를 해결하기 위해, 기존 이미지-텍스트 쌍을 통합 이미지로 변환한 새로운 데이터셋을 생성했습니다. 이 합성 데이터와 EXAMS-V를 사용해 소형 다국어 멀티모달 모델을 미세 조정한 결과, 13개 언어에서 평균 성능이 크게 향상되고, 언어 간 전이 학습에서도 강력한 성과를 보였습니다.

## 🎯 주요 포인트

- 1. 과학적 시각 질문 응답은 과학적 도표의 복잡성과 다중 모달 컨텍스트 때문에 비전-언어 모델에게 상당한 도전 과제를 제시합니다.
- 2. EXAMS-V는 시각적 및 텍스트 콘텐츠를 단일 이미지로 임베딩하는 새로운 패러다임을 도입했습니다.
- 3. "텍스트-인-이미지" 형식의 훈련 데이터 부족 문제를 해결하기 위해 기존의 이미지-텍스트 쌍을 통합 이미지로 변환하여 새로운 데이터셋을 합성했습니다.
- 4. 합성 데이터와 EXAMS-V를 혼합하여 소형 다국어 다중 모달 모델을 미세 조정한 결과, 13개 언어에서 평균적인 성능 향상과 교차 언어 전이를 보여주었습니다.


---

*Generated on 2025-09-26 09:12:55*