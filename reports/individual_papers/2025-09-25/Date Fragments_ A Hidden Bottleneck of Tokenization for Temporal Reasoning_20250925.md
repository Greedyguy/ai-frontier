---
keywords:
  - Large Language Model
  - Date Fragmentation Ratio
  - Temporal Reasoning
  - DateAugBench
  - Causal Attention-Hop Analysis
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2505.16088
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:25:35.511881",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Date Fragmentation Ratio",
    "Temporal Reasoning",
    "DateAugBench",
    "Causal Attention-Hop Analysis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Date Fragmentation Ratio": 0.78,
    "Temporal Reasoning": 0.82,
    "DateAugBench": 0.77,
    "Causal Attention-Hop Analysis": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This term is crucial for linking discussions on the capabilities and limitations of advanced NLP models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "date fragmentation ratio",
        "canonical": "Date Fragmentation Ratio",
        "aliases": [
          "date fragmentation",
          "fragmentation ratio"
        ],
        "category": "unique_technical",
        "rationale": "This new metric is central to the paper's contribution and offers a unique angle for temporal reasoning analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "temporal reasoning",
        "canonical": "Temporal Reasoning",
        "aliases": [
          "time reasoning",
          "temporal logic"
        ],
        "category": "specific_connectable",
        "rationale": "Temporal reasoning is a key concept in understanding how models process time-related data.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "DateAugBench",
        "canonical": "DateAugBench",
        "aliases": [
          "Date Augmentation Benchmark"
        ],
        "category": "unique_technical",
        "rationale": "This benchmark is a novel dataset that supports the evaluation of temporal reasoning tasks.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "causal attention-hop analyses",
        "canonical": "Causal Attention-Hop Analysis",
        "aliases": [
          "attention-hop analysis",
          "causal attention analysis"
        ],
        "category": "unique_technical",
        "rationale": "This analysis technique is pivotal in understanding the model's internal mechanisms for date abstraction.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "calendar dates",
      "token counts",
      "accuracy drops"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "date fragmentation ratio",
      "resolved_canonical": "Date Fragmentation Ratio",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "temporal reasoning",
      "resolved_canonical": "Temporal Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "DateAugBench",
      "resolved_canonical": "DateAugBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "causal attention-hop analyses",
      "resolved_canonical": "Causal Attention-Hop Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.16088.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2505.16088](https://arxiv.org/abs/2505.16088)

## 🔗 유사한 논문
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (79.6% similar)
- [[2025-09-23/Time Is a Feature_ Exploiting Temporal Dynamics in Diffusion Language Models_20250923|Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models]] (79.0% similar)
- [[2025-09-23/Breaking Token Into Concepts_ Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics_20250923|Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics]] (78.9% similar)
- [[2025-09-23/Time to Revist Exact Match_20250923|Time to Revist Exact Match]] (78.6% similar)
- [[2025-09-25/Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks_20250925|Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks]] (78.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Temporal Reasoning|Temporal Reasoning]]
**⚡ Unique Technical**: [[keywords/Date Fragmentation Ratio|Date Fragmentation Ratio]], [[keywords/DateAugBench|DateAugBench]], [[keywords/Causal Attention-Hop Analysis|Causal Attention-Hop Analysis]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.16088v3 Announce Type: replace-cross 
Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future time periods; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year $\rightarrow$ month $\rightarrow$ day). Our datasets and code are made publicly available \href{https://github.com/gagan3012/date-fragments}{here}.

## 📝 요약

현대의 BPE 토크나이저는 날짜를 의미 없는 조각으로 분할하여 토큰 수를 증가시키고 시간적 추론에 필요한 구조를 흐리게 합니다. 본 연구에서는 (1) 날짜 구성 요소를 얼마나 잘 보존하는지를 측정하는 '날짜 분할 비율'이라는 해석 가능한 지표를 도입하고, (2) 6500개의 예제를 포함한 DateAugBench를 공개하여 세 가지 시간적 추론 과제를 다루며, (3) 레이어별 분석을 통해 대형 언어 모델이 날짜 조각을 결합하여 시간적 추론을 수행하는 메커니즘을 발견했습니다. 실험 결과, 과도한 분할은 정확도에 최대 10점의 감소를 초래하며, 모델이 클수록 날짜 조각을 빠르게 통합하는 경향이 있음을 확인했습니다. 또한, 대형 언어 모델이 날짜 조각을 조립하는 경로가 인간의 해석과 다름을 관찰했습니다. 데이터셋과 코드는 공개되어 있습니다.

## 🎯 주요 포인트

- 1. 현대 BPE 토크나이저는 날짜를 의미 없는 조각으로 분할하여 토큰 수를 증가시키고 시간적 추론에 필요한 구조를 흐리게 한다.
- 2. 날짜 분할 비율이라는 새로운 지표를 도입하여 토크나이저가 다자리 날짜 구성 요소를 얼마나 잘 보존하는지 측정한다.
- 3. DateAugBench라는 6500개의 예제를 포함한 데이터셋을 공개하여 세 가지 시간적 추론 과제를 다룬다.
- 4. 대형 언어 모델이 날짜 구성 요소의 조각을 결합하여 시간적 추론을 수행하는 메커니즘을 발견했다.
- 5. 모델의 크기가 클수록 날짜 조각을 복구하는 추상화가 더 빠르게 이루어진다.


---

*Generated on 2025-09-25 16:25:35*