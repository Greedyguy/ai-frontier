---
keywords:
  - Large Language Model
  - Supervised Fine-Tuning
  - Emergent Misalignment
  - Data Curation
  - Robust Base Models
category: cs.CL
publish_date: 2025-09-25
arxiv_id: 2509.19325
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T08:41:21.419421",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Supervised Fine-Tuning",
    "Emergent Misalignment",
    "Data Curation",
    "Robust Base Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Supervised Fine-Tuning": 0.85,
    "Emergent Misalignment": 0.8,
    "Data Curation": 0.78,
    "Robust Base Models": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, connecting to existing research on language models.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.6,
        "link_intent_score": 0.9
      },
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "unique_technical",
        "rationale": "Key process discussed in the paper, relevant for understanding model adjustments.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Emergent Misalignment",
        "canonical": "Emergent Misalignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a specific issue arising from incorrect data, crucial for safety discussions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Data Curation",
        "canonical": "Data Curation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the importance of data quality, linking to broader data management practices.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Robust Base Models",
        "canonical": "Robust Base Models",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Emphasizes the value of using strong foundational models without fine-tuning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "impact",
      "threshold"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.6,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Emergent Misalignment",
      "resolved_canonical": "Emergent Misalignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Data Curation",
      "resolved_canonical": "Data Curation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Robust Base Models",
      "resolved_canonical": "Robust Base Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19325.pdf)
**Category**: cs.CL
**Published**: 2025-09-25
**ArXiv ID**: [2509.19325](https://arxiv.org/abs/2509.19325)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Unraveling Misinformation Propagation in LLM Reasoning_20250924|Unraveling Misinformation Propagation in LLM Reasoning]] (85.6% similar)
- [[2025-09-23/Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning_20250923|Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning]] (85.0% similar)
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (84.5% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (84.4% similar)
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Data Curation|Data Curation]]
**âš¡ Unique Technical**: [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]], [[keywords/Emergent Misalignment|Emergent Misalignment]], [[keywords/Robust Base Models|Robust Base Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19325v1 Announce Type: new 
Abstract: This paper investigates the impact of incorrect data on the performance and safety of large language models (LLMs), specifically gpt-4o, during supervised fine-tuning (SFT). Although LLMs become increasingly vital across broad domains like finance, coding, law, and health, fine-tuning on incorrect data can lead to "emergent misalignment," producing harmful or deceptive outputs unrelated to the intended task. We evaluate gpt-4o models fine-tuned with varying ratios (10\% to 90\% correct) of both obviously and subtly incorrect data across four domains: coding, finance, health, and legal. Our findings show that even modest amounts of incorrect data (10-25\%) dramatically degrade domain performance and not moral alignment. A clear threshold of at least 50\% correct data is needed for models to consistently recover strong performance, though they rarely match the robustness and safety of the base model, which exhibits near-perfect alignment and zero dangerous completions out-of-the-box. This research emphasizes that the cost of incorrect data is heavy, highlighting the critical need for extremely high-quality data curation or, alternatively, leveraging robust base models without unnecessary fine-tuning for high-stakes applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜ëª»ëœ ë°ì´í„°ê°€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM), íŠ¹íˆ gpt-4oì˜ ì„±ëŠ¥ê³¼ ì•ˆì „ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. ê¸ˆìœµ, ì½”ë”©, ë²•ë¥ , ê±´ê°• ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ LLMì˜ ì¤‘ìš”ì„±ì´ ì¦ê°€í•˜ê³  ìˆì§€ë§Œ, ì˜ëª»ëœ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ì˜ë„ì™€ ë‹¤ë¥¸ í•´ë¡œìš´ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” 10%ì—ì„œ 90%ê¹Œì§€ ë‹¤ì–‘í•œ ë¹„ìœ¨ì˜ ì˜ëª»ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤ ê°€ì§€ ë¶„ì•¼ì—ì„œ gpt-4o ëª¨ë¸ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, 10-25%ì˜ ì˜ëª»ëœ ë°ì´í„°ë§Œìœ¼ë¡œë„ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ë©°, ìµœì†Œ 50% ì´ìƒì˜ ì˜¬ë°”ë¥¸ ë°ì´í„°ê°€ ìˆì–´ì•¼ ì„±ëŠ¥ì„ íšŒë³µí•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ë³¸ ëª¨ë¸ì˜ ê°•ë ¥í•œ ì•ˆì „ì„±ê³¼ ì¼ì¹˜ì„±ì„ ì™„ì „íˆ ì¬í˜„í•˜ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ê³ ìœ„í—˜ ë¶„ì•¼ì—ì„œëŠ” ë¶ˆí•„ìš”í•œ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ê°•ë ¥í•œ ê¸°ë³¸ ëª¨ë¸ì„ í™œìš©í•  í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì˜ëª»ëœ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì˜ë„ëœ ì‘ì—…ê³¼ ë¬´ê´€í•œ ìœ í•´í•˜ê±°ë‚˜ ê¸°ë§Œì ì¸ ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ì—°êµ¬ ê²°ê³¼, 10-25%ì˜ ì˜ëª»ëœ ë°ì´í„°ë§Œìœ¼ë¡œë„ ë„ë©”ì¸ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ë©°, ë„ë•ì  ì •ë ¬ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 3. ëª¨ë¸ì´ ê°•ë ¥í•œ ì„±ëŠ¥ì„ íšŒë³µí•˜ë ¤ë©´ ìµœì†Œ 50% ì´ìƒì˜ ì˜¬ë°”ë¥¸ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ, ê¸°ë³¸ ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ ì•ˆì „ì„±ì„ ì™„ì „íˆ íšŒë³µí•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.
- 4. ì˜ëª»ëœ ë°ì´í„°ì˜ ë¹„ìš©ì´ í¬ë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ë©°, ê³ ìœ„í—˜ ì‘ìš© ë¶„ì•¼ì—ì„œëŠ” ë¶ˆí•„ìš”í•œ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ê²¬ê³ í•œ ê¸°ë³¸ ëª¨ë¸ì„ í™œìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 08:41:21*