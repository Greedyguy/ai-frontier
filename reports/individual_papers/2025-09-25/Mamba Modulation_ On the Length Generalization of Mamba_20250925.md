---
keywords:
  - Mamba Architecture
  - State-Space Models
  - Spectrum Scaling
  - Transition Matrix
  - Context Length Generalization
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19633
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:40:26.293372",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mamba Architecture",
    "State-Space Models",
    "Spectrum Scaling",
    "Transition Matrix",
    "Context Length Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mamba Architecture": 0.78,
    "State-Space Models": 0.82,
    "Spectrum Scaling": 0.75,
    "Transition Matrix": 0.7,
    "Context Length Generalization": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mamba",
        "canonical": "Mamba Architecture",
        "aliases": [
          "Mamba Model"
        ],
        "category": "unique_technical",
        "rationale": "Mamba is a specific architecture discussed in the paper, central to the research findings.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "state-space models",
        "canonical": "State-Space Models",
        "aliases": [
          "SSM"
        ],
        "category": "specific_connectable",
        "rationale": "State-space models are a key alternative to Transformer models, relevant for linking discussions on model architectures.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "spectrum scaling",
        "canonical": "Spectrum Scaling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Spectrum scaling is a novel technique proposed in the paper for improving Mamba's generalization.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "transition matrix",
        "canonical": "Transition Matrix",
        "aliases": [
          "State Transition Matrix"
        ],
        "category": "specific_connectable",
        "rationale": "Transition matrices are crucial for understanding the dynamics of state-space models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "context length extension",
        "canonical": "Context Length Generalization",
        "aliases": [
          "Length Generalization"
        ],
        "category": "unique_technical",
        "rationale": "The paper focuses on improving Mamba's performance with longer contexts, making this a key concept.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mamba",
      "resolved_canonical": "Mamba Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "state-space models",
      "resolved_canonical": "State-Space Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "spectrum scaling",
      "resolved_canonical": "Spectrum Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "transition matrix",
      "resolved_canonical": "Transition Matrix",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "context length extension",
      "resolved_canonical": "Context Length Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Mamba Modulation: On the Length Generalization of Mamba

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19633.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19633](https://arxiv.org/abs/2509.19633)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Achilles' Heel of Mamba_ Essential difficulties of the Mamba architecture demonstrated by synthetic data_20250923|Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data]] (88.6% similar)
- [[2025-09-23/Language Modeling with Learned Meta-Tokens_20250923|Language Modeling with Learned Meta-Tokens]] (82.5% similar)
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (82.4% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (81.8% similar)
- [[2025-09-24/When Long Helps Short_ How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models_20250924|When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/State-Space Models|State-Space Models]], [[keywords/Transition Matrix|Transition Matrix]]
**âš¡ Unique Technical**: [[keywords/Mamba Architecture|Mamba Architecture]], [[keywords/Spectrum Scaling|Spectrum Scaling]], [[keywords/Context Length Generalization|Context Length Generalization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19633v1 Announce Type: cross 
Abstract: The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Transformer ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ ê°œë°œëœ ì„œë¸Œ-ì¿¼ë“œëŸ¬í‹± ìŠ¤ì¼€ì¼ë§ ì•„í‚¤í…ì²˜ ì¤‘ í•˜ë‚˜ì¸ Mambaì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. MambaëŠ” ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, í›ˆë ¨ ì‹œë³´ë‹¤ ê¸´ ë¬¸ë§¥ì— ì ìš©í•  ë•Œ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë©ë‹ˆë‹¤. ì´ëŠ” ìƒíƒœ ì „ì´ í–‰ë ¬ $\mathbf{A}$ì˜ íŒŒë¼ë¯¸í„°í™”ì—ì„œ ê¸°ì¸í•œ ìƒíƒœ ê³µê°„ ë™ì—­í•™ì˜ ë¶„í¬ ì™¸ í–‰ë™ ë•Œë¬¸ì´ë¼ê³  ë¶„ì„í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° ì¸µì˜ $\mathbf{A}$ í–‰ë ¬ì˜ ìŠ¤í™íŠ¸ëŸ¼ì„ ì¡°ì ˆí•˜ì—¬ ê¸´ ë¬¸ë§¥ì—ì„œë„ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë‹¨ìˆœíˆ ì‹œê°„ ë‹¨ê³„ $\Delta_t$ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒë³´ë‹¤ íš¨ê³¼ì ì´ë©°, ìƒíƒœ ê³µê°„ ëª¨ë¸ì˜ ë¬¸ë§¥ ê¸¸ì´ ì¼ë°˜í™”ë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transformer ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë³µì¡ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Mambaì™€ ê°™ì€ ëŒ€ì²´ ì•„í‚¤í…ì²˜ê°€ ê°œë°œë˜ì—ˆìœ¼ë‚˜, MambaëŠ” í›ˆë ¨ ì‹œë³´ë‹¤ ê¸´ ë¬¸ë§¥ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.
- 2. Mambaì˜ ì„±ëŠ¥ ì €í•˜ëŠ” ìƒíƒœ ì „ì´ í–‰ë ¬ $\mathbf{A}$ì˜ ë§¤ê°œë³€ìˆ˜í™”ì—ì„œ ë°œìƒí•˜ëŠ” ìƒíƒœ ê³µê°„ ë™ì—­í•™ì˜ ë¶„í¬ ì™¸ í–‰ë™ì— ê¸°ì¸í•©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” ì…ë ¥ ê¸¸ì´ê°€ ë¬´í•œëŒ€ë¡œ ì ‘ê·¼í•  ë•Œ ìƒíƒœ ìˆ˜ë ´ í–‰ë™ê³¼ ì „ì´ í–‰ë ¬ $\mathbf{A}$ì˜ ìŠ¤í™íŠ¸ëŸ¼ ê°„ì˜ ì—°ê²°ì„ ì œì‹œí•˜ì—¬, ë¬¸ë§¥ ê¸¸ì´ í™•ì¥ ì‹œì˜ ë¯¼ê°ì„±ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
- 4. Mamba ëª¨ë¸ì˜ ìŠ¤í™íŠ¸ëŸ¼ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ê° ì¸µì˜ $\mathbf{A}$ í–‰ë ¬ì˜ ìŠ¤í™íŠ¸ëŸ¼ì„ ì„ íƒì ìœ¼ë¡œ ì¡°ì ˆí•˜ì—¬ ê¸´ ë¬¸ë§¥ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ $\Delta_t$ ì¡°ì ˆë§Œìœ¼ë¡œëŠ” ì„±ëŠ¥ ê°œì„ ì´ ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:40:26*