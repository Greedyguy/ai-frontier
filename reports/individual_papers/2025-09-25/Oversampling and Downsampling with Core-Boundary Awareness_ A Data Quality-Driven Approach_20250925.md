---
keywords:
  - Large Language Model
  - Self-supervised Learning
  - Multimodal Learning
  - Data-efficient Learning
category: cs.LG
publish_date: 2025-09-25
arxiv_id: 2509.19856
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:40:57.847016",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Self-supervised Learning",
    "Multimodal Learning",
    "Data-efficient Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Self-supervised Learning": 0.8,
    "Multimodal Learning": 0.78,
    "Data-efficient Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "The paper discusses implications for efficient model training in computationally expensive domains, specifically mentioning LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-supervised learning",
        "canonical": "Self-supervised Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The method's applicability to self-supervised learning scenarios is highlighted, offering potential for significant advancements.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The approach is extendable to multimodal learning, which is a trending area with high connectivity potential.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "Data-efficient learning",
        "canonical": "Data-efficient Learning",
        "aliases": [
          "Efficient Data Learning"
        ],
        "category": "unique_technical",
        "rationale": "The paper introduces a novel approach focusing on data efficiency, which is crucial for future AI advancements.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-supervised learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Data-efficient learning",
      "resolved_canonical": "Data-efficient Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19856.pdf)
**Category**: cs.LG
**Published**: 2025-09-25
**ArXiv ID**: [2509.19856](https://arxiv.org/abs/2509.19856)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (83.9% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (83.7% similar)
- [[2025-09-23/LASER_ Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy_20250923|LASER: Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy]] (83.5% similar)
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (83.0% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Data-efficient Learning|Data-efficient Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19856v1 Announce Type: new 
Abstract: The effectiveness of machine learning models, particularly in unbalanced classification tasks, is often hindered by the failure to differentiate between critical instances near the decision boundary and redundant samples concentrated in the core of the data distribution. In this paper, we propose a method to systematically identify and differentiate between these two types of data. Through extensive experiments on multiple benchmark datasets, we show that the boundary data oversampling method improves the F1 score by up to 10\% on 96\% of the datasets, whereas our core-aware reduction method compresses datasets up to 90\% while preserving their accuracy, making it 10 times more powerful than the original dataset. Beyond imbalanced classification, our method has broader implications for efficient model training, particularly in computationally expensive domains such as Large Language Model (LLM) training. By prioritizing high-quality, decision-relevant data, our approach can be extended to text, multimodal, and self-supervised learning scenarios, offering a pathway to faster convergence, improved generalization, and significant computational savings. This work paves the way for future research in data-efficient learning, where intelligent sampling replaces brute-force expansion, driving the next generation of AI advancements. Our code is available as a Python package at https://pypi.org/project/adaptive-resampling/ .

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¶ˆê· í˜• ë¶„ë¥˜ ì‘ì—…ì—ì„œ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì €í•´í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê²°ì • ê²½ê³„ ê·¼ì²˜ì˜ ì¤‘ìš”í•œ ë°ì´í„°ì™€ ë°ì´í„° ì¤‘ì‹¬ë¶€ì˜ ì¤‘ë³µ ìƒ˜í”Œì„ êµ¬ë³„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ê²½ê³„ ë°ì´í„° ì˜¤ë²„ìƒ˜í”Œë§ ë°©ë²•ì€ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ F1 ì ìˆ˜ë¥¼ ìµœëŒ€ 10%ê¹Œì§€ í–¥ìƒì‹œì¼°ìœ¼ë©°, ì½”ì–´ ì¸ì‹ ì¶•ì†Œ ë°©ë²•ì€ ë°ì´í„°ì…‹ì„ ìµœëŒ€ 90%ê¹Œì§€ ì••ì¶•í•˜ë©´ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ê³¼ ê°™ì€ ê³„ì‚° ë¹„ìš©ì´ ë†’ì€ ë¶„ì•¼ì—ì„œ íš¨ìœ¨ì ì¸ ëª¨ë¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, í…ìŠ¤íŠ¸, ë©€í‹°ëª¨ë‹¬, ìê°€ ì§€ë„ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œë„ ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì¼ë°˜í™” í–¥ìƒ, ê³„ì‚° ë¹„ìš© ì ˆê°ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë°ì´í„° íš¨ìœ¨ì  í•™ìŠµì˜ ë¯¸ë˜ ì—°êµ¬ë¥¼ ìœ„í•œ ê¸¸ì„ ì—´ë©°, ì§€ëŠ¥í˜• ìƒ˜í”Œë§ì´ ë¬´ì°¨ë³„ í™•ì¥ì„ ëŒ€ì²´í•˜ì—¬ AI ë°œì „ì„ ì´ëŒì–´ê°ˆ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œ íŒ¨í‚¤ì§€ëŠ” [ì—¬ê¸°](https://pypi.org/project/adaptive-resampling/)ì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ê²°ì • ê²½ê³„ ê·¼ì²˜ì˜ ì¤‘ìš”í•œ ë°ì´í„°ì™€ ë°ì´í„° ë¶„í¬ ì¤‘ì‹¬ì˜ ì¤‘ë³µ ìƒ˜í”Œì„ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ê²½ê³„ ë°ì´í„° ì˜¤ë²„ìƒ˜í”Œë§ ë°©ë²•ì€ F1 ì ìˆ˜ë¥¼ ìµœëŒ€ 10% í–¥ìƒì‹œí‚¤ë©°, ì½”ì–´ ì¸ì‹ ê°ì†Œ ë°©ë²•ì€ ë°ì´í„°ì…‹ì„ ìµœëŒ€ 90% ì••ì¶•í•˜ë©´ì„œ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ê³¼ ê°™ì€ ê³„ì‚° ë¹„ìš©ì´ í° ë¶„ì•¼ì—ì„œ íš¨ìœ¨ì ì¸ ëª¨ë¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. ë³¸ ì—°êµ¬ëŠ” í…ìŠ¤íŠ¸, ë©€í‹°ëª¨ë‹¬, ìê°€ ì§€ë„ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ë¹ ë¥¸ ìˆ˜ë ´, ê°œì„ ëœ ì¼ë°˜í™”, ìƒë‹¹í•œ ê³„ì‚° ì ˆê°ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. ë°ì´í„° íš¨ìœ¨ì  í•™ìŠµì— ëŒ€í•œ ë¯¸ë˜ ì—°êµ¬ì˜ ê¸¸ì„ ì—´ë©°, ì§€ëŠ¥í˜• ìƒ˜í”Œë§ì´ ë¬´ì°¨ë³„ í™•ì¥ì„ ëŒ€ì²´í•˜ëŠ” AI ë°œì „ì˜ ì°¨ì„¸ëŒ€ë¥¼ ì´ëŒ ê²ƒì…ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:40:57*