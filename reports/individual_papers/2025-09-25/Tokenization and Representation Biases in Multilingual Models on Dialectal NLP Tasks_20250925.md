---
keywords:
  - Tokenization Parity
  - Information Parity
  - Large Language Model
  - Dialect Classification
  - Extractive Question Answering
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20045
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:56:52.625933",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Tokenization Parity",
    "Information Parity",
    "Large Language Model",
    "Dialect Classification",
    "Extractive Question Answering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Tokenization Parity": 0.78,
    "Information Parity": 0.75,
    "Large Language Model": 0.8,
    "Dialect Classification": 0.77,
    "Extractive Question Answering": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Tokenization Parity",
        "canonical": "Tokenization Parity",
        "aliases": [
          "TP"
        ],
        "category": "unique_technical",
        "rationale": "Tokenization Parity is a novel metric introduced in the paper, crucial for understanding representational biases in multilingual models.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Information Parity",
        "canonical": "Information Parity",
        "aliases": [
          "IP"
        ],
        "category": "unique_technical",
        "rationale": "Information Parity is a new concept that helps in analyzing semantic task performance in multilingual models.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and provide a basis for comparing different model architectures.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dialect Classification",
        "canonical": "Dialect Classification",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Dialect Classification is a specific task that highlights the challenges of linguistic variation in NLP.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Extractive Question Answering",
        "canonical": "Extractive Question Answering",
        "aliases": [
          "Extractive QA"
        ],
        "category": "specific_connectable",
        "rationale": "Extractive Question Answering is a task that relies on syntactic and morphological cues, relevant to the study's findings.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Tokenization Parity",
      "resolved_canonical": "Tokenization Parity",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Information Parity",
      "resolved_canonical": "Information Parity",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dialect Classification",
      "resolved_canonical": "Dialect Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Extractive Question Answering",
      "resolved_canonical": "Extractive Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20045.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20045](https://arxiv.org/abs/2509.20045)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Beyond the Leaderboard_ Understanding Performance Disparities in Large Language Models via Model Diffing_20250924|Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing]] (85.5% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (85.3% similar)
- [[2025-09-22/Benchmarking Debiasing Methods for LLM-based Parameter Estimates_20250922|Benchmarking Debiasing Methods for LLM-based Parameter Estimates]] (85.1% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (85.1% similar)
- [[2025-09-24/Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs_20250924|Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Dialect Classification|Dialect Classification]], [[keywords/Extractive Question Answering|Extractive Question Answering]]
**âš¡ Unique Technical**: [[keywords/Tokenization Parity|Tokenization Parity]], [[keywords/Information Parity|Information Parity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20045v1 Announce Type: cross 
Abstract: Dialectal data are characterized by linguistic variation that appears small to humans but has a significant impact on the performance of models. This dialect gap has been related to various factors (e.g., data size, economic and social factors) whose impact, however, turns out to be inconsistent. In this work, we investigate factors impacting the model performance more directly: we correlate Tokenization Parity (TP) and Information Parity (IP), as measures of representational biases in pre-trained multilingual models, with the downstream performance. We compare state-of-the-art decoder-only LLMs with encoder-based models across three tasks: dialect classification, topic classification, and extractive question answering, controlling for varying scripts (Latin vs. non-Latin) and resource availability (high vs. low). Our analysis reveals that TP is a better predictor of the performance on tasks reliant on syntactic and morphological cues (e.g., extractive QA), while IP better predicts performance in semantic tasks (e.g., topic classification). Complementary analyses, including tokenizer behavior, vocabulary coverage, and qualitative insights, reveal that the language support claims of LLMs often might mask deeper mismatches at the script or token level.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë°©ì–¸ ë°ì´í„°ì˜ ì–¸ì–´ì  ë³€ì´ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” í† í°í™” ê· í˜•(TP)ê³¼ ì •ë³´ ê· í˜•(IP)ì„ ë‹¤êµ­ì–´ ëª¨ë¸ì˜ í‘œí˜„ì  í¸í–¥ ì¸¡ì •ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬, ì´ë“¤ì´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ê³¼ ì–´ë–»ê²Œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ”ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ë””ì½”ë” ì „ìš© LLMê³¼ ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ì„ ë°©ì–¸ ë¶„ë¥˜, ì£¼ì œ ë¶„ë¥˜, ì¶”ì¶œí˜• ì§ˆë¬¸ ì‘ë‹µ ë“± ì„¸ ê°€ì§€ ì‘ì—…ì—ì„œ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, TPëŠ” êµ¬ë¬¸ ë° í˜•íƒœë¡ ì  ë‹¨ì„œì— ì˜ì¡´í•˜ëŠ” ì‘ì—…(ì˜ˆ: ì¶”ì¶œí˜• QA)ì—ì„œ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ë©°, IPëŠ” ì˜ë¯¸ì  ì‘ì—…(ì˜ˆ: ì£¼ì œ ë¶„ë¥˜)ì—ì„œ ì„±ëŠ¥ ì˜ˆì¸¡ì— ë” ìœ ë¦¬í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì„ í†µí•´ LLMì˜ ì–¸ì–´ ì§€ì› ì£¼ì¥ì´ ìŠ¤í¬ë¦½íŠ¸ë‚˜ í† í° ìˆ˜ì¤€ì—ì„œì˜ ë¶ˆì¼ì¹˜ë¥¼ ê°ì¶œ ìˆ˜ ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë°©ì–¸ ë°ì´í„°ëŠ” ì¸ê°„ì—ê²ŒëŠ” ë¯¸ì„¸í•˜ê²Œ ë³´ì´ì§€ë§Œ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
- 2. Tokenization Parity(TP)ì™€ Information Parity(IP)ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì˜ í‘œí˜„ í¸í–¥ì„ ì¸¡ì •í•˜ë©°, ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ê³¼ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ë‹¤.
- 3. TPëŠ” êµ¬ë¬¸ ë° í˜•íƒœë¡ ì  ë‹¨ì„œì— ì˜ì¡´í•˜ëŠ” ì‘ì—…ì˜ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ë‹¤.
- 4. IPëŠ” ì˜ë¯¸ë¡ ì  ì‘ì—…ì˜ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ë‹¤.
- 5. LLMì˜ ì–¸ì–´ ì§€ì› ì£¼ì¥ì€ ìŠ¤í¬ë¦½íŠ¸ë‚˜ í† í° ìˆ˜ì¤€ì—ì„œì˜ ë¶ˆì¼ì¹˜ë¥¼ ê°€ë¦´ ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-25 15:56:52*