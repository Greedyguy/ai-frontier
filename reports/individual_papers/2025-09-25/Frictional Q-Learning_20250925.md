---
keywords:
  - Frictional Q-learning
  - Extrapolation Error
  - Batch-Constrained Reinforcement Learning
  - Continuous Control
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19771
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:48:24.937635",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Frictional Q-learning",
    "Extrapolation Error",
    "Batch-Constrained Reinforcement Learning",
    "Continuous Control"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Frictional Q-learning": 0.78,
    "Extrapolation Error": 0.82,
    "Batch-Constrained Reinforcement Learning": 0.8,
    "Continuous Control": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Frictional Q-learning",
        "canonical": "Frictional Q-learning",
        "aliases": [
          "Frictional QL"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach in reinforcement learning by drawing an analogy with static friction, which is not covered by existing canonicals.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "extrapolation error",
        "canonical": "Extrapolation Error",
        "aliases": [
          "Extrapolation Errors"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in off-policy reinforcement learning that connects to various learning stability discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "batch-constrained reinforcement learning",
        "canonical": "Batch-Constrained Reinforcement Learning",
        "aliases": [
          "Batch-Constrained RL"
        ],
        "category": "specific_connectable",
        "rationale": "A specific technique in reinforcement learning that connects to discussions on policy constraints and learning efficiency.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "continuous control",
        "canonical": "Continuous Control",
        "aliases": [
          "Continuous Action Control"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental area in reinforcement learning that connects to various control and robotics applications.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "static friction",
      "unsupported actions",
      "replay buffer"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Frictional Q-learning",
      "resolved_canonical": "Frictional Q-learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "extrapolation error",
      "resolved_canonical": "Extrapolation Error",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "batch-constrained reinforcement learning",
      "resolved_canonical": "Batch-Constrained Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "continuous control",
      "resolved_canonical": "Continuous Control",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Frictional Q-Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19771.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19771](https://arxiv.org/abs/2509.19771)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (83.8% similar)
- [[2025-09-23/Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality_20250923|Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality]] (83.5% similar)
- [[2025-09-23/Safe Guaranteed Dynamics Exploration with Probabilistic Models_20250923|Safe Guaranteed Dynamics Exploration with Probabilistic Models]] (83.4% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (83.2% similar)
- [[2025-09-23/Robust Reinforcement Learning with Dynamic Distortion Risk Measures_20250923|Robust Reinforcement Learning with Dynamic Distortion Risk Measures]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Continuous Control|Continuous Control]]
**ğŸ”— Specific Connectable**: [[keywords/Extrapolation Error|Extrapolation Error]], [[keywords/Batch-Constrained Reinforcement Learning|Batch-Constrained Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Frictional Q-learning|Frictional Q-learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19771v1 Announce Type: cross 
Abstract: We draw an analogy between static friction in classical mechanics and extrapolation error in off-policy RL, and use it to formulate a constraint that prevents the policy from drifting toward unsupported actions. In this study, we present Frictional Q-learning, a deep reinforcement learning algorithm for continuous control, which extends batch-constrained reinforcement learning. Our algorithm constrains the agent's action space to encourage behavior similar to that in the replay buffer, while maintaining a distance from the manifold of the orthonormal action space. The constraint preserves the simplicity of batch-constrained, and provides an intuitive physical interpretation of extrapolation error. Empirically, we further demonstrate that our algorithm is robustly trained and achieves competitive performance across standard continuous control benchmarks.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ì—ì„œëŠ” ê³ ì „ ì—­í•™ì˜ ì •ì§€ ë§ˆì°°ê³¼ ì˜¤í”„-ì •ì±… ê°•í™” í•™ìŠµì˜ ì™¸ì‚½ ì˜¤ë¥˜ë¥¼ ìœ ì‚¬í•˜ê²Œ ë³´ê³ , ì •ì±…ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” í–‰ë™ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ì œì•½ ì¡°ê±´ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Frictional Q-learningì´ë¼ëŠ” ì—°ì† ì œì–´ë¥¼ ìœ„í•œ ì‹¬ì¸µ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê³µê°„ì„ ì œì•½í•˜ì—¬ ë¦¬í”Œë ˆì´ ë²„í¼ì™€ ìœ ì‚¬í•œ í–‰ë™ì„ ìœ ë„í•˜ë©´ì„œë„ ì§êµ í–‰ë™ ê³µê°„ì˜ ë‹¤ì–‘ì²´ì™€ ê±°ë¦¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì œì•½ì€ ë°°ì¹˜ ì œì•½ì˜ ë‹¨ìˆœì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì™¸ì‚½ ì˜¤ë¥˜ì— ëŒ€í•œ ì§ê´€ì ì¸ ë¬¼ë¦¬ì  í•´ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ê²¬ê³ í•˜ê²Œ í›ˆë ¨ë˜ë©° í‘œì¤€ ì—°ì† ì œì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì •ì  ë§ˆì°°ê³¼ ì˜¤í”„í´ë¦¬ì‹œ ê°•í™”í•™ìŠµì˜ ì™¸ì‚½ ì˜¤ë¥˜ ê°„ì˜ ìœ ì‚¬ì„±ì„ í†µí•´ ì •ì±…ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” í–‰ë™ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ì œì•½ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 2. Frictional Q-learningì€ ì—°ì† ì œì–´ë¥¼ ìœ„í•œ ì‹¬ì¸µ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°°ì¹˜ ì œì•½ ê°•í™”í•™ìŠµì„ í™•ì¥í•©ë‹ˆë‹¤.
- 3. ì•Œê³ ë¦¬ì¦˜ì€ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê³µê°„ì„ ì œì•½í•˜ì—¬ ë¦¬í”Œë ˆì´ ë²„í¼ì™€ ìœ ì‚¬í•œ í–‰ë™ì„ ìœ ë„í•˜ë©´ì„œ ì§êµ í–‰ë™ ê³µê°„ì˜ ë‹¤ì–‘ì²´ì™€ì˜ ê±°ë¦¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- 4. ì œì•½ì€ ë°°ì¹˜ ì œì•½ì˜ ë‹¨ìˆœì„±ì„ ìœ ì§€í•˜ë©° ì™¸ì‚½ ì˜¤ë¥˜ì— ëŒ€í•œ ì§ê´€ì ì¸ ë¬¼ë¦¬ì  í•´ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ì ìœ¼ë¡œ, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ê²¬ê³ í•˜ê²Œ í›ˆë ¨ë˜ë©° í‘œì¤€ ì—°ì† ì œì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:48:24*