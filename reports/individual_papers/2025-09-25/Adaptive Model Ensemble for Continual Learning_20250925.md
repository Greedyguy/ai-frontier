---
keywords:
  - Model Ensemble
  - Continual Learning
  - Catastrophic Forgetting
  - Meta-learning
  - Meta-weight-ensembler
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19819
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:07:37.342319",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Model Ensemble",
    "Continual Learning",
    "Catastrophic Forgetting",
    "Meta-learning",
    "Meta-weight-ensembler"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Model Ensemble": 0.81,
    "Continual Learning": 0.88,
    "Catastrophic Forgetting": 0.84,
    "Meta-learning": 0.8,
    "Meta-weight-ensembler": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Model Ensemble",
        "canonical": "Model Ensemble",
        "aliases": [
          "Ensemble Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Model ensemble is a key technique in continual learning, facilitating knowledge fusion and mitigating catastrophic forgetting.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.81
      },
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "Lifelong Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Continual learning is a central theme of the paper, addressing the challenge of learning across tasks without forgetting.",
        "novelty_score": 0.48,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Catastrophic Forgetting",
        "canonical": "Catastrophic Forgetting",
        "aliases": [
          "Forgetting in Neural Networks"
        ],
        "category": "specific_connectable",
        "rationale": "Catastrophic forgetting is a critical issue in continual learning that the proposed method aims to alleviate.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.77,
        "link_intent_score": 0.84
      },
      {
        "surface": "Meta-learning",
        "canonical": "Meta-learning",
        "aliases": [
          "Learning to Learn"
        ],
        "category": "specific_connectable",
        "rationale": "Meta-learning is used to train the mixing coefficient generator, making it integral to the proposed solution.",
        "novelty_score": 0.52,
        "connectivity_score": 0.79,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Meta-weight-ensembler",
        "canonical": "Meta-weight-ensembler",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is the novel method proposed in the paper, specifically designed to address knowledge conflicts in model ensembles.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "knowledge fusion",
      "mixing coefficient"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Model Ensemble",
      "resolved_canonical": "Model Ensemble",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Catastrophic Forgetting",
      "resolved_canonical": "Catastrophic Forgetting",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.77,
        "link_intent": 0.84
      }
    },
    {
      "candidate_surface": "Meta-learning",
      "resolved_canonical": "Meta-learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.79,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Meta-weight-ensembler",
      "resolved_canonical": "Meta-weight-ensembler",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Adaptive Model Ensemble for Continual Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19819.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19819](https://arxiv.org/abs/2509.19819)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/AIMMerging_ Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning_20250923|AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning]] (82.7% similar)
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM: Hierarchical Adapter Merging for Scalable Continual Learning]] (82.4% similar)
- [[2025-09-23/Learning to Learn with Contrastive Meta-Objective_20250923|Learning to Learn with Contrastive Meta-Objective]] (80.6% similar)
- [[2025-09-24/Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity_20250924|Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity]] (80.6% similar)
- [[2025-09-24/Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning_20250924|Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning]] (80.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Model Ensemble|Model Ensemble]], [[keywords/Continual Learning|Continual Learning]], [[keywords/Catastrophic Forgetting|Catastrophic Forgetting]], [[keywords/Meta-learning|Meta-learning]]
**âš¡ Unique Technical**: [[keywords/Meta-weight-ensembler|Meta-weight-ensembler]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19819v1 Announce Type: new 
Abstract: Model ensemble is an effective strategy in continual learning, which alleviates catastrophic forgetting by interpolating model parameters, achieving knowledge fusion learned from different tasks. However, existing model ensemble methods usually encounter the knowledge conflict issue at task and layer levels, causing compromised learning performance in both old and new tasks. To solve this issue, we propose meta-weight-ensembler that adaptively fuses knowledge of different tasks for continual learning. Concretely, we employ a mixing coefficient generator trained via meta-learning to generate appropriate mixing coefficients for model ensemble to address the task-level knowledge conflict. The mixing coefficient is individually generated for each layer to address the layer-level knowledge conflict. In this way, we learn the prior knowledge about adaptively accumulating knowledge of different tasks in a fused model, achieving efficient learning in both old and new tasks. Meta-weight-ensembler can be flexibly combined with existing continual learning methods to boost their ability of alleviating catastrophic forgetting. Experiments on multiple continual learning datasets show that meta-weight-ensembler effectively alleviates catastrophic forgetting and achieves state-of-the-art performance.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì—°ì† í•™ìŠµì—ì„œ ë°œìƒí•˜ëŠ” ì§€ì‹ ì¶©ëŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë©”íƒ€-ì›¨ì´íŠ¸-ì—”ì…ˆë¸”ëŸ¬(meta-weight-ensembler)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ ì•™ìƒë¸” ë°©ë²•ì€ ê³¼ê±°ì™€ ìƒˆë¡œìš´ ì‘ì—… ê°„ì˜ í•™ìŠµ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•˜ëŠ” ì§€ì‹ ì¶©ëŒ ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë©”íƒ€-ì›¨ì´íŠ¸-ì—”ì…ˆë¸”ëŸ¬ëŠ” ë©”íƒ€ëŸ¬ë‹ì„ í†µí•´ í›ˆë ¨ëœ í˜¼í•© ê³„ìˆ˜ ìƒì„±ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ìˆ˜ì¤€ê³¼ ì¸µ ìˆ˜ì¤€ì—ì„œ ì ì ˆí•œ í˜¼í•© ê³„ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‘ì—…ì˜ ì§€ì‹ì„ ì ì‘ì ìœ¼ë¡œ ìœµí•©í•˜ì—¬ ê³¼ê±°ì™€ ìƒˆë¡œìš´ ì‘ì—… ëª¨ë‘ì—ì„œ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ ì—°ì† í•™ìŠµ ë°©ë²•ê³¼ ìœ ì—°í•˜ê²Œ ê²°í•©í•  ìˆ˜ ìˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ëŠ” ì´ ë°©ë²•ì´ ë§ê° í˜„ìƒì„ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•˜ê³  ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëª¨ë¸ ì•™ìƒë¸”ì€ ì—°ì† í•™ìŠµì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë³´ê°„í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì‘ì—…ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ìœµí•©í•¨ìœ¼ë¡œì¨ ë§ê°ì„ ì™„í™”í•˜ëŠ” íš¨ê³¼ì ì¸ ì „ëµì…ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ëª¨ë¸ ì•™ìƒë¸” ë°©ë²•ì€ ì‘ì—… ë° ë ˆì´ì–´ ìˆ˜ì¤€ì—ì„œ ì§€ì‹ ì¶©ëŒ ë¬¸ì œë¥¼ ê²ªì–´ í•™ìŠµ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.
- 3. ë©”íƒ€-ì›¨ì´íŠ¸-ì•™ìƒë¸”ëŸ¬ëŠ” ë©”íƒ€ ëŸ¬ë‹ì„ í†µí•´ í•™ìŠµëœ í˜¼í•© ê³„ìˆ˜ ìƒì„±ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ìˆ˜ì¤€ ë° ë ˆì´ì–´ ìˆ˜ì¤€ì˜ ì§€ì‹ ì¶©ëŒì„ í•´ê²°í•©ë‹ˆë‹¤.
- 4. ë©”íƒ€-ì›¨ì´íŠ¸-ì•™ìƒë¸”ëŸ¬ëŠ” ê¸°ì¡´ ì—°ì† í•™ìŠµ ë°©ë²•ê³¼ ìœ ì—°í•˜ê²Œ ê²°í•©í•˜ì—¬ ë§ê°ì„ ì™„í™”í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. ì—¬ëŸ¬ ì—°ì† í•™ìŠµ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ë©”íƒ€-ì›¨ì´íŠ¸-ì•™ìƒë¸”ëŸ¬ëŠ” ë§ê°ì„ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•˜ê³  ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-26 09:07:37*