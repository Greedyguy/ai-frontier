---
keywords:
  - Deep Learning
  - Learning-to-Rank
  - Pretrained Models
  - Label Scarcity
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2308.00177
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:14:08.521368",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Learning-to-Rank",
    "Pretrained Models",
    "Label Scarcity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.85,
    "Learning-to-Rank": 0.8,
    "Pretrained Models": 0.79,
    "Label Scarcity": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Learning",
        "canonical": "Deep Learning",
        "aliases": [
          "DL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is central to the paper's findings and connects to existing knowledge on neural networks.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Learning-to-Rank",
        "canonical": "Learning-to-Rank",
        "aliases": [
          "LTR"
        ],
        "category": "unique_technical",
        "rationale": "Learning-to-Rank is a specific application area where the paper demonstrates significant findings.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Pretrained Models",
        "canonical": "Pretrained Models",
        "aliases": [
          "Pretraining"
        ],
        "category": "specific_connectable",
        "rationale": "Pretrained models are crucial for the paper's methodology and connect to broader trends in model training.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Label Scarcity",
        "canonical": "Label Scarcity",
        "aliases": [
          "Limited Labels"
        ],
        "category": "unique_technical",
        "rationale": "Label scarcity is a key challenge addressed in the paper, relevant to data availability issues.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "tabular data",
      "outlier data",
      "ranking metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Learning-to-Rank",
      "resolved_canonical": "Learning-to-Rank",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Pretrained Models",
      "resolved_canonical": "Pretrained Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Label Scarcity",
      "resolved_canonical": "Label Scarcity",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2308.00177.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2308.00177](https://arxiv.org/abs/2308.00177)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (81.0% similar)
- [[2025-09-24/Is Pre-training Truly Better Than Meta-Learning?_20250924|Is Pre-training Truly Better Than Meta-Learning?]] (80.2% similar)
- [[2025-09-23/Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval_20250923|Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval]] (80.0% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (79.3% similar)
- [[2025-09-22/IGD_ Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation_20250922|IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation]] (79.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Pretrained Models|Pretrained Models]]
**âš¡ Unique Technical**: [[keywords/Learning-to-Rank|Learning-to-Rank]], [[keywords/Label Scarcity|Label Scarcity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2308.00177v5 Announce Type: replace-cross 
Abstract: On tabular data, a significant body of literature has shown that current deep learning (DL) models perform at best similarly to Gradient Boosted Decision Trees (GBDTs), while significantly underperforming them on outlier data. However, these works often study idealized problem settings which may fail to capture complexities of real-world scenarios. We identify a natural tabular data setting where DL models can outperform GBDTs: tabular Learning-to-Rank (LTR) under label scarcity. Tabular LTR applications, including search and recommendation, often have an abundance of unlabeled data, and scarce labeled data. We show that DL rankers can utilize unsupervised pretraining to exploit this unlabeled data. In extensive experiments over both public and proprietary datasets, we show that pretrained DL rankers consistently outperform GBDT rankers on ranking metrics -- sometimes by as much as 38% -- both overall and on outliers.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë”¥ëŸ¬ë‹(DL) ëª¨ë¸ì´ ë ˆì´ë¸”ì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œ í…Œì´ë¸”í˜• í•™ìŠµ-ìˆœìœ„í™”(LTR) ë¬¸ì œì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ê²°ì • íŠ¸ë¦¬(GBDT)ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ê²€ìƒ‰ ë° ì¶”ì²œê³¼ ê°™ì€ LTR ì‘ìš© ë¶„ì•¼ì—ì„œëŠ” ë ˆì´ë¸”ì´ ë¶€ì¡±í•œ ë°˜ë©´, ë¹„ë ˆì´ë¸” ë°ì´í„°ëŠ” í’ë¶€í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” DL ìˆœìœ„ìê°€ ë¹„ë ˆì´ë¸” ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ ë¹„ì§€ë„ ì‚¬ì „ í•™ìŠµì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê³µê³µ ë° ë…ì  ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ, ì‚¬ì „ í•™ìŠµëœ DL ìˆœìœ„ìê°€ GBDT ìˆœìœ„ìë³´ë‹¤ ìµœëŒ€ 38%ê¹Œì§€ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ë ˆì´ë¸”ì´ ë¶€ì¡±í•œ ìƒí™©ì˜ í…Œì´ë¸”í˜• í•™ìŠµ-ìˆœìœ„(LTR) ë¬¸ì œì—ì„œ GBDTë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.
- 2. ê²€ìƒ‰ ë° ì¶”ì²œê³¼ ê°™ì€ í…Œì´ë¸”í˜• LTR ì‘ìš©ì—ì„œëŠ” ë ˆì´ë¸”ì´ ë¶€ì¡±í•˜ê³  ë¹„ì§€ë„ í•™ìŠµì„ í†µí•´ ë¹„ë ˆì´ë¸” ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.
- 3. ì‚¬ì „ í•™ìŠµëœ ë”¥ëŸ¬ë‹ ìˆœìœ„ ëª¨ë¸ì€ ê³µê³µ ë° ë…ì  ë°ì´í„°ì…‹ì—ì„œ GBDT ìˆœìœ„ ëª¨ë¸ë³´ë‹¤ ìµœëŒ€ 38%ê¹Œì§€ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 4. ë”¥ëŸ¬ë‹ ìˆœìœ„ ëª¨ë¸ì€ ì „ì²´ì ìœ¼ë¡œë‚˜ ì´ìƒì¹˜ ë°ì´í„°ì—ì„œë„ GBDT ìˆœìœ„ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.


---

*Generated on 2025-09-25 16:14:08*