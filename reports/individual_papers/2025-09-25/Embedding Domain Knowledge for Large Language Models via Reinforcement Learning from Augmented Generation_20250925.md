---
keywords:
  - Large Language Model
  - Reinforcement Learning from Augmented Generation
  - Continual Pre-Training
  - Supervised Fine-Tuning
  - Domain Knowledge Embedding
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20162
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:00:20.964576",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning from Augmented Generation",
    "Continual Pre-Training",
    "Supervised Fine-Tuning",
    "Domain Knowledge Embedding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning from Augmented Generation": 0.8,
    "Continual Pre-Training": 0.78,
    "Supervised Fine-Tuning": 0.77,
    "Domain Knowledge Embedding": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This is a foundational concept in the paper, linking to a broad range of NLP and AI research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning from Augmented Generation",
        "canonical": "Reinforcement Learning from Augmented Generation",
        "aliases": [
          "RLAG"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel method proposed in the paper, central to its contributions.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Continual Pre-Training",
        "canonical": "Continual Pre-Training",
        "aliases": [
          "CPT"
        ],
        "category": "specific_connectable",
        "rationale": "A known technique in the field, relevant for linking to ongoing research in model adaptation.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "specific_connectable",
        "rationale": "A common method in machine learning, useful for connecting to discussions on model training techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "Domain Knowledge Embedding",
        "canonical": "Domain Knowledge Embedding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key concept for the paper's approach to improving LLMs with specific knowledge.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Augmented Generation",
      "resolved_canonical": "Reinforcement Learning from Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Continual Pre-Training",
      "resolved_canonical": "Continual Pre-Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Domain Knowledge Embedding",
      "resolved_canonical": "Domain Knowledge Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20162.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20162](https://arxiv.org/abs/2509.20162)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (89.8% similar)
- [[2025-09-23/GRIL_ Knowledge Graph Retrieval-Integrated Learning with Large Language Models_20250923|GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models]] (87.9% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (87.7% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (87.6% similar)
- [[2025-09-24/Brittleness and Promise_ Knowledge Graph Based Reward Modeling for Diagnostic Reasoning_20250924|Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning]] (87.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Continual Pre-Training|Continual Pre-Training]], [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning from Augmented Generation|Reinforcement Learning from Augmented Generation]], [[keywords/Domain Knowledge Embedding|Domain Knowledge Embedding]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.20162v1 Announce Type: cross 
Abstract: Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ íŠ¹ì • ë¶„ì•¼ì˜ ì‘ì—…ì—ì„œ ì„±ëŠ¥ì´ ì œí•œì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°•í™” í•™ìŠµ ê¸°ë°˜ì˜ ìƒˆë¡œìš´ ë°©ë²•ì¸ RLAG(Reinforcement Learning from Augmented Generation)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì§€ì†ì  ì‚¬ì „ í›ˆë ¨(CPT)ê³¼ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT) ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì, RLAGëŠ” ìƒì„±ëœ ì¶œë ¥ë¬¼ ì¤‘ ë†’ì€ ë¡œê·¸ í™•ë¥ ì„ ê°€ì§„ ê²ƒì„ ì„ íƒí•˜ê³ , ì„¸ ê°€ì§€ ë§ì¶¤í˜• ë³´ìƒ ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì„ ìµœì í™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¤‘ìš”í•œ ë¶„ì•¼ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ë‚´ì¬í™”í•˜ê³ , ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì— í•„ìš”í•œ ì¼ê´€ëœ ì§€ì‹ êµ¬ì¡°ë¥¼ ê°œë°œí•©ë‹ˆë‹¤. ì˜ë£Œ, ë²•ë¥ , ì²œë¬¸í•™ ë° ìµœì‹  ì‚¬ê±´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, RLAGê°€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì—°êµ¬ì˜ ì½”ë“œì™€ ë°ì´í„°ëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ í›ˆë ¨ ë°ì´í„°ì˜ ë¶ˆê· í˜•ê³¼ ì •ì  íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì „ë¬¸ ë¶„ì•¼ì˜ ì‘ì—…ì—ì„œ ì œí•œëœ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 2. ê¸°ì¡´ì˜ ì§€ì†ì  ì‚¬ì „ í›ˆë ¨(CPT)ê³¼ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT) ì ‘ê·¼ë²•ì€ ì¤‘ìš”í•œ ì§€ì‹ í¬ì¸íŠ¸ë¥¼ ìš°ì„ ì‹œí•˜ê±°ë‚˜ ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì„ ìœ„í•œ ì¼ê´€ëœ ì§€ì‹ êµ¬ì¡°ë¥¼ ê°œë°œí•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤.
- 3. ì œì•ˆëœ ê°•í™” í•™ìŠµ ê¸°ë°˜ì˜ ì¦ê°• ìƒì„±(RLAG) ë°©ë²•ì€ ìƒ˜í”Œë§ ìƒì„±ê³¼ ë³´ìƒ ìµœì í™”ë¥¼ ë°˜ë³µí•˜ì—¬ ì¤‘ìš”í•œ ë„ë©”ì¸ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ë‚´ì¬í™”í•œë‹¤.
- 4. RLAGëŠ” ë†’ì€ ë¡œê·¸ í™•ë¥ ì„ ê°€ì§„ ìƒì„± ì¶œë ¥ì„ ì„ íƒí•˜ê³ , ì„¸ ê°€ì§€ ë§ì¶¤í˜• ë³´ìƒ ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ ìµœì í™”ë¥¼ ìœ ë„í•œë‹¤.
- 5. ì˜ë£Œ, ë²•ë¥ , ì²œë¬¸í•™ ë° ì‹œì‚¬ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ì˜ ê¸°ì¤€ ì ‘ê·¼ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-25 16:00:20*