---
keywords:
  - Large Language Model
  - Reinforcement Learning from Augmented Generation
  - Continual Pre-Training
  - Supervised Fine-Tuning
  - Domain Knowledge Embedding
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.20162
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:00:20.964576",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning from Augmented Generation",
    "Continual Pre-Training",
    "Supervised Fine-Tuning",
    "Domain Knowledge Embedding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning from Augmented Generation": 0.8,
    "Continual Pre-Training": 0.78,
    "Supervised Fine-Tuning": 0.77,
    "Domain Knowledge Embedding": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This is a foundational concept in the paper, linking to a broad range of NLP and AI research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning from Augmented Generation",
        "canonical": "Reinforcement Learning from Augmented Generation",
        "aliases": [
          "RLAG"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel method proposed in the paper, central to its contributions.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Continual Pre-Training",
        "canonical": "Continual Pre-Training",
        "aliases": [
          "CPT"
        ],
        "category": "specific_connectable",
        "rationale": "A known technique in the field, relevant for linking to ongoing research in model adaptation.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "specific_connectable",
        "rationale": "A common method in machine learning, useful for connecting to discussions on model training techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "Domain Knowledge Embedding",
        "canonical": "Domain Knowledge Embedding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key concept for the paper's approach to improving LLMs with specific knowledge.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Augmented Generation",
      "resolved_canonical": "Reinforcement Learning from Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Continual Pre-Training",
      "resolved_canonical": "Continual Pre-Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Domain Knowledge Embedding",
      "resolved_canonical": "Domain Knowledge Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.20162.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.20162](https://arxiv.org/abs/2509.20162)

## 🔗 유사한 논문
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (89.8% similar)
- [[2025-09-23/GRIL_ Knowledge Graph Retrieval-Integrated Learning with Large Language Models_20250923|GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models]] (87.9% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (87.7% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (87.6% similar)
- [[2025-09-24/Brittleness and Promise_ Knowledge Graph Based Reward Modeling for Diagnostic Reasoning_20250924|Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning]] (87.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Continual Pre-Training|Continual Pre-Training]], [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]]
**⚡ Unique Technical**: [[keywords/Reinforcement Learning from Augmented Generation|Reinforcement Learning from Augmented Generation]], [[keywords/Domain Knowledge Embedding|Domain Knowledge Embedding]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.20162v1 Announce Type: cross 
Abstract: Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.

## 📝 요약

이 논문은 대형 언어 모델(LLM)이 특정 분야의 작업에서 성능이 제한적인 문제를 해결하기 위해 강화 학습 기반의 새로운 방법인 RLAG(Reinforcement Learning from Augmented Generation)를 제안합니다. 기존의 지속적 사전 훈련(CPT)과 지도 학습 미세 조정(SFT) 방법의 한계를 극복하고자, RLAG는 생성된 출력물 중 높은 로그 확률을 가진 것을 선택하고, 세 가지 맞춤형 보상 지표를 통해 모델을 최적화합니다. 이를 통해 중요한 분야 지식을 효과적으로 내재화하고, 복잡한 추론 작업에 필요한 일관된 지식 구조를 개발합니다. 의료, 법률, 천문학 및 최신 사건 데이터셋에 대한 실험 결과, RLAG가 기존 방법보다 뛰어난 성능을 보였습니다. 연구의 코드와 데이터는 공개되어 있습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 훈련 데이터의 불균형과 정적 특성으로 인해 전문 분야의 작업에서 제한된 성능을 보인다.
- 2. 기존의 지속적 사전 훈련(CPT)과 지도 학습 미세 조정(SFT) 접근법은 중요한 지식 포인트를 우선시하거나 복잡한 추론 작업을 위한 일관된 지식 구조를 개발하는 데 한계가 있다.
- 3. 제안된 강화 학습 기반의 증강 생성(RLAG) 방법은 샘플링 생성과 보상 최적화를 반복하여 중요한 도메인 지식을 효과적으로 내재화한다.
- 4. RLAG는 높은 로그 확률을 가진 생성 출력을 선택하고, 세 가지 맞춤형 보상 지표를 통해 모델 최적화를 유도한다.
- 5. 의료, 법률, 천문학 및 시사 데이터셋에서의 실험 결과, 제안된 방법이 기존의 기준 접근법보다 성능이 우수함을 보여준다.


---

*Generated on 2025-09-25 16:00:20*