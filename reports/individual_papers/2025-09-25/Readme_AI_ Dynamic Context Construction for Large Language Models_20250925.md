---
keywords:
  - Large Language Model
  - Readme_AI Model Context Protocol
  - Dynamic Context Construction
  - Metadata for Large Language Models
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19322
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:23:13.207276",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Readme_AI Model Context Protocol",
    "Dynamic Context Construction",
    "Metadata for Large Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Readme_AI Model Context Protocol": 0.8,
    "Dynamic Context Construction": 0.78,
    "Metadata for Large Language Models": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on improving LLM responses through dynamic context construction.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Readme_AI Model Context Protocol",
        "canonical": "Readme_AI Model Context Protocol",
        "aliases": [
          "Readme_AI",
          "MCP"
        ],
        "category": "unique_technical",
        "rationale": "A novel protocol introduced in the paper, essential for understanding the specific approach to context construction.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "dynamic context construction",
        "canonical": "Dynamic Context Construction",
        "aliases": [
          "context building",
          "context generation"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for enhancing LLM responses, linking to broader themes of context-aware AI.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "metadata for LLMs",
        "canonical": "Metadata for Large Language Models",
        "aliases": [
          "LLM metadata",
          "context metadata"
        ],
        "category": "specific_connectable",
        "rationale": "Crucial for understanding how LLMs are grounded in specific data sources.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Readme_AI Model Context Protocol",
      "resolved_canonical": "Readme_AI Model Context Protocol",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "dynamic context construction",
      "resolved_canonical": "Dynamic Context Construction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "metadata for LLMs",
      "resolved_canonical": "Metadata for Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Readme_AI: Dynamic Context Construction for Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19322.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19322](https://arxiv.org/abs/2509.19322)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (85.0% similar)
- [[2025-09-23/Tool Preferences in Agentic LLMs are Unreliable_20250923|Tool Preferences in Agentic LLMs are Unreliable]] (84.6% similar)
- [[2025-09-23/AI Assistants to Enhance and Exploit the PETSc Knowledge Base_20250923|AI Assistants to Enhance and Exploit the PETSc Knowledge Base]] (84.5% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (83.8% similar)
- [[2025-09-23/Adaptive Distraction_ Probing LLM Contextual Robustness with Automated Tree Search_20250923|Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search]] (83.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Dynamic Context Construction|Dynamic Context Construction]], [[keywords/Metadata for Large Language Models|Metadata for Large Language Models]]
**âš¡ Unique Technical**: [[keywords/Readme_AI Model Context Protocol|Readme_AI Model Context Protocol]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19322v1 Announce Type: cross 
Abstract: Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its responses. In this paper, we present a specification that can be used to dynamically build context for data sources. The data source owner creates the file containing metadata for LLMs to use when reasoning about dataset-related queries. To demonstrate our proposed specification, we created a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the metadata from the data source and uses it to dynamically build context. Some features that make this specification dynamic are the extensible types that represent crawling web-pages, fetching data from data repositories, downloading and parsing publications, and general text. The context is formatted and grouped using user-specified tags that provide clear contextual information for the LLM to reason about the content. We demonstrate the capabilities of this early prototype by asking the LLM about the NIST-developed Hedgehog library, for which common LLMs often provides inaccurate and irrelevant responses containing hallucinations. With Readme_AI, the LLM receives enough context that it is now able to reason about the library and its use, and even generate code interpolated from examples that were included in the Readme_AI file provided by Hedgehog's developer. Our primary contribution is a extensible protocol for dynamically grounding LLMs in specialized, owner-provided data, enhancing responses from LLMs and reducing hallucinations. The source code for the Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•´ ë¶€ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë°ì´í„° ì†ŒìŠ¤ì˜ ë§¥ë½ì„ ë™ì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ëª…ì„¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë°ì´í„° ì†ŒìŠ¤ ì†Œìœ ìê°€ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ìƒì„±í•˜ì—¬ LLMì´ ë°ì´í„°ì…‹ ê´€ë ¨ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ Readme_AI ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡œí† ì½œ(MCP) ì„œë²„ë¥¼ ê°œë°œí•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•´ ë§¥ë½ì„ ë™ì ìœ¼ë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ ëª…ì„¸ëŠ” ì›¹ í˜ì´ì§€ í¬ë¡¤ë§, ë°ì´í„° ì €ì¥ì†Œì—ì„œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°, ì¶œíŒë¬¼ ë‹¤ìš´ë¡œë“œ ë° êµ¬ë¬¸ ë¶„ì„ ë“±ì„ ì§€ì›í•˜ë©°, ì‚¬ìš©ì ì§€ì • íƒœê·¸ë¥¼ í†µí•´ LLMì´ ë‚´ìš©ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. NISTì˜ Hedgehog ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ LLMì˜ ë¶€ì •í™•í•œ ì‘ë‹µ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Readme_AIë¥¼ í™œìš©í•˜ì—¬ ë§¥ë½ì„ ì œê³µí•¨ìœ¼ë¡œì¨ LLMì´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì½”ë“œ ìƒì„±ê¹Œì§€ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” LLMì˜ ì‘ë‹µì„ ê°œì„ í•˜ê³  ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” í™•ì¥ ê°€ëŠ¥í•œ í”„ë¡œí† ì½œì„ ì œì‹œí•œ ê²ƒì…ë‹ˆë‹¤. Readme_AI ë„êµ¬ì˜ ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì— ê²Œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ë§¥ë½ì„ ì œê³µí•˜ë©´ ì‘ë‹µì˜ ìœ ìš©ì„±ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.
- 2. ë°ì´í„° ì†ŒìŠ¤ ì†Œìœ ìê°€ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ìƒì„±í•˜ì—¬ LLMì´ ë°ì´í„°ì…‹ ê´€ë ¨ ì§ˆì˜ì— ëŒ€í•´ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.
- 3. Readme_AI ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡œí† ì½œ(MCP) ì„œë²„ëŠ” ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë™ì ìœ¼ë¡œ ë§¥ë½ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ í”„ë¡œí† ì½œì€ ì›¹ í˜ì´ì§€ í¬ë¡¤ë§, ë°ì´í„° ì €ì¥ì†Œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°, ì¶œíŒë¬¼ ë‹¤ìš´ë¡œë“œ ë° êµ¬ë¬¸ ë¶„ì„ ë“±ì„ ì§€ì›í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ íƒ€ì…ì„ í¬í•¨í•©ë‹ˆë‹¤.
- 5. Readme_AI ë„êµ¬ëŠ” LLMì˜ ì‘ë‹µì„ ê°œì„ í•˜ê³  í™˜ê°ì„ ì¤„ì´ê¸° ìœ„í•´ ì „ë¬¸í™”ëœ ì†Œìœ ì ì œê³µ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ë™ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” í”„ë¡œí† ì½œì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:23:13*