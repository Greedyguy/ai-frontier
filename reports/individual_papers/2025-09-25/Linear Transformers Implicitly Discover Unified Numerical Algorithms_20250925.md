---
keywords:
  - Transformer
  - NystrÃ¶m Extrapolation
  - In-Context Learning
  - Matrix Completion
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19702
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:45:36.695458",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "NystrÃ¶m Extrapolation",
    "In-Context Learning",
    "Matrix Completion"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "NystrÃ¶m Extrapolation": 0.7,
    "In-Context Learning": 0.82,
    "Matrix Completion": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "linear attention transformer",
        "canonical": "Transformer",
        "aliases": [
          "linear transformer",
          "attention transformer"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader concept of Transformers, a key technology in machine learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "NystrÃ¶m extrapolation",
        "canonical": "NystrÃ¶m Extrapolation",
        "aliases": [
          "NystrÃ¶m method"
        ],
        "category": "unique_technical",
        "rationale": "A specialized technique in numerical analysis, relevant for linking to computational methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "in-context learning",
        "canonical": "In-Context Learning",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Represents a modern approach in machine learning that is gaining traction.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "masked-block matrix completion",
        "canonical": "Matrix Completion",
        "aliases": [
          "block matrix completion"
        ],
        "category": "specific_connectable",
        "rationale": "A specific task that can be linked to broader matrix operations and algorithms.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "full visibility",
      "distributed computation",
      "rank-limited updates"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "linear attention transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "NystrÃ¶m extrapolation",
      "resolved_canonical": "NystrÃ¶m Extrapolation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "in-context learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "masked-block matrix completion",
      "resolved_canonical": "Matrix Completion",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Linear Transformers Implicitly Discover Unified Numerical Algorithms

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19702.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19702](https://arxiv.org/abs/2509.19702)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Scaling Efficient LLMs_20250923|Scaling Efficient LLMs]] (84.3% similar)
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (83.2% similar)
- [[2025-09-24/Spectraformer_ A Unified Random Feature Framework for Transformer_20250924|Spectraformer: A Unified Random Feature Framework for Transformer]] (83.2% similar)
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (83.0% similar)
- [[2025-09-23/Measure-to-measure interpolation using Transformers_20250923|Measure-to-measure interpolation using Transformers]] (82.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Matrix Completion|Matrix Completion]]
**âš¡ Unique Technical**: [[keywords/NystrÃ¶m Extrapolation|NystrÃ¶m Extrapolation]]
**ğŸš€ Evolved Concepts**: [[keywords/In-Context Learning|In-Context Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19702v1 Announce Type: cross 
Abstract: We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nystr\"om extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nystr\"om extrapolation, highlighting a powerful capability of in-context learning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ìˆ˜ë°±ë§Œ ê°œì˜ ë§ˆìŠ¤í¬ëœ ë¸”ë¡ í–‰ë ¬ ì™„ì„± ì‘ì—…ì„ í†µí•´ ì„ í˜• ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê°€ì§„ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ì…ë ¥-ì¶œë ¥ ìŒê³¼ í‰ê·  ì œê³± ì˜¤ì°¨ë§Œì„ ì‚¬ìš©í•˜ë©°, ì‘ì—… ê°„ì˜ ê´€ê³„ì— ëŒ€í•œ ì •ë³´ ì—†ì´ í•™ìŠµí•©ë‹ˆë‹¤. í›ˆë ¨ í›„, ëŒ€ìˆ˜ì  ì „ê°œë¥¼ í†µí•´ ì„¸ ê°€ì§€ ê³„ì‚° ì²´ì œì—ì„œ ë™ì¼í•œ ë§¤ê°œë³€ìˆ˜ ì—†ëŠ” ì—…ë°ì´íŠ¸ ê·œì¹™ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê·œì¹™ì€ ì „ì²´ ë°°ì¹˜ ë¬¸ì œì—ì„œ 2ì°¨ ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ê³ , ë¶„ì‚° ê³„ì‚°ì˜ ë³µì¡ì„±ì„ ì¤„ì´ë©°, ì œí•œëœ ë­í¬ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì˜ˆì¸¡, ì¶”ì •, ê·¸ë¦¬ê³  NystrÃ¶m ì™¸ì‚½ì„ ì•„ìš°ë¥´ëŠ” í†µí•©ì ì´ê³  ìì› ì ì‘ì ì¸ ë°˜ë³µ ì†”ë²„ë¥¼ ì•”ë¬µì ìœ¼ë¡œ ë°œê²¬í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì„ í˜• ì£¼ì˜ ë³€í™˜ê¸°ëŠ” ìˆ˜ë°±ë§Œ ê°œì˜ ë§ˆìŠ¤í¬ëœ ë¸”ë¡ í–‰ë ¬ ì™„ì„± ì‘ì—…ì„ í†µí•´ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.
- 2. ëª¨ë¸ì€ ì…ë ¥-ì¶œë ¥ ìŒê³¼ í‰ê·  ì œê³± ì†ì‹¤ë§Œì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ë©°, ê´€ë ¨ëœ ì‘ì—…ì— ëŒ€í•œ íŒíŠ¸ ì—†ì´ë„ í•™ìŠµí•©ë‹ˆë‹¤.
- 3. í›ˆë ¨ í›„, ëŒ€ìˆ˜ì  ì „ê°œë¥¼ í†µí•´ ì„¸ ê°€ì§€ ê³„ì‚° ì²´ì œì—ì„œ ë™ì¼í•œ ë§¤ê°œë³€ìˆ˜ ì—†ëŠ” ì—…ë°ì´íŠ¸ ê·œì¹™ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ì´ ê·œì¹™ì€ ì „ì²´ ë°°ì¹˜ ë¬¸ì œì—ì„œ 2ì°¨ ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ê³ , ë¶„ì‚° ë°˜ë³µ ë³µì¡ì„±ì„ ì¤„ì´ë©°, ë­í¬ ì œí•œ ì£¼ì˜ì—ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ì˜ˆì¸¡, ì¶”ì • ë° Nystr\"om ì™¸ì‚½ì„ ì•„ìš°ë¥´ëŠ” í†µí•©ëœ ìì› ì ì‘í˜• ë°˜ë³µ ì†”ë²„ë¥¼ ì•”ì‹œì ìœ¼ë¡œ ë°œê²¬í•˜ëŠ” ë³€í™˜ê¸°ì˜ ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 15:45:36*