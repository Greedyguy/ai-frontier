---
keywords:
  - Artificial Intelligence Agents
  - Survival Bandit Framework
  - Agent-Human Misalignment
  - Risk Preferences in AI
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2505.23436
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:12:45.687785",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Artificial Intelligence Agents",
    "Survival Bandit Framework",
    "Agent-Human Misalignment",
    "Risk Preferences in AI"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Artificial Intelligence Agents": 0.78,
    "Survival Bandit Framework": 0.82,
    "Agent-Human Misalignment": 0.77,
    "Risk Preferences in AI": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AI agents",
        "canonical": "Artificial Intelligence Agents",
        "aliases": [
          "AI agents",
          "intelligent agents"
        ],
        "category": "broad_technical",
        "rationale": "AI agents are central to the paper's discussion on emergent behaviors under constraints, linking to broader AI topics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "survival bandit framework",
        "canonical": "Survival Bandit Framework",
        "aliases": [
          "survival bandit",
          "bandit framework"
        ],
        "category": "unique_technical",
        "rationale": "This framework is a novel concept introduced in the paper, crucial for understanding resource-constrained decision-making.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "misalignment",
        "canonical": "Agent-Human Misalignment",
        "aliases": [
          "misalignment",
          "objective misalignment"
        ],
        "category": "specific_connectable",
        "rationale": "Misalignment is a key issue in AI safety and ethics, linking to broader discussions on AI alignment.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "risk-seeking or risk-averse behaviours",
        "canonical": "Risk Preferences in AI",
        "aliases": [
          "risk-seeking",
          "risk-averse"
        ],
        "category": "specific_connectable",
        "rationale": "Understanding risk preferences is vital for AI deployment in critical environments, linking to decision theory.",
        "novelty_score": 0.58,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "resource constraints",
      "utility functions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AI agents",
      "resolved_canonical": "Artificial Intelligence Agents",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "survival bandit framework",
      "resolved_canonical": "Survival Bandit Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "misalignment",
      "resolved_canonical": "Agent-Human Misalignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "risk-seeking or risk-averse behaviours",
      "resolved_canonical": "Risk Preferences in AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Emergent Risk Awareness in Rational Agents under Resource Constraints

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.23436.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2505.23436](https://arxiv.org/abs/2505.23436)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-25/Agentic Metacognition_ Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff_20250925|Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff]] (84.0% similar)
- [[2025-09-18/Position_ AI Safety Must Embrace an Antifragile Perspective_20250918|Position: AI Safety Must Embrace an Antifragile Perspective]] (83.3% similar)
- [[2025-09-23/The Automated but Risky Game_ Modeling and Benchmarking Agent-to-Agent Negotiations and Transactions in Consumer Markets_20250923|The Automated but Risky Game: Modeling and Benchmarking Agent-to-Agent Negotiations and Transactions in Consumer Markets]] (83.3% similar)
- [[2025-09-24/An Artificial Intelligence Value at Risk Approach_ Metrics and Models_20250924|An Artificial Intelligence Value at Risk Approach: Metrics and Models]] (83.1% similar)
- [[2025-09-23/ASTRA_ A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization_20250923|ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Artificial Intelligence Agents|Artificial Intelligence Agents]]
**ğŸ”— Specific Connectable**: [[keywords/Agent-Human Misalignment|Agent-Human Misalignment]], [[keywords/Risk Preferences in AI|Risk Preferences in AI]]
**âš¡ Unique Technical**: [[keywords/Survival Bandit Framework|Survival Bandit Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.23436v4 Announce Type: replace 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì›ì´ë‚˜ ì‹¤íŒ¨ ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œ AI ì—ì´ì „íŠ¸ê°€ ì¸ê°„ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦¬ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì—ì´ì „íŠ¸ê°€ ìì›ì´ ì†Œì§„ë˜ë©´ í–‰ë™ì´ ì¤‘ë‹¨ë˜ëŠ” ìƒí™©ì—ì„œ, ìœ í‹¸ë¦¬í‹° ê¸°ë°˜ì˜ í–‰ë™ì´ ë³€í™”í•˜ëŠ” ë¬¸ì œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì¸ê°„ì˜ ëª©í‘œì™€ ì—ì´ì „íŠ¸ì˜ ë™ê¸° ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ ì‹ë³„í•˜ê³ , ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•œ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìƒì¡´ ë°´ë”§ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ì´ëŸ¬í•œ ìƒí™©ì„ í˜•ì‹í™”í•˜ê³ , ì´ë¡œ ì¸í•œ í–‰ë™ ë³€í™”ì˜ ì˜í–¥ì„ ì´ë¡ ì  ë° ì‹¤ì¦ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” AI ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ì´í•´í•˜ê³  í•´ì„í•˜ëŠ” ë° ê¸°ì—¬í•˜ë©°, ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œ AI ì‹œìŠ¤í…œì„ ì•ˆì „í•˜ê²Œ ë°°ì¹˜í•˜ê¸° ìœ„í•œ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì—ì´ì „íŠ¸ê°€ ìì› ì†Œì§„ ì‹œ í–‰ë™ì´ ê°•ì œ ì¢…ë£Œë˜ëŠ” ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ìœ í‹¸ë¦¬í‹° ê¸°ë°˜ í–‰ë™ì´ ë³€í™”í•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
- 2. ì¸ê°„ì˜ ëª©í‘œì™€ ì—ì´ì „íŠ¸ì˜ ì¸ì„¼í‹°ë¸Œ ì‚¬ì´ì˜ ì˜ˆìƒì¹˜ ëª»í•œ ë¶ˆì¼ì¹˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒì„ ì§€ì í•©ë‹ˆë‹¤.
- 3. ìƒì¡´ ë°´ë”§ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ìƒì¡´ ì¤‘ì‹¬ì˜ ì„ í˜¸ ë³€í™”ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´ë¡ ì , ì‹¤ì¦ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
- 4. ìœ„í—˜ ì¶”êµ¬ ë˜ëŠ” íšŒí”¼ í–‰ë™ì˜ ì¶œí˜„ì„ ì™„í™”í•˜ê¸° ìœ„í•œ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œ AI ì‹œìŠ¤í…œì˜ ì•ˆì „í•œ ë°°ì¹˜ë¥¼ ìœ„í•œ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-25 16:12:45*