---
keywords:
  - AI Model Caching
  - Coordinated Multipoint Broadcasting
  - Multi-Agent Learning Framework
  - Parameter Blocks
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.19341
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T15:27:10.760364",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AI Model Caching",
    "Coordinated Multipoint Broadcasting",
    "Multi-Agent Learning Framework",
    "Parameter Blocks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AI Model Caching": 0.75,
    "Coordinated Multipoint Broadcasting": 0.72,
    "Multi-Agent Learning Framework": 0.8,
    "Parameter Blocks": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AI model caching",
        "canonical": "AI Model Caching",
        "aliases": [
          "Model Caching",
          "Caching"
        ],
        "category": "unique_technical",
        "rationale": "AI Model Caching is a unique concept that addresses storage and retrieval challenges in edge networks, facilitating specific technical discussions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Coordinated multipoint broadcasting",
        "canonical": "Coordinated Multipoint Broadcasting",
        "aliases": [
          "CoMP Broadcasting",
          "Multipoint Broadcasting"
        ],
        "category": "unique_technical",
        "rationale": "This technique is crucial for optimizing spectrum utilization in multi-cell networks, providing a unique angle for technical exploration.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Multi-agent learning framework",
        "canonical": "Multi-Agent Learning Framework",
        "aliases": [
          "Multi-Agent Learning",
          "Distributed Learning Framework"
        ],
        "category": "specific_connectable",
        "rationale": "This framework is pivotal for enabling cooperation among edge nodes, making it a key concept for linking discussions on distributed AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "Parameter blocks",
        "canonical": "Parameter Blocks",
        "aliases": [
          "PBs",
          "Model Parameter Blocks"
        ],
        "category": "unique_technical",
        "rationale": "Parameter Blocks are a novel approach to optimize storage and delivery of AI models, offering a unique perspective on model management.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "6G networks",
      "end users",
      "edge nodes"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AI model caching",
      "resolved_canonical": "AI Model Caching",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Coordinated multipoint broadcasting",
      "resolved_canonical": "Coordinated Multipoint Broadcasting",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Multi-agent learning framework",
      "resolved_canonical": "Multi-Agent Learning Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Parameter blocks",
      "resolved_canonical": "Parameter Blocks",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19341.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.19341](https://arxiv.org/abs/2509.19341)

## 🔗 유사한 논문
- [[2025-09-25/A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks_20250925|A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks]] (84.6% similar)
- [[2025-09-23/Learn to Optimize Resource Allocation under QoS Constraint of AR_20250923|Learn to Optimize Resource Allocation under QoS Constraint of AR]] (82.5% similar)
- [[2025-09-24/Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks_20250924|Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks]] (82.3% similar)
- [[2025-09-23/Search-Optimized Quantization in Biomedical Ontology Alignment_20250923|Search-Optimized Quantization in Biomedical Ontology Alignment]] (82.0% similar)
- [[2025-09-23/Federated Learning with Ad-hoc Adapter Insertions_ The Case of Soft-Embeddings for Training Classifier-as-Retriever_20250923|Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever]] (82.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multi-Agent Learning Framework|Multi-Agent Learning Framework]]
**⚡ Unique Technical**: [[keywords/AI Model Caching|AI Model Caching]], [[keywords/Coordinated Multipoint Broadcasting|Coordinated Multipoint Broadcasting]], [[keywords/Parameter Blocks|Parameter Blocks]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19341v1 Announce Type: cross 
Abstract: 6G networks are envisioned to support on-demand AI model downloading to accommodate diverse inference requirements of end users. By proactively caching models at edge nodes, users can retrieve the requested models with low latency for on-device AI inference. However, the substantial size of contemporary AI models poses significant challenges for edge caching under limited storage capacity, as well as for the concurrent delivery of heterogeneous models over wireless channels. To address these challenges, we propose a fine-grained AI model caching and downloading system that exploits parameter reusability, stemming from the common practice of fine-tuning task-specific models from a shared pre-trained model with frozen parameters. This system selectively caches model parameter blocks (PBs) at edge nodes, eliminating redundant storage of reusable parameters across different cached models. Additionally, it incorporates coordinated multipoint (CoMP) broadcasting to simultaneously deliver reusable PBs to multiple users, thereby enhancing downlink spectrum utilization. Under this arrangement, we formulate a model downloading delay minimization problem to jointly optimize PB caching, migration (among edge nodes), and broadcasting beamforming. To tackle this intractable problem, we develop a distributed multi-agent learning framework that enables edge nodes to explicitly learn mutual influence among their actions, thereby facilitating cooperation. Furthermore, a data augmentation approach is proposed to adaptively generate synthetic training samples through a predictive model, boosting sample efficiency and accelerating policy learning. Both theoretical analysis and simulation experiments validate the superior convergence performance of the proposed learning framework.

## 📝 요약

이 논문은 6G 네트워크에서 사용자 요구에 맞춰 AI 모델을 신속하게 다운로드할 수 있도록 엣지 노드에서 AI 모델을 캐싱하는 시스템을 제안합니다. 주요 기여는 파라미터 재사용을 활용하여 엣지 노드에 모델 파라미터 블록(PB)을 선택적으로 캐싱함으로써 저장 공간을 효율적으로 사용하는 것입니다. 또한, CoMP 방송을 통해 여러 사용자에게 동시에 PB를 전달하여 스펙트럼 활용을 최적화합니다. 모델 다운로드 지연을 최소화하기 위해 PB 캐싱, 엣지 노드 간 마이그레이션, 방송 빔포밍을 최적화하는 문제를 설정하고, 분산형 다중 에이전트 학습 프레임워크를 개발하여 엣지 노드 간의 협력을 촉진합니다. 데이터 증강 기법을 통해 샘플 효율성을 높이고 정책 학습을 가속화합니다. 이 프레임워크는 이론적 분석과 시뮬레이션 실험을 통해 우수한 수렴 성능을 입증합니다.

## 🎯 주요 포인트

- 1. 6G 네트워크에서는 사용자 요구에 맞춰 AI 모델을 다운로드할 수 있도록 지원하며, 엣지 노드에서 모델을 캐싱하여 지연 시간을 줄입니다.
- 2. 현대 AI 모델의 큰 크기는 제한된 저장 용량과 무선 채널을 통한 이질적인 모델의 동시 전송에 도전 과제를 제시합니다.
- 3. 제안된 시스템은 공유된 사전 학습 모델에서 파라미터 재사용을 활용하여 엣지 노드에 모델 파라미터 블록을 선택적으로 캐싱합니다.
- 4. CoMP 방송을 통해 여러 사용자에게 재사용 가능한 파라미터 블록을 동시에 전달하여 하향 링크 스펙트럼 활용을 향상시킵니다.
- 5. 분산 멀티 에이전트 학습 프레임워크를 개발하여 엣지 노드가 행동 간의 상호 영향을 학습하고 협력을 촉진합니다.


---

*Generated on 2025-09-25 15:27:10*