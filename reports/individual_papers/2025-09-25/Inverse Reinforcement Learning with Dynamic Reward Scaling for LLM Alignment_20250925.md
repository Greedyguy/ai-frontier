---
keywords:
  - Large Language Model
  - Inverse Reinforcement Learning
  - Dynamic Reward Scaling
  - Group Relative Policy Optimization
  - Safety Alignment
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2503.18991
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:22:03.559061",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Inverse Reinforcement Learning",
    "Dynamic Reward Scaling",
    "Group Relative Policy Optimization",
    "Safety Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Inverse Reinforcement Learning": 0.85,
    "Dynamic Reward Scaling": 0.88,
    "Group Relative Policy Optimization": 0.8,
    "Safety Alignment": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on alignment techniques.",
        "novelty_score": 0.2,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.9
      },
      {
        "surface": "Inverse Reinforcement Learning",
        "canonical": "Inverse Reinforcement Learning",
        "aliases": [
          "IRL"
        ],
        "category": "unique_technical",
        "rationale": "Core method proposed for dynamic reward scaling.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Dynamic Reward Scaling",
        "canonical": "Dynamic Reward Scaling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Novel approach introduced for improving LLM alignment.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "Enhanced with dynamic reward scaling for better performance.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Safety Alignment",
        "canonical": "Safety Alignment",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Critical aspect of deploying LLMs addressed by the paper.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "reward model",
      "task difficulty",
      "preference pairs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Inverse Reinforcement Learning",
      "resolved_canonical": "Inverse Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Dynamic Reward Scaling",
      "resolved_canonical": "Dynamic Reward Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Safety Alignment",
      "resolved_canonical": "Safety Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.18991.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2503.18991](https://arxiv.org/abs/2503.18991)

## 🔗 유사한 논문
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (86.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.2% similar)
- [[2025-09-23/Reasoning-to-Defend_ Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking_20250923|Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking]] (86.2% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (86.1% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (85.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Safety Alignment|Safety Alignment]]
**⚡ Unique Technical**: [[keywords/Inverse Reinforcement Learning|Inverse Reinforcement Learning]], [[keywords/Dynamic Reward Scaling|Dynamic Reward Scaling]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2503.18991v4 Announce Type: replace-cross 
Abstract: Alignment is vital for safely deploying large language models (LLMs). Existing techniques are either reward-based (train a reward model on preference pairs and optimize with reinforcement learning) or reward-free (directly fine-tune on ranked outputs). Recent research shows that well-tuned reward-based pipelines remain robust, and single-response demonstrations can outperform pairwise preference data. However, two challenges persist: (1) imbalanced safety datasets that overrepresent common hazards while neglecting long-tail threats; and (2) static reward models that ignore task difficulty, limiting optimization efficiency and attainable gains. We propose DR-IRL (Dynamically adjusting Rewards through Inverse Reinforcement Learning). We first train category-specific reward models using a balanced safety dataset covering seven harmful categories via IRL. Then we enhance Group Relative Policy Optimization (GRPO) by introducing dynamic reward scaling--adjusting rewards by task difficulty--data-level hardness by text encoder cosine similarity, model-level responsiveness by reward gaps. Extensive experiments across various benchmarks and LLMs demonstrate that DR-IRL outperforms all baseline methods in safety alignment while maintaining usefulness.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 안전한 배포를 위한 정렬 방법을 제안합니다. 기존 방법은 보상 기반 또는 보상 비기반으로 나뉘며, 각각 강화 학습이나 순위 조정으로 최적화합니다. 그러나 불균형한 안전 데이터셋과 정적 보상 모델의 한계가 존재합니다. 이를 해결하기 위해 DR-IRL(역강화학습을 통한 동적 보상 조정)을 제안합니다. 이 방법은 7가지 유해 카테고리를 포함한 균형 잡힌 안전 데이터셋을 사용해 카테고리별 보상 모델을 훈련하고, 과제 난이도에 따라 보상을 조정하는 동적 보상 스케일링을 도입합니다. 다양한 벤치마크 실험 결과, DR-IRL은 안전 정렬에서 기존 방법들을 능가하면서도 유용성을 유지합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)의 안전한 배포를 위해 정렬이 중요하며, 기존 기술은 보상 기반 또는 보상 비사용 방식으로 나뉜다.
- 2. 기존 연구에 따르면 잘 조정된 보상 기반 파이프라인은 여전히 견고하며, 단일 응답 시연이 쌍별 선호 데이터보다 우수할 수 있다.
- 3. 두 가지 주요 문제는 불균형한 안전 데이터셋과 작업 난이도를 무시하는 정적 보상 모델이다.
- 4. DR-IRL은 역강화학습(IRL)을 통해 범주별 보상 모델을 훈련하고, 작업 난이도에 따라 보상을 조정하여 최적화 효율성을 높인다.
- 5. 다양한 벤치마크와 LLM에서 DR-IRL이 안전 정렬에서 모든 기준 방법을 능가하면서 유용성을 유지함을 실험적으로 입증했다.


---

*Generated on 2025-09-25 16:22:03*