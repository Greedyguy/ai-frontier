---
keywords:
  - Large Language Model
  - Inverse Reinforcement Learning
  - Dynamic Reward Scaling
  - Group Relative Policy Optimization
  - Safety Alignment
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2503.18991
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:22:03.559061",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Inverse Reinforcement Learning",
    "Dynamic Reward Scaling",
    "Group Relative Policy Optimization",
    "Safety Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Inverse Reinforcement Learning": 0.85,
    "Dynamic Reward Scaling": 0.88,
    "Group Relative Policy Optimization": 0.8,
    "Safety Alignment": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on alignment techniques.",
        "novelty_score": 0.2,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.9
      },
      {
        "surface": "Inverse Reinforcement Learning",
        "canonical": "Inverse Reinforcement Learning",
        "aliases": [
          "IRL"
        ],
        "category": "unique_technical",
        "rationale": "Core method proposed for dynamic reward scaling.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Dynamic Reward Scaling",
        "canonical": "Dynamic Reward Scaling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Novel approach introduced for improving LLM alignment.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "Enhanced with dynamic reward scaling for better performance.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Safety Alignment",
        "canonical": "Safety Alignment",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Critical aspect of deploying LLMs addressed by the paper.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "reward model",
      "task difficulty",
      "preference pairs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Inverse Reinforcement Learning",
      "resolved_canonical": "Inverse Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Dynamic Reward Scaling",
      "resolved_canonical": "Dynamic Reward Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Safety Alignment",
      "resolved_canonical": "Safety Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.18991.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2503.18991](https://arxiv.org/abs/2503.18991)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (86.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.2% similar)
- [[2025-09-23/Reasoning-to-Defend_ Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking_20250923|Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking]] (86.2% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (86.1% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (85.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Safety Alignment|Safety Alignment]]
**âš¡ Unique Technical**: [[keywords/Inverse Reinforcement Learning|Inverse Reinforcement Learning]], [[keywords/Dynamic Reward Scaling|Dynamic Reward Scaling]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.18991v4 Announce Type: replace-cross 
Abstract: Alignment is vital for safely deploying large language models (LLMs). Existing techniques are either reward-based (train a reward model on preference pairs and optimize with reinforcement learning) or reward-free (directly fine-tune on ranked outputs). Recent research shows that well-tuned reward-based pipelines remain robust, and single-response demonstrations can outperform pairwise preference data. However, two challenges persist: (1) imbalanced safety datasets that overrepresent common hazards while neglecting long-tail threats; and (2) static reward models that ignore task difficulty, limiting optimization efficiency and attainable gains. We propose DR-IRL (Dynamically adjusting Rewards through Inverse Reinforcement Learning). We first train category-specific reward models using a balanced safety dataset covering seven harmful categories via IRL. Then we enhance Group Relative Policy Optimization (GRPO) by introducing dynamic reward scaling--adjusting rewards by task difficulty--data-level hardness by text encoder cosine similarity, model-level responsiveness by reward gaps. Extensive experiments across various benchmarks and LLMs demonstrate that DR-IRL outperforms all baseline methods in safety alignment while maintaining usefulness.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „í•œ ë°°í¬ë¥¼ ìœ„í•œ ì •ë ¬ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ë³´ìƒ ê¸°ë°˜ ë˜ëŠ” ë³´ìƒ ë¹„ê¸°ë°˜ìœ¼ë¡œ ë‚˜ë‰˜ë©°, ê°ê° ê°•í™” í•™ìŠµì´ë‚˜ ìˆœìœ„ ì¡°ì •ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¶ˆê· í˜•í•œ ì•ˆì „ ë°ì´í„°ì…‹ê³¼ ì •ì  ë³´ìƒ ëª¨ë¸ì˜ í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DR-IRL(ì—­ê°•í™”í•™ìŠµì„ í†µí•œ ë™ì  ë³´ìƒ ì¡°ì •)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ 7ê°€ì§€ ìœ í•´ ì¹´í…Œê³ ë¦¬ë¥¼ í¬í•¨í•œ ê· í˜• ì¡íŒ ì•ˆì „ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ ì¹´í…Œê³ ë¦¬ë³„ ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ê³¼ì œ ë‚œì´ë„ì— ë”°ë¼ ë³´ìƒì„ ì¡°ì •í•˜ëŠ” ë™ì  ë³´ìƒ ìŠ¤ì¼€ì¼ë§ì„ ë„ì…í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, DR-IRLì€ ì•ˆì „ ì •ë ¬ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©´ì„œë„ ìœ ìš©ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „í•œ ë°°í¬ë¥¼ ìœ„í•´ ì •ë ¬ì´ ì¤‘ìš”í•˜ë©°, ê¸°ì¡´ ê¸°ìˆ ì€ ë³´ìƒ ê¸°ë°˜ ë˜ëŠ” ë³´ìƒ ë¹„ì‚¬ìš© ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰œë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ì— ë”°ë¥´ë©´ ì˜ ì¡°ì •ëœ ë³´ìƒ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì€ ì—¬ì „íˆ ê²¬ê³ í•˜ë©°, ë‹¨ì¼ ì‘ë‹µ ì‹œì—°ì´ ìŒë³„ ì„ í˜¸ ë°ì´í„°ë³´ë‹¤ ìš°ìˆ˜í•  ìˆ˜ ìˆë‹¤.
- 3. ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œëŠ” ë¶ˆê· í˜•í•œ ì•ˆì „ ë°ì´í„°ì…‹ê³¼ ì‘ì—… ë‚œì´ë„ë¥¼ ë¬´ì‹œí•˜ëŠ” ì •ì  ë³´ìƒ ëª¨ë¸ì´ë‹¤.
- 4. DR-IRLì€ ì—­ê°•í™”í•™ìŠµ(IRL)ì„ í†µí•´ ë²”ì£¼ë³„ ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì‘ì—… ë‚œì´ë„ì— ë”°ë¼ ë³´ìƒì„ ì¡°ì •í•˜ì—¬ ìµœì í™” íš¨ìœ¨ì„±ì„ ë†’ì¸ë‹¤.
- 5. ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ LLMì—ì„œ DR-IRLì´ ì•ˆì „ ì •ë ¬ì—ì„œ ëª¨ë“  ê¸°ì¤€ ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©´ì„œ ìœ ìš©ì„±ì„ ìœ ì§€í•¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í–ˆë‹¤.


---

*Generated on 2025-09-25 16:22:03*