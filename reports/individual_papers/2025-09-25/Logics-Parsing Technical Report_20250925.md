---
keywords:
  - Vision-Language Model
  - Optical Character Recognition
  - Reinforcement Learning
  - Logics-Parsing
  - LogicsParsingBench
category: cs.CV
publish_date: 2025-09-25
arxiv_id: 2509.19760
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-26T09:06:15.992429",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Optical Character Recognition",
    "Reinforcement Learning",
    "Logics-Parsing",
    "LogicsParsingBench"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Optical Character Recognition": 0.8,
    "Reinforcement Learning": 0.78,
    "Logics-Parsing": 0.7,
    "LogicsParsingBench": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Vision-Language models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for linking multimodal learning and document analysis advancements.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.68,
        "link_intent_score": 0.85
      },
      {
        "surface": "Optical Character Recognition",
        "canonical": "Optical Character Recognition",
        "aliases": [
          "OCR"
        ],
        "category": "specific_connectable",
        "rationale": "OCR is a key component in document parsing, connecting to broader text recognition technologies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "reinforcement learning",
        "canonical": "Reinforcement Learning",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is central to optimizing model performance, linking to adaptive learning strategies.",
        "novelty_score": 0.35,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Logics-Parsing",
        "canonical": "Logics-Parsing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Logics-Parsing represents a novel approach in document parsing, offering unique insights into layout analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "LogicsParsingBench",
        "canonical": "LogicsParsingBench",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "LogicsParsingBench is a unique dataset for evaluating document parsing models, enhancing research reproducibility.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "document parsing task",
      "structured outputs",
      "complex document types"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Vision-Language models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.68,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Optical Character Recognition",
      "resolved_canonical": "Optical Character Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "reinforcement learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.35,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Logics-Parsing",
      "resolved_canonical": "Logics-Parsing",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "LogicsParsingBench",
      "resolved_canonical": "LogicsParsingBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Logics-Parsing Technical Report

## 📋 메타데이터

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19760.pdf)
**Category**: cs.CV
**Published**: 2025-09-25
**ArXiv ID**: [2509.19760](https://arxiv.org/abs/2509.19760)

## 🔗 유사한 논문
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (82.3% similar)
- [[2025-09-23/Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models_20250923|Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models]] (82.2% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (82.2% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (82.2% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (82.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Optical Character Recognition|Optical Character Recognition]]
**⚡ Unique Technical**: [[keywords/Logics-Parsing|Logics-Parsing]], [[keywords/LogicsParsingBench|LogicsParsingBench]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19760v1 Announce Type: new 
Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred significant progress in document parsing task. Compared to traditional pipeline-based methods, end-to-end paradigms have shown their excellence in converting PDF images into structured outputs through integrated Optical Character Recognition (OCR), table recognition, mathematical formula recognition and so on. However, the absence of explicit analytical stages for document layouts and reading orders limits the LVLM's capability in handling complex document types such as multi-column newspapers or posters. To address this limitation, we propose in this report Logics-Parsing: an end-to-end LVLM-based model augmented with reinforcement learning. Our model incorporates meticulously designed reward mechanisms to optimize complex layout analysis and reading order inference. In addition, we expand the model's versatility by incorporating diverse data types such as chemical formulas and handwritten Chinese characters into supervised fine-tuning. Finally, to enable rigorous evaluation of our approach, we introduce LogicsParsingBench, a curated set of 1,078 page-level PDF images spanning nine major categories and over twenty sub-categories, which will be released later. Comprehensive experiments conducted on LogicsParsingBench have validated the efficacy and State-of-the-art (SOTA) performance of our proposed model across diverse document analysis scenarios. Project Page: https://github.com/alibaba/Logics-Parsing

## 📝 요약

최근 대형 비전-언어 모델(LVLM)의 발전은 문서 파싱 작업에 큰 진전을 가져왔습니다. 기존의 파이프라인 기반 방법과 달리, 통합된 광학 문자 인식(OCR), 표 인식, 수식 인식을 통해 PDF 이미지를 구조화된 출력으로 변환하는 종단 간 패러다임이 우수성을 보였습니다. 그러나 복잡한 문서 레이아웃과 읽기 순서를 명시적으로 분석하지 못해 다중 열 신문이나 포스터와 같은 복잡한 문서 유형을 처리하는 데 한계가 있었습니다. 이를 해결하기 위해, 우리는 강화 학습을 통해 향상된 Logics-Parsing 모델을 제안합니다. 이 모델은 복잡한 레이아웃 분석과 읽기 순서 추론을 최적화하기 위해 정교하게 설계된 보상 메커니즘을 통합합니다. 또한, 화학식과 손으로 쓴 한자를 포함한 다양한 데이터 유형을 감독 학습에 포함하여 모델의 범용성을 확장했습니다. 우리의 접근 방식을 엄격하게 평가하기 위해, 9개의 주요 카테고리와 20개 이상의 하위 카테고리를 포함하는 1,078개의 페이지 수준 PDF 이미지로 구성된 LogicsParsingBench를 소개합니다. 이 데이터셋을 통해 수행된 종합 실험은 다양한 문서 분석 시나리오에서 제안된 모델의 효능과 최첨단 성능을 입증했습니다.

## 🎯 주요 포인트

- 1. 대규모 비전-언어 모델(LVLM)은 문서 파싱 작업에서 중요한 발전을 이루었다.
- 2. 기존 파이프라인 기반 방법과 비교하여, 엔드투엔드 패러다임은 PDF 이미지를 구조화된 출력으로 변환하는 데 우수성을 보였다.
- 3. 복잡한 문서 레이아웃과 읽기 순서를 분석하는 명시적 단계가 없어 LVLM의 복잡한 문서 처리 능력이 제한된다.
- 4. 이를 해결하기 위해 강화 학습을 활용한 Logics-Parsing 모델을 제안하여 복잡한 레이아웃 분석과 읽기 순서 추론을 최적화했다.
- 5. LogicsParsingBench라는 평가 세트를 도입하여 다양한 문서 분석 시나리오에서 제안된 모델의 성능을 검증했다.


---

*Generated on 2025-09-26 09:06:15*