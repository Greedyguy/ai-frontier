---
keywords:
  - Large Language Model
  - Execution Trace
  - Semantic Information
  - Supervised Fine-Tuning
category: cs.AI
publish_date: 2025-09-25
arxiv_id: 2509.11686
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-25T16:34:56.503964",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Execution Trace",
    "Semantic Information",
    "Supervised Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Execution Trace": 0.78,
    "Semantic Information": 0.8,
    "Supervised Fine-Tuning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Code Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "Code LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader category of language models, crucial for understanding the context of code-specific applications.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "execution traces",
        "canonical": "Execution Trace",
        "aliases": [
          "runtime traces"
        ],
        "category": "unique_technical",
        "rationale": "Execution traces are pivotal for understanding program behavior, a unique aspect of this study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "semantic information",
        "canonical": "Semantic Information",
        "aliases": [
          "semantic data"
        ],
        "category": "specific_connectable",
        "rationale": "Semantic information is central to the study's exploration of enhancing reasoning in code models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "supervised fine-tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "specific_connectable",
        "rationale": "Supervised fine-tuning is a key process in improving model performance, relevant to the study's findings.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "program execution behavior",
      "test time scaling"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Code Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "execution traces",
      "resolved_canonical": "Execution Trace",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "semantic information",
      "resolved_canonical": "Semantic Information",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "supervised fine-tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250925|20250925]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.11686.pdf)
**Category**: cs.AI
**Published**: 2025-09-25
**ArXiv ID**: [2509.11686](https://arxiv.org/abs/2509.11686)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (85.5% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (85.4% similar)
- [[2025-09-24/LLM-based Vulnerability Discovery through the Lens of Code Metrics_20250924|LLM-based Vulnerability Discovery through the Lens of Code Metrics]] (85.4% similar)
- [[2025-09-25/Reverse Engineering User Stories from Code using Large Language Models_20250925|Reverse Engineering User Stories from Code using Large Language Models]] (85.0% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (83.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Semantic Information|Semantic Information]], [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Execution Trace|Execution Trace]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.11686v3 Announce Type: replace-cross 
Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming with their impressive capabilities. However, recent research has revealed critical limitations in their ability to reason about runtime behavior and understand the actual functionality of programs, which poses significant challenges for their post-training and practical deployment. Specifically, Code LLMs encounter two principal issues: (1) a lack of proficiency in reasoning about program execution behavior, as they struggle to interpret what programs actually do during runtime, and (2) the inconsistent and fragmented representation of semantic information, such as execution traces, across existing methods, which hinders their ability to generalize and reason effectively. These challenges underscore the necessity for more systematic approaches to enhance the reasoning capabilities of Code LLMs. To address these issues, we introduce a generic framework to support integrating semantic information~(e.g., execution trace) to code task-relevant prompts, and conduct a comprehensive study to explore the role of semantic information in enhancing the reasoning ability of Code LLMs accordingly. Specifically, we focus on investigating the usefulness of trace-based semantic information in boosting supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The experimental results surprisingly disagree with previous works and demonstrate that semantic information has limited usefulness for SFT and test time scaling of Code LLM.

## ğŸ“ ìš”ì•½

Code LLMsëŠ” í”„ë¡œê·¸ë˜ë°ì— í˜ì‹ ì„ ê°€ì ¸ì™”ì§€ë§Œ, ì‹¤í–‰ ì‹œ í”„ë¡œê·¸ë¨ì˜ ì‹¤ì œ ê¸°ëŠ¥ì„ ì´í•´í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ë¬¸ì œëŠ” (1) í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œì˜ í–‰ë™ì„ ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ê³ , (2) ê¸°ì¡´ ë°©ë²•ë“¤ì´ ì‹¤í–‰ ì¶”ì ê³¼ ê°™ì€ ì˜ë¯¸ ì •ë³´ë¥¼ ì¼ê´€ë˜ê²Œ í‘œí˜„í•˜ì§€ ëª»í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì½”ë“œ ê´€ë ¨ ì‘ì—…ì— ì˜ë¯¸ ì •ë³´ë¥¼ í†µí•©í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ê³ , ì˜ë¯¸ ì •ë³´ê°€ Code LLMsì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì—­í• ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì˜ë¯¸ ì •ë³´ê°€ SFTì™€ í…ŒìŠ¤íŠ¸ ì‹œ í™•ì¥ì— ì œí•œì ì¸ ìœ ìš©ì„±ì„ ê°€ì§„ë‹¤ëŠ” ì ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Code LLMsëŠ” ì‹¤í–‰ ì‹œ í”„ë¡œê·¸ë¨ì˜ ì‹¤ì œ ë™ì‘ì„ ì´í•´í•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤.
- 2. ì‹¤í–‰ ì¶”ì ê³¼ ê°™ì€ ì˜ë¯¸ ì •ë³´ì˜ ì¼ê´€ì„± ì—†ëŠ” í‘œí˜„ì´ Code LLMsì˜ ì¼ë°˜í™”ì™€ ì¶”ë¡ ì— ì¥ì• ê°€ ëœë‹¤.
- 3. Code LLMsì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì²´ê³„ì ì¸ ì ‘ê·¼ ë°©ì‹ì´ í•„ìš”í•˜ë‹¤.
- 4. ì˜ë¯¸ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ê³ , ì˜ë¯¸ ì •ë³´ê°€ Code LLMsì˜ ì¶”ë¡  ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì—°êµ¬í•˜ì˜€ë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì˜ë¯¸ ì •ë³´ëŠ” Code LLMsì˜ ê°ë…ëœ ë¯¸ì„¸ ì¡°ì • ë° í…ŒìŠ¤íŠ¸ ì‹œ í™•ì¥ì— ì œí•œì ì¸ ìœ ìš©ì„±ì„ ê°€ì§„ë‹¤.


---

*Generated on 2025-09-25 16:34:56*