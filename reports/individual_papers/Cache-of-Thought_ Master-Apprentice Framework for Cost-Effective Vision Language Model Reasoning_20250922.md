# Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning

**Korean Title:** "ìƒê°ì˜ ìºì‹œ: ë¹„ìš© íš¨ìœ¨ì ì¸ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ë§ˆìŠ¤í„°-ê²¬ìŠµìƒ í”„ë ˆì„ì›Œí¬"

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Master Apprentice Framework|Master Apprentice Framework]] [[keywords/specific/Multimodal Retrieval|Multimodal Retrieval]] [[keywords/specific/In-context Learning|In-context Learning]] [[keywords/broad/Vision Language Models|Vision Language Models]] [[keywords/unique/Cache of Thought|Cache of Thought]] [[categories/cs.LG|cs.LG]] [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (85.6% similar) [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (84.3% similar) [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Master Apprentice Framework
**ğŸ”— Specific Connectable**: Multimodal Retrieval, In-context Learning
**ğŸ”¬ Broad Technical**: Vision Language Models
**â­ Unique Technical**: Cache of Thought
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (85.6% similar)
- [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (84.3% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (84.1% similar)
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (84.1% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (83.9% similar)


**ArXiv ID**: [2502.20587](https://arxiv.org/abs/2502.20587)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2502.20587.pdf)


**ArXiv ID**: [2502.20587](https://arxiv.org/abs/2502.20587)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2502.20587.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Master Apprentice Framework
**ğŸ”— Specific Connectable**: In-context Learning, Multimodal Retrieval
**â­ Unique Technical**: Cache of Thought
**ğŸ”¬ Broad Technical**: Vision Language Models

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Vision Language Models` â€¢ 

`Multimodal Retrieval` â€¢ 

`In-context Learning` â€¢ 

`Cache of Thought` â€¢ 

`Master Apprentice Framework`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.20587v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved remarkable success in a wide range of vision applications of increasing complexity and scales, yet choosing the right VLM model size involves a trade-off between response quality and cost. While smaller VLMs are cheaper to run, they typically produce responses only marginally better than random guessing on benchmarks such as MMMU.
  In this paper, we propose Cache of Thought (CoT), a master apprentice framework for collaborative inference between large and small VLMs. CoT manages high quality query results from large VLMs (master) in a cache, which are then selected via a novel multi modal retrieval and in-context learning to aid the performance of small VLMs (apprentice). We extensively evaluate CoT on various widely recognized and challenging general reasoning benchmarks, and show that CoT increases overall reasoning performance by up to 7.7% under the same budget, and specifically boosts the performance of apprentice VLMs by up to 36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.20587v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë¹„ì „ ì–¸ì–´ ëª¨ë¸(Vision Language Models, VLMs)ì€ ì ì  ë” ë³µì¡í•˜ê³  ê·œëª¨ê°€ ì»¤ì§€ëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ì‘ìš© ë¶„ì•¼ì—ì„œ ë†€ë¼ìš´ ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ì ì ˆí•œ VLM ëª¨ë¸ í¬ê¸°ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì€ ì‘ë‹µ í’ˆì§ˆê³¼ ë¹„ìš© ê°„ì˜ ê· í˜•ì„ ìš”êµ¬í•©ë‹ˆë‹¤. ì‘ì€ VLMì€ ì‹¤í–‰ ë¹„ìš©ì´ ì €ë ´í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ MMMUì™€ ê°™ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¸¡ë³´ë‹¤ ì•½ê°„ ë” ë‚˜ì€ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.  
ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ë° ì†Œí˜• VLM ê°„ì˜ í˜‘ë ¥ì  ì¶”ë¡ ì„ ìœ„í•œ ë§ˆìŠ¤í„° ê²¬ìŠµìƒ í”„ë ˆì„ì›Œí¬ì¸ Cache of Thought (CoT)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CoTëŠ” ëŒ€í˜• VLM(ë§ˆìŠ¤í„°)ì—ì„œ ë†’ì€ í’ˆì§ˆì˜ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ê´€ë¦¬í•˜ê³ , ì´ë¥¼ ìƒˆë¡œìš´ ë‹¤ì¤‘ ëª¨ë‹¬ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ë‚´ í•™ìŠµì„ í†µí•´ ì„ íƒí•˜ì—¬ ì†Œí˜• VLM(ê²¬ìŠµìƒ)ì˜ ì„±ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ë„ë¦¬ ì¸ì •ë°›ëŠ” ë„ì „ì ì¸ ì¼ë°˜ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ CoTë¥¼ ê´‘ë²”ìœ„í•˜ê²Œ í‰ê°€í•˜ì˜€ìœ¼ë©°, CoTê°€ ë™ì¼í•œ ì˜ˆì‚° í•˜ì—ì„œ ì „ì²´ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœëŒ€ 7.7% í–¥ìƒì‹œí‚¤ê³ , íŠ¹íˆ ê²¬ìŠµìƒ VLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 36.6% í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/UIUC-MONET/Cache-of-Thoughtsì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Vision Language Models(VLMs)ì˜ íš¨ìœ¨ì ì¸ í™œìš©ì„ ìœ„í•´ Cache of Thought(CoT)ë¼ëŠ” ë§ˆìŠ¤í„°-ê²¬ìŠµìƒ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CoTëŠ” ëŒ€í˜• VLM(ë§ˆìŠ¤í„°)ì˜ ê³ í’ˆì§ˆ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ê³ , ì´ë¥¼ ì†Œí˜• VLM(ê²¬ìŠµìƒ)ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í™œìš©í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ ê²€ìƒ‰ ë° ë§¥ë½ ë‚´ í•™ìŠµì„ í†µí•´ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 36.6% í–¥ìƒì‹œí‚¤ë©°, ë™ì¼í•œ ì˜ˆì‚° ë‚´ì—ì„œ ì „ì²´ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœëŒ€ 7.7% ê°œì„ í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì¼ë°˜ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ CoTì˜ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Vision Language Models(VLMs)ëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ì‘ìš© ë¶„ì•¼ì—ì„œ ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ëª¨ë¸ í¬ê¸° ì„ íƒ ì‹œ ì‘ë‹µ í’ˆì§ˆê³¼ ë¹„ìš© ê°„ì˜ ê· í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤.

- 2. Cache of Thought(CoT)ëŠ” ëŒ€í˜• VLM(ë§ˆìŠ¤í„°)ê³¼ ì†Œí˜• VLM(ê²¬ìŠµìƒ) ê°„ì˜ í˜‘ë ¥ ì¶”ë¡ ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ëŒ€í˜• VLMì˜ ê³ í’ˆì§ˆ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ì—¬ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 3. CoTëŠ” ë‹¤ì–‘í•œ ì¼ë°˜ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 7.7%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ê²¬ìŠµìƒ VLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 36.6%ê¹Œì§€ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 4. CoTëŠ” ìƒˆë¡œìš´ ë©€í‹° ëª¨ë‹¬ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ í•™ìŠµì„ í†µí•´ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

- 5. CoTì˜ ì½”ë“œëŠ” https://github.com/UIUC-MONET/Cache-of-Thoughts ì—ì„œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 15:55:10*