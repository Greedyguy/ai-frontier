# On the (In)Significance of Feature Selection in High-Dimensional Datasets

**Korean Title:** 고차원 데이터셋에서 특징 선택의 (비)중요성에 대하여

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Random Feature Subsets|Random Feature Subsets]] [[keywords/specific/Computational Genomics|Computational Genomics]] [[keywords/broad/Feature Selection|Feature Selection]] [[keywords/broad/High-Dimensional Data|High-Dimensional Data]] [[categories/cs.LG|cs.LG]] [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (79.2% similar) [[2025-09-22/Top-$k$ Feature Importance Ranking_20250922|Top-$k$ Feature Importance Ranking]] (78.9% similar) [[2025-09-17/Beyond Correlation_ Causal Multi-View Unsupervised Feature Selection Learning_20250917|Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning]] (78.7% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Arbitrary Feature Sets
**🔗 Specific Connectable**: Predictive Performance
**🔬 Broad Technical**: Feature Selection, High-Dimensional Data
## 🔗 유사한 논문
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (79.2% similar)
- [[2025-09-22/Top-$k$ Feature Importance Ranking_20250922|Top-$k$ Feature Importance Ranking]] (78.9% similar)
- [[2025-09-17/Beyond Correlation_ Causal Multi-View Unsupervised Feature Selection Learning_20250917|Beyond Correlation Causal Multi-View Unsupervised Feature Selection Learning]] (78.7% similar)
- [[2025-09-22/IEFS-GMB_ Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders_20250922|IEFS-GMB Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders]] (78.5% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (77.4% similar)


**ArXiv ID**: [2508.03593](https://arxiv.org/abs/2508.03593)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2508.03593.pdf)


**ArXiv ID**: [2508.03593](https://arxiv.org/abs/2508.03593)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2508.03593.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Arbitrary Feature Sets
**🔗 Specific Connectable**: Predictive Performance
**🔬 Broad Technical**: Feature Selection, High-Dimensional Data

## 🏷️ 추출된 키워드



`Feature Selection` • 

`High-Dimensional Data` • 

`Predictive Performance` • 

`Arbitrary Feature Sets`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2508.03593v2 Announce Type: replace 
Abstract: Feature selection (FS) is assumed to improve predictive performance and identify meaningful features in high-dimensional datasets. Surprisingly, small random subsets of features (0.02-1%) match or outperform the predictive performance of both full feature sets and FS across 28 out of 30 diverse datasets (microarray, bulk and single-cell RNA-Seq, mass spectrometry, imaging, etc.). In short, any arbitrary set of features is as good as any other (with surprisingly low variance in results) - so how can a particular set of selected features be "important" if they perform no better than an arbitrary set? These results challenge the assumption that computationally selected features reliably capture meaningful signals, emphasizing the importance of rigorous validation before interpreting selected features as actionable, particularly in computational genomics.

## 🔍 Abstract (한글 번역)

arXiv:2508.03593v2 발표 유형: 교체  
초록: 특징 선택(FS)은 예측 성능을 향상시키고 고차원 데이터셋에서 의미 있는 특징을 식별할 수 있다고 가정됩니다. 놀랍게도, 작은 무작위 특징 하위 집합(0.02-1%)이 전체 특징 집합과 FS의 예측 성능을 30개의 다양한 데이터셋(마이크로어레이, 벌크 및 단일 세포 RNA-Seq, 질량 분석, 이미지 등) 중 28개에서 맞추거나 능가합니다. 요컨대, 임의의 특징 집합은 다른 어떤 집합과도 마찬가지로 좋습니다(결과의 변동성이 놀랍도록 낮음) - 그렇다면 특정 선택된 특징 집합이 임의의 집합보다 더 잘 수행하지 않는다면 어떻게 "중요"할 수 있을까요? 이러한 결과는 계산적으로 선택된 특징이 의미 있는 신호를 신뢰성 있게 포착한다는 가정을 도전하며, 특히 계산 유전체학에서 선택된 특징을 실행 가능한 것으로 해석하기 전에 엄격한 검증의 중요성을 강조합니다.

## 📝 요약

이 논문은 고차원 데이터셋에서 특징 선택(FS)이 예측 성능을 향상시키고 의미 있는 특징을 식별한다고 가정하지만, 실제로는 무작위로 선택한 작은 특징 집합(0.02-1%)이 전체 특징 집합이나 FS의 성능을 능가하거나 동등하다는 것을 발견했습니다. 30개의 다양한 데이터셋 중 28개에서 이러한 결과가 나타났으며, 이는 특정 특징 집합이 임의의 집합보다 중요하다고 볼 수 없음을 시사합니다. 이러한 결과는 계산적으로 선택된 특징이 의미 있는 신호를 포착한다고 믿는 가정을 도전하며, 특히 계산 유전체학에서 선택된 특징을 해석하기 전에 엄격한 검증의 중요성을 강조합니다.

## 🎯 주요 포인트


- 1. 작은 랜덤 피처 하위 집합이 전체 피처 집합과 피처 선택을 통한 예측 성능을 능가하거나 대등하게 나타냈습니다.

- 2. 임의로 선택된 피처 집합이 특정 피처 집합과 성능 면에서 거의 차이가 없다는 결과가 나왔습니다.

- 3. 컴퓨터로 선택된 피처가 의미 있는 신호를 포착한다는 가정에 의문을 제기합니다.

- 4. 선택된 피처를 해석하기 전에 철저한 검증의 중요성을 강조합니다.

- 5. 특히 계산 유전체학 분야에서 피처 선택의 신뢰성에 대한 재평가가 필요합니다.


---

*Generated on 2025-09-22 16:00:55*