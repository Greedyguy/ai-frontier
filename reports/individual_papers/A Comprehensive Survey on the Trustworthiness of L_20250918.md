
# A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare

**Korean Title:** 건강 분야에서 대형 언어 모델의 신뢰성에 대한 포괄적인 조사

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Multi-agent Collaboration|Multi-agent Collaboration]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Healthcare|Healthcare]] [[keywords/specific/Trustworthiness|Trustworthiness]] [[keywords/unique/LLMs|LLMs]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Multi-agent Collaboration
**🔬 Broad Technical**: Large Language Models, Healthcare
**🔗 Specific Connectable**: Ethical Deployment
**⭐ Unique Technical**: LLaVA

**ArXiv ID**: [2502.15871](https://arxiv.org/abs/2502.15871)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2502.15871.pdf)


## 🏷️ 추출된 키워드



`Large Language Models` • 

`Healthcare` • 

`Trustworthiness` • 

`LLaVA` • 

`Multi-agent Collaboration`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.15871v2 Announce Type: replace-cross 
Abstract: The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care. However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability. These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs. While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights. This survey addresses that gap by providing a comprehensive review of current methodologies and solutions aimed at mitigating risks across key trust dimensions. We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches. We also identify emerging challenges posed by evolving paradigms, such as multi-agent collaboration, multi-modal reasoning, and the development of small open-source medical models. Our goal is to guide future research toward more trustworthy, transparent, and clinically viable LLMs.

## 🔍 Abstract (한글 번역)

arXiv:2502.15871v2 발표 유형: 대체-교차
요약: 대규모 언어 모델(LLMs)의 응용은 임상 의사 결정, 의학 연구 및 환자 치료를 향상시키는 데 상당한 잠재력을 가지고 있습니다. 그러나 이러한 모델들을 현실 세계 임상 환경에 통합하는 것은 신뢰성에 대한 중요한 우려 사항을 제기하며, 특히 진실성, 개인 정보 보호, 안전성, 견고성, 공정성 및 설명 가능성과 같은 차원에 대한 우려가 있습니다. 이러한 차원들은 LLM이 신뢰할 수 있고 편향되지 않은 윤리적으로 타당한 결과를 생성하도록 하는 데 중요합니다. 최근 연구자들은 LLM 신뢰성을 평가하기 위한 벤치마크 및 평가 프레임워크를 개발하기 시작했지만, 의료 분야에서의 LLM 신뢰성은 아직 탐구되지 않았으며, 포괄적인 이해와 미래 전망을 제공하는 체계적 검토가 부족합니다. 본 조사는 주요 신뢰 차원을 횡단하여 위험을 완화하기 위한 현재 방법론 및 솔루션의 포괄적 검토를 제공함으로써 이 간극을 해소합니다. 우리는 각 차원이 의료 분야 LLM의 신뢰성과 윤리적 배치에 어떻게 영향을 미치는지 분석하고, 진행 중인 연구 노력을 종합하고, 기존 접근 방식의 중요한 간극을 식별합니다. 또한, 다중 에이전트 협업, 다중 모달 추론, 소규모 오픈 소스 의료 모델 개발과 같은 진화하는 패러다임에 의해 제기되는 신흥 도전 과제를 식별합니다. 우리의 목표는 미래 연구가 더 신뢰할 수 있고 투명하며 임상적으로 실용적인 LLM으로 향하도록 안내하는 것입니다.

## 📝 요약

최근 건강 관리 분야에서 대형 언어 모델(Large Language Models, LLMs)의 적용은 임상 의사 결정, 의학 연구 및 환자 치료의 향상된 가능성을 제공한다. 그러나 실제 임상 환경에 LLMs를 통합하는 것은 신뢰성 문제로 인해 중요한 우려를 불러일으킨다. 특히 진실성, 개인정보 보호, 안전성, 견고성, 공정성 및 설명 가능성과 같은 차원에서 신뢰성에 대한 우려가 있다. 이 연구는 건강 관리 분야에서 LLMs의 신뢰성을 평가하기 위한 벤치마크와 평가 프레임워크를 개발하는 연구자들이 최근에 시작되었지만, 신뢰성에 대한 체계적인 검토가 부족하여 현재의 방법론과 해결책을 종합적으로 검토하고 미래의 통찰력을 제공한다. 이 조사는 각 차원이 건강 관리 LLMs의 신뢰성과 윤리적 배치에 어떻게 영향을 미치는지 분석하고, 진행 중인 연구 노력을 종합하고, 기존 접근 방식의 중요한 간극을 식별한다. 또한, 다중 에이전트 협업, 다중 모달 추론 및 소규모 오픈 소스 의료 모델 개발과 같은 진화하는 패러다임에 의해 제기되는 신흥 도전 과제를 식별한다. 우리의 목표는 미래 연구가 더욱 신뢰할 수 있고 투명하며 임상적으로 신뢰할 수 있는 LLMs로 이끄는 것이다.

## 🎯 주요 포인트


- 1. 대형 언어 모델(LLMs)의 응용은 임상 의사 결정, 의학 연구 및 환자 치료를 향상시키는 데 중요하다.

- 2. LLM의 신뢰성과 윤리적 측면에 대한 우려는 중요하며, 신뢰성을 평가하기 위한 벤치마크 및 평가 프레임워크가 필요하다.

- 3. 의료 분야에서 LLM의 신뢰성은 아직 탐구되지 않았으며, 현재의 방법론과 해결책을 통해 위험을 완화하는 방법을 살펴본다.


---

*Generated on 2025-09-18 16:31:17*