---
keywords:
  - Compressive Sensing
  - Nonlinear Compressive Sensing
  - Probabilistic Reformulation
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:15:03.769310",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Compressive Sensing",
    "Nonlinear Compressive Sensing",
    "Probabilistic Reformulation"
  ],
  "rejected_keywords": [
    "Best Subset Selection"
  ],
  "similarity_scores": {
    "Compressive Sensing": 0.85,
    "Nonlinear Compressive Sensing": 0.8,
    "Probabilistic Reformulation": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Probabilistic and nonlinear compressive sensing

**Korean Title:** í™•ë¥ ì  ë° ë¹„ì„ í˜• ì••ì¶• ì„¼ì‹±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/Compressive Sensing|compressive sensing]], [[keywords/Nonlinear Compressive Sensing|nonlinear compressive sensing]], [[keywords/Probabilistic Reformulation|probabilistic reformulation]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[A Universal Banach--Bregman Framework for Stochastic Iterations_ Unifying Stochastic Mirror Descent, Learning and LLM Training_20250917|A Universal Banach--Bregman Framework for Stochastic Iterations Unifying Stochastic Mirror Descent, Learning and LLM Training]] (80.6% similar)
- [[Mini-Batch Robustness Verification of Deep Neural Networks_20250919|Mini-Batch Robustness Verification of Deep Neural Networks]] (80.6% similar)
- [[Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (80.4% similar)
- [[Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters_20250919|Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters]] (79.4% similar)
- [[Optimal Algorithms for Bandit Learning in Matching Markets_20250919|Optimal Algorithms for Bandit Learning in Matching Markets]] (78.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Lukas Silvester Barth, Paulo von Petersenn

## ğŸ“„ Abstract (ì›ë¬¸)

We present a smooth probabilistic reformulation of $\ell_0$ regularized
regression that does not require Monte Carlo sampling and allows for the
computation of exact gradients, facilitating rapid convergence to local optima
of the best subset selection problem. The method drastically improves
convergence speed compared to similar Monte Carlo based approaches.
Furthermore, we empirically demonstrate that it outperforms compressive sensing
algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and
signal-to-noise ratios. The implementation runs efficiently on both CPUs and
GPUs and is freely available at
https://github.com/L0-and-behold/probabilistic-nonlinear-cs.
  We also contribute to research on nonlinear generalizations of compressive
sensing by investigating when parameter recovery of a nonlinear teacher network
is possible through compression of a student network. Building upon theorems of
Fefferman and Markel, we show theoretically that the global optimum in the
infinite-data limit enforces recovery up to certain symmetries. For empirical
validation, we implement a normal-form algorithm that selects a canonical
representative within each symmetry class. However, while compression can help
to improve test loss, we find that exact parameter recovery is not even
possible up to symmetries. In particular, we observe a surprising rebound
effect where teacher and student configurations initially converge but
subsequently diverge despite continuous decrease in test loss. These findings
indicate fundamental differences between linear and nonlinear compressive
sensing.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë‹¤ìŒì€ $\ell_0$ ì •ê·œí™” íšŒê·€ì˜ ë§¤ë„ëŸ¬ìš´ í™•ë¥ ì  ì¬êµ¬ì„±ì„ ì œì‹œí•˜ë©°, ì´ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ì„ í•„ìš”ë¡œ í•˜ì§€ ì•Šê³  ì •í™•í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ ìµœì  ë¶€ë¶„ ì§‘í•© ì„ íƒ ë¬¸ì œì˜ ì§€ì—­ ìµœì ì ìœ¼ë¡œì˜ ë¹ ë¥¸ ìˆ˜ë ´ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìœ ì‚¬í•œ ëª¬í…Œì¹´ë¥¼ë¡œ ê¸°ë°˜ ì ‘ê·¼ë²•ì— ë¹„í•´ ìˆ˜ë ´ ì†ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ì„¤ì •ê³¼ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ì—ì„œ IHT ë° (Relaxed-) Lassoì™€ ê°™ì€ ì••ì¶• ì„¼ì‹± ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨ì„ ê²½í—˜ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤. ì´ êµ¬í˜„ì€ CPUì™€ GPU ëª¨ë‘ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, https://github.com/L0-and-behold/probabilistic-nonlinear-csì—ì„œ ë¬´ë£Œë¡œ ì œê³µë©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” ë˜í•œ í•™ìƒ ë„¤íŠ¸ì›Œí¬ì˜ ì••ì¶•ì„ í†µí•´ ë¹„ì„ í˜• êµì‚¬ ë„¤íŠ¸ì›Œí¬ì˜ ë§¤ê°œë³€ìˆ˜ ë³µêµ¬ê°€ ê°€ëŠ¥í•œ ê²½ìš°ë¥¼ ì¡°ì‚¬í•˜ì—¬ ì••ì¶• ì„¼ì‹±ì˜ ë¹„ì„ í˜• ì¼ë°˜í™”ì— ëŒ€í•œ ì—°êµ¬ì— ê¸°ì—¬í•©ë‹ˆë‹¤. Feffermanê³¼ Markelì˜ ì •ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë¬´í•œ ë°ì´í„° í•œê³„ì—ì„œì˜ ì „ì—­ ìµœì ì ì´ íŠ¹ì • ëŒ€ì¹­ê¹Œì§€ ë³µêµ¬ë¥¼ ê°•ì œí•¨ì„ ì´ë¡ ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ê²½í—˜ì  ê²€ì¦ì„ ìœ„í•´, ê° ëŒ€ì¹­ í´ë˜ìŠ¤ ë‚´ì—ì„œ ì •ê·œí˜• ëŒ€í‘œë¥¼ ì„ íƒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì••ì¶•ì´ í…ŒìŠ¤íŠ¸ ì†ì‹¤ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆì§€ë§Œ, ëŒ€ì¹­ê¹Œì§€ë„ ì •í™•í•œ ë§¤ê°œë³€ìˆ˜ ë³µêµ¬ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, êµì‚¬ì™€ í•™ìƒ êµ¬ì„±ì€ ì´ˆê¸°ì—ëŠ” ìˆ˜ë ´í•˜ì§€ë§Œ í…ŒìŠ¤íŠ¸ ì†ì‹¤ì´ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ì´í›„ì— ë°œì‚°í•˜ëŠ” ë†€ë¼ìš´ ë°˜ë“± íš¨ê³¼ë¥¼ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ ì„ í˜• ë° ë¹„ì„ í˜• ì••ì¶• ì„¼ì‹± ê°„ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ $\ell_0$ ì •ê·œí™” íšŒê·€ ë¬¸ì œë¥¼ ë¶€ë“œëŸ¬ìš´ í™•ë¥ ì  ì¬êµ¬ì„±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ ì—†ì´ë„ ì •í™•í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ì˜ ëª¬í…Œì¹´ë¥¼ë¡œ ê¸°ë°˜ ì ‘ê·¼ë²•ë³´ë‹¤ ìˆ˜ë ´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë˜í•œ, IHTì™€ (Relaxed-) Lasso ê°™ì€ ì••ì¶• ì„¼ì‹± ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë‹¤ì–‘í•œ ì„¤ì •ê³¼ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ CPUì™€ GPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ì†ŒìŠ¤ ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¹„ì„ í˜• ì••ì¶• ì„¼ì‹±ì˜ ì¼ë°˜í™” ì—°êµ¬ì—ì„œëŠ” ë¹„ì„ í˜• êµì‚¬ ë„¤íŠ¸ì›Œí¬ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ í•™ìƒ ë„¤íŠ¸ì›Œí¬ì˜ ì••ì¶•ì„ í†µí•´ ë³µêµ¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œëŠ” ë¬´í•œ ë°ì´í„° í•œê³„ì—ì„œ íŠ¹ì • ëŒ€ì¹­ì„±ì„ ì œì™¸í•˜ê³ ëŠ” ë³µêµ¬ê°€ ê°€ëŠ¥í•¨ì„ ë³´ì˜€ìœ¼ë‚˜, ì‹¤í—˜ì ìœ¼ë¡œëŠ” ëŒ€ì¹­ì„±ì„ ê³ ë ¤í•´ë„ ì •í™•í•œ ë§¤ê°œë³€ìˆ˜ ë³µêµ¬ê°€ ë¶ˆê°€ëŠ¥í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, êµì‚¬ì™€ í•™ìƒ ë„¤íŠ¸ì›Œí¬ê°€ ì´ˆê¸°ì—ëŠ” ìˆ˜ë ´í•˜ë‹¤ê°€ ì´í›„ì— ë°œì‚°í•˜ëŠ” í˜„ìƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì„ í˜•ê³¼ ë¹„ì„ í˜• ì••ì¶• ì„¼ì‹± ê°„ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ë¥¼ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. $\ell_0$ ì •ê·œí™” íšŒê·€ì˜ í™•ë¥ ì  ì¬êµ¬ì„±ì„ í†µí•´ ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ ì—†ì´ ì •í™•í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ìµœì ì˜ ë¶€ë¶„ ì§‘í•© ì„ íƒ ë¬¸ì œì˜ ë¹ ë¥¸ ìˆ˜ë ´ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

- 2. ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ì˜ ëª¬í…Œì¹´ë¥¼ë¡œ ê¸°ë°˜ ì ‘ê·¼ë²•ì— ë¹„í•´ ìˆ˜ë ´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 3. ë‹¤ì–‘í•œ ì„¤ì •ê³¼ ì‹ í˜¸ ëŒ€ ì¡ìŒ ë¹„ìœ¨ì—ì„œ IHT ë° (Relaxed-) Lassoì™€ ê°™ì€ ì••ì¶• ì„¼ì‹± ì•Œê³ ë¦¬ì¦˜ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

- 4. ë¹„ì„ í˜• ì••ì¶• ì„¼ì‹±ì˜ ì¼ë°˜í™” ì—°êµ¬ì— ê¸°ì—¬í•˜ë©°, ë¹„ì„ í˜• êµì‚¬ ë„¤íŠ¸ì›Œí¬ì˜ ë§¤ê°œë³€ìˆ˜ ë³µêµ¬ê°€ í•™ìƒ ë„¤íŠ¸ì›Œí¬ì˜ ì••ì¶•ì„ í†µí•´ ê°€ëŠ¥í•œì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤.

- 5. ë¹„ì„ í˜• ì••ì¶• ì„¼ì‹±ì—ì„œ êµì‚¬ì™€ í•™ìƒ ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì„±ì€ ì´ˆê¸°ì—ëŠ” ìˆ˜ë ´í•˜ì§€ë§Œ ì´í›„ì—ëŠ” í…ŒìŠ¤íŠ¸ ì†ì‹¤ì´ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë¶„ê¸°í•˜ëŠ” ë°˜ë™ íš¨ê³¼ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 01:05:50*