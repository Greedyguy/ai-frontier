---
keywords:
  - Transformer Architecture
  - Encoder-based Classifiers
  - Decoder-based Models
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:42:01.146084",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer Architecture",
    "Encoder-based Classifiers",
    "Decoder-based Models"
  ],
  "rejected_keywords": [
    "Social Bot Detection",
    "Large Language Models"
  ],
  "similarity_scores": {
    "Transformer Architecture": 0.8,
    "Encoder-based Classifiers": 0.78,
    "Decoder-based Models": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# A Comparative Analysis of Transformer Models in Social Bot Detection

**Korean Title:** ì†Œì…œ ë´‡ íƒì§€ì—ì„œì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¹„êµ ë¶„ì„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Transformer Architecture|Transformer Models]]
**âš¡ Unique Technical**: [[keywords/Encoder-based Classifiers|Encoder-based Classifiers]], [[keywords/Decoder-based Models|Decoder-based Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[SMARTER_ A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models_20250919|SMARTER A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models]] (78.4% similar)
- [[An Attention-Based Denoising Framework for Personality Detection in Social Media Texts_20250918|An Attention-Based Denoising Framework for Personality Detection in Social Media Texts]] (78.3% similar)
- [[Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection_20250918|Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection]] (78.1% similar)
- [[DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (77.9% similar)
- [[Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection_20250919|Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection]] (77.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Rohan Veit, Michael Lones

## ğŸ“„ Abstract (ì›ë¬¸)

Social media has become a key medium of communication in today's society.
This realisation has led to many parties employing artificial users (or bots)
to mislead others into believing untruths or acting in a beneficial manner to
such parties. Sophisticated text generation tools, such as large language
models, have further exacerbated this issue. This paper aims to compare the
effectiveness of bot detection models based on encoder and decoder
transformers. Pipelines are developed to evaluate the performance of these
classifiers, revealing that encoder-based classifiers demonstrate greater
accuracy and robustness. However, decoder-based models showed greater
adaptability through task-specific alignment, suggesting more potential for
generalisation across different use cases in addition to superior observa.
These findings contribute to the ongoing effort to prevent digital environments
being manipulated while protecting the integrity of online discussion.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì†Œì…œ ë¯¸ë””ì–´ëŠ” ì˜¤ëŠ˜ë‚  ì‚¬íšŒì—ì„œ ì¤‘ìš”í•œ ì˜ì‚¬ì†Œí†µ ìˆ˜ë‹¨ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¸ì‹ì€ ë§ì€ ë‹¹ì‚¬ìë“¤ì´ ì¸ê³µ ì‚¬ìš©ì(ë˜ëŠ” ë´‡)ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ì˜¤ë„í•˜ê³  ê±°ì§“ì„ ë¯¿ê²Œ í•˜ê±°ë‚˜ í•´ë‹¹ ë‹¹ì‚¬ìë“¤ì—ê²Œ ìœ ë¦¬í•œ ë°©ì‹ìœ¼ë¡œ í–‰ë™í•˜ë„ë¡ ìœ ë„í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ê°™ì€ ì •êµí•œ í…ìŠ¤íŠ¸ ìƒì„± ë„êµ¬ëŠ” ì´ ë¬¸ì œë¥¼ ë”ìš± ì•…í™”ì‹œì¼°ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì¸ì½”ë”ì™€ ë””ì½”ë” ë³€í™˜ê¸°ì— ê¸°ë°˜í•œ ë´‡ íƒì§€ ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ íŒŒì´í”„ë¼ì¸ì´ ê°œë°œë˜ì—ˆìœ¼ë©°, ì¸ì½”ë” ê¸°ë°˜ ë¶„ë¥˜ê¸°ê°€ ë” ë†’ì€ ì •í™•ì„±ê³¼ ê²¬ê³ ì„±ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ê³¼ì œë³„ ì •ë ¬ì„ í†µí•´ ë” í° ì ì‘ì„±ì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ê±¸ì³ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë” ë§ì€ ì ì¬ë ¥ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ ë””ì§€í„¸ í™˜ê²½ì´ ì¡°ì‘ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  ì˜¨ë¼ì¸ í† ë¡ ì˜ ë¬´ê²°ì„±ì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ ì§€ì†ì ì¸ ë…¸ë ¥ì— ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì†Œì…œ ë¯¸ë””ì–´ì—ì„œ ë´‡ì„ íƒì§€í•˜ê¸° ìœ„í•œ ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ì¸ì½”ë”ì™€ ë””ì½”ë” ê¸°ë°˜ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ì¸ì½”ë” ê¸°ë°˜ ë¶„ë¥˜ê¸°ê°€ ë” ë†’ì€ ì •í™•ì„±ê³¼ ê²¬ê³ ì„±ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë°˜ë©´, ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ íŠ¹ì • ì‘ì—…ì— ë§ì¶˜ ì ì‘ë ¥ì´ ë›°ì–´ë‚˜ ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ ë””ì§€í„¸ í™˜ê²½ì˜ ì¡°ì‘ì„ ë°©ì§€í•˜ê³  ì˜¨ë¼ì¸ í† ë¡ ì˜ ë¬´ê²°ì„±ì„ ë³´í˜¸í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì†Œì…œ ë¯¸ë””ì–´ì—ì„œ ì¸ê³µì§€ëŠ¥ ì‚¬ìš©ì(ë´‡)ë¥¼ í†µí•œ ì˜¤ì •ë³´ í™•ì‚° ë¬¸ì œê°€ ì‹¬ê°í•´ì§€ê³  ìˆë‹¤.

- 2. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ê°™ì€ ì •êµí•œ í…ìŠ¤íŠ¸ ìƒì„± ë„êµ¬ê°€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë”ìš± ì•…í™”ì‹œí‚¤ê³  ìˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì€ ì¸ì½”ë” ë° ë””ì½”ë” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ë´‡ íƒì§€ ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•œë‹¤.

- 4. ì¸ì½”ë” ê¸°ë°˜ ë¶„ë¥˜ê¸°ê°€ ë” ë†’ì€ ì •í™•ì„±ê³¼ ê°•ê±´ì„±ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.

- 5. ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ ê³¼ì œë³„ ì •ë ¬ì„ í†µí•´ ë” í° ì ì‘ì„±ì„ ë³´ì—¬ ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì—ì„œì˜ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•œë‹¤.

---

*Generated on 2025-09-20 02:42:18*