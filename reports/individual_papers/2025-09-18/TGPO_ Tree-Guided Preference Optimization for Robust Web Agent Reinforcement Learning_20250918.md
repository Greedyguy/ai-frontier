---
keywords:
  - Reinforcement Learning
  - Large Language Models
  - Vision-Language Models
category: cs.AI
publish_date: 2025-09-18
arxiv_id: 2509.14172
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:07:35.990802",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Large Language Models",
    "Vision-Language Models"
  ],
  "rejected_keywords": [
    "Tree-Guided Preference Optimization"
  ],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Large Language Models": 0.8,
    "Vision-Language Models": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning

**Korean Title:** TGPO: ê°•ê±´í•œ ì›¹ ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµì„ ìœ„í•œ íŠ¸ë¦¬ ê°€ì´ë“œ ì„ í˜¸ ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]], [[keywords/Vision-Language Models|Vision-Language Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[AgentCTG Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation]] (80.5% similar)
- [[Controllable Pareto Trade-off between Fairness and Accuracy]] (80.3% similar)
- [[Video-Language Critic Transferable Reward Functions for Language-Conditioned Robotics]] (80.1% similar)
- [[DRDT3 Diffusion-Refined Decision Test-Time Training Model]] (79.8% similar)
- [[THOR Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning]] (79.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14172v1 Announce Type: cross 
Abstract: With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14172v1 ë°œí‘œ ìœ í˜•: cross 
ì´ˆë¡: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ, ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì„ ìœ„í•´ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì›¹ ì—ì´ì „íŠ¸ë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì´ ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°•í™”í•™ìŠµìœ¼ë¡œ ì›¹ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ì‹ ìš© í• ë‹¹ ì˜¤ë¶„ë°°, ê·¹ë„ë¡œ ë†’ì€ ì£¼ì„ ë¹„ìš©, ê·¸ë¦¬ê³  ë³´ìƒ í¬ì†Œì„±ê³¼ ê°™ì€ ì¤‘ìš”í•œ ë¬¸ì œë“¤ì— ì§ë©´í•˜ê³  ìˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Tree-Guided Preference Optimization (TGPO)ì„ ì œì•ˆí•œë‹¤. ì´ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœë“¤ì„ ê¶¤ì ë“¤ ê°„ì— ë³‘í•©í•˜ì—¬ ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡° ê¶¤ì  í‘œí˜„ì„ ì œì•ˆí•˜ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬ì´ë‹¤. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” í•˜ìœ„ ëª©í‘œ ì§„í–‰ë„, ì¤‘ë³µì„± íƒì§€, ê·¸ë¦¬ê³  í–‰ë™ ê²€ì¦ì„ í†µí•´ ìë™ìœ¼ë¡œ ì„¸ë°€í•œ ë³´ìƒì„ ìƒì„±í•˜ëŠ” Process Reward Modelì„ í¬í•¨í•œë‹¤. ë˜í•œ, ë™ì  ê°€ì¤‘ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì´ í›ˆë ¨ ì¤‘ ë†’ì€ ì˜í–¥ë ¥ì„ ê°€ì§„ ê²°ì • ì§€ì ë“¤ì— ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬í•œë‹¤. Online-Mind2Webê³¼ ìš°ë¦¬ê°€ ìì²´ êµ¬ì¶•í•œ C-WebShop ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ì€ TGPOê°€ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ìƒë‹¹íˆ ëŠ¥ê°€í•˜ë©°, ë” ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¡œ ë” ë†’ì€ ì„±ê³µë¥ ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤€ë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ì— ë”°ë¼ ì›¹ ì—ì´ì „íŠ¸ë¡œì„œì˜ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°•í™” í•™ìŠµì„ í†µí•œ ì›¹ ì—ì´ì „íŠ¸ í›ˆë ¨ì€ ì—¬ëŸ¬ ë¬¸ì œì ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ Tree-Guided Preference Optimization (TGPO)ë¼ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. TGPOëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê¶¤ì  í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ê³ , í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ í†µí•´ ì„¸ë¶€ì ì¸ ë³´ìƒì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤. ë˜í•œ, ë™ì  ê°€ì¤‘ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í›ˆë ¨ ì‹œ ì¤‘ìš”í•œ ê²°ì • ì§€ì ì— ìš°ì„ ìˆœìœ„ë¥¼ ë‘¡ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TGPOëŠ” ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ê³¼ ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ ì›¹ ì—ì´ì „íŠ¸ì˜ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì´ ì¤‘ìš”í•´ì¡ŒìŠµë‹ˆë‹¤.

- 2. ì›¹ ì—ì´ì „íŠ¸ì˜ ê°•í™” í•™ìŠµì—ëŠ” ë³´ìƒ í• ë‹¹ ì˜¤ë¥˜, ë†’ì€ ì£¼ì„ ë¹„ìš©, ë³´ìƒ í¬ì†Œì„± ë“±ì˜ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

- 3. Tree-Guided Preference Optimization (TGPO)ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê²½ë¡œ í‘œí˜„ì„ í†µí•´ ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•©ë‹ˆë‹¤.

- 4. TGPOëŠ” ì„¸ë¶€ ë³´ìƒì„ ìë™ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ í¬í•¨í•˜ì—¬ í•™ìŠµ ì¤‘ ì¤‘ìš”í•œ ê²°ì • ì§€ì ì„ ìš°ì„ ì‹œí•˜ëŠ” ë™ì  ê°€ì¤‘ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼ TGPOëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ê³¼ ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-19 10:50:17*