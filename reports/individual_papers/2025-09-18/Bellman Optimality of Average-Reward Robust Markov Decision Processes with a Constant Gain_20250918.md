
# Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain

**Korean Title:** 상수 이득을 갖는 평균 보상 강건 마르코프 의사결정 과정의 벨만 최적성

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Average Reward Robust MDPs

## 🔗 유사한 논문
- [[Accelerated Gradient Methods with Biased Gradient Estimates Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (79.5% similar)
- [[Data-Driven_Distributed_Optimization_via_Aggregative_Tracking_and_Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (78.2% similar)
- [[Distributionally_Robust_Equilibria_over_the_Wasserstein_Distance_for_Generalized_Nash_Game_20250918|Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game]] (77.8% similar)
- [[Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation]] (77.6% similar)
- [[Graph Feedback Bandits on Similar Arms With and Without Graph Structures]] (77.1% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14203v1 Announce Type: cross 
Abstract: Learning and optimal control under robust Markov decision processes (MDPs) have received increasing attention, yet most existing theory, algorithms, and applications focus on finite-horizon or discounted models. The average-reward formulation, while natural in many operations research and management contexts, remains underexplored. This is primarily because the dynamic programming foundations are technically challenging and only partially understood, with several fundamental questions remaining open. This paper steps toward a general framework for average-reward robust MDPs by analyzing the constant-gain setting. We study the average-reward robust control problem with possible information asymmetries between the controller and an S-rectangular adversary. Our analysis centers on the constant-gain robust Bellman equation, examining both the existence of solutions and their relationship to the optimal average reward. Specifically, we identify when solutions to the robust Bellman equation characterize the optimal average reward and stationary policies, and we provide sufficient conditions ensuring solutions' existence. These findings expand the dynamic programming theory for average-reward robust MDPs and lay a foundation for robust dynamic decision making under long-run average criteria in operational environments.

## 🔍 Abstract (한글 번역)

arXiv:2509.14203v1 발표 유형: cross 
초록: 강건한 마르코프 결정 과정(MDP) 하에서의 학습과 최적 제어는 점점 더 많은 관심을 받고 있으나, 기존 이론, 알고리즘, 응용의 대부분은 유한 수평선(finite-horizon) 또는 할인된 모델에 초점을 맞추고 있다. 평균 보상 공식화는 많은 운영 연구 및 경영 맥락에서 자연스러우면서도 여전히 충분히 탐구되지 않고 있다. 이는 주로 동적 계획법의 기초가 기술적으로 어렵고 부분적으로만 이해되어 있으며, 몇 가지 근본적인 질문들이 여전히 미해결 상태로 남아있기 때문이다. 본 논문은 상수 이득(constant-gain) 설정을 분석함으로써 평균 보상 강건한 MDP에 대한 일반적인 프레임워크를 향한 한 걸음을 내딛는다. 우리는 제어기와 S-직사각형 적대자(S-rectangular adversary) 간의 가능한 정보 비대칭성을 가진 평균 보상 강건한 제어 문제를 연구한다. 우리의 분석은 상수 이득 강건한 벨만 방정식을 중심으로 하며, 해의 존재성과 최적 평균 보상과의 관계를 모두 검토한다. 구체적으로, 우리는 강건한 벨만 방정식의 해가 언제 최적 평균 보상과 정상 정책을 특성화하는지를 규명하고, 해의 존재를 보장하는 충분조건을 제시한다. 이러한 발견들은 평균 보상 강건한 MDP에 대한 동적 계획법 이론을 확장하고, 운영 환경에서 장기 평균 기준 하의 강건한 동적 의사결정을 위한 토대를 마련한다.

## 📝 요약

이 논문은 평균 보상 강건 마르코프 결정 과정(MDP)에 대한 일반적인 프레임워크를 제안합니다. 기존 연구는 주로 유한 시간 또는 할인 모델에 집중되어 있지만, 이 연구는 평균 보상 모델의 중요성을 강조합니다. 특히, 상수 이득 설정에서 강건 벨만 방정식을 분석하여 최적 평균 보상과 정적 정책을 특성화하는 조건을 식별합니다. 또한, 해의 존재를 보장하는 충분 조건을 제시하여 평균 보상 강건 MDP의 동적 프로그래밍 이론을 확장하고, 운영 환경에서 장기 평균 기준 하의 강건한 동적 의사결정의 기초를 마련합니다.

## 🎯 주요 포인트

- 1. 평균 보상 공식은 많은 운영 연구 및 관리 맥락에서 자연스럽지만, 기술적 도전으로 인해 충분히 탐구되지 않았다.

- 2. 본 논문은 상수 이득 설정을 분석하여 평균 보상 강건 마르코프 결정 과정(MDPs)을 위한 일반적인 프레임워크를 제시한다.

- 3. 상수 이득 강건 벨만 방정식의 해 존재 여부와 최적 평균 보상과의 관계를 분석하였다.

- 4. 강건 벨만 방정식의 해가 최적 평균 보상과 정적 정책을 특징짓는 조건을 식별하고, 해의 존재를 보장하는 충분 조건을 제공하였다.

- 5. 이러한 결과는 평균 보상 강건 MDPs에 대한 동적 프로그래밍 이론을 확장하고, 운영 환경에서 장기 평균 기준 하의 강건한 동적 의사 결정의 기초를 마련한다.

---

*Generated on 2025-09-19 11:33:52*