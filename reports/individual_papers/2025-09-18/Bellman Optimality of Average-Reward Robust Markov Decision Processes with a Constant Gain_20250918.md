
# Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain

**Korean Title:** ìƒìˆ˜ ì´ë“ì„ ê°–ëŠ” í‰ê·  ë³´ìƒ ê°•ê±´ ë§ˆë¥´ì½”í”„ ì˜ì‚¬ê²°ì • ê³¼ì •ì˜ ë²¨ë§Œ ìµœì ì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Average Reward Robust MDPs

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Accelerated Gradient Methods with Biased Gradient Estimates Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (79.5% similar)
- [[Data-Driven_Distributed_Optimization_via_Aggregative_Tracking_and_Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (78.2% similar)
- [[Distributionally_Robust_Equilibria_over_the_Wasserstein_Distance_for_Generalized_Nash_Game_20250918|Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game]] (77.8% similar)
- [[Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation]] (77.6% similar)
- [[Graph Feedback Bandits on Similar Arms With and Without Graph Structures]] (77.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14203v1 Announce Type: cross 
Abstract: Learning and optimal control under robust Markov decision processes (MDPs) have received increasing attention, yet most existing theory, algorithms, and applications focus on finite-horizon or discounted models. The average-reward formulation, while natural in many operations research and management contexts, remains underexplored. This is primarily because the dynamic programming foundations are technically challenging and only partially understood, with several fundamental questions remaining open. This paper steps toward a general framework for average-reward robust MDPs by analyzing the constant-gain setting. We study the average-reward robust control problem with possible information asymmetries between the controller and an S-rectangular adversary. Our analysis centers on the constant-gain robust Bellman equation, examining both the existence of solutions and their relationship to the optimal average reward. Specifically, we identify when solutions to the robust Bellman equation characterize the optimal average reward and stationary policies, and we provide sufficient conditions ensuring solutions' existence. These findings expand the dynamic programming theory for average-reward robust MDPs and lay a foundation for robust dynamic decision making under long-run average criteria in operational environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14203v1 ë°œí‘œ ìœ í˜•: cross 
ì´ˆë¡: ê°•ê±´í•œ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP) í•˜ì—ì„œì˜ í•™ìŠµê³¼ ìµœì  ì œì–´ëŠ” ì ì  ë” ë§ì€ ê´€ì‹¬ì„ ë°›ê³  ìˆìœ¼ë‚˜, ê¸°ì¡´ ì´ë¡ , ì•Œê³ ë¦¬ì¦˜, ì‘ìš©ì˜ ëŒ€ë¶€ë¶„ì€ ìœ í•œ ìˆ˜í‰ì„ (finite-horizon) ë˜ëŠ” í• ì¸ëœ ëª¨ë¸ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤. í‰ê·  ë³´ìƒ ê³µì‹í™”ëŠ” ë§ì€ ìš´ì˜ ì—°êµ¬ ë° ê²½ì˜ ë§¥ë½ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš°ë©´ì„œë„ ì—¬ì „íˆ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šê³  ìˆë‹¤. ì´ëŠ” ì£¼ë¡œ ë™ì  ê³„íšë²•ì˜ ê¸°ì´ˆê°€ ê¸°ìˆ ì ìœ¼ë¡œ ì–´ë µê³  ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì´í•´ë˜ì–´ ìˆìœ¼ë©°, ëª‡ ê°€ì§€ ê·¼ë³¸ì ì¸ ì§ˆë¬¸ë“¤ì´ ì—¬ì „íˆ ë¯¸í•´ê²° ìƒíƒœë¡œ ë‚¨ì•„ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë³¸ ë…¼ë¬¸ì€ ìƒìˆ˜ ì´ë“(constant-gain) ì„¤ì •ì„ ë¶„ì„í•¨ìœ¼ë¡œì¨ í‰ê·  ë³´ìƒ ê°•ê±´í•œ MDPì— ëŒ€í•œ ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ í–¥í•œ í•œ ê±¸ìŒì„ ë‚´ë”›ëŠ”ë‹¤. ìš°ë¦¬ëŠ” ì œì–´ê¸°ì™€ S-ì§ì‚¬ê°í˜• ì ëŒ€ì(S-rectangular adversary) ê°„ì˜ ê°€ëŠ¥í•œ ì •ë³´ ë¹„ëŒ€ì¹­ì„±ì„ ê°€ì§„ í‰ê·  ë³´ìƒ ê°•ê±´í•œ ì œì–´ ë¬¸ì œë¥¼ ì—°êµ¬í•œë‹¤. ìš°ë¦¬ì˜ ë¶„ì„ì€ ìƒìˆ˜ ì´ë“ ê°•ê±´í•œ ë²¨ë§Œ ë°©ì •ì‹ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ë©°, í•´ì˜ ì¡´ì¬ì„±ê³¼ ìµœì  í‰ê·  ë³´ìƒê³¼ì˜ ê´€ê³„ë¥¼ ëª¨ë‘ ê²€í† í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê°•ê±´í•œ ë²¨ë§Œ ë°©ì •ì‹ì˜ í•´ê°€ ì–¸ì œ ìµœì  í‰ê·  ë³´ìƒê³¼ ì •ìƒ ì •ì±…ì„ íŠ¹ì„±í™”í•˜ëŠ”ì§€ë¥¼ ê·œëª…í•˜ê³ , í•´ì˜ ì¡´ì¬ë¥¼ ë³´ì¥í•˜ëŠ” ì¶©ë¶„ì¡°ê±´ì„ ì œì‹œí•œë‹¤. ì´ëŸ¬í•œ ë°œê²¬ë“¤ì€ í‰ê·  ë³´ìƒ ê°•ê±´í•œ MDPì— ëŒ€í•œ ë™ì  ê³„íšë²• ì´ë¡ ì„ í™•ì¥í•˜ê³ , ìš´ì˜ í™˜ê²½ì—ì„œ ì¥ê¸° í‰ê·  ê¸°ì¤€ í•˜ì˜ ê°•ê±´í•œ ë™ì  ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ í† ëŒ€ë¥¼ ë§ˆë ¨í•œë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í‰ê·  ë³´ìƒ ê°•ê±´ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì— ëŒ€í•œ ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ìœ í•œ ì‹œê°„ ë˜ëŠ” í• ì¸ ëª¨ë¸ì— ì§‘ì¤‘ë˜ì–´ ìˆì§€ë§Œ, ì´ ì—°êµ¬ëŠ” í‰ê·  ë³´ìƒ ëª¨ë¸ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. íŠ¹íˆ, ìƒìˆ˜ ì´ë“ ì„¤ì •ì—ì„œ ê°•ê±´ ë²¨ë§Œ ë°©ì •ì‹ì„ ë¶„ì„í•˜ì—¬ ìµœì  í‰ê·  ë³´ìƒê³¼ ì •ì  ì •ì±…ì„ íŠ¹ì„±í™”í•˜ëŠ” ì¡°ê±´ì„ ì‹ë³„í•©ë‹ˆë‹¤. ë˜í•œ, í•´ì˜ ì¡´ì¬ë¥¼ ë³´ì¥í•˜ëŠ” ì¶©ë¶„ ì¡°ê±´ì„ ì œì‹œí•˜ì—¬ í‰ê·  ë³´ìƒ ê°•ê±´ MDPì˜ ë™ì  í”„ë¡œê·¸ë˜ë° ì´ë¡ ì„ í™•ì¥í•˜ê³ , ìš´ì˜ í™˜ê²½ì—ì„œ ì¥ê¸° í‰ê·  ê¸°ì¤€ í•˜ì˜ ê°•ê±´í•œ ë™ì  ì˜ì‚¬ê²°ì •ì˜ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í‰ê·  ë³´ìƒ ê³µì‹ì€ ë§ì€ ìš´ì˜ ì—°êµ¬ ë° ê´€ë¦¬ ë§¥ë½ì—ì„œ ìì—°ìŠ¤ëŸ½ì§€ë§Œ, ê¸°ìˆ ì  ë„ì „ìœ¼ë¡œ ì¸í•´ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.

- 2. ë³¸ ë…¼ë¬¸ì€ ìƒìˆ˜ ì´ë“ ì„¤ì •ì„ ë¶„ì„í•˜ì—¬ í‰ê·  ë³´ìƒ ê°•ê±´ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDPs)ì„ ìœ„í•œ ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•œë‹¤.

- 3. ìƒìˆ˜ ì´ë“ ê°•ê±´ ë²¨ë§Œ ë°©ì •ì‹ì˜ í•´ ì¡´ì¬ ì—¬ë¶€ì™€ ìµœì  í‰ê·  ë³´ìƒê³¼ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì˜€ë‹¤.

- 4. ê°•ê±´ ë²¨ë§Œ ë°©ì •ì‹ì˜ í•´ê°€ ìµœì  í‰ê·  ë³´ìƒê³¼ ì •ì  ì •ì±…ì„ íŠ¹ì§•ì§“ëŠ” ì¡°ê±´ì„ ì‹ë³„í•˜ê³ , í•´ì˜ ì¡´ì¬ë¥¼ ë³´ì¥í•˜ëŠ” ì¶©ë¶„ ì¡°ê±´ì„ ì œê³µí•˜ì˜€ë‹¤.

- 5. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” í‰ê·  ë³´ìƒ ê°•ê±´ MDPsì— ëŒ€í•œ ë™ì  í”„ë¡œê·¸ë˜ë° ì´ë¡ ì„ í™•ì¥í•˜ê³ , ìš´ì˜ í™˜ê²½ì—ì„œ ì¥ê¸° í‰ê·  ê¸°ì¤€ í•˜ì˜ ê°•ê±´í•œ ë™ì  ì˜ì‚¬ ê²°ì •ì˜ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•œë‹¤.

---

*Generated on 2025-09-19 11:33:52*