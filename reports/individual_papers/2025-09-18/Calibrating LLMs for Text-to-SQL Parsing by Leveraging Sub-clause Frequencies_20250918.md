---
keywords:
  - Large Language Models
  - Text-to-SQL Parsing
  - Sub-clause Frequency
category: cs.AI
publish_date: 2025-09-18
arxiv_id: 2505.23804
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:35:45.030820",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Text-to-SQL Parsing",
    "Sub-clause Frequency"
  ],
  "rejected_keywords": [
    "Platt Scaling",
    "Uncertainty Quantification"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Text-to-SQL Parsing": 0.78,
    "Sub-clause Frequency": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies

**Korean Title:** 하위 절 빈도를 활용한 텍스트-투-SQL 파싱을 위한 LLM 보정

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250918|2025-09-18]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**⚡ Unique Technical**: [[keywords/Text-to-SQL Parsing|Text-to-SQL Parsing]], [[keywords/Sub-clause Frequency|Sub-clause Frequency]]
**🚀 Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## 🔗 유사한 논문
- [[Crash Report Enhancement with Large Language Models An Empirical Study]] (80.1% similar)
- [[Evaluating_and_Improving_the_Robustness_of_Security_Attack_Detectors_Generated_by_LLMs_20250918|Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs]] (79.9% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (79.6% similar)
- [[Using LLMs in Generating Design Rationale for Software Architecture Decisions]] (79.6% similar)
- [[Post-Hoc_Split-Point_Self-Consistency_Verification_for_Efficient,_Unified_Quantification_of_Aleatoric_and_Epistemic_Uncertainty_in_Deep_Learning_20250918|Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning]] (79.1% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.23804v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.

## 🔍 Abstract (한글 번역)

arXiv:2505.23804v2 발표 유형: 교차 대체
초록: 대규모 언어 모델(LLM)이 텍스트-SQL 파싱에서 강력한 성능을 달성하지만, 때때로 확신에 찬 잘못된 답변을 제시하는 예상치 못한 실패를 보인다. 따라서 신뢰할 수 있는 텍스트-SQL 시스템을 구축하려면 LLM으로부터 신뢰할 수 있는 불확실성 측정치를 도출해야 한다. 본 논문에서는 출력 쿼리가 올바를 가능성을 전달하는 보정된 신뢰도 점수를 제공하는 문제를 연구한다. 우리의 연구는 LLM 기반 텍스트-SQL 파싱의 사후 보정을 위한 벤치마크를 최초로 확립한다. 특히, 보정을 위한 표준 방법인 Platt 스케일링이 원시 모델 출력 확률을 신뢰도 점수로 직접 사용하는 것보다 상당한 개선을 제공함을 보여준다. 나아가, SQL 쿼리의 구조화된 특성을 활용하여 더욱 세분화된 정확성 신호를 제공하는 텍스트-SQL 보정 방법을 제안하며, 이를 "부절 빈도"(SCF) 점수라고 명명한다. 표준 Platt 스케일링 기법의 확장인 다변량 Platt 스케일링(MPS)을 사용하여 개별 SCF 점수들을 전체적으로 정확하고 보정된 점수로 결합한다. 두 개의 인기 있는 텍스트-SQL 데이터셋에 대한 실증적 평가는 MPS와 SCF를 결합한 우리의 접근법이 전통적인 Platt 스케일링보다 보정 및 관련 오류 감지 작업에서 추가적인 개선을 제공함을 보여준다.

## 📝 요약

이 논문은 대형 언어 모델(LLM)을 활용한 텍스트-to-SQL 파싱에서 신뢰할 수 있는 불확실성 측정을 통해 시스템의 신뢰성을 높이는 방법을 연구합니다. 주요 기여는 LLM 기반 텍스트-to-SQL 파싱의 사후 보정 벤치마크를 최초로 제시한 것입니다. Platt 스케일링을 사용하여 모델 출력 확률보다 향상된 보정 점수를 제공하며, SQL 쿼리의 구조적 특성을 활용한 "서브-클라우스 빈도(SCF)" 점수를 제안합니다. 이 점수는 다변량 Platt 스케일링(MPS)과 결합되어 정확하고 보정된 점수를 제공합니다. 두 개의 텍스트-to-SQL 데이터셋에 대한 실험 결과, MPS와 SCF의 결합이 전통적인 Platt 스케일링보다 보정 및 오류 탐지에서 더 나은 성능을 보였습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 텍스트에서 SQL로의 파싱에서 강력한 성능을 보이지만, 때때로 확신에 찬 오류를 범할 수 있습니다.

- 2. LLM 기반 텍스트-투-SQL 파싱의 사후 보정 벤치마크를 최초로 수립하였습니다.

- 3. Platt 스케일링은 모델 출력 확률을 직접 사용하는 것보다 신뢰도 점수 보정에 있어 상당한 개선을 제공합니다.

- 4. SQL 쿼리의 구조적 특성을 활용한 "서브-클라우스 빈도"(SCF) 점수를 제안하여 정확성의 세분화된 신호를 제공합니다.

- 5. 다변량 Platt 스케일링(MPS)과 SCF 점수를 결합하여 보정 및 오류 탐지 작업에서 전통적인 Platt 스케일링보다 더 나은 성능을 보였습니다.

---

*Generated on 2025-09-19 11:07:18*