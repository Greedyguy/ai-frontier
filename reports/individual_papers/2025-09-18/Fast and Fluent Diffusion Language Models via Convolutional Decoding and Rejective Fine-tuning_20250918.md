# Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning

**Korean Title:** ë¹ ë¥´ê³  ìœ ì°½í•œ í™•ì‚° ì–¸ì–´ ëª¨ë¸: í•©ì„±ê³± ë””ì½”ë”© ë° ê±°ë¶€ì  ë¯¸ì„¸ ì¡°ì •ì„ í†µí•œ ì ‘ê·¼

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Yeongbin Seo|Yeongbin Seo]] [[authors/Dongha Lee|Dongha Lee]] [[authors/Jaehyung Kim|Jaehyung Kim]] [[authors/Jinyoung Yeo|Jinyoung Yeo]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Diffusion Language Models

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Hybrid Autoregressive-Diffusion Model for Real-Time Sign Language Production_20250919|Hybrid Autoregressive-Diffusion Model for Real-Time Sign Language Production]] (81.8% similar)
- [[LNE-Blocking_ An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models_20250918|LNE-Blocking An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models]] (81.3% similar)
- [[Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (81.3% similar)
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (81.1% similar)
- [[Evolving Language Models without Labels_ Majority Drives Selection, Novelty Promotes Variation_20250918|Evolving Language Models without Labels Majority Drives Selection, Novelty Promotes Variation]] (81.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo

## ğŸ“„ Abstract (ì›ë¬¸)

Autoregressive (AR) language models generate text one token at a time, which
limits their inference speed. Diffusion-based language models offer a promising
alternative, as they can decode multiple tokens in parallel. However, we
identify a key bottleneck in current diffusion LMs: the long decoding-window
problem, where tokens generated far from the input context often become
irrelevant or repetitive. Previous solutions like semi-autoregressive address
this issue by splitting windows into blocks, but this sacrifices speed and
bidirectionality, eliminating the main advantage of diffusion models. To
overcome this, we propose Convolutional decoding (Conv), a normalization-based
method that narrows the decoding window without hard segmentation, leading to
better fluency and flexibility. Additionally, we introduce Rejecting Rule-based
Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at
positions far from context. Our methods achieve state-of-the-art results on
open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM
baselines, with significantly lower step size than previous works,
demonstrating both speed and quality improvements.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ìê¸°íšŒê·€(AR) ì–¸ì–´ ëª¨ë¸ì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì„ ìƒì„±í•˜ì—¬ ì¶”ë¡  ì†ë„ê°€ ì œí•œë©ë‹ˆë‹¤. ë°˜ë©´, í™•ì‚° ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì€ ì—¬ëŸ¬ í† í°ì„ ë³‘ë ¬ë¡œ ë””ì½”ë”©í•  ìˆ˜ ìˆì–´ ìœ ë§í•œ ëŒ€ì•ˆì´ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì¬ì˜ í™•ì‚° ì–¸ì–´ ëª¨ë¸ì—ì„œ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: ê¸´ ë””ì½”ë”© ìœˆë„ìš° ë¬¸ì œë¡œ, ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ ìœ„ì¹˜ì—ì„œ ìƒì„±ëœ í† í°ì´ ì¢…ì¢… ê´€ë ¨ì„±ì´ ì—†ê±°ë‚˜ ë°˜ë³µì ì´ê²Œ ë©ë‹ˆë‹¤. ì´ì „ì˜ í•´ê²°ì±…ì¸ ë°˜ìê¸°íšŒê·€ ë°©ì‹ì€ ìœˆë„ìš°ë¥¼ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ë§Œ, ì´ëŠ” ì†ë„ì™€ ì–‘ë°©í–¥ì„±ì„ í¬ìƒì‹œì¼œ í™•ì‚° ëª¨ë¸ì˜ ì£¼ìš” ì¥ì ì„ ì—†ì• ë²„ë¦½ë‹ˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í•˜ë“œ ì„¸ë¶„í™” ì—†ì´ ë””ì½”ë”© ìœˆë„ìš°ë¥¼ ì¢íˆëŠ” ì •ê·œí™” ê¸°ë°˜ ë°©ë²•ì¸ ì»¨ë³¼ë£¨ì…˜ ë””ì½”ë”©(Conv)ì„ ì œì•ˆí•˜ì—¬ ë” ë‚˜ì€ ìœ ì°½ì„±ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ, ìš°ë¦¬ëŠ” ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ ìœ„ì¹˜ì˜ í† í°ì„ ë” ì˜ ì •ë ¬í•˜ëŠ” ì‚¬í›„ í›ˆë ¨ ë°©ì‹ì¸ ê·œì¹™ ê¸°ë°˜ ê±°ë¶€ ë¯¸ì„¸ ì¡°ì •(R2FT)ì„ ë„ì…í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ í™•ì‚° ì–¸ì–´ ëª¨ë¸ ê¸°ì¤€ì—ì„œ ê°œë°©í˜• ìƒì„± ë²¤ì¹˜ë§ˆí¬(ì˜ˆ: AlpacaEval)ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, ì´ì „ ì‘ì—…ë³´ë‹¤ í›¨ì”¬ ì‘ì€ ë‹¨ê³„ í¬ê¸°ë¡œ ì†ë„ì™€ í’ˆì§ˆ ëª¨ë‘ì—ì„œ ê°œì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í™•ì‚° ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ ë””ì½”ë”© ì†ë„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ í™•ì‚° ëª¨ë¸ì€ ê¸´ ë””ì½”ë”© ìœˆë„ìš° ë¬¸ì œë¡œ ì¸í•´ ì…ë ¥ ë¬¸ë§¥ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ í† í°ì´ ë¹„ê´€ë ¨ì ì´ê±°ë‚˜ ë°˜ë³µë˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°©ë²•ì€ ì»¨ë³¼ë£¨ì…˜ ë””ì½”ë”©(Conv)ìœ¼ë¡œ, í•˜ë“œ ì„¸ë¶„í™” ì—†ì´ ë””ì½”ë”© ìœˆë„ìš°ë¥¼ ì¢í˜€ ìœ ì°½ì„±ê³¼ ìœ ì—°ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë˜í•œ, R2FTë¼ëŠ” í›„ì²˜ë¦¬ í•™ìŠµ ê¸°ë²•ì„ ë„ì…í•˜ì—¬ ë¬¸ë§¥ì—ì„œ ë¨¼ ìœ„ì¹˜ì˜ í† í° ì •ë ¬ì„ ê°œì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ë“¤ì€ ê¸°ì¡´ í™•ì‚° ëª¨ë¸ ëŒ€ë¹„ ì†ë„ì™€ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ë©°, AlpacaEval ë“± ì˜¤í”ˆì—”ë“œ ìƒì„± ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í™•ì‚° ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì€ ë³‘ë ¬ë¡œ ì—¬ëŸ¬ í† í°ì„ ë””ì½”ë”©í•  ìˆ˜ ìˆì–´ ì¶”ë¡  ì†ë„ì—ì„œ ìœ ë¦¬í•˜ë‹¤.

- 2. ê¸°ì¡´ í™•ì‚° ì–¸ì–´ ëª¨ë¸ì˜ ì£¼ìš” ë³‘ëª© í˜„ìƒì€ ë””ì½”ë”© ìœˆë„ìš°ê°€ ê¸¸ì–´ì§ˆ ë•Œ ë°œìƒí•˜ëŠ” ë¹„ê´€ë ¨ì„± ë° ë°˜ë³µì„± ë¬¸ì œì´ë‹¤.

- 3. ì œì•ˆëœ ì»¨ë³¼ë£¨ì…˜ ë””ì½”ë”©(Conv)ì€ í•˜ë“œ ì„¸ë¶„í™” ì—†ì´ ë””ì½”ë”© ìœˆë„ìš°ë¥¼ ì¢í˜€ ìœ ì°½ì„±ê³¼ ìœ ì—°ì„±ì„ ê°œì„ í•œë‹¤.

- 4. R2FTëŠ” ë¬¸ë§¥ì—ì„œ ë¨¼ ìœ„ì¹˜ì˜ í† í° ì •ë ¬ì„ ê°œì„ í•˜ëŠ” í›„ì²˜ë¦¬ í•™ìŠµ ë°©ì‹ì´ë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ í™•ì‚° ì–¸ì–´ ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì´ì „ ì—°êµ¬ë³´ë‹¤ ì ì€ ìŠ¤í… í¬ê¸°ë¡œ ì†ë„ì™€ í’ˆì§ˆì„ ëª¨ë‘ ê°œì„ í•œë‹¤.

---

*Generated on 2025-09-20 00:53:32*