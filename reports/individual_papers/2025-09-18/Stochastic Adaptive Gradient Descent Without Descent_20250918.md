# Stochastic Adaptive Gradient Descent Without Descent

**Korean Title:** í™•ë¥ ì  ì ì‘ ê²½ì‚¬ í•˜ê°•ë²• ì—†ì´ í•˜ê°•í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Jean-FranÃ§ois Aujol|Jean-FranÃ§ois Aujol]] [[authors/JÃ©rÃ©mie Bigot|JÃ©rÃ©mie Bigot]] [[authors/Camille Castera|Camille Castera]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Adaptive Step-size Strategy

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (79.2% similar)
- [[A Universal Banach--Bregman Framework for Stochastic Iterations_ Unifying Stochastic Mirror Descent, Learning and LLM Training_20250917|A Universal Banach--Bregman Framework for Stochastic Iterations Unifying Stochastic Mirror Descent, Learning and LLM Training]] (77.2% similar)
- [[Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits_20250919|Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits]] (76.7% similar)
- [[Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (76.6% similar)
- [[Nonlinear Cooperative Salvo Guidance with Seeker-Limited Interceptors_20250919|Nonlinear Cooperative Salvo Guidance with Seeker-Limited Interceptors]] (75.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Jean-FranÃ§ois Aujol, JÃ©rÃ©mie Bigot, Camille Castera

## ğŸ“„ Abstract (ì›ë¬¸)

We introduce a new adaptive step-size strategy for convex optimization with
stochastic gradient that exploits the local geometry of the objective function
only by means of a first-order stochastic oracle and without any
hyper-parameter tuning. The method comes from a theoretically-grounded
adaptation of the Adaptive Gradient Descent Without Descent method to the
stochastic setting. We prove the convergence of stochastic gradient descent
with our step-size under various assumptions, and we show that it empirically
competes against tuned baselines.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ìƒˆë¡œìš´ ì ì‘í˜• ìŠ¤í… í¬ê¸° ì „ëµì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì „ëµì€ í™•ë¥ ì  ê¸°ìš¸ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë³¼ë¡ ìµœì í™”ì—ì„œ ëª©í‘œ í•¨ìˆ˜ì˜ êµ­ì†Œ ê¸°í•˜í•™ì„ ì²« ë²ˆì§¸ ì°¨ìˆ˜ì˜ í™•ë¥ ì  ì˜¤ë¼í´ë§Œì„ í†µí•´ í™œìš©í•˜ë©°, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ Adaptive Gradient Descent Without Descent ë°©ë²•ì„ í™•ë¥ ì  í™˜ê²½ì— ë§ê²Œ ì´ë¡ ì ìœ¼ë¡œ ì ì‘ì‹œí‚¨ ê²°ê³¼ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ê°€ì • í•˜ì—ì„œ ìš°ë¦¬ì˜ ìŠ¤í… í¬ê¸°ë¥¼ ì‚¬ìš©í•œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ë ´ì„±ì„ ì¦ëª…í•˜ë©°, ì´ ë°©ë²•ì´ ì¡°ì •ëœ ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬ ê²½í—˜ì ìœ¼ë¡œ ê²½ìŸë ¥ì„ ê°–ì¶˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì—ì„œ ìƒˆë¡œìš´ ì ì‘í˜• ìŠ¤í… ì‚¬ì´ì¦ˆ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª©í‘œ í•¨ìˆ˜ì˜ ì§€ì—­ ê¸°í•˜í•™ì„ í™œìš©í•˜ë©°, 1ì°¨ í™•ë¥ ì  ì˜¤ë¼í´ë§Œì„ ì‚¬ìš©í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ Adaptive Gradient Descent Without Descent ë°©ë²•ì„ í™•ë¥ ì  í™˜ê²½ì— ë§ê²Œ ì´ë¡ ì ìœ¼ë¡œ ì¡°ì •í•œ ê²ƒì…ë‹ˆë‹¤. ì œì•ˆëœ ìŠ¤í… ì‚¬ì´ì¦ˆë¥¼ ì‚¬ìš©í•œ SGDì˜ ìˆ˜ë ´ì„±ì„ ë‹¤ì–‘í•œ ê°€ì • í•˜ì— ì¦ëª…í•˜ì˜€ìœ¼ë©°, ì‹¤í—˜ì ìœ¼ë¡œ íŠœë‹ëœ ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ ê²½ìŸë ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìƒˆë¡œìš´ ì ì‘í˜• ìŠ¤í… í¬ê¸° ì „ëµì€ ëª©ì  í•¨ìˆ˜ì˜ ì§€ì—­ ê¸°í•˜í•™ì„ í™œìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì—†ì´ í™•ë¥ ì  ê¸°ìš¸ê¸° í•˜ê°•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- 2. ì´ ë°©ë²•ì€ Adaptive Gradient Descent Without Descent ë°©ë²•ì„ í™•ë¥ ì  ì„¤ì •ì— ë§ê²Œ ì´ë¡ ì ìœ¼ë¡œ ì ì‘ì‹œí‚¨ ê²ƒì…ë‹ˆë‹¤.

- 3. ì œì•ˆëœ ìŠ¤í… í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” í™•ë¥ ì  ê¸°ìš¸ê¸° í•˜ê°•ë²•ì˜ ìˆ˜ë ´ì„±ì„ ë‹¤ì–‘í•œ ê°€ì • í•˜ì— ì¦ëª…í•˜ì˜€ìŠµë‹ˆë‹¤.

- 4. ì œì•ˆëœ ë°©ë²•ì€ ì‹¤í—˜ì ìœ¼ë¡œ íŠœë‹ëœ ê¸°ì¤€ì„ ê³¼ ê²½ìŸë ¥ì„ ê°€ì§‘ë‹ˆë‹¤.

---

*Generated on 2025-09-20 01:39:34*