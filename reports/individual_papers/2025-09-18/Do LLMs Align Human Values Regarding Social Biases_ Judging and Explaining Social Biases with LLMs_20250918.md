---
keywords:
  - Large Language Models
  - Human Values
  - Social Biases
category: cs.AI
publish_date: 2025-09-18
arxiv_id: 2509.13869
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:32:13.848709",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Human Values",
    "Social Biases"
  ],
  "rejected_keywords": [
    "Model Families"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Human Values": 0.78,
    "Social Biases": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs

**Korean Title:** LLM이 사회적 편견에 대한 인간 가치를 일치시키는가? LLM을 사용하여 사회적 편견을 판단하고 설명하기

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250918|2025-09-18]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**⚡ Unique Technical**: [[keywords/Human Values|Human Values]], [[keywords/Social Biases|Social Biases]]
**🚀 Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## 🔗 유사한 논문
- [[How_Does_Cognitive_Bias_Affect_Large_Language_Models_A_Case_Study_on_the_Anchoring_Effect_in_Price_Negotiation_Simulations_20250918|How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations]] (85.3% similar)
- [[A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare]] (84.7% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (83.8% similar)
- [[Language Models Identify Ambiguities and Exploit Loopholes]] (83.2% similar)
- [[Emergent_Social_Dynamics_of_LLM_Agents_in_the_El_Farol_Bar_Problem_20250918|Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem]] (82.6% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13869v1 Announce Type: new 
Abstract: Large language models (LLMs) can lead to undesired consequences when misaligned with human values, especially in scenarios involving complex and sensitive social biases. Previous studies have revealed the misalignment of LLMs with human values using expert-designed or agent-based emulated bias scenarios. However, it remains unclear whether the alignment of LLMs with human values differs across different types of scenarios (e.g., scenarios containing negative vs. non-negative questions). In this study, we investigate the alignment of LLMs with human values regarding social biases (HVSB) in different types of bias scenarios. Through extensive analysis of 12 LLMs from four model families and four datasets, we demonstrate that LLMs with large model parameter scales do not necessarily have lower misalignment rate and attack success rate. Moreover, LLMs show a certain degree of alignment preference for specific types of scenarios and the LLMs from the same model family tend to have higher judgment consistency. In addition, we study the understanding capacity of LLMs with their explanations of HVSB. We find no significant differences in the understanding of HVSB across LLMs. We also find LLMs prefer their own generated explanations. Additionally, we endow smaller language models (LMs) with the ability to explain HVSB. The generation results show that the explanations generated by the fine-tuned smaller LMs are more readable, but have a relatively lower model agreeability.

## 🔍 Abstract (한글 번역)

arXiv:2509.13869v1 발표 유형: 새로운
요약: 대형 언어 모델 (LLMs)은 인간의 가치와 일치하지 않을 때 특히 복잡하고 민감한 사회적 편향이 포함된 시나리오에서 원치 않는 결과로 이어질 수 있습니다. 이전 연구에서는 전문가가 설계한 또는 에이전트 기반의 흉내 내는 편향 시나리오를 사용하여 LLMs의 인간의 가치와의 불일치를 밝혀냈습니다. 그러나 LLMs의 인간의 가치와의 일치 여부가 다른 유형의 시나리오(예: 부정적인 vs. 비부정적인 질문이 포함된 시나리오) 간에 다른지 여전히 불분명합니다. 본 연구에서는 다양한 유형의 편향 시나리오에서 LLMs의 인간의 가치와의 일치를 조사합니다. 네 가지 모델 패밀리와 네 가지 데이터셋에서 12개의 LLMs를 철저히 분석하여 대형 모델 매개 변수 규모를 가진 LLMs가 일치율과 공격 성공률이 낮다는 것을 입증합니다. 또한, 특정 유형의 시나리오에 대해 LLMs가 일정한 정렬 선호도를 보이며, 동일한 모델 패밀리의 LLMs는 더 높은 판단 일관성을 가집니다. 또한, LLMs의 HVSB 설명력을 연구합니다. LLMs 간 HVSB 이해에는 유의미한 차이가 없음을 발견합니다. 또한, LLMs는 자체 생성된 설명을 선호하는 것으로 나타냅니다. 추가로, 우리는 더 작은 언어 모델 (LMs)에게 HVSB를 설명할 수 있는 능력을 부여합니다. 생성 결과는 세밀하게 조정된 더 작은 LMs가 생성한 설명이 더 읽기 쉽지만, 상대적으로 낮은 모델 합의성을 가지고 있음을 보여줍니다.

## 📝 요약

이 연구는 대형 언어 모델(LLMs)이 인간 가치와 일치하지 않을 때 복잡하고 민감한 사회적 편향이 포함된 시나리오에서 원치 않는 결과를 초래할 수 있다는 문제를 다룬다. 이전 연구들은 전문가가 설계한 또는 에이전트 기반의 편향 시나리오를 사용하여 LLMs의 인간 가치와의 불일치를 밝혀냈다. 그러나 다양한 유형의 시나리오(예: 부정적 vs. 비부정적 질문이 포함된 시나리오)에서 LLMs의 인간 가치와의 일치 여부가 다른지 여부는 여전히 불분명하다. 본 연구에서는 다양한 유형의 편향 시나리오에서 LLMs의 인간 가치와의 일치를 조사한다. 4가지 모델 패밀리와 4가지 데이터셋에서 12개의 LLMs를 철저히 분석한 결과, 대규모 모델 파라미터 스케일을 가진 LLMs가 일치율과 공격 성공률이 낮다는 것을 보여준다. 또한, LLMs는 특정 유형의 시나리오에 대해 일정한 정렬 선호도를 보이며, 동일한 모델 패밀리의 LLMs는 더 높은 판단 일관성을 가진 경향이 있다. 또한, LLMs의 HVSB 이해 능력을 연구한다. LLMs 간 HVSB의 이해에는 유의미한 차이가 없음을 발견하며, LLMs는 자체 생성된 설명을 선호하는 경향이 있다. 더불어, 작은 언어 모델(LMs)에 HVSB를 설명할 수 있는 능력을 부여한다. 생성 결과는 미세 조정된 작은 LMs가 더 읽기 쉽지만 상대적으로 낮은 모델 합의성을 가진 설명을 생성한다.

## 🎯 주요 포인트

- 대형 언어 모델은 사회적 편향과의 일치를 조사하는 데 사용될 수 있음

- 대형 모델 파라미터 규모가 높을수록 사회적 편향과의 불일치율이 낮지 않을 수 있음

- 동일한 모델 패밀리의 대형 언어 모델은 판단 일관성이 높을 수 있음

---

*Generated on 2025-09-18 16:51:10*