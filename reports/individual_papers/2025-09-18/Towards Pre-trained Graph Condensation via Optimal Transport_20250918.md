---
keywords:
  - Graph Neural Networks
  - Graph Condensation
  - Optimal Transport
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:04:25.878984",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Neural Networks",
    "Graph Condensation",
    "Optimal Transport"
  ],
  "rejected_keywords": [
    "Semantic Harmonizer",
    "Uncertainty Quantification"
  ],
  "similarity_scores": {
    "Graph Neural Networks": 0.8,
    "Graph Condensation": 0.78,
    "Optimal Transport": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Towards Pre-trained Graph Condensation via Optimal Transport

**Korean Title:** ìµœì  ìˆ˜ì†¡ì„ í†µí•œ ì‚¬ì „ í•™ìŠµëœ ê·¸ë˜í”„ ì‘ì¶• ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]        [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Networks|Graph Neural Networks]]
**âš¡ Unique Technical**: [[keywords/Graph Condensation|Graph Condensation]], [[keywords/Optimal Transport|Optimal Transport]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods Reviving Transformer for Graph Clustering]] (83.0% similar)
- [[Exploring the Global-to-Local Attention Scheme in Graph Transformers_ An Empirical Study_20250918|Exploring the Global-to-Local Attention Scheme in Graph Transformers An Empirical Study]] (80.8% similar)
- [[Let's Grow an Unbiased Community_ Guiding the Fairness of Graphs via New Links_20250919|Let's Grow an Unbiased Community Guiding the Fairness of Graphs via New Links]] (80.4% similar)
- [[Brain-HGCN_ A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis_20250919|Brain-HGCN A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis]] (79.7% similar)
- [[GraphTorque_ Torque-Driven Rewiring Graph Neural Network_20250918|GraphTorque Torque-Driven Rewiring Graph Neural Network]] (79.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Yeyu Yan, Shuai Zheng, Wenjun Hui, Xiangkai Zhu, Dong Chen, Zhenfeng Zhu, Yao Zhao, Kunlun He

## ğŸ“„ Abstract (ì›ë¬¸)

Graph condensation (GC) aims to distill the original graph into a small-scale
graph, mitigating redundancy and accelerating GNN training. However,
conventional GC approaches heavily rely on rigid GNNs and task-specific
supervision. Such a dependency severely restricts their reusability and
generalization across various tasks and architectures. In this work, we revisit
the goal of ideal GC from the perspective of GNN optimization consistency, and
then a generalized GC optimization objective is derived, by which those
traditional GC methods can be viewed nicely as special cases of this
optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)
via optimal transport is proposed to transcend the limitations of task- and
architecture-dependent GC methods. Specifically, a hybrid-interval graph
diffusion augmentation is presented to suppress the weak generalization ability
of the condensed graph on particular architectures by enhancing the uncertainty
of node states. Meanwhile, the matching between optimal graph transport plan
and representation transport plan is tactfully established to maintain semantic
consistencies across source graph and condensed graph spaces, thereby freeing
graph condensation from task dependencies. To further facilitate the adaptation
of condensed graphs to various downstream tasks, a traceable semantic
harmonizer from source nodes to condensed nodes is proposed to bridge semantic
associations through the optimized representation transport plan in
pre-training. Extensive experiments verify the superiority and versatility of
PreGC, demonstrating its task-independent nature and seamless compatibility
with arbitrary GNNs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ê·¸ë˜í”„ ì‘ì¶•(GC)ì€ ì›ë˜ ê·¸ë˜í”„ë¥¼ ì†Œê·œëª¨ ê·¸ë˜í”„ë¡œ ì¦ë¥˜í•˜ì—¬ ì¤‘ë³µì„±ì„ ì¤„ì´ê³  GNN í›ˆë ¨ì„ ê°€ì†í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ GC ì ‘ê·¼ ë°©ì‹ì€ ê²½ì§ëœ GNNê³¼ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ê°ë…ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì˜ì¡´ì„±ì€ ë‹¤ì–‘í•œ ì‘ì—…ê³¼ ì•„í‚¤í…ì²˜ì— ê±¸ì³ ì¬ì‚¬ìš©ì„±ê³¼ ì¼ë°˜í™”ë¥¼ ì‹¬ê°í•˜ê²Œ ì œí•œí•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” GNN ìµœì í™” ì¼ê´€ì„±ì˜ ê´€ì ì—ì„œ ì´ìƒì ì¸ GCì˜ ëª©í‘œë¥¼ ì¬ê²€í† í•˜ê³ , ì´ë¥¼ í†µí•´ ì „í†µì ì¸ GC ë°©ë²•ë“¤ì´ ì´ ìµœì í™” íŒ¨ëŸ¬ë‹¤ì„ì˜ íŠ¹ë³„í•œ ì‚¬ë¡€ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆëŠ” ì¼ë°˜í™”ëœ GC ìµœì í™” ëª©í‘œë¥¼ ë„ì¶œí•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì  ìˆ˜ì†¡ì„ í†µí•œ ì‚¬ì „ í›ˆë ¨ëœ ê·¸ë˜í”„ ì‘ì¶•(PreGC)ì´ ì œì•ˆë˜ì–´ ì‘ì—… ë° ì•„í‚¤í…ì²˜ ì˜ì¡´ì ì¸ GC ë°©ë²•ì˜ í•œê³„ë¥¼ ì´ˆì›”í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, íŠ¹ì • ì•„í‚¤í…ì²˜ì—ì„œ ì‘ì¶•ëœ ê·¸ë˜í”„ì˜ ì•½í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì–µì œí•˜ê¸° ìœ„í•´ í•˜ì´ë¸Œë¦¬ë“œ ê°„ê²© ê·¸ë˜í”„ í™•ì‚° ì¦ê°•ì´ ì œì‹œë˜ì–´ ë…¸ë“œ ìƒíƒœì˜ ë¶ˆí™•ì‹¤ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤. ë™ì‹œì—, ìµœì  ê·¸ë˜í”„ ìˆ˜ì†¡ ê³„íšê³¼ í‘œí˜„ ìˆ˜ì†¡ ê³„íš ê°„ì˜ ë§¤ì¹­ì´ êµë¬˜í•˜ê²Œ ì„¤ì •ë˜ì–´ ì†ŒìŠ¤ ê·¸ë˜í”„ì™€ ì‘ì¶•ëœ ê·¸ë˜í”„ ê³µê°„ ê°„ì˜ ì˜ë¯¸ì  ì¼ê´€ì„±ì„ ìœ ì§€í•¨ìœ¼ë¡œì¨ ê·¸ë˜í”„ ì‘ì¶•ì„ ì‘ì—… ì˜ì¡´ì„±ì—ì„œ í•´ë°©ì‹œí‚µë‹ˆë‹¤. ì‘ì¶•ëœ ê·¸ë˜í”„ì˜ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ì ì‘ì„ ë”ìš± ì´‰ì§„í•˜ê¸° ìœ„í•´, ì‚¬ì „ í›ˆë ¨ì—ì„œ ìµœì í™”ëœ í‘œí˜„ ìˆ˜ì†¡ ê³„íšì„ í†µí•´ ì†ŒìŠ¤ ë…¸ë“œì™€ ì‘ì¶• ë…¸ë“œ ê°„ì˜ ì˜ë¯¸ì  ì—°ê´€ì„±ì„ ì—°ê²°í•˜ëŠ” ì¶”ì  ê°€ëŠ¥í•œ ì˜ë¯¸ ì¡°í™”ê¸°ê°€ ì œì•ˆë©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ PreGCì˜ ìš°ìˆ˜ì„±ê³¼ ë‹¤ì¬ë‹¤ëŠ¥í•¨ì´ ê²€ì¦ë˜ì—ˆìœ¼ë©°, ì‘ì—… ë…ë¦½ì ì¸ íŠ¹ì„±ê³¼ ì„ì˜ì˜ GNNê³¼ì˜ ì›í™œí•œ í˜¸í™˜ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ê·¸ë˜í”„ ì‘ì¶•(GC)ì€ ì›ë˜ ê·¸ë˜í”„ë¥¼ ì‘ì€ ê·œëª¨ë¡œ ì¶•ì†Œí•˜ì—¬ ì¤‘ë³µì„ ì¤„ì´ê³  GNN í›ˆë ¨ì„ ê°€ì†í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê¸°ì¡´ GC ë°©ë²•ì€ íŠ¹ì • ê³¼ì œì™€ ì•„í‚¤í…ì²˜ì— ì˜ì¡´í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ ì¼ë°˜í™”ê°€ ì œí•œë©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” GNN ìµœì í™” ì¼ê´€ì„± ê´€ì ì—ì„œ ì´ìƒì ì¸ GC ëª©í‘œë¥¼ ì¬ê²€í† í•˜ê³ , ì´ë¥¼ í†µí•´ ì „í†µì ì¸ GC ë°©ë²•ì„ íŠ¹ìˆ˜ ì‚¬ë¡€ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ì¼ë°˜í™”ëœ GC ìµœì í™” ëª©í‘œë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ìµœì  ìˆ˜ì†¡ì„ í†µí•œ ì‚¬ì „ í›ˆë ¨ ê·¸ë˜í”„ ì‘ì¶•(PreGC)ì„ ì œì•ˆí•˜ì—¬ ê³¼ì œ ë° ì•„í‚¤í…ì²˜ ì˜ì¡´ì„±ì„ ì´ˆì›”í•©ë‹ˆë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ê°„ê²© ê·¸ë˜í”„ í™•ì‚° ì¦ê°•ì„ í†µí•´ íŠ¹ì • ì•„í‚¤í…ì²˜ì—ì„œ ì‘ì¶• ê·¸ë˜í”„ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ìµœì  ê·¸ë˜í”„ ìˆ˜ì†¡ ê³„íšê³¼ í‘œí˜„ ìˆ˜ì†¡ ê³„íš ê°„ì˜ ë§¤ì¹­ì„ í†µí•´ ì˜ë¯¸ì  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘í•  ìˆ˜ ìˆë„ë¡ ì†ŒìŠ¤ ë…¸ë“œì™€ ì‘ì¶• ë…¸ë“œ ê°„ì˜ ì˜ë¯¸ì  ì—°ê²°ì„ ìœ„í•œ ì¶”ì  ê°€ëŠ¥í•œ ì˜ë¯¸ ì¡°í™”ê¸°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, PreGCì˜ ìš°ìˆ˜ì„±ê³¼ ë²”ìš©ì„±ì´ ì…ì¦ë˜ì—ˆìœ¼ë©°, ê³¼ì œ ë…ë¦½ì  íŠ¹ì„±ê³¼ ë‹¤ì–‘í•œ GNNê³¼ì˜ í˜¸í™˜ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê·¸ë˜í”„ ì‘ì¶•(GC)ì€ ì›ë˜ ê·¸ë˜í”„ë¥¼ ì†Œê·œëª¨ ê·¸ë˜í”„ë¡œ ì••ì¶•í•˜ì—¬ ì¤‘ë³µì„±ì„ ì¤„ì´ê³  GNN í›ˆë ¨ì„ ê°€ì†í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ GC ë°©ë²•ì€ ê²½ì§ëœ GNNê³¼ íŠ¹ì • ê³¼ì œì— ëŒ€í•œ ê°ë…ì— í¬ê²Œ ì˜ì¡´í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ ì¼ë°˜í™”ë¥¼ ì œí•œí•©ë‹ˆë‹¤.

- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” GNN ìµœì í™” ì¼ê´€ì„± ê´€ì ì—ì„œ ì´ìƒì ì¸ GC ëª©í‘œë¥¼ ì¬ê²€í† í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ë°˜í™”ëœ GC ìµœì í™” ëª©í‘œë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.

- 4. ìµœì  ìš´ì†¡ì„ í†µí•œ ì‚¬ì „ í›ˆë ¨ ê·¸ë˜í”„ ì‘ì¶•(PreGC)ì€ ê³¼ì œ ë° ì•„í‚¤í…ì²˜ ì˜ì¡´ì„±ì„ ì´ˆì›”í•˜ì—¬ ê·¸ë˜í”„ ì‘ì¶•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.

- 5. PreGCëŠ” ë‹¤ì–‘í•œ GNNê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê³¼ì œ ë…ë¦½ì ì¸ íŠ¹ì„±ì„ ì…ì¦í•˜ì—¬ ìš°ìˆ˜ì„±ê³¼ ë‹¤ì¬ë‹¤ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:44:49*