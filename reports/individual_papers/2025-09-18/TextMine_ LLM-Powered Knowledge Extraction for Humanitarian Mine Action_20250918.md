# TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action

**Korean Title:** í…ìŠ¤íŠ¸ë§ˆì¸: ì¸ë„ì  ì§€ë¢° ì œê±°ë¥¼ ìœ„í•œ LLM ê¸°ë°˜ ì§€ì‹ ì¶”ì¶œ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Chenyue Zhou|Chenyue Zhou]] [[authors/GÃ¼rkan Solmaz|GÃ¼rkan Solmaz]] [[authors/Flavio Cirillo|Flavio Cirillo]] [[authors/Kiril Gashteovski|Kiril Gashteovski]] [[authors/Jonathan FÃ¼rst|Jonathan FÃ¼rst]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Ontology-aligned Prompts

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (78.1% similar)
- [[MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE Metadata Extraction and Validation in Scientific Papers Using LLMs]] (77.9% similar)
- [[Engineering RAG Systems for Real-World Applications_ Design, Development, and Evaluation_20250919|Engineering RAG Systems for Real-World Applications Design, Development, and Evaluation]] (76.5% similar)
- [[Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models_20250917|Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models]] (76.4% similar)
- [[Automated and Context-Aware Code Documentation Leveraging Advanced LLMs_20250919|Automated and Context-Aware Code Documentation Leveraging Advanced LLMs]] (76.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Chenyue Zhou, GÃ¼rkan Solmaz, Flavio Cirillo, Kiril Gashteovski, Jonathan FÃ¼rst

## ğŸ“„ Abstract (ì›ë¬¸)

Humanitarian Mine Action has generated extensive best-practice knowledge, but
much remains locked in unstructured reports. We introduce TextMine, an
ontology-guided pipeline that uses Large Language Models to extract knowledge
triples from HMA texts. TextMine integrates document chunking, domain-aware
prompting, triple extraction, and both reference-based and LLM-as-a-Judge
evaluation. We also create the first HMA ontology and a curated dataset of
real-world demining reports. Experiments show ontology-aligned prompts boost
extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format
conformance by 20.9% over baselines. While validated on Cambodian reports,
TextMine can adapt to global demining efforts or other domains, transforming
unstructured data into structured knowledge.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì¸ë„ì  ì§€ë¢° ì œê±° í™œë™ì€ ê´‘ë²”ìœ„í•œ ëª¨ë²” ì‚¬ë¡€ ì§€ì‹ì„ ìƒì„±í–ˆì§€ë§Œ, ë§ì€ ë¶€ë¶„ì´ ë¹„êµ¬ì¡°í™”ëœ ë³´ê³ ì„œì— ì ê²¨ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” HMA í…ìŠ¤íŠ¸ì—ì„œ ì§€ì‹ ì‚¼ì¤‘í•­ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì¸ TextMineì„ ì†Œê°œí•©ë‹ˆë‹¤. TextMineì€ ë¬¸ì„œ ì²­í‚¹, ë„ë©”ì¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸, ì‚¼ì¤‘í•­ ì¶”ì¶œ, ì°¸ì¡° ê¸°ë°˜ ë° LLM-as-a-Judge í‰ê°€ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ìµœì´ˆì˜ HMA ì˜¨í†¨ë¡œì§€ì™€ ì‹¤ì œ ì§€ë¢° ì œê±° ë³´ê³ ì„œì˜ íë ˆì´ì…˜ëœ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ ì˜¨í†¨ë¡œì§€ ì •ë ¬ í”„ë¡¬í”„íŠ¸ê°€ ì¶”ì¶œ ì •í™•ì„±ì„ 44.2% í–¥ìƒì‹œí‚¤ê³ , í™˜ê°ì„ 22.5% ì¤„ì´ë©°, í˜•ì‹ ì¤€ìˆ˜ë¥¼ ê¸°ì¤€ì„  ëŒ€ë¹„ 20.9% ê°œì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ìº„ë³´ë””ì•„ ë³´ê³ ì„œì—ì„œ ê²€ì¦ë˜ì—ˆì§€ë§Œ, TextMineì€ ì „ ì„¸ê³„ ì§€ë¢° ì œê±° ë…¸ë ¥ì´ë‚˜ ë‹¤ë¥¸ ë„ë©”ì¸ì— ì ì‘í•  ìˆ˜ ìˆìœ¼ë©°, ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ ì§€ì‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ë„ì  ì§€ë¢° ì œê±° ë¶„ì•¼ì˜ ë¹„êµ¬ì¡°í™”ëœ ë³´ê³ ì„œì—ì„œ ì§€ì‹ ì¶”ì¶œì„ ìœ„í•œ TextMineì´ë¼ëŠ” ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. TextMineì€ ë¬¸ì„œ ë¶„í• , ë„ë©”ì¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸, ì‚¼ì¤‘í•­ ì¶”ì¶œ, í‰ê°€ ë°©ë²•ì„ í†µí•©í•˜ë©°, ìµœì´ˆì˜ HMA ì˜¨í†¨ë¡œì§€ì™€ ì‹¤ì œ ì§€ë¢° ì œê±° ë³´ê³ ì„œ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì˜¨í†¨ë¡œì§€ ì •ë ¬ í”„ë¡¬í”„íŠ¸ê°€ ì¶”ì¶œ ì •í™•ì„±ì„ 44.2% í–¥ìƒì‹œí‚¤ê³ , í™˜ê°ì„ 22.5% ì¤„ì´ë©°, í˜•ì‹ ì¤€ìˆ˜ì„±ì„ 20.9% ê°œì„ í–ˆìŠµë‹ˆë‹¤. TextMineì€ ìº„ë³´ë””ì•„ ë³´ê³ ì„œì—ì„œ ê²€ì¦ë˜ì—ˆìœ¼ë©°, ì „ ì„¸ê³„ ì§€ë¢° ì œê±° ë…¸ë ¥ì´ë‚˜ ë‹¤ë¥¸ ë¶„ì•¼ì—ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TextMineì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì¸ë„ì  ì§€ë¢° ì œê±°(HMA) í…ìŠ¤íŠ¸ì—ì„œ ì§€ì‹ íŠ¸ë¦¬í”Œì„ ì¶”ì¶œí•˜ëŠ” ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

- 2. TextMineì€ ë¬¸ì„œ ì²­í‚¹, ë„ë©”ì¸ ì¸ì‹ í”„ë¡¬í”„íŠ¸, íŠ¸ë¦¬í”Œ ì¶”ì¶œ, ì°¸ì¡° ê¸°ë°˜ ë° LLM-as-a-Judge í‰ê°€ë¥¼ í†µí•©í•©ë‹ˆë‹¤.

- 3. ì‹¤í—˜ ê²°ê³¼, ì˜¨í†¨ë¡œì§€ ì •ë ¬ í”„ë¡¬í”„íŠ¸ëŠ” ì¶”ì¶œ ì •í™•ì„±ì„ 44.2% í–¥ìƒì‹œí‚¤ê³ , í™˜ê°ì„ 22.5% ì¤„ì´ë©°, í˜•ì‹ ì¤€ìˆ˜ë¥¼ 20.9% ê°œì„ í–ˆìŠµë‹ˆë‹¤.

- 4. TextMineì€ ìº„ë³´ë””ì•„ ë³´ê³ ì„œì—ì„œ ê²€ì¦ë˜ì—ˆìœ¼ë©°, ì „ ì„¸ê³„ ì§€ë¢° ì œê±° ë…¸ë ¥ì´ë‚˜ ë‹¤ë¥¸ ë„ë©”ì¸ì—ë„ ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 5. ìš°ë¦¬ëŠ” ìµœì´ˆì˜ HMA ì˜¨í†¨ë¡œì§€ì™€ ì‹¤ì œ ì§€ë¢° ì œê±° ë³´ê³ ì„œì˜ íë ˆì´ì…˜ëœ ë°ì´í„°ì…‹ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 01:02:52*