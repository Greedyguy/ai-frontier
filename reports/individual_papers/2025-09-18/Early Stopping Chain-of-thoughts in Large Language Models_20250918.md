---
keywords:
  - Large Language Models
  - Chain-of-thought Reasoning
  - Inference Optimization
category: cs.AI
publish_date: 2025-09-18
arxiv_id: 2509.14004
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:31:40.620291",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Chain-of-thought Reasoning",
    "Inference Optimization"
  ],
  "rejected_keywords": [
    "Self-consistency Prompting"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Chain-of-thought Reasoning": 0.78,
    "Inference Optimization": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Early Stopping Chain-of-thoughts in Large Language Models

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì¡°ê¸° ì¤‘ë‹¨ëœ ì‚¬ê³  ì—°ì‡„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Large Language Models|Large Language Models]]
**âš¡ Unique Technical**: [[keywords/Chain-of-thought Reasoning|Chain-of-thoughts]]
**ğŸš€ Evolved Concepts**: [[keywords/Inference Optimization|Inference-time method]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (90.1% similar)
- [[Uni-cot_Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (84.8% similar)
- [[MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning]] (81.9% similar)
- [[Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness]] (80.7% similar)
- [[THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning]] (80.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14004v1 Announce Type: new 
Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities in solving complicated problems by generating long chain-of-thoughts (CoT), but such a lengthy CoT incurs high inference costs. In this study, we introduce ES-CoT, an inference-time method that shortens CoT generation by detecting answer convergence and stopping early with minimal performance loss. At the end of each reasoning step, we prompt the LLM to output its current final answer, denoted as a step answer. We then track the run length of consecutive identical step answers as a measure of answer convergence. Once the run length exhibits a sharp increase and exceeds a minimum threshold, the generation is terminated. We provide both empirical and theoretical support for this heuristic: step answers steadily converge to the final answer, and large run-length jumps reliably mark this convergence. Experiments on five reasoning datasets across three LLMs show that ES-CoT reduces the number of inference tokens by about 41\% on average while maintaining accuracy comparable to standard CoT. Further, ES-CoT integrates seamlessly with self-consistency prompting and remains robust across hyperparameter choices, highlighting it as a practical and effective approach for efficient reasoning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14004v1 ë°œí‘œ ìœ í˜•: ìƒˆë¡œìš´
ìš”ì•½: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ (LLMs)ì€ ê¸´ ì‚¬ê³  ì²´ì¸ (CoT)ì„ ìƒì„±í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì´ëŸ¬í•œ ê¸´ CoTì€ ë†’ì€ ì¶”ë¡  ë¹„ìš©ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ES-CoTë¼ëŠ” ì¶”ë¡  ì‹œê°„ ë°©ë²•ì„ ì†Œê°œí•˜ì—¬ ë‹µë³€ ìˆ˜ë ´ì„ ê°ì§€í•˜ê³  ìµœì†Œì˜ ì„±ëŠ¥ ì†ì‹¤ë¡œ ì¡°ê¸° ì¤‘ë‹¨í•˜ì—¬ CoT ìƒì„±ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤. ê° ì¶”ë¡  ë‹¨ê³„ì˜ ëì—ì„œ LLMì—ê²Œ í˜„ì¬ ìµœì¢… ë‹µë³€ì„ ì¶œë ¥í•˜ë„ë¡ ìœ ë„í•˜ê³ , ì´ë¥¼ ë‹¨ê³„ ë‹µë³€ì´ë¼ê³  í‘œì‹œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì—°ì†ì ìœ¼ë¡œ ë™ì¼í•œ ë‹¨ê³„ ë‹µë³€ì˜ ì‹¤í–‰ ê¸¸ì´ë¥¼ ë‹µë³€ ìˆ˜ë ´ì˜ ì¸¡ì •ìœ¼ë¡œ ì¶”ì í•©ë‹ˆë‹¤. ì‹¤í–‰ ê¸¸ì´ê°€ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìµœì†Œ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ìƒì„±ì´ ì¢…ë£Œë©ë‹ˆë‹¤. ì´ íœ´ë¦¬ìŠ¤í‹±ì— ëŒ€í•œ ê²½í—˜ì  ë° ì´ë¡ ì  ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤: ë‹¨ê³„ ë‹µë³€ì€ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ê¾¸ì¤€íˆ ìˆ˜ë ´í•˜ê³  í° ì‹¤í–‰ ê¸¸ì´ì˜ ì í”„ëŠ” ì´ ìˆ˜ë ´ì„ ì‹ ë¢°í•  ìˆ˜ ìˆê²Œ í‘œì‹œí•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ LLMì„ í†µí•´ ë‹¤ì„¯ ê°€ì§€ ì¶”ë¡  ë°ì´í„°ì…‹ì—ì„œ ìˆ˜í–‰í•œ ì‹¤í—˜ ê²°ê³¼, ES-CoTëŠ” í‘œì¤€ CoTì™€ ë¹„êµ ê°€ëŠ¥í•œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¶”ë¡  í† í°ì˜ ìˆ˜ë¥¼ í‰ê· ì ìœ¼ë¡œ ì•½ 41% ì¤„ì…ë‹ˆë‹¤. ë” ë‚˜ì•„ê°€, ES-CoTëŠ” ìê¸° ì¼ê´€ì„± í”„ë¡¬í”„íŒ…ê³¼ ë§¤ë„ëŸ½ê²Œ í†µí•©ë˜ë©° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒì— ë”°ë¼ ê²¬ê³ í•˜ê²Œ ìœ ì§€ë˜ì–´ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ìœ„í•œ ì‹¤ìš©ì ì´ê³  íš¨ê³¼ì ì¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ê°•ì¡°ë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ë™ì•ˆ, ê¸´ ì‚¬ê³  ì²´ì¸(Chain-of-Thoughts, CoT)ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¶”ë¡  ë¹„ìš©ì„ ì¦ê°€ì‹œí‚¨ë‹¤ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ES-CoTë¼ëŠ” ì¶”ë¡  ì‹œê°„ ë°©ë²•ì„ ì†Œê°œí•˜ì—¬ ë‹µë³€ ìˆ˜ë ´ì„ ê°ì§€í•˜ê³  ìµœì†Œí•œì˜ ì„±ëŠ¥ ì†ì‹¤ë¡œ ì¼ì° ì¤‘ì§€ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ì‹¤í—˜ ê²°ê³¼, ES-CoTëŠ” í‰ê· ì ìœ¼ë¡œ ì¶”ë¡  í† í° ìˆ˜ë¥¼ ì•½ 41% ì¤„ì´ë©´ì„œ í‘œì¤€ CoTì™€ ë¹„êµ ê°€ëŠ¥í•œ ì •í™•ë„ë¥¼ ìœ ì§€í•¨ì„ ë³´ì—¬ì£¼ë©°, ìê¸° ì¼ê´€ì„± í”„ë¡¬í”„íŒ…ê³¼ ì›í™œí•˜ê²Œ í†µí•©ë˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒì— ê°•ê±´í•˜ë‹¤ëŠ” ê²ƒì„ ê°•ì¡°í•œë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- ES-CoTëŠ” ë‹µë³€ ìˆ˜ë ´ì„ ê°ì§€í•˜ì—¬ CoT ìƒì„±ì„ ë‹¨ì¶•ì‹œí‚¤ëŠ” ì¶”ë¡  ì‹œê°„ ë°©ë²•ì´ë‹¤.

- ES-CoTëŠ” ì¶”ë¡  í† í° ìˆ˜ë¥¼ í‰ê·  41% ì¤„ì´ë©´ì„œ ì •í™•ë„ë¥¼ ìœ ì§€í•œë‹¤.

- ES-CoTëŠ” ìê¸° ì¼ê´€ì„± í”„ë¡¬í”„íŒ…ê³¼ ì›í™œí•˜ê²Œ í†µí•©ë˜ë©° íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ìœ„í•œ ì‹¤ìš©ì ì´ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì´ë‹¤.

---

*Generated on 2025-09-18 16:51:35*