---
keywords:
  - Transformer Architecture
  - Natural Language Processing
  - Patent NLP
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:16:42.397070",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer Architecture",
    "Natural Language Processing",
    "Patent NLP"
  ],
  "rejected_keywords": [
    "FlashAttention",
    "Domain-specific Pretraining"
  ],
  "similarity_scores": {
    "Transformer Architecture": 0.8,
    "Natural Language Processing": 0.75,
    "Patent NLP": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Patent Language Model Pretraining with ModernBERT

**Korean Title:** íŠ¹í—ˆ ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨: ModernBERT í™œìš©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**ğŸ”— Specific Connectable**: [[keywords/Transformer Architecture|Transformer-based language models]]
**âš¡ Unique Technical**: [[keywords/Patent NLP|Patent-focused NLP tasks]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[BERTector_ An Intrusion Detection Framework Constructed via Joint-dataset Learning Based on Language Model_20250918|BERTector An Intrusion Detection Framework Constructed via Joint-dataset Learning Based on Language Model]] (79.7% similar)
- [[FroM_ Frobenius Norm-Based Data-Free Adaptive Model Merging_20250918|FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (78.5% similar)
- [[Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (78.4% similar)
- [[WebCoT_ Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback_20250919|WebCoT Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback]] (78.3% similar)
- [[Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (77.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Amirhossein Yousefiramandi, Ciaran Cooney

## ğŸ“„ Abstract (ì›ë¬¸)

Transformer-based language models such as BERT have become foundational in
NLP, yet their performance degrades in specialized domains like patents, which
contain long, technical, and legally structured text. Prior approaches to
patent NLP have primarily relied on fine-tuning general-purpose models or
domain-adapted variants pretrained with limited data. In this work, we pretrain
3 domain-specific masked language models for patents, using the ModernBERT
architecture and a curated corpus of over 60 million patent records. Our
approach incorporates architectural optimizations, including FlashAttention,
rotary embeddings, and GLU feed-forward layers. We evaluate our models on four
downstream patent classification tasks. Our model, ModernBERT-base-PT,
consistently outperforms the general-purpose ModernBERT baseline on three out
of four datasets and achieves competitive performance with a baseline
PatentBERT. Additional experiments with ModernBERT-base-VX and
Mosaic-BERT-large demonstrate that scaling the model size and customizing the
tokenizer further enhance performance on selected tasks. Notably, all
ModernBERT variants retain substantially faster inference over - 3x that of
PatentBERT - underscoring their suitability for time-sensitive applications.
These results underscore the benefits of domain-specific pretraining and
architectural improvements for patent-focused NLP tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸, ì˜ˆë¥¼ ë“¤ì–´ BERTëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ ê¸°ë³¸ì ì¸ ì—­í• ì„ í•˜ê³  ìˆì§€ë§Œ, íŠ¹í—ˆì™€ ê°™ì´ ê¸¸ê³  ê¸°ìˆ ì ì´ë©° ë²•ì ìœ¼ë¡œ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ì „ë¬¸ ë¶„ì•¼ì—ì„œëŠ” ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. ì´ì „ì˜ íŠ¹í—ˆ NLP ì ‘ê·¼ë²•ì€ ì£¼ë¡œ ì¼ë°˜ ëª©ì ì˜ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê±°ë‚˜ ì œí•œëœ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµëœ ë„ë©”ì¸ ì ì‘ ë³€í˜•ì— ì˜ì¡´í•´ ì™”ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ModernBERT ì•„í‚¤í…ì²˜ì™€ 6ì²œë§Œ ê±´ ì´ìƒì˜ íŠ¹í—ˆ ê¸°ë¡ìœ¼ë¡œ êµ¬ì„±ëœ íë ˆì´ì…˜ëœ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹í—ˆë¥¼ ìœ„í•œ 3ê°œì˜ ë„ë©”ì¸ íŠ¹í™” ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ FlashAttention, íšŒì „ ì„ë² ë”©, GLU í”¼ë“œí¬ì›Œë“œ ë ˆì´ì–´ë¥¼ í¬í•¨í•œ ì•„í‚¤í…ì²˜ ìµœì í™”ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë„¤ ê°€ì§€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íŠ¹í—ˆ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ìš°ë¦¬ì˜ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª¨ë¸ì¸ ModernBERT-base-PTëŠ” ë„¤ ê°œì˜ ë°ì´í„°ì…‹ ì¤‘ ì„¸ ê°œì—ì„œ ì¼ë°˜ ëª©ì ì˜ ModernBERT ê¸°ì¤€ ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, ê¸°ì¤€ ëª¨ë¸ì¸ PatentBERTì™€ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ModernBERT-base-VX ë° Mosaic-BERT-largeì™€ì˜ ì¶”ê°€ ì‹¤í—˜ì€ ëª¨ë¸ í¬ê¸°ë¥¼ í™•ì¥í•˜ê³  í† í¬ë‚˜ì´ì €ë¥¼ ë§ì¶¤í™”í•˜ëŠ” ê²ƒì´ ì„ íƒëœ ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ëª¨ë“  ModernBERT ë³€í˜•ì€ PatentBERTì˜ 3ë°° ì´ìƒ ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ìœ ì§€í•˜ì—¬ ì‹œê°„ ë¯¼ê°í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” íŠ¹í—ˆ ì¤‘ì‹¬ì˜ NLP ì‘ì—…ì„ ìœ„í•œ ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „ í•™ìŠµê³¼ ì•„í‚¤í…ì²˜ ê°œì„ ì˜ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” íŠ¹í—ˆ ë¶„ì•¼ì˜ NLP ì‘ì—…ì— ì í•©í•œ ë„ë©”ì¸ íŠ¹í™” ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ModernBERT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, 6ì²œë§Œ ê°œ ì´ìƒì˜ íŠ¹í—ˆ ê¸°ë¡ì„ ì‚¬ìš©í•˜ì—¬ ì„¸ ê°€ì§€ ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. FlashAttention, íšŒì „ ì„ë² ë”©, GLU í”¼ë“œí¬ì›Œë“œ ë ˆì´ì–´ ë“± ì•„í‚¤í…ì²˜ ìµœì í™”ë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤. ë„¤ ê°€ì§€ íŠ¹í—ˆ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í‰ê°€í•œ ê²°ê³¼, ModernBERT-base-PT ëª¨ë¸ì€ ì¼ë°˜ì ì¸ ModernBERTë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, PatentBERTì™€ë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤. ëª¨ë¸ í¬ê¸° í™•ì¥ê³¼ í† í¬ë‚˜ì´ì € ë§ì¶¤í™”ëŠ” ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œì¼°ìœ¼ë©°, ëª¨ë“  ModernBERT ë³€í˜• ëª¨ë¸ì€ PatentBERTë³´ë‹¤ 3ë°° ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹í—ˆ ì¤‘ì‹¬ NLP ì‘ì—…ì—ì„œ ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „ í›ˆë ¨ê³¼ ì•„í‚¤í…ì²˜ ê°œì„ ì˜ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¹í—ˆ ë¶„ì•¼ì˜ NLP ì‘ì—…ì—ì„œ ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „ í•™ìŠµê³¼ ì•„í‚¤í…ì²˜ ê°œì„ ì˜ ì´ì ì´ ê°•ì¡°ë˜ì—ˆìŠµë‹ˆë‹¤.

- 2. ModernBERT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ 3ê°œì˜ ë„ë©”ì¸ íŠ¹í™” ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.

- 3. ModernBERT-base-PT ëª¨ë¸ì€ 4ê°œì˜ íŠ¹í—ˆ ë¶„ë¥˜ ì‘ì—… ì¤‘ 3ê°œì—ì„œ ì¼ë°˜ ëª©ì ì˜ ModernBERTë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

- 4. ëª¨ë¸ í¬ê¸° í™•ì¥ê³¼ í† í¬ë‚˜ì´ì € ë§ì¶¤í™”ê°€ íŠ¹ì • ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

- 5. ëª¨ë“  ModernBERT ë³€í˜• ëª¨ë¸ì€ PatentBERTë³´ë‹¤ 3ë°° ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ìœ ì§€í•˜ì—¬ ì‹œê°„ ë¯¼ê°í˜• ì‘ìš©ì— ì í•©í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 02:43:07*