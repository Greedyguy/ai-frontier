# Stochastic Bilevel Optimization with Heavy-Tailed Noise

**Korean Title:** í™•ë¥ ì  ì´ì¸µ ìµœì í™”ì™€ ë‘êº¼ìš´ ê¼¬ë¦¬ ì¡ìŒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Zhuanghua Liu|Zhuanghua Liu]] [[authors/Luo Luo|Luo Luo]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Nonconvex Strongly Concave Minimax Optimization

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (83.0% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (81.5% similar)
- [[Optimal Algorithms for Bandit Learning in Matching Markets_20250919|Optimal Algorithms for Bandit Learning in Matching Markets]] (81.1% similar)
- [[Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits_20250919|Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits]] (80.7% similar)
- [[A Universal Banach--Bregman Framework for Stochastic Iterations_ Unifying Stochastic Mirror Descent, Learning and LLM Training_20250917|A Universal Banach--Bregman Framework for Stochastic Iterations Unifying Stochastic Mirror Descent, Learning and LLM Training]] (80.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Zhuanghua Liu, Luo Luo

## ğŸ“„ Abstract (ì›ë¬¸)

This paper considers the smooth bilevel optimization in which the lower-level
problem is strongly convex and the upper-level problem is possibly nonconvex.
We focus on the stochastic setting that the algorithm can access the unbiased
stochastic gradient evaluation with heavy-tailed noise, which is prevalent in
many machine learning applications such as training large language models and
reinforcement learning. We propose a nested-loop normalized stochastic bilevel
approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the
stochastic first-order oracle (SFO) complexity of
$\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number,
$p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the
noise level. Furthermore, we specialize our idea to solve the
nonconvex-strongly-concave minimax optimization problem, achieving an
$\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal
O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known
results under the special case of the bounded variance setting, i.e., $p=2$.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì´ ë…¼ë¬¸ì€ í•˜ìœ„ ìˆ˜ì¤€ ë¬¸ì œê°€ ê°•í•˜ê²Œ ë³¼ë¡í•˜ê³  ìƒìœ„ ìˆ˜ì¤€ ë¬¸ì œê°€ ë¹„ë³¼ë¡ì¼ ìˆ˜ ìˆëŠ” ë§¤ë„ëŸ¬ìš´ ì´ìˆ˜ì¤€ ìµœì í™”ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ë° ê°•í™” í•™ìŠµê³¼ ê°™ì€ ë§ì€ ê¸°ê³„ í•™ìŠµ ì‘ìš©ì—ì„œ ì¼ë°˜ì ì¸ ë¬´ê±°ìš´ ê¼¬ë¦¬ ì¡ìŒì„ ê°€ì§„ í¸í–¥ë˜ì§€ ì•Šì€ í™•ë¥ ì  ê¸°ìš¸ê¸° í‰ê°€ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì  ì„¤ì •ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¤‘ì²© ë£¨í”„ ì •ê·œí™” í™•ë¥ ì  ì´ìˆ˜ì¤€ ê·¼ì‚¬(N$^2$SBA)ë¥¼ ì œì•ˆí•˜ì—¬ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ë„ê°€ $\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$ì¸ $\epsilon$-ì •ë¥˜ì ($\epsilon$-stationary point)ì„ ì°¾ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ $\kappa$ëŠ” ì¡°ê±´ ìˆ˜ì´ê³ , $p\in(1,2]$ëŠ” ì¡ìŒì˜ ì¤‘ì‹¬ ëª¨ë©˜íŠ¸ì˜ ì°¨ìˆ˜ì´ë©°, $\sigma$ëŠ” ì¡ìŒ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ë”ìš±ì´, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ì•„ì´ë””ì–´ë¥¼ ë¹„ë³¼ë¡-ê°•í•˜ê²Œ ì˜¤ëª©í•œ ë¯¸ë‹ˆë§¥ìŠ¤ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° íŠ¹í™”í•˜ì—¬, SFO ë³µì¡ë„ê°€ $\tilde{\mathcal O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{3p-2}{p-1}}\big)$ì¸ $\epsilon$-ì •ë¥˜ì ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ìœ„ì˜ ëª¨ë“  ìƒí•œì€ ì œí•œëœ ë¶„ì‚° ì„¤ì •, ì¦‰ $p=2$ì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ì—ì„œ ê°€ì¥ ì˜ ì•Œë ¤ì§„ ê²°ê³¼ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í•˜ìœ„ ë¬¸ì œê°€ ê°•í•˜ê²Œ ë³¼ë¡í•˜ê³  ìƒìœ„ ë¬¸ì œê°€ ë¹„ë³¼ë¡ì¼ ìˆ˜ ìˆëŠ” ë§¤ë„ëŸ¬ìš´ ì´ìˆ˜ì¤€ ìµœì í™” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨ ë° ê°•í™” í•™ìŠµê³¼ ê°™ì€ ë§ì€ ê¸°ê³„ í•™ìŠµ ì‘ìš©ì—ì„œ í”íˆ ë°œìƒí•˜ëŠ” ë¬´ê±°ìš´ ê¼¬ë¦¬ ì¡ìŒì´ ìˆëŠ” í¸í–¥ë˜ì§€ ì•Šì€ í™•ë¥ ì  ê¸°ìš¸ê¸° í‰ê°€ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì  ì„¤ì •ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. ì €ìë“¤ì€ $\epsilon$-ì •ì§€ì ì„ ì°¾ê¸° ìœ„í•œ ì¤‘ì²© ë£¨í”„ ì •ê·œí™” í™•ë¥ ì  ì´ìˆ˜ì¤€ ê·¼ì‚¬(N$^2$SBA) ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ë„ê°€ $\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$ì…ë‹ˆë‹¤. ë˜í•œ, ë¹„ë³¼ë¡-ê°•í•˜ê²Œ ì˜¤ëª©í•œ ë¯¸ë‹ˆë§¥ìŠ¤ ìµœì í™” ë¬¸ì œì— ì´ ì•„ì´ë””ì–´ë¥¼ ì ìš©í•˜ì—¬ $\epsilon$-ì •ì§€ì ì„ SFO ë³µì¡ë„ $\tilde{\mathcal O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{3p-2}{p-1}}\big)$ë¡œ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŠ” ì œí•œëœ ë¶„ì‚° ì„¤ì •($p=2$)ì—ì„œ ì•Œë ¤ì§„ ìµœìƒì˜ ê²°ê³¼ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ í•˜ìœ„ ë¬¸ì œê°€ ê°•í•˜ê²Œ ë³¼ë¡í•˜ê³  ìƒìœ„ ë¬¸ì œê°€ ë¹„ë³¼ë¡ì¼ ìˆ˜ ìˆëŠ” ë§¤ë„ëŸ¬ìš´ ì´ìˆ˜ì¤€ ìµœì í™”ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

- 2. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ì¤‘ì²© ë£¨í”„ ì •ê·œí™” í™•ë¥ ì  ì´ìˆ˜ì¤€ ê·¼ì‚¬(N$^2$SBA)ë¡œ, $\epsilon$-ì •ë¥˜ì  ì°¾ê¸°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

- 3. ì œì•ˆëœ ë°©ë²•ì€ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ë„ê°€ $\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$ë¡œ í‰ê°€ë©ë‹ˆë‹¤.

- 4. ë¹„ë³¼ë¡-ê°•í•˜ê²Œ ì˜¤ëª©í•œ ë¯¸ë‹ˆë§¥ìŠ¤ ìµœì í™” ë¬¸ì œì— ëŒ€í•´ ì œì•ˆëœ ë°©ë²•ì„ íŠ¹í™”í•˜ì—¬ $\epsilon$-ì •ë¥˜ì ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì˜ ìƒí•œì€ ì œí•œëœ ë¶„ì‚° ì„¤ì •($p=2$)ì—ì„œ ì•Œë ¤ì§„ ìµœìƒì˜ ê²°ê³¼ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 01:42:24*