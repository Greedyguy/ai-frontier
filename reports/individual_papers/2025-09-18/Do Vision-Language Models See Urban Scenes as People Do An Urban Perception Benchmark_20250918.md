# Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark

**Korean Title:** ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì€ ë„ì‹œ ì¥ë©´ì„ ì‚¬ëŒì²˜ëŸ¼ ì¸ì‹í•˜ëŠ”ê°€? ë„ì‹œ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Rashid Mushkani|Rashid Mushkani]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Participatory Urban Analysis

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[From Pixels to Urban Policy-Intelligence_ Recovering Legacy Effects of Redlining with a Multimodal LLM_20250919|From Pixels to Urban Policy-Intelligence Recovering Legacy Effects of Redlining with a Multimodal LLM]] (83.0% similar)
- [[The Art of Saying Maybe_ A Conformal Lens for Uncertainty Benchmarking in VLMs_20250918|The Art of Saying Maybe A Conformal Lens for Uncertainty Benchmarking in VLMs]] (79.6% similar)
- [[Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (79.3% similar)
- [[MINGLE_ VLMs for Semantically Complex Region Detection in Urban Scenes_20250918|MINGLE VLMs for Semantically Complex Region Detection in Urban Scenes]] (79.1% similar)
- [[V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (78.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Rashid Mushkani

## ğŸ“„ Abstract (ì›ë¬¸)

Understanding how people read city scenes can inform design and planning. We
introduce a small benchmark for testing vision-language models (VLMs) on urban
perception using 100 Montreal street images, evenly split between photographs
and photorealistic synthetic scenes. Twelve participants from seven community
groups supplied 230 annotation forms across 30 dimensions mixing physical
attributes and subjective impressions. French responses were normalized to
English. We evaluated seven VLMs in a zero-shot setup with a structured prompt
and deterministic parser. We use accuracy for single-choice items and Jaccard
overlap for multi-label items; human agreement uses Krippendorff's alpha and
pairwise Jaccard. Results suggest stronger model alignment on visible,
objective properties than subjective appraisals. The top system (claude-sonnet)
reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human
agreement coincides with better model scores. Synthetic images slightly lower
scores. We release the benchmark, prompts, and harness for reproducible,
uncertainty-aware evaluation in participatory urban analysis.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë„ì‹œ ì¥ë©´ì„ ì‚¬ëŒë“¤ì´ ì–´ë–»ê²Œ ì½ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ ì„¤ê³„ì™€ ê³„íšì— ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë„ì‹œ ì¸ì‹ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ 100ê°œì˜ ëª¬íŠ¸ë¦¬ì˜¬ ê±°ë¦¬ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì§„ê³¼ ì‚¬ì‹¤ì ì¸ í•©ì„± ì¥ë©´ì„ ê· ë“±í•˜ê²Œ ë‚˜ëˆˆ ì†Œê·œëª¨ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. 7ê°œì˜ ì»¤ë®¤ë‹ˆí‹° ê·¸ë£¹ì—ì„œ ì˜¨ 12ëª…ì˜ ì°¸ê°€ìê°€ ë¬¼ë¦¬ì  ì†ì„±ê³¼ ì£¼ê´€ì  ì¸ìƒì„ í˜¼í•©í•œ 30ê°œ ì°¨ì›ì— ê±¸ì³ 230ê°œì˜ ì£¼ì„ ì–‘ì‹ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. í”„ë‘ìŠ¤ì–´ ì‘ë‹µì€ ì˜ì–´ë¡œ í‘œì¤€í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ì™€ ê²°ì •ë¡ ì  íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì œë¡œìƒ· ì„¤ì •ì—ì„œ 7ê°œì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ë‹¨ì¼ ì„ íƒ í•­ëª©ì—ëŠ” ì •í™•ë„ë¥¼ ì‚¬ìš©í•˜ê³ , ë‹¤ì¤‘ ë ˆì´ë¸” í•­ëª©ì—ëŠ” ìì¹´ë“œ ì¤‘ì²©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¸ê°„ì˜ í•©ì˜ëŠ” Krippendorffì˜ ì•ŒíŒŒì™€ ìŒë³„ ìì¹´ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ì£¼ê´€ì  í‰ê°€ë³´ë‹¤ ê°€ì‹œì ì´ê³  ê°ê´€ì ì¸ ì†ì„±ì—ì„œ ëª¨ë¸ ì •ë ¬ì´ ë” ê°•í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ìµœê³  ì‹œìŠ¤í…œ(claude-sonnet)ì€ ë‹¤ì¤‘ ë ˆì´ë¸” í•­ëª©ì—ì„œ ë§¤í¬ë¡œ 0.31ê³¼ í‰ê·  ìì¹´ë“œ 0.48ì— ë„ë‹¬í•©ë‹ˆë‹¤. ì¸ê°„ì˜ í•©ì˜ê°€ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ ì ìˆ˜ê°€ ë” ì¢‹ìŠµë‹ˆë‹¤. í•©ì„± ì´ë¯¸ì§€ëŠ” ì ìˆ˜ë¥¼ ì•½ê°„ ë‚®ì¶¥ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì°¸ì—¬ì  ë„ì‹œ ë¶„ì„ì—ì„œ ì¬í˜„ ê°€ëŠ¥í•˜ê³  ë¶ˆí™•ì‹¤ì„±ì„ ì¸ì‹í•˜ëŠ” í‰ê°€ë¥¼ ìœ„í•´ ë²¤ì¹˜ë§ˆí¬, í”„ë¡¬í”„íŠ¸ ë° í•˜ë„¤ìŠ¤ë¥¼ ê³µê°œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë„ì‹œ ì¥ë©´ì„ ì½ëŠ” ë°©ë²•ì„ ì´í•´í•˜ì—¬ ë””ìì¸ê³¼ ê³„íšì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì—°êµ¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ëª¬íŠ¸ë¦¬ì˜¬ ê±°ë¦¬ ì´ë¯¸ì§€ 100ì¥ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì‘ì€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. 7ê°œ ì»¤ë®¤ë‹ˆí‹° ê·¸ë£¹ì˜ 12ëª…ì˜ ì°¸ê°€ìê°€ 30ê°œ ì°¨ì›ì—ì„œ ë¬¼ë¦¬ì  ì†ì„±ê³¼ ì£¼ê´€ì  ì¸ìƒì„ í˜¼í•©í•œ 230ê°œì˜ ì£¼ì„ ì–‘ì‹ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. 7ê°œì˜ VLMsë¥¼ ì œë¡œìƒ· ì„¤ì •ìœ¼ë¡œ í‰ê°€í–ˆìœ¼ë©°, ê²°ê³¼ëŠ” ê°€ì‹œì ì´ê³  ê°ê´€ì ì¸ ì†ì„±ì—ì„œ ëª¨ë¸ì˜ ì¼ì¹˜ë„ê°€ ë” ë†’ìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ìµœê³  ì„±ëŠ¥ì„ ë³´ì¸ ì‹œìŠ¤í…œì€ ë‹¤ì¤‘ ë ˆì´ë¸” í•­ëª©ì—ì„œ ë§¤í¬ë¡œ 0.31ê³¼ í‰ê·  ìì¹´ë“œ 0.48ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì¸ê°„ì˜ í•©ì˜ë„ê°€ ë†’ì€ ê²½ìš° ëª¨ë¸ ì ìˆ˜ë„ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ì¸ê³µ ì´ë¯¸ì§€ëŠ” ì ìˆ˜ë¥¼ ì•½ê°„ ë‚®ì·„ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë²¤ì¹˜ë§ˆí¬, í”„ë¡¬í”„íŠ¸ ë° í‰ê°€ ë„êµ¬ë¥¼ ê³µê°œí•˜ì—¬ ì°¸ì—¬í˜• ë„ì‹œ ë¶„ì„ì—ì„œ ì¬í˜„ ê°€ëŠ¥í•˜ê³  ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë„ì‹œ ì¥ë©´ì„ ì½ëŠ” ë°©ë²•ì— ëŒ€í•œ ì´í•´ëŠ” ë””ìì¸ê³¼ ê³„íšì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 2. ëª¬íŠ¸ë¦¬ì˜¬ ê±°ë¦¬ ì´ë¯¸ì§€ 100ì¥ì„ ì‚¬ìš©í•˜ì—¬ ë„ì‹œ ì¸ì‹ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs) ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

- 3. 7ê°œì˜ ì»¤ë®¤ë‹ˆí‹° ê·¸ë£¹ì—ì„œ 12ëª…ì˜ ì°¸ê°€ìê°€ 30ê°€ì§€ ì°¨ì›ì— ê±¸ì³ 230ê°œì˜ ì£¼ì„ ì–‘ì‹ì„ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤.

- 4. ì—°êµ¬ ê²°ê³¼, ëª¨ë¸ì€ ê°€ì‹œì ì´ê³  ê°ê´€ì ì¸ ì†ì„±ì— ëŒ€í•´ ë” ê°•í•œ ì¼ì¹˜ë¥¼ ë³´ì˜€ìœ¼ë©°, ì£¼ê´€ì ì¸ í‰ê°€ì—ì„œëŠ” ì•½í•œ ì¼ì¹˜ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

- 5. ë²¤ì¹˜ë§ˆí¬, í”„ë¡¬í”„íŠ¸ ë° í‰ê°€ ë„êµ¬ë¥¼ ê³µê°œí•˜ì—¬ ì°¸ì—¬í˜• ë„ì‹œ ë¶„ì„ì—ì„œ ì¬í˜„ ê°€ëŠ¥í•˜ê³  ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ í‰ê°€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:51:28*