---
keywords:
  - Self-Supervised Learning
  - Dependency Controlled Pre-training
  - Instance-wise Patch Normalization
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:33:24.537285",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-Supervised Learning",
    "Dependency Controlled Pre-training",
    "Instance-wise Patch Normalization"
  ],
  "rejected_keywords": [
    "Hierarchical Dependency Controlled Learning",
    "Instance-level Contrastive Module"
  ],
  "similarity_scores": {
    "Self-Supervised Learning": 0.82,
    "Dependency Controlled Pre-training": 0.78,
    "Instance-wise Patch Normalization": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training

**Korean Title:** DeCoP: ì¢…ì†ì„± ì œì–´ ì‚¬ì „ í›ˆë ¨ì„ í†µí•œ ìê¸° ì§€ë„í˜• ì‹œê³„ì—´ í‘œí˜„ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]        [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Self-Supervised Learning|Self-Supervised Time Series Representation]]
**âš¡ Unique Technical**: [[keywords/Dependency Controlled Pre-training|Dependency Controlled Pre-training]], [[keywords/Instance-wise Patch Normalization|Instance-wise Patch Normalization]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Beyond Marginals_ Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection_20250918|Beyond Marginals Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection]] (78.7% similar)
- [[Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (78.3% similar)
- [[DPANet_ Dual Pyramid Attention Network for Multivariate Time Series Forecasting_20250918|DPANet Dual Pyramid Attention Network for Multivariate Time Series Forecasting]] (77.8% similar)
- [[Super-Linear_ A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting_20250918|Super-Linear A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting]] (77.7% similar)
- [[ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (77.0% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Yuemin Wu, Zhongze Wu, Xiu Su, Feng Yang, Hongyan Xu, Xi Lin, Wenti Huang, Shan You, Chang Xu

## ğŸ“„ Abstract (ì›ë¬¸)

Modeling dynamic temporal dependencies is a critical challenge in time series
pre-training, which evolve due to distribution shifts and multi-scale patterns.
This temporal variability severely impairs the generalization of pre-trained
models to downstream tasks. Existing frameworks fail to capture the complex
interactions of short- and long-term dependencies, making them susceptible to
spurious correlations that degrade generalization. To address these
limitations, we propose DeCoP, a Dependency Controlled Pre-training framework
that explicitly models dynamic, multi-scale dependencies by simulating evolving
inter-patch dependencies. At the input level, DeCoP introduces Instance-wise
Patch Normalization (IPN) to mitigate distributional shifts while preserving
the unique characteristics of each patch, creating a robust foundation for
representation learning. At the latent level, a hierarchical Dependency
Controlled Learning (DCL) strategy explicitly models inter-patch dependencies
across multiple temporal scales, with an Instance-level Contrastive Module
(ICM) enhances global generalization by learning instance-discriminative
representations from time-invariant positive pairs. DeCoP achieves
state-of-the-art results on ten datasets with lower computing resources,
improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë™ì  ì‹œê°„ ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì€ ì‹œê³„ì—´ ì‚¬ì „ í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ê³¼ì œì…ë‹ˆë‹¤. ì´ëŠ” ë¶„í¬ ë³€í™”ì™€ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŒ¨í„´ìœ¼ë¡œ ì¸í•´ ì§„í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹œê°„ì  ë³€ë™ì„±ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ì‹¬ê°í•˜ê²Œ ì €í•´í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¨ê¸° ë° ì¥ê¸° ì˜ì¡´ì„±ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ í¬ì°©í•˜ì§€ ëª»í•˜ì—¬, ì¼ë°˜í™”ë¥¼ ì €í•´í•˜ëŠ” ì˜ëª»ëœ ìƒê´€ê´€ê³„ì— ì·¨ì•½í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” DeCoP(Dependency Controlled Pre-training)ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì§„í™”í•˜ëŠ” íŒ¨ì¹˜ ê°„ ì˜ì¡´ì„±ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ë™ì ì´ê³  ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ì˜ ì˜ì¡´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì…ë ¥ ìˆ˜ì¤€ì—ì„œ DeCoPëŠ” ì¸ìŠ¤í„´ìŠ¤ë³„ íŒ¨ì¹˜ ì •ê·œí™”(IPN)ë¥¼ ë„ì…í•˜ì—¬ ê° íŒ¨ì¹˜ì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë¶„í¬ ë³€í™”ë¥¼ ì™„í™”í•˜ì—¬ í‘œí˜„ í•™ìŠµì„ ìœ„í•œ ê²¬ê³ í•œ ê¸°ë°˜ì„ ë§Œë“­ë‹ˆë‹¤. ì ì¬ ìˆ˜ì¤€ì—ì„œëŠ” ê³„ì¸µì  ì˜ì¡´ì„± ì œì–´ í•™ìŠµ(DCL) ì „ëµì´ ì—¬ëŸ¬ ì‹œê°„ì  ìŠ¤ì¼€ì¼ì— ê±¸ì³ íŒ¨ì¹˜ ê°„ ì˜ì¡´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ë©°, ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ì˜ ëŒ€ì¡° ëª¨ë“ˆ(ICM)ì€ ì‹œê°„ ë¶ˆë³€ì˜ ê¸ì • ìŒì—ì„œ ì¸ìŠ¤í„´ìŠ¤ êµ¬ë³„ í‘œí˜„ì„ í•™ìŠµí•˜ì—¬ ì „ë°˜ì ì¸ ì¼ë°˜í™”ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. DeCoPëŠ” 10ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, PatchTST ëŒ€ë¹„ ETTh1ì—ì„œ MSEë¥¼ 3% ê°œì„ í•˜ë©´ì„œë„ ì—°ì‚° ìì›ì„ 37%ë§Œ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

DeCoPëŠ” ì‹œê³„ì—´ ë°ì´í„°ì˜ ë™ì  ì‹œê°„ ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ê¸°ì¡´ ëª¨ë¸ë“¤ì´ ë‹¨ê¸° ë° ì¥ê¸° ì˜ì¡´ì„±ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ í¬ì°©í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. DeCoPëŠ” ì¸ìŠ¤í„´ìŠ¤ë³„ íŒ¨ì¹˜ ì •ê·œí™”(IPN)ë¥¼ í†µí•´ ë¶„í¬ ë³€í™”ì— ëŒ€ì‘í•˜ê³ , ê³„ì¸µì  ì˜ì¡´ì„± ì œì–´ í•™ìŠµ(DCL) ì „ëµìœ¼ë¡œ ì—¬ëŸ¬ ì‹œê°„ ê·œëª¨ì— ê±¸ì¹œ íŒ¨ì¹˜ ê°„ ì˜ì¡´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ë˜í•œ, ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ëŒ€ì¡° ëª¨ë“ˆ(ICM)ì„ í†µí•´ ì „ì—­ ì¼ë°˜í™”ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. DeCoPëŠ” 10ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, PatchTST ëŒ€ë¹„ 37%ì˜ ì—°ì‚° ìì›ìœ¼ë¡œ ETTh1ì—ì„œ MSEë¥¼ 3% ê°œì„ í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹œê°„ì  ì˜ì¡´ì„± ëª¨ë¸ë§ì€ ì‹œê³„ì—´ ì‚¬ì „ í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ë„ì „ ê³¼ì œë¡œ, ë¶„í¬ ë³€í™”ì™€ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŒ¨í„´ìœ¼ë¡œ ì¸í•´ ë°œì „í•©ë‹ˆë‹¤.

- 2. ê¸°ì¡´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¨ê¸° ë° ì¥ê¸° ì˜ì¡´ì„±ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ í¬ì°©í•˜ì§€ ëª»í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.

- 3. DeCoPëŠ” ë™ì , ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì˜ì¡´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì´ëŸ¬í•œ ì œí•œì„ ê·¹ë³µí•©ë‹ˆë‹¤.

- 4. ì…ë ¥ ìˆ˜ì¤€ì—ì„œ DeCoPëŠ” ì¸ìŠ¤í„´ìŠ¤ë³„ íŒ¨ì¹˜ ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ë¶„í¬ ë³€í™” ì™„í™”ì™€ íŒ¨ì¹˜ì˜ ê³ ìœ  íŠ¹ì„±ì„ ë³´ì¡´í•©ë‹ˆë‹¤.

- 5. DeCoPëŠ” 10ê°œ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, PatchTST ëŒ€ë¹„ 37%ì˜ FLOPsë§Œìœ¼ë¡œ MSEë¥¼ 3% ê°œì„ í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:47:31*