---
keywords:
  - Foundation Models
  - Self-Supervised Learning
  - Mixture-of-Experts
category: cs.AI
publish_date: 2025-09-18
arxiv_id: 2509.14104
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:36:17.332000",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Foundation Models",
    "Self-Supervised Learning",
    "Mixture-of-Experts"
  ],
  "rejected_keywords": [
    "Masked Autoencoders"
  ],
  "similarity_scores": {
    "Foundation Models": 0.78,
    "Self-Supervised Learning": 0.8,
    "Mixture-of-Experts": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts

**Korean Title:** CSMoE: ë¶€ë“œëŸ¬ìš´ ì „ë¬¸ê°€ í˜¼í•©ì„ ê°–ì¶˜ íš¨ìœ¨ì ì¸ ì›ê²© ê°ì§€ ê¸°ë°˜ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Self-Supervised Learning|Self-supervised learning]]
**âš¡ Unique Technical**: [[keywords/Mixture-of-Experts|Soft mixture-of-experts]]
**ğŸš€ Evolved Concepts**: [[keywords/Foundation Models|Remote sensing foundation model]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Semi-MoE_Mixture-of-Experts_meets_Semi-Supervised_Histopathology_Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (82.5% similar)
- [[Masked_Feature_Modeling_Enhances_Adaptive_Segmentation_20250918|Masked Feature Modeling Enhances Adaptive Segmentation]] (81.1% similar)
- [[Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection]] (81.1% similar)
- [[NavMoE_Hybrid_Model-_and_Learning-based_Traversability_Estimation_for_Local_Navigation_via_Mixture_of_Experts_20250918|NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts]] (79.5% similar)
- [[Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions]] (79.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14104v1 Announce Type: new 
Abstract: Self-supervised learning through masked autoencoders has attracted great attention for remote sensing (RS) foundation model (FM) development, enabling improved representation learning across diverse sensors and downstream tasks. However, existing RS FMs often either suffer from substantial computational complexity during both training and inference or exhibit limited representational capacity. These issues restrict their practical applicability in RS. To address this limitation, we propose an adaptation for enhancing the efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism into the FM. The integration of Soft MoEs into the FM allows modality-specific expert specialization alongside shared cross-sensor representation learning. To demonstrate the effectiveness of our adaptation, we apply it on the Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic descriptor-driven sampling strategy for the construction of a representative and diverse training set to train our CSMoE model. Extensive experiments on scene classification, semantic segmentation, and content-based image retrieval demonstrate that our adaptation yields a reduction in computational requirements while maintaining or improving representational performance. Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off between representational capacity, accuracy, and computational efficiency. On average, CSMoE achieves more than twice the computational efficiency of existing RS FMs, while maintaining competitive performance across all experiments. These results show the effectiveness of the proposed adaptation for creating computationally efficient RS FMs. The code for the model, the training set creation, and the model weights will be available at https://git.tu-berlin.de/rsim/csmoe.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14104v1 ë°œí‘œ ìœ í˜•: ìƒˆë¡œìš´
ìš”ì•½: ê°€ë¦¬ê³  ìˆëŠ” ì˜¤í† ì¸ì½”ë”ë¥¼ í†µí•œ ìê¸° ì§€ë„ í•™ìŠµì€ ì›ê²© ê°ì§€(RS) ê¸°ë°˜ ëª¨ë¸(FM) ê°œë°œì— í° ê´€ì‹¬ì„ ëŒê³  ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ì„¼ì„œ ë° í•˜ë¥˜ ì‘ì—…ì— ê±¸ì³ í–¥ìƒëœ í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ RS FMsëŠ” ì¢…ì¢… í›ˆë ¨ ë° ì¶”ë¡  ì¤‘ì— ìƒë‹¹í•œ ê³„ì‚° ë³µì¡ì„±ì„ ê²ªê±°ë‚˜ í‘œí˜„ ëŠ¥ë ¥ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ RSì—ì„œì˜ ì‹¤ìš©ì  ì ìš©ì„ ì œí•œí•©ë‹ˆë‹¤. ì´ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Soft mixture-of-experts (MoE) ë©”ì»¤ë‹ˆì¦˜ì„ FMì— í†µí•©í•˜ì—¬ RS FMsì˜ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì ì‘ì„ ì œì•ˆí•©ë‹ˆë‹¤. Soft MoEsë¥¼ FMì— í†µí•©í•¨ìœ¼ë¡œì¨, FMì—ëŠ” ëª¨ë‹¬ë¦¬í‹°ë³„ ì „ë¬¸ê°€ ì „ë¬¸í™”ì™€ í•¨ê»˜ ê³µìœ ëœ êµì°¨ ì„¼ì„œ í‘œí˜„ í•™ìŠµì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ì‘ì˜ íš¨ê³¼ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Cross-Sensor Masked Autoencoder (CSMAE) ëª¨ë¸ì— ì´ë¥¼ ì ìš©í•˜ì—¬ Cross-Sensor Mixture-of-Experts (CSMoE) ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ëŒ€í‘œì ì´ê³  ë‹¤ì–‘í•œ í›ˆë ¨ ì„¸íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í…Œë§ˆ-ê¸°í›„ì  ì„¤ëª…ì ì£¼ë„ ìƒ˜í”Œë§ ì „ëµì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ CSMoE ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´. ì¥ë©´ ë¶„ë¥˜, ì˜ë¯¸ì  ë¶„í•  ë° ì½˜í…ì¸  ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ì ì‘ì´ ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì„ ì¤„ì´ë©´ì„œ í‘œí˜„ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê±°ë‚˜ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìµœì‹  ê¸°ìˆ ì˜ RS FMsì™€ ë¹„êµí–ˆì„ ë•Œ, CSMoEëŠ” í‘œí˜„ ëŠ¥ë ¥, ì •í™•ë„ ë° ê³„ì‚° íš¨ìœ¨ì„± ì‚¬ì´ì—ì„œ ìš°ìˆ˜í•œ ê· í˜•ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. í‰ê· ì ìœ¼ë¡œ, CSMoEëŠ” ê¸°ì¡´ì˜ RS FMsë³´ë‹¤ ë‘ ë°° ì´ìƒì˜ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ë©´ì„œ ëª¨ë“  ì‹¤í—˜ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì œì•ˆëœ ì ì‘ì´ ê³„ì‚° íš¨ìœ¨ì ì¸ RS FMsë¥¼ ë§Œë“œëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë¸, í›ˆë ¨ ì„¸íŠ¸ ìƒì„± ë° ëª¨ë¸ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ì½”ë“œëŠ” https://git.tu-berlin.de/rsim/csmoeì—ì„œ ì œê³µë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë³¸ ì—°êµ¬ëŠ” ì›ê²© ê°ì§€ë¥¼ ìœ„í•œ ìê¸° ê°ë… í•™ìŠµì„ í†µí•´ íš¨ìœ¨ì ì¸ í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” Soft MoE ë©”ì»¤ë‹ˆì¦˜ì„ RS ê¸°ë°˜ ëª¨ë¸ì— í†µí•©í•˜ì—¬ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. Cross-Sensor Masked Autoencoder (CSMAE) ëª¨ë¸ì— ì´ë¥¼ ì ìš©í•œ Cross-Sensor Mixture-of-Experts (CSMoE) ëª¨ë¸ì„ ì†Œê°œí•˜ì˜€ìœ¼ë©°, ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ì ì‘ ë°©ë²•ì´ ê¸°ì¡´ RS ëª¨ë¸ì— ë¹„í•´ ë›°ì–´ë‚œ í‘œí˜„ ëŠ¥ë ¥, ì •í™•ë„ ë° ê³„ì‚° íš¨ìœ¨ì„±ì„ ì œê³µí•¨ì„ ì…ì¦í•˜ì˜€ë‹¤. ì´ë¥¼ í†µí•´ ì œì•ˆëœ ì ì‘ ë°©ë²•ì´ ê³„ì‚°ì ìœ¼ë¡œ íš¨ìœ¨ì ì¸ RS ëª¨ë¸ì„ ë§Œë“œëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- ìê¸° ì§€ë„ í•™ìŠµì„ í†µí•œ ì›ê²© ê°ì§€ ê¸°ë°˜ ëª¨ë¸ ê°œë°œì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŒ

- Soft MoE ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•©í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒì„ ì œì•ˆí•¨

- CSMoE ëª¨ë¸ì€ ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì„ ì¤„ì´ê³  í‘œí˜„ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚´

- ê¸°ì¡´ RS FMsë³´ë‹¤ ë›°ì–´ë‚œ í‘œí˜„ ëŠ¥ë ¥, ì •í™•ë„ ë° ê³„ì‚° íš¨ìœ¨ì„±ì„ ë³´ì„

---

*Generated on 2025-09-18 17:03:42*