---
keywords:
  - Large Language Models
  - Iterative Unlearning
  - Self-generated Data
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:12:57.459269",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Iterative Unlearning",
    "Self-generated Data"
  ],
  "rejected_keywords": [
    "Natural Language Processing"
  ],
  "similarity_scores": {
    "Large Language Models": 0.9,
    "Iterative Unlearning": 0.82,
    "Self-generated Data": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Reveal and Release: Iterative LLM Unlearning with Self-generated Data

**Korean Title:** ë“œëŸ¬ë‚´ê³  í•´ì œí•˜ê¸°: ìê¸° ìƒì„± ë°ì´í„°ë¥¼ í†µí•œ ë°˜ë³µì  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ í•™ìŠµ í•´ì œ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]        [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/Iterative Unlearning|Iterative Unlearning]], [[keywords/Self-generated Data|Self-generated Data]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Model]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (84.7% similar)
- [[LNE-Blocking_ An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models_20250918|LNE-Blocking An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models]] (84.6% similar)
- [[Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (83.6% similar)
- [[Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (83.4% similar)
- [[Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (83.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Linxi Xie, Xin Teng, Shichang Ke, Hongyi Wen, Shengjie Wang

## ğŸ“„ Abstract (ì›ë¬¸)

Large language model (LLM) unlearning has demonstrated effectiveness in
removing the influence of undesirable data (also known as forget data).
Existing approaches typically assume full access to the forget dataset,
overlooking two key challenges: (1) Forget data is often privacy-sensitive,
rare, or legally regulated, making it expensive or impractical to obtain (2)
The distribution of available forget data may not align with how that
information is represented within the model. To address these limitations, we
propose a ``Reveal-and-Release'' method to unlearn with self-generated data,
where we prompt the model to reveal what it knows using optimized instructions.
To fully utilize the self-generated forget data, we propose an iterative
unlearning framework, where we make incremental adjustments to the model's
weight space with parameter-efficient modules trained on the forget data.
Experimental results demonstrate that our method balances the tradeoff between
forget quality and utility preservation.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ìŠê¸°(unlearning)ëŠ” ë°”ëŒì§í•˜ì§€ ì•Šì€ ë°ì´í„°(ìŠì–´ì•¼ í•  ë°ì´í„°ë¼ê³ ë„ í•¨)ì˜ ì˜í–¥ì„ ì œê±°í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì€ ì¼ë°˜ì ìœ¼ë¡œ ìŠì–´ì•¼ í•  ë°ì´í„°ì…‹ì— ëŒ€í•œ ì™„ì „í•œ ì ‘ê·¼ì„ ê°€ì •í•˜ì—¬ ë‘ ê°€ì§€ ì£¼ìš” ê³¼ì œë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤: (1) ìŠì–´ì•¼ í•  ë°ì´í„°ëŠ” ì¢…ì¢… ê°œì¸ì •ë³´ ë³´í˜¸ì— ë¯¼ê°í•˜ê±°ë‚˜ ë“œë¬¼ê±°ë‚˜ ë²•ì ìœ¼ë¡œ ê·œì œë˜ì–´ ìˆì–´ ì´ë¥¼ ì–»ëŠ” ê²ƒì´ ë¹„ìš©ì´ ë§ì´ ë“¤ê±°ë‚˜ ë¹„í˜„ì‹¤ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (2) ì‚¬ìš© ê°€ëŠ¥í•œ ìŠì–´ì•¼ í•  ë°ì´í„°ì˜ ë¶„í¬ê°€ ëª¨ë¸ ë‚´ì—ì„œ í•´ë‹¹ ì •ë³´ê°€ í‘œí˜„ë˜ëŠ” ë°©ì‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ìµœì í™”ëœ ì§€ì‹œë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” ê²ƒì„ ë“œëŸ¬ë‚´ë„ë¡ ìœ ë„í•˜ëŠ” "ë“œëŸ¬ë‚´ê³ -ë°°í¬í•˜ê¸°(Reveal-and-Release)" ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìê°€ ìƒì„±ëœ ìŠì–´ì•¼ í•  ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìŠì–´ì•¼ í•  ë°ì´í„°ë¡œ í›ˆë ¨ëœ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ê³µê°„ì— ì ì§„ì ì¸ ì¡°ì •ì„ ê°€í•˜ëŠ” ë°˜ë³µì  ìŠê¸° í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ìŠê¸°ì˜ ì§ˆê³¼ ìœ ìš©ì„± ë³´ì¡´ ê°„ì˜ ê· í˜•ì„ ì˜ ë§ì¶˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ë¶ˆí•„ìš”í•œ ë°ì´í„°ì˜ ì˜í–¥ì„ ì œê±°í•˜ëŠ” 'ìŠê¸°' ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ìŠê¸° ë°ì´í„°ì— ëŒ€í•œ ì™„ì „í•œ ì ‘ê·¼ì„ ê°€ì •í•˜ì§€ë§Œ, ì´ëŠ” í”„ë¼ì´ë²„ì‹œ ë¬¸ì œë‚˜ ë²•ì  ê·œì œë¡œ ì¸í•´ ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ìƒì„±í•œ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” 'ê³µê°œ ë° í•´ì œ' ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìµœì í™”ëœ ì§€ì‹œë¥¼ í†µí•´ ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” ê²ƒì„ ë“œëŸ¬ë‚´ë„ë¡ í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ë°˜ë³µì  í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì€ ìŠê¸° í’ˆì§ˆê³¼ ìœ ìš©ì„± ë³´ì¡´ ê°„ì˜ ê· í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ë¶ˆí•„ìš”í•œ ë°ì´í„°ì˜ ì˜í–¥ì„ ì œê±°í•˜ëŠ” 'ìŠê¸°' ë°©ë²•ì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.

- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ìŠê¸° ë°ì´í„°ì— ëŒ€í•œ ì™„ì „í•œ ì ‘ê·¼ì„ ê°€ì •í•˜ì§€ë§Œ, ì´ëŠ” í”„ë¼ì´ë²„ì‹œ ë¬¸ì œì™€ ë²•ì  ê·œì œë¡œ ì¸í•´ ì‹¤ì§ˆì ìœ¼ë¡œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 3. 'Reveal-and-Release' ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ìƒì„±í•œ ë°ì´í„°ë¥¼ í†µí•´ ìŠê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- 4. ì œì•ˆëœ ë°©ë²•ì€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ê³µê°„ì„ ì ì§„ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ìŠê¸° ë°ì´í„°ì— ëŒ€í•´ íš¨ìœ¨ì ì¸ í•™ìŠµ ëª¨ë“ˆì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ìŠê¸° í’ˆì§ˆê³¼ ìœ ìš©ì„± ë³´ì¡´ ê°„ì˜ ê· í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:48:32*