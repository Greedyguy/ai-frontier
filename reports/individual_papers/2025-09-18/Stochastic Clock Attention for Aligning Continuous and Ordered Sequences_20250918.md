---
keywords:
  - Transformer Architecture
  - Attention Mechanism
  - Stochastic Clock Attention
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:09:02.152895",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer Architecture",
    "Attention Mechanism",
    "Stochastic Clock Attention"
  ],
  "rejected_keywords": [
    "Sequence-to-Sequence Tasks"
  ],
  "similarity_scores": {
    "Transformer Architecture": 0.78,
    "Attention Mechanism": 0.8,
    "Stochastic Clock Attention": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Stochastic Clock Attention for Aligning Continuous and Ordered Sequences

**Korean Title:** í™•ë¥ ì  ì‹œê³„ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ ì—°ì†ì ì´ê³  ìˆœì°¨ì ì¸ ì‹œí€€ìŠ¤ ì •ë ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Transformer Architecture|Transformer text-to-speech]], [[keywords/Attention Mechanism|attention mechanism]]
**âš¡ Unique Technical**: [[keywords/Stochastic Clock Attention|stochastic clock attention]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention A Scalable Multilevel Attention Mechanism for Text and Images]] (79.3% similar)
- [[Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models_20250919|Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models]] (76.9% similar)
- [[Hybrid Autoregressive-Diffusion Model for Real-Time Sign Language Production_20250919|Hybrid Autoregressive-Diffusion Model for Real-Time Sign Language Production]] (76.9% similar)
- [[Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery_20250918|Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery]] (76.6% similar)
- [[Beyond Marginals_ Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection_20250918|Beyond Marginals Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection]] (75.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Hyungjoon Soh, Junghyo Jo

## ğŸ“„ Abstract (ì›ë¬¸)

We formulate an attention mechanism for continuous and ordered sequences that
explicitly functions as an alignment model, which serves as the core of many
sequence-to-sequence tasks. Standard scaled dot-product attention relies on
positional encodings and masks but does not enforce continuity or monotonicity,
which are crucial for frame-synchronous targets. We propose learned nonnegative
\emph{clocks} to source and target and model attention as the meeting
probability of these clocks; a path-integral derivation yields a closed-form,
Gaussian-like scoring rule with an intrinsic bias toward causal, smooth,
near-diagonal alignments, without external positional regularizers. The
framework supports two complementary regimes: normalized clocks for parallel
decoding when a global length is available, and unnormalized clocks for
autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In
a Transformer text-to-speech testbed, this construction produces more stable
alignments and improved robustness to global time-scaling while matching or
improving accuracy over scaled dot-product baselines. We hypothesize
applicability to other continuous targets, including video and temporal signal
modeling.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì—°ì†ì ì´ê³  ìˆœì„œê°€ ìˆëŠ” ì‹œí€€ìŠ¤ë¥¼ ìœ„í•œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì •ì‹í™”í•˜ì—¬, ì´ëŠ” ë§ì€ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ì‘ì—…ì˜ í•µì‹¬ ì—­í• ì„ í•˜ëŠ” ì •ë ¬ ëª¨ë¸ë¡œ ëª…ì‹œì ìœ¼ë¡œ ê¸°ëŠ¥í•©ë‹ˆë‹¤. í‘œì¤€ ìŠ¤ì¼€ì¼ë“œ ë‹·-í”„ë¡œë•íŠ¸ ì–´í…ì…˜ì€ ìœ„ì¹˜ ì¸ì½”ë”©ê³¼ ë§ˆìŠ¤í¬ì— ì˜ì¡´í•˜ì§€ë§Œ, í”„ë ˆì„ ë™ê¸°í™” ëŒ€ìƒì— ì¤‘ìš”í•œ ì—°ì†ì„±ì´ë‚˜ ë‹¨ì¡°ì„±ì„ ê°•ì œí•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì— ëŒ€í•´ í•™ìŠµëœ ë¹„ìŒìˆ˜ \emph{ì‹œê³„}ë¥¼ ì œì•ˆí•˜ê³ , ì´ ì‹œê³„ë“¤ì˜ ë§Œë‚¨ í™•ë¥ ë¡œ ì–´í…ì…˜ì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ê²½ë¡œ ì ë¶„ ìœ ë„ëŠ” ì™¸ë¶€ ìœ„ì¹˜ ì •ê·œí™” ì—†ì´ ì¸ê³¼ì ì´ê³  ë¶€ë“œëŸ¬ìš´, ê±°ì˜ ëŒ€ê°ì„ ì— ê°€ê¹Œìš´ ì •ë ¬ì— ë‚´ì¬ëœ í¸í–¥ì„ ê°€ì§„ íì‡„í˜•ì˜ ê°€ìš°ì‹œì•ˆ ìœ ì‚¬ ì ìˆ˜ ê·œì¹™ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‘ ê°€ì§€ ìƒí˜¸ ë³´ì™„ì ì¸ ì²´ì œë¥¼ ì§€ì›í•©ë‹ˆë‹¤: ì „ì—­ ê¸¸ì´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë³‘ë ¬ ë””ì½”ë”©ì„ ìœ„í•œ ì •ê·œí™”ëœ ì‹œê³„ì™€, ìíšŒê·€ ë””ì½”ë”©ì„ ìœ„í•œ ë¹„ì •ê·œí™”ëœ ì‹œê³„ë¡œ, ë‘˜ ë‹¤ ê±°ì˜ ë§¤ê°œë³€ìˆ˜ê°€ ì—†ëŠ”, ëŒ€ì²´ ê°€ëŠ¥í•œ í˜•íƒœì…ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜ ì‹œí—˜ í™˜ê²½ì—ì„œ, ì´ êµ¬ì¡°ëŠ” ë” ì•ˆì •ì ì¸ ì •ë ¬ì„ ìƒì„±í•˜ê³ , ìŠ¤ì¼€ì¼ë“œ ë‹·-í”„ë¡œë•íŠ¸ ê¸°ì¤€ì„ ë³´ë‹¤ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ë©´ì„œ ì „ì—­ ì‹œê°„ ìŠ¤ì¼€ì¼ë§ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¹„ë””ì˜¤ ë° ì‹œê°„ ì‹ í˜¸ ëª¨ë¸ë§ì„ í¬í•¨í•œ ë‹¤ë¥¸ ì—°ì†ì  ëŒ€ìƒì— ëŒ€í•œ ì ìš© ê°€ëŠ¥ì„±ì„ ê°€ì •í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì—°ì†ì ì´ê³  ìˆœì„œê°€ ìˆëŠ” ì‹œí€€ìŠ¤ë¥¼ ìœ„í•œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ì‘ì—…ì˜ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ìœ„ì¹˜ ì¸ì½”ë”©ê³¼ ë§ˆìŠ¤í¬ì— ì˜ì¡´í•˜ì§€ë§Œ ì—°ì†ì„±ì´ë‚˜ ë‹¨ì¡°ì„±ì„ ë³´ì¥í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì— í•™ìŠµëœ ë¹„ìŒìˆ˜ 'í´ë¡'ì„ ë„ì…í•˜ê³ , ì´ í´ë¡ì˜ ë§Œë‚¨ í™•ë¥ ë¡œ ì£¼ì˜ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ì™¸ë¶€ ìœ„ì¹˜ ì •ê·œí™” ì—†ì´ ì¸ê³¼ì ì´ê³  ë¶€ë“œëŸ¬ìš´ ì •ë ¬ì„ ìœ ë„í•˜ëŠ” ê°€ìš°ì‹œì•ˆ ìœ ì‚¬ ì ìˆ˜ ê·œì¹™ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë³‘ë ¬ ë””ì½”ë”©ì„ ìœ„í•œ ì •ê·œí™” í´ë¡ê³¼ ìê°€íšŒê·€ ë””ì½”ë”©ì„ ìœ„í•œ ë¹„ì •ê·œí™” í´ë¡ì„ ì§€ì›í•˜ë©°, ê±°ì˜ ë§¤ê°œë³€ìˆ˜ê°€ í•„ìš” ì—†ëŠ” ëŒ€ì²´ë¬¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜ í…ŒìŠ¤íŠ¸ì—ì„œ ì´ ë°©ë²•ì€ ì•ˆì •ì ì¸ ì •ë ¬ê³¼ ì‹œê°„ í™•ì¥ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ì œê³µí•˜ë©´ì„œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¹„ë””ì˜¤ ë° ì‹œê°„ ì‹ í˜¸ ëª¨ë¸ë§ê³¼ ê°™ì€ ë‹¤ë¥¸ ì—°ì† íƒ€ê²Ÿì—ë„ ì ìš© ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì—°ì†ì ì´ê³  ìˆœì„œê°€ ìˆëŠ” ì‹œí€€ìŠ¤ë¥¼ ìœ„í•œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì •ì‹í™”í•˜ì—¬ ì •ë ¬ ëª¨ë¸ë¡œ ê¸°ëŠ¥í•˜ë„ë¡ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì£¼ì˜ëŠ” ìœ„ì¹˜ ì¸ì½”ë”©ê³¼ ë§ˆìŠ¤í¬ì— ì˜ì¡´í•˜ì§€ë§Œ, ì—°ì†ì„±ê³¼ ë‹¨ì¡°ì„±ì„ ë³´ì¥í•˜ì§€ ëª»í•©ë‹ˆë‹¤.

- 3. ì œì•ˆëœ ëª¨ë¸ì€ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì— í•™ìŠµëœ ë¹„ìŒìˆ˜ 'ì‹œê³„'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì˜ë¥¼ ì‹œê³„ì˜ ë§Œë‚¨ í™•ë¥ ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.

- 4. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ë³‘ë ¬ ë””ì½”ë”©ì„ ìœ„í•œ ì •ê·œí™”ëœ ì‹œê³„ì™€ ìê°€íšŒê·€ ë””ì½”ë”©ì„ ìœ„í•œ ë¹„ì •ê·œí™”ëœ ì‹œê³„ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

- 5. í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ëª¨ë¸ì€ ë” ì•ˆì •ì ì¸ ì •ë ¬ê³¼ í–¥ìƒëœ ì „ì—­ ì‹œê°„ ìŠ¤ì¼€ì¼ë§ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:45:36*