
# xGen-MM (BLIP-3): A Family of Open Large Multimodal Models

**Korean Title:** xGen-MM (BLIP-3): 오픈 대규모 멀티모달 모델군

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Open-source LMMs

## 🔗 유사한 논문
- [[LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (83.2% similar)
- [[Apertus Democratizing Open and Compliant LLMs for Global Language Environments]] (82.0% similar)
- [[A Culturally-diverse Multilingual Multimodal Video Benchmark & Model]] (80.4% similar)
- [[Is GPT-4o mini Blinded by its Own Safety Filters_ Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection_20250918|Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection]] (80.3% similar)
- [[Humor in Pixels Benchmarking Large Multimodal Models Understanding of Online Comics]] (80.1% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2408.08872v4 Announce Type: replace-cross 
Abstract: This paper introduces BLIP-3, an open framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. We release 4B and 14B models, including both the pre-trained base model and the instruction fine-tuned ones. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our models demonstrate competitive performance among open-source LMMs with similar model sizes. Our resulting LMMs demonstrate competitive performance among open-source LMMs with similar model sizes, with the ability to comprehend interleaved image-text inputs. Our training code, models, and all datasets used in this work, including the three largescale datasets we create and the preprocessed ones, will be open-sourced to better support the research community.

## 🔍 Abstract (한글 번역)

arXiv:2408.08872v4 발표 유형: replace-cross
초록: 본 논문은 대규모 멀티모달 모델(Large Multimodal Models, LMMs) 개발을 위한 오픈 프레임워크인 BLIP-3를 소개한다. 이 프레임워크는 세심하게 큐레이션된 데이터셋, 훈련 레시피, 모델 아키텍처, 그리고 이로부터 도출된 LMMs 제품군으로 구성된다. 우리는 사전 훈련된 기본 모델과 명령어 미세조정된 모델을 포함하여 4B 및 14B 모델을 공개한다. 우리의 모델들은 단일 및 다중 이미지 벤치마크를 포함한 다양한 태스크에서 엄격한 평가를 거쳤다. 우리의 모델들은 유사한 모델 크기를 가진 오픈소스 LMMs 중에서 경쟁력 있는 성능을 보여준다. 결과적으로 도출된 LMMs는 유사한 모델 크기의 오픈소스 LMMs 중에서 경쟁력 있는 성능을 보여주며, 교차 배치된 이미지-텍스트 입력을 이해할 수 있는 능력을 갖추고 있다. 본 연구에서 사용된 훈련 코드, 모델, 그리고 우리가 생성한 3개의 대규모 데이터셋과 전처리된 데이터셋을 포함한 모든 데이터셋은 연구 커뮤니티를 더 잘 지원하기 위해 오픈소스로 공개될 예정이다.

## 📝 요약

이 논문은 대규모 멀티모달 모델(LMMs)을 개발하기 위한 개방형 프레임워크인 BLIP-3를 소개합니다. 이 프레임워크는 신중하게 선별된 데이터셋, 훈련 레시피, 모델 아키텍처로 구성되어 있으며, 4B 및 14B 모델을 포함합니다. 모델은 다양한 단일 및 다중 이미지 벤치마크에서 평가되었으며, 유사한 크기의 오픈소스 LMMs 중 경쟁력 있는 성능을 보였습니다. 특히 이미지와 텍스트가 혼합된 입력을 이해하는 능력을 갖추고 있습니다. 모든 훈련 코드, 모델 및 데이터셋은 연구 커뮤니티 지원을 위해 공개될 예정입니다.

## 🎯 주요 포인트

- 1. BLIP-3는 대형 멀티모달 모델(LMMs) 개발을 위한 개방형 프레임워크를 소개합니다.

- 2. 4B 및 14B 모델을 포함하여 사전 학습된 기본 모델과 지시 기반으로 미세 조정된 모델을 공개합니다.

- 3. 다양한 단일 및 다중 이미지 벤치마크에서 모델의 성능을 철저히 평가합니다.

- 4. 유사한 모델 크기의 오픈 소스 LMMs 중에서 경쟁력 있는 성능을 보여줍니다.

- 5. 연구 커뮤니티 지원을 위해 모든 훈련 코드, 모델 및 사용된 데이터셋을 오픈 소스로 제공합니다.

---

*Generated on 2025-09-19 10:57:53*