
# AppAgent v2: Advanced Agent for Flexible Mobile Interactions

**Korean Title:** AppAgent v2: 유연한 모바일 상호작용을 위한 고급 에이전트

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: LLM-based multimodal agent framework

## 🔗 유사한 논문
- [[$Agent^2$ An Agent-Generates-Agent Framework for Reinforcement Learning Automation]] (85.7% similar)
- [[LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (83.8% similar)
- [[VeriOS Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents]] (83.8% similar)
- [[DuetUI A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces]] (83.3% similar)
- [[Semantic_Alignment-Enhanced_Code_Translation_via_an_LLM-Based_Multi-Agent_System_20250918|Semantic Alignment-Enhanced Code Translation via an LLM-Based Multi-Agent System]] (83.0% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2408.11824v4 Announce Type: replace-cross 
Abstract: With the advancement of Multimodal Large Language Models (MLLM), LLM-driven visual agents are increasingly impacting software interfaces, particularly those with graphical user interfaces. This work introduces a novel LLM-based multimodal agent framework for mobile devices. This framework, capable of navigating mobile devices, emulates human-like interactions. Our agent constructs a flexible action space that enhances adaptability across various applications including parser, text and vision descriptions. The agent operates through two main phases: exploration and deployment. During the exploration phase, functionalities of user interface elements are documented either through agent-driven or manual explorations into a customized structured knowledge base. In the deployment phase, RAG technology enables efficient retrieval and update from this knowledge base, thereby empowering the agent to perform tasks effectively and accurately. This includes performing complex, multi-step operations across various applications, thereby demonstrating the framework's adaptability and precision in handling customized task workflows. Our experimental results across various benchmarks demonstrate the framework's superior performance, confirming its effectiveness in real-world scenarios. Our code will be open source soon.

## 🔍 Abstract (한글 번역)

arXiv:2408.11824v4 발표 유형: replace-cross 
초록: 멀티모달 대형 언어 모델(MLLM)의 발전과 함께, LLM 기반 시각 에이전트는 소프트웨어 인터페이스, 특히 그래픽 사용자 인터페이스를 가진 인터페이스에 점점 더 큰 영향을 미치고 있다. 본 연구는 모바일 기기를 위한 새로운 LLM 기반 멀티모달 에이전트 프레임워크를 소개한다. 모바일 기기 탐색이 가능한 이 프레임워크는 인간과 유사한 상호작용을 모방한다. 우리의 에이전트는 파서, 텍스트 및 비전 설명을 포함한 다양한 애플리케이션 전반에서 적응성을 향상시키는 유연한 액션 공간을 구축한다. 에이전트는 탐색과 배포라는 두 가지 주요 단계를 통해 작동한다. 탐색 단계에서는 사용자 인터페이스 요소의 기능이 에이전트 주도 또는 수동 탐색을 통해 맞춤형 구조화된 지식 베이스에 문서화된다. 배포 단계에서는 RAG 기술이 이 지식 베이스로부터 효율적인 검색과 업데이트를 가능하게 하여, 에이전트가 효과적이고 정확하게 작업을 수행할 수 있도록 한다. 이는 다양한 애플리케이션에서 복잡한 다단계 작업을 수행하는 것을 포함하며, 이를 통해 맞춤형 작업 워크플로우 처리에서 프레임워크의 적응성과 정확성을 입증한다. 다양한 벤치마크에 대한 우리의 실험 결과는 프레임워크의 우수한 성능을 보여주며, 실제 시나리오에서의 효과성을 확인해준다. 우리의 코드는 곧 오픈 소스로 공개될 예정이다.

## 📝 요약

이 논문은 모바일 기기용 LLM 기반 다중 모드 에이전트 프레임워크를 소개합니다. 이 에이전트는 인간과 유사한 상호작용을 모방하며, 다양한 애플리케이션에서 적응성을 높이기 위해 유연한 행동 공간을 구축합니다. 에이전트는 탐색과 배포의 두 가지 주요 단계를 통해 작동합니다. 탐색 단계에서는 사용자 인터페이스 요소의 기능을 구조화된 지식 베이스에 기록하고, 배포 단계에서는 RAG 기술을 통해 이 지식 베이스에서 효율적으로 정보를 검색 및 업데이트하여 복잡한 작업을 정확하게 수행합니다. 다양한 벤치마크 실험 결과, 이 프레임워크가 실제 환경에서 우수한 성능을 발휘함을 확인했습니다. 코드도 곧 오픈 소스로 제공될 예정입니다.

## 🎯 주요 포인트

- 1. 본 연구는 모바일 기기를 위한 새로운 LLM 기반 다중 모드 에이전트 프레임워크를 소개합니다.

- 2. 에이전트는 탐색과 배포의 두 가지 주요 단계를 통해 작동하며, 사용자 인터페이스 요소의 기능을 문서화합니다.

- 3. RAG 기술을 활용하여 지식 기반에서 효율적으로 정보를 검색하고 업데이트하여 에이전트의 작업 수행 능력을 향상시킵니다.

- 4. 다양한 벤치마크 실험 결과, 프레임워크의 우수한 성능이 입증되었으며, 실제 시나리오에서의 효과성을 확인했습니다.

- 5. 코드가 곧 오픈 소스로 공개될 예정입니다.

---

*Generated on 2025-09-19 10:58:19*