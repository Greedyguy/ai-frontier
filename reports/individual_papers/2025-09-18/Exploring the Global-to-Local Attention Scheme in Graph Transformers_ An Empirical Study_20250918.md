---
keywords:
  - Graph Neural Networks
  - Attention Mechanism
  - Transformer Architecture
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:29:10.153494",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Neural Networks",
    "Attention Mechanism",
    "Transformer Architecture"
  ],
  "rejected_keywords": [
    "Global-to-Local Attention Scheme"
  ],
  "similarity_scores": {
    "Graph Neural Networks": 0.85,
    "Attention Mechanism": 0.82,
    "Transformer Architecture": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study

**Korean Title:** ê·¸ë˜í”„ ë³€í™˜ê¸°ì—ì„œì˜ ê¸€ë¡œë²Œ-ë¡œì»¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ íƒêµ¬: ì‹¤ì¦ì  ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Networks|Graph Neural Networks]], [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Transformer Architecture|Graph Transformers]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods Reviving Transformer for Graph Clustering]] (86.9% similar)
- [[GraphTorque_ Torque-Driven Rewiring Graph Neural Network_20250918|GraphTorque Torque-Driven Rewiring Graph Neural Network]] (80.3% similar)
- [[Brain-HGCN_ A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis_20250919|Brain-HGCN A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis]] (79.3% similar)
- [[Soft Graph Transformer for MIMO Detection_20250918|Soft Graph Transformer for MIMO Detection]] (79.1% similar)
- [[Federated Hypergraph Learning with Local Differential Privacy_ Toward Privacy-Aware Hypergraph Structure Completion_20250919|Federated Hypergraph Learning with Local Differential Privacy Toward Privacy-Aware Hypergraph Structure Completion]] (77.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Zhengwei Wang, Gang Wu

## ğŸ“„ Abstract (ì›ë¬¸)

Graph Transformers (GTs) show considerable potential in graph representation
learning. The architecture of GTs typically integrates Graph Neural Networks
(GNNs) with global attention mechanisms either in parallel or as a precursor to
attention mechanisms, yielding a local-and-global or local-to-global attention
scheme. However, as the global attention mechanism primarily captures
long-range dependencies between nodes, these integration schemes may suffer
from information loss, where the local neighborhood information learned by GNN
could be diluted by the attention mechanism. Therefore, we propose G2LFormer,
featuring a novel global-to-local attention scheme where the shallow network
layers use attention mechanisms to capture global information, while the deeper
layers employ GNN modules to learn local structural information, thereby
preventing nodes from ignoring their immediate neighbors. An effective
cross-layer information fusion strategy is introduced to allow local layers to
retain beneficial information from global layers and alleviate information
loss, with acceptable trade-offs in scalability. To validate the feasibility of
the global-to-local attention scheme, we compare G2LFormer with
state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The
results indicate that G2LFormer exhibits excellent performance while keeping
linear complexity.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ê·¸ë˜í”„ ë³€í™˜ê¸°(Graph Transformers, GTs)ëŠ” ê·¸ë˜í”„ í‘œí˜„ í•™ìŠµì—ì„œ ìƒë‹¹í•œ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. GTì˜ ì•„í‚¤í…ì²˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê·¸ë˜í”„ ì‹ ê²½ë§(Graph Neural Networks, GNNs)ê³¼ ì „ì—­ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë³‘ë ¬ë¡œ ë˜ëŠ” ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì „ ë‹¨ê³„ë¡œ í†µí•©í•˜ì—¬ ë¡œì»¬ ë° ê¸€ë¡œë²Œ ë˜ëŠ” ë¡œì»¬ì—ì„œ ê¸€ë¡œë²Œë¡œì˜ ì£¼ì˜ ì²´ê³„ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì „ì—­ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì£¼ë¡œ ë…¸ë“œ ê°„ì˜ ì¥ê±°ë¦¬ ì¢…ì†ì„±ì„ í¬ì°©í•˜ê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ í†µí•© ì²´ê³„ëŠ” GNNì´ í•™ìŠµí•œ ë¡œì»¬ ì´ì›ƒ ì •ë³´ê°€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— ì˜í•´ í¬ì„ë  ìˆ˜ ìˆëŠ” ì •ë³´ ì†ì‹¤ì„ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” G2LFormerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì–•ì€ ë„¤íŠ¸ì›Œí¬ ê³„ì¸µì´ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ë¡œë²Œ ì •ë³´ë¥¼ í¬ì°©í•˜ê³ , ë” ê¹Šì€ ê³„ì¸µì€ GNN ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ êµ¬ì¡° ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ìƒˆë¡œìš´ ê¸€ë¡œë²Œì—ì„œ ë¡œì»¬ë¡œì˜ ì£¼ì˜ ì²´ê³„ë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•˜ì—¬ ë…¸ë“œê°€ ì¦‰ê°ì ì¸ ì´ì›ƒì„ ë¬´ì‹œí•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤. íš¨ê³¼ì ì¸ ê³„ì¸µ ê°„ ì •ë³´ ìœµí•© ì „ëµì´ ë„ì…ë˜ì–´ ë¡œì»¬ ê³„ì¸µì´ ê¸€ë¡œë²Œ ê³„ì¸µì—ì„œ ìœ ìµí•œ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  ì •ë³´ ì†ì‹¤ì„ ì™„í™”í•˜ë©°, í™•ì¥ì„±ì—ì„œ ìˆ˜ìš© ê°€ëŠ¥í•œ ì ˆì¶©ì„ ì œê³µí•©ë‹ˆë‹¤. ê¸€ë¡œë²Œì—ì„œ ë¡œì»¬ë¡œì˜ ì£¼ì˜ ì²´ê³„ì˜ íƒ€ë‹¹ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” G2LFormerë¥¼ ìµœì²¨ë‹¨ ì„ í˜• GT ë° GNNê³¼ ë…¸ë“œ ìˆ˜ì¤€ ë° ê·¸ë˜í”„ ìˆ˜ì¤€ ì‘ì—…ì—ì„œ ë¹„êµí•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” G2LFormerê°€ ì„ í˜• ë³µì¡ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Graph Transformers(GTs)ëŠ” ê·¸ë˜í”„ í‘œí˜„ í•™ìŠµì—ì„œ ì ì¬ë ¥ì„ ë³´ì´ì§€ë§Œ, ê¸°ì¡´ì˜ GT êµ¬ì¡°ëŠ” ì •ë³´ ì†ì‹¤ ë¬¸ì œë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ê¸€ë¡œë²Œ-íˆ¬-ë¡œì»¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê°–ì¶˜ G2LFormerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì–•ì€ ë„¤íŠ¸ì›Œí¬ ì¸µì—ì„œëŠ” ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•´ ê¸€ë¡œë²Œ ì •ë³´ë¥¼ í¬ì°©í•˜ê³ , ê¹Šì€ ì¸µì—ì„œëŠ” GNN ëª¨ë“ˆì„ ì‚¬ìš©í•´ ì§€ì—­ êµ¬ì¡° ì •ë³´ë¥¼ í•™ìŠµí•˜ì—¬ ë…¸ë“œê°€ ì¸ì ‘ ì´ì›ƒì„ ë¬´ì‹œí•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, íš¨ê³¼ì ì¸ ì¸µ ê°„ ì •ë³´ ìœµí•© ì „ëµì„ ë„ì…í•˜ì—¬ ì§€ì—­ ì¸µì´ ê¸€ë¡œë²Œ ì¸µì˜ ìœ ìµí•œ ì •ë³´ë¥¼ ìœ ì§€í•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤. G2LFormerëŠ” ë…¸ë“œ ë° ê·¸ë˜í”„ ìˆ˜ì¤€ì˜ ì‘ì—…ì—ì„œ ê¸°ì¡´ì˜ GTs ë° GNNsì™€ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì„ í˜• ë³µì¡ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Graph TransformersëŠ” ê·¸ë˜í”„ í‘œí˜„ í•™ìŠµì—ì„œ í° ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ë©°, ì£¼ë¡œ GNNê³¼ ê¸€ë¡œë²Œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•©í•˜ì—¬ ì§€ì—­ ë° ê¸€ë¡œë²Œ ì£¼ì˜ ì²´ê³„ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ í†µí•© ë°©ì‹ì€ ê¸€ë¡œë²Œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ë…¸ë“œ ê°„ ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì„ ì£¼ë¡œ í¬ì°©í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 3. G2LFormerëŠ” ì–•ì€ ë„¤íŠ¸ì›Œí¬ ì¸µì—ì„œ ê¸€ë¡œë²Œ ì •ë³´ë¥¼ í¬ì°©í•˜ê³ , ê¹Šì€ ì¸µì—ì„œ GNN ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì§€ì—­ êµ¬ì¡° ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ê¸€ë¡œë²Œ-íˆ¬-ë¡œì»¬ ì£¼ì˜ ì²´ê³„ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 4. íš¨ê³¼ì ì¸ ì¸µ ê°„ ì •ë³´ ìœµí•© ì „ëµì„ í†µí•´ ì§€ì—­ ì¸µì´ ê¸€ë¡œë²Œ ì¸µì˜ ìœ ìµí•œ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  ì •ë³´ ì†ì‹¤ì„ ì™„í™”í•©ë‹ˆë‹¤.

- 5. G2LFormerëŠ” ìµœì²¨ë‹¨ ì„ í˜• GT ë° GNNê³¼ ë¹„êµí•˜ì—¬ ë…¸ë“œ ë° ê·¸ë˜í”„ ìˆ˜ì¤€ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì„ í˜• ë³µì¡ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 02:46:26*