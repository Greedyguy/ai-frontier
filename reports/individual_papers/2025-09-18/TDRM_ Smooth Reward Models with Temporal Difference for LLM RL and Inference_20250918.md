# TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference

**Korean Title:** TDRM: LLM ê°•í™” í•™ìŠµ ë° ì¶”ë¡ ì„ ìœ„í•œ ì‹œê°„ ì°¨ì´ ê¸°ë°˜ì˜ ë¶€ë“œëŸ¬ìš´ ë³´ìƒ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Dan Zhang|Dan Zhang]] [[authors/Min Cai|Min Cai]] [[authors/Jonathan Li|Jonathan Li]] [[authors/Ziniu Hu|Ziniu Hu]] [[authors/Yisong Yue|Yisong Yue]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Temporal Difference Regularization

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (85.2% similar)
- [[Evolving Language Models without Labels_ Majority Drives Selection, Novelty Promotes Variation_20250918|Evolving Language Models without Labels Majority Drives Selection, Novelty Promotes Variation]] (84.8% similar)
- [[Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents_20250919|Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents]] (83.6% similar)
- [[FlowRL_ Matching Reward Distributions for LLM Reasoning_20250919|FlowRL Matching Reward Distributions for LLM Reasoning]] (83.6% similar)
- [[TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250917|TGPO Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (82.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Dan Zhang, Min Cai, Jonathan Li, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang

## ğŸ“„ Abstract (ì›ë¬¸)

Reward models are central to both reinforcement learning (RL) with language
models and inference-time verification. However, existing reward models often
lack temporal consistency, leading to ineffective policy updates and unstable
RL training. We introduce TDRM, a method for learning smoother and more
reliable reward models by minimizing temporal differences during training. This
temporal-difference (TD) regularization produces smooth rewards and improves
alignment with long-term objectives. Incorporating TDRM into the actor-critic
style online RL loop yields consistent empirical gains. It is worth noting that
TDRM is a supplement to verifiable reward methods, and both can be used in
series. Experiments show that TD-trained process reward models (PRMs) improve
performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)
settings. When combined with Reinforcement Learning with Verifiable Rewards
(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable
performance with just 2.5k data to what baseline methods require 50.1k data to
attain -- and yield higher-quality language model policies on 8 model variants
(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,
Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release
all code at https://github.com/THUDM/TDRM.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë³´ìƒ ëª¨ë¸ì€ ì–¸ì–´ ëª¨ë¸ê³¼ ì¶”ë¡  ì‹œ ê²€ì¦ì„ í¬í•¨í•œ ê°•í™” í•™ìŠµ(RL)ì˜ ì¤‘ì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ë³´ìƒ ëª¨ë¸ì€ ì¢…ì¢… ì‹œê°„ì  ì¼ê´€ì„±ì´ ë¶€ì¡±í•˜ì—¬ ë¹„íš¨ìœ¨ì ì¸ ì •ì±… ì—…ë°ì´íŠ¸ì™€ ë¶ˆì•ˆì •í•œ RL í›ˆë ¨ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í›ˆë ¨ ì¤‘ ì‹œê°„ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ì—¬ ë” ë¶€ë“œëŸ½ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë³´ìƒ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì¸ TDRMì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì‹œê°„ ì°¨ì´(TD) ì •ê·œí™”ëŠ” ë¶€ë“œëŸ¬ìš´ ë³´ìƒì„ ìƒì„±í•˜ê³  ì¥ê¸° ëª©í‘œì™€ì˜ ì •ë ¬ì„ ê°œì„ í•©ë‹ˆë‹¤. TDRMì„ ì•¡í„°-í¬ë¦¬í‹± ìŠ¤íƒ€ì¼ì˜ ì˜¨ë¼ì¸ RL ë£¨í”„ì— í†µí•©í•˜ë©´ ì¼ê´€ëœ ê²½í—˜ì  ì´ë“ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. TDRMì€ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ë°©ë²•ì˜ ë³´ì¡° ìˆ˜ë‹¨ì´ë©°, ë‘ ë°©ë²• ëª¨ë‘ ì—°ì†ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ì£¼ëª©í•  ê°€ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TDë¡œ í›ˆë ¨ëœ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM)ì€ Best-of-N(ìµœëŒ€ 6.6%) ë° íŠ¸ë¦¬ íƒìƒ‰(ìµœëŒ€ 23.7%) ì„¤ì •ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒê³¼ í•¨ê»˜í•˜ëŠ” ê°•í™” í•™ìŠµ(RLVR)ê³¼ ê²°í•©í•  ê²½ìš°, TDë¡œ í›ˆë ¨ëœ PRMì€ ë°ì´í„° íš¨ìœ¨ì ì¸ RLì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬, ê¸°ì¤€ ë°©ë²•ì´ 50.1k ë°ì´í„°ê°€ í•„ìš”ë¡œ í•˜ëŠ” ì„±ëŠ¥ì„ ë‹¨ì§€ 2.5k ë°ì´í„°ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, 8ê°œì˜ ëª¨ë¸ ë³€í˜•(5ê°œì˜ ì‹œë¦¬ì¦ˆ)ì—ì„œ ë” ë†’ì€ í’ˆì§ˆì˜ ì–¸ì–´ ëª¨ë¸ ì •ì±…ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Qwen2.5-(0.5B, 1.5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), DeepSeek-R1-Distill-Qwen-(1.5B, 7B) ë“±ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì½”ë“œëŠ” https://github.com/THUDM/TDRMì—ì„œ ê³µê°œë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL)ì—ì„œ ë³´ìƒ ëª¨ë¸ì˜ ì‹œê°„ì  ì¼ê´€ì„± ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ TDRMì´ë¼ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. TDRMì€ í•™ìŠµ ì¤‘ ì‹œê°„ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ì—¬ ë” ë¶€ë“œëŸ½ê³  ì‹ ë¢°ì„± ìˆëŠ” ë³´ìƒ ëª¨ë¸ì„ í•™ìŠµí•˜ë©°, ì¥ê¸° ëª©í‘œì™€ì˜ ì •ë ¬ì„ ê°œì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì˜¨ë¼ì¸ RL ë£¨í”„ì— ì ìš©í•˜ë©´ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TDRMì„ í†µí•´ í•™ìŠµëœ ë³´ìƒ ëª¨ë¸ì€ Best-of-N ë° íŠ¸ë¦¬ ê²€ìƒ‰ ì„¤ì •ì—ì„œ ê°ê° ìµœëŒ€ 6.6%ì™€ 23.7%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, TDRMì€ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ë°©ë²•ê³¼ ê²°í•©í•˜ì—¬ ë°ì´í„° íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë” ë†’ì€ í’ˆì§ˆì˜ ì •ì±…ì„ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë“  ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TDRMì€ ê°•í™” í•™ìŠµì—ì„œ ì‹œê°„ì  ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ì—¬ ë” ë¶€ë“œëŸ½ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë³´ìƒ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

- 2. TDRMì„ í™œìš©í•˜ë©´ ì¥ê¸° ëª©í‘œì™€ì˜ ì •ë ¬ì´ ê°œì„ ë˜ê³ , ê°•í™” í•™ìŠµì˜ ë¶ˆì•ˆì •ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 3. TDRMì€ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ë°©ë²•ì„ ë³´ì™„í•˜ë©°, ë‘ ë°©ë²•ì„ ì—°ì†ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, TDRMì„ ì ìš©í•œ ë³´ìƒ ëª¨ë¸ì€ Best-of-N ë° íŠ¸ë¦¬ íƒìƒ‰ ì„¤ì •ì—ì„œ ì„±ëŠ¥ì„ ê°ê° ìµœëŒ€ 6.6% ë° 23.7% í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 5. TDRMê³¼ RLVRì„ ê²°í•©í•˜ë©´ ë°ì´í„° íš¨ìœ¨ì„±ì´ ë†’ì•„ì ¸, 2.5k ë°ì´í„°ë§Œìœ¼ë¡œë„ ê¸°ì¡´ ë°©ë²•ì´ 50.1k ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 01:01:16*