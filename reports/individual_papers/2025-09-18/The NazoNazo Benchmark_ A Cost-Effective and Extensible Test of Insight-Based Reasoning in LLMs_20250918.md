---
keywords:
  - Large Language Models
  - Insight-Based Reasoning
  - Benchmark Saturation
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:05:12.475457",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Insight-Based Reasoning",
    "Benchmark Saturation"
  ],
  "rejected_keywords": [
    "Meta-Cognitive Weakness"
  ],
  "similarity_scores": {
    "Large Language Models": 0.85,
    "Insight-Based Reasoning": 0.7,
    "Benchmark Saturation": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs

**Korean Title:** NazoNazo ë²¤ì¹˜ë§ˆí¬: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ í†µì°° ê¸°ë°˜ ì¶”ë¡ ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]        [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/Insight-Based Reasoning|Insight-Based Reasoning]], [[keywords/Benchmark Saturation|Benchmark Saturation]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (79.6% similar)
- [[Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models A Perspective from Schoenfeld's Episode Theory]] (78.7% similar)
- [[Humor in Pixels_ Benchmarking Large Multimodal Models Understanding of Online Comics_20250918|Humor in Pixels Benchmarking Large Multimodal Models Understanding of Online Comics]] (77.6% similar)
- [[Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (77.3% similar)
- [[Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (77.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Masaharu Mizumoto, Dat Nguyen, Zhiheng Han, Jiyuan Fang, Heyuan Guan, Xingfu Li, Naoya Shiraishi, Xuyang Tian, Yo Nakawake, Le Minh Nguyen

## ğŸ“„ Abstract (ì›ë¬¸)

Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë²¤ì¹˜ë§ˆí¬ í¬í™”ì™€ ì˜¤ì—¼ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) í‰ê°€ì— ëŒ€í•œ ì‹ ë¢°ë¥¼ ì €í•´í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í†µì°° ê¸°ë°˜ì˜ ì¶”ë¡ ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ì¼ë³¸ ì–´ë¦°ì´ ìˆ˜ìˆ˜ê»˜ë¼ë¡œ êµ¬ì„±ëœ ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ì¸ Nazonazoë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. í•­ëª©ì€ ì§§ê³ (ëŒ€ë¶€ë¶„ í•œ ë¬¸ì¥), ì „ë¬¸ì ì¸ ë„ë©”ì¸ ì§€ì‹ì„ í•„ìš”ë¡œ í•˜ì§€ ì•Šìœ¼ë©°, ëŒ€ê·œëª¨ë¡œ ìƒì„±í•  ìˆ˜ ìˆì–´ ìœ ì¶œì´ ì˜ì‹¬ë  ë•Œ ë¸”ë¼ì¸ë“œ ì„¸íŠ¸ë¥¼ ì‹ ì†í•˜ê²Œ ê°±ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” 120ê°œì˜ ìˆ˜ìˆ˜ê»˜ë¼ì— ëŒ€í•´ 38ê°œì˜ ìµœì²¨ë‹¨ ëª¨ë¸ê³¼ 126ëª…ì˜ ì„±ì¸ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. GPT-5ë¥¼ ì œì™¸í•œ ì–´ë–¤ ëª¨ë¸ë„ ì¸ê°„ì˜ ì„±ëŠ¥ê³¼ ë¹„êµí•  ìˆ˜ ì—†ìœ¼ë©°, ì¸ê°„ì€ í‰ê·  52.9%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. í™•ì¥ëœ 201ê°œ í•­ëª©ì— ëŒ€í•œ ëª¨ë¸ ë¹„êµì—ì„œëŠ” ì¶”ë¡  ëª¨ë¸ì´ ë¹„ì¶”ë¡  ëª¨ë¸ë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ëª¨ë¸ í¬ê¸°ì™€ ì •í™•ë„ ê°„ì—ëŠ” ì‹ ë¢°í•  ë§Œí•œ ì—°ê´€ì„±ì´ ì—†ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì§‘ê³„ëœ ì •í™•ë„ë¥¼ ë„˜ì–´, ì‚¬ê³  ê¸°ë¡ì— ëŒ€í•œ ë¹„ê³µì‹ì ì¸ í›„ë³´ ì¶”ì  ë¶„ì„ì€ ë§ì€ ê²€ì¦ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤: ëª¨ë¸ì€ ì¢…ì¢… ì¤‘ê°„ í›„ë³´ ì¤‘ ì˜¬ë°”ë¥¸ í•´ë‹µì„ ìƒì„±í•˜ì§€ë§Œ ì´ë¥¼ ìµœì¢… ë‹µìœ¼ë¡œ ì„ íƒí•˜ì§€ ëª»í•˜ë©°, ì´ëŠ” ì—¬ëŸ¬ ëª¨ë¸ì—ì„œ ê´€ì°°ëœ ëŒ€í‘œì ì¸ ì˜ˆë¡œ ì„¤ëª…ë©ë‹ˆë‹¤. ë”°ë¼ì„œ NazonazoëŠ” í˜„ì¬ì˜ í‰ê°€ ìœ„ê¸°ë¥¼ í•´ê²°í•˜ë©´ì„œë„ ë°˜ë³µì ì¸ ë©”íƒ€ì¸ì§€ì  ì•½ì ì„ ì‹œì‚¬í•˜ë©°, í–¥í›„ ì œì–´ ë° ë³´ì • ë°©ë²•ì„ ìœ„í•œ ëª…í™•í•œ ëª©í‘œë¥¼ ì œê³µí•˜ëŠ” ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•˜ë©° ì‰½ê²Œ ê°±ì‹  ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ í˜•ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

NazonazoëŠ” ì¼ë³¸ ì–´ë¦°ì´ ìˆ˜ìˆ˜ê»˜ë¼ë¥¼ í™œìš©í•œ ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ë¡œ, í†µì°° ê¸°ë°˜ ì¶”ë¡ ì„ í‰ê°€í•©ë‹ˆë‹¤. 38ê°œì˜ ìµœì‹  ëª¨ë¸ê³¼ 126ëª…ì˜ ì„±ì¸ì„ ëŒ€ìƒìœ¼ë¡œ 120ê°œì˜ ìˆ˜ìˆ˜ê»˜ë¼ë¥¼ í‰ê°€í•œ ê²°ê³¼, ì¸ê°„ì˜ í‰ê·  ì •í™•ë„ëŠ” 52.9%ë¡œ, GPT-5ë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ì´ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 201ê°œì˜ í™•ì¥ëœ í•­ëª©ì—ì„œ ì¶”ë¡  ëª¨ë¸ì´ ë¹„ì¶”ë¡  ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¬ì§€ë§Œ, ëª¨ë¸ í¬ê¸°ì™€ ì •í™•ë„ ê°„ì˜ ëª…í™•í•œ ìƒê´€ê´€ê³„ëŠ” ì—†ì—ˆìŠµë‹ˆë‹¤. ìƒê°ì˜ ê¸°ë¡ì„ ë¶„ì„í•œ ê²°ê³¼, ëª¨ë¸ë“¤ì´ ì¤‘ê°„ í›„ë³´ ì¤‘ ì˜¬ë°”ë¥¸ ë‹µì„ ìƒì„±í•˜ê³ ë„ ìµœì¢… ë‹µìœ¼ë¡œ ì„ íƒí•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìŠµë‹ˆë‹¤. NazonazoëŠ” í˜„ì¬ í‰ê°€ ìœ„ê¸°ë¥¼ í•´ê²°í•˜ê³ , ëª¨ë¸ì˜ ë©”íƒ€ì¸ì§€ì  ì•½ì ì„ ë“œëŸ¬ë‚´ì–´ í–¥í›„ ê°œì„ ì˜ ëª©í‘œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. NazonazoëŠ” ì¼ë³¸ ì–´ë¦°ì´ ìˆ˜ìˆ˜ê»˜ë¼ë¥¼ í™œìš©í•˜ì—¬ í†µì°° ê¸°ë°˜ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.

- 2. 38ê°œì˜ ìµœì‹  ëª¨ë¸ê³¼ 126ëª…ì˜ ì„±ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ í‰ê°€ì—ì„œ GPT-5ë¥¼ ì œì™¸í•œ ëª¨ë“  ëª¨ë¸ì€ ì¸ê°„ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

- 3. ì¶”ë¡  ëª¨ë¸ì€ ë¹„ì¶”ë¡  ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©°, ëª¨ë¸ í¬ê¸°ì™€ ì •í™•ë„ ê°„ì—ëŠ” ì‹ ë¢°í•  ë§Œí•œ ì—°ê´€ì„±ì´ ì—†ìŠµë‹ˆë‹¤.

- 4. ë¹„ê³µì‹ì ì¸ ì‚¬ê³  ì¶”ì  ë¶„ì„ì—ì„œ ëª¨ë¸ì´ ì¤‘ê°„ í›„ë³´ ì¤‘ ì˜¬ë°”ë¥¸ í•´ë‹µì„ ì œì‹œí•˜ê³ ë„ ìµœì¢… ë‹µìœ¼ë¡œ ì„ íƒí•˜ì§€ ëª»í•˜ëŠ” ê²€ì¦ ì‹¤íŒ¨ ì‚¬ë¡€ê°€ ë‹¤ìˆ˜ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.

- 5. NazonazoëŠ” í˜„ì¬ì˜ í‰ê°€ ìœ„ê¸°ë¥¼ í•´ê²°í•˜ê³  ë¯¸ë˜ì˜ ì œì–´ ë° ë³´ì • ë°©ë²•ì„ ìœ„í•œ ëª…í™•í•œ ëª©í‘œë¥¼ ì œê³µí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ í˜•ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:45:12*