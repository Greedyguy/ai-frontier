---
keywords:
  - Data Coarse Graining
  - High-Pass Scheme
  - Ridge-Regularized Linear Regression
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:33:38.696919",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Data Coarse Graining",
    "High-Pass Scheme",
    "Ridge-Regularized Linear Regression"
  ],
  "rejected_keywords": [
    "Renormalization Group"
  ],
  "similarity_scores": {
    "Data Coarse Graining": 0.78,
    "High-Pass Scheme": 0.75,
    "Ridge-Regularized Linear Regression": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Data coarse graining can improve model performance

**Korean Title:** ë°ì´í„° ê±°ì¹ ê¸°(coarse graining)ëŠ” ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]      [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Ridge-Regularized Linear Regression|ridge-regularized linear regression]]
**âš¡ Unique Technical**: [[keywords/Data Coarse Graining|data coarse graining]], [[keywords/High-Pass Scheme|high-pass scheme]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (81.8% similar)
- [[CUFG_ Curriculum Unlearning Guided by the Forgetting Gradient_20250918|CUFG Curriculum Unlearning Guided by the Forgetting Gradient]] (81.3% similar)
- [[Not All Degradations Are Equal_ A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution_20250918|Not All Degradations Are Equal A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution]] (80.7% similar)
- [[Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model_20250918|Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model]] (80.6% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (80.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Alex Nguyen, David J. Schwab, Vudtiwat Ngampruetikorn

## ğŸ“„ Abstract (ì›ë¬¸)

Lossy data transformations by definition lose information. Yet, in modern
machine learning, methods like data pruning and lossy data augmentation can
help improve generalization performance. We study this paradox using a solvable
model of high-dimensional, ridge-regularized linear regression under 'data
coarse graining.' Inspired by the renormalization group in statistical physics,
we analyze coarse-graining schemes that systematically discard features based
on their relevance to the learning task. Our results reveal a nonmonotonic
dependence of the prediction risk on the degree of coarse graining. A
'high-pass' scheme--which filters out less relevant, lower-signal features--can
help models generalize better. By contrast, a 'low-pass' scheme that integrates
out more relevant, higher-signal features is purely detrimental. Crucially,
using optimal regularization, we demonstrate that this nonmonotonicity is a
distinct effect of data coarse graining and not an artifact of double descent.
Our framework offers a clear, analytical explanation for why careful data
augmentation works: it strips away less relevant degrees of freedom and
isolates more predictive signals. Our results highlight a complex, nonmonotonic
risk landscape shaped by the structure of the data, and illustrate how ideas
from statistical physics provide a principled lens for understanding modern
machine learning phenomena.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì†ì‹¤ ë°ì´í„° ë³€í™˜ì€ ì •ì˜ìƒ ì •ë³´ë¥¼ ìƒìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµì—ì„œëŠ” ë°ì´í„° ê°€ì§€ì¹˜ê¸° ë° ì†ì‹¤ ë°ì´í„° ì¦ê°•ê³¼ ê°™ì€ ë°©ë²•ì´ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” 'ë°ì´í„° ê±°ì¹ ê¸°' í•˜ì—ì„œ ê³ ì°¨ì›, ë¦¿ì§€ ì •ê·œí™” ì„ í˜• íšŒê·€ì˜ í•´ê²° ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ ì—­ì„¤ì„ ì—°êµ¬í•©ë‹ˆë‹¤. í†µê³„ ë¬¼ë¦¬í•™ì˜ ì¬ê·œê²©í™” êµ°ì—ì„œ ì˜ê°ì„ ë°›ì•„, í•™ìŠµ ê³¼ì œì™€ì˜ ê´€ë ¨ì„±ì— ë”°ë¼ ì²´ê³„ì ìœ¼ë¡œ íŠ¹ì§•ì„ ë²„ë¦¬ëŠ” ê±°ì¹ ê¸° ë°©ì‹ì„ ë¶„ì„í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ì˜ˆì¸¡ ìœ„í—˜ì´ ê±°ì¹ ê¸° ì •ë„ì— ë”°ë¼ ë¹„ë‹¨ì¡°ì ìœ¼ë¡œ ì˜ì¡´í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. 'ê³ ì—­ í†µê³¼' ë°©ì‹â€”ëœ ê´€ë ¨ì„± ìˆëŠ”, ë‚®ì€ ì‹ í˜¸ íŠ¹ì§•ì„ ê±¸ëŸ¬ë‚´ëŠ” ë°©ì‹â€”ì€ ëª¨ë¸ì´ ë” ì˜ ì¼ë°˜í™”í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, ë” ê´€ë ¨ì„± ìˆëŠ”, ë†’ì€ ì‹ í˜¸ íŠ¹ì§•ì„ í†µí•©í•˜ëŠ” 'ì €ì—­ í†µê³¼' ë°©ì‹ì€ ìˆœì „íˆ í•´ë¡­ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€, ìµœì ì˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë¹„ë‹¨ì¡°ì„±ì´ ë°ì´í„° ê±°ì¹ ê¸°ì˜ ë…íŠ¹í•œ íš¨ê³¼ì´ë©° ì´ì¤‘ í•˜ê°•ì˜ ì‚°ë¬¼ì´ ì•„ë‹˜ì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” ì™œ ì‹ ì¤‘í•œ ë°ì´í„° ì¦ê°•ì´ íš¨ê³¼ì ì¸ì§€ë¥¼ ëª…í™•í•˜ê³  ë¶„ì„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤: ëœ ê´€ë ¨ì„± ìˆëŠ” ììœ ë„ë¥¼ ì œê±°í•˜ê³  ë” ì˜ˆì¸¡ì ì¸ ì‹ í˜¸ë¥¼ ê³ ë¦½ì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ë°ì´í„° êµ¬ì¡°ì— ì˜í•´ í˜•ì„±ëœ ë³µì¡í•˜ê³  ë¹„ë‹¨ì¡°ì ì¸ ìœ„í—˜ ì§€í˜•ì„ ê°•ì¡°í•˜ë©°, í†µê³„ ë¬¼ë¦¬í•™ì˜ ì•„ì´ë””ì–´ê°€ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµ í˜„ìƒì„ ì´í•´í•˜ëŠ” ë° ì›ì¹™ì ì¸ ë Œì¦ˆë¥¼ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì •ë³´ ì†ì‹¤ì„ ìˆ˜ë°˜í•˜ëŠ” ë°ì´í„° ë³€í™˜ì´ ì–´ë–»ê²Œ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê³ ì°¨ì› ë¦¿ì§€ ì •ê·œí™” ì„ í˜• íšŒê·€ ëª¨ë¸ì„ í†µí•´ ë¶„ì„í•©ë‹ˆë‹¤. í†µê³„ ë¬¼ë¦¬í•™ì˜ ì¬ê·œê²©í™” êµ°ì—ì„œ ì˜ê°ì„ ë°›ì•„, í•™ìŠµ ê³¼ì œì™€ ê´€ë ¨ëœ íŠ¹ì§•ì„ ì²´ê³„ì ìœ¼ë¡œ ë²„ë¦¬ëŠ” 'ë°ì´í„° ì¡°ì¡í™”' ë°©ë²•ì„ ì—°êµ¬í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì˜ˆì¸¡ ìœ„í—˜ì´ ì¡°ì¡í™” ì •ë„ì— ë”°ë¼ ë¹„ë‹¨ì¡°ì ìœ¼ë¡œ ë³€í™”í•˜ë©°, ëœ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì œê±°í•˜ëŠ” 'ê³ ì—­í†µê³¼' ë°©ì‹ì´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì œê±°í•˜ëŠ” 'ì €ì—­í†µê³¼' ë°©ì‹ì€ í•´ë¡­ìŠµë‹ˆë‹¤. ìµœì ì˜ ì •ê·œí™”ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë¹„ë‹¨ì¡°ì„±ì´ ë°ì´í„° ì¡°ì¡í™”ì˜ ê³ ìœ í•œ íš¨ê³¼ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë°ì´í„° êµ¬ì¡°ì— ì˜í•´ í˜•ì„±ëœ ë³µì¡í•œ ìœ„í—˜ ì§€í˜•ì„ ê°•ì¡°í•˜ë©°, í†µê³„ ë¬¼ë¦¬í•™ì˜ ê°œë…ì´ í˜„ëŒ€ ë¨¸ì‹ ëŸ¬ë‹ í˜„ìƒì„ ì´í•´í•˜ëŠ” ë° ìœ ìš©í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì •ë³´ ì†ì‹¤ì„ ìˆ˜ë°˜í•˜ëŠ” ë°ì´í„° ë³€í™˜ì´ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì—°êµ¬í–ˆìŠµë‹ˆë‹¤.

- 2. ë°ì´í„°ì˜ ê´€ë ¨ì„±ì— ë”°ë¼ íŠ¹ì§•ì„ ì²´ê³„ì ìœ¼ë¡œ ë²„ë¦¬ëŠ” 'ë°ì´í„° ì¡°ëŒ€í™”' ë°©ì‹ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.

- 3. 'í•˜ì´íŒ¨ìŠ¤' ë°©ì‹ì€ ëœ ê´€ë ¨ ìˆëŠ” íŠ¹ì§•ì„ ê±¸ëŸ¬ë‚´ì–´ ëª¨ë¸ì˜ ì¼ë°˜í™”ë¥¼ ë•ìŠµë‹ˆë‹¤.

- 4. 'ë¡œìš°íŒ¨ìŠ¤' ë°©ì‹ì€ ë” ê´€ë ¨ ìˆëŠ” íŠ¹ì§•ì„ í†µí•©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì— í•´ë¡­ìŠµë‹ˆë‹¤.

- 5. ìµœì ì˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ì¡°ëŒ€í™”ì˜ ë¹„ë‹¨ì¡°ì  íš¨ê³¼ê°€ ë‘ë“œëŸ¬ì§€ë©°, ì´ëŠ” ì´ì¤‘ í•˜ê°•ì˜ ì¸ê³µë¬¼ì´ ì•„ë‹˜ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:54:13*