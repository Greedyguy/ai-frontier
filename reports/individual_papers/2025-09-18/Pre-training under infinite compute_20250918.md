---
keywords:
  - Large Language Models
  - Pre-Training
  - Scaling Laws
category: cs.AI
publish_date: 2025-09-18
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:15:25.020235",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Pre-Training",
    "Scaling Laws"
  ],
  "rejected_keywords": [
    "Ensemble Scaling",
    "Data Efficiency"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Pre-Training": 0.85,
    "Scaling Laws": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Pre-training under infinite compute

**Korean Title:** ë¬´í•œ ê³„ì‚°ì—ì„œì˜ ì‚¬ì „ í›ˆë ¨

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250918|2025-09-18]]       [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Large Language Models|language model]], [[keywords/Scaling Laws|scaling laws]]
**âš¡ Unique Technical**: [[keywords/Pre-Training|pre-training]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (82.7% similar)
- [[From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (81.9% similar)
- [[Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250918|Mind the Gap Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (81.4% similar)
- [[(P)rior(D)yna(F)low_ A Priori Dynamic Workflow Construction via Multi-Agent Collaboration_20250919|(P)rior(D)yna(F)low A Priori Dynamic Workflow Construction via Multi-Agent Collaboration]] (81.4% similar)
- [[PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (80.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Konwoo Kim, Suhas Kotha, Percy Liang, Tatsunori Hashimoto

## ğŸ“„ Abstract (ì›ë¬¸)

Since compute grows much faster than web text available for language model
pre-training, we ask how one should approach pre-training under fixed data and
no compute constraints. We first show that existing data-constrained approaches
of increasing epoch count and parameter count eventually overfit, and we
significantly improve upon such recipes by properly tuning regularization,
finding that the optimal weight decay is $30\times$ larger than standard
practice. Since our regularized recipe monotonically decreases loss following a
simple power law in parameter count, we estimate its best possible performance
via the asymptote of its scaling law rather than the performance at a fixed
compute budget. We then identify that ensembling independently trained models
achieves a significantly lower loss asymptote than the regularized recipe. Our
best intervention combining epoching, regularization, parameter scaling, and
ensemble scaling achieves an asymptote at 200M tokens using $5.17\times$ less
data than our baseline, and our data scaling laws predict that this improvement
persists at higher token budgets. We find that our data efficiency gains can be
realized at much smaller parameter counts as we can distill an ensemble into a
student model that is 8$\times$ smaller and retains $83\%$ of the ensembling
benefit. Finally, our interventions designed for validation loss generalize to
downstream benchmarks, achieving a $9\%$ improvement for pre-training evals and
a $17.5\times$ data efficiency improvement over continued pre-training on math
mid-training data. Our results show that simple algorithmic improvements can
enable significantly more data-efficient pre-training in a compute-rich future.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì»´í“¨íŒ… ì„±ëŠ¥ì´ ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í•™ìŠµì„ ìœ„í•œ ì›¹ í…ìŠ¤íŠ¸ì˜ ê°€ìš©ì„±ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ ì¦ê°€í•¨ì— ë”°ë¼, ìš°ë¦¬ëŠ” ê³ ì •ëœ ë°ì´í„°ì™€ ì»´í“¨íŒ… ì œì•½ ì—†ì´ ì‚¬ì „ í•™ìŠµì„ ì–´ë–»ê²Œ ì ‘ê·¼í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•´ ì§ˆë¬¸í•©ë‹ˆë‹¤. ë¨¼ì €, ì—í¬í¬ ìˆ˜ì™€ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê¸°ì¡´ì˜ ë°ì´í„° ì œì•½ ì ‘ê·¼ ë°©ì‹ì´ ê²°êµ­ ê³¼ì í•©ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê³ , ì ì ˆí•œ ì •ê·œí™”ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë°©ì‹ë“¤ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤. ìµœì ì˜ ê°€ì¤‘ì¹˜ ê°ì‡ ê°€ í‘œì¤€ ê´€í–‰ë³´ë‹¤ $30\times$ ë” í¬ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì •ê·œí™”ëœ ë ˆì‹œí”¼ëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ì— ë”°ë¼ ë‹¨ìˆœí•œ ë©±ë²•ì¹™ì„ ë”°ë¥´ë©° ì†ì‹¤ì„ ë‹¨ì¡°ë¡­ê²Œ ê°ì†Œì‹œí‚¤ë¯€ë¡œ, ê³ ì •ëœ ì»´í“¨íŒ… ì˜ˆì‚°ì—ì„œì˜ ì„±ëŠ¥ë³´ë‹¤ëŠ” ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì˜ ì ê·¼ì„ ì„ í†µí•´ ìµœìƒì˜ ì„±ëŠ¥ì„ ì¶”ì •í•©ë‹ˆë‹¤. ì´í›„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ëŠ” ê²ƒì´ ì •ê·œí™”ëœ ë ˆì‹œí”¼ë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ì†ì‹¤ ì ê·¼ì„ ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì—í¬í‚¹, ì •ê·œí™”, íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§, ì•™ìƒë¸” ìŠ¤ì¼€ì¼ë§ì„ ê²°í•©í•œ ìš°ë¦¬ì˜ ìµœì  ê°œì…ì€ 200M í† í°ì—ì„œ ì ê·¼ì„ ì„ ë‹¬ì„±í•˜ë©°, ì´ëŠ” ê¸°ë³¸ê°’ë³´ë‹¤ $5.17\times$ ì ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì€ ì´ ê°œì„ ì´ ë” ë†’ì€ í† í° ì˜ˆì‚°ì—ì„œë„ ì§€ì†ë¨ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì•™ìƒë¸”ì„ 8ë°° ì‘ì€ í•™ìƒ ëª¨ë¸ë¡œ ì¦ë¥˜í•˜ì—¬ ì•™ìƒë¸”ì˜ ì´ì ì˜ $83\%$ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ì˜ ë°ì´í„° íš¨ìœ¨ì„± í–¥ìƒì´ í›¨ì”¬ ì‘ì€ íŒŒë¼ë¯¸í„° ìˆ˜ì—ì„œë„ ì‹¤í˜„ë  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ê²€ì¦ ì†ì‹¤ì„ ìœ„í•´ ì„¤ê³„ëœ ìš°ë¦¬ì˜ ê°œì…ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë²¤ì¹˜ë§ˆí¬ì— ì¼ë°˜í™”ë˜ì–´, ì‚¬ì „ í•™ìŠµ í‰ê°€ì—ì„œ $9\%$ì˜ ê°œì„ ê³¼ ìˆ˜í•™ ì¤‘ê°„ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì§€ì†ì ì¸ ì‚¬ì „ í•™ìŠµì— ë¹„í•´ $17.5\times$ì˜ ë°ì´í„° íš¨ìœ¨ì„± ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì´ ì»´í“¨íŒ…ì´ í’ë¶€í•œ ë¯¸ë˜ì— í›¨ì”¬ ë” ë°ì´í„° íš¨ìœ¨ì ì¸ ì‚¬ì „ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³ ì •ëœ ë°ì´í„°ì™€ ì œí•œëœ ì»´í“¨íŒ… í™˜ê²½ì—ì„œ ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°ì´í„° ì œì•½ ì ‘ê·¼ë²•ì€ ì—í¬í¬ ìˆ˜ì™€ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ì§€ë§Œ ê²°êµ­ ê³¼ì í•© ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì •ê·œí™”ë¥¼ ì ì ˆíˆ ì¡°ì •í•˜ì—¬, ìµœì ì˜ ê°€ì¤‘ì¹˜ ê°ì‡ ê°€ ê¸°ì¡´ë³´ë‹¤ 30ë°° í¬ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ëŠ” ê²ƒì´ ë” ë‚®ì€ ì†ì‹¤ ë¹„ìœ¨ì„ ë‹¬ì„±í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì—í¬í¬, ì •ê·œí™”, íŒŒë¼ë¯¸í„° ë° ì•™ìƒë¸” ìŠ¤ì¼€ì¼ë§ì„ ê²°í•©í•œ ìµœì ì˜ ë°©ë²•ì€ 200M í† í°ì—ì„œ ê¸°ì¡´ë³´ë‹¤ 5.17ë°° ì ì€ ë°ì´í„°ë¡œ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‘ì€ íŒŒë¼ë¯¸í„° ìˆ˜ì—ì„œë„ íš¨ê³¼ì ì´ë©°, ì•™ìƒë¸”ì„ 8ë°° ì‘ì€ í•™ìƒ ëª¨ë¸ë¡œ ì¦ë¥˜í•˜ì—¬ 83%ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œì„ ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ì¼ë°˜í™”ë˜ì–´, ì‚¬ì „ í•™ìŠµ í‰ê°€ì—ì„œ 9% ê°œì„ ê³¼ ìˆ˜í•™ ì¤‘ê°„ í•™ìŠµ ë°ì´í„°ì—ì„œ 17.5ë°°ì˜ ë°ì´í„° íš¨ìœ¨ì„±ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì´ ë°ì´í„° íš¨ìœ¨ì ì¸ ì‚¬ì „ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³ ì •ëœ ë°ì´í„°ì™€ ì»´í“¨íŒ… ì œì•½ ì¡°ê±´ í•˜ì—ì„œì˜ ì‚¬ì „ í›ˆë ¨ ì ‘ê·¼ë²•ì„ íƒêµ¬í•˜ë©°, ê¸°ì¡´ì˜ ë°ì´í„° ì œì•½ ì ‘ê·¼ë²•ì´ ê²°êµ­ ê³¼ì í•©ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì •ê·œí™”ë¥¼ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

- 2. ìµœì ì˜ ê°€ì¤‘ì¹˜ ê°ì†Œê°€ ì¼ë°˜ì ì¸ ê´€í–‰ë³´ë‹¤ 30ë°° ë” í¬ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ê³ , ë§¤ê°œë³€ìˆ˜ ìˆ˜ì— ë”°ë¥¸ ì†ì‹¤ ê°ì†Œê°€ ë‹¨ìˆœí•œ ë©± ë²•ì¹™ì„ ë”°ë¦„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

- 3. ë…ë¦½ì ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ëŠ” ê²ƒì´ ì •ê·œí™”ëœ ë°©ë²•ë³´ë‹¤ ë” ë‚®ì€ ì†ì‹¤ ë¹„ëŒ€ì¹­ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

- 4. ì—í¬í‚¹, ì •ê·œí™”, ë§¤ê°œë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§, ì•™ìƒë¸” ìŠ¤ì¼€ì¼ë§ì„ ê²°í•©í•œ ìµœì ì˜ ê°œì…ì´ 200M í† í°ì—ì„œ ë¹„ëŒ€ì¹­ì„ ë‹¬ì„±í•˜ë©°, ì´ëŠ” ê¸°ì¤€ì„ ë³´ë‹¤ 5.17ë°° ì ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

- 5. ë°ì´í„° íš¨ìœ¨ì„± í–¥ìƒì´ í›¨ì”¬ ì‘ì€ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì—ì„œë„ ì‹¤í˜„ ê°€ëŠ¥í•˜ë©°, ì•™ìƒë¸”ì„ 8ë°° ì‘ì€ í•™ìƒ ëª¨ë¸ë¡œ ì¦ë¥˜í•˜ì—¬ ì•™ìƒë¸” ì´ì ì˜ 83%ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:41:29*