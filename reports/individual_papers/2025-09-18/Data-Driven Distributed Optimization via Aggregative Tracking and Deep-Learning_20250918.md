
# Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning

**Korean Title:** ë°ì´í„° ê¸°ë°˜ ë¶„ì‚° ìµœì í™”ë¥¼ ìœ„í•œ ì§‘ê³„ ì¶”ì  ë° ë”¥ ëŸ¬ë‹ì„ í†µí•œ ë°©ë²•Maintain the academic tone and technical terminology appropriately.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Data-Driven Optimization

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Distributionally_Robust_Equilibria_over_the_Wasserstein_Distance_for_Generalized_Nash_Game_20250918|Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game]] (81.8% similar)
- [[Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced Energy Storage Dispatch]] (81.4% similar)
- [[Polynomial-Time_Approximation_Schemes_via_Utility_Alignment_Unit-Demand_Pricing_and_More_20250918|Polynomial-Time Approximation Schemes via Utility Alignment: Unit-Demand Pricing and More]] (80.1% similar)
- [[Welfare and Cost Aggregation for Multi-Agent Control: When to Choose Which Social Cost Function, and Why?]] (79.9% similar)
- [[Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction]] (79.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.04668v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel distributed data-driven optimization scheme. In detail, we focus on the so-called aggregative framework, a scenario in which a set of agents aim to cooperatively minimize the sum of local costs, each depending on both local decision variables and an aggregation of all of them. We consider a data-driven setup where each objective function is unknown and can be sampled at a single point per iteration (thanks to, e.g., feedback from users or sensors). We address this scenario through a distributed algorithm combining three components: (i) a learning part leveraging neural networks to learn the local costs descent direction, (ii) an optimization routine steering the estimates according to the learned direction to minimize the global cost, and (iii) a tracking mechanism locally reconstructing the unavailable global quantities. Using tools from system theory, i.e., timescale separation and averaging theory, we formally prove that in strongly convex setups, the distributed scheme linearly converges to a neighborhood of the optimum, whose radius depends on the accuracy of the neural networks. Finally, numerical simulations validate the theoretical results.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2503.04668v2 ê³µì§€ ìœ í˜•: êµì²´-êµì°¨
ìš”ì•½: ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ë¶„ì‚° ë°ì´í„° ì£¼ë„ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì§‘í•©ì´ í˜‘ë ¥í•˜ì—¬ ì§€ì—­ ë¹„ìš©ì˜ í•©ì„ ìµœì†Œí™”í•˜ë ¤ê³  í•˜ëŠ” ìƒí™©ì¸ ì§‘í•©ì  í”„ë ˆì„ì›Œí¬ì— ì´ˆì ì„ ë§ì¶˜ë‹¤. ê°ê°ì˜ ì§€ì—­ ì˜ì‚¬ê²°ì • ë³€ìˆ˜ì™€ ê·¸ë“¤ ëª¨ë‘ì˜ ì§‘í•©ì— ì˜ì¡´í•˜ëŠ” ì§€ì—­ ë¹„ìš©ì— ëŒ€í•´ ê³ ë ¤í•œë‹¤. ìš°ë¦¬ëŠ” ê° ëª©ì  í•¨ìˆ˜ê°€ ì•Œë ¤ì§€ì§€ ì•Šê³  ë°˜ë³µë‹¹ í•œ ì§€ì ì—ì„œ ìƒ˜í”Œë§ë  ìˆ˜ ìˆëŠ” ë°ì´í„° ì£¼ë„ ì„¤ì •ì„ ê³ ë ¤í•œë‹¤ (ì˜ˆ: ì‚¬ìš©ì ë˜ëŠ” ì„¼ì„œë¡œë¶€í„°ì˜ í”¼ë“œë°±ì„ í†µí•´). ìš°ë¦¬ëŠ” ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¥¼ ê²°í•©í•œ ë¶„ì‚° ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì´ ìƒí™©ì„ ë‹¤ë£¬ë‹¤: (i) ì§€ì—­ ë¹„ìš© í•˜ê°• ë°©í–¥ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‹ ê²½ë§ì„ í™œìš©í•˜ëŠ” í•™ìŠµ ë¶€ë¶„, (ii) í•™ìŠµëœ ë°©í–¥ì— ë”°ë¼ ì¶”ì •ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ì „ì—­ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì í™” ë£¨í‹´, ê·¸ë¦¬ê³  (iii) ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì „ì—­ ì–‘ì„ ì§€ì—­ì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì¶”ì  ë©”ì»¤ë‹ˆì¦˜. ì‹œìŠ¤í…œ ì´ë¡ ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬, ì¦‰ ì‹œê°„ ì²™ë„ ë¶„ë¦¬ ë° í‰ê·  ì´ë¡ ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ê°•í•˜ê²Œ ë³¼ë¡í•œ ì„¤ì •ì—ì„œ ë¶„ì‚° ë°©ì‹ì´ ìµœì ì ì˜ ê·¼ì²˜ë¡œ ì„ í˜• ìˆ˜ë ´í•¨ì„ í˜•ì‹ì ìœ¼ë¡œ ì¦ëª…í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìˆ˜ì¹˜ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ë¡ ì  ê²°ê³¼ë¥¼ ê²€ì¦í•œë‹¤.

## ğŸ“ ìš”ì•½

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ë¶„ì‚° ë°ì´í„° ì£¼ë„ ìµœì í™” ë°©ë²•ë¡ ì„ ì œì•ˆí•œë‹¤. ë³¸ ì—°êµ¬ëŠ” ì§‘í•©ì  í”„ë ˆì„ì›Œí¬ì— ì´ˆì ì„ ë§ì¶”ë©°, ì¼ë ¨ì˜ ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥í•˜ì—¬ ì§€ì—­ ë¹„ìš©ì˜ í•©ì„ ìµœì†Œí™”í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£¬ë‹¤. ê°ê°ì˜ ì§€ì—­ ì˜ì‚¬ê²°ì • ë³€ìˆ˜ì™€ ê·¸ë“¤ì˜ ì§‘í•©ì„ ì§‘ê³„í•œ ê²ƒì— ë”°ë¼ ì§€ì—­ ë¹„ìš©ì´ ë‹¬ë¼ì§€ëŠ” ìƒí™©ì´ë‹¤. ìš°ë¦¬ëŠ” ê° ëª©ì  í•¨ìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì´ë©° í•œ ë²ˆì˜ ë°˜ë³µë‹¹ í•˜ë‚˜ì˜ ì§€ì ì—ì„œ ìƒ˜í”Œë§ë  ìˆ˜ ìˆëŠ” ë°ì´í„° ì£¼ë„ ì„¤ì •ì„ ê³ ë ¤í•œë‹¤. ìš°ë¦¬ëŠ” ì‹ ê²½ë§ì„ í™œìš©í•˜ì—¬ ì§€ì—­ ë¹„ìš© í•˜ê°• ë°©í–¥ì„ í•™ìŠµí•˜ëŠ” í•™ìŠµ ë¶€ë¶„, í•™ìŠµëœ ë°©í–¥ì— ë”°ë¼ ì¶”ì •ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ì „ì—­ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì í™” ë£¨í‹´, ê·¸ë¦¬ê³  ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì „ì—­ ì–‘ì„ ì§€ì—­ì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì¶”ì  ë©”ì»¤ë‹ˆì¦˜ì„ ê²°í•©í•œ ë¶„ì‚° ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•œë‹¤. ê°•í•˜ê²Œ ë³¼ë¡í•œ ì„¤ì •ì—ì„œ, ë¶„ì‚° ë°©ë²•ë¡ ì´ ìµœì ì  ê·¼ì²˜ë¡œ ì„ í˜• ìˆ˜ë ´í•¨ì„ ê³µì‹ì ìœ¼ë¡œ ì¦ëª…í•˜ë©°, ì´ë•Œ ìµœì ì ì˜ ë°˜ê²½ì€ ì‹ ê²½ë§ì˜ ì •í™•ë„ì— ë”°ë¼ ê²°ì •ëœë‹¤. ìµœì¢…ì ìœ¼ë¡œ, ìˆ˜ì¹˜ ì‹œë®¬ë ˆì´ì…˜ì€ ì´ë¡ ì  ê²°ê³¼ë¥¼ ê²€ì¦í•œë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¶„ì‚° ë°ì´í„° ê¸°ë°˜ ìµœì í™” ë°©ë²• ì œì•ˆ

- 2. ê° ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥ì ìœ¼ë¡œ ì§€ì—­ ë¹„ìš©ì˜ í•©ì„ ìµœì†Œí™”í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ì´ˆì 

- 3. ì‹ ê²½ë§ì„ í™œìš©í•˜ì—¬ ì§€ì—­ ë¹„ìš© í•˜ê°• ë°©í–¥ì„ í•™ìŠµí•˜ê³  ì „ì—­ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì í™” ë£¨í‹´ì„ ê²°í•©í•˜ëŠ” ë¶„ì‚° ì•Œê³ ë¦¬ì¦˜ ì œì•ˆ

- 4. ê°•í•˜ê²Œ ë³¼ë¡í•œ ì„¤ì •ì—ì„œ ë¶„ì‚° ë°©ë²•ì´ ìµœì ì  ì£¼ë³€ìœ¼ë¡œ ì„ í˜• ìˆ˜ë ´í•¨ì„ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…

- 5. ìˆ˜ì¹˜ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ë¡ ì  ê²°ê³¼ë¥¼ ê²€ì¦í•¨.

---

*Generated on 2025-09-18 17:26:18*