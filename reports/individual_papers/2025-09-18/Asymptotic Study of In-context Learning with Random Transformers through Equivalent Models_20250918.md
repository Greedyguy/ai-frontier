# Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models

**Korean Title:** ëœë¤ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í†µí•œ ë§¥ë½ ë‚´ í•™ìŠµì˜ ì ê·¼ì  ì—°êµ¬: ë™ë“± ëª¨ë¸ì„ í†µí•œ ì ‘ê·¼

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[authors/Samet Demir|Samet Demir]] [[authors/Zafer Dogan|Zafer Dogan]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Double-descent Phenomenon

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (77.8% similar)
- [[Hierarchical Learning for Maze Navigation_ Emergence of Mental Representations via Second-Order Learning_20250917|Hierarchical Learning for Maze Navigation Emergence of Mental Representations via Second-Order Learning]] (77.6% similar)
- [[Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control_20250919|Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control]] (77.2% similar)
- [[Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (77.1% similar)
- [[Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (77.0% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Samet Demir, Zafer Dogan

## ğŸ“„ Abstract (ì›ë¬¸)

We study the in-context learning (ICL) capabilities of pretrained
Transformers in the setting of nonlinear regression. Specifically, we focus on
a random Transformer with a nonlinear MLP head where the first layer is
randomly initialized and fixed while the second layer is trained. Furthermore,
we consider an asymptotic regime where the context length, input dimension,
hidden dimension, number of training tasks, and number of training samples
jointly grow. In this setting, we show that the random Transformer behaves
equivalent to a finite-degree Hermite polynomial model in terms of ICL error.
This equivalence is validated through simulations across varying activation
functions, context lengths, hidden layer widths (revealing a double-descent
phenomenon), and regularization settings. Our results offer theoretical and
empirical insights into when and how MLP layers enhance ICL, and how
nonlinearity and over-parameterization influence model performance.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë¹„ì„ í˜• íšŒê·€ ì„¤ì •ì—ì„œ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë§¥ë½ ë‚´ í•™ìŠµ(ICL) ëŠ¥ë ¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì²« ë²ˆì§¸ ì¸µì´ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë˜ê³  ê³ ì •ë˜ëŠ” ë¹„ì„ í˜• MLP í—¤ë“œë¥¼ ê°€ì§„ ë¬´ì‘ìœ„ íŠ¸ëœìŠ¤í¬ë¨¸ì— ì´ˆì ì„ ë§ì¶”ë©°, ë‘ ë²ˆì§¸ ì¸µì€ í›ˆë ¨ë©ë‹ˆë‹¤. ë˜í•œ, ë§¥ë½ ê¸¸ì´, ì…ë ¥ ì°¨ì›, ì€ë‹‰ ì°¨ì›, í›ˆë ¨ ê³¼ì œì˜ ìˆ˜, í›ˆë ¨ ìƒ˜í”Œì˜ ìˆ˜ê°€ í•¨ê»˜ ì¦ê°€í•˜ëŠ” ì ê·¼ì  ì²´ì œë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ì´ ì„¤ì •ì—ì„œ, ìš°ë¦¬ëŠ” ë¬´ì‘ìœ„ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ICL ì˜¤ë¥˜ ì¸¡ë©´ì—ì„œ ìœ í•œ ì°¨ìˆ˜ì˜ ì—ë¥´ë¯¸íŠ¸ ë‹¤í•­ì‹ ëª¨ë¸ê³¼ ë™ë“±í•˜ê²Œ ì‘ìš©í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë™ë“±ì„±ì€ ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜, ë§¥ë½ ê¸¸ì´, ì€ë‹‰ì¸µ ë„ˆë¹„(ì´ì¤‘ í•˜ê°• í˜„ìƒì„ ë“œëŸ¬ëƒ„), ì •ê·œí™” ì„¤ì •ì— ê±¸ì¹œ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ê²€ì¦ë©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” MLP ì¸µì´ ì–¸ì œ ê·¸ë¦¬ê³  ì–´ë–»ê²Œ ICLì„ í–¥ìƒì‹œí‚¤ëŠ”ì§€, ê·¸ë¦¬ê³  ë¹„ì„ í˜•ì„±ê³¼ ê³¼ë§¤ê°œë³€ìˆ˜ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ì— ëŒ€í•œ ì´ë¡ ì  ë° ì‹¤ì¦ì  í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ì„ í˜• íšŒê·€ í™˜ê²½ì—ì„œ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë¬¸ë§¥ í•™ìŠµ(ICL) ëŠ¥ë ¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ëœë¤ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë¹„ì„ í˜• MLP í—¤ë“œë¥¼ ì‚¬ìš©í•˜ì—¬, ì²« ë²ˆì§¸ ì¸µì€ ëœë¤ ì´ˆê¸°í™” í›„ ê³ ì •í•˜ê³  ë‘ ë²ˆì§¸ ì¸µì€ í•™ìŠµí•©ë‹ˆë‹¤. ë¬¸ë§¥ ê¸¸ì´, ì…ë ¥ ì°¨ì›, ìˆ¨ê²¨ì§„ ì°¨ì›, í•™ìŠµ ê³¼ì œ ìˆ˜, ìƒ˜í”Œ ìˆ˜ê°€ í•¨ê»˜ ì¦ê°€í•˜ëŠ” ë¹„ëŒ€ì¹­ì  í™˜ê²½ì—ì„œ, ëœë¤ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ìœ í•œ ì°¨ìˆ˜ì˜ ì—ë¥´ë¯¸íŠ¸ ë‹¤í•­ì‹ ëª¨ë¸ê³¼ ICL ì˜¤ë¥˜ ì¸¡ë©´ì—ì„œ ë™ë“±í•˜ê²Œ ì‘ë™í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜, ë¬¸ë§¥ ê¸¸ì´, ìˆ¨ê²¨ì§„ ì¸µ ë„ˆë¹„, ì •ê·œí™” ì„¤ì •ì„ í†µí•´ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ê²€ì¦í•˜ì˜€ìœ¼ë©°, MLP ì¸µì´ ICLì„ ê°•í™”í•˜ëŠ” ì¡°ê±´ê³¼ ë¹„ì„ í˜•ì„± ë° ê³¼ë§¤ê°œë³€ìˆ˜ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´ë¡ ì , ê²½í—˜ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ì„ í˜• íšŒê·€ ì„¤ì •ì—ì„œ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë¬¸ë§¥ ë‚´ í•™ìŠµ(ICL) ëŠ¥ë ¥ì„ ì—°êµ¬í–ˆìŠµë‹ˆë‹¤.

- 2. ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ì²« ë²ˆì§¸ ì¸µê³¼ í•™ìŠµëœ ë‘ ë²ˆì§¸ ì¸µì„ ê°€ì§„ ë¹„ì„ í˜• MLP í—¤ë“œì˜ ëœë¤ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì—°êµ¬ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

- 3. ë¬¸ë§¥ ê¸¸ì´, ì…ë ¥ ì°¨ì›, ì€ë‹‰ ì°¨ì›, í•™ìŠµ ê³¼ì œ ìˆ˜, í•™ìŠµ ìƒ˜í”Œ ìˆ˜ê°€ í•¨ê»˜ ì¦ê°€í•˜ëŠ” ë¹„ëŒ€ì¹­ì  ìƒí™©ì„ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤.

- 4. ëœë¤ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ìœ í•œ ì°¨ìˆ˜ì˜ ì—ë¥´ë¯¸íŠ¸ ë‹¤í•­ì‹ ëª¨ë¸ê³¼ ICL ì˜¤ë¥˜ ì¸¡ë©´ì—ì„œ ë™ë“±í•˜ê²Œ ì‘ë™í•¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

- 5. ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜, ë¬¸ë§¥ ê¸¸ì´, ì€ë‹‰ì¸µ ë„ˆë¹„, ì •ê·œí™” ì„¤ì •ì—ì„œì˜ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ ë™ë“±ì„±ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 00:56:54*