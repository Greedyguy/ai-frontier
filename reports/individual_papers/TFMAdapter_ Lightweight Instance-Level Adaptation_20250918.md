
# TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates

**Korean Title:** TFMAdapter: ê°€ë²¼ìš´ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ì˜ ì½”ë² ë¦¬ì—ì´íŠ¸ë¥¼ í™œìš©í•œ Foundation ëª¨ë¸ ì˜ˆì¸¡ ì ì‘

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Non-parametric Cascade|Non-parametric Cascade]] [[keywords/broad/Time Series Foundation Models|Time Series Foundation Models]] [[keywords/broad/Covariates|Covariates]] [[keywords/specific/Instance-Level Adaptation|Instance-Level Adaptation]] [[keywords/unique/TFMAdapter|TFMAdapter]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Two-stage Method
**ğŸ”¬ Broad Technical**: Time Series Foundation Models, Covariates
**ğŸ”— Specific Connectable**: Instance-Level Adaptation
**â­ Unique Technical**: TFMAdapter

**ArXiv ID**: [2509.13906](https://arxiv.org/abs/2509.13906)
**Published**: 2025-09-18
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.13906.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Time Series Foundation Models` â€¢ 

`Covariates` â€¢ 

`Instance-Level Adaptation` â€¢ 

`TFMAdapter` â€¢ 

`Two-stage Method`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13906v1 Announce Type: new 
Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art performance in univariate forecasting on new time series simply by conditioned on a brief history of past values. Their success demonstrates that large-scale pretraining across diverse domains can acquire the inductive bias to generalize from temporal patterns in a brief history. However, most TSFMs are unable to leverage covariates -- future-available exogenous variables critical for accurate forecasting in many applications -- due to their domain-specific nature and the lack of associated inductive bias. We propose TFMAdapter, a lightweight, instance-level adapter that augments TSFMs with covariate information without fine-tuning. Instead of retraining, TFMAdapter operates on the limited history provided during a single model call, learning a non-parametric cascade that combines covariates with univariate TSFM forecasts. However, such learning would require univariate forecasts at all steps in the history, requiring too many calls to the TSFM. To enable training on the full historical context while limiting TSFM invocations, TFMAdapter uses a two-stage method: (1) generating pseudo-forecasts with a simple regression model, and (2) training a Gaussian Process regressor to refine predictions using both pseudo- and TSFM forecasts alongside covariates. Extensive experiments on real-world datasets demonstrate that TFMAdapter consistently outperforms both foundation models and supervised baselines, achieving a 24-27\% improvement over base foundation models with minimal data and computational overhead. Our results highlight the potential of lightweight adapters to bridge the gap between generic foundation models and domain-specific forecasting needs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13906v1 ë°œí‘œ ìœ í˜•: ìƒˆë¡œìš´
ìš”ì•½: ìµœê·¼ì—ëŠ” ì‹œê³„ì—´ ê¸°ë°˜ ëª¨ë¸(Time Series Foundation Models, TSFMs)ì´ ë‹¨ìˆœíˆ ê³¼ê±° ê°’ì˜ ê°„ë‹¨í•œ ê¸°ë¡ì— ì˜ì¡´í•˜ì—¬ ìƒˆë¡œìš´ ì‹œê³„ì—´ì—ì„œ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì˜ ì„±ê³µì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ì´ ê°„ë‹¨í•œ ê¸°ë¡ì˜ ì‹œê°„ì  íŒ¨í„´ìœ¼ë¡œë¶€í„° ì¼ë°˜í™”í•˜ê¸° ìœ„í•œ ê·€ë‚© í¸í–¥ì„ ìŠµë“í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ TSFMsëŠ” ë„ë©”ì¸ íŠ¹ì •ì„±ê³¼ ê´€ë ¨ëœ ê·€ë‚© í¸í–¥ì˜ ë¶€ì¬ë¡œ ì¸í•´ ì •í™•í•œ ì˜ˆì¸¡ì„ ìœ„í•œ ë§¤ìš° ì¤‘ìš”í•œ ë¯¸ë˜ ê°€ëŠ¥í•œ ì™¸ìƒ ë³€ìˆ˜ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” TFMAdapterë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” TSFMsì— ë¯¸ì„¸ ì¡°ì • ì—†ì´ ì™¸ìƒ ë³€ìˆ˜ ì •ë³´ë¥¼ ë³´ì™„í•˜ëŠ” ê°€ë²¼ìš´ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ì–´ëŒ‘í„°ì…ë‹ˆë‹¤. ì¬í›ˆë ¨ ëŒ€ì‹ , TFMAdapterëŠ” ë‹¨ì¼ ëª¨ë¸ í˜¸ì¶œ ì¤‘ì— ì œê³µëœ ì œí•œëœ ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ì—¬ ì™¸ìƒ ë³€ìˆ˜ì™€ ë‹¨ë³€ëŸ‰ TSFM ì˜ˆì¸¡ì„ ê²°í•©í•˜ëŠ” ë¹„ëª¨ìˆ˜ì  ì¹´ìŠ¤ì¼€ì´ë“œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ í•™ìŠµì€ ëª¨ë“  ë‹¨ê³„ì—ì„œ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì„ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— TSFMì— ë„ˆë¬´ ë§ì€ í˜¸ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤. TSFM í˜¸ì¶œì„ ì œí•œí•˜ë©´ì„œ ì „ì²´ ì—­ì‚¬ì  ë§¥ë½ì—ì„œ í›ˆë ¨í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ TFMAdapterëŠ” ë‘ ë‹¨ê³„ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: (1) ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€ìƒ ì˜ˆì¸¡ ìƒì„± ë° (2) ê°€ìƒ ë° TSFM ì˜ˆì¸¡ê³¼ ì™¸ìƒ ë³€ìˆ˜ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ì •êµí™”í•˜ê¸° ìœ„í•´ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ìë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œì˜ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” TFMAdapterê°€ ê¸°ì´ˆ ëª¨ë¸ê³¼ ì§€ë„ ê¸°ì¤€ì„ ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, ìµœì†Œí•œì˜ ë°ì´í„° ë° ê³„ì‚° ì˜¤ë²„í—¤ë“œë¡œ ê¸°ì´ˆ ëª¨ë¸ì— ëŒ€í•´ 24-27%ì˜ ê°œì„ ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ê°€ë²¼ìš´ ì–´ëŒ‘í„°ì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•˜ì—¬ ì¼ë°˜ì ì¸ ê¸°ì´ˆ ëª¨ë¸ê³¼ ë„ë©”ì¸ íŠ¹ì • ì˜ˆì¸¡ ìš”êµ¬ ì‚¬ì´ì˜ ê°„ê·¹ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

## ğŸ“ ìš”ì•½

ìµœê·¼ì—ëŠ” ì‹œê³„ì—´ ê¸°ë°˜ ëª¨ë¸(Time Series Foundation Models, TSFMs)ì´ ê³¼ê±° ê°’ì˜ ê°„ë‹¨í•œ ì´ë ¥ì— ì˜ì¡´í•˜ì—¬ ìƒˆë¡œìš´ ì‹œê³„ì—´ì˜ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì—ì„œ ìµœê³  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ë“¤ì˜ ì„±ê³µì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ì´ ê°„ë‹¨í•œ ì´ë ¥ì˜ ì‹œê°„ì  íŒ¨í„´ìœ¼ë¡œë¶€í„° ì¼ë°˜í™”ì˜ í¸í–¥ì„ ìŠµë“í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ TSFMì€ ë„ë©”ì¸ë³„ íŠ¹ì„±ê³¼ ê´€ë ¨ëœ ì¶”ë¡  í¸í–¥ì˜ ë¶€ì¬ë¡œ ì¸í•´ ë§ì€ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì •í™•í•œ ì˜ˆì¸¡ì„ ìœ„í•´ ì¤‘ìš”í•œ ë¯¸ë˜ ê°€ëŠ¥í•œ ì™¸ìƒ ë³€ìˆ˜ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” TSFMì— ì„¸ë¶€ ì •ë³´ë¥¼ ë§ë¶™ì´ëŠ” ê²½ëŸ‰ì˜ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ì–´ëŒ‘í„°ì¸ TFMAdapterë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. TFMAdapterëŠ” ì¬í›ˆë ¨ ì—†ì´ TSFMì„ ì™¸ìƒ ë³€ìˆ˜ ì •ë³´ë¡œ ë³´ê°•í•©ë‹ˆë‹¤. TFMAdapterëŠ” í•œ ëª¨ë¸ í˜¸ì¶œ ì¤‘ì— ì œê³µëœ ì œí•œëœ ì´ë ¥ì—ì„œ ì‘ë™í•˜ë©°, ì™¸ìƒ ë³€ìˆ˜ì™€ ë‹¨ë³€ëŸ‰ TSFM ì˜ˆì¸¡ì„ ê²°í•©í•˜ëŠ” ë¹„ëª¨ìˆ˜ì  ìºìŠ¤ì¼€ì´ë“œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ì‹œê³„ì—´ ê¸°ë°˜ ëª¨ë¸(TSFMs)ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ ì‚¬ì „ í›ˆë ¨ì„ í†µí•´ ì‹œê°„ íŒ¨í„´ì„ ì¼ë°˜í™”í•˜ëŠ” ì¸ì§€ í¸í–¥ì„ ìŠµë“í•˜ì—¬ ìµœê·¼ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì—ì„œ ìµœê³  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

- 2. TFMAdapterëŠ” ë„ë©”ì¸ íŠ¹ì •ì„±ê³¼ ê´€ë ¨ëœ ì¸ì§€ í¸í–¥ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë§ì€ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì •í™•í•œ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ë¯¸ë˜ ê°€ëŠ¥í•œ ì™¸ìƒ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì§€ ëª»í•˜ëŠ” TSFMsë¥¼ ë³´ì™„í•˜ëŠ” ê²½ëŸ‰ì˜ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ì–´ëŒ‘í„°ë¥¼ ì œì•ˆí•œë‹¤.

- 3. TFMAdapterëŠ” TSFMsì— ì™¸ìƒ ë³€ìˆ˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ì„ ì„±ëŠ¥ í–¥ìƒì‹œí‚¤ê³ , ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì—ì„œ 24-27%ì˜ ê°œì„ ì„ ì‹¤í˜„í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.


---

*Generated on 2025-09-18 16:39:24*