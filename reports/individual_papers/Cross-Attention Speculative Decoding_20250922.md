# Cross-Attention Speculative Decoding

**Korean Title:** êµì°¨ ì£¼ì˜ ì¶”ë¡  ë””ì½”ë”©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Cross-attention Speculative Decoding

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (79.7% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop A Novel Regularization Method for Transformer Models]] (79.1% similar)
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (78.9% similar)
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC) A Cognitive-Inspired Approach for Attention Management in Transformers]] (78.8% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (78.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.24544v2 Announce Type: replace-cross 
Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.24544v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì—ì„œ ë„ë¦¬ ì±„íƒëœ ì ‘ê·¼ ë°©ì‹ì¸ ì¶”ì¸¡ ë””ì½”ë”©(SD)ì€ ì´ˆì•ˆ ëª¨ë¸ê³¼ ëª©í‘œ ëª¨ë¸ì´ ì˜ ì •ë ¬ë  ë•Œ íŠ¹íˆ íš¨ê³¼ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœì²¨ë‹¨ SD ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ ë³´ì¡° í’€ë§ ë˜ëŠ” ìœµí•© ë ˆì´ì–´ë¡œ ë³´ê°•ëœ, ë°€ì ‘í•˜ê²Œ ê²°í•©ëœ ìê¸° ì£¼ì˜ ê¸°ë°˜ Transformer ë””ì½”ë”ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°í•©ì€ ëª¨ë¸ ê°„ì˜ ì¼ë°˜í™”ê°€ ì ì  ë” ë³µì¡í•˜ê³  ì–´ë ¤ì›Œì§€ê²Œ ë§Œë“­ë‹ˆë‹¤. ìš°ë¦¬ëŠ”, ìš°ë¦¬ê°€ ì•„ëŠ” í•œ ìµœì´ˆë¡œ, êµì°¨ ì£¼ì˜ ê¸°ë°˜ Transformer ë””ì½”ë” SD ëª¨ë¸ì¸ Budget EAGLE (Beagle)ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” í’€ë§ì´ë‚˜ ë³´ì¡° êµ¬ì„± ìš”ì†Œì˜ í•„ìš”ì„±ì„ ì œê±°í•˜ë©´ì„œ, ì•„í‚¤í…ì²˜ë¥¼ ë‹¨ìˆœí™”í•˜ê³ , í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ë©°, í›ˆë ¨ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ë™ì•ˆ ì•ˆì •ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ ì§€í•˜ë©´ì„œ, ì„ ë„ì ì¸ ìê¸° ì£¼ì˜ SD ëª¨ë¸(EAGLE-v2)ê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì˜ íš¨ê³¼ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¸”ë¡ ìˆ˜ì¤€ ì£¼ì˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì¸ ì´ë‹¨ê³„ ë¸”ë¡ ì£¼ì˜ í›ˆë ¨ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—¬ëŸ¬ LLMê³¼ ë°ì´í„°ì…‹ì— ê±¸ì¹œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ Beagleì´ EAGLE-v2ë³´ë‹¤ ê²½ìŸë ¥ ìˆëŠ” ì¶”ë¡  ì†ë„ í–¥ìƒê³¼ ë” ë†’ì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ì—¬ ì¶”ì¸¡ ë””ì½”ë”© ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ê°•ë ¥í•œ ëŒ€ì•ˆì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Budget EAGLE(Beagle)ì€ ìµœì´ˆë¡œ êµì°¨ ì£¼ì˜ ê¸°ë°˜ì˜ Transformer ë””ì½”ë”ë¥¼ ì‚¬ìš©í•œ ì¶”ë¡  ê°€ì†í™” ëª¨ë¸ë¡œ, ê¸°ì¡´ì˜ ìê¸° ì£¼ì˜ ê¸°ë°˜ ëª¨ë¸(EAGLE-v2)ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. Beagleì€ ë³´ì¡° êµ¬ì„± ìš”ì†Œ ì—†ì´ ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìƒˆë¡œìš´ Two-Stage Block-Attention Training ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì—¬ ë¸”ë¡ ìˆ˜ì¤€ ì£¼ì˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ Beagleì€ EAGLE-v2ë³´ë‹¤ ë†’ì€ í›ˆë ¨ íš¨ìœ¨ì„±ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì¶”ë¡  ì†ë„ë¥¼ ë³´ì—¬, ì¶”ë¡  ê°€ì†í™” ì•„í‚¤í…ì²˜ì˜ ê°•ë ¥í•œ ëŒ€ì•ˆì´ ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Budget EAGLE (Beagle)ëŠ” ìµœì´ˆì˜ êµì°¨ ì£¼ì˜ ê¸°ë°˜ Transformer ë””ì½”ë” ì¶”ë¡  ëª¨ë¸ë¡œ, ê¸°ì¡´ì˜ ìê¸° ì£¼ì˜ ê¸°ë°˜ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ë³´ì¡° êµ¬ì„± ìš”ì†Œ ì—†ì´ ì•„í‚¤í…ì²˜ë¥¼ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.

- 2. Beagleì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë†’ì´ê³  í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ë™ì•ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ì˜ ì•ˆì •ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

- 3. ìƒˆë¡œìš´ Two-Stage Block-Attention Training ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë¸”ë¡ ìˆ˜ì¤€ ì£¼ì˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 4. ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ Beagleì€ EAGLE-v2ë³´ë‹¤ ê²½ìŸë ¥ ìˆëŠ” ì¶”ë¡  ì†ë„ í–¥ìƒê³¼ ë†’ì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:52:44*