
# Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness

**Korean Title:** λ…μ‹μ  μ¶”λ΅ μ€ λ” λ‚μ€ νμ‚¬λ¥Ό λ§λ“ λ‹¤: μ •ν™•μ„±, ν¨μ¨μ„± λ° κ²¬κ³ μ„±μ— λ€ν• μ²΄κ³„μ  μ—°κµ¬

## π“‹ λ©”νƒ€λ°μ΄ν„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Multilingual setting|Multilingual setting]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/RewardBench tasks|RewardBench tasks]] [[keywords/specific/Few-shot Learning|Few-shot Learning]] [[keywords/unique/Qwen 3 models|Qwen 3 models]] [[categories/cs.AI|cs.AI]]

## π·οΈ μΉ΄ν…κ³ λ¦¬ν™”λ ν‚¤μ›λ“
**π€ Evolved Concepts**: Multilingual Setting
**π”¬ Broad Technical**: Large Language Models, RewardBench tasks
**π”— Specific Connectable**: Few-shot Learning
**β­ Unique Technical**: Qwen 3 models

**ArXiv ID**: [2509.13332](https://arxiv.org/abs/2509.13332)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.13332.pdf)


## π·οΈ μ¶”μ¶λ ν‚¤μ›λ“



`Large Language Models` β€Ά 

`RewardBench tasks` β€Ά 

`Few-shot Learning` β€Ά 

`Qwen 3 models` β€Ά 

`Multilingual Setting`



## π“‹ μ €μ μ •λ³΄

**Authors:** 

## π“„ Abstract (μ›λ¬Έ)

arXiv:2509.13332v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges in benchmarking and reward modeling, ensuring their reliability, efficiency, and robustness has become critical. In this work, we present a systematic comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B parameters). We evaluate both accuracy and computational efficiency (FLOPs) on RewardBench tasks, and further examine augmentation strategies for non-thinking models, including in-context learning, rubric-guided judging, reference-based evaluation, and n-best aggregation. Our results show that despite these enhancements, non-thinking models generally fall short of their thinking counterparts. Our results show that thinking models achieve approximately 10% points higher accuracy with little overhead (under 2x), in contrast to augmentation strategies like few-shot learning, which deliver modest gains at a higher cost (>8x). Bias and robustness analyses further demonstrate that thinking models maintain significantly greater consistency under a variety of bias conditions such as positional, bandwagon, identity, diversity, and random biases (6% higher on average). We further extend our experiments to the multilingual setting and our results confirm that explicit reasoning extends its benefits beyond English. Overall, our work results in several important findings that provide systematic evidence that explicit reasoning offers clear advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency but also in robustness.

## π” Abstract (ν•κΈ€ λ²μ—­)

arXiv:2509.13332v1 λ°ν‘ μ ν•: μƒλ΅μ΄
μ”μ•½: λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ (LLMs)μ΄ λ²¤μΉλ§ν‚Ή λ° λ³΄μƒ λ¨λΈλ§μ—μ„ μλ™ μ‹¬μ‚¬κ΄€μΌλ΅ μ±„νƒλλ©΄μ„, κ·Έλ“¤μ μ‹ λΆ°μ„±, ν¨μ¨μ„± λ° κ²¬κ³ μ„±μ„ λ³΄μ¥ν•λ” κ²ƒμ΄ μ¤‘μ”ν•΄μ΅μµλ‹λ‹¤. λ³Έ μ—°κµ¬μ—μ„λ” μƒλ€μ μΌλ΅ μ‘μ€ ν¬κΈ°μ μ¤ν” μ†μ¤ Qwen 3 λ¨λΈ (0.6B, 1.7B λ° 4B λ§¤κ°λ³€μ)λ¥Ό μ‚¬μ©ν•μ—¬ LLM-μ‹¬μ‚¬κ΄€ ν¨λ¬λ‹¤μ„μ—μ„ "μ‚¬κ³ " λ° "λΉ„μ‚¬κ³ " LLMμ„ μ²΄κ³„μ μΌλ΅ λΉ„κµν•©λ‹λ‹¤. RewardBench μ‘μ—…μ—μ„ μ •ν™•λ„μ™€ κ³„μ‚° ν¨μ¨μ„± (FLOPs)μ„ ν‰κ°€ν•κ³ , λΉ„μ‚¬κ³  λ¨λΈμ— λ€ν• μ¦κ°• μ „λµμ„ λ” μμ„Έν μ΅°μ‚¬ν•©λ‹λ‹¤. μ΄λ¬ν• μ¦κ°• μ „λµμ—λ” λ¬Έλ§¥ λ‚΄ ν•™μµ, λ£¨λΈλ¦­ μ•λ‚΄ μ‹¬μ‚¬, μ°Έμ΅° κΈ°λ° ν‰κ°€ λ° n-best μ§‘κ³„κ°€ ν¬ν•¨λ©λ‹λ‹¤. κ²°κ³Όλ” μ΄λ¬ν• ν–¥μƒμ—λ„ λ¶κµ¬ν•κ³ , λΉ„μ‚¬κ³  λ¨λΈμ΄ μΌλ°μ μΌλ΅ μ‚¬κ³  λ¨λΈμ— λΉ„ν•΄ μ„±κ³Όκ°€ λ–¨μ–΄μ§„λ‹¤λ” κ²ƒμ„ λ³΄μ—¬μ¤λ‹λ‹¤. κ²°κ³Όλ” μ‚¬κ³  λ¨λΈμ΄ μ†κ·λ¨μ μ¶”κ°€ λ¶€λ‹΄ μ—†μ΄ (2λ°° λ―Έλ§) μ•½ 10% ν¬μΈνΈ λ” λ†’μ€ μ •ν™•λ„λ¥Ό λ‹¬μ„±ν•λ” λ°λ©΄, μ†μ ν•™μµκ³Ό κ°™μ€ μ¦κ°• μ „λµμ€ λ†’μ€ λΉ„μ©μΌλ΅ μ΅°κΈμ”© μ„±κ³Όλ¥Ό λμ–΄λ‚΄λ” κ²ƒμ„ ν™•μΈν•©λ‹λ‹¤. νΈν–¥ λ° κ²¬κ³ μ„± λ¶„μ„μ€ λν• μ‚¬κ³  λ¨λΈμ΄ μ„μΉ, λ¬΄λ¦¬ λ”°λ¦„, μ •μ²΄μ„±, λ‹¤μ–‘μ„± λ° λ¬΄μ‘μ„ νΈν–¥κ³Ό κ°™μ€ λ‹¤μ–‘ν• νΈν–¥ μ΅°κ±΄ ν•μ—μ„ μƒλ‹Ήν λ” ν° μΌκ΄€μ„±μ„ μ μ§€ν•λ‹¤λ” κ²ƒμ„ λ³΄μ—¬μ¤λ‹λ‹¤ (ν‰κ·  6% λ” λ†’μ). μ°λ¦¬λ” λ‹¤κµ­μ–΄ ν™κ²½μΌλ΅ μ‹¤ν—μ„ ν™•μ¥ν•κ³  κ²°κ³Όλ” λ…μ‹μ  μ¶”λ΅ μ΄ μμ–΄ μ΄μƒμ μ΄μ μ„ μ κ³µν•λ‹¤λ” κ²ƒμ„ ν™•μΈν•©λ‹λ‹¤. μ „λ°μ μΌλ΅, μ°λ¦¬μ μ—°κµ¬λ” λ…μ‹μ  μ¶”λ΅ μ΄ LLM-μ‹¬μ‚¬κ΄€ ν¨λ¬λ‹¤μ„μ—μ„ μ •ν™•μ„±κ³Ό ν¨μ¨μ„±λΏλ§ μ•„λ‹λΌ κ²¬κ³ μ„±μ—μ„λ„ λ…ν™•ν• μ¥μ μ„ μ κ³µν•λ‹¤λ” μ²΄κ³„μ μΈ μ¦κ±°λ¥Ό μ κ³µν•©λ‹λ‹¤.

## π“ μ”μ•½

μ΄ μ—°κµ¬λ” λ€ν• μ–Έμ–΄ λ¨λΈμ΄ λ²¤μΉλ§ν‚Ή λ° λ³΄μƒ λ¨λΈλ§μ—μ„ μλ™ μ‹¬μ‚¬κ΄€μΌλ΅ μ±„νƒλ¨μ— λ”°λΌ κ·Έ μ‹ λΆ°μ„±, ν¨μ¨μ„± λ° κ²¬κ³ μ„±μ„ λ³΄μ¥ν•λ” κ²ƒμ΄ μ¤‘μ”ν•΄μ΅λ‹¤. μ‘μ€ ν¬κΈ°μ μ¤ν” μ†μ¤ Qwen 3 λ¨λΈ(0.6B, 1.7B λ° 4B λ§¤κ°λ³€μ)μ„ μ‚¬μ©ν•μ—¬ "μ‚¬κ³ " λ° "λΉ„μ‚¬κ³ " λ€ν• μ–Έμ–΄ λ¨λΈμ„ LLM-μ‹¬μ‚¬κ΄€ ν¨λ¬λ‹¤μ„μ—μ„ μ²΄κ³„μ μΌλ΅ λΉ„κµν•λ‹¤. RewardBench μ‘μ—…μ—μ„ μ •ν™•λ„μ™€ κ³„μ‚° ν¨μ¨μ„±(FLOPs)μ„ ν‰κ°€ν•κ³ , λΉ„μ‚¬κ³  λ¨λΈμ— λ€ν• μ¦κ°• μ „λµμ„ κ²€ν† ν•λ‹¤. κ²°κ³Όλ” μ‚¬κ³  λ¨λΈμ΄ λΉ„μ‚¬κ³  λ¨λΈλ³΄λ‹¤ μ•½ 10% ν¬μΈνΈ λ†’μ€ μ •ν™•λ„λ¥Ό λ‹¬μ„±ν•λ©° μ μ€ μ¤λ²„ν—¤λ“λ΅(2λ°° λ―Έλ§) μ μ§€λλ” κ²ƒμ„ λ³΄μ—¬μ¤€λ‹¤. λ‡λ‡ μƒ· ν•™μµκ³Ό κ°™μ€ μ¦κ°• μ „λµμ€ λ†’μ€ λΉ„μ©(8λ°° μ΄μƒ)μ—λ„ λ―Έλ―Έν• μ΄λ“μ„ μ κ³µν•λ‹¤. μ‚¬κ³  λ¨λΈμ€ λ‹¤μ–‘ν• νΈν–¥ μ΅°κ±΄μ—μ„ λ” ν° μΌκ΄€μ„±μ„ μ μ§€ν•λ‹¤. λ…μ‹μ  μ¶”λ΅ μ€ μμ–΄ μ΄μ™Έμ λ‹¤κµ­μ–΄ ν™κ²½μ—μ„λ„ μ΄μ μ„ μ κ³µν•¨μ„ ν™•μΈν•λ‹¤. μ΄ μ—°κµ¬λ” λ…μ‹μ  μ¶”λ΅ μ΄ LLM-μ‹¬μ‚¬κ΄€ ν¨λ¬λ‹¤μ„μ—μ„ μ •ν™•μ„±, ν¨μ¨μ„± λ° κ²¬κ³ μ„± μΈ΅λ©΄μ—μ„ λ…ν™•ν• μ¥μ μ„ μ κ³µν•¨μ„ μ‹μ¤ν…μ μΌλ΅ μ…μ¦ν•λ” μ¤‘μ”ν• λ°κ²¬μ„ μ κ³µν•λ‹¤.

## π― μ£Όμ” ν¬μΈνΈ


- 1. λ€ν• μ–Έμ–΄ λ¨λΈμ„ νλ‹¨μλ΅ μ‚¬μ©ν•  λ• λ…μ‹μ  μ¶”λ΅ μ΄ μ •ν™•μ„±, ν¨μ¨μ„± λ° κ²¬κ³ μ„±μ—μ„ μ λ¦¬ν•¨μ„ μ…μ¦

- 2. λΉ„νμ  μ‚¬κ³  λ¨λΈμ€ λΉ„νμ μ΄μ§€ μ•μ€ λ¨λΈλ³΄λ‹¤ μ•½ 10% λ†’μ€ μ •ν™•λ„λ¥Ό λ³΄μ„

- 3. λ…μ‹μ  μ¶”λ΅ μ€ λ‹¤μ–‘ν• νΈν–¥ μ΅°κ±΄μ—μ„ λ” ν° μΌκ΄€μ„±μ„ μ μ§€ν•λ©° λ‹¤κµ­μ–΄ μ„¤μ •μ—μ„λ„ ν¨κ³Όμ μΌλ΅ μ‘λ™ν•¨.


---

*Generated on 2025-09-18 16:13:13*