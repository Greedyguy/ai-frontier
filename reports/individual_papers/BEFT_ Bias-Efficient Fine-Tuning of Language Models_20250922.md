# BEFT: Bias-Efficient Fine-Tuning of Language Models

**Korean Title:** BEFT: ì–¸ì–´ ëª¨ë¸ì˜ í¸í–¥ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Parameter Efficient Fine Tuning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (82.7% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (82.3% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (81.0% similar)
- [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (81.0% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (80.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15974v1 Announce Type: cross 
Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and competitive performance, especially in low-data regimes. Bias-only fine-tuning has the potential for unprecedented parameter efficiency. However, the link between fine-tuning different bias terms (i.e., bias terms in the query, key, or value projections) and downstream performance remains unclear. The existing approaches, e.g., based on the magnitude of bias change or empirical Fisher information, provide limited guidance for selecting the particular bias term for effective fine-tuning. In this paper, we propose an approach for selecting the bias term to be fine-tuned, forming the foundation of our bias-efficient fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against other bias-selection approaches, across a wide range of large language models (LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B parameters. Our results demonstrate the effectiveness and superiority of our bias-efficient approach on diverse downstream tasks, including classification, multiple-choice, and generation tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15974v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëª¨ë“  ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ê¸°ë²• ì¤‘ì—ì„œ íŠ¹íˆ ì €ë°ì´í„° í™˜ê²½ì—ì„œì˜ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ì„±ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ ë•ë¶„ì— ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ë°”ì´ì–´ìŠ¤ ì „ìš© ë¯¸ì„¸ ì¡°ì •ì€ ì „ë¡€ ì—†ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ì œê³µí•  ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì„œë¡œ ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ í•­ëª©(ì¦‰, ì¿¼ë¦¬, í‚¤, ë˜ëŠ” ê°’ íˆ¬ì˜ì˜ ë°”ì´ì–´ìŠ¤ í•­ëª©)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ ê°„ì˜ ì—°ê²°ì€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°”ì´ì–´ìŠ¤ ë³€í™”ì˜ í¬ê¸°ë‚˜ ê²½í—˜ì  í”¼ì…” ì •ë³´ì— ê¸°ë°˜í•œ ê¸°ì¡´ ì ‘ê·¼ë²•ì€ íš¨ê³¼ì ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ íŠ¹ì • ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ë° ì œí•œëœ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¯¸ì„¸ ì¡°ì •í•  ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ìš°ë¦¬ì˜ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(BEFT)ì˜ ê¸°ì´ˆë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¸ì½”ë” ì „ìš© ë° ë””ì½”ë” ì „ìš© ì•„í‚¤í…ì²˜ë¥¼ í¬í•¨í•˜ì—¬ 110Mì—ì„œ 6.7B íŒŒë¼ë¯¸í„°ì— ì´ë¥´ëŠ” ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ëŒ€ìƒìœ¼ë¡œ ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ ì„ íƒ ì ‘ê·¼ë²•ê³¼ ë¹„êµí•˜ì—¬ ìš°ë¦¬ì˜ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ì ‘ê·¼ë²•ì„ ê´‘ë²”ìœ„í•˜ê²Œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ë¶„ë¥˜, ë‹¤ì¤‘ ì„ íƒ, ìƒì„± ì‘ì—…ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ìš°ë¦¬ì˜ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ì ‘ê·¼ë²•ì˜ íš¨ê³¼ì„±ê³¼ ìš°ìˆ˜ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì •(PEFT) ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ ëª¨ë“  ë°”ì´ì–´ìŠ¤ í•­ëª©ì˜ ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, ë°ì´í„°ê°€ ì ì€ í™˜ê²½ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°, ë°”ì´ì–´ìŠ¤ í•­ëª©ë§Œì„ ì¡°ì •í•˜ëŠ” ê²ƒì´ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•  ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¿¼ë¦¬, í‚¤, ê°’ í”„ë¡œì ì…˜ì˜ ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì‹¤ì œ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ëª…í™•í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ë²•ì€ íŠ¹ì • ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ë° ì œí•œì ì¸ ì§€ì¹¨ë§Œ ì œê³µí–ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë°”ì´ì–´ìŠ¤ í•­ëª© ì„ íƒì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(BEFT)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ë¶„ë¥˜, ë‹¤ì¤‘ ì„ íƒ, ìƒì„± ì‘ì—… ë“± ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ íš¨ê³¼ì ì´ê³  ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëª¨ë“  ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì€ ë‚®ì€ ë°ì´í„° í™˜ê²½ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

- 2. ë°”ì´ì–´ìŠ¤ í•­ëª©ë§Œì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì€ ì „ë¡€ ì—†ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ì œê³µí•  ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” íš¨ê³¼ì ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ íŠ¹ì • ë°”ì´ì–´ìŠ¤ í•­ëª© ì„ íƒì— ëŒ€í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 4. ì œì•ˆëœ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(BEFT) ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ ì„ íƒ ì ‘ê·¼ ë°©ì‹ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì…ì¦í•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ë¶„ë¥˜, ë‹¤ì¤‘ ì„ íƒ, ìƒì„± ì‘ì—…ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ íš¨ê³¼ì ì´ê³  ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:20:17*