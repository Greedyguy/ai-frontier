# VLA-Mark: A cross modal watermark for large vision-language alignment model

**Korean Title:** VLA-Mark: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ì •ë ¬ ëª¨ë¸ì„ ìœ„í•œ êµì°¨ ëª¨ë‹¬ ì›Œí„°ë§ˆí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Quality Preserving Multimodal Watermarking

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition A Defense Against Adversarial Attacks]] (83.9% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (83.8% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (83.2% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (82.3% similar)
- [[2025-09-19/Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models_20250919|Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models]] (81.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.14067v2 Announce Type: replace-cross 
Abstract: Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.14067v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ì¼ê´€ì„±ì„ ì†ìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ì§€ì  ì¬ì‚°ì„ ë³´í˜¸í•  ìˆ˜ ìˆëŠ” ì›Œí„°ë§ˆí‚¹ ì†”ë£¨ì…˜ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í…ìŠ¤íŠ¸ ì›Œí„°ë§ˆí‚¹ ë°©ë²•ì€ í¸í–¥ëœ í† í° ì„ íƒê³¼ ì •ì  ì „ëµì„ í†µí•´ ì‹œê°-í…ìŠ¤íŠ¸ ì •ë ¬ì„ ë°©í•´í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ì¤‘ìš”í•œ ê°œë…ì„ ì·¨ì•½í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì˜ë¯¸ì  ì¶©ì‹¤ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê²€ì¶œ ê°€ëŠ¥í•œ ì›Œí„°ë§ˆí¬ë¥¼ ì‚½ì…í•˜ëŠ” ë¹„ì „ ì •ë ¬ í”„ë ˆì„ì›Œí¬ì¸ VLA-Markë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ ì¬í›ˆë ¨ ì—†ì´ ì›Œí„°ë§ˆí¬ ì‚½ì…ì„ ì•ˆë‚´í•˜ê¸° ìœ„í•´ ì§€ì—­í™”ëœ íŒ¨ì¹˜ ì¹œí™”ì„±, ì „ì—­ ì˜ë¯¸ì  ì¼ê´€ì„± ë° ë§¥ë½ì  ì£¼ì˜ íŒ¨í„´ì„ ê²°í•©í•œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹œê°-í…ìŠ¤íŠ¸ ì •ë ¬ ë©”íŠ¸ë¦­ì„ í†µí•©í•©ë‹ˆë‹¤. ì—”íŠ¸ë¡œí”¼ ë¯¼ê° ë©”ì»¤ë‹ˆì¦˜ì€ ì›Œí„°ë§ˆí¬ ê°•ë„ì™€ ì˜ë¯¸ ë³´ì¡´ì„ ë™ì ìœ¼ë¡œ ê· í˜• ì¡ì•„, ë¶ˆí™•ì‹¤ì„±ì´ ë‚®ì€ ìƒì„± ë‹¨ê³„ì—ì„œ ì‹œê°ì  ê¸°ë°˜ì„ ìš°ì„ ì‹œí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ 7.4% ë‚®ì€ PPLê³¼ 26.6% ë†’ì€ BLEUë¥¼ ë³´ì—¬ì£¼ë©°, ê±°ì˜ ì™„ë²½í•œ ê²€ì¶œ(98.8% AUC)ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” íŒ¨ëŸ¬í”„ë ˆì´ì§• ë° ë™ì˜ì–´ ëŒ€ì²´ì™€ ê°™ì€ ê³µê²©ì— ëŒ€í•´ 96.1%ì˜ ê³µê²© ì €í•­ì„±ì„ ë³´ì—¬ì£¼ë©´ì„œ í…ìŠ¤íŠ¸-ì‹œê° ì¼ê´€ì„±ì„ ìœ ì§€í•˜ì—¬ í’ˆì§ˆ ë³´ì¡´ ë‹¤ì¤‘ ëª¨ë‹¬ ì›Œí„°ë§ˆí‚¹ì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì„ í™•ë¦½í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì  ì¬ì‚°ì„ ë³´í˜¸í•˜ë©´ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ëŠ” ì›Œí„°ë§ˆí‚¹ ì†”ë£¨ì…˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í…ìŠ¤íŠ¸ ì›Œí„°ë§ˆí‚¹ ë°©ë²•ì€ ì‹œê°-í…ìŠ¤íŠ¸ ì •ë ¬ì„ ë°©í•´í•˜ì§€ë§Œ, VLA-MarkëŠ” ì‹œê°ì  ì •ë ¬ì„ ìœ ì§€í•˜ë©´ì„œ ì›Œí„°ë§ˆí¬ë¥¼ ì‚½ì…í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ì˜ ì‹œê°-í…ìŠ¤íŠ¸ ì •ë ¬ ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ëª¨ë¸ ì¬í›ˆë ¨ ì—†ì´ ì›Œí„°ë§ˆí¬ë¥¼ ì‚½ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ 7.4% ë‚®ì€ PPLê³¼ 26.6% ë†’ì€ BLEU ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, 98.8%ì˜ AUCë¡œ ê±°ì˜ ì™„ë²½í•œ íƒì§€ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, 96.1%ì˜ ê³µê²© ì €í•­ì„±ì„ ë‚˜íƒ€ë‚´ë©° í…ìŠ¤íŠ¸-ë¹„ì£¼ì–¼ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ë¡œì¨ í’ˆì§ˆì„ ë³´ì¡´í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì›Œí„°ë§ˆí‚¹ì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì„¸ì› ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VLA-MarkëŠ” ì‹œê°ì -ì–¸ì–´ì  ì¼ì¹˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê°ì§€ ê°€ëŠ¥í•œ ì›Œí„°ë§ˆí¬ë¥¼ ì‚½ì…í•˜ëŠ” ë¹„ì „ ì •ë ¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. ì´ ì ‘ê·¼ë²•ì€ ì§€ì—­ì  íŒ¨ì¹˜ ì¹œí™”ì„±, ì „ì—­ì  ì˜ë¯¸ ì¼ê´€ì„±, ë§¥ë½ì  ì£¼ì˜ íŒ¨í„´ì„ ê²°í•©í•˜ì—¬ ì›Œí„°ë§ˆí¬ ì£¼ì…ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

- 3. ì—”íŠ¸ë¡œí”¼ ë¯¼ê° ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì›Œí„°ë§ˆí¬ ê°•ë„ì™€ ì˜ë¯¸ ë³´ì¡´ì„ ë™ì ìœ¼ë¡œ ê· í˜• ì¡ì•„, ë¶ˆí™•ì‹¤ì„±ì´ ë‚®ì€ ìƒì„± ë‹¨ê³„ì—ì„œ ì‹œê°ì  ê¸°ë°˜ì„ ìš°ì„ ì‹œí•©ë‹ˆë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ 7.4% ë‚®ì€ PPLê³¼ 26.6% ë†’ì€ BLEUë¥¼ ê¸°ë¡í•˜ë©°, ê±°ì˜ ì™„ë²½í•œ ê°ì§€ìœ¨(98.8% AUC)ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

- 5. í”„ë ˆì„ì›Œí¬ëŠ” íŒ¨ëŸ¬í”„ë ˆì´ì§• ë° ë™ì˜ì–´ ëŒ€ì²´ì™€ ê°™ì€ ê³µê²©ì— ëŒ€í•´ 96.1%ì˜ ê³µê²© ì €í•­ì„±ì„ ë³´ì´ë©°, í…ìŠ¤íŠ¸-ë¹„ì£¼ì–¼ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:57:21*