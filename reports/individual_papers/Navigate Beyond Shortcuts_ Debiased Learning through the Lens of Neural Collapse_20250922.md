# Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse

**Korean Title:** ì§€ë¦„ê¸¸ì„ ë„˜ì–´ íƒìƒ‰í•˜ê¸°: ì‹ ê²½ ë¶•ê´´ì˜ ê´€ì ì—ì„œ ë³¸ í¸í–¥ ì œê±° í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Neural Collapse|Neural Collapse]] [[keywords/specific/Biased Classification|Biased Classification]] [[keywords/broad/Neural Networks|Neural Networks]] [[keywords/unique/Avoid-Shortcut Learning Framework|Avoid-Shortcut Learning Framework]] [[categories/cs.LG|cs.LG]] [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.2% similar) [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar) [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Neural Collapse
**ğŸ”— Specific Connectable**: Shortcut Learning
**ğŸ”¬ Broad Technical**: Neural Networks
**â­ Unique Technical**: Avoid-Shortcut Learning Framework
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.2% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.3% similar)
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (81.0% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (80.1% similar)


**ArXiv ID**: [2405.05587](https://arxiv.org/abs/2405.05587)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2405.05587.pdf)


**ArXiv ID**: [2405.05587](https://arxiv.org/abs/2405.05587)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2405.05587.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Neural Collapse
**ğŸ”— Specific Connectable**: Debiased Learning
**â­ Unique Technical**: Avoid-Shortcut Learning Framework
**ğŸ”¬ Broad Technical**: Neural Networks

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Neural Networks` â€¢ 

`Shortcut Learning` â€¢ 

`Avoid-Shortcut Learning Framework` â€¢ 

`Neural Collapse`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2405.05587v2 Announce Type: replace-cross 
Abstract: Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure. In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes. We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability. To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity. With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations. Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets. Our code is available at https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/main.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2405.05587v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ì‹ ê²½ ë¶•ê´´(Neural Collapse)ë¼ëŠ” í¥ë¯¸ë¡œìš´ í˜„ìƒì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‹ ê²½ë§ì´ íŠ¹ì§• ê³µê°„ê³¼ í•™ìŠµ ëª©í‘œ ê°„ì˜ ì˜¬ë°”ë¥¸ ìƒê´€ê´€ê³„ë¥¼ ì„¤ì •í•  ë•Œ, ë§ˆì§€ë§‰ ì¸µì˜ íŠ¹ì§•ê³¼ ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ê°€ ì•ˆì •ì ì´ê³  ëŒ€ì¹­ì ì¸ êµ¬ì¡°ë¡œ ë¶•ê´´í•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¶ˆê· í˜•í•œ ì†ì„±ì„ ê°€ì§„ í¸í–¥ëœ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì‹ ê²½ ë¶•ê´´ ì—°êµ¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëª¨ë¸ì´ í•™ìŠµ ì´ˆê¸°ì— ì‰½ê²Œ ì§€ë¦„ê¸¸ í•™ìŠµì˜ í•¨ì •ì— ë¹ ì ¸ í¸í–¥ë˜ê³  ë¶•ê´´ë˜ì§€ ì•Šì€ íŠ¹ì§• ê³µê°„ì„ í˜•ì„±í•˜ê²Œ ë˜ë©°, ì´ëŠ” ë˜ëŒë¦¬ê¸° ì–´ë µê³  ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì œí•œí•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. í¸í–¥ëœ ë¶„ë¥˜ì˜ ê·¼ë³¸ ì›ì¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìµœê·¼ì˜ ì£¼ìš” í•™ìŠµ ì˜ê°ì„ ë”°ë¥´ê³  ì¶”ê°€ì ì¸ í•™ìŠµ ë³µì¡ì„± ì—†ì´ ì§€ë¦„ê¸¸ íšŒí”¼ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹ ê²½ ë¶•ê´´ êµ¬ì¡°ì— ê¸°ë°˜í•œ ì˜ ì„¤ê³„ëœ ì§€ë¦„ê¸¸ í”„ë¼ì„ì„ í†µí•´, ëª¨ë¸ì€ ë‹¨ìˆœí•œ ì§€ë¦„ê¸¸ ì¶”êµ¬ë¥¼ í”¼í•˜ê³  ë³¸ì§ˆì ì¸ ìƒê´€ê´€ê³„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬ì°©í•˜ë„ë¡ ìœ ë„ë©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ í•™ìŠµ ì¤‘ ë” ë‚˜ì€ ìˆ˜ë ´ íŠ¹ì„±ì„ ìœ ë„í•˜ë©°, í•©ì„± ë° ì‹¤ì œ í¸í–¥ëœ ë°ì´í„°ì…‹ ëª¨ë‘ì—ì„œ ìµœì²¨ë‹¨ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/mainì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Neural Collapse í˜„ìƒì„ ë¶ˆê· í˜• ì†ì„±ì„ ê°€ì§„ í¸í–¥ëœ ë°ì´í„°ì…‹ì— ì ìš©í•˜ì—¬ ì—°êµ¬ë¥¼ í™•ì¥í–ˆìŠµë‹ˆë‹¤. ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ì—ì„œ ëª¨ë¸ì´ í¸í–¥ëœ ë¹„ìˆ˜ë ´ íŠ¹ì§• ê³µê°„ì„ í˜•ì„±í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì¶”ê°€ì ì¸ í•™ìŠµ ë³µì¡ë„ ì—†ì´ 'ì§€ë¦„ê¸¸ í•™ìŠµ íšŒí”¼ í”„ë ˆì„ì›Œí¬'ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. Neural Collapse êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ 'ì§€ë¦„ê¸¸ í”„ë¼ì„'ì„ í†µí•´ ëª¨ë¸ì´ ë‹¨ìˆœí•œ ì§€ë¦„ê¸¸ì„ í”¼í•˜ê³  ë³¸ì§ˆì ì¸ ìƒê´€ê´€ê³„ë¥¼ í¬ì°©í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ í•™ìŠµ ì¤‘ ë” ë‚˜ì€ ìˆ˜ë ´ íŠ¹ì„±ì„ ìœ ë„í•˜ê³ , í¸í–¥ëœ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Neural Collapse í˜„ìƒì€ ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ì¸µ íŠ¹ì§•ê³¼ ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ê°€ ì•ˆì •ì ì´ê³  ëŒ€ì¹­ì ì¸ êµ¬ì¡°ë¡œ ìˆ˜ë ´í•˜ëŠ” í˜„ìƒì´ë‹¤.

- 2. í¸í–¥ëœ ì†ì„±ì„ ê°€ì§„ ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ Neural Collapseë¥¼ ì¡°ì‚¬í•œ ê²°ê³¼, ëª¨ë¸ì´ ì´ˆê¸°ì— í¸í–¥ëœ ë¹„ìˆ˜ë ´ íŠ¹ì§• ê³µê°„ì„ í˜•ì„±í•˜ì—¬ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì œí•œí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

- 3. í¸í–¥ëœ ë¶„ë¥˜ì˜ ê·¼ë³¸ ì›ì¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ì¶”ê°€ì ì¸ í›ˆë ¨ ë³µì¡ì„± ì—†ì´ ì§€ë¦„ê¸¸ í•™ìŠµì„ í”¼í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí–ˆë‹¤.

- 4. Neural Collapse êµ¬ì¡°ì— ê¸°ë°˜í•œ ì§€ë¦„ê¸¸ í”„ë¼ì„ì„ í†µí•´ ëª¨ë¸ì´ ë‹¨ìˆœí•œ ì§€ë¦„ê¸¸ì„ í”¼í•˜ê³  ë³¸ì§ˆì ì¸ ìƒê´€ê´€ê³„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬ì°©í•˜ë„ë¡ ìœ ë„í–ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ í›ˆë ¨ ì¤‘ ë” ë‚˜ì€ ìˆ˜ë ´ íŠ¹ì„±ì„ ìœ ë„í•˜ë©°, í•©ì„± ë° ì‹¤ì œ í¸í–¥ ë°ì´í„°ì…‹ ëª¨ë‘ì—ì„œ ìµœì²¨ë‹¨ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.


---

*Generated on 2025-09-22 16:05:54*