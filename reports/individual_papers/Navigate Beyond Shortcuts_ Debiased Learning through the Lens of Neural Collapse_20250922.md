# Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse

**Korean Title:** 지름길을 넘어 탐색하기: 신경 붕괴의 관점에서 본 편향 제거 학습

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Neural Collapse|Neural Collapse]] [[keywords/specific/Biased Classification|Biased Classification]] [[keywords/broad/Neural Networks|Neural Networks]] [[keywords/unique/Avoid-Shortcut Learning Framework|Avoid-Shortcut Learning Framework]] [[categories/cs.LG|cs.LG]] [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.2% similar) [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar) [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.3% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Neural Collapse
**🔗 Specific Connectable**: Shortcut Learning
**🔬 Broad Technical**: Neural Networks
**⭐ Unique Technical**: Avoid-Shortcut Learning Framework
## 🔗 유사한 논문
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.2% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.3% similar)
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (81.0% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (80.1% similar)


**ArXiv ID**: [2405.05587](https://arxiv.org/abs/2405.05587)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2405.05587.pdf)


**ArXiv ID**: [2405.05587](https://arxiv.org/abs/2405.05587)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2405.05587.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Neural Collapse
**🔗 Specific Connectable**: Debiased Learning
**⭐ Unique Technical**: Avoid-Shortcut Learning Framework
**🔬 Broad Technical**: Neural Networks

## 🏷️ 추출된 키워드



`Neural Networks` • 

`Shortcut Learning` • 

`Avoid-Shortcut Learning Framework` • 

`Neural Collapse`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2405.05587v2 Announce Type: replace-cross 
Abstract: Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure. In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes. We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability. To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity. With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations. Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets. Our code is available at https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/main.

## 🔍 Abstract (한글 번역)

arXiv:2405.05587v2 발표 유형: 교차 교체  
초록: 최근 연구에서는 신경 붕괴(Neural Collapse)라는 흥미로운 현상이 주목받고 있습니다. 이는 신경망이 특징 공간과 학습 목표 간의 올바른 상관관계를 설정할 때, 마지막 층의 특징과 분류기 가중치가 안정적이고 대칭적인 구조로 붕괴하는 현상입니다. 본 논문에서는 불균형한 속성을 가진 편향된 데이터셋에 대해 신경 붕괴 연구를 확장합니다. 우리는 모델이 학습 초기에 쉽게 지름길 학습의 함정에 빠져 편향되고 붕괴되지 않은 특징 공간을 형성하게 되며, 이는 되돌리기 어렵고 일반화 능력을 제한한다는 것을 관찰했습니다. 편향된 분류의 근본 원인을 해결하기 위해, 우리는 최근의 주요 학습 영감을 따르고 추가적인 학습 복잡성 없이 지름길 회피 학습 프레임워크를 제안합니다. 신경 붕괴 구조에 기반한 잘 설계된 지름길 프라임을 통해, 모델은 단순한 지름길 추구를 피하고 본질적인 상관관계를 자연스럽게 포착하도록 유도됩니다. 실험 결과는 우리의 방법이 학습 중 더 나은 수렴 특성을 유도하며, 합성 및 실제 편향된 데이터셋 모두에서 최첨단의 일반화 성능을 달성함을 보여줍니다. 우리의 코드는 https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/main에서 확인할 수 있습니다.

## 📝 요약

이 논문은 Neural Collapse 현상을 불균형 속성을 가진 편향된 데이터셋에 적용하여 연구를 확장했습니다. 초기 학습 단계에서 모델이 편향된 비수렴 특징 공간을 형성하는 문제를 해결하기 위해, 추가적인 학습 복잡도 없이 '지름길 학습 회피 프레임워크'를 제안합니다. Neural Collapse 구조를 기반으로 한 '지름길 프라임'을 통해 모델이 단순한 지름길을 피하고 본질적인 상관관계를 포착하도록 유도합니다. 실험 결과, 제안된 방법이 학습 중 더 나은 수렴 특성을 유도하고, 편향된 데이터셋에서 최첨단 일반화 성능을 달성함을 보여줍니다.

## 🎯 주요 포인트


- 1. Neural Collapse 현상은 신경망의 마지막 층 특징과 분류기 가중치가 안정적이고 대칭적인 구조로 수렴하는 현상이다.

- 2. 편향된 속성을 가진 불균형 데이터셋에서 Neural Collapse를 조사한 결과, 모델이 초기에 편향된 비수렴 특징 공간을 형성하여 일반화 능력을 제한한다는 것을 발견했다.

- 3. 편향된 분류의 근본 원인을 해결하기 위해, 추가적인 훈련 복잡성 없이 지름길 학습을 피하는 프레임워크를 제안했다.

- 4. Neural Collapse 구조에 기반한 지름길 프라임을 통해 모델이 단순한 지름길을 피하고 본질적인 상관관계를 자연스럽게 포착하도록 유도했다.

- 5. 제안된 방법은 훈련 중 더 나은 수렴 특성을 유도하며, 합성 및 실제 편향 데이터셋 모두에서 최첨단 일반화 성능을 달성했다.


---

*Generated on 2025-09-22 16:05:54*