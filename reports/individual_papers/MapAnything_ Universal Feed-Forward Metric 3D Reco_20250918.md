
# MapAnything: Universal Feed-Forward Metric 3D Reconstruction

**Korean Title:** MapAnything: 유니버설 피드-포워드 메트릭 3D 재구성

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Calibrated Multi-view Stereo|Calibrated Multi-view Stereo]] [[keywords/broad/Transformer|Transformer]] [[keywords/broad/Computer Vision|Computer Vision]] [[keywords/specific/Multi-view Scene Geometry|Multi-view Scene Geometry]] [[keywords/unique/MapAnything|MapAnything]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Calibrated Multi-view Stereo
**🔬 Broad Technical**: Transformer, 3D Vision
**🔗 Specific Connectable**: Multi-view Scene Geometry
**⭐ Unique Technical**: MapAnything

**ArXiv ID**: [2509.13414](https://arxiv.org/abs/2509.13414)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.13414.pdf)


## 🏷️ 추출된 키워드



`Transformer` • 

`3D Vision` • 

`Multi-view Scene Geometry` • 

`MapAnything` • 

`Calibrated Multi-view Stereo`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13414v1 Announce Type: cross 
Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.

## 🔍 Abstract (한글 번역)

arXiv:2509.13414v1 발표 유형: 교차
요약: 우리는 MapAnything이라는 통합된 변환기 기반 피드 포워드 모델을 소개합니다. 이 모델은 하나 이상의 이미지와 선택적인 기하학적 입력(카메라 내부 파라미터, 포즈, 깊이 또는 부분 재구성)을 흡수한 후, 측정 3D 장면 지오메트리와 카메라를 직접 회귀합니다. MapAnything은 다중 뷰 장면 지오메트리의 요소화된 표현을 활용합니다. 즉, 깊이 맵, 지역 레이 맵, 카메라 포즈 및 메트릭 스케일 요소의 모음으로 지역 재구성을 전역적으로 일관된 메트릭 프레임으로 업그레이드합니다. 다양한 데이터셋 간의 감독 및 훈련을 표준화하고 유연한 입력 증강을 통해 MapAnything은 단일 피드 포워드 패스에서 미교정 구조-움직임, 보정된 다중 뷰 스테레오, 단안 깊이 추정, 카메라 위치 결정, 깊이 완성 등 다양한 3D 비전 작업을 처리할 수 있습니다. MapAnything이 전문적인 피드 포워드 모델을 능가하거나 맞먹는 것을 증명하는 방대한 실험적 분석과 모델 제거를 제공하며, 더 효율적인 공동 훈련 행동을 제공하여 보다 효율적인 3D 재구성 백본으로 나아가는 길을 열어놓습니다.

## 📝 요약

MapAnything은 이미지 및 카메라 내부 파라미터, 포즈, 깊이 등의 기하학적 입력을 활용하여 메트릭 3D 장면 지오메트리와 카메라를 직접 회귀하는 통합 트랜스포머 기반의 전방향 모델을 제안한다. MapAnything은 다양한 데이터셋에 대한 표준화된 지도 및 훈련을 통해 다양한 3D 비전 작업을 처리할 수 있으며, 전문가용 전방향 모델을 능가하거나 맞먹는 성능을 보여준다. 이를 통해 보다 효율적인 합동 훈련을 제공하며, 보다 효율적인 3D 재구성 백본을 제시하는 길을 열어둔다.

## 🎯 주요 포인트


- 1. MapAnything은 이미지와 기하학적 입력을 활용하여 메트릭 3D 장면 지오메트리와 카메라를 직접 회귀하는 통합형 transformer 기반 모델이다.

- 2. MapAnything은 다양한 데이터셋에 대해 표준화된 지도 및 훈련을 통해 다양한 3D 비전 작업을 처리할 수 있으며, 효율적인 합동 훈련 행동을 제공한다.

- 3. MapAnything은 전문가용 feed-forward 모델을 능가하거나 맞추면서 보다 효율적인 합동 훈련 행동을 제공하여, 보다 효율적인 3D 재구성 백본을 제공한다.


---

*Generated on 2025-09-18 16:18:57*