# A method for improving multilingual quality and diversity of instruction fine-tuning datasets

**Korean Title:** ë‹¤êµ­ì–´ í’ˆì§ˆ ë° ë‹¤ì–‘ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ êµìœ¡ ë¯¸ì„¸ ì¡°ì • ë°ì´í„°ì…‹ì˜ ë°©ë²•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Superficial Alignment Hypothesis|Superficial Alignment Hypothesis]] [[keywords/specific/Data Selection|Data Selection]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Multilingual Instruction Fine-Tuning|Multilingual Instruction Fine-Tuning]] [[keywords/unique/Multilingual Data Quality and Diversity|Multilingual Data Quality and Diversity]] [[categories/cs.CL|cs.CL]] [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (88.5% similar) [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.9% similar) [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Superficial Alignment Hypothesis
**ğŸ”— Specific Connectable**: Data Selection
**ğŸ”¬ Broad Technical**: Large Language Models, Multilingual Instruction Fine-Tuning
**â­ Unique Technical**: Multilingual Data Quality and Diversity
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (88.5% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.9% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (84.9% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope A Dimensional Lens for Probing Cultural Understanding in LLMs]] (84.8% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (84.3% similar)


**ArXiv ID**: [2509.15549](https://arxiv.org/abs/2509.15549)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15549.pdf)


**ArXiv ID**: [2509.15549](https://arxiv.org/abs/2509.15549)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15549.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Superficial Alignment Hypothesis
**ğŸ”— Specific Connectable**: Data Selection
**â­ Unique Technical**: Multilingual Data Quality and Diversity
**ğŸ”¬ Broad Technical**: Large Language Models, Multilingual Instruction Fine-Tuning

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Multilingual Instruction Fine-Tuning` â€¢ 

`Data Selection` â€¢ 

`Multilingual Data Quality and Diversity` â€¢ 

`Superficial Alignment Hypothesis`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15549v1 Announce Type: new 
Abstract: Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large language models (LLMs) to generalize effectively across diverse linguistic and cultural contexts. However, the scarcity of high-quality multilingual training data and corresponding building method remains a critical bottleneck. While data selection has shown promise in English settings, existing methods often fail to generalize across languages due to reliance on simplistic heuristics or language-specific assumptions. In this work, we introduce Multilingual Data Quality and Diversity (M-DaQ), a novel method for improving LLMs multilinguality, by selecting high-quality and semantically diverse multilingual IFT samples. We further conduct the first systematic investigation of the Superficial Alignment Hypothesis (SAH) in multilingual setting. Empirical results across 18 languages demonstrate that models fine-tuned with M-DaQ method achieve significant performance gains over vanilla baselines over 60% win rate. Human evaluations further validate these gains, highlighting the increment of cultural points in the response. We release the M-DaQ code to support future research.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15549v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤êµ­ì–´ ì§€ì‹œ ë¯¸ì„¸ ì¡°ì •(IFT)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì–‘í•œ ì–¸ì–´ì  ë° ë¬¸í™”ì  ë§¥ë½ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³ í’ˆì§ˆì˜ ë‹¤êµ­ì–´ í›ˆë ¨ ë°ì´í„°ì™€ ì´ì— ìƒì‘í•˜ëŠ” êµ¬ì¶• ë°©ë²•ì˜ ë¶€ì¡±ì€ ì—¬ì „íˆ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒìœ¼ë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ì„ íƒì€ ì˜ì–´ í™˜ê²½ì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±ì´ë‚˜ ì–¸ì–´ íŠ¹ìœ ì˜ ê°€ì •ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì— ì–¸ì–´ ì „ë°˜ì— ê±¸ì³ ì¼ë°˜í™”í•˜ëŠ” ë° ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ í’ˆì§ˆì˜ ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ IFT ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ LLMì˜ ë‹¤êµ­ì–´ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì¸ ë‹¤êµ­ì–´ ë°ì´í„° í’ˆì§ˆ ë° ë‹¤ì–‘ì„±(M-DaQ)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë˜í•œ ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ í”¼ìƒì  ì •ë ¬ ê°€ì„¤(SAH)ì— ëŒ€í•œ ìµœì´ˆì˜ ì²´ê³„ì ì¸ ì¡°ì‚¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. 18ê°œ ì–¸ì–´ì— ê±¸ì¹œ ì‹¤ì¦ì  ê²°ê³¼ëŠ” M-DaQ ë°©ë²•ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì´ ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ì— ë¹„í•´ 60% ì´ìƒì˜ ìŠ¹ë¥ ë¡œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¸ê°„ í‰ê°€ ë˜í•œ ì´ëŸ¬í•œ ì„±ê³¼ë¥¼ ê²€ì¦í•˜ë©°, ì‘ë‹µì—ì„œ ë¬¸í™”ì  ìš”ì†Œì˜ ì¦ê°€ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ M-DaQ ì½”ë“œë¥¼ ê³µê°œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‹¤êµ­ì–´ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ë©€í‹°ë§êµ¬ì–¼ ë°ì´í„° í’ˆì§ˆ ë° ë‹¤ì–‘ì„±(M-DaQ) ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°ì´í„° ì„ íƒ ë°©ì‹ì´ ì–¸ì–´ ê°„ ì¼ë°˜í™”ì— í•œê³„ë¥¼ ë³´ì´ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì, ê³ í’ˆì§ˆ ë° ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ í•™ìŠµ ìƒ˜í”Œì„ ì„ íƒí•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ í‘œë©´ì  ì •ë ¬ ê°€ì„¤(SAH)ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. 18ê°œ ì–¸ì–´ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, M-DaQ ë°©ë²•ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì´ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ 60% ì´ìƒì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, ì¸ê°„ í‰ê°€ì—ì„œë„ ë¬¸í™”ì  ìš”ì†Œê°€ ì¦ê°€í•œ ì‘ë‹µì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. M-DaQ ì½”ë“œëŠ” í›„ì† ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ë‹¤êµ­ì–´ ì§€ì¹¨ ë¯¸ì„¸ ì¡°ì •(IFT)ì€ ë‹¤ì–‘í•œ ì–¸ì–´ ë° ë¬¸í™”ì  ë§¥ë½ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¼ë°˜í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.

- 2. ê³ í’ˆì§ˆ ë‹¤êµ­ì–´ í›ˆë ¨ ë°ì´í„°ì˜ ë¶€ì¡±ê³¼ ì´ì— ìƒì‘í•˜ëŠ” êµ¬ì¶• ë°©ë²•ì€ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒìœ¼ë¡œ ë‚¨ì•„ ìˆë‹¤.

- 3. M-DaQëŠ” ê³ í’ˆì§ˆ ë° ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ IFT ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ LLMì˜ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì´ë‹¤.

- 4. M-DaQ ë°©ë²•ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì€ 18ê°œ ì–¸ì–´ì—ì„œ 60% ì´ìƒì˜ ìŠ¹ë¥ ë¡œ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤.

- 5. ì¸ê°„ í‰ê°€ë¥¼ í†µí•´ ì´ëŸ¬í•œ ì„±ëŠ¥ í–¥ìƒì´ ë¬¸í™”ì  ìš”ì†Œì˜ ì¦ê°€ë¥¼ ë°˜ì˜í•œë‹¤ëŠ” ì ì´ ê²€ì¦ë˜ì—ˆë‹¤.


---

*Generated on 2025-09-22 16:22:47*