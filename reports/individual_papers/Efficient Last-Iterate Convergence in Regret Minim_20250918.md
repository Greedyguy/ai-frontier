
# Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation

**Korean Title:** ì ì‘ì  ë³´ìƒ ë³€í™˜ì„ í†µí•œ í›„íšŒ ìµœì†Œí™”ì—ì„œ íš¨ìœ¨ì ì¸ ë§ˆì§€ë§‰ ë°˜ë³µ ìˆ˜ë ´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Last-Iterate Convergence|Last-Iterate Convergence]] [[keywords/broad/Regret Minimization|Regret Minimization]] [[keywords/broad/Reward Transformation|Reward Transformation]] [[keywords/specific/Nash Equilibria|Nash Equilibria]] [[keywords/unique/RT Regret Matching|RT Regret Matching]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Last-Iterate Convergence
**ğŸ”¬ Broad Technical**: Regret Minimization, Reward Transformation
**ğŸ”— Specific Connectable**: RT Regret Matching
**â­ Unique Technical**: Adaptive Reward Transformation

**ArXiv ID**: [2509.13653](https://arxiv.org/abs/2509.13653)
**Published**: 2025-09-18
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.13653.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Regret Minimization` â€¢ 

`Reward Transformation` â€¢ 

`RT Regret Matching` â€¢ 

`Adaptive Reward Transformation` â€¢ 

`Last-Iterate Convergence`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13653v1 Announce Type: cross 
Abstract: Regret minimization is a powerful method for finding Nash equilibria in Normal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically guarantees convergence only for the average strategy. However, computing the average strategy requires significant computational resources or introduces additional errors, limiting its practical applicability. The Reward Transformation (RT) framework was introduced to regret minimization to achieve last-iterate convergence through reward function regularization. However, it faces practical challenges: its performance is highly sensitive to manually tuned parameters, which often deviate from theoretical convergence conditions, leading to slow convergence, oscillations, or stagnation in local optima.
  Inspired by previous work, we propose an adaptive technique to address these issues, ensuring better consistency between theoretical guarantees and practical performance for RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR), and their variants in solving NFGs and EFGs more effectively. Our adaptive methods dynamically adjust parameters, balancing exploration and exploitation while improving regret accumulation, ultimately enhancing asymptotic last-iterate convergence and achieving linear convergence. Experimental results demonstrate that our methods significantly accelerate convergence, outperforming state-of-the-art algorithms.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13653v1 ë°œí‘œ ìœ í˜•: êµì°¨
ìš”ì•½: í›„íšŒ ìµœì†Œí™”ëŠ” ì¼ë°˜ í˜•íƒœ ê²Œì„(NFGs) ë° ê´‘ë²”ìœ„ í˜•íƒœ ê²Œì„(EFGs)ì—ì„œ ë‚´ì‰¬ ê· í˜•ì„ ì°¾ëŠ” ê°•ë ¥í•œ ë°©ë²•ì´ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ í‰ê·  ì „ëµì— ëŒ€í•´ì„œë§Œ ìˆ˜ë ´ì„ ë³´ì¥í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, í‰ê·  ì „ëµì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ìƒë‹¹í•œ ê³„ì‚° ìì›ì´ í•„ìš”í•˜ê±°ë‚˜ ì¶”ê°€ ì˜¤ë¥˜ë¥¼ ë„ì…í•˜ì—¬ ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±ì„ ì œí•œí•©ë‹ˆë‹¤. ë³´ìƒ ë³€í™˜(RT) í”„ë ˆì„ì›Œí¬ëŠ” í›„íšŒ ìµœì†Œí™”ì— ë„ì…ë˜ì–´ ë³´ìƒ í•¨ìˆ˜ ì •ê·œí™”ë¥¼ í†µí•´ ë§ˆì§€ë§‰ ë°˜ë³µ ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ì´ëŠ” ì‹¤ì œì ì¸ ë„ì „ì— ì§ë©´í•˜ê³  ìˆìŠµë‹ˆë‹¤: ì„±ëŠ¥ì´ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì •ëœ ë§¤ê°œ ë³€ìˆ˜ì— ë§¤ìš° ë¯¼ê°í•˜ë©°, ì´ëŠ” ì¢…ì¢… ì´ë¡ ì  ìˆ˜ë ´ ì¡°ê±´ì—ì„œ ë²—ì–´ë‚˜ ëŠë¦° ìˆ˜ë ´, ì§„ë™ ë˜ëŠ” ì§€ì—­ ìµœì ì ì—ì„œì˜ ì •ì²´ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.
  ì´ì „ ì—°êµ¬ë¥¼ ì˜ê°ì„ ë°›ì•„, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì ì‘ ê¸°ìˆ ì„ ì œì•ˆí•˜ë©°, NFGs ë° EFGsë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ê¸° ìœ„í•´ RT í›„íšŒ ë§¤ì¹­(RTRM), RT ì—­ì‚¬ì‹¤ì  í›„íšŒ ìµœì†Œí™”(RTCFR) ë° ê·¸ ë³€í˜•ì— ëŒ€í•œ ì´ë¡ ì  ë³´ì¥ê³¼ ì‹¤ì œì  ì„±ëŠ¥ ì‚¬ì´ì˜ ë” ë‚˜ì€ ì¼ê´€ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ì‘ì  ë°©ë²•ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ íƒìƒ‰ê³¼ ê°œë°œì„ ê· í˜•ìˆê²Œ ìœ ì§€í•˜ë©´ì„œ í›„íšŒ ëˆ„ì ì„ í–¥ìƒì‹œí‚¤ë©°, ê¶ê·¹ì ìœ¼ë¡œ ì ì§„ì  ë§ˆì§€ë§‰ ë°˜ë³µ ìˆ˜ë ´ì„ í–¥ìƒì‹œí‚¤ê³  ì„ í˜• ìˆ˜ë ´ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ìˆ˜ë ´ì„ í˜„ì €íˆ ê°€ì†í™”ì‹œí‚¤ê³  ìµœì²¨ë‹¨ ì•Œê³ ë¦¬ì¦˜ì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë³´ìƒ ë³€í™˜(Reward Transformation) í”„ë ˆì„ì›Œí¬ë¥¼ ê°œì„ í•˜ì—¬ Normal-Form Games (NFGs) ë° Extensive-Form Games (EFGs)ì—ì„œ Nash ê· í˜•ì„ ì°¾ëŠ” ë° íš¨ìœ¨ì ì¸ Regret Minimization ê¸°ë²•ì„ ì œì•ˆí•œë‹¤. ì´ì „ ì—°êµ¬ë¥¼ ì˜ê°ì„ ë°›ì•„ ì´ìŠˆë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì ì‘ ê¸°ìˆ ì„ ë„ì…í•˜ì—¬ RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR) ë° ê·¸ ë³€í˜•ë“¤ì´ NFGs ë° EFGsë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ë„ë¡ ë³´ì¥í•œë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ìˆ˜ë ´ ì†ë„ë¥¼ í˜„ì €íˆ ë†’ì´ê³  ìµœì‹  ì•Œê³ ë¦¬ì¦˜ì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•œë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- Regret minimizationì€ Normal-Form Games (NFGs)ì™€ Extensive-Form Games (EFGs)ì—ì„œ Nash ê· í˜•ì„ ì°¾ëŠ” ê°•ë ¥í•œ ë°©ë²•ì´ë‹¤.

- Reward Transformation (RT) í”„ë ˆì„ì›Œí¬ëŠ” ë§ˆì§€ë§‰ ë°˜ë³µ ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë„ì…ë˜ì—ˆì§€ë§Œ, ìˆ˜ë™ìœ¼ë¡œ ì¡°ì •ëœ ë§¤ê°œë³€ìˆ˜ì— ë§¤ìš° ë¯¼ê°í•˜ë‹¤.

- ìš°ë¦¬ì˜ ì ì‘ ê¸°ìˆ ì€ RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR) ë° ê·¸ ë³€í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ì—¬ ìˆ˜ë ´ì„ ê°€ì†í™”í•˜ê³  ìµœì‹  ì•Œê³ ë¦¬ì¦˜ì„ ëŠ¥ê°€í•œë‹¤.


---

*Generated on 2025-09-18 16:43:19*