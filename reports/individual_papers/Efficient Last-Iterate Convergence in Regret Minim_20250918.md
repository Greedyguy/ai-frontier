
# Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation

**Korean Title:** 적응적 보상 변환을 통한 후회 최소화에서 효율적인 마지막 반복 수렴

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Last-Iterate Convergence|Last-Iterate Convergence]] [[keywords/broad/Regret Minimization|Regret Minimization]] [[keywords/broad/Reward Transformation|Reward Transformation]] [[keywords/specific/Nash Equilibria|Nash Equilibria]] [[keywords/unique/RT Regret Matching|RT Regret Matching]] [[categories/cs.LG|cs.LG]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Last-Iterate Convergence
**🔬 Broad Technical**: Regret Minimization, Reward Transformation
**🔗 Specific Connectable**: RT Regret Matching
**⭐ Unique Technical**: Adaptive Reward Transformation

**ArXiv ID**: [2509.13653](https://arxiv.org/abs/2509.13653)
**Published**: 2025-09-18
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.13653.pdf)


## 🏷️ 추출된 키워드



`Regret Minimization` • 

`Reward Transformation` • 

`RT Regret Matching` • 

`Adaptive Reward Transformation` • 

`Last-Iterate Convergence`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13653v1 Announce Type: cross 
Abstract: Regret minimization is a powerful method for finding Nash equilibria in Normal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically guarantees convergence only for the average strategy. However, computing the average strategy requires significant computational resources or introduces additional errors, limiting its practical applicability. The Reward Transformation (RT) framework was introduced to regret minimization to achieve last-iterate convergence through reward function regularization. However, it faces practical challenges: its performance is highly sensitive to manually tuned parameters, which often deviate from theoretical convergence conditions, leading to slow convergence, oscillations, or stagnation in local optima.
  Inspired by previous work, we propose an adaptive technique to address these issues, ensuring better consistency between theoretical guarantees and practical performance for RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR), and their variants in solving NFGs and EFGs more effectively. Our adaptive methods dynamically adjust parameters, balancing exploration and exploitation while improving regret accumulation, ultimately enhancing asymptotic last-iterate convergence and achieving linear convergence. Experimental results demonstrate that our methods significantly accelerate convergence, outperforming state-of-the-art algorithms.

## 🔍 Abstract (한글 번역)

arXiv:2509.13653v1 발표 유형: 교차
요약: 후회 최소화는 일반 형태 게임(NFGs) 및 광범위 형태 게임(EFGs)에서 내쉬 균형을 찾는 강력한 방법이지만, 일반적으로 평균 전략에 대해서만 수렴을 보장합니다. 그러나, 평균 전략을 계산하는 것은 상당한 계산 자원이 필요하거나 추가 오류를 도입하여 실제 적용 가능성을 제한합니다. 보상 변환(RT) 프레임워크는 후회 최소화에 도입되어 보상 함수 정규화를 통해 마지막 반복 수렴을 달성하기 위해 고안되었습니다. 그러나, 이는 실제적인 도전에 직면하고 있습니다: 성능이 수동으로 조정된 매개 변수에 매우 민감하며, 이는 종종 이론적 수렴 조건에서 벗어나 느린 수렴, 진동 또는 지역 최적점에서의 정체로 이어집니다.
  이전 연구를 영감을 받아, 우리는 이러한 문제를 해결하기 위한 적응 기술을 제안하며, NFGs 및 EFGs를 더 효과적으로 해결하기 위해 RT 후회 매칭(RTRM), RT 역사실적 후회 최소화(RTCFR) 및 그 변형에 대한 이론적 보장과 실제적 성능 사이의 더 나은 일관성을 보장합니다. 우리의 적응적 방법은 매개 변수를 동적으로 조정하여 탐색과 개발을 균형있게 유지하면서 후회 누적을 향상시키며, 궁극적으로 점진적 마지막 반복 수렴을 향상시키고 선형 수렴을 달성합니다. 실험 결과는 우리의 방법이 수렴을 현저히 가속화시키고 최첨단 알고리즘을 능가한다는 것을 입증합니다.

## 📝 요약

이 연구는 보상 변환(Reward Transformation) 프레임워크를 개선하여 Normal-Form Games (NFGs) 및 Extensive-Form Games (EFGs)에서 Nash 균형을 찾는 데 효율적인 Regret Minimization 기법을 제안한다. 이전 연구를 영감을 받아 이슈를 해결하기 위해 적응 기술을 도입하여 RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR) 및 그 변형들이 NFGs 및 EFGs를 효과적으로 해결하도록 보장한다. 실험 결과는 우리의 방법이 수렴 속도를 현저히 높이고 최신 알고리즘을 능가한다는 것을 입증한다.

## 🎯 주요 포인트


- Regret minimization은 Normal-Form Games (NFGs)와 Extensive-Form Games (EFGs)에서 Nash 균형을 찾는 강력한 방법이다.

- Reward Transformation (RT) 프레임워크는 마지막 반복 수렴을 달성하기 위해 도입되었지만, 수동으로 조정된 매개변수에 매우 민감하다.

- 우리의 적응 기술은 RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR) 및 그 변형을 효과적으로 해결하여 수렴을 가속화하고 최신 알고리즘을 능가한다.


---

*Generated on 2025-09-18 16:43:19*