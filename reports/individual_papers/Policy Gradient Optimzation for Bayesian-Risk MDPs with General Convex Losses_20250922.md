# Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses

**Korean Title:** ë² ì´ì§€ì•ˆ ìœ„í—˜ MDPì—ì„œ ì¼ë°˜ì ì¸ ë³¼ë¡ ì†ì‹¤ì„ ê³ ë ¤í•œ ì •ì±… ê²½ì‚¬ ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Episodic Setting Extension

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Online Bayesian Risk-Averse Reinforcement Learning_20250917|Online Bayesian Risk-Averse Reinforcement Learning]] (85.7% similar)
- [[2025-09-19/Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks_20250919|Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks]] (81.6% similar)
- [[2025-09-18/Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain_20250918|Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain]] (81.5% similar)
- [[2025-09-22/Deep Reinforcement Learning with Gradient Eligibility Traces_20250922|Deep Reinforcement Learning with Gradient Eligibility Traces]] (81.2% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (81.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15509v1 Announce Type: new 
Abstract: Motivated by many application problems, we consider Markov decision processes (MDPs) with a general loss function and unknown parameters. To mitigate the epistemic uncertainty associated with unknown parameters, we take a Bayesian approach to estimate the parameters from data and impose a coherent risk functional (with respect to the Bayesian posterior distribution) on the loss. Since this formulation usually does not satisfy the interchangeability principle, it does not admit Bellman equations and cannot be solved by approaches based on dynamic programming. Therefore, We propose a policy gradient optimization method, leveraging the dual representation of coherent risk measures and extending the envelope theorem to continuous cases. We then show the stationary analysis of the algorithm with a convergence rate of $O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations and $r$ is the sample size of the gradient estimator. We further extend our algorithm to an episodic setting, and establish the global convergence of the extended algorithm and provide bounds on the number of iterations needed to achieve an error bound $O(\epsilon)$ in each episode.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15509v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì–‘í•œ ì‘ìš© ë¬¸ì œì— ë™ê¸° ë¶€ì—¬ë˜ì–´, ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDPs)ì„ ê³ ë ¤í•©ë‹ˆë‹¤. ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ì™€ ê´€ë ¨ëœ ì¸ì‹ë¡ ì  ë¶ˆí™•ì‹¤ì„±ì„ ì™„í™”í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³  ì†ì‹¤ì— ëŒ€í•´ ë² ì´ì§€ì•ˆ ì‚¬í›„ ë¶„í¬ì— ëŒ€í•œ ì¼ê´€ëœ ìœ„í—˜ í•¨ìˆ˜(coherent risk functional)ë¥¼ ë¶€ê³¼í•˜ëŠ” ë² ì´ì§€ì•ˆ ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•©ë‹ˆë‹¤. ì´ í˜•ì‹ì€ ì¼ë°˜ì ìœ¼ë¡œ êµí™˜ ê°€ëŠ¥ì„± ì›ì¹™ì„ ì¶©ì¡±í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë²¨ë§Œ ë°©ì •ì‹ì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë©° ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì¼ê´€ëœ ìœ„í—˜ ì¸¡ì •ì˜ ì´ì¤‘ í‘œí˜„ì„ í™œìš©í•˜ê³  ì—°ì†ì ì¸ ê²½ìš°ì— ëŒ€í•´ ë´‰íˆ¬ ì •ë¦¬ë¥¼ í™•ì¥í•˜ì—¬ ì •ì±… ê²½ì‚¬ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ìš°ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ì •ì  ë¶„ì„ì„ ë³´ì—¬ì£¼ë©°, ì •ì±… ê²½ì‚¬ ë°˜ë³µ íšŸìˆ˜ $T$ì™€ ê¸°ìš¸ê¸° ì¶”ì •ê¸°ì˜ ìƒ˜í”Œ í¬ê¸° $r$ì— ëŒ€í•´ ìˆ˜ë ´ ì†ë„ê°€ $O(T^{-1/2}+r^{-1/2})$ì„ì„ ì¦ëª…í•©ë‹ˆë‹¤. ë˜í•œ, ì—í”¼ì†Œë“œ ì„¤ì •ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì„ í™•ì¥í•˜ê³ , í™•ì¥ëœ ì•Œê³ ë¦¬ì¦˜ì˜ ì „ì—­ ìˆ˜ë ´ì„ í™•ë¦½í•˜ë©° ê° ì—í”¼ì†Œë“œì—ì„œ ì˜¤ë¥˜ ê²½ê³„ $O(\epsilon)$ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë°˜ë³µ íšŸìˆ˜ì— ëŒ€í•œ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ ë‹¤ë£¹ë‹ˆë‹¤. ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³ , ì†ì‹¤ì— ëŒ€í•´ ë² ì´ì§€ì•ˆ ì‚¬í›„ ë¶„í¬ì— ê¸°ë°˜í•œ ì¼ê´€ëœ ìœ„í—˜ í•¨ìˆ˜ì ì„ ì ìš©í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë²¨ë§Œ ë°©ì •ì‹ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì €ìë“¤ì€ ì •ì±… ê²½ì‚¬ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì¼ê´€ëœ ìœ„í—˜ ì¸¡ì •ì˜ ì´ì¤‘ í‘œí˜„ì„ í™œìš©í•˜ê³  ì—°ì†ì ì¸ ê²½ìš°ë¡œ ì™¸ì—° ì •ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ì†ë„ëŠ” $O(T^{-1/2}+r^{-1/2})$ì´ë©°, ì—¬ê¸°ì„œ $T$ëŠ” ì •ì±… ê²½ì‚¬ ë°˜ë³µ íšŸìˆ˜, $r$ëŠ” ê²½ì‚¬ ì¶”ì •ê¸°ì˜ ìƒ˜í”Œ í¬ê¸°ì…ë‹ˆë‹¤. ë˜í•œ, ì•Œê³ ë¦¬ì¦˜ì„ ì—í”¼ì†Œë“œ ì„¤ì •ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ê¸€ë¡œë²Œ ìˆ˜ë ´ì„±ì„ ì…ì¦í•˜ê³ , ê° ì—í”¼ì†Œë“œì—ì„œ $O(\epsilon)$ì˜ ì˜¤ë¥˜ ê²½ê³„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë°˜ë³µ íšŸìˆ˜ì— ëŒ€í•œ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDPs)ì„ ë‹¤ë£¨ë©°, ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì„ í†µí•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³  ì†ì‹¤ì— ëŒ€í•œ ì¼ê´€ëœ ë¦¬ìŠ¤í¬ í•¨ìˆ˜ì ì„ ì ìš©í•©ë‹ˆë‹¤.

- 2. ì œì•ˆëœ ë¬¸ì œëŠ” êµí™˜ ê°€ëŠ¥ì„± ì›ì¹™ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ ë²¨ë§Œ ë°©ì •ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©°, ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

- 3. ì •ì±… ê²½ì‚¬ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì¼ê´€ëœ ë¦¬ìŠ¤í¬ ì¸¡ì •ì˜ ì´ì¤‘ í‘œí˜„ì„ í™œìš©í•˜ê³  ì—°ì†ì ì¸ ê²½ìš°ë¡œ ì™¸í”¼ ì •ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.

- 4. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ì†ë„ëŠ” $O(T^{-1/2}+r^{-1/2})$ì´ë©°, ì—¬ê¸°ì„œ $T$ëŠ” ì •ì±… ê²½ì‚¬ ë°˜ë³µ íšŸìˆ˜, $r$ì€ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ê¸°ì˜ ìƒ˜í”Œ í¬ê¸°ì…ë‹ˆë‹¤.

- 5. ì•Œê³ ë¦¬ì¦˜ì„ ì—í”¼ì†Œë“œ ì„¤ì •ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì „ì—­ ìˆ˜ë ´ì„±ì„ í™•ë¦½í•˜ê³ , ê° ì—í”¼ì†Œë“œì—ì„œ ì˜¤ë¥˜ ê²½ê³„ $O(\epsilon)$ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë°˜ë³µ íšŸìˆ˜ì— ëŒ€í•œ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:17:08*