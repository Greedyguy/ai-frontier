# Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì§ˆë¬¸ ì‘ë‹µì— ëŒ€í•œ ìì—°ì–´ ì„¤ëª…ì—ì„œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Robust Uncertainty Estimation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Efficient Real-time Refinement of Language Model Text Generation_20250922|Efficient Real-time Refinement of Language Model Text Generation]] (84.0% similar)
- [[2025-09-22/Knowledge-Driven Hallucination in Large Language Models_ An Empirical Study on Process Modeling_20250922|Knowledge-Driven Hallucination in Large Language Models An Empirical Study on Process Modeling]] (83.5% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (82.8% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (82.6% similar)
- [[2025-09-17/Do Large Language Models Understand Word Senses_20250917|Do Large Language Models Understand Word Senses]] (82.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15403v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong capabilities, enabling concise, context-aware answers in question answering (QA) tasks. The lack of transparency in complex LLMs has inspired extensive research aimed at developing methods to explain large language behaviors. Among existing explanation methods, natural language explanations stand out due to their ability to explain LLMs in a self-explanatory manner and enable the understanding of model behaviors even when the models are closed-source. However, despite these promising advancements, there is no existing work studying how to provide valid uncertainty guarantees for these generated natural language explanations. Such uncertainty quantification is critical in understanding the confidence behind these explanations. Notably, generating valid uncertainty estimates for natural language explanations is particularly challenging due to the auto-regressive generation process of LLMs and the presence of noise in medical inquiries. To bridge this gap, in this work, we first propose a novel uncertainty estimation framework for these generated natural language explanations, which provides valid uncertainty guarantees in a post-hoc and model-agnostic manner. Additionally, we also design a novel robust uncertainty estimation method that maintains valid uncertainty guarantees even under noise. Extensive experiments on QA tasks demonstrate the desired performance of our methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15403v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì§ˆë¬¸ ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ê°„ê²°í•˜ê³  ë§¥ë½ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë³µì¡í•œ LLMì˜ íˆ¬ëª…ì„± ë¶€ì¡±ì€ ëŒ€í˜• ì–¸ì–´ í–‰ë™ì„ ì„¤ëª…í•˜ëŠ” ë°©ë²•ì„ ê°œë°œí•˜ê¸° ìœ„í•œ ê´‘ë²”ìœ„í•œ ì—°êµ¬ë¥¼ ì´‰ë°œí–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì„¤ëª… ë°©ë²• ì¤‘ì—ì„œ ìì—°ì–´ ì„¤ëª…ì€ LLMì„ ìê¸° ì„¤ëª… ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ëª¨ë¸ì´ ë¹„ê³µê°œ ì†ŒìŠ¤ì¼ ë•Œë„ ëª¨ë¸ í–‰ë™ì„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤ëŠ” ì ì—ì„œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ìœ ë§í•œ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ , ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•´ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•œ ê¸°ì¡´ ì—°êµ¬ëŠ” ì—†ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ëŠ” ì´ëŸ¬í•œ ì„¤ëª…ì˜ ì‹ ë¢°ë„ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. íŠ¹íˆ, ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ LLMì˜ ìë™ íšŒê·€ ìƒì„± ê³¼ì •ê³¼ ì˜ë£Œ ë¬¸ì˜ì—ì„œì˜ ì¡ìŒ ì¡´ì¬ë¡œ ì¸í•´ íŠ¹íˆ ì–´ë µìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¨¼ì € ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•´ ì‚¬í›„ì ì´ê³  ëª¨ë¸ ë¹„ì¢…ì†ì ì¸ ë°©ì‹ìœ¼ë¡œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ìƒˆë¡œìš´ ë¶ˆí™•ì‹¤ì„± ì¶”ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, ì¡ìŒì´ ìˆëŠ” ìƒí™©ì—ì„œë„ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ìœ ì§€í•˜ëŠ” ìƒˆë¡œìš´ ê°•ë ¥í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. QA ì‘ì—…ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ë°©ë²•ì´ ì›í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì§ˆë¬¸ ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ, ê·¸ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ íˆ¬ëª…ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” LLMì˜ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìì—°ì–´ ì„¤ëª…ì€ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•˜ëŠ” ë° ìœ ë¦¬í•˜ì§€ë§Œ, ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. íŠ¹íˆ, LLMì˜ ìë™ íšŒê·€ ìƒì„± ê³¼ì •ê³¼ ì˜ë£Œ ë¬¸ì˜ì˜ ë…¸ì´ì¦ˆë¡œ ì¸í•´ ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì´ ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì‚¬í›„ì ì´ê³  ëª¨ë¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ë¶ˆí™•ì‹¤ì„± ì¶”ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ê³ , ë…¸ì´ì¦ˆ í™˜ê²½ì—ì„œë„ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ìœ ì§€í•˜ëŠ” ê°•ë ¥í•œ ì¶”ì • ë°©ë²•ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. QA ì‘ì—…ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì˜ ì„±ëŠ¥ì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì§ˆë¬¸ ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ê°„ê²°í•˜ê³  ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 2. ìì—°ì–´ ì„¤ëª…ì€ LLMì˜ í–‰ë™ì„ ìì²´ ì„¤ëª… ë°©ì‹ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆê²Œ í•˜ë©°, ëª¨ë¸ì´ ë¹„ê³µê°œ ì†ŒìŠ¤ì¼ ë•Œë„ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.

- 3. ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ì—°êµ¬ëŠ” ì•„ì§ ì—†ìœ¼ë©°, ì´ëŠ” ì„¤ëª…ì˜ ì‹ ë¢°ë„ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.

- 4. ë³¸ ì—°êµ¬ì—ì„œëŠ” ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•´ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ìƒˆë¡œìš´ ë¶ˆí™•ì‹¤ì„± ì¶”ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ìƒí™©ì—ì„œë„ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ìœ ì§€í•˜ëŠ” ê°•ë ¥í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:38:07*