# Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues

**Korean Title:** ë‹¨ì–´ë¥¼ ë„˜ì–´: ë¹„ì–¸ì–´ì  ì‹ í˜¸ë¥¼ í†µí•œ ìš•ë§, ê°ì • ë° ê°ì„± ì¸ì‹ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Text-guided Image Decoder|Text-guided Image Decoder]] [[keywords/specific/Cross-modal Alignment|Cross-modal Alignment]] [[keywords/broad/Multimodal Learning|Multimodal Learning]] [[keywords/broad/Sentiment Analysis|Sentiment Analysis]] [[keywords/unique/Symmetrical Bidirectional Multimodal Learning Framework|Symmetrical Bidirectional Multimodal Learning Framework]] [[categories/cs.CL|cs.CL]] [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (82.5% similar) [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (81.4% similar) [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot MEEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Text-guided Image Decoder
**ğŸ”— Specific Connectable**: Cross-modal Alignment
**ğŸ”¬ Broad Technical**: Multimodal Learning, Sentiment Analysis
**â­ Unique Technical**: Symmetrical Bidirectional Multimodal Learning Framework
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (82.5% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1 Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (81.4% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot MEEG Visual Decoding_20250919|UMind A Unified Multitask Network for Zero-Shot MEEG Visual Decoding]] (81.0% similar)
- [[2025-09-18/Humor in Pixels_ Benchmarking Large Multimodal Models Understanding of Online Comics_20250918|Humor in Pixels Benchmarking Large Multimodal Models Understanding of Online Comics]] (80.1% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (80.1% similar)


**ArXiv ID**: [2509.15540](https://arxiv.org/abs/2509.15540)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15540.pdf)


**ArXiv ID**: [2509.15540](https://arxiv.org/abs/2509.15540)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15540.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Text-guided Image Decoder
**ğŸ”— Specific Connectable**: Cross-modal Alignment
**â­ Unique Technical**: Symmetrical Bidirectional Multimodal Learning Framework
**ğŸ”¬ Broad Technical**: Multimodal Learning, Sentiment Analysis

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Multimodal Learning` â€¢ 

`Sentiment Analysis` â€¢ 

`Cross-modal Alignment` â€¢ 

`Symmetrical Bidirectional Multimodal Learning Framework` â€¢ 

`Text-guided Image Decoder`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15540v1 Announce Type: cross 
Abstract: Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15540v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¸ê°„ í–‰ë™ì„ ì´ë„ëŠ” ì˜ë„ë¡œì„œì˜ ìš•ë§ì€ ê°ì • ë° ì •ì„œì™€ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ë©€í‹°ëª¨ë‹¬ í•™ìŠµì€ ì •ì„œ ë° ê°ì • ì¸ì‹ì„ ë°œì „ì‹œì¼°ì§€ë§Œ, ì¸ê°„ ìš•ë§ ì´í•´ë¥¼ íŠ¹ë³„íˆ ê²¨ëƒ¥í•œ ë©€í‹°ëª¨ë‹¬ ì ‘ê·¼ë²•ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì •ì„œ ë¶„ì„ ë°©ë²•ì€ ì£¼ë¡œ ì–¸ì–´ì  ë‹¨ì„œì— ì¤‘ì ì„ ë‘ê³  ì´ë¯¸ì§€ì™€ ê°™ì€ ë³´ì™„ì ì¸ ë¹„ì–¸ì–´ì  ë‹¨ì„œë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìš•ë§, ê°ì •, ì •ì„œ ì¸ì‹ì„ ìœ„í•œ ëŒ€ì¹­ì  ì–‘ë°©í–¥ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒí˜¸ ì§€ë„ë¥¼ í†µí•´ ì´ë¯¸ì§€ì—ì„œ ì˜ë„ ê´€ë ¨ í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ êµì°¨ ëª¨ë‹¬ ì •ë ¬ì„ ìœ„í•œ ì „ì—­ ì‹œê° í‘œí˜„ì„ ì–»ê³ , ê³ í•´ìƒë„ ì´ë¯¸ì§€ëŠ” í•˜ìœ„ ì´ë¯¸ì§€ë¡œ ë¶„í• í•˜ì—¬ ì„¸ë°€í•œ ì§€ì—­ì  íŠ¹ì§•ì„ í¬ì°©í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ëª¨ë¸ë§ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ìœ ë„ ì´ë¯¸ì§€ ë””ì½”ë”ì™€ ì´ë¯¸ì§€ ìœ ë„ í…ìŠ¤íŠ¸ ë””ì½”ë”ë¥¼ ë„ì…í•˜ì—¬ ì´ë¯¸ì§€ ì •ë³´ì˜ ì§€ì—­ ë° ì „ì—­ í‘œí˜„ì—ì„œ ê¹Šì€ êµì°¨ ëª¨ë‹¬ ìƒí˜¸ì‘ìš©ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ë˜í•œ, ì§€ê°ì  ì´ë“ê³¼ ê³„ì‚° ë¹„ìš©ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´, ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ í•˜ìœ„ ì´ë¯¸ì§€ë¡œ ì˜ë¼ ë§ˆìŠ¤í¬ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ëŠ” í˜¼í•© ê·œëª¨ ì´ë¯¸ì§€ ì „ëµì´ ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ì ‘ê·¼ë²•ì€ ìš•ë§ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì™€ ê°ì • ë° ì •ì„œ ì¸ì‹ì„ í¬í•¨í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì¸ MSEDì—ì„œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ë‹¤ë¥¸ ìµœì²¨ë‹¨ ë°©ë²•ì— ë¹„í•´ ì¼ê´€ëœ ê°œì„ ì„ ë‚˜íƒ€ë‚´ë©°, ì œì•ˆëœ ë°©ë²•ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ê¸°ì¡´ ì ‘ê·¼ë²•ì„ ëŠ¥ê°€í•˜ì—¬ ìš•ë§ ì´í•´ì—ì„œ 1.1%, ê°ì • ì¸ì‹ì—ì„œ 0.6%, ì •ì„œ ë¶„ì„ì—ì„œ 0.9%ì˜ F1 ì ìˆ˜ ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” ë‹¤ìŒì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: https://github.com/especiallyW/SyDES.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ì˜ ìš•ë§, ê°ì •, ê°ì • ì¸ì‹ì„ ìœ„í•œ ëŒ€ì¹­ì  ì–‘ë°©í–¥ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ê°ì • ë¶„ì„ ë°©ë²•ì€ ì£¼ë¡œ ì–¸ì–´ì  ë‹¨ì„œì— ì˜ì¡´í•˜ë©° ì´ë¯¸ì§€ì™€ ê°™ì€ ë¹„ì–¸ì–´ì  ë‹¨ì„œë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ìƒí˜¸ ì§€ë„ë¥¼ í†µí•´ ì´ë¯¸ì§€ì—ì„œ ì˜ë„ ê´€ë ¨ í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ ì‹œê° í‘œí˜„ì„ ì–»ê³ , ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì„¸ë¶„í™”í•˜ì—¬ ì„¸ë°€í•œ ì§€ì—­ì  íŠ¹ì§•ì„ í¬ì°©í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ê¹Šì€ ìƒí˜¸ì‘ìš©ì„ ìœ„í•´ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë””ì½”ë”ì™€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë””ì½”ë”ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ MSED ë°ì´í„°ì…‹ì—ì„œ ìš•ë§ ì´í•´, ê°ì • ì¸ì‹, ê°ì • ë¶„ì„ì—ì„œ ê°ê° 1.1%, 0.6%, 0.9%ì˜ F1-score í–¥ìƒì„ ë³´ì´ë©° ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ì¸ê°„ì˜ ìš•êµ¬ ì´í•´ë¥¼ ëª©í‘œë¡œ í•œ ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµ ì ‘ê·¼ë²•ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.

- 2. ì œì•ˆëœ ëŒ€ì¹­ ì–‘ë°©í–¥ ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ìƒí˜¸ ì§€ë„ë¥¼ í†µí•´ ì˜ë„ ê´€ë ¨ í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•œë‹¤.

- 3. ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ ì‹œê° í‘œí˜„ì„ ì–»ê³ , ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì„œë¸Œ ì´ë¯¸ì§€ë¡œ ë¶„í• í•˜ì—¬ ì„¸ë¶€ì ì¸ ì§€ì—­ íŠ¹ì§•ì„ í¬ì°©í•œë‹¤.

- 4. ì œì•ˆëœ ë°©ë²•ì€ MSED ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ì…‹ì—ì„œ ìš•êµ¬ ì´í•´, ê°ì • ë° ê°ì„± ì¸ì‹ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ìš•êµ¬ ì´í•´ì—ì„œ 1.1%, ê°ì • ì¸ì‹ì—ì„œ 0.6%, ê°ì„± ë¶„ì„ì—ì„œ 0.9%ì˜ F1-score í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤.


---

*Generated on 2025-09-22 16:31:52*