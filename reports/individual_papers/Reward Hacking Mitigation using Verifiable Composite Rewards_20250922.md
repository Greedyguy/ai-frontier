# Reward Hacking Mitigation using Verifiable Composite Rewards

**Korean Title:** ë³´ì¦ ê°€ëŠ¥í•œ ë³µí•© ë³´ìƒì„ ì´ìš©í•œ ë³´ìƒ í•´í‚¹ ì™„í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Reinforcement Learning from Verifiable Rewards

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (85.4% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.3% similar)
- [[2025-09-19/Generalizable Geometric Image Caption Synthesis_20250919|Generalizable Geometric Image Caption Synthesis]] (84.5% similar)
- [[2025-09-18/Evolving Language Models without Labels_ Majority Drives Selection, Novelty Promotes Variation_20250918|Evolving Language Models without Labels Majority Drives Selection, Novelty Promotes Variation]] (83.7% similar)
- [[2025-09-19/FlowRL_ Matching Reward Distributions for LLM Reasoning_20250919|FlowRL Matching Reward Distributions for LLM Reasoning]] (82.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15557v1 Announce Type: cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that extending RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15557v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒìœ¼ë¡œë¶€í„°ì˜ ê°•í™” í•™ìŠµ(RLVR)ì€ ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì§ì ‘ì ì¸ ê°ë… ì—†ì´ë„ ìì²´ì ì¸ ì¶”ë¡ ì„ ê°œë°œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì˜ë£Œ ë¶„ì•¼, íŠ¹íˆ ì§ˆë¬¸ ì‘ë‹µì—ì˜ ì‘ìš©ì€ ì¶”ë¡  ë‹¨ê³„ì—ì„œ ìƒë‹¹í•œ ë³´ìƒ í•´í‚¹ì— ì·¨ì•½í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ í–‰ë™ì˜ ë‘ ê°€ì§€ ì£¼ìš” í˜•íƒœë¥¼ ë‹¤ë£¹ë‹ˆë‹¤: i) ì¶”ë¡  ì—†ì´ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒ, ii) ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ì„ ì•…ìš©í•˜ê¸° ìœ„í•œ ë¹„í‘œì¤€ ì¶”ë¡  í˜•ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ. ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í–‰ë™ì— ëŒ€í•œ íŠ¹ì • í˜ë„í‹°ê°€ í¬í•¨ëœ ë³µí•© ë³´ìƒ í•¨ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ì œì•ˆëœ ë³´ìƒ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ RLVRì„ í™•ì¥í•  ê²½ìš°, ë” ë‚˜ì€ í˜•ì‹ì˜ ì¶”ë¡ ê³¼ ì ì€ ë³´ìƒ í•´í‚¹, ê·¸ë¦¬ê³  ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬ ì¢‹ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ë³´ìƒ í•´í‚¹ì„ ì¤„ì´ê³  RLVRì„ í™œìš©í•˜ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í•œ ê±¸ìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ê°•í™” í•™ìŠµ(RLVR)ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì í•œë‹¤. íŠ¹íˆ, ì§ˆë¬¸ ì‘ë‹µ ì‹œìŠ¤í…œì—ì„œ ë°œìƒí•˜ëŠ” ë³´ìƒ í•´í‚¹ ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤. ì—°êµ¬ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œ, ì¦‰ i) ì´ìœ  ì—†ì´ ìµœì¢… ë‹µë³€ë§Œ ì œê³µí•˜ëŠ” ê²ƒê³¼ ii) ë¹„í‘œì¤€ì ì¸ ì¶”ë¡  í˜•ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì‹ë³„í–ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íŠ¹ì • í–‰ë™ì— ëŒ€í•œ íŒ¨ë„í‹°ë¥¼ í¬í•¨í•œ ë³µí•© ë³´ìƒ í•¨ìˆ˜ë¥¼ ì œì•ˆí–ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë³´ìƒ ëª¨ë¸ì„ ì ìš©í•˜ë©´ ë³´ìƒ í•´í‚¹ì´ ì¤„ì–´ë“¤ê³  ì •í™•ë„ê°€ í–¥ìƒë˜ë©°, ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RLVRëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ì§ì ‘ì ì¸ ê°ë… ì—†ì´ ìì²´ì ì¸ ì¶”ë¡ ì„ ê°œë°œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

- 2. ì˜ë£Œ ë¶„ì•¼ì˜ ì§ˆë¬¸ ì‘ë‹µ ì‘ìš©ì—ì„œëŠ” ë³´ìƒ í•´í‚¹ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

- 3. ë³´ìƒ í•´í‚¹ì„ ì¤„ì´ê¸° ìœ„í•´ ë³µí•© ë³´ìƒ í•¨ìˆ˜ì™€ íŠ¹ì • í˜ë„í‹°ë¥¼ ë„ì…í•˜ì˜€ë‹¤.

- 4. ì œì•ˆëœ ë³´ìƒ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ë³´ìƒ í•´í‚¹ì´ ì¤„ì–´ë“¤ê³ , í¬ë§·ì´ ì˜ ê°–ì¶°ì§„ ì¶”ë¡ ì„ ì œê³µí•œë‹¤.

- 5. ì´ ì ‘ê·¼ë²•ì€ RLVRì„ í™œìš©í•˜ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•œë‹¤.

---

*Generated on 2025-09-22 14:02:08*