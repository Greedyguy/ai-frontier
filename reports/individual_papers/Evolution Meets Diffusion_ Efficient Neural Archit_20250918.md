
# Evolution Meets Diffusion: Efficient Neural Architecture Generation

**Korean Title:** ì§„í™”ì™€ í™•ì‚°ì´ ë§Œë‚˜ë‹¤: íš¨ìœ¨ì ì¸ ì‹ ê²½ êµ¬ì¡° ìƒì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Efficient Architecture Generation|Efficient Architecture Generation]] [[keywords/broad/Neural Architecture Search|Neural Architecture Search]] [[keywords/broad/Evolutionary Algorithms|Evolutionary Algorithms]] [[keywords/specific/Diffusion Models|Diffusion Models]] [[keywords/unique/Evolutionary Diffusion-based Neural Architecture Generation (EDNAG|Evolutionary Diffusion-based Neural Architecture Generation (EDNAG]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Efficient Architecture Generation
**ğŸ”¬ Broad Technical**: Neural Architecture Search, Evolutionary Algorithms
**ğŸ”— Specific Connectable**: Diffusion Models
**â­ Unique Technical**: Evolutionary Diffusion-based Neural Architecture Generation (EDNAG

**ArXiv ID**: [2504.17827](https://arxiv.org/abs/2504.17827)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2504.17827.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Neural Architecture Search` â€¢ 

`Evolutionary Algorithms` â€¢ 

`Diffusion Models` â€¢ 

`Evolutionary Diffusion-based Neural Architecture Generation (EDNAG` â€¢ 

`Efficient Architecture Generation`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.17827v4 Announce Type: replace-cross 
Abstract: Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Architecture Generation (NAG) addresses this by reframing NAS as a generation problem, enabling the precise generation of optimal architectures for specific tasks. Despite its promise, mainstream methods like diffusion models face limitations in global search capabilities and are still hindered by high computational and time demands. To overcome these challenges, we propose Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel approach that achieves efficient and training-free architecture generation. EDNAG leverages evolutionary algorithms to simulate the denoising process in diffusion models, using fitness to guide the transition from random Gaussian distributions to optimal architecture distributions. This approach combines the strengths of evolutionary strategies and diffusion models, enabling rapid and effective architecture generation. Extensive experiments demonstrate that EDNAG achieves state-of-the-art (SOTA) performance in architecture optimization, with an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need for time-consuming training and boosts inference speed by an average of 50 times, showcasing its exceptional efficiency and effectiveness.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2504.17827v4 ê³µì§€ ìœ í˜•: êµì²´-êµì°¨
ìš”ì•½: ì‹ ê²½ êµ¬ì¡° íƒìƒ‰(NAS)ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„ì—ì„œ í˜ì‹ ì ì¸ ì ì¬ë ¥ìœ¼ë¡œ ë„ë¦¬ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ NASì˜ ë°©ëŒ€í•˜ê³  ë³µì¡í•œ íƒìƒ‰ ê³µê°„ì€ ìƒë‹¹í•œ ê³„ì‚° ë° ì‹œê°„ ë¹„ìš©ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì‹ ê²½ êµ¬ì¡° ìƒì„±(NAG)ì€ NASë¥¼ ìƒì„± ë¬¸ì œë¡œ ì¬êµ¬ì„±í•˜ì—¬ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ìµœì  ì•„í‚¤í…ì²˜ì˜ ì •í™•í•œ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì£¼ë¥˜ ë°©ë²•ì¸ í™•ì‚° ëª¨ë¸ì€ ì „ì—­ íƒìƒ‰ ëŠ¥ë ¥ì— ì œí•œì´ ìˆê³  ì—¬ì „íˆ ë†’ì€ ê³„ì‚° ë° ì‹œê°„ ìš”êµ¬ì— ì œì•½ì„ ë°›ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë„ì „ì— ëŒ€ì²˜í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ì§„í™” í™•ì‚° ê¸°ë°˜ ì‹ ê²½ êµ¬ì¡° ìƒì„±(EDNAG)ì´ë¼ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. EDNAGëŠ” ì§„í™” ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ í™•ì‚° ëª¨ë¸ì—ì„œ ì¡ìŒ ì œê±° ê³¼ì •ì„ ëª¨ë°©í•˜ë©°, ì í•©ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ìµœì  ì•„í‚¤í…ì²˜ ë¶„í¬ë¡œì˜ ì „í™˜ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì§„í™” ì „ëµê³¼ í™•ì‚° ëª¨ë¸ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ì‹ ì†í•˜ê³  íš¨ê³¼ì ì¸ ì•„í‚¤í…ì²˜ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” EDNAGê°€ ì•„í‚¤í…ì²˜ ìµœì í™”ì—ì„œ ìµœì‹  ê¸°ìˆ (SOTA) ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì •í™•ë„ê°€ ìµœëŒ€ 10.45% í–¥ìƒë˜ì—ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ë¶ˆì–´, ì´ëŠ” ì‹œê°„ ì†Œëª¨ì ì¸ í›ˆë ¨ì˜ í•„ìš”ì„±ì„ ì œê±°í•˜ê³  ì¶”ë¡  ì†ë„ë¥¼ í‰ê·  50ë°° í–¥ìƒì‹œì¼œ íƒì›”í•œ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

í•œêµ­ì–´ ìš”ì•½:
ìµœê·¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„ì—ì„œ í˜ì‹ ì ì¸ ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‹ ê²½êµ¬ì¡° íƒìƒ‰(NAS)ì´ ì£¼ëª©ë°›ê³  ìˆì§€ë§Œ, NASì˜ ë°©ëŒ€í•˜ê³  ë³µì¡í•œ íƒìƒ‰ ê³µê°„ì€ ìƒë‹¹í•œ ê³„ì‚° ë° ì‹œê°„ ë¹„ìš©ì„ ì•¼ê¸°í•œë‹¤. ì´ì— ì‹ ê²½êµ¬ì¡° ìƒì„±(NAG)ì€ NASë¥¼ ìƒì„± ë¬¸ì œë¡œ ì¬êµ¬ì„±í•˜ì—¬ íŠ¹ì • ì‘ì—…ì— ìµœì ì˜ êµ¬ì¡°ë¥¼ ì •í™•í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Evolutionary Diffusion-based Neural Architecture Generation (EDNAG)ì´ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•œë‹¤. ì´ ë°©ë²•ì€ ì§„í™” ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ í™•ì‚° ëª¨ë¸ì˜ ì†ŒìŒ ì œê±° ê³¼ì •ì„ ëª¨ë°©í•˜ê³ , ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ìµœì ì˜ êµ¬ì¡° ë¶„í¬ë¡œì˜ ì „ì´ë¥¼ ì´ëŒì–´ë‚´ëŠ” ë°ì— ì í•©ì„±ì„ í™œìš©í•œë‹¤. EDNAGì€ ì§„í™” ì „ëµê³¼ í™•ì‚° ëª¨ë¸ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ì‹ ì†í•˜ê³  íš¨ê³¼ì ì¸ êµ¬ì¡° ìƒì„±ì„ ê°€ëŠ¥ì¼€ í•œë‹¤. ì‹¤í—˜ ê²°ê³¼, EDNAGì€ ìµœëŒ€ 10.45%ì˜ ì •í™•ë„ í–¥ìƒì„ ë‹¬ì„±í•˜ë©°, í›ˆë ¨ ì‹œê°„ì„ í•„ìš”ë¡œ í•˜ì§€ ì•Šê³  ì¶”ë¡  ì†ë„ë¥¼ í‰ê·  50ë°° í–¥ìƒì‹œì¼œ íƒì›”í•œ íš¨ìœ¨ì„±ì„ ë³´ì—¬ì¤€ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- Neural Architecture Search(NAS)ì˜ ë³µì¡í•œ íƒìƒ‰ ê³µê°„ìœ¼ë¡œ ì¸í•œ ê³„ì‚° ë° ì‹œê°„ ë¹„ìš© ë¬¸ì œ

- Evolutionary Diffusion-based Neural Architecture Generation(EDNAG)ì€ íš¨ìœ¨ì ì´ê³  í›ˆë ¨ í•„ìš” ì—†ëŠ” ì•„í‚¤í…ì²˜ ìƒì„±ì„ ë‹¬ì„±

- EDNAGì€ ìµœëŒ€ 10.45%ì˜ ì •í™•ë„ í–¥ìƒê³¼ 50ë°°ì˜ ì¶”ë¡  ì†ë„ í–¥ìƒì„ ë³´ì—¬ì¤Œ


---

*Generated on 2025-09-18 16:32:45*