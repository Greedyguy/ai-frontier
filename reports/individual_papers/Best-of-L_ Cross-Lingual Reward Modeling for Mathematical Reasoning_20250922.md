# Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning

**Korean Title:** Best-of-L: ìˆ˜í•™ì  ì¶”ë¡ ì„ ìœ„í•œ êµì°¨ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ë§

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multilingual Reasoning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (84.9% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (84.9% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (84.8% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony On Multilingual Data Allocation for Large Language Models Pretraining]] (83.8% similar)
- [[2025-09-17/THOR_ Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning_20250917|THOR Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning]] (83.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15811v1 Announce Type: cross 
Abstract: While the reasoning abilities of large language models (LLMs) continue to advance, it remains unclear how such ability varies across languages in multilingual LLMs and whether different languages produce reasoning paths that complement each other. To investigate this question, we train a reward model to rank generated responses for a given question across languages. Our results show that our cross-lingual reward model substantially improves mathematical reasoning performance compared to using reward modeling within a single language, benefiting even high-resource languages. While English often exhibits the highest performance in multilingual models, we find that cross-lingual sampling particularly benefits English under low sampling budgets. Our findings reveal new opportunities to improve multilingual reasoning by leveraging the complementary strengths of diverse languages.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15811v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì´ ê³„ì† ë°œì „í•˜ê³  ìˆì§€ë§Œ, ë‹¤êµ­ì–´ LLMì—ì„œ ì´ëŸ¬í•œ ëŠ¥ë ¥ì´ ì–¸ì–´ì— ë”°ë¼ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€, ê·¸ë¦¬ê³  ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ê°€ ì„œë¡œ ë³´ì™„í•˜ëŠ” ì¶”ë¡  ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ”ì§€ ì—¬ë¶€ëŠ” ì—¬ì „íˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì— ê±¸ì³ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„±ëœ ì‘ë‹µì„ ìˆœìœ„ ë§¤ê¸°ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ë‹¨ì¼ ì–¸ì–´ ë‚´ì—ì„œ ë³´ìƒ ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì— ë¹„í•´, êµì°¨ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ì´ ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ ìƒë‹¹íˆ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ë„ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ì–´ëŠ” ì¢…ì¢… ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìš°ë¦¬ëŠ” ë‚®ì€ ìƒ˜í”Œë§ ì˜ˆì‚° í•˜ì—ì„œ êµì°¨ ì–¸ì–´ ìƒ˜í”Œë§ì´ íŠ¹íˆ ì˜ì–´ì— ì´ì ì„ ì œê³µí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°œê²¬ì€ ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìƒí˜¸ ë³´ì™„ì ì¸ ê°•ì ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì¶”ë¡ ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤êµ­ì–´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì´ ì–¸ì–´ë³„ë¡œ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ì™€ ê° ì–¸ì–´ê°€ ìƒí˜¸ ë³´ì™„ì ì¸ ì¶”ë¡  ê²½ë¡œë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ìƒì„±ëœ ì‘ë‹µì„ í‰ê°€í•˜ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë‹¤êµ­ì–´ ë³´ìƒ ëª¨ë¸ì´ ë‹¨ì¼ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ì— ë¹„í•´ ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìœ¼ë©°, ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ë„ ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤. íŠ¹íˆ, ì˜ì–´ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë‚®ì€ ìƒ˜í”Œë§ ì˜ˆì‚°ì—ì„œëŠ” ë‹¤êµ­ì–´ ìƒ˜í”Œë§ì´ ì˜ì–´ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìƒí˜¸ ë³´ì™„ì  ê°•ì ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì¶”ë¡ ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì€ ì–¸ì–´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë©°, ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ê°€ ìƒí˜¸ ë³´ì™„ì ì¸ ì¶”ë¡  ê²½ë¡œë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.

- 2. ì—°êµ¬ ê²°ê³¼, ë‹¤êµ­ì–´ ë³´ìƒ ëª¨ë¸ì€ ë‹¨ì¼ ì–¸ì–´ ë‚´ ë³´ìƒ ëª¨ë¸ë§ë³´ë‹¤ ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ì´ëŠ” ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ë„ ì´ì ì„ ì œê³µí•œë‹¤.

- 3. ì˜ì–´ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ì¢…ì¢… ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë‚®ì€ ìƒ˜í”Œë§ ì˜ˆì‚° í•˜ì—ì„œëŠ” ì˜ì–´ë„ ë‹¤êµ­ì–´ ìƒ˜í”Œë§ì˜ í˜œíƒì„ ë°›ì„ ìˆ˜ ìˆë‹¤.

- 4. ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìƒí˜¸ ë³´ì™„ì  ê°•ì ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì¶”ë¡ ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸°íšŒê°€ ë°œê²¬ë˜ì—ˆë‹¤.

---

*Generated on 2025-09-22 14:13:14*