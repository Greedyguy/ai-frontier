# SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection

**Korean Title:** SABER: 교차 계층 잔차 연결을 통한 안전 정렬의 취약점 발견

## 📋 메타데이터

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Safety Alignment Bypass

## 🔗 유사한 논문
- [[2025-09-22/Activation Space Interventions Can Be Transferred Between Large Language Models_20250922|Activation Space Interventions Can Be Transferred Between Large Language Models]] (84.2% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (83.8% similar)
- [[2025-09-18/Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs_20250918|Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs]] (83.0% similar)
- [[2025-09-18/MUSE_ MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models_20250918|MUSE MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models]] (83.0% similar)
- [[2025-09-18/LLM Jailbreak Detection for (Almost) Free!_20250918|LLM Jailbreak Detection for (Almost) Free!]] (83.0% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with safe-alignment training are powerful instruments with robust language comprehension capabilities. These models typically undergo meticulous alignment procedures involving human feedback to ensure the acceptance of safe inputs while rejecting harmful or unsafe ones. However, despite their massive scale and alignment efforts, LLMs remain vulnerable to jailbreak attacks, where malicious users manipulate the model to produce harmful outputs that it was explicitly trained to avoid. In this study, we find that the safety mechanisms in LLMs are predominantly embedded in the middle-to-late layers. Building on this insight, we introduce a novel white-box jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which connects two intermediate layers $s$ and $e$ such that $s < e$, through a residual connection. Our approach achieves a 51% improvement over the best-performing baseline on the HarmBench test set. Furthermore, SABER induces only a marginal shift in perplexity when evaluated on the HarmBench validation set. The source code is publicly available at https://github.com/PalGitts/SABER.

## 🔍 Abstract (한글 번역)

arXiv:2509.16060v1 발표 유형: 신규  
초록: 안전 정렬 훈련을 받은 대형 언어 모델(LLM)은 강력한 언어 이해 능력을 가진 도구입니다. 이러한 모델은 일반적으로 안전한 입력을 수용하고 유해하거나 안전하지 않은 입력을 거부하도록 하기 위해 인간의 피드백을 포함한 세심한 정렬 절차를 거칩니다. 그러나 대규모와 정렬 노력에도 불구하고, LLM은 악의적인 사용자가 모델을 조작하여 명시적으로 회피하도록 훈련된 유해한 출력을 생성하게 하는 탈옥 공격에 취약합니다. 본 연구에서는 LLM의 안전 메커니즘이 주로 중간에서 후반 레이어에 내장되어 있음을 발견했습니다. 이러한 통찰을 바탕으로, 우리는 새로운 화이트박스 탈옥 방법인 SABER(Safety Alignment Bypass via Extra Residuals)를 소개합니다. 이는 잔여 연결을 통해 두 개의 중간 레이어 $s$와 $e$를 연결하며, $s < e$입니다. 우리의 접근 방식은 HarmBench 테스트 세트에서 가장 성능이 좋은 기준선보다 51% 향상된 결과를 달성했습니다. 또한, SABER는 HarmBench 검증 세트에서 평가할 때 혼란도에 거의 미미한 변화를 유도합니다. 소스 코드는 https://github.com/PalGitts/SABER에서 공개적으로 이용 가능합니다.

## 📝 요약

이 연구는 안전 정렬 훈련을 거친 대형 언어 모델(LLM)이 여전히 탈옥 공격에 취약하다는 점을 밝힙니다. 연구진은 LLM의 안전 메커니즘이 주로 중간부터 후반 레이어에 내재되어 있음을 발견하고, 이를 기반으로 SABER라는 새로운 화이트박스 탈옥 방법을 제안합니다. SABER는 두 중간 레이어를 잇는 잔차 연결을 통해 안전 정렬을 우회하며, HarmBench 테스트 세트에서 기존 최고 성능 대비 51% 개선을 달성했습니다. 또한, 검증 세트에서 혼란도 변화는 미미했습니다. 연구의 소스 코드는 공개되어 있습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 안전한 입력을 수용하고 유해한 입력을 거부하도록 인간 피드백을 통해 정렬 절차를 거치지만, 여전히 탈옥 공격에 취약하다.

- 2. LLM의 안전 메커니즘은 주로 중간에서 후반 층에 내장되어 있다.

- 3. 본 연구에서는 두 개의 중간 층을 잇는 잉여 연결을 통해 새로운 화이트박스 탈옥 방법인 SABER를 제안한다.

- 4. SABER는 HarmBench 테스트 세트에서 기존 최적 성능 대비 51%의 성능 향상을 달성했다.

- 5. SABER의 소스 코드는 https://github.com/PalGitts/SABER에서 공개적으로 제공된다.

---

*Generated on 2025-09-22 15:31:11*