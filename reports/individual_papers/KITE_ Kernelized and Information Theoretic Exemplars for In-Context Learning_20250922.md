# KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning

**Korean Title:** KITE: ë¬¸ë§¥ ë‚´ í•™ìŠµì„ ìœ„í•œ ì»¤ë„í™” ë° ì •ë³´ ì´ë¡ ì  ì˜ˆì œë“¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: In-Context Learning, Kernel Trick

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (84.4% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (83.4% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (82.5% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.1% similar)
- [[2025-09-22/Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text_ Development and Deployment in Real-World Scenarios_20250922|Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text Development and Deployment in Real-World Scenarios]] (81.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15676v1 Announce Type: cross 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective. We first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of exemplars from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15676v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë¬¸ë§¥ í•™ìŠµ(In-context learning, ICL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‘ì—…ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìœ¼ë©°, í”„ë¡¬í”„íŠ¸ì— ì œì‹œëœ ëª‡ ê°€ì§€ ì‹ ì¤‘í•˜ê²Œ ì„ íƒëœ ì‘ì—…ë³„ ì˜ˆì‹œë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ LLMì˜ ì œí•œëœ ë¬¸ë§¥ í¬ê¸°ë¥¼ ê³ ë ¤í•  ë•Œ, ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì´ ì œê¸°ë©ë‹ˆë‹¤: ì£¼ì–´ì§„ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì˜ˆì‹œë¥¼ ì„ íƒí•´ì•¼ í• ê¹Œìš”? KATEì™€ ê°™ì€ ìµœê·¼ì ‘ ì´ì›ƒ ê¸°ë°˜ ë°©ë²•ì´ ì´ ëª©ì ì„ ìœ„í•´ ë„ë¦¬ ì±„íƒë˜ì—ˆì§€ë§Œ, ê³ ì°¨ì› ì„ë² ë”© ê³µê°„ì—ì„œ ì¼ë°˜í™” ë¶€ì¡±ê³¼ ë‹¤ì–‘ì„± ê²°ì—¬ì™€ ê°™ì€ ì˜ ì•Œë ¤ì§„ ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ ì›ì¹™ì ì¸ ê´€ì ì—ì„œ ICLì—ì„œì˜ ì˜ˆì‹œ ì„ íƒ ë¬¸ì œë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¨¼ì € LLMì„ ì…ë ¥ ì„ë² ë”©ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¡œ ëª¨ë¸ë§í•˜ê³ , ì˜ˆì‹œ ì„ íƒ ì‘ì—…ì„ íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ìµœì í™” ë¬¸ì œë¡œ êµ¬ì„±í•©ë‹ˆë‹¤: íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë” í° ì˜ˆì‹œ ì€í–‰ì—ì„œ ì˜ˆì‹œì˜ ë¶€ë¶„ ì§‘í•©ì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ê³µì‹ì€ íŠ¹ì • ì¿¼ë¦¬ ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ì •í™•í•œ ì˜ˆì¸¡ì„ ëª©í‘œë¡œ í•¨ìœ¼ë¡œì¨ ì „í†µì ì¸ ì¼ë°˜í™” ì¤‘ì‹¬ì˜ í•™ìŠµ ì´ë¡ ì  ì ‘ê·¼ ë°©ì‹ì—ì„œ ë²—ì–´ë‚©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê·¼ì‚¬ì ìœ¼ë¡œ ë¶€ë¶„ ëª¨ë“ˆëŸ¬ì¸ ì›ì¹™ì ì¸ ëŒ€ë¦¬ ëª©í‘œë¥¼ ë„ì¶œí•˜ì—¬ ê·¼ì‚¬ ë³´ì¥ì´ ìˆëŠ” íƒìš• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, (i) ëª…ì‹œì  ë§¤í•‘ ì—†ì´ ê³ ì°¨ì› íŠ¹ì§• ê³µê°„ì—ì„œ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ ì»¤ë„ íŠ¸ë¦­ì„ í†µí•©í•˜ê³ , (ii) ì„ íƒëœ ì˜ˆì‹œì˜ ë‹¤ì–‘ì„±ì„ ì¥ë ¤í•˜ê¸° ìœ„í•´ ìµœì  ì„¤ê³„ ê¸°ë°˜ ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ë°©ë²•ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í‘œì¤€ ê²€ìƒ‰ ë°©ë²•ì— ë¹„í•´ ìƒë‹¹í•œ ê°œì„ ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œ ì„¸ê³„ì˜ ë¼ë²¨ì´ ë¶€ì¡±í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ICLì„ ìœ„í•œ êµ¬ì¡° ì¸ì‹ì ì´ê³  ë‹¤ì–‘í•œ ì˜ˆì‹œ ì„ íƒì˜ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ë°ì´í„° ë¶€ì¡± ê³¼ì œì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ì¸ ë¬¸ë§¥ ë‚´ í•™ìŠµ(ICL)ì˜ ì˜ˆì œ ì„ íƒ ë¬¸ì œë¥¼ ì •ë³´ ì´ë¡ ì  ê´€ì ì—ì„œ ì—°êµ¬í•©ë‹ˆë‹¤. LLMì˜ ì œí•œëœ ë¬¸ë§¥ í¬ê¸° ë‚´ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ì˜ˆì œë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìµœê·¼ì ‘ ì´ì›ƒ ê¸°ë°˜ ë°©ë²•ë“¤ì€ ê³ ì°¨ì› ì„ë² ë”© ê³µê°„ì—ì„œ ì¼ë°˜í™” ë¶€ì¡±ê³¼ ë‹¤ì–‘ì„± ê²°ì—¬ ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” LLMì„ ì…ë ¥ ì„ë² ë”©ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¡œ ëª¨ë¸ë§í•˜ê³ , íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì˜ˆì œ ì„ íƒ ë¬¸ì œë¥¼ ìµœì í™” ë¬¸ì œë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê·¼ì‚¬ì ìœ¼ë¡œ ë¶€ë¶„ ëª¨ë“ˆëŸ¬í•œ ëŒ€ë¦¬ ëª©í‘œë¥¼ ë„ì¶œí•˜ê³ , íƒìš• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ì˜ˆì œ ì„ íƒì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ ê³ ì°¨ì› íŠ¹ì§• ê³µê°„ì—ì„œ ì»¤ë„ íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ê³ , ìµœì  ì„¤ê³„ ê¸°ë°˜ ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ì„ íƒëœ ì˜ˆì œì˜ ë‹¤ì–‘ì„±ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë‹¤ì–‘í•œ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í‘œì¤€ ê²€ìƒ‰ ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë¨ì„ ë³´ì—¬ì£¼ë©°, êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë‹¤ì–‘í•œ ì˜ˆì œ ì„ íƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. In-context learning (ICL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‘ì—…ì— ì ì‘ì‹œí‚¤ëŠ” ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.

- 2. LLMì˜ ì œí•œëœ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ë•Œë¬¸ì—, ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì˜ˆì œë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•œ ë¬¸ì œë¡œ ëŒ€ë‘ëœë‹¤.

- 3. ë³¸ ì—°êµ¬ëŠ” ì •ë³´ ì´ë¡  ê¸°ë°˜ì˜ ê´€ì ì—ì„œ ICLì—ì„œì˜ ì˜ˆì œ ì„ íƒ ë¬¸ì œë¥¼ ë‹¤ë£¨ë©°, ì¿¼ë¦¬ë³„ ìµœì í™” ë¬¸ì œë¡œ ì˜ˆì œ ì„ íƒì„ ëª¨ë¸ë§í•œë‹¤.

- 4. ì„œë¸Œëª¨ë“ˆëŸ¬ ê·¼ì‚¬ ë³´ì¥ì„ ì œê³µí•˜ëŠ” íƒìš• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì œ ì„ íƒì„ ìµœì í™”í•˜ë©°, ì»¤ë„ íŠ¸ë¦­ê³¼ ìµœì  ì„¤ê³„ ê¸°ë°˜ì˜ ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ë‹¤ì–‘ì„±ì„ ì´‰ì§„í•œë‹¤.

- 5. ë‹¤ì–‘í•œ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í‘œì¤€ ê²€ìƒ‰ ë°©ë²•ì— ë¹„í•´ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŒì„ ì‹¤ì¦ì ìœ¼ë¡œ ë³´ì—¬ì£¼ë©°, ICLì—ì„œì˜ êµ¬ì¡° ì¸ì‹ ë° ë‹¤ì–‘í•œ ì˜ˆì œ ì„ íƒì˜ ì´ì ì„ ê°•ì¡°í•œë‹¤.

---

*Generated on 2025-09-22 14:08:13*