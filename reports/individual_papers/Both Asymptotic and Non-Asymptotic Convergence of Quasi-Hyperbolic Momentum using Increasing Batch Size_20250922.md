# Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size

**Korean Title:** ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€ì˜ ì ê·¼ì  ë° ë¹„ì ê·¼ì  ìˆ˜ë ´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Increasing Batch Size|Increasing Batch Size]] [[keywords/specific/Stochastic Nonconvex Optimization|Stochastic Nonconvex Optimization]] [[keywords/broad/Deep Neural Networks|Deep Neural Networks]] [[keywords/unique/Quasi-Hyperbolic Momentum|Quasi-Hyperbolic Momentum]] [[categories/cs.LG|cs.LG]] [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (84.1% similar) [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (83.0% similar) [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Increasing Batch Size Strategy
**ğŸ”— Specific Connectable**: Mini-batch Gradient Descent
**ğŸ”¬ Broad Technical**: Stochastic Optimization, Deep Neural Networks
**â­ Unique Technical**: Quasi-Hyperbolic Momentum
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (84.1% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (83.0% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (82.2% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.5% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (80.8% similar)


**ArXiv ID**: [2506.23544](https://arxiv.org/abs/2506.23544)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2506.23544.pdf)


**ArXiv ID**: [2506.23544](https://arxiv.org/abs/2506.23544)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2506.23544.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Increasing Batch Size Strategy
**ğŸ”— Specific Connectable**: Mini-batch Gradient Descent
**â­ Unique Technical**: Quasi-Hyperbolic Momentum
**ğŸ”¬ Broad Technical**: Stochastic Optimization, Deep Neural Networks

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Stochastic Optimization` â€¢ 

`Deep Neural Networks` â€¢ 

`Mini-batch Gradient Descent` â€¢ 

`Quasi-Hyperbolic Momentum` â€¢ 

`Increasing Batch Size Strategy`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.23544v3 Announce Type: replace 
Abstract: Momentum methods were originally introduced for their superiority to stochastic gradient descent (SGD) in deterministic settings with convex objective functions. However, despite their widespread application to deep neural networks -- a representative case of stochastic nonconvex optimization -- the theoretical justification for their effectiveness in such settings remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that generalizes various momentum methods and has been studied to better understand the class of momentum-based algorithms as a whole. In this paper, we provide both asymptotic and non-asymptotic convergence results for mini-batch QHM with an increasing batch size. We show that achieving asymptotic convergence requires either a decaying learning rate or an increasing batch size. Since a decaying learning rate adversely affects non-asymptotic convergence, we demonstrate that using mini-batch QHM with an increasing batch size -- without decaying the learning rate -- can be a more effective strategy. Our experiments show that even a finite increase in batch size can provide benefits for training neural networks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.23544v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëª¨ë©˜í…€ ë°©ë²•ì€ ì›ë˜ ê²°ì •ë¡ ì  í™˜ê²½ì—ì„œ ë³¼ë¡ ëª©ì  í•¨ìˆ˜ì— ëŒ€í•´ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ì´ìœ ë¡œ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¬ì¸µ ì‹ ê²½ë§ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì ìš©ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŠ” í™•ë¥ ì  ë¹„ë³¼ë¡ ìµœì í™”ì˜ ëŒ€í‘œì ì¸ ì‚¬ë¡€ë¡œ, ì´ëŸ¬í•œ í™˜ê²½ì—ì„œì˜ íš¨ê³¼ì— ëŒ€í•œ ì´ë¡ ì  ì •ë‹¹ì„±ì€ ì—¬ì „íˆ ì œí•œì ì…ë‹ˆë‹¤. ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€(QHM)ì€ ë‹¤ì–‘í•œ ëª¨ë©˜í…€ ë°©ë²•ì„ ì¼ë°˜í™”í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ëª¨ë©˜í…€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ì „ì²´ í´ë˜ìŠ¤ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ì—°êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì¦ê°€í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì— ëŒ€í•œ ì ê·¼ì  ë° ë¹„ì ê·¼ì  ìˆ˜ë ´ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì ê·¼ì  ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ë ¤ë©´ í•™ìŠµë¥ ì„ ê°ì†Œì‹œí‚¤ê±°ë‚˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¦ê°€ì‹œì¼œì•¼ í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•™ìŠµë¥  ê°ì†ŒëŠ” ë¹„ì ê·¼ì  ìˆ˜ë ´ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì—, í•™ìŠµë¥ ì„ ê°ì†Œì‹œí‚¤ì§€ ì•Šê³  ë°°ì¹˜ í¬ê¸°ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¸ ì „ëµì´ ë  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ë°°ì¹˜ í¬ê¸°ì˜ ìœ í•œí•œ ì¦ê°€ë§Œìœ¼ë¡œë„ ì‹ ê²½ë§ í›ˆë ¨ì— ì´ì ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëª¨ë©˜í…€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜í™”ëœ í˜•íƒœì¸ ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€(QHM)ì„ ì—°êµ¬í•˜ì—¬, í™•ë¥ ì  ë¹„ë³¼ë¡ ìµœì í™” ë¬¸ì œì—ì„œì˜ íš¨ê³¼ì„±ì„ ì´ë¡ ì ìœ¼ë¡œ ë’·ë°›ì¹¨í•˜ê³ ì í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì˜ ìˆ˜ë ´ ê²°ê³¼ë¥¼ ì œì‹œí•˜ë©°, ë¹„ê°ì†Œ í•™ìŠµë¥ ê³¼ í•¨ê»˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ë¹„ë¹„ê°ì†Œ ìˆ˜ë ´ì— ìœ ë¦¬í•¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë°°ì¹˜ í¬ê¸°ì˜ ìœ í•œí•œ ì¦ê°€ë§Œìœ¼ë¡œë„ ì‹ ê²½ë§ í›ˆë ¨ì— ì´ì ì´ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëª¨ë©˜í…€ ë°©ë²•ì€ ì›ë˜ ê²°ì •ë¡ ì  í™˜ê²½ì—ì„œ SGDë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ì†Œê°œë˜ì—ˆìœ¼ë‚˜, ë¹„ê²°ì •ë¡ ì  ë¹„ë³¼ë¡ ìµœì í™” í™˜ê²½ì—ì„œì˜ ì´ë¡ ì  ê·¼ê±°ëŠ” ì œí•œì ì´ë‹¤.

- 2. ì¿¼ì‹œ-í•˜ì´í¼ë³¼ë¦­ ëª¨ë©˜í…€(QHM)ì€ ë‹¤ì–‘í•œ ëª¨ë©˜í…€ ë°©ë²•ì„ ì¼ë°˜í™”í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ëª¨ë©˜í…€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ì „ì²´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì—°êµ¬ë˜ì—ˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì˜ ì ê·¼ì  ë° ë¹„ì ê·¼ì  ìˆ˜ë ´ ê²°ê³¼ë¥¼ ì œì‹œí•œë‹¤.

- 4. ì ê·¼ì  ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ë ¤ë©´ í•™ìŠµë¥  ê°ì†Œ ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ í•„ìš”í•˜ë©°, í•™ìŠµë¥  ê°ì†ŒëŠ” ë¹„ì ê·¼ì  ìˆ˜ë ´ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼, ë°°ì¹˜ í¬ê¸°ì˜ ìœ í•œí•œ ì¦ê°€ë§Œìœ¼ë¡œë„ ì‹ ê²½ë§ í›ˆë ¨ì— ì´ì ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-22 16:00:07*