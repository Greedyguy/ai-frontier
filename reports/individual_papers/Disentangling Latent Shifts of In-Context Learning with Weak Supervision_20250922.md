# Disentangling Latent Shifts of In-Context Learning with Weak Supervision

**Korean Title:** ì ì¬ì  ë³€í™”ì˜ ë§¥ë½ ë‚´ í•™ìŠµì„ ì•½í•œ ì§€ë„ í•™ìŠµìœ¼ë¡œ ë¶„ë¦¬í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Pseudo-label Correction|Pseudo-label Correction]] [[keywords/specific/Few-shot Learning|Few-shot Learning]] [[keywords/specific/Weak Supervision|Weak Supervision]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/unique/Disentangled Latent Shifts|Disentangled Latent Shifts]] [[categories/cs.LG|cs.LG]] [[2025-09-22/KITE_ Kernelized and Information Theoretic Exemplars for In-Context Learning_20250922|KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning]] (85.6% similar) [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (81.4% similar) [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release: Iterative LLM Unlearning with Self-generated Data]] (80.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Pseudo-label Correction
**ğŸ”— Specific Connectable**: Few-shot Learning, Weak Supervision
**ğŸ”¬ Broad Technical**: Large Language Models
**â­ Unique Technical**: Disentangled Latent Shifts
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KITE_ Kernelized and Information Theoretic Exemplars for In-Context Learning_20250922|KITE Kernelized and Information Theoretic Exemplars for In-Context Learning]] (85.6% similar)
- [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (81.4% similar)
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (80.9% similar)
- [[2025-09-22/Global Pre-fixing, Local Adjusting_ A Simple yet Effective Contrastive Strategy for Continual Learning_20250922|Global Pre-fixing, Local Adjusting A Simple yet Effective Contrastive Strategy for Continual Learning]] (80.4% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (80.1% similar)


**ArXiv ID**: [2410.01508](https://arxiv.org/abs/2410.01508)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2410.01508.pdf)


**ArXiv ID**: [2410.01508](https://arxiv.org/abs/2410.01508)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2410.01508.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Weak-to-strong Generalization
**ğŸ”— Specific Connectable**: Few-shot Learning, Weak Supervision
**â­ Unique Technical**: Demonstration-induced Latent Shifts
**ğŸ”¬ Broad Technical**: Large Language Models

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Few-shot Learning` â€¢ 

`Weak Supervision` â€¢ 

`Disentangled Latent Shifts` â€¢ 

`Pseudo-label Correction`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.01508v2 Announce Type: replace-cross 
Abstract: In-context learning (ICL) enables large language models to perform few-shot learning by conditioning on labeled examples in the prompt. Despite its flexibility, ICL suffers from instability -- especially as prompt length increases with more demonstrations. To address this, we treat ICL as a source of weak supervision and propose a parameter-efficient method that disentangles demonstration-induced latent shifts from those of the query. An ICL-based teacher generates pseudo-labels on unlabeled queries, while a student predicts them using only the query input, updating a lightweight adapter. This captures demonstration effects in a compact, reusable form, enabling efficient inference while remaining composable with new demonstrations. Although trained on noisy teacher outputs, the student often outperforms its teacher through pseudo-label correction and coverage expansion, consistent with the weak-to-strong generalization effect. Empirically, our method improves generalization, stability, and efficiency across both in-domain and out-of-domain tasks, surpassing standard ICL and prior disentanglement methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2410.01508v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ë§¥ë½ ë‚´ í•™ìŠµ(ICL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ì— ë ˆì´ë¸”ì´ ìˆëŠ” ì˜ˆì œë¥¼ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬ ì†Œìˆ˜ ìƒ· í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ê·¸ ìœ ì—°ì„±ì—ë„ ë¶ˆêµ¬í•˜ê³ , ICLì€ ë¶ˆì•ˆì •ì„± ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. íŠ¹íˆ, ë” ë§ì€ ì‹œì—°ê³¼ í•¨ê»˜ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ê·¸ë ‡ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ICLì„ ì•½í•œ ê°ë…ì˜ ì›ì²œìœ¼ë¡œ ê°„ì£¼í•˜ê³ , ì‹œì—°ì— ì˜í•´ ìœ ë„ëœ ì ì¬ì  ë³€í™”ë¥¼ ì¿¼ë¦¬ì˜ ë³€í™”ì™€ ë¶„ë¦¬í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ICL ê¸°ë°˜ì˜ êµì‚¬ëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ì¿¼ë¦¬ì— ëŒ€í•´ ì˜ì‚¬ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê³ , í•™ìƒì€ ì¿¼ë¦¬ ì…ë ¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ì˜ˆì¸¡í•˜ë©° ê°€ë²¼ìš´ ì–´ëŒ‘í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ëŠ” ì‹œì—° íš¨ê³¼ë¥¼ ê°„ê²°í•˜ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ í¬ì°©í•˜ì—¬ ìƒˆë¡œìš´ ì‹œì—°ê³¼ ê²°í•© ê°€ëŠ¥í•˜ë©´ì„œë„ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë¹„ë¡ êµì‚¬ì˜ ì¶œë ¥ì´ ë…¸ì´ì¦ˆê°€ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, í•™ìƒì€ ì¢…ì¢… ì˜ì‚¬ ë ˆì´ë¸” ìˆ˜ì • ë° ë²”ìœ„ í™•ì¥ì„ í†µí•´ êµì‚¬ë¥¼ ëŠ¥ê°€í•˜ë©°, ì´ëŠ” ì•½í•œ ê°ë…ì—ì„œ ê°•í•œ ì¼ë°˜í™” íš¨ê³¼ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤. ê²½í—˜ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ë„ë©”ì¸ ë‚´ ë° ë„ë©”ì¸ ì™¸ ì‘ì—… ëª¨ë‘ì—ì„œ ì¼ë°˜í™”, ì•ˆì •ì„± ë° íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ì—¬ í‘œì¤€ ICL ë° ì´ì „ì˜ ë¶„ë¦¬ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë¶ˆì•ˆì •ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì•½í•œ ê°ë…ìœ¼ë¡œì„œì˜ ë§¥ë½ ë‚´ í•™ìŠµ(ICL)ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ICLì€ í”„ë¡¬í”„íŠ¸ì— ë ˆì´ë¸”ì´ ìˆëŠ” ì˜ˆì‹œë¥¼ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬ ì†Œìˆ˜ì˜ ìƒ˜í”Œë¡œ í•™ìŠµì„ ìˆ˜í–‰í•˜ì§€ë§Œ, ì‹œì—°ì´ ë§ì•„ì§ˆìˆ˜ë¡ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ICLì„ ì•½í•œ ê°ë…ì˜ ì›ì²œìœ¼ë¡œ ë³´ê³ , ì‹œì—°ì— ì˜í•´ ìœ ë„ëœ ì ì¬ì  ë³€í™”ë¥¼ ì¿¼ë¦¬ì˜ ë³€í™”ì™€ ë¶„ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ICL ê¸°ë°˜ì˜ êµì‚¬ê°€ ë ˆì´ë¸”ì´ ì—†ëŠ” ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì§œ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê³ , í•™ìƒ ëª¨ë¸ì€ ê²½ëŸ‰ ì–´ëŒ‘í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¿¼ë¦¬ ì…ë ¥ë§Œìœ¼ë¡œ ì´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‹œì—°ì˜ íš¨ê³¼ë¥¼ ê°„ê²°í•˜ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ìº¡ì²˜í•˜ì—¬ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì¼ë°˜í™”, ì•ˆì •ì„±, íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ë©°, í‘œì¤€ ICL ë° ê¸°ì¡´ì˜ ë¶„ë¦¬ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. In-context learning(ICL)ì€ í”„ë¡¬í”„íŠ¸ ë‚´ ë ˆì´ë¸”ì´ ìˆëŠ” ì˜ˆì‹œë¥¼ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ì†Œìˆ˜ì˜ ìƒ· í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

- 2. ICLì€ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ë¶ˆì•ˆì •ì„±ì´ ì¦ê°€í•˜ëŠ” ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤.

- 3. ì œì•ˆëœ ë°©ë²•ì€ ICLì„ ì•½í•œ ê°ë…ì˜ ì›ì²œìœ¼ë¡œ ë³´ê³ , ì‹œì—°ì— ì˜í•´ ìœ ë„ëœ ì ì¬ì  ë³€í™”ë¥¼ ì¿¼ë¦¬ì˜ ë³€í™”ì™€ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.

- 4. ICL ê¸°ë°˜ì˜ êµì‚¬ê°€ ë ˆì´ë¸”ì´ ì—†ëŠ” ì¿¼ë¦¬ì— ëŒ€í•´ ì˜ì‚¬ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê³ , í•™ìƒì€ ì¿¼ë¦¬ ì…ë ¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ì˜ˆì¸¡í•˜ë©° ê²½ëŸ‰ ì–´ëŒ‘í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ í‘œì¤€ ICLê³¼ ì´ì „ì˜ ë¶„ë¦¬ ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©°, ì¼ë°˜í™”, ì•ˆì •ì„±, íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:07:40*