# Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models

**Korean Title:** ë™ì¼í•œ ìµœì í™” ì½”ì¸ì˜ ë‘ ê°€ì§€ ì¸¡ë©´: ê·¸ë˜í”„ ê¸°ì´ˆ ëª¨ë¸ì—ì„œì˜ ëª¨ë¸ ì—´í™”ì™€ í‘œí˜„ ë¶•ê´´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Edge-wise Semantic Fusion|Edge-wise Semantic Fusion]] [[keywords/specific/Domain Generalization|Domain Generalization]] [[keywords/specific/Graph VQ MAE|Graph VQ MAE]] [[keywords/broad/Graph Foundation Models|Graph Foundation Models]] [[keywords/unique/MoT|MoT]] [[categories/cs.LG|cs.LG]] [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (82.8% similar) [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.6% similar) [[2025-09-22/Advances in Multimodal Adaptation and Generalization_ From Traditional Approaches to Foundation Models_20250922|Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Edge-wise Semantic Fusion
**ğŸ”— Specific Connectable**: Domain Generalization
**ğŸ”¬ Broad Technical**: Graph Foundation Models
**â­ Unique Technical**: MoT
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (82.8% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.6% similar)
- [[2025-09-22/Advances in Multimodal Adaptation and Generalization_ From Traditional Approaches to Foundation Models_20250922|Advances in Multimodal Adaptation and Generalization From Traditional Approaches to Foundation Models]] (82.3% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (81.8% similar)
- [[2025-09-17/Class-invariant Test-Time Augmentation for Domain Generalization_20250917|Class-invariant Test-Time Augmentation for Domain Generalization]] (81.5% similar)


**ArXiv ID**: [2509.08401](https://arxiv.org/abs/2509.08401)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.08401.pdf)


**ArXiv ID**: [2509.08401](https://arxiv.org/abs/2509.08401)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.08401.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Discrete Embedding Spaces
**ğŸ”— Specific Connectable**: Domain Generalization, Semantic Fusion
**â­ Unique Technical**: MoT
**ğŸ”¬ Broad Technical**: Graph Foundation Models

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Graph Foundation Models` â€¢ 

`Domain Generalization` â€¢ 

`Graph Neural Network` â€¢ 

`Graph VQ-MAE` â€¢ 

`MoT`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.08401v4 Announce Type: replace 
Abstract: Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.08401v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: LLMì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„, GFMì€ ë‹¤ì¤‘ ë„ë©”ì¸ í…ìŠ¤íŠ¸ ì†ì„± ê·¸ë˜í”„ë¡œë¶€í„° ìµœì ì˜ ì„ë² ë”© í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í¬ë¡œìŠ¤ íƒœìŠ¤í¬ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê°–ì¶”ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì¤‘ì—ì„œ, ê·¸ë˜í”„ VQ-MAEëŠ” GFMì˜ ì ì  ë” ë‹¤ì–‘í•´ì§€ëŠ” í™˜ê²½ì—ì„œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ ë„ë©”ì¸ì˜ í† í´ë¡œì§€ì™€ í…ìŠ¤íŠ¸ ì†ì„±ì„ ëª…í™•í•œ ì˜ë¯¸ ê²½ê³„ë¥¼ ê°€ì§„ ì´ì‚° ì„ë² ë”© ê³µê°„ìœ¼ë¡œ í•¨ê»˜ ì¸ì½”ë”©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì— ê¸°ì¸í•©ë‹ˆë‹¤. ê·¸ ì ì¬ë ¥ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë„ë©”ì¸ ì¼ë°˜í™” ì¶©ëŒì€ ë¯¸ì„¸í•œ í•¨ì •ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ êµ¬ì²´í™”í•˜ë©°, ì´ëŠ” GFM ìµœì í™”ì˜ ì–‘ë©´ê³¼ ê°™ìŠµë‹ˆë‹¤ - ì¸¡ë©´ 1 ëª¨ë¸ ì—´í™”: ì¸ì½”ë”ì™€ ì½”ë“œë¶ì´ ì…ë ¥ì˜ ë‹¤ì–‘ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•¨; ì¸¡ë©´ 2 í‘œí˜„ ë¶•ê´´: ìˆ¨ê²¨ì§„ ì„ë² ë”©ê³¼ ì½”ë“œë¶ ë²¡í„°ê°€ ì¢ì€ í‘œí˜„ í•˜ìœ„ ê³µê°„ì˜ ì œì•½ìœ¼ë¡œ ì¸í•´ ì˜ë¯¸ì  ë¶„ë¦¬ì„±ì„ ìœ ì§€í•˜ì§€ ëª»í•¨. ì´ ë‘ ê°€ì§€ í•¨ì •(ì¸¡ë©´)ì€ ë””ì½”ë”ë¥¼ ì§‘í•©ì ìœ¼ë¡œ ì†ìƒì‹œí‚¤ê³  ì €í’ˆì§ˆì˜ ì¬êµ¬ì„±ëœ ê°ë…ì„ ìƒì„±í•˜ì—¬ ì‚¬ì „ í•™ìŠµ ì¤‘ GFM ìµœì í™” ë”œë ˆë§ˆ(ë™ì „)ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ê²½í—˜ì  ì¡°ì‚¬ë¥¼ í†µí•´, ìš°ë¦¬ëŠ” ìœ„ì˜ ë¬¸ì œë¥¼ ì •ë³´ ë³‘ëª© í˜„ìƒê³¼ ì •ê·œí™” ê²°í•ì— ê¸°ì¸í•œë‹¤ê³  ë´…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” MoTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤ - (1) ë‘ ê°€ì§€ í•¨ì •ì„ ìœ„í•œ ì •ë³´ ì¡°ì •, ì´ëŠ” ì—£ì§€ ê¸°ë°˜ ì˜ë¯¸ ìœµí•© ì „ëµê³¼ ë„ë©”ì¸ ì¸ì‹ ë¼ìš°íŒ…ì„ ê°–ì¶˜ ì½”ë“œë¶ í˜¼í•©ì„ í™œìš©í•˜ì—¬ ì •ë³´ ìš©ëŸ‰ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. (2) ìµœì í™” ë™ì „ì„ ìœ„í•œ ì •ê·œí™” ì¡°ì •, ì´ëŠ” ì œì•ˆëœ ì •ë³´ ì¡°ì •ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê°ë…ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì¶”ê°€ ì •ê·œí™”ë¥¼ í™œìš©í•©ë‹ˆë‹¤. íŠ¹íˆ, ìœ ì—°í•œ ì•„í‚¤í…ì²˜ë¡œì„œ MoTëŠ” GFMì˜ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ ì œì–´ ê°€ëŠ¥í•œ ëª¨ë¸ ê·œëª¨ë¥¼ ì œê³µí•©ë‹ˆë‹¤. SOTA ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬, 6ê°œ ë„ë©”ì¸ì— ê±¸ì¹œ 22ê°œì˜ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ì€ MoTê°€ ê°ë…, ì†Œìˆ˜ ìƒ·, ì œë¡œ ìƒ· ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ LLMì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„, ë‹¤ì¤‘ ë„ë©”ì¸ í…ìŠ¤íŠ¸ ì†ì„± ê·¸ë˜í”„ì—ì„œ ìµœì ì˜ ì„ë² ë”© í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì— ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” GFMì„ ì„¤ê³„í•©ë‹ˆë‹¤. íŠ¹íˆ, ê·¸ë˜í”„ VQ-MAEëŠ” ì—¬ëŸ¬ ë„ë©”ì¸ì˜ í† í´ë¡œì§€ì™€ í…ìŠ¤íŠ¸ ì†ì„±ì„ ëª…í™•í•œ ì˜ë¯¸ ê²½ê³„ë¥¼ ê°€ì§„ ì´ì‚° ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë„ë©”ì¸ ì¼ë°˜í™”ì˜ ë¬¸ì œë¡œ ì¸í•´ ëª¨ë¸ ì—´í™”ì™€ í‘œí˜„ ë¶•ê´´ë¼ëŠ” ë‘ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì •ë³´ ë³‘ëª© í˜„ìƒê³¼ ê·œì œ ë¶€ì¡±ì„ ì›ì¸ìœ¼ë¡œ ì§€ëª©í•˜ê³ , MoTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MoTëŠ” ë‘ ê°€ì§€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì •ë³´ ì¡°ì • ì „ëµê³¼ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¶”ê°€ ê·œì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MoTëŠ” 6ê°œ ë„ë©”ì¸ 22ê°œ ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Graph VQ-MAEëŠ” ë‹¤ì¤‘ ë„ë©”ì¸ì˜ í…ìŠ¤íŠ¸ ì†ì„±ì„ ëª…í™•í•œ ì˜ë¯¸ ê²½ê³„ë¥¼ ê°€ì§„ ì´ì‚° ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

- 2. ë„ë©”ì¸ ì¼ë°˜í™”ì˜ í•œê³„ë¡œ ì¸í•´ ëª¨ë¸ ì—´í™”ì™€ í‘œí˜„ ë¶•ê´´ë¼ëŠ” ë‘ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí•˜ë©°, ì´ëŠ” GFM ìµœì í™” ë”œë ˆë§ˆë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤.

- 3. ì •ë³´ ë³‘ëª© í˜„ìƒê³¼ ì •ê·œí™” ë¶€ì¡±ì´ ì´ëŸ¬í•œ ë¬¸ì œì˜ ì›ì¸ìœ¼ë¡œ ì§€ëª©ë˜ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ MoTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 4. MoTëŠ” ì •ë³´ ìš©ëŸ‰ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì—£ì§€ ê¸°ë°˜ ì˜ë¯¸ ìœµí•© ì „ëµê³¼ ë„ë©”ì¸ ì¸ì‹ ë¼ìš°íŒ…ì„ í™œìš©í•œ ì½”ë“œë¶ í˜¼í•©ì„ í¬í•¨í•©ë‹ˆë‹¤.

- 5. MoTëŠ” 6ê°œ ë„ë©”ì¸ì— ê±¸ì¹œ 22ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ SOTA ê¸°ì¤€ì„ ì´ˆê³¼í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°, ìœ ì—°í•œ ì•„í‚¤í…ì²˜ë¡œì„œ GFMì˜ í™•ì¥ ë²•ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:03:24*