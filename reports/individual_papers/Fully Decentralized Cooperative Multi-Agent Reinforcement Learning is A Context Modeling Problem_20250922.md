# Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem

**Korean Title:** ì™„ì „ íƒˆì¤‘ì•™í™”ëœ í˜‘ë ¥ì  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì€ ë§¥ë½ ëª¨ë¸ë§ ë¬¸ì œì´ë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Decentralized Cooperative Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity_20250919|Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity]] (85.7% similar)
- [[2025-09-19/Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention_20250919|Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention]] (82.8% similar)
- [[2025-09-19/CRAFT_ Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks_20250919|CRAFT Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks]] (82.7% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.4% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (82.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15519v1 Announce Type: new 
Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15519v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ ê° ì—ì´ì „íŠ¸ê°€ ìƒíƒœ, ìì‹ ì˜ ì§€ì—­ì  í–‰ë™ ë° ê³µìœ  ë³´ìƒë§Œì„ ê´€ì°°í•˜ëŠ” ì™„ì „ ë¶„ì‚° í˜‘ë ¥ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ì—°êµ¬í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²ƒì€ ì¢…ì¢… ê°€ì¹˜ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì‹œ ë¹„ì •ìƒì„±ì„ ì´ˆë˜í•˜ê³  ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì • ì‹œ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ë¥¼ ì´ˆë˜í•˜ì—¬ íš¨ê³¼ì ì¸ í˜‘ë ¥ ì •ì±… í•™ìŠµì„ ë°©í•´í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì™„ì „ ë¶„ì‚° ì„¤ì •ì—ì„œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê³µë™ ì •ì±…ì„ ëª¨ë¸ë§í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Dynamics-Aware Context (DAC)ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ê° ì—ì´ì „íŠ¸ê°€ ì§€ì—­ì ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ì‘ì—…ì„ ë¬¸ë§¥ì  ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •ìœ¼ë¡œ í˜•ì‹í™”í•˜ê³ , ë™ì  ì¸ì‹ ë¬¸ë§¥ ëª¨ë¸ë§ì„ í†µí•´ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ ëª¨ë‘ í•´ê²°í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, DACëŠ” ê° ì—ì´ì „íŠ¸ì˜ ë¹„ì •ìƒì ì¸ ì§€ì—­ ì‘ì—… ë™íƒœë¥¼ ê´€ì°°ë˜ì§€ ì•Šì€ ë¬¸ë§¥ ê°„ì˜ ì „í™˜ìœ¼ë¡œ ê·€ì†ì‹œí‚¤ë©°, ê° ë¬¸ë§¥ì€ ë…íŠ¹í•œ ê³µë™ ì •ì±…ì— í•´ë‹¹í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ DACëŠ” ì ì¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê³„ë³„ ë™íƒœ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³  ì´ë¥¼ ë¬¸ë§¥ìœ¼ë¡œ ì§€ì¹­í•©ë‹ˆë‹¤. ê° ì—ì´ì „íŠ¸ì— ëŒ€í•´ DACëŠ” ê°€ì¹˜ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì‹œ ë¹„ì •ìƒì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¬¸ë§¥ ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì •ì„ ìœ„í•´, í˜‘ë ¥ì  í–‰ë™ ì„ íƒì„ ì´‰ì§„í•˜ê¸° ìœ„í•œ ë‚™ê´€ì  í•œê³„ ê°€ì¹˜ë¥¼ ë„ì¶œí•˜ì—¬ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ í˜‘ë ¥ ì‘ì—…(í–‰ë ¬ ê²Œì„, í¬ì‹ìì™€ ë¨¹ì´, SMAC í¬í•¨)ì—ì„œ DACë¥¼ í‰ê°€í•˜ì˜€ìœ¼ë©°, ì—¬ëŸ¬ ê¸°ì¤€ì„ ì— ëŒ€í•œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì€ ê·¸ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì™„ì „í•œ ë¶„ì‚°í˜• í˜‘ë ¥ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ì—°êµ¬í•˜ë©°, ê° ì—ì´ì „íŠ¸ëŠ” ìƒíƒœ, ë¡œì»¬ í–‰ë™, ê³µìœ  ë³´ìƒë§Œì„ ê´€ì°°í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ì•Œ ìˆ˜ ì—†ì–´ ê°€ì¹˜ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì‹œ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ê°€ ë°œìƒí•˜ì—¬ í˜‘ë ¥ ì •ì±… í•™ìŠµì´ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ëŠ” 'Dynamics-Aware Context (DAC)'ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. DACëŠ” ê° ì—ì´ì „íŠ¸ê°€ ì¸ì‹í•˜ëŠ” ê³¼ì œë¥¼ ë§¥ë½ì  ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •ìœ¼ë¡œ ê³µì‹í™”í•˜ê³ , ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. DACëŠ” ìˆ¨ê²¨ì§„ ë§¥ë½ì˜ ì „í™˜ìœ¼ë¡œ ë¹„ì •ìƒì  ë¡œì»¬ ê³¼ì œ ë™íƒœë¥¼ ì„¤ëª…í•˜ê³ , ì ì¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë‹¨ê³„ë³„ ë™íƒœ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ê° ì—ì´ì „íŠ¸ì— ëŒ€í•´, DACëŠ” ë¹„ì •ìƒì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§¥ë½ ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í•˜ê³ , í˜‘ë ¥ í–‰ë™ ì„ íƒì„ ì´‰ì§„í•˜ëŠ” ë‚™ê´€ì  í•œê³„ ê°€ì¹˜ë¥¼ ë„ì¶œí•˜ì—¬ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DACëŠ” ë‹¤ì–‘í•œ í˜‘ë ¥ ê³¼ì œì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ì™„ì „í•œ íƒˆì¤‘ì•™í™” í˜‘ë ¥ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ì—°êµ¬í•˜ë©°, ê° ì—ì´ì „íŠ¸ëŠ” ìƒíƒœ, ì§€ì—­ì  í–‰ë™, ê³µìœ  ë³´ìƒë§Œì„ ê´€ì°°í•©ë‹ˆë‹¤.

- 2. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì™„ì „í•œ íƒˆì¤‘ì•™í™” ì„¤ì •ì—ì„œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê³µë™ ì •ì±…ì„ ëª¨ë¸ë§í•˜ì§€ ëª»í•´ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ì§€ ëª»í•©ë‹ˆë‹¤.

- 3. ì œì•ˆëœ Dynamics-Aware Context (DAC) ë°©ë²•ì€ ê° ì—ì´ì „íŠ¸ê°€ ì¸ì§€í•˜ëŠ” ê³¼ì œë¥¼ ë§¥ë½ì  ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •ìœ¼ë¡œ í˜•ì‹í™”í•˜ê³ , ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ ë™ì  ì¸ì‹ ë§¥ë½ ëª¨ë¸ë§ì„ í†µí•´ í•´ê²°í•©ë‹ˆë‹¤.

- 4. DACëŠ” ë¹„ì •ìƒì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§¥ë½ ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í•˜ê³ , í˜‘ë ¥ì  í–‰ë™ ì„ íƒì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ë‚™ê´€ì  í•œê³„ ê°€ì¹˜ë¥¼ ë„ì¶œí•˜ì—¬ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

- 5. ë‹¤ì–‘í•œ í˜‘ë ¥ ê³¼ì œì—ì„œ DACì˜ ì‹¤í—˜ì  í‰ê°€ ê²°ê³¼, ì—¬ëŸ¬ ê¸°ì¤€ì„  ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ ê·¸ íš¨ê³¼ì„±ì„ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:18:13*