# Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ìê¸° êµ¬ì„± ì§€ì‹ ì‚¼ì¤‘í•­ì„ í†µí•œ ê°œë… í•™ìŠµ ì œê±°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Machine Unlearning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (86.5% similar)
- [[2025-09-18/CUFG_ Curriculum Unlearning Guided by the Forgetting Gradient_20250918|CUFG Curriculum Unlearning Guided by the Forgetting Gradient]] (85.1% similar)
- [[2025-09-17/Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning_20250917|Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning]] (82.8% similar)
- [[2025-09-22/Pre-Forgettable Models_ Prompt Learning as a Native Mechanism for Unlearning_20250922|Pre-Forgettable Models Prompt Learning as a Native Mechanism for Unlearning]] (82.0% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (81.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15621v1 Announce Type: cross 
Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a solution to privacy and copyright issues in large language models (LLMs). Existing MU methods aim to remove specific target sentences from an LLM while minimizing damage to unrelated knowledge. However, these approaches require explicit target sentences and do not support removing broader concepts, such as persons or events. To address this limitation, we introduce Concept Unlearning (CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to represent the LLM's internal knowledge and define CU as removing the forgetting target nodes and associated edges. This graph-based formulation enables a more intuitive unlearning and facilitates the design of more effective methods. We propose a novel method that prompts the LLM to generate knowledge triplets and explanatory sentences about the forgetting target and applies the unlearning process to these representations. Our approach enables more precise and comprehensive concept removal by aligning the unlearning process with the LLM's internal knowledge representations. Experiments on real-world and synthetic datasets demonstrate that our method effectively achieves concept-level unlearning while preserving unrelated knowledge.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15621v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê¸°ê³„ í•™ìŠµ ì œê±°(Machine Unlearning, MU)ëŠ” ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œì˜ í”„ë¼ì´ë²„ì‹œ ë° ì €ì‘ê¶Œ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ì±…ìœ¼ë¡œ ìƒë‹¹í•œ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ MU ë°©ë²•ì€ íŠ¹ì • ëŒ€ìƒ ë¬¸ì¥ì„ LLMì—ì„œ ì œê±°í•˜ë©´ì„œ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì— ëŒ€í•œ ì†ìƒì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ëª…ì‹œì ì¸ ëŒ€ìƒ ë¬¸ì¥ì„ í•„ìš”ë¡œ í•˜ë©°, ì‚¬ëŒì´ë‚˜ ì‚¬ê±´ê³¼ ê°™ì€ ë” ë„“ì€ ê°œë…ì„ ì œê±°í•˜ëŠ” ê²ƒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” LLM ì œê±°ì— ëŒ€í•œ ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ê°œë… ì œê±°(Concept Unlearning, CU)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ë‚´ë¶€ ì§€ì‹ì„ í‘œí˜„í•˜ê³ , CUë¥¼ ë§ê° ëŒ€ìƒ ë…¸ë“œì™€ ê´€ë ¨ëœ ì—£ì§€ë¥¼ ì œê±°í•˜ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì´ ê·¸ë˜í”„ ê¸°ë°˜ì˜ ê³µì‹í™”ëŠ” ë³´ë‹¤ ì§ê´€ì ì¸ ì œê±°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , ë” íš¨ê³¼ì ì¸ ë°©ë²•ì˜ ì„¤ê³„ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì´ ë§ê° ëŒ€ìƒì— ëŒ€í•œ ì§€ì‹ ì‚¼ì¤‘í•­ê³¼ ì„¤ëª… ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ê³ , ì´ëŸ¬í•œ í‘œí˜„ì— ì œê±° ê³¼ì •ì„ ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ì œê±° ê³¼ì •ì„ LLMì˜ ë‚´ë¶€ ì§€ì‹ í‘œí˜„ê³¼ ì¼ì¹˜ì‹œí‚´ìœ¼ë¡œì¨ ë³´ë‹¤ ì •ë°€í•˜ê³  í¬ê´„ì ì¸ ê°œë… ì œê±°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤ì œ ë° í•©ì„± ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ë°©ë²•ì´ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì„ ë³´ì¡´í•˜ë©´ì„œ ê°œë… ìˆ˜ì¤€ì˜ ì œê±°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ê°œì¸ì •ë³´ ë° ì €ì‘ê¶Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê¸°ê³„ í•™ìŠµ ì œê±°(MU)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì ìƒˆë¡œìš´ ê°œë… ì œê±°(CU) ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ MU ë°©ë²•ì€ íŠ¹ì • ë¬¸ì¥ì„ ì œê±°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì§€ë§Œ, CUëŠ” ì‚¬ëŒì´ë‚˜ ì‚¬ê±´ê³¼ ê°™ì€ ë” ë„“ì€ ê°œë…ì„ ì œê±°í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ë‚´ë¶€ ì§€ì‹ì„ í‘œí˜„í•˜ê³ , ì œê±°í•  ê°œë…ì„ ê·¸ë˜í”„ì˜ ë…¸ë“œì™€ ê°„ì„ ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ LLMì´ ìƒì„±í•œ ì§€ì‹ ì‚¼ì¤‘í•­ê³¼ ì„¤ëª… ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì œê±° ê³¼ì •ì„ ìˆ˜í–‰í•˜ì—¬, ë³´ë‹¤ ì •ë°€í•˜ê³  í¬ê´„ì ì¸ ê°œë… ì œê±°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì„ ë³´ì¡´í•˜ë©´ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ê°œë… ìˆ˜ì¤€ì˜ ì œê±°ë¥¼ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì—ì„œ íŠ¹ì • ë¬¸ì¥ì„ ì œê±°í•˜ëŠ” ê¸°ì¡´ ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ê°œë… í•™ìŠµ ì œê±°(CU)ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.

- 2. CUëŠ” ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ ë‚´ë¶€ ì§€ì‹ì„ í‘œí˜„í•˜ê³ , ì œê±° ëŒ€ìƒ ë…¸ë“œì™€ ê´€ë ¨ëœ ì—£ì§€ë¥¼ ì‚­ì œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤.

- 3. ì œì•ˆëœ ë°©ë²•ì€ ëª¨ë¸ì´ ìŠì–´ì•¼ í•  ëŒ€ìƒì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹ ì‚¼ì¤‘í•­ê³¼ ë¬¸ì¥ì„ ìƒì„±í•˜ê²Œ í•˜ì—¬, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê°œë… ìˆ˜ì¤€ì—ì„œì˜ í•™ìŠµ ì œê±°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©´ì„œë„ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì€ ë³´ì¡´í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:40:37*