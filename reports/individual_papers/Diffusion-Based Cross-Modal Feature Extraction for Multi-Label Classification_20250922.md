# Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification

**Korean Title:** ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ í™•ì‚° ê¸°ë°˜ì˜ êµì°¨ ëª¨ë‹¬ íŠ¹ì§• ì¶”ì¶œ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Cross-modal Feature Extraction

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/DiffCut_ Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut_20250919|DiffCut Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut]] (83.2% similar)
- [[2025-09-17/SpecDiff_ Accelerating Diffusion Model Inference with Self-Speculation_20250917|SpecDiff Accelerating Diffusion Model Inference with Self-Speculation]] (82.1% similar)
- [[2025-09-19/Superpose Task-specific Features for Model Merging_20250919|Superpose Task-specific Features for Model Merging]] (81.3% similar)
- [[2025-09-19/MARIC_ Multi-Agent Reasoning for Image Classification_20250919|MARIC Multi-Agent Reasoning for Image Classification]] (80.3% similar)
- [[2025-09-18/Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model_20250918|Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model]] (80.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15553v1 Announce Type: cross 
Abstract: Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \textit{Diff-Feat}, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious "Layer $12$" consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts. The code is available at https://github.com/lt-0123/Diff-Feat.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15553v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ëŠ” ê´‘ë²”ìœ„í•œ ì‘ìš© ë¶„ì•¼ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ë‹¤ì¤‘ ë ˆì´ë¸” ìƒí˜¸ì‘ìš©ì„ í¬ì°©í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ í‘œí˜„ì— ì˜ì¡´í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì‚¬ì „ í•™ìŠµëœ í™•ì‚°-íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ì¤‘ê°„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ í›„ì† ì‘ì—…ì— ìœµí•©í•˜ëŠ” ê°„ë‹¨í•˜ì§€ë§Œ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì¸ \textit{Diff-Feat}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ë¹„ì „ ì‘ì—…ì˜ ê²½ìš°, í™•ì‚° ê³¼ì •ì—ì„œ ê°€ì¥ ë³€ë³„ë ¥ ìˆëŠ” ì¤‘ê°„ íŠ¹ì§•ì€ ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ë°œìƒí•˜ë©° íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¤‘ê°„ ë¸”ë¡ì— ìœ„ì¹˜í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì–¸ì–´ ì‘ì—…ì˜ ê²½ìš°, ìµœê³ ì˜ íŠ¹ì§•ì€ ì¡ìŒì´ ì—†ëŠ” ë‹¨ê³„ì—ì„œ ë°œìƒí•˜ë©° ê°€ì¥ ê¹Šì€ ë¸”ë¡ì— ìœ„ì¹˜í•©ë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ë†€ë¼ìš´ í˜„ìƒì„ ê´€ì°°í–ˆëŠ”ë°, ì‹ ë¹„ë¡œìš´ "ë ˆì´ì–´ $12$"ê°€ ì´ë¯¸ì§€ì— ëŒ€í•œ ë‹¤ì–‘í•œ í›„ì† ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤ (DiT-XL/2-256$\times$256 í•˜ì—ì„œ). ìš°ë¦¬ëŠ” ëª‡ ê°€ì§€ í›„ë³´ ì¤‘ì—ì„œ "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸"$\times$"ë¸”ë¡-ì‹œê°„ ë‹¨ê³„" ìŒì„ ì§€ì—­ ìµœì í™”í•˜ëŠ” íœ´ë¦¬ìŠ¤í‹± ì§€ì—­ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê³ ì•ˆí•˜ì—¬, ì „ì²´ì ì¸ ê·¸ë¦¬ë“œ íƒìƒ‰ì„ í”¼í•©ë‹ˆë‹¤. ì„ íƒëœ í‘œí˜„ì„ ë‹¨ìˆœ ìœµí•©-ì„ í˜• íˆ¬ì˜ í›„ ë”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤: MS-COCO-í–¥ìƒì—ì„œ 98.6% mAPì™€ Visual Genome 500ì—ì„œ 45.7% mAPë¡œ, ê°•ë ¥í•œ CNN, ê·¸ë˜í”„, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ì¤€ì„ ì„ í¬ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤. t-SNE ë° í´ëŸ¬ìŠ¤í„°ë§ ë©”íŠ¸ë¦­ì€ \textit{Diff-Feat}ê°€ ë‹¨ì¼ ëª¨ë‹¬ ëŒ€ì•ˆë³´ë‹¤ ë” ê¸´ë°€í•œ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•œë‹¤ëŠ” ê²ƒì„ ì¶”ê°€ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/lt-0123/Diff-Featì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ì— íš¨ê³¼ì ì¸ \textit{Diff-Feat} í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ì‚¬ì „ í•™ìŠµëœ í™•ì‚°-íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ì¤‘ê°„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— í™œìš©í•©ë‹ˆë‹¤. ì‹œê°ì  ì‘ì—…ì—ì„œëŠ” ì¤‘ê°„ ë‹¨ê³„ì˜ ì¤‘ê°„ ë¸”ë¡ì—ì„œ ê°€ì¥ êµ¬ë³„ë˜ëŠ” íŠ¹ì§•ì´ ë‚˜íƒ€ë‚˜ë©°, ì–¸ì–´ ì‘ì—…ì—ì„œëŠ” ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ë‹¨ê³„ì˜ ê°€ì¥ ê¹Šì€ ë¸”ë¡ì—ì„œ ìµœì ì˜ íŠ¹ì§•ì´ ë°œê²¬ë©ë‹ˆë‹¤. íŠ¹íˆ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ "Layer 12"ê°€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì§€ì—­ ìµœì ì˜ "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸"ì™€ "ë¸”ë¡-ì‹œê°„ ë‹¨ê³„" ìŒì„ ì°¾ê¸° ìœ„í•œ íœ´ë¦¬ìŠ¤í‹± íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì„ íƒëœ í‘œí˜„ì˜ ë‹¨ìˆœí•œ ìœµí•©-ì„ í˜• íˆ¬ì˜ê³¼ ì¶”ê°€ëŠ” MS-COCO-enhancedì—ì„œ 98.6% mAP, Visual Genome 500ì—ì„œ 45.7% mAPë¥¼ ê¸°ë¡í•˜ë©° ê¸°ì¡´ì˜ CNN, ê·¸ë˜í”„, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. t-SNE ë° í´ëŸ¬ìŠ¤í„°ë§ ì§€í‘œëŠ” \textit{Diff-Feat}ê°€ ë‹¨ì¼ ëª¨ë‹¬ë³´ë‹¤ ë” ë°€ì§‘ëœ ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. \textit{Diff-Feat}ëŠ” ì‚¬ì „ í•™ìŠµëœ í™•ì‚°-Transformer ëª¨ë¸ì—ì„œ ì¤‘ê°„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê°•ë ¥í•œ í‘œí˜„ì„ ì œê³µí•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 2. ì‹œê°ì  ì‘ì—…ì—ì„œëŠ” ì¤‘ê°„ ë‹¨ê³„ì™€ Transformerì˜ ì¤‘ê°„ ë¸”ë¡ì—ì„œ ê°€ì¥ êµ¬ë³„ë ¥ ìˆëŠ” íŠ¹ì§•ì´ ë‚˜íƒ€ë‚˜ë©°, ì–¸ì–´ ì‘ì—…ì—ì„œëŠ” ì¡ìŒì´ ì—†ëŠ” ë‹¨ê³„ì™€ ê°€ì¥ ê¹Šì€ ë¸”ë¡ì—ì„œ ìµœìƒì˜ íŠ¹ì§•ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

- 3. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ "Layer 12"ê°€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” í˜„ìƒì´ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.

- 4. ì§€ì—­ ìµœì ì˜ "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸"ì™€ "ë¸”ë¡-íƒ€ì„ìŠ¤í…" ìŒì„ ì°¾ê¸° ìœ„í•œ íœ´ë¦¬ìŠ¤í‹± ë¡œì»¬ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬, ì „ì²´ ê·¸ë¦¬ë“œ íƒìƒ‰ì„ í”¼í–ˆìŠµë‹ˆë‹¤.

- 5. ì„ íƒëœ í‘œí˜„ì˜ ê°„ë‹¨í•œ ìœµí•©-ì„ í˜• íˆ¬ì˜ê³¼ ì¶”ê°€ë¥¼ í†µí•´ MS-COCO-enhancedì—ì„œ 98.6% mAP, Visual Genome 500ì—ì„œ 45.7% mAPë¥¼ ê¸°ë¡í•˜ë©°, ê¸°ì¡´ì˜ ê°•ë ¥í•œ CNN, ê·¸ë˜í”„, Transformer ê¸°ë°˜ì„ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:01:30*