
# Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems

**Korean Title:** 보이지만 읽을 수 없는: 서체 시스템을 통해 시각 언어 모델의 체계적인 시각적 블라인드 스팟

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Multimodal Systems|Multimodal Systems]] [[keywords/broad/Vision Language Models|Vision Language Models]] [[keywords/broad/Psychophysics|Psychophysics]] [[keywords/specific/Symbol Segmentation|Symbol Segmentation]] [[keywords/unique/Visible but Unreadable Stimuli|Visible but Unreadable Stimuli]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Multimodal Systems
**🔬 Broad Technical**: Vision Language Models, Psychophysics
**🔗 Specific Connectable**: Symbol Segmentation
**⭐ Unique Technical**: Visible but Unreadable Stimuli

**ArXiv ID**: [2509.06996](https://arxiv.org/abs/2509.06996)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.06996.pdf)


## 🏷️ 추출된 키워드



`Vision Language Models` • 

`Psychophysics` • 

`Symbol Segmentation` • 

`Visible but Unreadable Stimuli` • 

`Multimodal Systems`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.06996v2 Announce Type: replace-cross 
Abstract: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.

## 🔍 Abstract (한글 번역)

arXiv:2509.06996v2 발표 유형: 대체-교차
요약: 쓰기는 상징적 커뮤니케이션을 위해 시각을 재활용하는 보편적 문화 기술입니다. 인간은 뛰어난 탄력성을 보여줍니다: 문자가 파편화되거나 융합되거나 부분적으로 가려져 있어도 우리는 쉽게 단어를 인식합니다. 본 논문은 고급 시각 언어 모델(VLMs)이 이러한 탄력성을 공유하는지 조사합니다. 우리는 중국 로고그래프와 영어 알파벳 단어의 서로 다른 쓰기 시스템을 대상으로, 모델에게는 ''가시적이지만 읽을 수 없는'' 자극을 만들어내기 위해 글리프를 잘라내고 재결합하고 겹쳐놓는 두 가지 심리물리학적 영감을 받은 벤치마크를 구축합니다. 깨끗한 텍스트에서 뛰어난 성능을 보이더라도, 현대 VLMs는 이러한 변형에 대해 심각한 하락을 보이며 종종 관련 없거나 일관성 없는 결과물을 생성합니다. 이 패턴은 구조적 한계를 시사합니다: 모델은 일반적인 시각 불변성을 강하게 활용하지만 견고한 문해력을 위해 필요한 구성 우선순위에는 적게 의존합니다. 우리는 투명한 복제와 후속 작업을 촉진하기 위해 자극 생성 코드, 프롬프트 및 평가 프로토콜을 공개합니다. 우리의 연구 결과는 다양한 스크립트 간의 기호 분할, 구성 및 결합을 인코딩하는 아키텍처와 교육, 접근성, 문화 유산 및 보안 분야에서 다중 모달 시스템을 배치하는 데 구체적인 도전 과제를 제시합니다.

## 📝 요약

이 연구는 고급 시각 언어 모델(VLMs)이 문자가 파편화되거나 일부가 가려져 있을 때도 단어를 인식할 수 있는 인간의 강한 탄력성을 모방하는지 조사한다. 중국어 로고그래프와 영어 알파벳 단어를 대상으로 한 두 가지 벤치마크를 구축하여 모델에게는 "보이지만 읽을 수 없는" 자극을 제공한다. 현재 VLMs는 깨끗한 텍스트에서 강력한 성능을 보이지만 이러한 변형에서는 심각한 성능 하락을 보인다. 이러한 패턴은 모델이 일반적인 시각 불변성을 강하게 활용하지만 강력한 문해력을 위해 필요한 구성 사전을 충분히 활용하지 못하는 구조적 한계를 시사한다. 연구 결과는 다양한 분야에서 다중 모달 시스템을 배치하는 데 필요한 구조와 교육, 접근성, 문화유산, 보안 등에 대한 구체적인 과제를 제시한다.

## 🎯 주요 포인트


- 1. 고급 시각 언어 모델(VLMs)은 단어를 인식하는 데 사람들과는 다른 한계를 보인다.

- 2. 현재 VLMs는 시각적 변형에 취약하며 일관된 결과를 내지 못한다.

- 3. 연구 결과는 교육, 접근성, 문화유산 및 보안 분야에서 다중 모달 시스템을 구현하는 데 구체적인 도전 과제를 제시한다.


---

*Generated on 2025-09-18 16:35:28*