# Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models

**Korean Title:** í¬ì†Œ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ë‚´ë¶€ í‘œí˜„ í•™ìŠµ ì œê±° ê¸°ë²•ì„ í™œìš©í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Activation Alignment

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (90.1% similar)
- [[2025-09-22/Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets_20250922|Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets]] (88.1% similar)
- [[2025-09-17/Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning_20250917|Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning]] (84.9% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.1% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know_20250922|Do Retrieval Augmented Language Models Know When They Don't Know]] (83.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15631v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed across various applications, privacy and copyright concerns have heightened the need for more effective LLM unlearning techniques. Many existing unlearning methods aim to suppress undesirable outputs through additional training (e.g., gradient ascent), which reduces the probability of generating such outputs. While such suppression-based approaches can control model outputs, they may not eliminate the underlying knowledge embedded in the model's internal activations; muting a response is not the same as forgetting it. Moreover, such suppression-based methods often suffer from model collapse. To address these issues, we propose a novel unlearning method that directly intervenes in the model's internal activations. In our formulation, forgetting is defined as a state in which the activation of a forgotten target is indistinguishable from that of ``unknown'' entities. Our method introduces an unlearning objective that modifies the activation of the target entity away from those of known entities and toward those of unknown entities in a sparse autoencoder latent space. By aligning the target's internal activation with those of unknown entities, we shift the model's recognition of the target entity from ``known'' to ``unknown'', achieving genuine forgetting while avoiding over-suppression and model collapse. Empirically, we show that our method effectively aligns the internal activations of the forgotten target, a result that the suppression-based approaches do not reliably achieve. Additionally, our method effectively reduces the model's recall of target knowledge in question-answering tasks without significant damage to the non-target knowledge.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15631v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ì  ë” ë§ì´ ë°°í¬ë¨ì— ë”°ë¼, í”„ë¼ì´ë²„ì‹œì™€ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì¸í•´ ë³´ë‹¤ íš¨ê³¼ì ì¸ LLM ìŠê¸° ê¸°ë²•ì˜ í•„ìš”ì„±ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ë§ì€ ìŠê¸° ë°©ë²•ì€ ì¶”ê°€ì ì¸ í›ˆë ¨(ì˜ˆ: ê¸°ìš¸ê¸° ìƒìŠ¹)ì„ í†µí•´ ë°”ëŒì§í•˜ì§€ ì•Šì€ ì¶œë ¥ì„ ì–µì œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ë©°, ì´ëŠ” ê·¸ëŸ¬í•œ ì¶œë ¥ì„ ìƒì„±í•  í™•ë¥ ì„ ì¤„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ ì¶œë ¥ì„ ì œì–´í•  ìˆ˜ ìˆì§€ë§Œ, ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ë‚´ì¬ëœ ê¸°ë³¸ ì§€ì‹ì„ ì œê±°í•˜ì§€ëŠ” ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ë‹µì„ ë¬´ìŒ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ê·¸ê²ƒì„ ìŠëŠ” ê²ƒê³¼ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²Œë‹¤ê°€, ì´ëŸ¬í•œ ì–µì œ ê¸°ë°˜ ë°©ë²•ì€ ì¢…ì¢… ëª¨ë¸ ë¶•ê´´ë¥¼ ê²ªìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ì§ì ‘ ê°œì…í•˜ëŠ” ìƒˆë¡œìš´ ìŠê¸° ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê³µì‹í™”ì—ì„œ, ìŠê¸°ëŠ” ìŠí˜€ì§„ ëŒ€ìƒì˜ í™œì„±í™”ê°€ "ì•Œë ¤ì§€ì§€ ì•Šì€" ì—”í‹°í‹°ì™€ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ìƒíƒœë¡œ ì •ì˜ë©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ í¬ì†Œ ì˜¤í† ì¸ì½”ë” ì ì¬ ê³µê°„ì—ì„œ ì•Œë ¤ì§„ ì—”í‹°í‹°ì˜ í™œì„±í™”ì™€ ì•Œë ¤ì§€ì§€ ì•Šì€ ì—”í‹°í‹°ì˜ í™œì„±í™” ì‚¬ì´ë¡œ ëŒ€ìƒ ì—”í‹°í‹°ì˜ í™œì„±í™”ë¥¼ ìˆ˜ì •í•˜ëŠ” ìŠê¸° ëª©í‘œë¥¼ ë„ì…í•©ë‹ˆë‹¤. ëŒ€ìƒì˜ ë‚´ë¶€ í™œì„±í™”ë¥¼ ì•Œë ¤ì§€ì§€ ì•Šì€ ì—”í‹°í‹°ì˜ í™œì„±í™”ì™€ ì¼ì¹˜ì‹œí‚´ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ëŒ€ìƒ ì—”í‹°í‹°ë¥¼ "ì•Œë ¤ì§„" ìƒíƒœì—ì„œ "ì•Œë ¤ì§€ì§€ ì•Šì€" ìƒíƒœë¡œ ì¸ì‹í•˜ë„ë¡ ì „í™˜í•˜ì—¬ ê³¼ë„í•œ ì–µì œì™€ ëª¨ë¸ ë¶•ê´´ë¥¼ í”¼í•˜ë©´ì„œ ì§„ì •í•œ ìŠê¸°ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì‹¤ì¦ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ìŠí˜€ì§„ ëŒ€ìƒì˜ ë‚´ë¶€ í™œì„±í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì •ë ¬í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì–µì œ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ ì‹ ë¢°ì„± ìˆê²Œ ë‹¬ì„±í•˜ì§€ ëª»í•˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ë¹„ëŒ€ìƒ ì§€ì‹ì— í° ì†ìƒì„ ì£¼ì§€ ì•Šê³  ì§ˆë¬¸-ì‘ë‹µ ì‘ì—…ì—ì„œ ëŒ€ìƒ ì§€ì‹ì˜ íšŒìƒì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œì˜ íš¨ê³¼ì ì¸ 'ìŠê¸°' ê¸°ìˆ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì–µì œ ê¸°ë°˜ ë°©ë²•ë“¤ì€ ëª¨ë¸ ì¶œë ¥ë§Œì„ í†µì œí•  ìˆ˜ ìˆì–´ ê·¼ë³¸ì ì¸ ì§€ì‹ì„ ì œê±°í•˜ì§€ ëª»í•˜ê³  ëª¨ë¸ ë¶•ê´´ì˜ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ëŠ” ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ì§ì ‘ ê°œì…í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìŠê³ ì í•˜ëŠ” ëŒ€ìƒì˜ í™œì„±í™”ë¥¼ 'ì•Œë ¤ì§„' ì—”í‹°í‹°ì—ì„œ 'ì•Œ ìˆ˜ ì—†ëŠ”' ì—”í‹°í‹°ë¡œ ì´ë™ì‹œì¼œ ì§„ì •í•œ ìŠê¸°ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ë²•ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ìƒ ì§€ì‹ì„ ìŠê²Œ í•˜ë©°, ë¹„ëŒ€ìƒ ì§€ì‹ì— í° ì†ìƒì„ ì£¼ì§€ ì•Šê³ ë„ ì§ˆë¬¸-ì‘ë‹µ ì‘ì—…ì—ì„œ ëª¨ë¸ì˜ ê¸°ì–µì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í”„ë¼ì´ë²„ì‹œ ë° ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì¸í•´ íš¨ê³¼ì ì¸ LLM ë§ê° ê¸°ìˆ ì˜ í•„ìš”ì„±ì´ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ ì–µì œ ê¸°ë°˜ ë°©ë²•ì€ ëª¨ë¸ ì¶œë ¥ ì œì–´ì—ëŠ” íš¨ê³¼ì ì´ì§€ë§Œ, ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ë‚´ì¬ëœ ì§€ì‹ì„ ì œê±°í•˜ì§€ ëª»í•˜ë©° ëª¨ë¸ ë¶•ê´´ ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤.

- 3. ì œì•ˆëœ ìƒˆë¡œìš´ ë§ê° ë°©ë²•ì€ ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ì§ì ‘ ê°œì…í•˜ì—¬, ìŠí˜€ì§ˆ ëŒ€ìƒì˜ í™œì„±í™”ë¥¼ ì•Œë ¤ì§€ì§€ ì•Šì€ ì—”í‹°í‹°ì™€ êµ¬ë³„í•  ìˆ˜ ì—†ë„ë¡ ì¡°ì •í•©ë‹ˆë‹¤.

- 4. ì´ ë°©ë²•ì€ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ë²•ì´ ì‹ ë¢°ì„± ìˆê²Œ ë‹¬ì„±í•˜ì§€ ëª»í•˜ëŠ” ìŠí˜€ì§ˆ ëŒ€ìƒì˜ ë‚´ë¶€ í™œì„±í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ë¹„ëŒ€ìƒ ì§€ì‹ì— í° ì†ìƒì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ ì§ˆë¬¸-ì‘ë‹µ ì‘ì—…ì—ì„œ ëŒ€ìƒ ì§€ì‹ì˜ íšŒìƒì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:41:00*