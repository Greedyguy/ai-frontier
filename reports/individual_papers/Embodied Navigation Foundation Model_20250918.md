
# Embodied Navigation Foundation Model

**Korean Title:** êµ¬ì²´í™”ëœ ë‚´ë¹„ê²Œì´ì…˜ ê¸°ë°˜ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Cross-embodiment Navigation|Cross-embodiment Navigation]] [[keywords/broad/Vision-Language Models|Vision-Language Models]] [[keywords/broad/Embodied AI|Embodied AI]] [[keywords/specific/Multimodal Navigation|Multimodal Navigation]] [[keywords/unique/NavFoM|NavFoM]] [[categories/cs.RO|cs.RO]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Cross-embodiment Navigation
**ğŸ”¬ Broad Technical**: Vision-Language Models, Embodied AI
**ğŸ”— Specific Connectable**: Multimodal Navigation
**â­ Unique Technical**: NavFoM

**ArXiv ID**: [2509.12129](https://arxiv.org/abs/2509.12129)
**Published**: 2025-09-18
**Category**: cs.RO
**PDF**: [Download](https://arxiv.org/pdf/2509.12129.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Vision-Language Models` â€¢ 

`Embodied AI` â€¢ 

`Multimodal Navigation` â€¢ 

`NavFoM` â€¢ 

`Cross-embodiment Navigation`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.12129v2 Announce Type: replace 
Abstract: Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.12129v2 ë°œí‘œ ìœ í˜•: ëŒ€ì²´
ìš”ì•½: ë‚´ì¬ëœ AIì—ì„œì˜ íƒìƒ‰ì€ ì–¸ì–´ ì§€ì‹œë¥¼ ë”°ë¥´ë©° ë¬¼ë¦¬ì  í™˜ê²½ ë‚´ì—ì„œ ì§€ê°í•˜ê³  ìƒí˜¸ ì‘ìš©í•˜ëŠ” ë° í•„ìš”í•œ ì§€ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ëŒ€í˜• Vision-Language ëª¨ë¸(VLMs)ì—ì„œ ìƒë‹¹í•œ ì§„ì „ì´ ìˆì—ˆì§€ë§Œ, ì¼ë°˜ì ì¸ ë¹„ì „-ì–¸ì–´ ì‘ì—…ì—ì„œ ë†€ë¼ìš´ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë³´ì´ëŠ” VLMsì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì€ ì£¼ë¡œ ì¢ì€ ì‘ì—… ì„¤ì •ê³¼ êµ¬ì²´ì ì¸ êµ¬ì¡°ì— í•œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë„¤ ë°œ, ë“œë¡ , ë°”í€´ ë¡œë´‡ ë° ì°¨ëŸ‰ì„ í¬í•¨í•œ ì—¬ëŸ ë°±ë§Œ ê°œì˜ íƒìƒ‰ ìƒ˜í”Œì—ì„œ í›ˆë ¨ëœ êµì°¨ êµ¬í˜„ ë° êµì°¨ ì‘ì—… ë„¤ë¹„ê²Œì´ì…˜ ê¸°ì´ˆ ëª¨ë¸(NavFoM)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë¹„ì „-ì–¸ì–´ íƒìƒ‰, ë¬¼ì²´ ê²€ìƒ‰, ëŒ€ìƒ ì¶”ì  ë° ììœ¨ ì£¼í–‰ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ í¬ê´„í•˜ë©°, ë‹¤ì–‘í•œ ì¹´ë©”ë¼ êµ¬ì„±ê³¼ íƒìƒ‰ ì§€í‰ì„ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¹´ë©”ë¼ ì„¤ì •ê³¼ ì‹œê°„ì  ì§€í‰ì„ ìˆ˜ìš©í•˜ê¸° ìœ„í•´ NavFoMì€ êµ¬í˜„ì²´ì˜ ì¹´ë©”ë¼ ë³´ê¸° ì •ë³´ì™€ ì‘ì—…ì˜ ì‹œê°„ì  ë§¥ë½ì„ í¬í•¨í•˜ëŠ” ì‹ë³„ì í† í°ì„ í†µí•©í•©ë‹ˆë‹¤. ë˜í•œ, ì‹¤ì œ ì„¸ê³„ ë°°ì¹˜ ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ NavFoMì€ ì œí•œëœ í† í° ê¸¸ì´ ì˜ˆì‚° í•˜ì—ì„œ ëª¨ë“  ê´€ì¸¡ í† í°ì„ ë™ì ìœ¼ë¡œ ì¡°ì •ëœ ìƒ˜í”Œë§ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì œì–´í•©ë‹ˆë‹¤. ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ê´‘ë²”ìœ„í•œ í‰ê°€ ê²°ê³¼, ë³¸ ëª¨ë¸ì´ ì‘ì—…ë³„ íŠ¹ì • ì„¸ë¶€ ì¡°ì •ì´ í•„ìš”í•˜ì§€ ì•Šê³  ë‹¤ì–‘í•œ íƒìƒ‰ ì‘ì—…ê³¼ êµ¬í˜„ì²´ì—ì„œ ìµœì²¨ë‹¨ ë˜ëŠ” ë§¤ìš° ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì‹¤ì œ ì„¸ê³„ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì˜ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ ì‹¤ìš©ì  ì ìš© ê°€ëŠ¥ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë³¸ ì—°êµ¬ëŠ” ì‹ ì²´ì  AIì—ì„œì˜ í•µì‹¬ ëŠ¥ë ¥ì¸ ë‚´ë¹„ê²Œì´ì…˜ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ Vision-Language ëª¨ë¸(VLMs)ì˜ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì˜ ë‚´ë¹„ê²Œì´ì…˜ ì¼ë°˜í™” ëŠ¥ë ¥ì€ ì¢ì€ ê³¼ì œ ì„¤ì •ê³¼ íŠ¹ì • êµ¬ì¡°ì— ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‹¤ì–‘í•œ ì„ë¬´ì™€ ë‹¤ì–‘í•œ ì¹´ë©”ë¼ ì„¤ì •ì—ì„œ í•™ìŠµëœ í¬ë¡œìŠ¤-ì„ë² ë””ë¨¼íŠ¸ ë° í¬ë¡œìŠ¤-íƒœìŠ¤í¬ ë‚´ë¹„ê²Œì´ì…˜ ê¸°ë°˜ ëª¨ë¸(NavFoM)ì„ ì œì•ˆí•©ë‹ˆë‹¤. NavFoMì€ ë‹¤ì–‘í•œ ì¹´ë©”ë¼ êµ¬ì„±ê³¼ ë‚´ë¹„ê²Œì´ì…˜ ì‹œì•¼ì—ì„œì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ë‚´ë¹„ê²Œì´ì…˜ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ë©°, ì‹¤ì œ ë°°ì¹˜ ìš”êµ¬ë¥¼ ì¶©ì¡±í•˜ê¸° ìœ„í•´ ì œí•œëœ í† í° ê¸¸ì´ ì˜ˆì‚° í•˜ì— ëª¨ë“  ê´€ì¸¡ í† í°ì„ ë™ì ìœ¼ë¡œ ì¡°ì •ëœ ìƒ˜í”Œë§ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ í­ë„“ì€ í‰ê°€ ê²°ê³¼, ë³¸ ëª¨ë¸ì€ ê³¼ì œ íŠ¹ì • ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš” ì—†ì´ ë‹¤ì–‘í•œ ë‚´ë¹„ê²Œì´ì…˜ ì‘ì—…ê³¼ ì„ë² ë””ë¨¼íŠ¸ì—ì„œ ìµœì²¨ë‹¨ ë˜ëŠ” ë§¤ìš° ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¶”ê°€ì ì¸ í˜„ì‹¤ ì„¸ê³„ ì‹¤í—˜ì€ ìš°ë¦¬ ë°©ë²•ì˜ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ ì‹¤ìš©ì  ì ìš© ê°€ëŠ¥ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ì‹ ê²½ë§ ëª¨ë¸ NavFoMì€ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì™€ íƒœì²´ì— ëŒ€í•´ ë›°ì–´ë‚œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ

- 2. NavFoMì€ ë‹¤ì–‘í•œ ì¹´ë©”ë¼ ì„¤ì •ê³¼ ì‹œê°„ì  ë²”ìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ í†µí•© ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•¨

- 3. ëª¨ë¸ì€ ì„ë² ë”©ëœ ì‹ ì› ì •ë³´ ë° ì„ë¬´ì˜ ì‹œê°„ì  ë§¥ë½ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ì‹ë³„ì í† í°ì„ ì‚¬ìš©í•¨.


---

*Generated on 2025-09-18 17:21:34*