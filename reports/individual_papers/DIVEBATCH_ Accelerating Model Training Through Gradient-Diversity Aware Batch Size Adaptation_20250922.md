# DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation

**Korean Title:** DIVEBATCH: ê¸°ìš¸ê¸° ë‹¤ì–‘ì„± ì¸ì‹ ë°°ì¹˜ í¬ê¸° ì ì‘ì„ í†µí•œ ëª¨ë¸ í›ˆë ¨ ê°€ì†í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Adaptive Batch Size, Gradient Diversity

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (83.1% similar)
- [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (82.1% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (79.8% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (79.7% similar)
- [[2025-09-18/Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients_20250918|Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients]] (79.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16173v1 Announce Type: new 
Abstract: The goal of this paper is to accelerate the training of machine learning models, a critical challenge since the training of large-scale deep neural models can be computationally expensive. Stochastic gradient descent (SGD) and its variants are widely used to train deep neural networks. In contrast to traditional approaches that focus on tuning the learning rate, we propose a novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts the batch size. Adapting the batch size is challenging: using large batch sizes is more efficient due to parallel computation, but small-batch training often converges in fewer epochs and generalizes better. To address this challenge, we introduce a data-driven adaptation based on gradient diversity, enabling DiveBatch to maintain the generalization performance of small-batch training while improving convergence speed and computational efficiency. Gradient diversity has a strong theoretical justification: it emerges from the convergence analysis of SGD. Evaluations of DiveBatch on synthetic and CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a slight trade-off in performance.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16173v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì´ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ëŒ€ê·œëª¨ ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì˜ í›ˆë ¨ì´ ê³„ì‚°ì ìœ¼ë¡œ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ í›ˆë ¨ì„ ê°€ì†í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD) ë° ê·¸ ë³€í˜•ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ í›ˆë ¨í•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì— ì¤‘ì ì„ ë‘” ì „í†µì ì¸ ì ‘ê·¼ ë°©ì‹ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ìƒˆë¡œìš´ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° SGD ì•Œê³ ë¦¬ì¦˜ì¸ DiveBatchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì€ ë„ì „ì ì…ë‹ˆë‹¤: í° ë°°ì¹˜ í¬ê¸°ëŠ” ë³‘ë ¬ ê³„ì‚° ë•ë¶„ì— ë” íš¨ìœ¨ì ì´ì§€ë§Œ, ì‘ì€ ë°°ì¹˜ í›ˆë ¨ì€ ì¢…ì¢… ë” ì ì€ ì—í¬í¬ì—ì„œ ìˆ˜ë ´í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì— ê¸°ë°˜í•œ ë°ì´í„° ê¸°ë°˜ ì ì‘ì„ ë„ì…í•˜ì—¬ DiveBatchê°€ ì‘ì€ ë°°ì¹˜ í›ˆë ¨ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì€ ê°•ë ¥í•œ ì´ë¡ ì  ê·¼ê±°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤: ì´ëŠ” SGDì˜ ìˆ˜ë ´ ë¶„ì„ì—ì„œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. DiveBatchë¥¼ í•©ì„± ë°ì´í„° ë° CiFar-10, CiFar-100, Tiny-ImageNetì—ì„œ í‰ê°€í•œ ê²°ê³¼, DiveBatchëŠ” í‘œì¤€ SGD ë° AdaBatchë³´ë‹¤ ìƒë‹¹íˆ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©°(1.06 -- 5.0ë°°), ì„±ëŠ¥ì— ì•½ê°„ì˜ ì ˆì¶©ì´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì˜ í•™ìŠµì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í•™ìŠµë¥  ì¡°ì • ëŒ€ì‹ , ì´ ë…¼ë¬¸ì—ì„œëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ìƒˆë¡œìš´ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° SGD ì•Œê³ ë¦¬ì¦˜ì¸ DiveBatchë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. DiveBatchëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬, ì‘ì€ ë°°ì¹˜ í¬ê¸°ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì´ë¡ ì ìœ¼ë¡œë„ SGDì˜ ìˆ˜ë ´ ë¶„ì„ì—ì„œ ìœ ë„ëœ ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì— ì˜í•´ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤. DiveBatchëŠ” CiFar-10, CiFar-100, Tiny-ImageNet ë“±ì—ì„œ í‘œì¤€ SGDì™€ AdaBatchë³´ë‹¤ ìµœëŒ€ 5ë°° ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©°, ì„±ëŠ¥ì—ì„œ ì•½ê°„ì˜ ì ˆì¶©ì´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì˜ í›ˆë ¨ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° SGD ì•Œê³ ë¦¬ì¦˜ì¸ DiveBatchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. DiveBatchëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì‘ì€ ë°°ì¹˜ í›ˆë ¨ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.

- 3. DiveBatchëŠ” ê²½ì‚¬ ë‹¤ì–‘ì„±ì— ê¸°ë°˜í•œ ë°ì´í„° ê¸°ë°˜ ì ì‘ì„ ë„ì…í•˜ì—¬, ì´ë¡ ì ìœ¼ë¡œë„ SGDì˜ ìˆ˜ë ´ ë¶„ì„ì—ì„œ ê·¸ ì •ë‹¹ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

- 4. DiveBatchëŠ” CiFar-10, CiFar-100, Tiny-ImageNet ë“±ì˜ í‰ê°€ì—ì„œ í‘œì¤€ SGD ë° AdaBatchë³´ë‹¤ 1.06ë°°ì—ì„œ 5.0ë°°ê¹Œì§€ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤.

- 5. DiveBatchëŠ” ì„±ëŠ¥ì—ì„œ ì•½ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ìˆì§€ë§Œ, ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì—ì„œ í° ê°œì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:34:15*