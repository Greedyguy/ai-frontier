# Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents

**Korean Title:** ì¼ë°˜í™” ê°€ëŠ¥í•œ ê·¸ë˜í”„ ê¸°ë°˜ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Automated Cyber Defense

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (83.1% similar)
- [[2025-09-22/Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem_20250922|Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem]] (82.7% similar)
- [[2025-09-18/$Agent^2$_ An Agent-Generates-Agent Framework for Reinforcement Learning Automation_20250918|$Agent^2$ An Agent-Generates-Agent Framework for Reinforcement Learning Automation]] (82.1% similar)
- [[2025-09-19/A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks_20250919|A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks]] (81.3% similar)
- [[2025-09-22/The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI_20250922|The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI]] (80.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16151v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for automated cyber defense (ACD). The traditional RL approach represents networks as a list of computers in various states of safety or threat. Unfortunately, these models are forced to overfit to specific network topologies, rendering them ineffective when faced with even small environmental perturbations. In this work, we frame ACD as a two-player context-based partially observable Markov decision problem with observations represented as attributed graphs. This approach allows our agents to reason through the lens of relational inductive bias. Agents learn how to reason about hosts interacting with other system entities in a more general manner, and their actions are understood as edits to the graph representing the environment. By introducing this bias, we will show that our agents can better reason about the states of networks and zero-shot adapt to new ones. We show that this approach outperforms the state-of-the-art by a wide margin, and makes our agents capable of defending never-before-seen networks against a wide range of adversaries in a variety of complex, and multi-agent environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16151v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì€ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´(ACD)ë¥¼ ìœ„í•œ ì‹¤í˜„ ê°€ëŠ¥í•œ ì „ëµìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì „í†µì ì¸ RL ì ‘ê·¼ë²•ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ì–‘í•œ ì•ˆì „ ë˜ëŠ” ìœ„í˜‘ ìƒíƒœì— ìˆëŠ” ì»´í“¨í„° ëª©ë¡ìœ¼ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë¶ˆí–‰íˆë„, ì´ëŸ¬í•œ ëª¨ë¸ì€ íŠ¹ì • ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ì— ê³¼ì í•©ë˜ë„ë¡ ê°•ìš”ë°›ì•„, í™˜ê²½ì˜ ì‘ì€ ë³€í™”ì—ë„ íš¨ê³¼ì ì´ì§€ ëª»í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ACDë¥¼ ì†ì„± ê·¸ë˜í”„ë¡œ í‘œí˜„ëœ ê´€ì°°ì„ ì‚¬ìš©í•˜ëŠ” ë‘ í”Œë ˆì´ì–´ ê¸°ë°˜ì˜ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ë¬¸ì œë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì—ì´ì „íŠ¸ê°€ ê´€ê³„ì  ê·€ë‚© í¸í–¥ì˜ ê´€ì ì—ì„œ ì¶”ë¡ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ë‹¤ë¥¸ ì‹œìŠ¤í…œ ì—”í‹°í‹°ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” í˜¸ìŠ¤íŠ¸ì— ëŒ€í•´ ë³´ë‹¤ ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ë©°, ê·¸ë“¤ì˜ í–‰ë™ì€ í™˜ê²½ì„ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ì— ëŒ€í•œ í¸ì§‘ìœ¼ë¡œ ì´í•´ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í¸í–¥ì„ ë„ì…í•¨ìœ¼ë¡œì¨, ì—ì´ì „íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ëŒ€í•´ ë” ì˜ ì¶”ë¡ í•˜ê³  ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ì— ì œë¡œìƒ·ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤„ ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì ‘ê·¼ë²•ì´ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, ë‹¤ì–‘í•œ ë³µì¡í•˜ê³  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ ì ì— ëŒ€í•´ ì´ì „ì— ë³¸ ì  ì—†ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ë°©ì–´í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì„ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´(ACD)ì— ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ RL ëª¨ë¸ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ íŠ¹ì • í† í´ë¡œì§€ì— ë§ì¶° ê³¼ì í•©ë˜ê¸° ì‰¬ìš´ ë¬¸ì œë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ACDë¥¼ ì†ì„± ê·¸ë˜í”„ë¡œ í‘œí˜„ëœ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ë¬¸ì œë¡œ ì¬êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê´€ê³„ì  ê·€ë‚© í¸í–¥ì„ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ ë” ì¼ë°˜í™”ëœ ë°©ì‹ìœ¼ë¡œ ì´í•´í•˜ê³ , ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ì— ì¦‰ê° ì ì‘í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ì ‘ê·¼ë²•ì€ ìµœì‹  ê¸°ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¤ì–‘í•œ ë³µì¡í•œ í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•œ ë°©ì–´ ëŠ¥ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì€ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´(ACD)ë¥¼ ìœ„í•œ ìœ ë§í•œ ì „ëµìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.

- 2. ê¸°ì¡´ì˜ RL ì ‘ê·¼ ë°©ì‹ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ì–‘í•œ ì•ˆì „ ë˜ëŠ” ìœ„í˜‘ ìƒíƒœì˜ ì»´í“¨í„° ëª©ë¡ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ íŠ¹ì • ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ì— ê³¼ì í•©ë˜ëŠ” ë¬¸ì œë¥¼ ê°€ì§„ë‹¤.

- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” ACDë¥¼ ë‘ ëª…ì˜ í”Œë ˆì´ì–´ê°€ ì°¸ì—¬í•˜ëŠ” ë¬¸ë§¥ ê¸°ë°˜ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ë¬¸ì œë¡œ êµ¬ì„±í•˜ê³ , ê´€ì°°ì„ ì†ì„± ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ì˜€ë‹¤.

- 4. ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì€ ê´€ê³„ì  ê·€ë‚© í¸í–¥ì„ í†µí•´ ì—ì´ì „íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ ë” ì˜ ì¶”ë¡ í•˜ê³  ìƒˆë¡œìš´ í™˜ê²½ì— ì ì‘í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

- 5. ë³¸ ì—°êµ¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ìµœì‹  ê¸°ìˆ ì„ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, ë‹¤ì–‘í•œ ë³µì¡í•˜ê³  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°©ì–´í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì œê³µí•œë‹¤.

---

*Generated on 2025-09-22 15:33:57*