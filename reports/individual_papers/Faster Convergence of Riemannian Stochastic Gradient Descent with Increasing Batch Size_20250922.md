# Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size

**Korean Title:** ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ë ´ ì†ë„ í–¥ìƒ: ë°°ì¹˜ í¬ê¸° ì¦ê°€ì™€ í•¨ê»˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Riemannian Stochastic Gradient Descent|Riemannian Stochastic Gradient Descent]] [[keywords/specific/Increasing Batch Size|Increasing Batch Size]] [[keywords/specific/Cosine Annealing Decay|Cosine Annealing Decay]] [[keywords/broad/Stochastic Gradient Descent|Stochastic Gradient Descent]] [[keywords/broad/Riemannian Optimization|Riemannian Optimization]] [[categories/cs.LG|cs.LG]] [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (83.4% similar) [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (82.3% similar) [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Increasing Batch Size Strategy
**ğŸ”— Specific Connectable**: Riemannian Optimization, Batch Size Scheduling
**ğŸ”¬ Broad Technical**: Stochastic Gradient Descent
**â­ Unique Technical**: Riemannian Stochastic Gradient Descent
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (83.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (82.3% similar)
- [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (80.4% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (79.5% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (79.3% similar)


**ArXiv ID**: [2501.18164](https://arxiv.org/abs/2501.18164)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2501.18164.pdf)


**ArXiv ID**: [2501.18164](https://arxiv.org/abs/2501.18164)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2501.18164.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Increasing Batch Size Strategy
**ğŸ”— Specific Connectable**: Riemannian Optimization, Principal Component Analysis
**â­ Unique Technical**: Riemannian Stochastic Gradient Descent
**ğŸ”¬ Broad Technical**: Stochastic Gradient Descent

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Stochastic Gradient Descent` â€¢ 

`Riemannian Stochastic Gradient Descent` â€¢ 

`Principal Component Analysis` â€¢ 

`Increasing Batch Size`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.18164v4 Announce Type: replace 
Abstract: We theoretically analyzed the convergence behavior of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster convergence than using a constant batch size, not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. The convergence rate improves from $O(T^{-1}+C)$ with a constant batch size to $O(T^{-1})$ with an increasing batch size, where $T$ denotes the total number of iterations and $C$ is a constant. Using principal component analysis and low-rank matrix completion, we investigated, both theoretically and numerically, how an increasing batch size affects computational time as quantified by stochastic first-order oracle (SFO) complexity. An increasing batch size was found to reduce the SFO complexity of RSGD. Furthermore, an increasing batch size was found to offer the advantages of both small and large constant batch sizes.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2501.18164v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” ë¦¬ë§Œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(RSGD)ì˜ ìˆ˜ë ´ í–‰ë™ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìœ¼ë©°, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë¹ ë¥¸ ìˆ˜ë ´ì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¼ì •í•œ í•™ìŠµë¥ ë¿ë§Œ ì•„ë‹ˆë¼ ì½”ì‚¬ì¸ ì¡°ì • ê°ì‡  ë° ë‹¤í•­ì‹ ê°ì‡ ì™€ ê°™ì€ ê°ì‡  í•™ìŠµë¥ ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ìˆ˜ë ´ ì†ë„ëŠ” ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ì˜ ê²½ìš° $O(T^{-1}+C)$ì—ì„œ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ì˜ ê²½ìš° $O(T^{-1})$ë¡œ ê°œì„ ë˜ë©°, ì—¬ê¸°ì„œ $T$ëŠ” ì´ ë°˜ë³µ íšŸìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ê³  $C$ëŠ” ìƒìˆ˜ì…ë‹ˆë‹¤. ì£¼ì„±ë¶„ ë¶„ì„ê³¼ ì €ë­í¬ í–‰ë ¬ ì™„ì„±ì„ ì‚¬ìš©í•˜ì—¬, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ì„±ìœ¼ë¡œ ì •ëŸ‰í™”ëœ ê³„ì‚° ì‹œê°„ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ì´ë¡ ì  ë° ìˆ˜ì¹˜ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” RSGDì˜ SFO ë³µì¡ì„±ì„ ì¤„ì´ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë˜í•œ, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ì‘ì€ ìƒìˆ˜ ë°°ì¹˜ í¬ê¸°ì™€ í° ìƒìˆ˜ ë°°ì¹˜ í¬ê¸°ì˜ ì¥ì ì„ ëª¨ë‘ ì œê³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(RSGD)ì˜ ìˆ˜ë ´ í–‰ë™ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ê²½ìš° ê³ ì • ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ë” ë¹ ë¥¸ ìˆ˜ë ´ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê³ ì • í•™ìŠµë¥ ë¿ë§Œ ì•„ë‹ˆë¼ ì½”ì‚¬ì¸ ê°ì†Œ ë° ë‹¤í•­ì‹ ê°ì†Œì™€ ê°™ì€ ê°ì†Œ í•™ìŠµë¥ ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ìˆ˜ë ´ ì†ë„ê°€ $O(T^{-1}+C)$ì—ì„œ $O(T^{-1})$ë¡œ ê°œì„ ë©ë‹ˆë‹¤. ì£¼ì„±ë¶„ ë¶„ì„ê³¼ ì €ë­í¬ í–‰ë ¬ ì™„ì„±ì„ í†µí•´ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ í™•ë¥ ì  1ì°¨ ì˜ˆì¸¡ ë³µì¡ë„(SFO)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´ë¡ ì , ìˆ˜ì¹˜ì ìœ¼ë¡œ ì¡°ì‚¬í•œ ê²°ê³¼, SFO ë³µì¡ë„ê°€ ê°ì†Œí•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ì‘ì€ ê³ ì • ë°°ì¹˜ í¬ê¸°ì™€ í° ê³ ì • ë°°ì¹˜ í¬ê¸°ì˜ ì¥ì ì„ ëª¨ë‘ ì œê³µí•œë‹¤ëŠ” ì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(RSGD)ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œ ë” ë¹ ë¥´ë‹¤.

- 2. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ê²½ìš°, ìˆ˜ë ´ ì†ë„ê°€ $O(T^{-1}+C)$ì—ì„œ $O(T^{-1})$ë¡œ ê°œì„ ëœë‹¤.

- 3. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” RSGDì˜ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ì„±ì„ ì¤„ì¸ë‹¤.

- 4. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ì‘ì€ ë°°ì¹˜ í¬ê¸°ì™€ í° ë°°ì¹˜ í¬ê¸°ì˜ ì¥ì ì„ ëª¨ë‘ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-22 15:54:01*