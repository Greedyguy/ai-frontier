
# Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations

**Korean Title:** ì‚¬ì „ í›ˆë ¨ëœ í‘œí˜„ì„ ë³´ì¡´í•¨ìœ¼ë¡œì¨ ì‹œê°-ì–¸ì–´-í–‰ë™ ëª¨ë¸ì—ì„œ ì¼ë°˜í™” í–¥ìƒí•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Spatial Reasoning|Spatial Reasoning]] [[keywords/broad/Vision-Language-Action Models|Vision-Language-Action Models]] [[keywords/broad/Pretrained Representations|Pretrained Representations]] [[keywords/specific/Co-training Strategy|Co-training Strategy]] [[keywords/unique/String-based Action Tokenizer|String-based Action Tokenizer]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Co-training Strategy
**ğŸ”¬ Broad Technical**: Vision-Language-Action Models, Pretrained Representations
**ğŸ”— Specific Connectable**: Dual-encoder Design
**â­ Unique Technical**: String-based Action Tokenizer

**ArXiv ID**: [2509.11417](https://arxiv.org/abs/2509.11417)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.11417.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Vision-Language-Action Models` â€¢ 

`Pretrained Representations` â€¢ 

`Dual-encoder Design` â€¢ 

`String-based Action Tokenizer` â€¢ 

`Co-training Strategy`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.11417v2 Announce Type: replace-cross 
Abstract: Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.11417v2 ë°œí‘œ ìœ í˜•: replace-cross
ìš”ì•½: ì‹œê°-ì–¸ì–´-í–‰ë™ (VLA) ëª¨ë¸ì€ ì‹œê°-ì–¸ì–´ ëª¨ë¸ (VLMs)ì—ì„œ ì„¸ë°€ ì¡°ì •ëœ ê²ƒìœ¼ë¡œ, ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í™˜ê²½ì„ ê°€ë¡œì§€ë¥´ëŠ” ì¼ë°˜ì ì¸ ë¡œë´‡ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ í’ë¶€í•œ ì‚¬ì „ í•™ìŠµëœ í‘œí˜„ì„ í™œìš©í•˜ëŠ” ê°€ëŠ¥ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¡œë´‡ ë°ì´í„°ì— ì§ì ‘ ì„¸ë°€ ì¡°ì •í•˜ëŠ” ê²ƒì€ ì¢…ì¢… ì´ëŸ¬í•œ í‘œí˜„ì„ ë°©í•´í•˜ê³  ì¼ë°˜í™”ë¥¼ ì œí•œí•©ë‹ˆë‹¤. ì €í¬ëŠ” ì‚¬ì „ í•™ìŠµëœ íŠ¹ì§•ì„ ë³´ë‹¤ ì˜ ë³´ì¡´í•˜ë©´ì„œ ë¡œë´‡ ì¡°ì‘ì„ ìœ„í•´ ì ì‘ì‹œí‚¤ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì €í¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¥¼ ì†Œê°œí•©ë‹ˆë‹¤: (i) ì‚¬ì „ í•™ìŠµëœ íŠ¹ì§•ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ í•˜ë‚˜ëŠ” ì–¼ë¦° ì‹œê° ì¸ì½”ë”ì™€ ì‘ì—… ì ì‘ì„ ìœ„í•´ ë‹¤ë¥¸ í•˜ë‚˜ê°€ í›ˆë ¨ ê°€ëŠ¥í•œ ì´ì¤‘ ì¸ì½”ë” ì„¤ê³„, (ii) ì—°ì†ì ì¸ ì‘ì—…ì„ ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµ ë„ë©”ì¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë¬¸ì ê¸°ë°˜ í–‰ë™ í† í¬ë‚˜ì´ì €, ê·¸ë¦¬ê³  (iii) ë¡œë´‡ ë°ëª¨ì™€ ê³µê°„ ì¶”ë¡  ë° affordancesë¥¼ ê°•ì¡°í•˜ëŠ” ì‹œê°-ì–¸ì–´ ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ëŠ” ê³µë™ í›ˆë ¨ ì „ëµ. ì‹œë®¬ë ˆì´ì…˜ ë° ì‹¤ì œ ë¡œë´‡ì—ì„œì˜ í‰ê°€ ê²°ê³¼, ì €í¬ ë°©ë²•ì€ ì‹œê°ì  ê²©ë™ì— ëŒ€í•œ ê°•ê±´ì„±, ìƒˆë¡œìš´ ì§€ì‹œì‚¬í•­ ë° í™˜ê²½ì— ëŒ€í•œ ì¼ë°˜í™”, ê·¸ë¦¬ê³  ê¸°ë³¸ì„ ì— ë¹„í•´ ì „ë°˜ì ì¸ ì‘ì—… ì„±ê³µì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì‹œê°-ì–¸ì–´-í–‰ë™(VLA) ëª¨ë¸ì´ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ë¡œë¶€í„° ì„¸ë°€ ì¡°ì •ë˜ì–´ ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í™˜ê²½ì— ê±¸ì³ ì¼ë°˜ì ì¸ ë¡œë´‡ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ì•½ì†í•œë‹¤. ê·¸ëŸ¬ë‚˜ ë¡œë´‡ ë°ì´í„°ì— ì§ì ‘ ì„¸ë°€ ì¡°ì •í•˜ëŠ” ê²ƒì€ ì¢…ì¢… ì´ëŸ¬í•œ í‘œí˜„ì„ ë°©í•´í•˜ê³  ì¼ë°˜í™”ë¥¼ ì œí•œí•œë‹¤. ë³¸ ì—°êµ¬ëŠ” ì‚¬ì „ í›ˆë ¨ëœ íŠ¹ì§•ì„ ë³´ë‹¤ ì˜ ë³´ì¡´í•˜ë©´ì„œ ë¡œë´‡ ì¡°ì‘ì„ ìœ„í•´ ì ì‘ì‹œí‚¤ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•œë‹¤. ì ‘ê·¼ ë°©ì‹ì€ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¥¼ ë„ì…í•œë‹¤: (i) ì‚¬ì „ í›ˆë ¨ëœ íŠ¹ì§•ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ í•˜ë‚˜ëŠ” ê³ ì •ëœ ë¹„ì „ ì¸ì½”ë”ì™€ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì‘ì—… ì ì‘ì„ ìœ„í•´ í›ˆë ¨ ê°€ëŠ¥í•œ ì´ì¤‘ ì¸ì½”ë” ì„¤ê³„, (ii) ì—°ì†ì ì¸ í–‰ë™ì„ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ ë„ë©”ì¸ì— ë§ê²Œ ë¬¸ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë¬¸ì ê¸°ë°˜ í–‰ë™ í† í¬ë‚˜ì´ì €, (iii) ë¡œë´‡ ë°ëª¨ì™€ ê³µê°„ ì¶”ë¡  ë° affordancesë¥¼ ê°•ì¡°í•˜ëŠ” ì‹œê°-ì–¸ì–´ ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ëŠ” ê³µë™ í›ˆë ¨ ì „ëµ. ì‹œë®¬ë ˆì´ì…˜ ë° ì‹¤ì œ ë¡œë´‡ì—ì„œì˜ í‰ê°€ ê²°ê³¼, ë³¸ ë°©ë²•ì€ ì‹œê°ì  ê²©ë™ì— ëŒ€í•œ ê°•ê±´ì„±, ìƒˆë¡œìš´ ì§€ì‹œì‚¬í•­ ë° í™˜ê²½ì— ëŒ€í•œ ì¼ë°˜í™”, ì „ë°˜ì ì¸ ì‘ì—… ì„±ê³µ ë©´ì—ì„œ ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬ ê°œì„ ë¨ì„ ë³´ì—¬ì¤€ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- VLA ëª¨ë¸ì€ VLMìœ¼ë¡œë¶€í„° finetunedë˜ë©° ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í™˜ê²½ì— ê±¸ì³ ì¼ë°˜ì ì¸ ë¡œë´‡ì„ êµ¬ì¶•í•˜ëŠ” ë° ìœ ìš©í•˜ë‹¤.

- ë¡œë´‡ ë°ì´í„°ì— ëŒ€í•œ ì§ì ‘ì ì¸ finetuningì€ pretrained íŠ¹ì„±ì„ íŒŒê´´í•˜ê³  ì¼ë°˜í™”ë¥¼ ì œí•œí•  ìˆ˜ ìˆë‹¤.

- ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ëŠ” pretrained íŠ¹ì„±ì„ ë³´ì¡´í•˜ë©´ì„œ ë¡œë´‡ ì¡°ì‘ì„ ìœ„í•´ ì ì‘ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.


---

*Generated on 2025-09-18 16:35:58*