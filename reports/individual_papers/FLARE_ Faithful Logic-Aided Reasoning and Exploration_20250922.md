# FLARE: Faithful Logic-Aided Reasoning and Exploration

**Korean Title:** FLARE: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë…¼ë¦¬ ë³´ì¡° ì¶”ë¡  ë° íƒìƒ‰

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Chain of Thought, Multi-hop Search

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/WebCoT_ Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback_20250919|WebCoT Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback]] (85.1% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (84.7% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (84.6% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (83.5% similar)
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression A Self-Optimizing Framework]] (83.0% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.11900v5 Announce Type: replace 
Abstract: Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2410.11900v5 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í˜„ëŒ€ì˜ ì§ˆë¬¸ ì‘ë‹µ(QA) ë° ì¶”ë¡  ì ‘ê·¼ ë°©ì‹ì€ ì¼ë°˜ì ìœ¼ë¡œ Chain-of-Thought (CoT)ì™€ ê°™ì€ í”„ë¡¬í”„íŠ¸ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ê³µê°„ê³¼ ë²”ìœ„ì— ëŒ€í•œ ë” ì„¸ë¶„í™”ëœ íƒìƒ‰ê³¼ ì¶”ë¡ ì„ ìœ ë„í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ì€ ëª¨ë¸ì´ ìƒì„±í•œ ì¤‘ê°„ ì¶”ë¡  ì²´ì¸ì— ì¶©ì‹¤í•œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë°˜ë©´ì—, Faithful CoT (F-CoT)ì™€ ê°™ì€ ì‹ ê²½-ê¸°í˜¸ì  ë°©ë²•ì€ LLMsë¥¼ ì™¸ë¶€ ê¸°í˜¸ ì†”ë²„ì™€ ê²°í•©í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ë†’ì€ ì¶©ì‹¤ë„ë¥¼ ìë‘í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ì½”ë“œ ìƒì„±ì— í›ˆë ¨ëœ ëª¨ë¸ì´ í•„ìš”í•˜ë©° ëª¨í˜¸í•˜ê±°ë‚˜ ì—„ê²©í•˜ê²Œ í˜•ì‹í™”í•˜ê¸° ì–´ë ¤ìš´ ì‘ì—…ì—ì„œëŠ” ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¬¸ì œ ê³µê°„ì„ íƒìƒ‰í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í•´ì„ ê°€ëŠ¥í•œ ì ‘ê·¼ ë°©ì‹ì¸ $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì„ ì‚¬ìš©í•˜ì—¬ ì†”ë£¨ì…˜ì„ ê³„íší•˜ê³ , ë…¼ë¦¬ í”„ë¡œê·¸ë˜ë° ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì‚¬ì‹¤ê³¼ ìˆ ì–´ë¡œ ë¶€ë“œëŸ½ê²Œ í˜•ì‹í™”í•˜ë©°, ì •ì˜ëœ ê³µê°„ì— ëŒ€í•œ ì² ì €í•œ ë‹¤ì¤‘ í™‰ ê²€ìƒ‰ì„ í†µí•´ ê·¸ ì½”ë“œ ì‹¤í–‰ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ì™¸ë¶€ ì†”ë²„ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ìƒì„±ëœ ì½”ë“œì— ëŒ€í•œ ì¶”ë¡  ê³¼ì •ì˜ ì¶©ì‹¤ë„ë¥¼ ê³„ì‚°í•˜ê³  ë‹¤ì¤‘ í™‰ ê²€ìƒ‰ì˜ ë‹¨ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ $\mathbf{9}$ê°œì˜ ë‹¤ì–‘í•œ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì¤‘ $\mathbf{7}$ê°œì—ì„œ SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ëª¨ë¸ì˜ ì¶©ì‹¤ë„ê°€ ì „ì²´ ì„±ëŠ¥ê³¼ ê¸ì •ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, $\textbf{FLARE}$ê°€ ë‹¤ì¤‘ í™‰ ê²€ìƒ‰ ì¤‘ ìµœì ì˜ ì¶”ë¡ ì„ í†µí•´ ì˜¬ë°”ë¥¸ ë‹µë³€ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì¶©ë¶„í•œ ê²°ì •ì  ìš”ì†Œë¥¼ ì •í™•íˆ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŒì„ ì¶”ê°€ë¡œ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ í˜„ëŒ€ì˜ ì§ˆë¬¸ ë‹µë³€(QA) ë° ì¶”ë¡  ë°©ë²•ë¡ ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ FLAREë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ì¤‘ê°„ ì¶”ë¡  ê³¼ì •ì˜ ì¶©ì‹¤ì„±ì„ ë³´ì¥í•˜ê¸° ì–´ë ¤ìš´ ë°˜ë©´, FLAREëŠ” LLMê³¼ ë…¼ë¦¬ í”„ë¡œê·¸ë˜ë°ì„ ê²°í•©í•˜ì—¬ ë¬¸ì œ ê³µê°„ì„ íƒìƒ‰í•©ë‹ˆë‹¤. FLAREëŠ” ë¬¸ì œë¥¼ ì„¸ë¶„í™”í•˜ê³  ë…¼ë¦¬ì  ì‚¬ì‹¤ê³¼ ìˆ ì–´ë¡œ ë³€í™˜í•œ í›„, ë‹¤ë‹¨ê³„ ê²€ìƒ‰ì„ í†µí•´ ì¶”ë¡  ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ì¶©ì‹¤ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì™¸ë¶€ ì†”ë²„ì— ì˜ì¡´í•˜ì§€ ì•Šê³ ë„ ì¶”ë¡  ê³¼ì •ì„ ë¶„ì„í•  ìˆ˜ ìˆìœ¼ë©°, 9ê°œì˜ ë‹¤ì–‘í•œ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì¤‘ 7ê°œì—ì„œ ìµœì²¨ë‹¨(SOTA) ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì˜ ì¶©ì‹¤ì„±ì´ ì „ì²´ ì„±ëŠ¥ê³¼ ê¸ì •ì ì¸ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ë©°, FLAREëŠ” ìµœì ì˜ ì¶”ë¡ ì„ í†µí•´ ì •ë‹µì„ ë„ì¶œí•˜ëŠ” ê²°ì •ì  ìš”ì¸ì„ ì‹ë³„í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í˜„ëŒ€ì˜ ì§ˆë¬¸ ì‘ë‹µ(QA) ë° ì¶”ë¡  ì ‘ê·¼ë²•ì€ ì£¼ë¡œ Chain-of-Thought(CoT)ì™€ ê°™ì€ í”„ë¡¬í”„íŠ¸ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì¤‘ê°„ ì¶”ë¡  ì²´ì¸ì— ì¶©ì‹¤í•œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

- 2. Faithful CoT(F-CoT)ì™€ ê°™ì€ ì‹ ê²½-ê¸°í˜¸ì  ë°©ë²•ì€ LLMê³¼ ì™¸ë¶€ ê¸°í˜¸ í•´ì„ê¸°ë¥¼ ê²°í•©í•˜ì—¬ ë†’ì€ ì¶©ì‹¤ë„ë¥¼ ìë‘í•˜ì§€ë§Œ, ì½”ë“œ ìƒì„±ì— í›ˆë ¨ëœ ëª¨ë¸ì´ í•„ìš”í•˜ê³  ëª¨í˜¸í•˜ê±°ë‚˜ ì—„ê²©íˆ í˜•ì‹í™”í•˜ê¸° ì–´ë ¤ìš´ ì‘ì—…ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

- 3. FLAREëŠ” ë¬¸ì œ ê³µê°„ì„ íƒìƒ‰í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í•´ì„ ê°€ëŠ¥í•œ ì ‘ê·¼ë²•ìœ¼ë¡œ, LLMì„ ì‚¬ìš©í•˜ì—¬ í•´ê²°ì±…ì„ ê³„íší•˜ê³  ë…¼ë¦¬ í”„ë¡œê·¸ë˜ë° ì½”ë“œë¥¼ í†µí•´ ì¿¼ë¦¬ë¥¼ ì‚¬ì‹¤ê³¼ ìˆ ì–´ë¡œ ì†Œí”„íŠ¸ í˜•ì‹í™”í•˜ì—¬ ì •ì˜ëœ ê³µê°„ì—ì„œ ì² ì €í•œ ë©€í‹°í™‰ ê²€ìƒ‰ì„ í†µí•´ ì½”ë“œ ì‹¤í–‰ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

- 4. FLAREëŠ” ì™¸ë¶€ í•´ì„ê¸°ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ìƒì„±ëœ ì½”ë“œì— ëŒ€í•œ ì¶”ë¡  ê³¼ì •ì˜ ì¶©ì‹¤ë„ë¥¼ ê³„ì‚°í•˜ê³  ë©€í‹°í™‰ ê²€ìƒ‰ì˜ ë‹¨ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

- 5. FLAREëŠ” 9ê°œì˜ ë‹¤ì–‘í•œ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì¤‘ 7ê°œì—ì„œ SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ëª¨ë¸ì˜ ì¶©ì‹¤ë„ê°€ ì „ì²´ ì„±ëŠ¥ê³¼ ê¸ì •ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ìˆìŒì„ ë³´ì—¬ì£¼ê³ , ë©€í‹°í™‰ ê²€ìƒ‰ ì¤‘ ìµœì ì˜ ì¶”ë¡ ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ì´ëŒì–´ë‚´ëŠ” ê²°ì •ì  ìš”ì¸ì„ ì •í™•íˆ ì§€ì í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:28:52*