# Sparsity May Be All You Need: Sparse Random Parameter Adaptation

**Korean Title:** í¬ì†Œì„±ë§Œìœ¼ë¡œ ì¶©ë¶„í•  ìˆ˜ ìˆë‹¤: í¬ì†Œ ëœë¤ ë§¤ê°œë³€ìˆ˜ ì ì‘

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Low Rank Adaptation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Don't Forget the Nonlinearity_ Unlocking Activation Functions in Efficient Fine-Tuning_20250919|Don't Forget the Nonlinearity Unlocking Activation Functions in Efficient Fine-Tuning]] (86.3% similar)
- [[2025-09-22/BEFT_ Bias-Efficient Fine-Tuning of Language Models_20250922|BEFT Bias-Efficient Fine-Tuning of Language Models]] (83.3% similar)
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM Hierarchical Adapter Merging for Scalable Continual Learning]] (82.3% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (82.3% similar)
- [[2025-09-17/Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications_20250917|Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications]] (82.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.15975v3 Announce Type: replace-cross 
Abstract: Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on, while fixing all other parameters, without any additional prior assumptions such as low-rank structures. In this paper, we compare the efficiency and performance of our proposed approach to other PEFT methods as well as full parameter fine-tuning. We find our method to be competitive with LoRA when using a similar number of trainable parameters. Our findings suggest that what truly matters for a PEFT technique to perform well is not necessarily the specific adapter structure, but rather the number of trainable parameters being used.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.15975v3 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì •ë ¬ ë° ì‘ì—… ì ì‘ì„ ìœ„í•œ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì€ ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ ì§€ë‚˜ì¹˜ê²Œ ë¹„ìš©ì´ ë§ì´ ë“¤ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì€ ëª¨ë“  ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ëŒ€ì‹  ì†Œìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ë§Œì„ í•™ìŠµí•˜ì—¬ ì´ëŸ¬í•œ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì— í•„ìš”í•œ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ìì›ì„ í¬ê²Œ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. í˜„ì¬ ê°€ì¥ ì¸ê¸° ìˆëŠ” PEFT ë°©ë²•ì€ Low-Rank Adaptation (LoRA)ë¡œ, ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ì •í•˜ê³  ì €ë­í¬ í–‰ë ¬ í˜•íƒœì˜ ì†Œìˆ˜ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì €ë­í¬ êµ¬ì¡°ì™€ ê°™ì€ ì¶”ê°€ì ì¸ ì‚¬ì „ ê°€ì • ì—†ì´, ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ì¤‘ ì†Œìˆ˜ì˜ ë¹„ìœ¨ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ í•™ìŠµí•˜ê³  ë‚˜ë¨¸ì§€ ë§¤ê°œë³€ìˆ˜ëŠ” ê³ ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¨ìˆœíˆ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ì œì•ˆëœ ì ‘ê·¼ë²•ì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ë‹¤ë¥¸ PEFT ë°©ë²• ë° ì „ì²´ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •ê³¼ ë¹„êµí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìœ ì‚¬í•œ ìˆ˜ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ LoRAì™€ ê²½ìŸí•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” PEFT ê¸°ìˆ ì´ ì˜ ìˆ˜í–‰ë˜ê¸° ìœ„í•´ ì§„ì •ìœ¼ë¡œ ì¤‘ìš”í•œ ê²ƒì€ íŠ¹ì • ì–´ëŒ‘í„° êµ¬ì¡°ê°€ ì•„ë‹ˆë¼ ì‚¬ìš©ë˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì€ ëª¨ë¸ í¬ê¸°ì˜ ì¦ê°€ë¡œ ì¸í•´ ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. ì´ì— ë”°ë¼, íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì´ ì£¼ëª©ë°›ê³  ìˆìœ¼ë©°, ì´ ì¤‘ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë°©ë²•ì€ ì €ë­í¬ ì ì‘(LoRA)ì…ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì €ë­í¬ êµ¬ì¡°ì™€ ê°™ì€ ì¶”ê°€ ê°€ì • ì—†ì´, ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¤‘ ì¼ë¶€ë§Œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ë¥¸ PEFT ë°©ë²• ë° ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •ê³¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, PEFT ê¸°ìˆ ì˜ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ê²ƒì€ ì–´ëŒ‘í„° êµ¬ì¡°ê°€ ì•„ë‹Œ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì€ ëª¨ë¸ í¬ê¸°ì˜ ì¦ê°€ë¡œ ì¸í•´ ë¹„ìš©ì´ ë§¤ìš° ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.

- 2. PEFT ë°©ë²•ì€ ëª¨ë¸ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ê°€ ì•„ë‹Œ ì†Œìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ë§Œì„ í›ˆë ¨í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ì— í•„ìš”í•œ ìì›ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

- 3. í˜„ì¬ ê°€ì¥ ì¸ê¸° ìˆëŠ” PEFT ë°©ë²•ì€ LoRAë¡œ, ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ì •í•˜ê³  ì €ë­í¬ í–‰ë ¬ í˜•íƒœì˜ ì†Œìˆ˜ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤.

- 4. ìš°ë¦¬ëŠ” ì €ë­í¬ êµ¬ì¡°ì™€ ê°™ì€ ì¶”ê°€ì ì¸ ê°€ì • ì—†ì´ ì„ì˜ë¡œ ì„ íƒëœ ì†Œìˆ˜ì˜ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë§Œì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ìœ ì‚¬í•œ ìˆ˜ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ LoRAì™€ ê²½ìŸí•  ìˆ˜ ìˆìœ¼ë©°, PEFT ê¸°ìˆ ì˜ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ê²ƒì€ íŠ¹ì • ì–´ëŒ‘í„° êµ¬ì¡°ê°€ ì•„ë‹ˆë¼ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:43:18*