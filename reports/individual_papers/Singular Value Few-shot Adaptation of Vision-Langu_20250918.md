
# Singular Value Few-shot Adaptation of Vision-Language Models

**Korean Title:** ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ íŠ¹ì´ê°’ ì†Œìˆ˜ ìƒ· ì ì‘

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Multi-modal Adaptation|Multi-modal Adaptation]] [[keywords/broad/Vision-Language Models|Vision-Language Models]] [[keywords/broad/Singular Value Decomposition|Singular Value Decomposition]] [[keywords/specific/Few-shot Learning|Few-shot Learning]] [[keywords/unique/CLIP-SVD|CLIP-SVD]] [[categories/cs.CL|cs.CL]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multi-modal Adaptation
**ğŸ”¬ Broad Technical**: Vision-Language Models, Singular Value Decomposition
**ğŸ”— Specific Connectable**: Few-shot Learning
**â­ Unique Technical**: CLIP-SVD

**ArXiv ID**: [2509.03740](https://arxiv.org/abs/2509.03740)
**Published**: 2025-09-18
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.03740.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Vision-Language Models` â€¢ 

`Singular Value Decomposition` â€¢ 

`Few-shot Learning` â€¢ 

`CLIP-SVD` â€¢ 

`Domain Adaptation`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.03740v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.03740v2 ë°œí‘œ ìœ í˜•: replace-cross
ìš”ì•½: CLIPì™€ ê°™ì€ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì¸ìƒì ì¸ ì œë¡œìƒ· ë° í“¨ìƒ· í•™ìŠµ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ëª¨ë¸ì„ ìƒˆë¡œìš´ ì„¸ë¶€ ë„ë©”ì¸ì— ì ì‘ì‹œí‚¤ëŠ” ê²ƒì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì— ì˜ì¡´í•˜ê³  ì „ì²´ ëª¨ë¸ íŒŒì¸íŠœë‹ì˜ ë†’ì€ ë¹„ìš© ë•Œë¬¸ì— ì—¬ì „íˆ ì–´ë ¤ì›€ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì ì‘ ì ‘ê·¼ ë°©ì‹ì€ í”„ë¡¬í”„íŠ¸ í† í° ë° ì–´ëŒ‘í„° ëª¨ë“ˆê³¼ ê°™ì€ ë³´ì¡° êµ¬ì„± ìš”ì†Œì— ì˜ì¡´í•˜ëŠ”ë°, ì´ëŠ” ì ì‘ í’ˆì§ˆì„ ì œí•œí•˜ê³  ëª¨ë¸ì„ ë¶ˆì•ˆì •í•˜ê²Œ ë§Œë“¤ë©° ì‚¬ì „ í›ˆë ¨ ì¤‘ì— í•™ìŠµí•œ í’ë¶€í•œ ì§€ì‹ì„ ì†ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” CLIP-SVDë¼ëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ ë° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ì ì‘ ê¸°ìˆ ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ Singular Value Decomposition (SVD)ì„ í™œìš©í•˜ì—¬ CLIPì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ê³µê°„ì„ ìˆ˜ì •í•˜ì—¬ ì¶”ê°€ ëª¨ë“ˆì„ ì£¼ì…í•˜ì§€ ì•Šê³ ë„ ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ ê¸°ì € ë²¡í„°ë¥¼ ì¬ì¡°ì •í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” CLIP íŒŒë¼ë¯¸í„° í–‰ë ¬ì˜ íŠ¹ì´ê°’ë§Œì„ íŒŒì¸íŠœë‹í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ìœ ì§€í•œ ì±„ ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ ê¸°ì € ë²¡í„°ë¥¼ ì¬ì¡°ì •í•©ë‹ˆë‹¤. ì´ ì„¤ê³„ëŠ” ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ì¤‘ 0.04%ë§Œì„ ì‚¬ìš©í•˜ì—¬ í–¥ìƒëœ ì ì‘ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë” ì˜ ë³´ì¡´í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. CLIP-SVDëŠ” 11ê°œì˜ ìì—° ë° 10ê°œì˜ ìƒë¬¼ì˜í•™ ë°ì´í„°ì…‹ì—ì„œ ìµœì‹  ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, ì ì€ ìƒ· ì„¤ì •ì—ì„œ ì •í™•ë„ì™€ ì¼ë°˜í™” ë©´ì—ì„œ ì´ì „ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” CLIP ì ì‘ì˜ íš¨ê³¼ì™€ ì—­í•™ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ìì—°ì–´ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ í™œìš©í•˜ì—¬ CLIP-SVDì˜ í•´ì„ ê°€ëŠ¥ì„±ì„ í—ˆìš©í•©ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/HealthX-Lab/CLIP-SVDì—ì„œ ê³µê°œì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” CLIP-SVDë¼ëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ ë° ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ ì ì‘ ê¸°ìˆ ì„ ì œì•ˆí•œë‹¤. ì´ ê¸°ìˆ ì€ CLIPì˜ ë‚´ë¶€ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì„ ìˆ˜ì •í•˜ê¸° ìœ„í•´ íŠ¹ì´ê°’ ë¶„í•´(SVD)ë¥¼ í™œìš©í•˜ë©°, ì¶”ê°€ ëª¨ë“ˆì„ ì‚½ì…í•˜ì§€ ì•Šê³  ë„ë©”ì¸ ì ì‘ì„ ìœ„í•´ CLIP ë§¤ê°œë³€ìˆ˜ í–‰ë ¬ì˜ íŠ¹ì´ê°’ë§Œì„ ë¯¸ì„¸ì¡°ì •í•œë‹¤. ì´ ì„¤ê³„ëŠ” ëª¨ë¸ì˜ ì´ ë§¤ê°œë³€ìˆ˜ì˜ 0.04%ë§Œì„ ì‚¬ìš©í•˜ì—¬ í–¥ìƒëœ ì ì‘ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë” ì˜ ë³´ì¡´í•  ìˆ˜ ìˆê²Œ í•œë‹¤. CLIP-SVDëŠ” 11ê°œì˜ ìì—° ë° 10ê°œì˜ ìƒë¬¼ì˜í•™ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, ì ì€ ìƒ· ì„¤ì •ì—ì„œ ì •í™•ë„ì™€ ì¼ë°˜í™” ë©´ì—ì„œ ì´ì „ ë°©ë²•ì„ ëŠ¥ê°€í•œë‹¤. ë˜í•œ CLIP-SVDì˜ íš¨ê³¼ì™€ ì—­í•™ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ìì—°ì–´ ê¸°ë°˜ ì ‘ê·¼ë²•ì„ í™œìš©í•˜ì—¬ CLIPì˜ ì ì‘ì„ í•´ì„í•  ìˆ˜ ìˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. CLIP-SVDëŠ” prompt engineeringì— ì˜ì¡´í•˜ì§€ ì•Šê³  ìƒˆë¡œìš´ ì„¸ë¶€ ë„ë©”ì¸ì— ì ì‘í•˜ëŠ” í˜ì‹ ì ì¸ ë©€í‹°ëª¨ë‹¬ ë° ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ ì ì‘ ê¸°ìˆ ì´ë‹¤.

- 2. CLIP-SVDëŠ” CLIPì˜ ë‚´ë¶€ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì„ ìˆ˜ì •í•˜ê¸° ìœ„í•´ SVDë¥¼ í™œìš©í•˜ë©° ì¶”ê°€ ëª¨ë“ˆì„ ì£¼ì…í•˜ì§€ ì•ŠëŠ”ë‹¤.

- 3. CLIP-SVDëŠ” ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë” ì˜ ë³´ì¡´í•˜ë©´ì„œ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ìµœê³  ìˆ˜ì¤€ì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.


---

*Generated on 2025-09-18 16:58:10*