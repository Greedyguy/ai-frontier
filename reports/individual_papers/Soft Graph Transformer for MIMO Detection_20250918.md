
# Soft Graph Transformer for MIMO Detection

**Korean Title:** MIMO ê°ì§€ë¥¼ ìœ„í•œ ì†Œí”„íŠ¸ ê·¸ë˜í”„ íŠ¸ëœìŠ¤í¬ë¨¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Graph-aware Cross-attention|Graph-aware Cross-attention]] [[keywords/broad/MIMO Detection|MIMO Detection]] [[keywords/broad/Transformer|Transformer]] [[keywords/specific/Soft Graph Transformer|Soft Graph Transformer]] [[keywords/unique/SGT|SGT]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Structured Message Passing
**ğŸ”¬ Broad Technical**: MIMO Detection, Transformer
**ğŸ”— Specific Connectable**: Soft Graph Transformer
**â­ Unique Technical**: SGT

**ArXiv ID**: [2509.12694](https://arxiv.org/abs/2509.12694)
**Published**: 2025-09-18
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.12694.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`MIMO Detection` â€¢ 

`Transformer` â€¢ 

`Soft Graph Transformer` â€¢ 

`SGT` â€¢ 

`Structured Message Passing`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.12694v2 Announce Type: replace 
Abstract: We propose the Soft Graph Transformer (SGT), a soft-input-soft-output neural architecture designed for MIMO detection. While Maximum Likelihood (ML) detection achieves optimal accuracy, its exponential complexity makes it infeasible in large systems, and conventional message-passing algorithms rely on asymptotic assumptions that often fail in finite dimensions. Recent Transformer-based detectors show strong performance but typically overlook the MIMO factor graph structure and cannot exploit prior soft information. SGT addresses these limitations by combining self-attention, which encodes contextual dependencies within symbol and constraint subgraphs, with graph-aware cross-attention, which performs structured message passing across subgraphs. Its soft-input interface allows the integration of auxiliary priors, producing effective soft outputs while maintaining computational efficiency. Experiments demonstrate that SGT achieves near-ML performance and offers a flexible and interpretable framework for receiver systems that leverage soft priors.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.12694v2 ë°œí‘œ ìœ í˜•: ëŒ€ì²´
ìš”ì•½: ìš°ë¦¬ëŠ” MIMO ê°ì§€ë¥¼ ìœ„í•´ ì„¤ê³„ëœ ì†Œí”„íŠ¸ ì…ë ¥-ì†Œí”„íŠ¸ ì¶œë ¥ ì‹ ê²½ êµ¬ì¡° ì¸ Soft Graph Transformer (SGT)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìµœëŒ€ ìš°ë„ (ML) ê°ì§€ëŠ” ìµœì ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì§€ë§Œ ì§€ìˆ˜ì  ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì—ì„œ ë¶ˆê°€ëŠ¥í•˜ë©°, ì „í†µì ì¸ ë©”ì‹œì§€ ì „ë‹¬ ì•Œê³ ë¦¬ì¦˜ì€ ì¢…ì¡±ì  ê°€ì •ì— ì˜ì¡´í•˜ì—¬ ì¢…ì¡±ì  ê°€ì •ì´ ì¢…ì¢… ìœ í•œ ì°¨ì›ì—ì„œ ì‹¤íŒ¨í•©ë‹ˆë‹¤. ìµœê·¼ì˜ Transformer ê¸°ë°˜ ê°ì§€ê¸°ëŠ” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ MIMO íŒ©í„° ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ê°„ê³¼í•˜ê³  ì´ì „ì˜ ì†Œí”„íŠ¸ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. SGTëŠ” ì‹¬ë³¼ ë° ì œì•½ ë¶€ë¶„ ê·¸ë˜í”„ ë‚´ì—ì„œ ë¬¸ë§¥ ì˜ì¡´ì„±ì„ ì¸ì½”ë”©í•˜ëŠ” ìê¸° ì£¼ì˜(self-attention)ì™€ ë¶€ë¶„ ê·¸ë˜í”„ ê°„ì— êµ¬ì¡°í™”ëœ ë©”ì‹œì§€ ì „ë‹¬ì„ ìˆ˜í–‰í•˜ëŠ” ê·¸ë˜í”„ ì¸ì‹ êµì°¨ ì£¼ì˜(cross-attention)ë¥¼ ê²°í•©í•˜ì—¬ ì´ëŸ¬í•œ ì œí•œ ì‚¬í•­ì„ í•´ê²°í•©ë‹ˆë‹¤. ê·¸ì˜ ì†Œí”„íŠ¸ ì…ë ¥ ì¸í„°í˜ì´ìŠ¤ëŠ” ë³´ì¡° ì‚¬ì „ì„ í†µí•©í•˜ì—¬ íš¨ê³¼ì ì¸ ì†Œí”„íŠ¸ ì¶œë ¥ì„ ìƒì„±í•˜ë©´ì„œ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” SGTê°€ ê±°ì˜ ML ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ì†Œí”„íŠ¸ ì‚¬ì „ì„ í™œìš©í•˜ëŠ” ìˆ˜ì‹ ê¸° ì‹œìŠ¤í…œì— ëŒ€í•´ ìœ ì—°í•˜ê³  í•´ì„ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë³¸ ì—°êµ¬ì—ì„œëŠ” MIMO ê²€ì¶œì„ ìœ„í•´ ì„¤ê³„ëœ ì†Œí”„íŠ¸ ì…ë ¥-ì†Œí”„íŠ¸ ì¶œë ¥ ì‹ ê²½ êµ¬ì¡° ì¸ Soft Graph Transformer (SGT)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìµœëŒ€ ìš°ë„ (ML) ê²€ì¶œì€ ìµœì ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì§€ë§Œ ì§€ìˆ˜ì ì¸ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì—ì„œ ì‹¤í–‰í•˜ê¸° ì–´ë µê³ , ì „í†µì ì¸ ë©”ì‹œì§€ ì „ë‹¬ ì•Œê³ ë¦¬ì¦˜ì€ ì¢…ì¢… ìœ í•œ ì°¨ì›ì—ì„œ ì‹¤íŒ¨í•©ë‹ˆë‹¤. ìµœê·¼ Transformer ê¸°ë°˜ ê²€ì¶œê¸°ëŠ” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ MIMO ìš”ì¸ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ê°„ê³¼í•˜ê³  ì†Œí”„íŠ¸ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. SGTëŠ” ì‹¬ë³¼ ë° ì œì•½ ë¶€ë¶„ ê·¸ë˜í”„ ë‚´ì—ì„œ ë¬¸ë§¥ ì˜ì¡´ì„±ì„ ì¸ì½”ë”©í•˜ëŠ” self-attentionê³¼ ë¶€ë¶„ ê·¸ë˜í”„ ê°„ì— êµ¬ì¡°í™”ëœ ë©”ì‹œì§€ ì „ë‹¬ì„ ìˆ˜í–‰í•˜ëŠ” ê·¸ë˜í”„ ì¸ì‹ cross-attentionì„ ê²°í•©í•˜ì—¬ ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SGTëŠ” ê±°ì˜ ML ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©° ì†Œí”„íŠ¸ ì‚¬ì „ì„ í™œìš©í•˜ëŠ” ìˆ˜ì‹ ê¸° ì‹œìŠ¤í…œì— ëŒ€í•œ ìœ ì—°í•˜ê³  í•´ì„ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Soft Graph Transformer (SGT)ëŠ” MIMO ê°ì§€ë¥¼ ìœ„í•´ ì„¤ê³„ëœ ì†Œí”„íŠ¸ ì…ë ¥-ì†Œí”„íŠ¸ ì¶œë ¥ ì‹ ê²½ êµ¬ì¡°ì´ë‹¤.

- 2. SGTëŠ” ìê¸° ì£¼ì˜(self-attention)ì™€ ê·¸ë˜í”„ ì¸ì‹ êµì°¨ ì£¼ì˜(graph-aware cross-attention)ë¥¼ ê²°í•©í•˜ì—¬ ì œí•œ ì„œë¸Œê·¸ë˜í”„ ê°„ì— êµ¬ì¡°í™”ëœ ë©”ì‹œì§€ ì „ë‹¬ì„ ìˆ˜í–‰í•œë‹¤.

- 3. SGTëŠ” ë¶€ê°€ì ì¸ ì‚¬ì „ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ íš¨ê³¼ì ì¸ ì†Œí”„íŠ¸ ì¶œë ¥ì„ ìƒì„±í•˜ë©´ì„œ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•œë‹¤.


---

*Generated on 2025-09-18 16:48:20*