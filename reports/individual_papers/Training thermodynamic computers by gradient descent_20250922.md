# Training thermodynamic computers by gradient descent

**Korean Title:** ì—´ì—­í•™ ì»´í“¨í„°ì˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•œ í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Neural Network Dynamics

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (83.4% similar)
- [[2025-09-19/Trainability of Quantum Models Beyond Known Classical Simulability_20250919|Trainability of Quantum Models Beyond Known Classical Simulability]] (80.7% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (80.3% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks A Second-Order Optimization Perspective]] (80.2% similar)
- [[2025-09-17/A Neural Network for the Identical Kuramoto Equation_ Architectural Considerations and Performance Evaluation_20250917|A Neural Network for the Identical Kuramoto Equation Architectural Considerations and Performance Evaluation]] (80.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15324v1 Announce Type: cross 
Abstract: We show how to adjust the parameters of a thermodynamic computer by gradient descent in order to perform a desired computation at a specified observation time. Within a digital simulation of a thermodynamic computer, training proceeds by maximizing the probability with which the computer would generate an idealized dynamical trajectory. The idealized trajectory is designed to reproduce the activations of a neural network trained to perform the desired computation. This teacher-student scheme results in a thermodynamic computer whose finite-time dynamics enacts a computation analogous to that of the neural network. The parameters identified in this way can be implemented in the hardware realization of the thermodynamic computer, which will perform the desired computation automatically, driven by thermal noise. We demonstrate the method on a standard image-classification task, and estimate the thermodynamic advantage -- the ratio of energy costs of the digital and thermodynamic implementations -- to exceed seven orders of magnitude. Our results establish gradient descent as a viable training method for thermodynamic computing, enabling application of the core methodology of machine learning to this emerging field.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15324v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ìš°ë¦¬ëŠ” ì§€ì •ëœ ê´€ì°° ì‹œê°„ì— ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì—´ì—­í•™ì  ì»´í“¨í„°ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—´ì—­í•™ì  ì»´í“¨í„°ì˜ ë””ì§€í„¸ ì‹œë®¬ë ˆì´ì…˜ ë‚´ì—ì„œ, í›ˆë ¨ì€ ì»´í“¨í„°ê°€ ì´ìƒì ì¸ ë™ì  ê¶¤ì ì„ ìƒì„±í•  í™•ë¥ ì„ ìµœëŒ€í™”í•¨ìœ¼ë¡œì¨ ì§„í–‰ë©ë‹ˆë‹¤. ì´ìƒì ì¸ ê¶¤ì ì€ ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë„ë¡ í›ˆë ¨ëœ ì‹ ê²½ë§ì˜ í™œì„±í™”ë¥¼ ì¬í˜„í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ êµì‚¬-í•™ìƒ ì²´ê³„ëŠ” ìœ í•œ ì‹œê°„ ë™ì—­í•™ì´ ì‹ ê²½ë§ì˜ ê³„ì‚°ê³¼ ìœ ì‚¬í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ì—´ì—­í•™ì  ì»´í“¨í„°ë¥¼ ê²°ê³¼ë¡œ ë‚³ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì‹ë³„ëœ ë§¤ê°œë³€ìˆ˜ëŠ” ì—´ì—­í•™ì  ì»´í“¨í„°ì˜ í•˜ë“œì›¨ì–´ êµ¬í˜„ì— ì ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì—´ì¡ìŒì— ì˜í•´ ìë™ìœ¼ë¡œ ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í‘œì¤€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì´ ë°©ë²•ì„ ì‹œì—°í•˜ê³ , ë””ì§€í„¸ ë° ì—´ì—­í•™ì  êµ¬í˜„ì˜ ì—ë„ˆì§€ ë¹„ìš© ë¹„ìœ¨ì¸ ì—´ì—­í•™ì  ì´ì ì„ 7ì°¨ ì´ìƒì˜ í¬ê¸°ë¡œ ì´ˆê³¼í•œë‹¤ê³  ì¶”ì •í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ì—´ì—­í•™ì  ì»´í“¨íŒ…ì„ ìœ„í•œ ì‹¤í˜„ ê°€ëŠ¥í•œ í›ˆë ¨ ë°©ë²•ìœ¼ë¡œì„œ ê²½ì‚¬ í•˜ê°•ë²•ì„ í™•ë¦½í•˜ë©°, ì´ ì‹ í¥ ë¶„ì•¼ì— ê¸°ê³„ í•™ìŠµì˜ í•µì‹¬ ë°©ë²•ë¡ ì„ ì ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì—´ì—­í•™ì  ì»´í“¨í„°ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ì¡°ì •í•˜ì—¬ íŠ¹ì • ì‹œê°„ì— ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ë””ì§€í„¸ ì‹œë®¬ë ˆì´ì…˜ ë‚´ì—ì„œ, ì»´í“¨í„°ê°€ ì´ìƒì ì¸ ë™ì  ê¶¤ì ì„ ìƒì„±í•  í™•ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ í›ˆë ¨ì´ ì§„í–‰ë©ë‹ˆë‹¤. ì´ ê¶¤ì ì€ ì‹ ê²½ë§ì˜ í™œì„±í™”ë¥¼ ì¬í˜„í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ì—´ì—­í•™ì  ì»´í“¨í„°ê°€ ì‹ ê²½ë§ê³¼ ìœ ì‚¬í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë§¤ê°œë³€ìˆ˜ëŠ” í•˜ë“œì›¨ì–´ì— êµ¬í˜„ë˜ì–´ ì—´ì¡ìŒì— ì˜í•´ ìë™ìœ¼ë¡œ ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í‘œì¤€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì´ ë°©ë²•ì„ ì‹œì—°í•˜ì˜€ìœ¼ë©°, ë””ì§€í„¸ êµ¬í˜„ê³¼ ì—´ì—­í•™ì  êµ¬í˜„ì˜ ì—ë„ˆì§€ ë¹„ìš© ë¹„ìœ¨ì´ 7ìë¦¿ìˆ˜ ì´ìƒì„ì„ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ì—´ì—­í•™ì  ì»´í“¨íŒ…ì—ì„œ ê²½ì‚¬ í•˜ê°•ë²•ì´ ìœ íš¨í•œ í›ˆë ¨ ë°©ë²•ì„ì„ ì…ì¦í•˜ë©°, ê¸°ê³„ í•™ìŠµì˜ í•µì‹¬ ë°©ë²•ë¡ ì„ ì´ ìƒˆë¡œìš´ ë¶„ì•¼ì— ì ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì—´ì—­í•™ì  ì»´í“¨í„°ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ì¡°ì •í•˜ì—¬ ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

- 2. ë””ì§€í„¸ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì—´ì—­í•™ì  ì»´í“¨í„°ì˜ í›ˆë ¨ì€ ì´ìƒì ì¸ ë™ì  ê¶¤ì ì„ ìƒì„±í•  í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

- 3. ì´ ë°©ë²•ì€ ì‹ ê²½ë§ì˜ ê³„ì‚°ì„ ëª¨ë°©í•˜ëŠ” ì—´ì—­í•™ì  ì»´í“¨í„°ë¥¼ êµ¬í˜„í•˜ë©°, í•˜ë“œì›¨ì–´ì—ì„œ ìë™ìœ¼ë¡œ ì›í•˜ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

- 4. í‘œì¤€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì— ì´ ë°©ë²•ì„ ì ìš©í•˜ì—¬ ë””ì§€í„¸ êµ¬í˜„ê³¼ ì—´ì—­í•™ì  êµ¬í˜„ì˜ ì—ë„ˆì§€ ë¹„ìš© ë¹„ìœ¨ì´ 7ìë¦¿ìˆ˜ ì´ìƒì„ì„ ì¶”ì •í•©ë‹ˆë‹¤.

- 5. ê²½ì‚¬ í•˜ê°•ë²•ì´ ì—´ì—­í•™ì  ì»´í“¨íŒ…ì˜ ìœ íš¨í•œ í›ˆë ¨ ë°©ë²•ì„ì„ ì…ì¦í•˜ì—¬, ê¸°ê³„ í•™ìŠµì˜ í•µì‹¬ ë°©ë²•ë¡ ì„ ì´ ìƒˆë¡œìš´ ë¶„ì•¼ì— ì ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:37:06*