# Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers

**Korean Title:** í•˜ì´ë¸Œë¦¬ë“œ ë‹¨í•­-ì´í•­ ì„¤ê³„: ê³±ì…ˆê¸° ì—†ëŠ” ì¸ì‡„í˜• ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Architecture Aware Training

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (83.3% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (81.7% similar)
- [[2025-09-19/MaRVIn_ A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration_20250919|MaRVIn A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration]] (80.0% similar)
- [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (79.2% similar)
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need Sparse Random Parameter Adaptation]] (79.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15316v1 Announce Type: new 
Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to silicon for implementing machine learning (ML) circuits, but their large feature sizes limit classifier complexity. Leveraging PE's low fabrication and NRE costs, designers can tailor hardware to specific ML models, simplifying circuit design. This work explores alternative arithmetic and proposes a hybrid unary-binary architecture that removes costly encoders and enables efficient, multiplier-less execution of MLP classifiers. We also introduce architecture-aware training to further improve area and power efficiency. Evaluation on six datasets shows average reductions of 46% in area and 39% in power, with minimal accuracy loss, surpassing other state-of-the-art MLP designs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15316v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì¸ì‡„ ì „ì(Printed Electronics, PE)ëŠ” ê¸°ê³„ í•™ìŠµ(ML) íšŒë¡œ êµ¬í˜„ì— ìˆì–´ ì‹¤ë¦¬ì½˜ì— ëŒ€í•œ ìœ ì—°í•˜ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ ëŒ€ì•ˆì„ ì œê³µí•˜ì§€ë§Œ, í° í”¼ì²˜ í¬ê¸°ë¡œ ì¸í•´ ë¶„ë¥˜ê¸° ë³µì¡ì„±ì´ ì œí•œë©ë‹ˆë‹¤. PEì˜ ë‚®ì€ ì œì¡° ë° ë¹„ë°˜ë³µì  ì—”ì§€ë‹ˆì–´ë§(NRE) ë¹„ìš©ì„ í™œìš©í•˜ì—¬, ì„¤ê³„ìëŠ” íŠ¹ì • ML ëª¨ë¸ì— ë§ì¶˜ í•˜ë“œì›¨ì–´ë¥¼ ì„¤ê³„í•˜ì—¬ íšŒë¡œ ì„¤ê³„ë¥¼ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ëŒ€ì²´ ì‚°ìˆ ì„ íƒêµ¬í•˜ê³ , ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì¸ì½”ë”ë¥¼ ì œê±°í•˜ê³  ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP) ë¶„ë¥˜ê¸°ì˜ íš¨ìœ¨ì ì¸ ê³±ì…ˆê¸° ì—†ëŠ” ì‹¤í–‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ìœ ë‹ˆí„°ë¦¬-ë°”ì´ë„ˆë¦¬ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, ì˜ì—­ ë° ì „ë ¥ íš¨ìœ¨ì„±ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì•„í‚¤í…ì²˜ ì¸ì‹ í›ˆë ¨ì„ ë„ì…í•©ë‹ˆë‹¤. ì—¬ì„¯ ê°œì˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼, í‰ê· ì ìœ¼ë¡œ ì˜ì—­ì€ 46%, ì „ë ¥ì€ 39% ê°ì†Œí•˜ë©°, ë‹¤ë¥¸ ìµœì²¨ë‹¨ MLP ì„¤ê³„ë¥¼ ëŠ¥ê°€í•˜ëŠ” ìµœì†Œí•œì˜ ì •í™•ë„ ì†ì‹¤ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ì‡„ ì „ì(PE)ë¥¼ í™œìš©í•œ ê¸°ê³„ í•™ìŠµ(ML) íšŒë¡œ ì„¤ê³„ì˜ íš¨ìœ¨ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤. PEëŠ” ì‹¤ë¦¬ì½˜ ëŒ€ë¹„ ìœ ì—°í•˜ê³  ë¹„ìš© íš¨ìœ¨ì ì´ì§€ë§Œ, í° íŠ¹ì§• í¬ê¸°ë¡œ ì¸í•´ ë¶„ë¥˜ê¸° ë³µì¡ì„±ì´ ì œí•œë©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìëŠ” í•˜ì´ë¸Œë¦¬ë“œ ìœ ë‹ˆí„°ë¦¬-ë°”ì´ë„ˆë¦¬ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•˜ì—¬ ê³ ê°€ì˜ ì¸ì½”ë”ë¥¼ ì œê±°í•˜ê³ , ê³±ì…ˆê¸° ì—†ì´ MLP ë¶„ë¥˜ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì•„í‚¤í…ì²˜ ì¸ì‹ í›ˆë ¨ì„ ë„ì…í•˜ì—¬ ë©´ì ê³¼ ì „ë ¥ íš¨ìœ¨ì„±ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. 6ê°œì˜ ë°ì´í„°ì…‹ í‰ê°€ ê²°ê³¼, í‰ê· ì ìœ¼ë¡œ ë©´ì ì€ 46%, ì „ë ¥ì€ 39% ê°ì†Œí•˜ë©´ì„œë„ ì •í™•ë„ ì†ì‹¤ì€ ìµœì†Œí™”ë˜ì–´ ê¸°ì¡´ MLP ì„¤ê³„ë¥¼ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ì‡„ ì „ì(PE)ëŠ” ì‹¤ë¦¬ì½˜ì— ë¹„í•´ ìœ ì—°í•˜ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ ê¸°ê³„ í•™ìŠµ(ML) íšŒë¡œ êµ¬í˜„ ëŒ€ì•ˆì„ ì œê³µí•˜ì§€ë§Œ, í° íŠ¹ì§• í¬ê¸°ë¡œ ì¸í•´ ë¶„ë¥˜ê¸° ë³µì¡ì„±ì´ ì œí•œëœë‹¤.

- 2. PEì˜ ë‚®ì€ ì œì‘ ë° NRE ë¹„ìš©ì„ í™œìš©í•˜ì—¬ í•˜ë“œì›¨ì–´ë¥¼ íŠ¹ì • ML ëª¨ë¸ì— ë§ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” íšŒë¡œ ì„¤ê³„ë¥¼ ë‹¨ìˆœí™”í•œë‹¤.

- 3. ë³¸ ì—°êµ¬ëŠ” ëŒ€ì•ˆì ì¸ ì‚°ìˆ  ë°©ì‹ì„ íƒêµ¬í•˜ê³ , ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì¸ì½”ë”ë¥¼ ì œê±°í•˜ë©° íš¨ìœ¨ì ì¸ ê³±ì…ˆê¸° ì—†ëŠ” MLP ë¶„ë¥˜ê¸° ì‹¤í–‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ìœ ë‹ˆí„°ë¦¬-ë°”ì´ë„ˆë¦¬ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•œë‹¤.

- 4. ì•„í‚¤í…ì²˜ ì¸ì‹ í›ˆë ¨ì„ ë„ì…í•˜ì—¬ ë©´ì  ë° ì „ë ¥ íš¨ìœ¨ì„±ì„ ë”ìš± í–¥ìƒì‹œí‚¨ë‹¤.

- 5. ì—¬ì„¯ ê°œì˜ ë°ì´í„°ì…‹ í‰ê°€ ê²°ê³¼, í‰ê· ì ìœ¼ë¡œ ë©´ì  46% ë° ì „ë ¥ 39% ê°ì†Œë¥¼ ë³´ì´ë©°, ì •í™•ë„ ì†ì‹¤ì´ ìµœì†Œí™”ë˜ì–´ ë‹¤ë¥¸ ìµœì²¨ë‹¨ MLP ì„¤ê³„ë¥¼ ëŠ¥ê°€í•œë‹¤.

---

*Generated on 2025-09-22 15:09:47*