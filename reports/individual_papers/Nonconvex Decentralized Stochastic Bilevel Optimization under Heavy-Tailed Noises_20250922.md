# Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises

**Korean Title:** 비대칭 꼬리 잡음 하의 비볼록 분산 확률적 이층 최적화

## 📋 메타데이터

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Nonconvex Optimization under Heavy-Tailed Noises

## 🔗 유사한 논문
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (88.5% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (87.1% similar)
- [[2025-09-18/Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (83.7% similar)
- [[2025-09-17/Decentralized Optimization with Topology-Independent Communication_20250917|Decentralized Optimization with Topology-Independent Communication]] (83.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (80.9% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15543v1 Announce Type: new 
Abstract: Existing decentralized stochastic optimization methods assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.

## 🔍 Abstract (한글 번역)

arXiv:2509.15543v1 발표 유형: 신규  
초록: 기존의 분산 확률적 최적화 방법은 하위 수준의 손실 함수가 강하게 볼록하고 확률적 기울기 노이즈가 유한한 분산을 갖는다고 가정합니다. 이러한 강한 가정은 일반적으로 실제 세계의 기계 학습 모델에서는 충족되지 않습니다. 이러한 한계를 해결하기 위해, 우리는 비볼록 이중 수준 최적화 문제에서 중간 꼬리 노이즈 하에 새로운 분산 확률적 이중 수준 최적화 알고리즘을 개발합니다. 구체적으로, 우리는 클리핑 작업에 의존하지 않는 정규화된 확률적 분산 감소 이중 수준 기울기 하강 알고리즘을 개발합니다. 더욱이, 우리는 비볼록 분산 이중 수준 최적화 문제에 대해 중간 꼬리 노이즈 하에서 상호 의존적인 기울기 시퀀스를 혁신적으로 제한함으로써 그 수렴 속도를 확립합니다. 우리가 아는 한, 이것은 중간 꼬리 노이즈 하에서 엄격한 이론적 보장을 갖춘 최초의 분산 이중 수준 최적화 알고리즘입니다. 광범위한 실험 결과는 중간 꼬리 노이즈를 처리하는 데 있어 우리의 알고리즘의 효과를 확인합니다.

## 📝 요약

이 논문은 기존의 분산 확률 최적화 방법이 강한 볼록성과 유한한 분산의 확률적 기울기 노이즈를 가정하는 한계를 극복하기 위해, 비볼록 이중 수준 최적화 문제에서 중대한 노이즈를 다루는 새로운 알고리즘을 제안합니다. 제안된 알고리즘은 정규화된 확률적 분산 감소 이중 수준 기울기 하강법을 사용하며, 클리핑 연산에 의존하지 않습니다. 또한, 중대한 노이즈 하에서 비볼록 분산 이중 수준 최적화 문제의 수렴 속도를 이론적으로 보장합니다. 이는 중대한 노이즈 하에서 이론적 보장을 제공하는 최초의 분산 이중 수준 최적화 알고리즘입니다. 실험 결과는 제안된 알고리즘의 효과성을 입증합니다.

## 🎯 주요 포인트

- 1. 기존의 분산 확률 최적화 방법은 하위 수준 손실 함수가 강하게 볼록하고 확률적 기울기 노이즈가 유한한 분산을 가진다고 가정하지만, 이는 실제 머신러닝 모델에서는 잘 성립되지 않는다.

- 2. 비볼록 이중 수준 최적화 문제에서 중대한 노이즈를 처리하기 위해 새로운 분산 확률 이중 수준 최적화 알고리즘을 개발하였다.

- 3. 클리핑 작업에 의존하지 않는 정규화된 확률적 분산 감소 이중 수준 기울기 하강 알고리즘을 제안하였다.

- 4. 비볼록 분산 이중 수준 최적화 문제에서 중대한 노이즈 하에서 상호 의존적인 기울기 시퀀스를 혁신적으로 제한하여 수렴 속도를 확립하였다.

- 5. 제안된 알고리즘은 중대한 노이즈를 처리하는 데 있어 효과적임을 실험 결과를 통해 확인하였다.

---

*Generated on 2025-09-22 15:18:55*