
# MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues

**Korean Title:** MOOM: 초장기 롤플레잉 대화에서의 기억 유지, 조직화 및 최적화

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Competition-inhibition memory theory|Competition-inhibition memory theory]] [[keywords/broad/Memory extraction|Memory extraction]] [[keywords/broad/Large language model|Large language model]] [[keywords/specific/Dual-branch memory plugin|Dual-branch memory plugin]] [[keywords/unique/MOOM|MOOM]] [[categories/cs.CL|cs.CL]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Competition-inhibition memory theory
**🔬 Broad Technical**: Memory extraction, Large language model
**🔗 Specific Connectable**: Dual-branch memory plugin
**⭐ Unique Technical**: MOOM

**ArXiv ID**: [2509.11860](https://arxiv.org/abs/2509.11860)
**Published**: 2025-09-18
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.11860.pdf)


## 🏷️ 추출된 키워드



`Memory extraction` • 

`Large language model` • 

`Dual-branch memory plugin` • 

`MOOM` • 

`Competition-inhibition memory theory`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.11860v2 Announce Type: replace 
Abstract: Memory extraction is crucial for maintaining coherent ultra-long dialogues in human-robot role-playing scenarios. However, existing methods often exhibit uncontrolled memory growth. To address this, we propose MOOM, the first dual-branch memory plugin that leverages literary theory by modeling plot development and character portrayal as core storytelling elements. Specifically, one branch summarizes plot conflicts across multiple time scales, while the other extracts the user's character profile. MOOM further integrates a forgetting mechanism, inspired by the ``competition-inhibition'' memory theory, to constrain memory capacity and mitigate uncontrolled growth. Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset specifically designed for role-playing, featuring dialogues that average 600 turns and include manually annotated memory information. Experimental results demonstrate that MOOM outperforms all state-of-the-art memory extraction methods, requiring fewer large language model invocations while maintaining a controllable memory capacity.

## 🔍 Abstract (한글 번역)

arXiv:2509.11860v2 발표 유형: 대체
요약: 기억 추출은 인간-로봇 롤플레잉 시나리오에서 일관된 초장대 대화를 유지하는 데 중요합니다. 그러나 기존 방법들은 종종 제어되지 않은 기억 증가를 보입니다. 이를 해결하기 위해 우리는 MOOM을 제안합니다. 이는 플롯 개발과 캐릭터 묘사를 핵심 이야기 요소로 모델링하는 문학 이론을 활용한 최초의 이중 분기 메모리 플러그인입니다. 구체적으로, 한 분기는 여러 시간 단위에 걸친 플롯 갈등을 요약하고, 다른 하나는 사용자의 캐릭터 프로필을 추출합니다. MOOM은 기억 용량을 제한하고 제어되지 않은 성장을 완화하기 위해 "경쟁-억제" 기억 이론에서 영감을 받은 망각 메커니즘을 통합합니다. 더 나아가, 우리는 롤플레잉을 위해 특별히 설계된 중국어 초장대 대화 데이터셋인 ZH-4O를 제시합니다. 이 데이터셋은 평균 600회의 대화를 포함하며 수동으로 주석이 달린 기억 정보를 포함합니다. 실험 결과는 MOOM이 모든 최첨단 기억 추출 방법을 능가하며, 대형 언어 모델 호출을 더 적게 필요로 하면서도 제어 가능한 기억 용량을 유지한다는 것을 보여줍니다.

## 📝 요약

이 연구는 인간-로봇 롤플레잉 시나리오에서 일관된 초장 대화를 유지하는 데 중요한 기억 추출 문제를 다룬다. 기존 방법은 기억 용량이 제어되지 않는 문제를 보여주었지만, MOOM은 문학 이론을 활용한 이중 분기 메모리 플러그인으로 이를 해결한다. MOOM은 플롯 개발과 캐릭터 묘사를 핵심 스토리텔링 요소로 모델링하여 플롯 충돌을 요약하고 사용자 캐릭터 프로필을 추출한다. 또한 "경쟁-억제" 메모리 이론에서 영감을 받은 망각 메커니즘을 통해 메모리 용량을 제어하고 불규칙한 성장을 완화한다. 실험 결과는 MOOM이 모든 최신 메모리 추출 방법을 능가하며 대화 중 메모리 정보를 수동으로 주석 처리한 중국어 초장 대화 데이터셋 ZH-4O를 사용한다.

## 🎯 주요 포인트


- MOOM은 소설 이론을 활용한 이중 분기 메모리 플러그인으로 기존 방법보다 우수한 성능을 보임

- MOOM은 플롯 충돌 및 사용자 캐릭터 프로필을 요약하고 기억 용량을 제어하는 기능을 제공

- ZH-4O는 역할 연기를 위해 특별히 설계된 중국어 초장대 대화 데이터셋을 소개함


---

*Generated on 2025-09-18 16:57:26*