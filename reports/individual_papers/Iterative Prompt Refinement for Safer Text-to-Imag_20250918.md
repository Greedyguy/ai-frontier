
# Iterative Prompt Refinement for Safer Text-to-Image Generation

**Korean Title:** 보다 안전한 텍스트에서 이미지 생성을 위한 반복적인 프롬프트 세분화

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Visual Feedback|Visual Feedback]] [[keywords/broad/Text-to-Image Generation|Text-to-Image Generation]] [[keywords/broad/Vision Language Models|Vision Language Models]] [[keywords/specific/Iterative Prompt Refinement|Iterative Prompt Refinement]] [[keywords/unique/IPR (Iterative Prompt Refinement|IPR (Iterative Prompt Refinement]] [[categories/cs.CV|cs.CV]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Visual Feedback
**🔬 Broad Technical**: Text-to-Image Generation, Vision Language Models
**🔗 Specific Connectable**: Iterative Prompt Refinement
**⭐ Unique Technical**: IPR (Iterative Prompt Refinement

**ArXiv ID**: [2509.13760](https://arxiv.org/abs/2509.13760)
**Published**: 2025-09-18
**Category**: cs.CV
**PDF**: [Download](https://arxiv.org/pdf/2509.13760.pdf)


## 🏷️ 추출된 키워드



`Text-to-Image Generation` • 

`Vision Language Models` • 

`Iterative Prompt Refinement` • 

`IPR (Iterative Prompt Refinement` • 

`Visual Feedback`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13760v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images from text prompts, but their output quality and safety still depend heavily on how prompts are phrased. Existing safety methods typically refine prompts using large language models (LLMs), but they overlook the images produced, which can result in unsafe outputs or unnecessary changes to already safe prompts. To address this, we propose an iterative prompt refinement algorithm that uses Vision Language Models (VLMs) to analyze both the input prompts and the generated images. By leveraging visual feedback, our method refines prompts more effectively, improving safety while maintaining user intent and reliability comparable to existing LLM-based approaches. Additionally, we introduce a new dataset labeled with both textual and visual safety signals using off-the-shelf multi-modal LLM, enabling supervised fine-tuning. Experimental results demonstrate that our approach produces safer outputs without compromising alignment with user intent, offering a practical solution for generating safer T2I content. Our code is available at https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper contains examples of harmful or inappropriate images generated by models.

## 🔍 Abstract (한글 번역)

arXiv:2509.13760v1 발표 유형: 새로운
요약: 텍스트에서 이미지로 (T2I) 모델은 텍스트 프롬프트로부터 이미지를 생성하는 데 놀라운 진전을 이루었지만, 그들의 출력 품질과 안전성은 여전히 프롬프트가 어떻게 구성되었는지에 크게 의존합니다. 기존의 안전 방법은 일반적으로 대형 언어 모델 (LLM)을 사용하여 프롬프트를 개선하지만, 생성된 이미지를 간과하여 안전하지 않은 출력물이나 이미 안전한 프롬프트에 불필요한 변경이 발생할 수 있습니다. 이를 해결하기 위해, 우리는 Vision Language Models (VLMs)를 사용하여 입력 프롬프트와 생성된 이미지를 분석하는 반복적인 프롬프트 개선 알고리즘을 제안합니다. 시각적 피드백을 활용함으로써, 우리의 방법은 프롬프트를 더 효과적으로 개선하여 안전성을 향상시키면서 기존의 LLM 기반 접근 방식과 비교 가능한 사용자 의도와 신뢰성을 유지합니다. 또한, 우리는 오프-더-셀프 멀티모달 LLM을 사용하여 텍스트 및 시각적 안전 신호로 레이블이 지정된 새로운 데이터셋을 소개하여 지도된 미세 조정을 가능하게 합니다. 실험 결과는 우리의 접근 방식이 사용자 의도와 일치를 저해하지 않으면서 더 안전한 출력물을 생성한다는 것을 보여주며, 보다 안전한 T2I 콘텐츠를 생성하기 위한 실용적인 솔루션을 제공합니다. 우리의 코드는 https://github.com/ku-dmlab/IPR에서 사용할 수 있습니다. \textbf{\textcolor{red}경고: 본 논문에는 모델에 의해 생성된 해로운 또는 부적절한 이미지의 예시가 포함되어 있습니다.

## 📝 요약

이 연구는 텍스트를 이미지로 변환하는 모델의 안전성을 향상시키기 위한 새로운 방법을 제안한다. 기존 안전 방법은 대부분 큰 언어 모델을 사용하여 안전성을 개선하지만, 이미지 생성물을 간과한다. 이에 우리는 시각 언어 모델을 활용하여 입력 텍스트와 생성된 이미지를 분석하는 반복적인 프롬프트 개선 알고리즘을 제안한다. 실험 결과는 사용자 의도와 일치하면서도 안전한 출력물을 생성하는 우리의 방법이 기존 방법과 비교해 안전성을 향상시키는 것을 보여준다. 이는 안전한 T2I 콘텐츠 생성을 위한 실용적인 해결책을 제공한다.

## 🎯 주요 포인트


- 1. Text-to-Image 모델의 안전성 문제를 해결하기 위해 시각 언어 모델을 활용한 반복적 프롬프트 정제 알고리즘 제안

- 2. 이미지 생성에 영향을 미치는 프롬프트를 개선하여 안전성 향상 및 사용자 의도 유지

- 3. 텍스트 및 시각적 안전 신호를 활용한 새로운 데이터셋 도입으로 안전한 T2I 콘텐츠 생성 가능성 입증.


---

*Generated on 2025-09-18 17:00:23*