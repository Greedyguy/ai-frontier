# Can Large Language Models Infer Causal Relationships from Real-World Text?

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Benchmark Development

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Causal-Counterfactual RAG_ The Integration of Causal-Counterfactual Reasoning into RAG_20250919|Causal-Counterfactual RAG The Integration of Causal-Counterfactual Reasoning into RAG]] (86.3% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (85.5% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (85.5% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (85.3% similar)
- [[2025-09-19/CLEAR_ A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models_20250919|CLEAR A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models]] (84.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.18931v2 Announce Type: replace 
Abstract: Understanding and inferring causal relationships from texts is a core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work evaluating LLM causal reasoning primarily focuses on synthetically generated texts which involve straightforward causal relationships that are explicitly mentioned in the text. This fails to reflect the complexities of real-world tasks. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop a benchmark drawn from real-world academic literature which includes diverse texts with respect to length, complexity of relationships (different levels of explicitness, number of nodes, and causal relationships), and domains and sub-domains. To the best of our knowledge, our benchmark is the first-ever real-world dataset for this task. Our experiments on this dataset show that LLMs face significant challenges in inferring causal relationships from real-world text, with the best-performing model achieving an average F1 score of only 0.477. Through systematic analysis across aspects of real-world text (degree of confounding, size of graph, length of text, domain), our benchmark offers targeted insights for further research into advancing LLM causal reasoning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.18931v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì´í•´í•˜ê³  ì¶”ë¡ í•˜ëŠ” ê²ƒì€ ì¸ê°„ ì¸ì§€ì˜ í•µì‹¬ ì¸¡ë©´ì´ë©°, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ ì¸ê³µì§€ëŠ¥ ì¼ë°˜ ì§€ëŠ¥ìœ¼ë¡œ ë°œì „ì‹œí‚¤ê¸° ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ LLM ì¸ê³¼ ì¶”ë¡  í‰ê°€ ì‘ì—…ì€ ì£¼ë¡œ í…ìŠ¤íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê°„ë‹¨í•œ ì¸ê³¼ ê´€ê³„ë¥¼ í¬í•¨í•˜ëŠ” í•©ì„±ì ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ê³¼ì œì˜ ë³µì¡ì„±ì„ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” LLMì´ ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸(ê¸¸ì´, ê´€ê³„ì˜ ë³µì¡ì„±, ëª…ì‹œì„±ì˜ ìˆ˜ì¤€, ë…¸ë“œì˜ ìˆ˜, ì¸ê³¼ ê´€ê³„) ë° ë„ë©”ì¸ê³¼ í•˜ìœ„ ë„ë©”ì¸ì— ëŒ€í•œ ì‹¤ì œ í•™ìˆ  ë¬¸í—Œì—ì„œ ì¶”ì¶œí•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì´ ì‘ì—…ì„ ìœ„í•œ ìµœì´ˆì˜ ì‹¤ì œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, LLMì€ ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë° ìƒë‹¹í•œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì¡°ì°¨ í‰ê·  F1 ì ìˆ˜ 0.477ì— ê·¸ì³¤ìŠµë‹ˆë‹¤. ì‹¤ì œ í…ìŠ¤íŠ¸ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´(í˜¼ë€ì˜ ì •ë„, ê·¸ë˜í”„ì˜ í¬ê¸°, í…ìŠ¤íŠ¸ì˜ ê¸¸ì´, ë„ë©”ì¸)ì— ëŒ€í•œ ì²´ê³„ì ì¸ ë¶„ì„ì„ í†µí•´, ìš°ë¦¬ì˜ ë²¤ì¹˜ë§ˆí¬ëŠ” LLM ì¸ê³¼ ì¶”ë¡ ì„ ë°œì „ì‹œí‚¤ê¸° ìœ„í•œ ì¶”ê°€ ì—°êµ¬ì— ëŒ€í•œ ëª©í‘œ ì§€í–¥ì ì¸ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ëª…ì‹œì ì¸ ì¸ê³¼ ê´€ê³„ê°€ í¬í•¨ëœ í•©ì„± í…ìŠ¤íŠ¸ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆì–´ í˜„ì‹¤ ì„¸ê³„ì˜ ë³µì¡ì„±ì„ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ ê¸¸ì´ì™€ ê´€ê³„ ë³µì¡ì„±, ë„ë©”ì¸ì„ í¬í•¨í•œ ì‹¤ì œ í•™ìˆ  ë¬¸í—Œì—ì„œ ì¶”ì¶œí•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, LLMì´ ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ í‰ê·  F1 ì ìˆ˜ëŠ” 0.477ì— ë¶ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ì¸ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì¶”ê°€ ì—°êµ¬ì— ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¸ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì‹¤ì œ í•™ìˆ  ë¬¸í—Œì—ì„œ ì¶”ì¶œí•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

- 2. ì´ ì—°êµ¬ëŠ” LLMì´ ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë° ìƒë‹¹í•œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- 3. ê°œë°œëœ ë²¤ì¹˜ë§ˆí¬ëŠ” ë‹¤ì–‘í•œ ê¸¸ì´ì™€ ë³µì¡ì„±ì„ ê°€ì§„ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ë©°, ì´ëŠ” ì¸ê³¼ ê´€ê³„ ì¶”ë¡  ì—°êµ¬ì— ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ë„ í‰ê·  F1 ì ìˆ˜ê°€ 0.477ì— ë¶ˆê³¼í•˜ì—¬ LLMì˜ ì¸ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì— í•œê³„ê°€ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

- 5. ì´ ì—°êµ¬ëŠ” LLM ì¸ê³¼ ì¶”ë¡ ì„ ë°œì „ì‹œí‚¤ê¸° ìœ„í•œ ì¶”ê°€ ì—°êµ¬ì˜ ë°©í–¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:31:57*