---
keywords:
  - Hyperparameter Optimization
  - Group Relative Policy Optimization
  - Policy Churn Regularization
  - Reinforcement Learning
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17105
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:47:05.400481",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hyperparameter Optimization",
    "Group Relative Policy Optimization",
    "Policy Churn Regularization",
    "Reinforcement Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hyperparameter Optimization": 0.72,
    "Group Relative Policy Optimization": 0.81,
    "Policy Churn Regularization": 0.77,
    "Reinforcement Learning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Hyperparameter Optimization",
        "canonical": "Hyperparameter Optimization",
        "aliases": [
          "HPO"
        ],
        "category": "broad_technical",
        "rationale": "Hyperparameter Optimization is a fundamental concept in machine learning, facilitating connections to various optimization techniques.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "GRPO is a novel approach in the context of reinforcement learning, offering unique insights into optimization strategies.",
        "novelty_score": 0.78,
        "connectivity_score": 0.67,
        "specificity_score": 0.82,
        "link_intent_score": 0.81
      },
      {
        "surface": "Policy Churn Regularization",
        "canonical": "Policy Churn Regularization",
        "aliases": [
          "PCR"
        ],
        "category": "unique_technical",
        "rationale": "Policy Churn Regularization is a specific technique introduced to enhance training stability, relevant for advanced RL discussions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.64,
        "specificity_score": 0.79,
        "link_intent_score": 0.77
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a key area in AI, crucial for linking to various learning and optimization frameworks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "model performance",
      "experimental results",
      "baseline methods"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Hyperparameter Optimization",
      "resolved_canonical": "Hyperparameter Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.67,
        "specificity": 0.82,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Policy Churn Regularization",
      "resolved_canonical": "Policy Churn Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.64,
        "specificity": 0.79,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17105.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17105](https://arxiv.org/abs/2509.17105)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (85.6% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (83.6% similar)
- [[2025-09-23/Advancing Speech Understanding in Speech-Aware Language Models with GRPO_20250923|Advancing Speech Understanding in Speech-Aware Language Models with GRPO]] (82.8% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (82.7% similar)
- [[2025-09-23/GEPO_ Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning_20250923|GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Hyperparameter Optimization|Hyperparameter Optimization]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]], [[keywords/Policy Churn Regularization|Policy Churn Regularization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17105v1 Announce Type: new 
Abstract: Hyperparameter optimization (HPO) plays a critical role in improving model performance. Transformer-based HPO methods have shown great potential; however, existing approaches rely heavily on large-scale historical optimization trajectories and lack effective reinforcement learning (RL) techniques, thereby limiting their efficiency and performance improvements. Inspired by the success of Group Relative Policy Optimization (GRPO) in large language models (LLMs), we propose GRPOformer -- a novel hyperparameter optimization framework that integrates reinforcement learning (RL) with Transformers. In GRPOformer, Transformers are employed to generate new hyperparameter configurations from historical optimization trajectories, while GRPO enables rapid trajectory construction and optimization strategy learning from scratch. Moreover, we introduce Policy Churn Regularization (PCR) to enhance the stability of GRPO training. Experimental results on OpenML demonstrate that GRPOformer consistently outperforms baseline methods across diverse tasks, offering new insights into the application of RL for HPO.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”(HPO)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì œì•ˆëœ GRPOformerë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ Transformer ê¸°ë°˜ HPO ë°©ë²•ë“¤ì€ ëŒ€ê·œëª¨ì˜ ê³¼ê±° ìµœì í™” ê²½ë¡œì— ì˜ì¡´í•˜ê³  ê°•í™” í•™ìŠµ(RL) ê¸°ìˆ ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. GRPOformerëŠ” Transformerì™€ RLì„ ê²°í•©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. TransformerëŠ” ê³¼ê±° ê²½ë¡œì—ì„œ ìƒˆë¡œìš´ í•˜ì´í¼íŒŒë¼ë¯¸í„° êµ¬ì„±ì„ ìƒì„±í•˜ê³ , GRPOëŠ” ë¹ ë¥¸ ê²½ë¡œ êµ¬ì¶•ê³¼ ìµœì í™” ì „ëµ í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤. ë˜í•œ, ì •ì±… ë³€ë™ ê·œì œ(PCR)ë¥¼ ë„ì…í•˜ì—¬ GRPO í›ˆë ¨ì˜ ì•ˆì •ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. OpenMLì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, GRPOformerëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”(HPO)ëŠ” ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- 2. ê¸°ì¡´ì˜ Transformer ê¸°ë°˜ HPO ë°©ë²•ì€ ëŒ€ê·œëª¨ì˜ ê³¼ê±° ìµœì í™” ê²½ë¡œì— ì˜ì¡´í•˜ë©°, ê°•í™” í•™ìŠµ(RL) ê¸°ë²•ì´ ë¶€ì¡±í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê°œì„ ì— í•œê³„ê°€ ìˆë‹¤.
- 3. GRPOformerëŠ” ê°•í™” í•™ìŠµê³¼ Transformersë¥¼ í†µí•©í•˜ì—¬ ìƒˆë¡œìš´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. GRPOformerëŠ” OpenML ì‹¤í—˜ ê²°ê³¼ì—ì„œ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
- 5. Policy Churn Regularization(PCR)ì„ ë„ì…í•˜ì—¬ GRPO í›ˆë ¨ì˜ ì•ˆì •ì„±ì„ í–¥ìƒì‹œì¼°ë‹¤.


---

*Generated on 2025-09-24 01:47:05*