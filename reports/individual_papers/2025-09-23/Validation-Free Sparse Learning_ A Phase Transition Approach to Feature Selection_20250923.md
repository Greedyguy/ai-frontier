---
keywords:
  - Sparse Models
  - Phase Transition in Feature Selection
  - Neural Network
  - Sparsity-Promoting Penalties
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2411.17180
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:57:40.613996",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Sparse Models",
    "Phase Transition in Feature Selection",
    "Neural Network",
    "Sparsity-Promoting Penalties"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Sparse Models": 0.79,
    "Phase Transition in Feature Selection": 0.72,
    "Neural Network": 0.77,
    "Sparsity-Promoting Penalties": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Sparse Models",
        "canonical": "Sparse Models",
        "aliases": [
          "Sparse Learning",
          "Sparse Representation"
        ],
        "category": "specific_connectable",
        "rationale": "Sparse models are crucial for reducing complexity and enhancing interpretability in AI, making them a strong link to feature selection and efficiency.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Phase Transition",
        "canonical": "Phase Transition in Feature Selection",
        "aliases": [
          "Feature Selection Phase Transition"
        ],
        "category": "unique_technical",
        "rationale": "The concept of phase transition in feature selection is a novel approach that enhances understanding of sparsity in models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Artificial Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "ANN",
          "Deep Neural Networks"
        ],
        "category": "broad_technical",
        "rationale": "Neural networks are foundational to the study and application of sparse learning and feature selection.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "Sparsity-Promoting Penalties",
        "canonical": "Sparsity-Promoting Penalties",
        "aliases": [
          "Sparsity Penalties",
          "Regularization Techniques"
        ],
        "category": "specific_connectable",
        "rationale": "These penalties are essential for implementing sparsity in models, directly linking to the paper's focus on feature selection.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Validation-Free",
      "Real-World Data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Sparse Models",
      "resolved_canonical": "Sparse Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Phase Transition",
      "resolved_canonical": "Phase Transition in Feature Selection",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Artificial Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Sparsity-Promoting Penalties",
      "resolved_canonical": "Sparsity-Promoting Penalties",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Validation-Free Sparse Learning: A Phase Transition Approach to Feature Selection

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2411.17180.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2411.17180](https://arxiv.org/abs/2411.17180)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning_20250918|Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning]] (82.0% similar)
- [[2025-09-22/Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria_20250922|Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria]] (81.9% similar)
- [[2025-09-23/Model Guidance via Robust Feature Attribution_20250923|Model Guidance via Robust Feature Attribution]] (80.0% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (79.3% similar)
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Sparse Models|Sparse Models]], [[keywords/Sparsity-Promoting Penalties|Sparsity-Promoting Penalties]]
**âš¡ Unique Technical**: [[keywords/Phase Transition in Feature Selection|Phase Transition in Feature Selection]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2411.17180v4 Announce Type: replace-cross 
Abstract: The growing environmental footprint of artificial intelligence (AI), especially in terms of storage and computation, calls for more frugal and interpretable models. Sparse models (e.g., linear, neural networks) offer a promising solution by selecting only the most relevant features, reducing complexity, preventing over-fitting and enabling interpretation-marking a step towards truly intelligent AI.
  The concept of a right amount of sparsity (without too many false positive or too few true positive) is subjective. So we propose a new paradigm previously only observed and mathematically studied for compressed sensing (noiseless linear models): obtaining a phase transition in the probability of retrieving the relevant features. We show in practice how to obtain this phase transition for a class of sparse learners. Our approach is flexible and applicable to complex models ranging from linear to shallow and deep artificial neural networks while supporting various loss functions and sparsity-promoting penalties. It does not rely on cross-validation or on a validation set to select its single regularization parameter. For real-world data, it provides a good balance between predictive accuracy and feature sparsity.
  A Python package is available at https://github.com/VcMaxouuu/HarderLASSO containing all the simulations and ready-to-use models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ í™˜ê²½ì  ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ í¬ì†Œ ëª¨ë¸ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. í¬ì†Œ ëª¨ë¸ì€ ê´€ë ¨ ìˆëŠ” íŠ¹ì§•ë§Œ ì„ íƒí•˜ì—¬ ë³µì¡ì„±ì„ ì¤„ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ë©° í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì €ìë“¤ì€ ì••ì¶• ì„¼ì‹±ì—ì„œ ê´€ì°°ëœ ê°œë…ì„ ì°¨ìš©í•˜ì—¬, ê´€ë ¨ íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” í™•ë¥ ì  ì „ì´ í˜„ìƒì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì„ í˜• ëª¨ë¸ë¶€í„° ì‹¬ì¸µ ì‹ ê²½ë§ê¹Œì§€ ë‹¤ì–‘í•œ ë³µì¡í•œ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•˜ë©°, êµì°¨ ê²€ì¦ ì—†ì´ë„ ì ì ˆí•œ ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ì— ì ìš© ì‹œ ì˜ˆì¸¡ ì •í™•ë„ì™€ íŠ¹ì§• í¬ì†Œì„± ê°„ì˜ ê· í˜•ì„ ì˜ ìœ ì§€í•©ë‹ˆë‹¤. ê´€ë ¨ Python íŒ¨í‚¤ì§€ë„ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê³µì§€ëŠ¥ì˜ í™˜ê²½ì  ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ í•´ì„ ê°€ëŠ¥í•œ í¬ì†Œ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
- 2. í¬ì†Œ ëª¨ë¸ì€ ë³µì¡ì„±ì„ ì¤„ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ë©° í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 3. ì ì ˆí•œ í¬ì†Œì„±ì˜ ê°œë…ì€ ì£¼ê´€ì ì´ë©°, ìš°ë¦¬ëŠ” ê´€ë ¨ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” í™•ë¥ ì˜ ìƒì „ì´ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ì ‘ê·¼ë²•ì€ ë‹¤ì–‘í•œ ë³µì¡í•œ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•˜ë©°, êµì°¨ ê²€ì¦ì— ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 5. Python íŒ¨í‚¤ì§€ë¥¼ í†µí•´ ì œì•ˆëœ ëª¨ë¸ê³¼ ì‹œë®¬ë ˆì´ì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:57:40*