---
keywords:
  - Vision-Language Model
  - Test-time Adaptation
  - Zero-Shot Learning
  - Class-Balanced Pseudo-labeling
  - Context-aware Module
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17598
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:55:59.697395",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Test-time Adaptation",
    "Zero-Shot Learning",
    "Class-Balanced Pseudo-labeling",
    "Context-aware Module"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "Test-time Adaptation": 0.78,
    "Zero-Shot Learning": 0.85,
    "Class-Balanced Pseudo-labeling": 0.7,
    "Context-aware Module": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "CLIP"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for linking multimodal learning approaches and their adaptation capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "Test-time Adaptation",
        "canonical": "Test-time Adaptation",
        "aliases": [
          "TTA"
        ],
        "category": "unique_technical",
        "rationale": "Test-time Adaptation is a novel approach that addresses distribution shift, enhancing model adaptability.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "ZSL"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a key concept in adapting models to new tasks without labeled data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Class-Balanced Pseudo-labeling",
        "canonical": "Class-Balanced Pseudo-labeling",
        "aliases": [
          "CBPL"
        ],
        "category": "unique_technical",
        "rationale": "This technique addresses class imbalance, a common issue in machine learning, enhancing model robustness.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Context-aware Module",
        "canonical": "Context-aware Module",
        "aliases": [
          "Context Module"
        ],
        "category": "unique_technical",
        "rationale": "The context-aware module is integral for integrating domain-specific knowledge into models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.67,
        "specificity_score": 0.77,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "distribution shift",
      "source domain",
      "target domain"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Test-time Adaptation",
      "resolved_canonical": "Test-time Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Class-Balanced Pseudo-labeling",
      "resolved_canonical": "Class-Balanced Pseudo-labeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Context-aware Module",
      "resolved_canonical": "Context-aware Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.67,
        "specificity": 0.77,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# COLA: Context-aware Language-driven Test-time Adaptation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17598.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17598](https://arxiv.org/abs/2509.17598)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/CLIPTTA_ Robust Contrastive Vision-Language Test-Time Adaptation_20250922|CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation]] (87.6% similar)
- [[2025-09-22/CoDoL_ Conditional Domain Prompt Learning for Out-of-Distribution Generalization_20250922|CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization]] (84.0% similar)
- [[2025-09-23/Training-Free Label Space Alignment for Universal Domain Adaptation_20250923|Training-Free Label Space Alignment for Universal Domain Adaptation]] (83.8% similar)
- [[2025-09-23/Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification_20250923|Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification]] (83.5% similar)
- [[2025-09-23/TACO_ Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration_20250923|TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Test-time Adaptation|Test-time Adaptation]], [[keywords/Class-Balanced Pseudo-labeling|Class-Balanced Pseudo-labeling]], [[keywords/Context-aware Module|Context-aware Module]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17598v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing ``distribution shift'' issue while simultaneously protecting data privacy.
  However, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability.
  In this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno, CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA (COLA).
  The proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively.
  It is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë°ì´í„° í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•˜ë©´ì„œ ë¶„í¬ ë³€í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‹œì  ì ì‘(TTA)ì˜ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë™ì¼í•œ ë ˆì´ë¸” ê³µê°„ì„ ê³µìœ í•˜ëŠ” ì†ŒìŠ¤ ë° íƒ€ê²Ÿ ë„ë©”ì¸ì„ ê°€ì •í•˜ì§€ë§Œ, ë³¸ ì—°êµ¬ëŠ” ë ˆì´ë¸” ê³µìœ  ì—†ì´ ì—¬ëŸ¬ íƒ€ê²Ÿ ë„ë©”ì¸ì— ì ì‘ ê°€ëŠ¥í•œ ì†ŒìŠ¤ ëª¨ë¸ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì‚¬ì „ í›ˆë ¨ëœ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM), ì˜ˆë¥¼ ë“¤ì–´ CLIPì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í´ë˜ìŠ¤ ì„¤ëª…ì„ ë§¤ì¹­í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ VLMì˜ ì œë¡œìƒ· ì„±ëŠ¥ì€ ë›°ì–´ë‚˜ì§€ë§Œ íƒ€ê²Ÿ ë„ë©”ì¸ì˜ ë…íŠ¹í•œ ì†ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ì—°êµ¬ëŠ” Context-aware Language-driven TTA(COLA)ë¥¼ ì œì•ˆí•˜ë©°, ì´ëŠ” ê²½ëŸ‰ì˜ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëª¨ë“ˆì„ í¬í•¨í•˜ì—¬ VLMì˜ ì§€ì‹ê³¼ ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì„ íƒìƒ‰í•©ë‹ˆë‹¤. ë˜í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ ê· í˜• ì˜ì‚¬ ë ˆì´ë¸”ë§(CBPL) ì „ëµì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ TTAì™€ í´ë˜ìŠ¤ ì¼ë°˜í™” ì‘ì—…ì—ì„œ íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ë¼ë²¨ ê³µê°„ì„ ê³µìœ í•˜ì§€ ì•ŠëŠ” ì—¬ëŸ¬ íƒ€ê²Ÿ ë„ë©”ì¸ì— ì ì‘ ê°€ëŠ¥í•œ ì†ŒìŠ¤ ëª¨ë¸ì„ ì—°êµ¬í•©ë‹ˆë‹¤.
- 2. ì‚¬ì „ í•™ìŠµëœ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì¸ CLIPì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ì¸ì‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì¸ COLAëŠ” ê²½ëŸ‰ì˜ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëª¨ë“ˆì„ í†µí•´ ê³¼ì œë³„, ë„ë©”ì¸ë³„, ê·¸ë¦¬ê³  ì‚¬ì „ ì§€ì‹ì„ íƒìƒ‰í•©ë‹ˆë‹¤.
- 4. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ í´ë˜ìŠ¤ ê· í˜• ê°€ì§œ ë¼ë²¨ë§(CBPL) ì „ëµì„ ë„ì…í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ TTA ì‹œë‚˜ë¦¬ì˜¤ë¿ë§Œ ì•„ë‹ˆë¼ í´ë˜ìŠ¤ ì¼ë°˜í™” ì‘ì—…ì—ì„œë„ íš¨ê³¼ì ì„ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:55:59*