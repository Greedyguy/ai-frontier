---
keywords:
  - Vision-Language Model
  - Test-time Adaptation
  - Zero-Shot Learning
  - Class-Balanced Pseudo-labeling
  - Context-aware Module
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17598
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:55:59.697395",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Test-time Adaptation",
    "Zero-Shot Learning",
    "Class-Balanced Pseudo-labeling",
    "Context-aware Module"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "Test-time Adaptation": 0.78,
    "Zero-Shot Learning": 0.85,
    "Class-Balanced Pseudo-labeling": 0.7,
    "Context-aware Module": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "CLIP"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for linking multimodal learning approaches and their adaptation capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "Test-time Adaptation",
        "canonical": "Test-time Adaptation",
        "aliases": [
          "TTA"
        ],
        "category": "unique_technical",
        "rationale": "Test-time Adaptation is a novel approach that addresses distribution shift, enhancing model adaptability.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "ZSL"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a key concept in adapting models to new tasks without labeled data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Class-Balanced Pseudo-labeling",
        "canonical": "Class-Balanced Pseudo-labeling",
        "aliases": [
          "CBPL"
        ],
        "category": "unique_technical",
        "rationale": "This technique addresses class imbalance, a common issue in machine learning, enhancing model robustness.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Context-aware Module",
        "canonical": "Context-aware Module",
        "aliases": [
          "Context Module"
        ],
        "category": "unique_technical",
        "rationale": "The context-aware module is integral for integrating domain-specific knowledge into models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.67,
        "specificity_score": 0.77,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "distribution shift",
      "source domain",
      "target domain"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Test-time Adaptation",
      "resolved_canonical": "Test-time Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Class-Balanced Pseudo-labeling",
      "resolved_canonical": "Class-Balanced Pseudo-labeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Context-aware Module",
      "resolved_canonical": "Context-aware Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.67,
        "specificity": 0.77,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# COLA: Context-aware Language-driven Test-time Adaptation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17598.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17598](https://arxiv.org/abs/2509.17598)

## 🔗 유사한 논문
- [[2025-09-22/CLIPTTA_ Robust Contrastive Vision-Language Test-Time Adaptation_20250922|CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation]] (87.6% similar)
- [[2025-09-22/CoDoL_ Conditional Domain Prompt Learning for Out-of-Distribution Generalization_20250922|CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization]] (84.0% similar)
- [[2025-09-23/Training-Free Label Space Alignment for Universal Domain Adaptation_20250923|Training-Free Label Space Alignment for Universal Domain Adaptation]] (83.8% similar)
- [[2025-09-23/Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification_20250923|Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification]] (83.5% similar)
- [[2025-09-23/TACO_ Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration_20250923|TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Test-time Adaptation|Test-time Adaptation]], [[keywords/Class-Balanced Pseudo-labeling|Class-Balanced Pseudo-labeling]], [[keywords/Context-aware Module|Context-aware Module]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17598v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing ``distribution shift'' issue while simultaneously protecting data privacy.
  However, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability.
  In this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno, CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA (COLA).
  The proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively.
  It is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.

## 📝 요약

이 논문은 데이터 프라이버시를 보호하면서 분포 변화 문제를 해결하는 테스트 시점 적응(TTA)의 새로운 접근법을 제안합니다. 기존 방법들은 동일한 레이블 공간을 공유하는 소스 및 타겟 도메인을 가정하지만, 본 연구는 레이블 공유 없이 여러 타겟 도메인에 적응 가능한 소스 모델을 탐구합니다. 이를 위해 사전 훈련된 비전-언어 모델(VLM), 예를 들어 CLIP을 사용하여 이미지와 클래스 설명을 매칭합니다. 그러나 VLM의 제로샷 성능은 뛰어나지만 타겟 도메인의 독특한 속성을 효과적으로 포착하지 못합니다. 이를 해결하기 위해 본 연구는 Context-aware Language-driven TTA(COLA)를 제안하며, 이는 경량의 컨텍스트 인식 모듈을 포함하여 VLM의 지식과 도메인 특화 지식을 탐색합니다. 또한 클래스 불균형 문제를 완화하기 위한 클래스 균형 의사 레이블링(CBPL) 전략을 도입했습니다. 제안된 방법은 TTA와 클래스 일반화 작업에서 효과적임을 입증했습니다.

## 🎯 주요 포인트

- 1. 본 논문은 라벨 공간을 공유하지 않는 여러 타겟 도메인에 적응 가능한 소스 모델을 연구합니다.
- 2. 사전 학습된 비전-언어 모델(VLM)인 CLIP을 활용하여 이미지 인식을 수행합니다.
- 3. 제안된 방법인 COLA는 경량의 컨텍스트 인식 모듈을 통해 과제별, 도메인별, 그리고 사전 지식을 탐색합니다.
- 4. 클래스 불균형 문제를 완화하기 위해 클래스 균형 가짜 라벨링(CBPL) 전략을 도입합니다.
- 5. 제안된 방법은 TTA 시나리오뿐만 아니라 클래스 일반화 작업에서도 효과적임을 입증합니다.


---

*Generated on 2025-09-24 04:55:59*