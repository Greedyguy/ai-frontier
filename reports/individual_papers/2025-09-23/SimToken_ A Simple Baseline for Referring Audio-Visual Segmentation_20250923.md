---
keywords:
  - Referring Audio-Visual Segmentation
  - Multimodal Learning
  - Segment Anything Model
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17537
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:55:01.732909",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Referring Audio-Visual Segmentation",
    "Multimodal Learning",
    "Segment Anything Model",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Referring Audio-Visual Segmentation": 0.8,
    "Multimodal Learning": 0.85,
    "Segment Anything Model": 0.78,
    "Vision-Language Model": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Referring Audio-Visual Segmentation",
        "canonical": "Referring Audio-Visual Segmentation",
        "aliases": [
          "Ref-AVS"
        ],
        "category": "unique_technical",
        "rationale": "This is the core task discussed in the paper, offering a unique intersection of audio, visual, and text modalities.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Large Language Model",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLM"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the trending concept of integrating multiple modalities in learning models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Segment Anything Model",
        "canonical": "Segment Anything Model",
        "aliases": [
          "SAM"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific model used in the paper, crucial for understanding the segmentation process.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Highlights the integration of visual and language data, a key aspect of the paper's methodology.",
        "novelty_score": 0.65,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "cross-modal reasoning",
      "fine-grained object localization"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Referring Audio-Visual Segmentation",
      "resolved_canonical": "Referring Audio-Visual Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Large Language Model",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Segment Anything Model",
      "resolved_canonical": "Segment Anything Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# SimToken: A Simple Baseline for Referring Audio-Visual Segmentation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17537.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17537](https://arxiv.org/abs/2509.17537)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (84.1% similar)
- [[2025-09-23/The 1st Solution for 7th LSVOS RVOS Track_ SaSaSa2VA_20250923|The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA]] (83.0% similar)
- [[2025-09-23/BiPrompt-SAM_ Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts_20250923|BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts]] (82.9% similar)
- [[2025-09-23/SAMSON_ 3rd Place Solution of LSVOS 2025 VOS Challenge_20250923|SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge]] (82.3% similar)
- [[2025-09-23/SAM-DCE_ Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation_20250923|SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Referring Audio-Visual Segmentation|Referring Audio-Visual Segmentation]], [[keywords/Segment Anything Model|Segment Anything Model]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17537v1 Announce Type: new 
Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific objects in videos based on natural language expressions involving audio, vision, and text information. This task poses significant challenges in cross-modal reasoning and fine-grained object localization. In this paper, we propose a simple framework, SimToken, that integrates a multimodal large language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided to generate a special semantic token representing the referred object. This compact token, enriched with contextual information from all modalities, acts as a prompt to guide SAM to segment objectsacross video frames. To further improve semantic learning, we introduce a novel target-consistent semantic alignment loss that aligns token embeddings from different expressions but referring to the same object. Experiments on the Ref-AVS benchmark demonstrate that our approach achieves superior performance compared to existing methods.Code will be available at https://github.com/DianJin-HFUT/SimToken

## ğŸ“ ìš”ì•½

Ref-AVSëŠ” ìì—°ì–´ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • ê°ì²´ë¥¼ ë¶„í• í•˜ëŠ” ì‘ì—…ìœ¼ë¡œ, êµì°¨ ëª¨ë‹¬ ì¶”ë¡ ê³¼ ì„¸ë°€í•œ ê°ì²´ ìœ„ì¹˜ ì§€ì •ì— ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” SimTokenì´ë¼ëŠ” ê°„ë‹¨í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ê³¼ Segment Anything Model(SAM)ì„ í†µí•©í•˜ì—¬, MLLMì´ ì°¸ì¡° ê°ì²´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ë³„í•œ ì˜ë¯¸ í† í°ì„ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ í† í°ì€ ëª¨ë“  ëª¨ë‹¬ì˜ ë§¥ë½ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ SAMì´ ë¹„ë””ì˜¤ í”„ë ˆì„ ì „ë°˜ì— ê±¸ì³ ê°ì²´ë¥¼ ë¶„í• í•˜ë„ë¡ ì•ˆë‚´í•©ë‹ˆë‹¤. ë˜í•œ, ë™ì¼ ê°ì²´ë¥¼ ì°¸ì¡°í•˜ëŠ” í‘œí˜„ì˜ í† í° ì„ë² ë”©ì„ ì •ë ¬í•˜ëŠ” ìƒˆë¡œìš´ ëª©í‘œ ì¼ê´€ì„± ì˜ë¯¸ ì •ë ¬ ì†ì‹¤ì„ ë„ì…í•˜ì—¬ ì˜ë¯¸ í•™ìŠµì„ ê°œì„ í•©ë‹ˆë‹¤. Ref-AVS ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Ref-AVSëŠ” ì˜¤ë””ì˜¤, ë¹„ì „, í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•œ ìì—°ì–´ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • ê°ì²´ë¥¼ ì„¸ë¶„í™”í•˜ëŠ” ì‘ì—…ì´ë‹¤.
- 2. SimTokenì€ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ê³¼ Segment Anything Model(SAM)ì„ í†µí•©í•œ ê°„ë‹¨í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 3. MLLMì€ ì°¸ì¡° ê°ì²´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜í•œ ì˜ë¯¸ í† í°ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„ë˜ë©°, ì´ í† í°ì€ SAMì´ ë¹„ë””ì˜¤ í”„ë ˆì„ ì „ë°˜ì— ê±¸ì³ ê°ì²´ë¥¼ ì„¸ë¶„í™”í•˜ë„ë¡ ì•ˆë‚´í•œë‹¤.
- 4. ìƒˆë¡œìš´ ëŒ€ìƒ ì¼ê´€ì„± ì˜ë¯¸ ì •ë ¬ ì†ì‹¤ì„ ë„ì…í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ì—ì„œ ë™ì¼í•œ ê°ì²´ë¥¼ ì°¸ì¡°í•˜ëŠ” í† í° ì„ë² ë”©ì„ ì •ë ¬í•˜ì—¬ ì˜ë¯¸ í•™ìŠµì„ ê°œì„ í•œë‹¤.
- 5. Ref-AVS ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì´ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.


---

*Generated on 2025-09-24 04:55:01*