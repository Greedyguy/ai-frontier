---
keywords:
  - Neural Processing Unit
  - Embedded Microcontroller
  - Energy Efficiency
  - ARM Cortex-M55
  - Real-time Edge Device
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17533
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:00:45.382197",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Processing Unit",
    "Embedded Microcontroller",
    "Energy Efficiency",
    "ARM Cortex-M55",
    "Real-time Edge Device"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Processing Unit": 0.82,
    "Embedded Microcontroller": 0.78,
    "Energy Efficiency": 0.7,
    "ARM Cortex-M55": 0.75,
    "Real-time Edge Device": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Processing Units",
        "canonical": "Neural Processing Unit",
        "aliases": [
          "NPU",
          "NPUs"
        ],
        "category": "unique_technical",
        "rationale": "NPUs are critical for enhancing energy efficiency in embedded ML, offering a unique technical focus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Embedded Microcontrollers",
        "canonical": "Embedded Microcontroller",
        "aliases": [
          "MCU",
          "MCUs"
        ],
        "category": "unique_technical",
        "rationale": "Embedded microcontrollers are central to the study's context, providing a specific technical domain.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Energy Efficiency",
        "canonical": "Energy Efficiency",
        "aliases": [
          "Power Efficiency"
        ],
        "category": "broad_technical",
        "rationale": "Energy efficiency is a key concern in embedded ML, linking to broader sustainability goals.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "ARM Cortex-M55",
        "canonical": "ARM Cortex-M55",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The ARM Cortex-M55 is a specific hardware platform used in the study, crucial for replication and context.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "Real-time Edge Devices",
        "canonical": "Real-time Edge Device",
        "aliases": [
          "Edge Devices"
        ],
        "category": "specific_connectable",
        "rationale": "Real-time edge devices are vital for understanding the deployment environment of embedded ML.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "methodology",
      "experimental results",
      "measurement"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Processing Units",
      "resolved_canonical": "Neural Processing Unit",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Embedded Microcontrollers",
      "resolved_canonical": "Embedded Microcontroller",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Energy Efficiency",
      "resolved_canonical": "Energy Efficiency",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "ARM Cortex-M55",
      "resolved_canonical": "ARM Cortex-M55",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Real-time Edge Devices",
      "resolved_canonical": "Real-time Edge Device",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17533.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17533](https://arxiv.org/abs/2509.17533)

## 🔗 유사한 논문
- [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (87.4% similar)
- [[2025-09-19/MaRVIn_ A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration_20250919|MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration]] (82.9% similar)
- [[2025-09-22/Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers_20250922|Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers]] (82.2% similar)
- [[2025-09-22/Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs_20250922|Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs]] (82.1% similar)
- [[2025-09-22/Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices_20250922|Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices]] (80.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Energy Efficiency|Energy Efficiency]]
**🔗 Specific Connectable**: [[keywords/Real-time Edge Device|Real-time Edge Device]]
**⚡ Unique Technical**: [[keywords/Neural Processing Unit|Neural Processing Unit]], [[keywords/Embedded Microcontroller|Embedded Microcontroller]], [[keywords/ARM Cortex-M55|ARM Cortex-M55]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17533v1 Announce Type: cross 
Abstract: The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quantization and pruning reduce model size and computation, hardware acceleration has emerged as a decisive enabler for efficient embedded inference. This paper evaluates the impact of Neural Processing Units (NPUs) on MCU-based ML execution, using the ARM Cortex-M55 core combined with the Ethos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a representative platform. A rigorous measurement methodology was employed, incorporating per-inference net energy accounting via GPIO-triggered high-resolution digital multimeter synchronization and idle-state subtraction, ensuring accurate attribution of energy costs. Experimental results across six representative ML models -including MiniResNet, MobileNetV2, FD-MobileNet, MNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains when inference is offloaded to the NPU. For moderate to large networks, latency improvements ranged from 7x to over 125x, with per-inference net energy reductions up to 143x. Notably, the NPU enabled execution of models unsupported on CPU-only paths, such as SSD-MobileNet, highlighting its functional as well as efficiency advantages. These findings establish NPUs as a cornerstone of energy-aware embedded AI, enabling real-time, power-constrained ML inference at the MCU level.

## 📝 요약

이 논문은 마이크로컨트롤러(MCU)에서 머신러닝(ML) 모델을 실행할 때의 에너지, 지연 시간, 메모리 제약을 해결하기 위해 하드웨어 가속의 중요성을 평가합니다. ARM Cortex-M55 코어와 Ethos-U55 NPU를 사용한 실험에서, NPU를 활용한 경우 ML 모델의 추론 효율성이 크게 향상됨을 보여줍니다. MiniResNet, MobileNetV2 등 6개의 대표적인 ML 모델을 대상으로 한 실험 결과, NPU를 사용하면 지연 시간이 최대 125배, 에너지는 최대 143배 절감되었습니다. 특히, SSD-MobileNet과 같은 모델은 CPU만으로는 실행할 수 없지만, NPU를 통해 가능해졌습니다. 이러한 결과는 NPU가 에너지 효율적인 임베디드 AI 구현에 핵심적임을 입증합니다.

## 🎯 주요 포인트

- 1. 마이크로컨트롤러(MCU) 기반의 머신러닝(ML) 실행에서 하드웨어 가속, 특히 신경처리장치(NPU)의 활용이 효율적인 임베디드 추론에 결정적인 역할을 한다.
- 2. ARM Cortex-M55 코어와 Ethos-U55 NPU를 활용한 실험에서, NPU로 추론을 오프로드할 경우 상당한 효율성 향상이 관찰되었다.
- 3. 중간에서 대형 네트워크의 경우, 지연 시간 개선은 7배에서 125배 이상, 추론당 에너지 소비는 최대 143배까지 감소하였다.
- 4. NPU는 CPU만으로는 실행할 수 없는 모델, 예를 들어 SSD-MobileNet, 을 실행 가능하게 하여 기능적 및 효율성 측면에서 이점을 제공한다.
- 5. 이러한 결과는 NPU가 에너지 효율적인 임베디드 AI의 핵심 요소임을 입증하며, 실시간 전력 제약 ML 추론을 MCU 수준에서 가능하게 한다.


---

*Generated on 2025-09-24 00:00:45*