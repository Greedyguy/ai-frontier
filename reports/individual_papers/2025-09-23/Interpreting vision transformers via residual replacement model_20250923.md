---
keywords:
  - Transformer
  - Residual Replacement Model
  - Sparse Autoencoder
  - Feature Evolution
  - Debiasing Techniques
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17401
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:54:50.189852",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Residual Replacement Model",
    "Sparse Autoencoder",
    "Feature Evolution",
    "Debiasing Techniques"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Residual Replacement Model": 0.8,
    "Sparse Autoencoder": 0.78,
    "Feature Evolution": 0.7,
    "Debiasing Techniques": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformers",
        "canonical": "Transformer",
        "aliases": [
          "ViT",
          "Vision Transformer"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in deep learning, and linking this to the broader concept of Transformers facilitates understanding of its application in vision tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Residual Replacement Model",
        "canonical": "Residual Replacement Model",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel model introduced in the paper, offering a new perspective on interpretability in vision transformers.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Sparse Autoencoders",
        "canonical": "Sparse Autoencoder",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Sparse autoencoders are a specific technique used in the analysis, linking to broader discussions on feature extraction and representation learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Feature Evolution",
        "canonical": "Feature Evolution",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Understanding feature evolution is crucial for interpreting how vision transformers process information, making it a unique technical concept.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Debiasing Spurious Correlations",
        "canonical": "Debiasing Techniques",
        "aliases": [
          "Debiasing Spurious Correlations"
        ],
        "category": "evolved_concepts",
        "rationale": "Debiasing is a growing area of interest in machine learning, and linking this concept helps in understanding its application in vision transformers.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "feature types",
      "spatial positions",
      "curves"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Residual Replacement Model",
      "resolved_canonical": "Residual Replacement Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Sparse Autoencoders",
      "resolved_canonical": "Sparse Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Feature Evolution",
      "resolved_canonical": "Feature Evolution",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Debiasing Spurious Correlations",
      "resolved_canonical": "Debiasing Techniques",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Interpreting vision transformers via residual replacement model

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17401.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17401](https://arxiv.org/abs/2509.17401)

## 🔗 유사한 논문
- [[2025-09-22/Large Vision Models Can Solve Mental Rotation Problems_20250922|Large Vision Models Can Solve Mental Rotation Problems]] (85.4% similar)
- [[2025-09-22/Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks_20250922|Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks]] (85.3% similar)
- [[2025-09-18/[Re] Improving Interpretation Faithfulness for Vision Transformers_20250918|[Re] Improving Interpretation Faithfulness for Vision Transformers]] (82.9% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (81.6% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (81.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Sparse Autoencoder|Sparse Autoencoder]]
**⚡ Unique Technical**: [[keywords/Residual Replacement Model|Residual Replacement Model]], [[keywords/Feature Evolution|Feature Evolution]]
**🚀 Evolved Concepts**: [[keywords/Debiasing Techniques|Debiasing Techniques]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17401v1 Announce Type: cross 
Abstract: How do vision transformers (ViTs) represent and process the world? This paper addresses this long-standing question through the first systematic analysis of 6.6K features across all layers, extracted via sparse autoencoders, and by introducing the residual replacement model, which replaces ViT computations with interpretable features in the residual stream. Our analysis reveals not only a feature evolution from low-level patterns to high-level semantics, but also how ViTs encode curves and spatial positions through specialized feature types. The residual replacement model scalably produces a faithful yet parsimonious circuit for human-scale interpretability by significantly simplifying the original computations. As a result, this framework enables intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility of our framework in debiasing spurious correlations.

## 📝 요약

이 논문은 비전 트랜스포머(ViT)의 작동 방식을 체계적으로 분석합니다. 6,600개의 특징을 층별로 분석하고, 잔여 대체 모델을 도입하여 ViT의 계산을 해석 가능한 특징으로 대체합니다. 분석 결과, ViT가 저수준 패턴에서 고수준 의미로 특징을 발전시키고, 곡선 및 공간 위치를 특수한 특징 유형으로 인코딩하는 방식을 밝혀냈습니다. 잔여 대체 모델은 원래의 복잡한 계산을 단순화하여 인간이 이해할 수 있는 해석 가능한 회로를 제공합니다. 이 프레임워크는 ViT 메커니즘에 대한 직관적인 이해를 가능하게 하며, 편향된 상관관계를 제거하는 데 유용함을 입증합니다.

## 🎯 주요 포인트

- 1. 이 논문은 비전 트랜스포머(ViTs)의 모든 층에서 6.6K 특징을 체계적으로 분석하여 ViTs가 세상을 어떻게 표현하고 처리하는지를 탐구합니다.
- 2. 희소 오토인코더를 통해 추출된 특징을 바탕으로, ViTs의 특징이 저수준 패턴에서 고수준 의미로 진화하는 과정을 밝혀냅니다.
- 3. ViTs가 곡선과 공간적 위치를 특수화된 특징 유형을 통해 인코딩하는 방식을 분석합니다.
- 4. 잔여 대체 모델을 도입하여 ViT 계산을 해석 가능한 특징으로 대체함으로써, 인간이 이해할 수 있는 직관적인 ViT 메커니즘을 제공합니다.
- 5. 제안된 프레임워크는 편향된 상관관계를 제거하는 데 유용함을 입증합니다.


---

*Generated on 2025-09-23 23:54:50*