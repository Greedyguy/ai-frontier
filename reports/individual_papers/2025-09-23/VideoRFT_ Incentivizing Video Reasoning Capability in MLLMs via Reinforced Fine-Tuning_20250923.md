---
keywords:
  - Reinforcement Fine-Tuning
  - Multimodal Learning
  - Chain-of-Thought Annotations
  - Semantic-Consistency Reward
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2505.12434
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:23:04.385775",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Fine-Tuning",
    "Multimodal Learning",
    "Chain-of-Thought Annotations",
    "Semantic-Consistency Reward",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Fine-Tuning": 0.78,
    "Multimodal Learning": 0.82,
    "Chain-of-Thought Annotations": 0.77,
    "Semantic-Consistency Reward": 0.75,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Fine-Tuning",
        "canonical": "Reinforcement Fine-Tuning",
        "aliases": [
          "RFT"
        ],
        "category": "unique_technical",
        "rationale": "Reinforcement Fine-Tuning is a novel approach specific to this paper, enhancing video reasoning in MLLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that connects language and vision processing, relevant to the paper's focus.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Chain-of-Thought Annotations",
        "canonical": "Chain-of-Thought Annotations",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Chain-of-Thought Annotations are crucial for reasoning processes in the proposed method, providing a unique link.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Semantic-Consistency Reward",
        "canonical": "Semantic-Consistency Reward",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This reward mechanism is a unique contribution of the paper, enhancing the alignment of textual and visual reasoning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a rapidly evolving concept that this paper contributes to by improving video reasoning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "video reasoning",
      "human intelligence",
      "visual input"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Fine-Tuning",
      "resolved_canonical": "Reinforcement Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Chain-of-Thought Annotations",
      "resolved_canonical": "Chain-of-Thought Annotations",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Semantic-Consistency Reward",
      "resolved_canonical": "Semantic-Consistency Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.12434.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2505.12434](https://arxiv.org/abs/2505.12434)

## 🔗 유사한 논문
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (87.3% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (86.9% similar)
- [[2025-09-23/On the Suitability of Reinforcement Fine-Tuning to Visual Tasks_20250923|On the Suitability of Reinforcement Fine-Tuning to Visual Tasks]] (86.4% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (85.6% similar)
- [[2025-09-23/EvoCoT_ Overcoming the Exploration Bottleneck in Reinforcement Learning_20250923|EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning]] (85.1% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Reinforcement Fine-Tuning|Reinforcement Fine-Tuning]], [[keywords/Chain-of-Thought Annotations|Chain-of-Thought Annotations]], [[keywords/Semantic-Consistency Reward|Semantic-Consistency Reward]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.12434v3 Announce Type: replace 
Abstract: Reinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VIDEORFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a multi-expert, cognition-inspired CoT curation pipeline. First, we devise a cognition-inspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a MLLM conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strengthen the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning and visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VIDEORFT achieves state-of-the-art performance on six video reasoning benchmarks.

## 📝 요약

논문에서는 대형 언어 모델(LLM)의 인간 수준 추론 능력을 강화하는 강화 미세 조정(RFT)을 비디오 추론에 확장한 VIDEORFT를 제안합니다. VIDEORFT는 비디오의 복잡한 논리 및 시간적 구조를 다루기 위해, 체인 오브 쏘트(CoT) 주석을 활용한 지도 학습(SFT)과 강화 학습(RL)의 두 단계로 구성됩니다. 비디오 CoT 데이터셋의 부족 문제를 해결하기 위해, 다중 전문가 기반의 CoT 큐레이션 파이프라인을 개발하여, 비디오 콘텐츠의 구조적 표현을 바탕으로 초기 CoT를 생성하고, 실제 비디오를 기반으로 수정합니다. 이를 통해 두 개의 새로운 데이터셋을 구축하였으며, RL 단계에서는 텍스트 추론과 시각적 증거의 일치를 촉진하는 새로운 의미 일관성 보상을 도입했습니다. 실험 결과, VIDEORFT는 여섯 가지 비디오 추론 벤치마크에서 최첨단 성능을 달성했습니다.

## 🎯 주요 포인트

- 1. VIDEORFT는 강화 학습 미세 조정(RFT) 패러다임을 확장하여 MLLM에서 인간과 같은 비디오 추론 능력을 개발하는 새로운 접근법입니다.
- 2. 비디오 도메인에서의 주요 과제는 대규모, 고품질 비디오 CoT 데이터셋의 부족이며, 이를 해결하기 위해 다중 전문가, 인지 영감을 받은 CoT 큐레이션 파이프라인을 구축했습니다.
- 3. 새로운 데이터셋 VideoRFT-CoT-102K와 VideoRFT-RL-310K를 생성하여 감독 미세 조정(SFT) 및 강화 학습(RL)에 활용합니다.
- 4. RL 단계에서 텍스트 추론과 시각적 증거의 일치를 촉진하는 새로운 의미 일관성 보상을 도입하여 모델이 시각적 입력에 기반한 일관되고 맥락 인식적인 추론 출력을 생성하도록 유도합니다.
- 5. VIDEORFT는 여섯 가지 비디오 추론 벤치마크에서 최첨단 성능을 달성했습니다.


---

*Generated on 2025-09-24 05:23:04*