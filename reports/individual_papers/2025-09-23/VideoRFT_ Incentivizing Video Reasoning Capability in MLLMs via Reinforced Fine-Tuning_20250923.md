---
keywords:
  - Reinforcement Fine-Tuning
  - Multimodal Learning
  - Chain-of-Thought Annotations
  - Semantic-Consistency Reward
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2505.12434
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:23:04.385775",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Fine-Tuning",
    "Multimodal Learning",
    "Chain-of-Thought Annotations",
    "Semantic-Consistency Reward",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Fine-Tuning": 0.78,
    "Multimodal Learning": 0.82,
    "Chain-of-Thought Annotations": 0.77,
    "Semantic-Consistency Reward": 0.75,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Fine-Tuning",
        "canonical": "Reinforcement Fine-Tuning",
        "aliases": [
          "RFT"
        ],
        "category": "unique_technical",
        "rationale": "Reinforcement Fine-Tuning is a novel approach specific to this paper, enhancing video reasoning in MLLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that connects language and vision processing, relevant to the paper's focus.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Chain-of-Thought Annotations",
        "canonical": "Chain-of-Thought Annotations",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Chain-of-Thought Annotations are crucial for reasoning processes in the proposed method, providing a unique link.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Semantic-Consistency Reward",
        "canonical": "Semantic-Consistency Reward",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This reward mechanism is a unique contribution of the paper, enhancing the alignment of textual and visual reasoning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a rapidly evolving concept that this paper contributes to by improving video reasoning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "video reasoning",
      "human intelligence",
      "visual input"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Fine-Tuning",
      "resolved_canonical": "Reinforcement Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Chain-of-Thought Annotations",
      "resolved_canonical": "Chain-of-Thought Annotations",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Semantic-Consistency Reward",
      "resolved_canonical": "Semantic-Consistency Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.12434.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2505.12434](https://arxiv.org/abs/2505.12434)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (87.3% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (86.9% similar)
- [[2025-09-23/On the Suitability of Reinforcement Fine-Tuning to Visual Tasks_20250923|On the Suitability of Reinforcement Fine-Tuning to Visual Tasks]] (86.4% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (85.6% similar)
- [[2025-09-23/EvoCoT_ Overcoming the Exploration Bottleneck in Reinforcement Learning_20250923|EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Fine-Tuning|Reinforcement Fine-Tuning]], [[keywords/Chain-of-Thought Annotations|Chain-of-Thought Annotations]], [[keywords/Semantic-Consistency Reward|Semantic-Consistency Reward]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.12434v3 Announce Type: replace 
Abstract: Reinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VIDEORFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a multi-expert, cognition-inspired CoT curation pipeline. First, we devise a cognition-inspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a MLLM conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strengthen the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning and visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VIDEORFT achieves state-of-the-art performance on six video reasoning benchmarks.

## ğŸ“ ìš”ì•½

ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¸ê°„ ìˆ˜ì¤€ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•˜ëŠ” ê°•í™” ë¯¸ì„¸ ì¡°ì •(RFT)ì„ ë¹„ë””ì˜¤ ì¶”ë¡ ì— í™•ì¥í•œ VIDEORFTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. VIDEORFTëŠ” ë¹„ë””ì˜¤ì˜ ë³µì¡í•œ ë…¼ë¦¬ ë° ì‹œê°„ì  êµ¬ì¡°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´, ì²´ì¸ ì˜¤ë¸Œ ì˜íŠ¸(CoT) ì£¼ì„ì„ í™œìš©í•œ ì§€ë„ í•™ìŠµ(SFT)ê³¼ ê°•í™” í•™ìŠµ(RL)ì˜ ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë¹„ë””ì˜¤ CoT ë°ì´í„°ì…‹ì˜ ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë‹¤ì¤‘ ì „ë¬¸ê°€ ê¸°ë°˜ì˜ CoT íë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ì„ ê°œë°œí•˜ì—¬, ë¹„ë””ì˜¤ ì½˜í…ì¸ ì˜ êµ¬ì¡°ì  í‘œí˜„ì„ ë°”íƒ•ìœ¼ë¡œ ì´ˆê¸° CoTë¥¼ ìƒì„±í•˜ê³ , ì‹¤ì œ ë¹„ë””ì˜¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‘ ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì˜€ìœ¼ë©°, RL ë‹¨ê³„ì—ì„œëŠ” í…ìŠ¤íŠ¸ ì¶”ë¡ ê³¼ ì‹œê°ì  ì¦ê±°ì˜ ì¼ì¹˜ë¥¼ ì´‰ì§„í•˜ëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ ì¼ê´€ì„± ë³´ìƒì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, VIDEORFTëŠ” ì—¬ì„¯ ê°€ì§€ ë¹„ë””ì˜¤ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VIDEORFTëŠ” ê°•í™” í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(RFT) íŒ¨ëŸ¬ë‹¤ì„ì„ í™•ì¥í•˜ì—¬ MLLMì—ì„œ ì¸ê°„ê³¼ ê°™ì€ ë¹„ë””ì˜¤ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°œë°œí•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.
- 2. ë¹„ë””ì˜¤ ë„ë©”ì¸ì—ì„œì˜ ì£¼ìš” ê³¼ì œëŠ” ëŒ€ê·œëª¨, ê³ í’ˆì§ˆ ë¹„ë””ì˜¤ CoT ë°ì´í„°ì…‹ì˜ ë¶€ì¡±ì´ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ì „ë¬¸ê°€, ì¸ì§€ ì˜ê°ì„ ë°›ì€ CoT íë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.
- 3. ìƒˆë¡œìš´ ë°ì´í„°ì…‹ VideoRFT-CoT-102Kì™€ VideoRFT-RL-310Kë¥¼ ìƒì„±í•˜ì—¬ ê°ë… ë¯¸ì„¸ ì¡°ì •(SFT) ë° ê°•í™” í•™ìŠµ(RL)ì— í™œìš©í•©ë‹ˆë‹¤.
- 4. RL ë‹¨ê³„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ë¡ ê³¼ ì‹œê°ì  ì¦ê±°ì˜ ì¼ì¹˜ë¥¼ ì´‰ì§„í•˜ëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ ì¼ê´€ì„± ë³´ìƒì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì´ ì‹œê°ì  ì…ë ¥ì— ê¸°ë°˜í•œ ì¼ê´€ë˜ê³  ë§¥ë½ ì¸ì‹ì ì¸ ì¶”ë¡  ì¶œë ¥ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
- 5. VIDEORFTëŠ” ì—¬ì„¯ ê°€ì§€ ë¹„ë””ì˜¤ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:23:04*