---
keywords:
  - Vision-Caption aware Supervised FineTuning
  - Vision-Language Model
  - Zero-Shot Learning
  - ScienceQA
  - HiSciVQA
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16628
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:28:23.649826",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Caption aware Supervised FineTuning",
    "Vision-Language Model",
    "Zero-Shot Learning",
    "ScienceQA",
    "HiSciVQA"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Caption aware Supervised FineTuning": 0.8,
    "Vision-Language Model": 0.78,
    "Zero-Shot Learning": 0.79,
    "ScienceQA": 0.77,
    "HiSciVQA": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Caption aware Supervised FineTuning",
        "canonical": "Vision-Caption aware Supervised FineTuning",
        "aliases": [
          "VCASFT"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel learning paradigm introduced in the paper, central to its contributions.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a core focus of the paper, connecting to recent trends in multimodal learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-shot prompts",
        "canonical": "Zero-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a key technique leveraged in the study, linking to broader AI strategies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      },
      {
        "surface": "ScienceQA",
        "canonical": "ScienceQA",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "ScienceQA is a specific dataset used for benchmarking, relevant for linking to educational contexts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "HiSciVQA",
        "canonical": "HiSciVQA",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "HiSciVQA is a newly developed dataset for low-resource languages, critical for testing the proposed method.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Caption aware Supervised FineTuning",
      "resolved_canonical": "Vision-Caption aware Supervised FineTuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-shot prompts",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "ScienceQA",
      "resolved_canonical": "ScienceQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "HiSciVQA",
      "resolved_canonical": "HiSciVQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16628.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16628](https://arxiv.org/abs/2509.16628)

## 🔗 유사한 논문
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (84.6% similar)
- [[2025-09-23/ProtoVQA_ An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering_20250923|ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering]] (84.5% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (84.5% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (84.4% similar)
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (83.5% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Vision-Caption aware Supervised FineTuning|Vision-Caption aware Supervised FineTuning]], [[keywords/ScienceQA|ScienceQA]], [[keywords/HiSciVQA|HiSciVQA]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16628v1 Announce Type: new 
Abstract: In this study, we introduce Vision-Caption aware Supervised FineTuning (VCASFT), a novel learning paradigm designed to enhance the performance of smaller Vision Language Models(VLMs) on scientific visual question answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts alongside question-answer pairs and instruction-tunes models to yield significant performance improvements. To comprehensively evaluate VCASFT, we benchmark it on ScienceQA, which consists of questions across diverse languages, subjects, and fields, demonstrating its adaptability and effectiveness in a variety of educational contexts. Additionally, to further demonstrate the effectiveness of this technique on lowresource languages, we developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated Hindi multimodal Q&amp;A pairs. This dataset addresses the critical need for low-resource language Q&amp;A datasets and serves as a foundation for testing VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness surpassing traditional n-gram matching accuracy metrics. We are committed to advancing the field by open-sourcing all code files and the HiSciVQA dataset for the research community.

## 📝 요약

이 연구에서는 작은 규모의 비전 언어 모델(VLM)의 과학적 시각 질문 응답(VQA) 성능을 향상시키기 위한 새로운 학습 패러다임인 Vision-Caption aware Supervised FineTuning(VCASFT)을 소개합니다. VCASFT는 이미지 캡션을 제로샷 프롬프트로 활용하여 질문-응답 쌍과 함께 모델을 조정하여 성능을 크게 향상시킵니다. ScienceQA를 통해 다양한 언어와 주제에서 VCASFT의 적응성과 효과를 입증하였으며, 저자들은 저자원 언어에서의 효과성을 입증하기 위해 2,245개의 힌디어 멀티모달 Q&A 쌍으로 구성된 HiSciVQA 데이터셋을 개발했습니다. 또한, 전통적인 n-그램 매칭 정확도를 넘어서는 새로운 LLM 기반 평가 방식을 도입하여 모델의 효과를 심층적으로 평가합니다. 모든 코드 파일과 HiSciVQA 데이터셋은 연구 커뮤니티를 위해 오픈 소스로 제공됩니다.

## 🎯 주요 포인트

- 1. VCASFT는 작은 규모의 Vision Language Models(VLMs)의 성능을 향상시키기 위해 이미지 캡션을 제로샷 프롬프트로 활용하는 새로운 학습 패러다임입니다.
- 2. VCASFT는 다양한 언어, 주제, 분야의 질문을 포함한 ScienceQA에서 벤치마크되어 교육적 맥락에서의 적응성과 효과성을 입증했습니다.
- 3. 저자들은 저자원 언어에서의 VCASFT의 효과성을 입증하기 위해 2,245개의 힌디어 다중 모달 Q&A 쌍으로 구성된 HiSciVQA 데이터셋을 개발했습니다.
- 4. HiSciVQA 데이터셋은 저자원 언어 Q&A 데이터셋의 필요성을 충족시키며 VCASFT 테스트의 기초를 제공합니다.
- 5. 연구진은 모든 코드 파일과 HiSciVQA 데이터셋을 오픈 소스로 공개하여 연구 커뮤니티의 발전에 기여하고자 합니다.


---

*Generated on 2025-09-24 04:28:23*