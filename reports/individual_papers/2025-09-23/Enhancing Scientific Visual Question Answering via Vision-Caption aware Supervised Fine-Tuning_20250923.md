---
keywords:
  - Vision-Caption aware Supervised FineTuning
  - Vision-Language Model
  - Zero-Shot Learning
  - ScienceQA
  - HiSciVQA
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16628
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:28:23.649826",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Caption aware Supervised FineTuning",
    "Vision-Language Model",
    "Zero-Shot Learning",
    "ScienceQA",
    "HiSciVQA"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Caption aware Supervised FineTuning": 0.8,
    "Vision-Language Model": 0.78,
    "Zero-Shot Learning": 0.79,
    "ScienceQA": 0.77,
    "HiSciVQA": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Caption aware Supervised FineTuning",
        "canonical": "Vision-Caption aware Supervised FineTuning",
        "aliases": [
          "VCASFT"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel learning paradigm introduced in the paper, central to its contributions.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a core focus of the paper, connecting to recent trends in multimodal learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-shot prompts",
        "canonical": "Zero-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a key technique leveraged in the study, linking to broader AI strategies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      },
      {
        "surface": "ScienceQA",
        "canonical": "ScienceQA",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "ScienceQA is a specific dataset used for benchmarking, relevant for linking to educational contexts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "HiSciVQA",
        "canonical": "HiSciVQA",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "HiSciVQA is a newly developed dataset for low-resource languages, critical for testing the proposed method.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Caption aware Supervised FineTuning",
      "resolved_canonical": "Vision-Caption aware Supervised FineTuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-shot prompts",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "ScienceQA",
      "resolved_canonical": "ScienceQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "HiSciVQA",
      "resolved_canonical": "HiSciVQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16628.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16628](https://arxiv.org/abs/2509.16628)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (84.6% similar)
- [[2025-09-23/ProtoVQA_ An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering_20250923|ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering]] (84.5% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (84.5% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (84.4% similar)
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Vision-Caption aware Supervised FineTuning|Vision-Caption aware Supervised FineTuning]], [[keywords/ScienceQA|ScienceQA]], [[keywords/HiSciVQA|HiSciVQA]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16628v1 Announce Type: new 
Abstract: In this study, we introduce Vision-Caption aware Supervised FineTuning (VCASFT), a novel learning paradigm designed to enhance the performance of smaller Vision Language Models(VLMs) on scientific visual question answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts alongside question-answer pairs and instruction-tunes models to yield significant performance improvements. To comprehensively evaluate VCASFT, we benchmark it on ScienceQA, which consists of questions across diverse languages, subjects, and fields, demonstrating its adaptability and effectiveness in a variety of educational contexts. Additionally, to further demonstrate the effectiveness of this technique on lowresource languages, we developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated Hindi multimodal Q&amp;A pairs. This dataset addresses the critical need for low-resource language Q&amp;A datasets and serves as a foundation for testing VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness surpassing traditional n-gram matching accuracy metrics. We are committed to advancing the field by open-sourcing all code files and the HiSciVQA dataset for the research community.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ì—ì„œëŠ” ì‘ì€ ê·œëª¨ì˜ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ê³¼í•™ì  ì‹œê° ì§ˆë¬¸ ì‘ë‹µ(VQA) ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì¸ Vision-Caption aware Supervised FineTuning(VCASFT)ì„ ì†Œê°œí•©ë‹ˆë‹¤. VCASFTëŠ” ì´ë¯¸ì§€ ìº¡ì…˜ì„ ì œë¡œìƒ· í”„ë¡¬í”„íŠ¸ë¡œ í™œìš©í•˜ì—¬ ì§ˆë¬¸-ì‘ë‹µ ìŒê³¼ í•¨ê»˜ ëª¨ë¸ì„ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ScienceQAë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ì£¼ì œì—ì„œ VCASFTì˜ ì ì‘ì„±ê³¼ íš¨ê³¼ë¥¼ ì…ì¦í•˜ì˜€ìœ¼ë©°, ì €ìë“¤ì€ ì €ìì› ì–¸ì–´ì—ì„œì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•˜ê¸° ìœ„í•´ 2,245ê°œì˜ íŒë””ì–´ ë©€í‹°ëª¨ë‹¬ Q&A ìŒìœ¼ë¡œ êµ¬ì„±ëœ HiSciVQA ë°ì´í„°ì…‹ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì „í†µì ì¸ n-ê·¸ë¨ ë§¤ì¹­ ì •í™•ë„ë¥¼ ë„˜ì–´ì„œëŠ” ìƒˆë¡œìš´ LLM ê¸°ë°˜ í‰ê°€ ë°©ì‹ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ëª¨ë“  ì½”ë“œ íŒŒì¼ê³¼ HiSciVQA ë°ì´í„°ì…‹ì€ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ìœ„í•´ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VCASFTëŠ” ì‘ì€ ê·œëª¨ì˜ Vision Language Models(VLMs)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì´ë¯¸ì§€ ìº¡ì…˜ì„ ì œë¡œìƒ· í”„ë¡¬í”„íŠ¸ë¡œ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤.
- 2. VCASFTëŠ” ë‹¤ì–‘í•œ ì–¸ì–´, ì£¼ì œ, ë¶„ì•¼ì˜ ì§ˆë¬¸ì„ í¬í•¨í•œ ScienceQAì—ì„œ ë²¤ì¹˜ë§ˆí¬ë˜ì–´ êµìœ¡ì  ë§¥ë½ì—ì„œì˜ ì ì‘ì„±ê³¼ íš¨ê³¼ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 3. ì €ìë“¤ì€ ì €ìì› ì–¸ì–´ì—ì„œì˜ VCASFTì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•˜ê¸° ìœ„í•´ 2,245ê°œì˜ íŒë””ì–´ ë‹¤ì¤‘ ëª¨ë‹¬ Q&A ìŒìœ¼ë¡œ êµ¬ì„±ëœ HiSciVQA ë°ì´í„°ì…‹ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- 4. HiSciVQA ë°ì´í„°ì…‹ì€ ì €ìì› ì–¸ì–´ Q&A ë°ì´í„°ì…‹ì˜ í•„ìš”ì„±ì„ ì¶©ì¡±ì‹œí‚¤ë©° VCASFT í…ŒìŠ¤íŠ¸ì˜ ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ì§„ì€ ëª¨ë“  ì½”ë“œ íŒŒì¼ê³¼ HiSciVQA ë°ì´í„°ì…‹ì„ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œí•˜ì—¬ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì˜ ë°œì „ì— ê¸°ì—¬í•˜ê³ ì í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:28:23*