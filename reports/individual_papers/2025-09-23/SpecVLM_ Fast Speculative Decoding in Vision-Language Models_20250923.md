---
keywords:
  - Vision-Language Model
  - Speculative Decoding
  - Elastic Visual Compressor
  - Online-Logit Distillation
  - Autoregressive Inference
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.11815
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:29:30.507887",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Speculative Decoding",
    "Elastic Visual Compressor",
    "Online-Logit Distillation",
    "Autoregressive Inference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Speculative Decoding": 0.8,
    "Elastic Visual Compressor": 0.78,
    "Online-Logit Distillation": 0.77,
    "Autoregressive Inference": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "This is a trending concept that bridges vision and language processing, crucial for linking multimodal research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique technique for accelerating model inference, relevant for linking to performance optimization studies.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Elastic Visual Compressor",
        "canonical": "Elastic Visual Compressor",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel component for adaptive visual data processing, useful for linking to efficiency and compression research.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Online-Logit Distillation",
        "canonical": "Online-Logit Distillation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces an innovative training protocol, relevant for linking to distillation and training efficiency.",
        "novelty_score": 0.72,
        "connectivity_score": 0.58,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Autoregressive Inference",
        "canonical": "Autoregressive Inference",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A foundational concept in model inference, essential for linking to various model architectures.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Elastic Visual Compressor",
      "resolved_canonical": "Elastic Visual Compressor",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Online-Logit Distillation",
      "resolved_canonical": "Online-Logit Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.58,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Autoregressive Inference",
      "resolved_canonical": "Autoregressive Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# SpecVLM: Fast Speculative Decoding in Vision-Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.11815.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.11815](https://arxiv.org/abs/2509.11815)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (94.0% similar)
- [[2025-09-23/Spec-VLA_ Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance_20250923|Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance]] (89.9% similar)
- [[2025-09-23/Spiffy_ Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding_20250923|Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding]] (85.9% similar)
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (85.0% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Autoregressive Inference|Autoregressive Inference]]
**âš¡ Unique Technical**: [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Elastic Visual Compressor|Elastic Visual Compressor]], [[keywords/Online-Logit Distillation|Online-Logit Distillation]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.11815v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at https://github.com/haiduo/SpecVLM.

## ğŸ“ ìš”ì•½

SpecVLMì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì‹œìŠ¤í…œìœ¼ë¡œ, EAGLE-2 ìŠ¤íƒ€ì¼ì˜ EagleVLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì „ì²´ ìê°€íšŒê·€ ì¶”ë¡  ëŒ€ë¹„ 1.5~2.3ë°°ì˜ ì†ë„ í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì ì‘í˜• ì‹œê° ì••ì¶•ê¸°ë¥¼ í™œìš©í•˜ì—¬ FLOPs/íŒŒë¼ë¯¸í„°ì™€ ì •í™•ë„ ê°„ì˜ ê· í˜•ì„ ë§ì¶”ë©°, ì˜¤í”„ë¼ì¸ ì¦ë¥˜ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì˜¨ë¼ì¸ ë¡œì§“ ì¦ë¥˜ í”„ë¡œí† ì½œì„ í†µí•´ ì €ì¥ ë° ì „ì²˜ë¦¬ ì—†ì´ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥ì¼€ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ SpecVLMì€ LLaVA ë° MMMUì—ì„œ 5 ì—í¬í¬ ë‚´ì— 2.5~2.9ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•˜ë©°, í•´ìƒë„ì™€ ì‘ì—… ë‚œì´ë„ì— ê´€ê³„ì—†ì´ ëª©í‘œ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SpecVLMì€ EAGLE-2 ìŠ¤íƒ€ì¼ì˜ EagleVLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬, ì™„ì „í•œ ìíšŒê·€ ì¶”ë¡ ì— ë¹„í•´ 1.5~2.3ë°°ì˜ ì†ë„ í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. SpecVLMì€ íƒ„ë ¥ì ì¸ ë¹„ì£¼ì–¼ ì••ì¶•ê¸°ë¥¼ ë„ì…í•˜ì—¬, FLOPs/íŒŒë¼ë¯¸í„°ì™€ ì •í™•ë„ë¥¼ ê· í˜• ìˆê²Œ ì¡°ì •í•˜ë©° VLM ì¶”ë¡ ì„ ê°€ì†í™”í•©ë‹ˆë‹¤.
- 3. ì˜¨ë¼ì¸ ë¡œì§“ ì¦ë¥˜ í”„ë¡œí† ì½œì„ ì œì•ˆí•˜ì—¬ ì €ì¥ ë° ì „ì²˜ë¦¬ ì—†ì´ë„ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ìœ ì§€í•˜ë©°, í›ˆë ¨ ì‹œê°„ ë™ì•ˆ ì´ˆì•ˆ ëª¨ë¸ì˜ ìˆ˜ìš© ê¸¸ì´ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
- 4. SpecVLMì€ LLaVAì™€ MMMUì—ì„œ 5 ì—í¬í¬ ë‚´ì— 2.5~2.9ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•˜ë©°, í•´ìƒë„ì™€ ì‘ì—… ë‚œì´ë„ì— ê´€ê³„ì—†ì´ ì¼ê´€ëœ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. SpecVLMì€ ëŒ€ìƒ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì†ì‹¤ ì—†ëŠ” ë””ì½”ë”©ì„ ì‹¤í˜„í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:29:30*