---
keywords:
  - Large Language Model
  - Mathematical Reasoning
  - Chain-of-Thought Prompting
  - Overfitting in Contextual Learning
  - Corrective Rationales
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2502.08550
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:45:58.406502",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Mathematical Reasoning",
    "Chain-of-Thought Prompting",
    "Overfitting in Contextual Learning",
    "Corrective Rationales"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Mathematical Reasoning": 0.7,
    "Chain-of-Thought Prompting": 0.78,
    "Overfitting in Contextual Learning": 0.72,
    "Corrective Rationales": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on learning dynamics without explicit rationales.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "math reasoning tasks",
        "canonical": "Mathematical Reasoning",
        "aliases": [
          "math reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Specific task type explored for LLM performance improvements.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      },
      {
        "surface": "chain-of-thought prompting",
        "canonical": "Chain-of-Thought Prompting",
        "aliases": [
          "CoT prompting"
        ],
        "category": "specific_connectable",
        "rationale": "A specific prompting strategy compared against in the study.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "overfitting to in-context rationales",
        "canonical": "Overfitting in Contextual Learning",
        "aliases": [
          "overfitting to rationales"
        ],
        "category": "unique_technical",
        "rationale": "Describes a key limitation observed in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "corrective rationales",
        "canonical": "Corrective Rationales",
        "aliases": [
          "rationales",
          "correction rationales"
        ],
        "category": "specific_connectable",
        "rationale": "Central concept in evaluating LLM learning without explicit feedback.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "evaluations",
      "context length"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "math reasoning tasks",
      "resolved_canonical": "Mathematical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "chain-of-thought prompting",
      "resolved_canonical": "Chain-of-Thought Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "overfitting to in-context rationales",
      "resolved_canonical": "Overfitting in Contextual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "corrective rationales",
      "resolved_canonical": "Corrective Rationales",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# No Need for Explanations: LLMs can implicitly learn from mistakes in-context

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.08550.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2502.08550](https://arxiv.org/abs/2502.08550)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (87.8% similar)
- [[2025-09-23/Question Answering with LLMs and Learning from Answer Sets_20250923|Question Answering with LLMs and Learning from Answer Sets]] (86.7% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (85.9% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.7% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (85.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Chain-of-Thought Prompting|Chain-of-Thought Prompting]], [[keywords/Corrective Rationales|Corrective Rationales]]
**âš¡ Unique Technical**: [[keywords/Mathematical Reasoning|Mathematical Reasoning]], [[keywords/Overfitting in Contextual Learning|Overfitting in Contextual Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.08550v3 Announce Type: replace-cross 
Abstract: Showing incorrect answers to Large Language Models (LLMs) is a popular strategy to improve their performance in reasoning-intensive tasks. It is widely assumed that, in order to be helpful, the incorrect answers must be accompanied by comprehensive rationales, explicitly detailing where the mistakes are and how to correct them. However, in this work we present a counterintuitive finding: we observe that LLMs perform better in math reasoning tasks when these rationales are eliminated from the context and models are left to infer on their own what makes an incorrect answer flawed. This approach also substantially outperforms chain-of-thought prompting in our evaluations. These results are consistent across LLMs of different sizes and varying reasoning abilities. To gain an understanding of why LLMs learn from mistakes more effectively without explicit corrective rationales, we perform a thorough analysis, investigating changes in context length and answer diversity between different prompting strategies, and their effect on performance. We also examine evidence of overfitting to the in-context rationales when these are provided, and study the extent to which LLMs are able to autonomously infer high-quality corrective rationales given only incorrect answers as input. We find evidence that, while incorrect answers are more beneficial for LLM learning than additional diverse correct answers, explicit corrective rationales over-constrain the model, thus limiting those benefits.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì˜ëª»ëœ ë‹µë³€ì„ ì œì‹œí•˜ëŠ” ì „ëµì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì˜ëª»ëœ ë‹µë³€ì€ ì˜¤ë¥˜ë¥¼ ì„¤ëª…í•˜ê³  ìˆ˜ì • ë°©ë²•ì„ ì œì‹œí•˜ëŠ” ë…¼ë¦¬ì™€ í•¨ê»˜ ì œê³µë˜ì–´ì•¼ í•œë‹¤ê³  ìƒê°ë˜ì§€ë§Œ, ì´ ì—°êµ¬ëŠ” ë°˜ëŒ€ë¡œ ë…¼ë¦¬ ì—†ì´ ì˜ëª»ëœ ë‹µë³€ë§Œ ì œê³µí•  ë•Œ ìˆ˜í•™ì  ì¶”ë¡  ê³¼ì œì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ë°œê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ë‹¤ì–‘í•œ í¬ê¸°ì™€ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ LLMì—ì„œ ì¼ê´€ëœ ì„±ê³¼ë¥¼ ë³´ì´ë©°, ì²´ì¸ ì˜¤ë¸Œ ì˜íŠ¸ í”„ë¡¬íŒ…ë³´ë‹¤ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” ëª…ì‹œì ì¸ ë…¼ë¦¬ê°€ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•˜ë©°, LLMì´ ì˜ëª»ëœ ë‹µë³€ë§Œìœ¼ë¡œë„ ìŠ¤ìŠ¤ë¡œ ë†’ì€ í’ˆì§ˆì˜ ìˆ˜ì • ë…¼ë¦¬ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì˜ëª»ëœ ë‹µë³€ì´ ë‹¤ì–‘í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ë³´ë‹¤ í•™ìŠµì— ë” ìœ ìµí•˜ë‹¤ëŠ” ì¦ê±°ë„ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìˆ˜í•™ì  ì¶”ë¡  ì‘ì—…ì—ì„œ ëª…ì‹œì ì¸ ì„¤ëª… ì—†ì´ ì˜ëª»ëœ ë‹µë³€ë§Œ ì œê³µë  ë•Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 2. ì˜ëª»ëœ ë‹µë³€ì— ëŒ€í•œ ëª…ì‹œì ì¸ ì„¤ëª…ì´ ì—†ì„ ë•Œ, LLMì˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ë©° ì´ëŠ” ë‹¤ì–‘í•œ í¬ê¸°ì™€ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ ëª¨ë¸ì—ì„œë„ ì¼ê´€ë˜ê²Œ ë‚˜íƒ€ë‚œë‹¤.
- 3. ëª…ì‹œì ì¸ ì„¤ëª…ì´ ì œê³µë˜ë©´ LLMì´ ê³¼ì í•©ë  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ í•™ìŠµ íš¨ê³¼ë¥¼ ì œí•œí•œë‹¤.
- 4. ì˜ëª»ëœ ë‹µë³€ë§Œì„ ì…ë ¥ìœ¼ë¡œ ì œê³µë°›ì€ LLMì€ ê³ í’ˆì§ˆì˜ ìˆ˜ì • ì„¤ëª…ì„ ììœ¨ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤.
- 5. ì˜ëª»ëœ ë‹µë³€ì€ ë‹¤ì–‘í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ë³´ë‹¤ LLM í•™ìŠµì— ë” ìœ ìµí•˜ë‹¤.


---

*Generated on 2025-09-24 00:45:58*