---
keywords:
  - Vision-Language Model
  - Referential Segmentation
  - Multimodal Learning
  - Controllable Image Generation
  - Large Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.16806
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:25:17.212953",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Referential Segmentation",
    "Multimodal Learning",
    "Controllable Image Generation",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Referential Segmentation": 0.78,
    "Multimodal Learning": 0.8,
    "Controllable Image Generation": 0.75,
    "Large Language Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Modeling",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Modeling",
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper and connects to the broader field of multimodal learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Referential Segmentation",
        "canonical": "Referential Segmentation",
        "aliases": [
          "Referring Segmentation"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task addressed by the paper, linking segmentation with language understanding.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal Understanding",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Understanding"
        ],
        "category": "specific_connectable",
        "rationale": "The paper contributes to the field of multimodal learning by integrating vision and language.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Controllable Image Generation",
        "canonical": "Controllable Image Generation",
        "aliases": [
          "Image Generation Control"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach described in the paper, enhancing the specificity of image editing tasks.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Vision Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LVLMs"
        ],
        "category": "broad_technical",
        "rationale": "The paper discusses advancements in large-scale models combining vision and language capabilities.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "segmentation masks",
      "diffusion decoder"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Modeling",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Referential Segmentation",
      "resolved_canonical": "Referential Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal Understanding",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Controllable Image Generation",
      "resolved_canonical": "Controllable Image Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Vision Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.16806.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.16806](https://arxiv.org/abs/2506.16806)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (84.3% similar)
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (83.8% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (83.2% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (82.8% similar)
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Referential Segmentation|Referential Segmentation]], [[keywords/Controllable Image Generation|Controllable Image Generation]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.16806v2 Announce Type: replace 
Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat "what to see" and "how to edit" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€ê·œëª¨ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì‹œê°ì  ì´í•´ì™€ ìƒì„± ëª¨ë¸ë§ì„ í†µí•©í•˜ì—¬ ì •í™•í•œ ì½˜í…ì¸  ì´í•´ì™€ ìœ ì—°í•œ í¸ì§‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì ‘ê·¼ë²•ì€ "ë¬´ì—‡ì„ ë³¼ì§€"ì™€ "ì–´ë–»ê²Œ í¸ì§‘í• ì§€"ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” FOCUSë¼ëŠ” í†µí•© LVLMì„ ì œì•ˆí•©ë‹ˆë‹¤. FOCUSëŠ” ì„¸ë¶„í™” ì¸ì‹ê³¼ ê°ì²´ ì¤‘ì‹¬ ìƒì„± ê¸°ëŠ¥ì„ í†µí•©í•œ ì¢…ë‹¨ ê°„ í”„ë ˆì„ì›Œí¬ë¡œ, ì´ì¤‘ ë¸Œëœì¹˜ ì‹œê° ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë°˜ì ì¸ ì˜ë¯¸ì™€ ì„¸ë¶€ ì •ë³´ë¥¼ ë™ì‹œì— í¬ì°©í•©ë‹ˆë‹¤. ë˜í•œ, MoVQGAN ê¸°ë°˜ ì‹œê° í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•˜ì—¬ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì„¸ë¶„í™” ë§ˆìŠ¤í¬ë¥¼ ê³µê°„ ì¡°ê±´ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ í¸ì§‘ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì ì§„ì  ë‹¤ë‹¨ê³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì „ëµì€ ì‹œê° ì¸ì½”ë”©, ì„¸ë¶„í™”, ìƒì„± ëª¨ë“ˆì„ ì •ë ¬í•˜ì—¬ ì„¸ë¶„í™” ì¸ì‹ê³¼ ì„¸ë¶€ ì‹œê° í•©ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‹¤í—˜ ê²°ê³¼, FOCUSëŠ” ì‹œê°ì  ì¸ì‹ê³¼ ìƒì„± ëŠ¥ë ¥ì„ ê³µë™ ìµœì í™”í•˜ì—¬ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. FOCUSëŠ” ì‹œê°ì  ì´í•´ì™€ ìƒì„± ëª¨ë¸ë§ì„ í†µí•©í•˜ì—¬ ì •í™•í•œ ì½˜í…ì¸  ì´í•´ì™€ ìœ ì—°í•œ í¸ì§‘ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í†µí•©ëœ ëŒ€í˜• ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.
- 2. FOCUSëŠ” ê¸€ë¡œë²Œ ì˜ë¯¸ ë§¥ë½ê³¼ ì„¸ë°€í•œ ê³µê°„ ì„¸ë¶€ ì‚¬í•­ì„ ë™ì‹œì— í¬ì°©í•˜ëŠ” ì´ì¤‘ ë¶„ê¸° ì‹œê° ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 3. MoVQGAN ê¸°ë°˜ì˜ ì‹œê° í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•˜ì—¬ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ì´ì‚° ì‹œê° í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 4. ì ì§„ì ì¸ ë‹¤ë‹¨ê³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ì„¸ë¶„í™” ë§ˆìŠ¤í¬ë¥¼ ìµœì í™”í•˜ê³  ê³µê°„ ì¡°ê±´ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ í¸ì§‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 5. FOCUSëŠ” ì‹œê°ì  ì¸ì‹ê³¼ ìƒì„± ê¸°ëŠ¥ì„ ê³µë™ìœ¼ë¡œ ìµœì í™”í•˜ì—¬ ë‹¤ì¤‘ ëª¨ë‹¬ ì´í•´, ì°¸ì¡° ì„¸ë¶„í™” ì •í™•ë„, ì œì–´ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ ìƒì„± ë“±ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:25:17*