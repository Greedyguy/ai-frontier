---
keywords:
  - Vision-Language Model
  - Referential Segmentation
  - Multimodal Learning
  - Controllable Image Generation
  - Large Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.16806
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:25:17.212953",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Referential Segmentation",
    "Multimodal Learning",
    "Controllable Image Generation",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Referential Segmentation": 0.78,
    "Multimodal Learning": 0.8,
    "Controllable Image Generation": 0.75,
    "Large Language Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Modeling",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Modeling",
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper and connects to the broader field of multimodal learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Referential Segmentation",
        "canonical": "Referential Segmentation",
        "aliases": [
          "Referring Segmentation"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task addressed by the paper, linking segmentation with language understanding.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal Understanding",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Understanding"
        ],
        "category": "specific_connectable",
        "rationale": "The paper contributes to the field of multimodal learning by integrating vision and language.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Controllable Image Generation",
        "canonical": "Controllable Image Generation",
        "aliases": [
          "Image Generation Control"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach described in the paper, enhancing the specificity of image editing tasks.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Vision Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LVLMs"
        ],
        "category": "broad_technical",
        "rationale": "The paper discusses advancements in large-scale models combining vision and language capabilities.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "segmentation masks",
      "diffusion decoder"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Modeling",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Referential Segmentation",
      "resolved_canonical": "Referential Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal Understanding",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Controllable Image Generation",
      "resolved_canonical": "Controllable Image Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Vision Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.16806.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.16806](https://arxiv.org/abs/2506.16806)

## 🔗 유사한 논문
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (84.3% similar)
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (83.8% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (83.2% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (82.8% similar)
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (82.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Referential Segmentation|Referential Segmentation]], [[keywords/Controllable Image Generation|Controllable Image Generation]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.16806v2 Announce Type: replace 
Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat "what to see" and "how to edit" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.

## 📝 요약

최근 대규모 비전 언어 모델(LVLMs)은 시각적 이해와 생성 모델링을 통합하여 정확한 콘텐츠 이해와 유연한 편집을 가능하게 합니다. 그러나 기존 접근법은 "무엇을 볼지"와 "어떻게 편집할지"를 별도로 처리합니다. 이를 해결하기 위해, 우리는 FOCUS라는 통합 LVLM을 제안합니다. FOCUS는 세분화 인식과 객체 중심 생성 기능을 통합한 종단 간 프레임워크로, 이중 브랜치 시각 인코더를 사용하여 전반적인 의미와 세부 정보를 동시에 포착합니다. 또한, MoVQGAN 기반 시각 토크나이저를 활용하여 생성 품질을 향상시킵니다. 우리는 세분화 마스크를 공간 조건 프롬프트로 사용하여 정확하고 제어 가능한 이미지 편집을 가능하게 하는 점진적 다단계 학습 파이프라인을 제안합니다. 이 전략은 시각 인코딩, 세분화, 생성 모듈을 정렬하여 세분화 인식과 세부 시각 합성을 효과적으로 연결합니다. 다양한 실험 결과, FOCUS는 시각적 인식과 생성 능력을 공동 최적화하여 강력한 성능을 발휘함을 보여줍니다.

## 🎯 주요 포인트

- 1. FOCUS는 시각적 이해와 생성 모델링을 통합하여 정확한 콘텐츠 이해와 유연한 편집을 가능하게 하는 통합된 대형 비전 언어 모델입니다.
- 2. FOCUS는 글로벌 의미 맥락과 세밀한 공간 세부 사항을 동시에 포착하는 이중 분기 시각 인코더를 사용합니다.
- 3. MoVQGAN 기반의 시각 토크나이저를 활용하여 생성 품질을 향상시키는 이산 시각 토큰을 생성합니다.
- 4. 점진적인 다단계 학습 파이프라인을 통해 세분화 마스크를 최적화하고 공간 조건 프롬프트로 사용하여 정확하고 제어 가능한 이미지 편집을 가능하게 합니다.
- 5. FOCUS는 시각적 인식과 생성 기능을 공동으로 최적화하여 다중 모달 이해, 참조 세분화 정확도, 제어 가능한 이미지 생성 등에서 강력한 성능을 발휘합니다.


---

*Generated on 2025-09-24 05:25:17*