---
keywords:
  - Neural Collapse
  - Flatness
  - Grokking
  - Generalization
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17738
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:56:17.626563",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Collapse",
    "Flatness",
    "Grokking",
    "Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Collapse": 0.78,
    "Flatness": 0.77,
    "Grokking": 0.75,
    "Generalization": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Collapse",
        "canonical": "Neural Collapse",
        "aliases": [
          "Class-wise Clustered Representations"
        ],
        "category": "unique_technical",
        "rationale": "Neural Collapse is a unique phenomenon observed in deep networks that is crucial for understanding generalization processes.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Flatness",
        "canonical": "Flatness",
        "aliases": [
          "Loss Landscape Flatness"
        ],
        "category": "unique_technical",
        "rationale": "Flatness is a key geometric property linked to generalization, offering a distinct perspective on model performance.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Grokking",
        "canonical": "Grokking",
        "aliases": [
          "Memorization Preceding Generalization"
        ],
        "category": "unique_technical",
        "rationale": "Grokking provides a unique training regime to study the temporal separation of memorization and generalization.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "Generalization",
        "canonical": "Generalization",
        "aliases": [
          "Model Generalization"
        ],
        "category": "broad_technical",
        "rationale": "Generalization is a fundamental concept in machine learning, central to understanding model performance.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Training Dynamics",
      "Empirical Co-occurrence"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Collapse",
      "resolved_canonical": "Neural Collapse",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Flatness",
      "resolved_canonical": "Flatness",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Grokking",
      "resolved_canonical": "Grokking",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Generalization",
      "resolved_canonical": "Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17738.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17738](https://arxiv.org/abs/2509.17738)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Navigate Beyond Shortcuts_ Debiased Learning through the Lens of Neural Collapse_20250922|Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse]] (81.2% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (81.1% similar)
- [[2025-09-23/A Closer Look at Model Collapse_ From a Generalization-to-Memorization Perspective_20250923|A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective]] (80.1% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (78.1% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (77.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Generalization|Generalization]]
**âš¡ Unique Technical**: [[keywords/Neural Collapse|Neural Collapse]], [[keywords/Flatness|Flatness]], [[keywords/Grokking|Grokking]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17738v1 Announce Type: new 
Abstract: Neural collapse, i.e., the emergence of highly symmetric, class-wise clustered representations, is frequently observed in deep networks and is often assumed to reflect or enable generalization. In parallel, flatness of the loss landscape has been theoretically and empirically linked to generalization. Yet, the causal role of either phenomenon remains unclear: Are they prerequisites for generalization, or merely by-products of training dynamics? We disentangle these questions using grokking, a training regime in which memorization precedes generalization, allowing us to temporally separate generalization from training dynamics and we find that while both neural collapse and relative flatness emerge near the onset of generalization, only flatness consistently predicts it. Models encouraged to collapse or prevented from collapsing generalize equally well, whereas models regularized away from flat solutions exhibit delayed generalization. Furthermore, we show theoretically that neural collapse implies relative flatness under classical assumptions, explaining their empirical co-occurrence. Our results support the view that relative flatness is a potentially necessary and more fundamental property for generalization, and demonstrate how grokking can serve as a powerful probe for isolating its geometric underpinnings.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹ ê²½ ë¶•ê´´ì™€ ì†ì‹¤ ì§€í˜•ì˜ í‰íƒ„í•¨ì´ ì¼ë°˜í™”ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” 'grokking'ì´ë¼ëŠ” í›ˆë ¨ ì²´ì œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™”ì™€ í›ˆë ¨ ë™ë ¥ì„ ì‹œê°„ì ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì‹ ê²½ ë¶•ê´´ì™€ í‰íƒ„í•¨ ëª¨ë‘ ì¼ë°˜í™” ì‹œì‘ ì‹œì ì— ë‚˜íƒ€ë‚˜ì§€ë§Œ, í‰íƒ„í•¨ë§Œì´ ì¼ê´€ë˜ê²Œ ì¼ë°˜í™”ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ë¶•ê´´ë¥¼ ìœ ë„í•˜ê±°ë‚˜ ë°©ì§€í•œ ëª¨ë¸ì€ ì¼ë°˜í™”ì— ì°¨ì´ê°€ ì—†ì—ˆìœ¼ë‚˜, í‰íƒ„í•œ í•´ë¥¼ í”¼í•˜ë„ë¡ ì •ê·œí™”ëœ ëª¨ë¸ì€ ì¼ë°˜í™”ê°€ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ ì‹ ê²½ ë¶•ê´´ê°€ í‰íƒ„í•¨ì„ ì•”ì‹œí•¨ì„ ë³´ì´ë©°, í‰íƒ„í•¨ì´ ì¼ë°˜í™”ì— ë” ê·¼ë³¸ì ì¸ ì†ì„±ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹ ê²½ ë¶•ê´´ì™€ ì†ì‹¤ ì§€í˜•ì˜ í‰íƒ„í•¨ì€ ì¼ë°˜í™”ì™€ ê´€ë ¨ì´ ìˆì§€ë§Œ, ê·¸ ì¸ê³¼ì  ì—­í• ì€ ë¶ˆë¶„ëª…í•˜ë‹¤.
- 2. grokking í›ˆë ¨ ì²´ì œë¥¼ í†µí•´ ì¼ë°˜í™”ì™€ í›ˆë ¨ ì—­í•™ì„ ì‹œê°„ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì—°êµ¬ë¥¼ ì§„í–‰í–ˆë‹¤.
- 3. ì‹ ê²½ ë¶•ê´´ì™€ í‰íƒ„í•¨ì€ ì¼ë°˜í™” ì‹œì‘ ì‹œì ì— ë‚˜íƒ€ë‚˜ì§€ë§Œ, í‰íƒ„í•¨ë§Œì´ ì¼ê´€ë˜ê²Œ ì¼ë°˜í™”ë¥¼ ì˜ˆì¸¡í•œë‹¤.
- 4. ì‹ ê²½ ë¶•ê´´ëŠ” ìƒëŒ€ì  í‰íƒ„í•¨ì„ ì•”ì‹œí•˜ë©°, ì´ëŠ” ì´ë“¤ì˜ ê²½í—˜ì  ë™ì‹œ ë°œìƒì„ ì„¤ëª…í•œë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ìƒëŒ€ì  í‰íƒ„í•¨ì´ ì¼ë°˜í™”ë¥¼ ìœ„í•œ ì ì¬ì ìœ¼ë¡œ í•„ìˆ˜ì ì´ê³  ê·¼ë³¸ì ì¸ ì†ì„±ì„ì„ ì§€ì§€í•œë‹¤.


---

*Generated on 2025-09-24 01:56:17*