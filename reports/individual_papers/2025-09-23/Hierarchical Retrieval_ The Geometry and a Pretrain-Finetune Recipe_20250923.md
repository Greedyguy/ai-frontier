---
keywords:
  - Hierarchical Retrieval
  - Dual Encoder
  - Pretrain-Finetune Strategy
  - WordNet
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16411
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:08:14.932959",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hierarchical Retrieval",
    "Dual Encoder",
    "Pretrain-Finetune Strategy",
    "WordNet"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hierarchical Retrieval": 0.8,
    "Dual Encoder": 0.7,
    "Pretrain-Finetune Strategy": 0.78,
    "WordNet": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Hierarchical Retrieval",
        "canonical": "Hierarchical Retrieval",
        "aliases": [
          "HR"
        ],
        "category": "unique_technical",
        "rationale": "Hierarchical Retrieval is a central concept in the paper, focusing on document retrieval within a hierarchy, which is not covered by existing canonical terms.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dual Encoder",
        "canonical": "Dual Encoder",
        "aliases": [
          "DE"
        ],
        "category": "specific_connectable",
        "rationale": "Dual Encoder models are pivotal in the paper's methodology for embedding queries and documents, offering strong connectivity to retrieval methods.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      },
      {
        "surface": "Pretrain-Finetune Recipe",
        "canonical": "Pretrain-Finetune Strategy",
        "aliases": [
          "Pretrain-Finetune"
        ],
        "category": "specific_connectable",
        "rationale": "The pretrain-finetune approach is a key innovation in the paper, enhancing retrieval performance and linking to broader training strategies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "WordNet",
        "canonical": "WordNet",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "WordNet is used as a realistic hierarchy for experiments, providing a strong link to lexical databases and semantic hierarchies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "embedding space",
      "retrieval accuracy",
      "document set"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Hierarchical Retrieval",
      "resolved_canonical": "Hierarchical Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dual Encoder",
      "resolved_canonical": "Dual Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Pretrain-Finetune Recipe",
      "resolved_canonical": "Pretrain-Finetune Strategy",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "WordNet",
      "resolved_canonical": "WordNet",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16411.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16411](https://arxiv.org/abs/2509.16411)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Federated Learning with Ad-hoc Adapter Insertions_ The Case of Soft-Embeddings for Training Classifier-as-Retriever_20250923|Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever]] (80.6% similar)
- [[2025-09-22/Database-Augmented Query Representation for Information Retrieval_20250922|Database-Augmented Query Representation for Information Retrieval]] (80.5% similar)
- [[2025-09-23/MindRef_ Mimicking Human Memory for Hierarchical Reference Retrieval with Fine-Grained Location Awareness_20250923|MindRef: Mimicking Human Memory for Hierarchical Reference Retrieval with Fine-Grained Location Awareness]] (80.4% similar)
- [[2025-09-23/LightRetriever_ A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference_20250923|LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference]] (79.5% similar)
- [[2025-09-23/DETACH_ Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts_20250923|DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts]] (79.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Dual Encoder|Dual Encoder]], [[keywords/Pretrain-Finetune Strategy|Pretrain-Finetune Strategy]], [[keywords/WordNet|WordNet]]
**âš¡ Unique Technical**: [[keywords/Hierarchical Retrieval|Hierarchical Retrieval]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16411v1 Announce Type: cross 
Abstract: Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their simplicity and scalability. However, the Euclidean geometry of the embedding space limits the expressive power of DEs, which may compromise their quality. This paper investigates such limitations in the context of hierarchical retrieval (HR), where the document set has a hierarchical structure and the matching documents for a query are all of its ancestors. We first prove that DEs are feasible for HR as long as the embedding dimension is linear in the depth of the hierarchy and logarithmic in the number of documents. Then we study the problem of learning such embeddings in a standard retrieval setup where DEs are trained on samples of matching query and document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, we introduce a pretrain-finetune recipe that significantly improves long-distance retrieval without sacrificing performance on closer documents. We experiment on a realistic hierarchy from WordNet for retrieving documents at various levels of abstraction, and show that pretrain-finetune boosts the recall on long-distance pairs from 19% to 76%. Finally, we demonstrate that our method improves retrieval of relevant products on a shopping queries dataset.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì •ë³´ ê²€ìƒ‰ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë“€ì–¼ ì¸ì½”ë”(DE) ëª¨ë¸ì˜ í•œê³„ë¥¼ ê³„ì¸µì  ê²€ìƒ‰(HR) ë§¥ë½ì—ì„œ ì¡°ì‚¬í•©ë‹ˆë‹¤. DE ëª¨ë¸ì€ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¥¼ ìœ ì‚¬í•œ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ì§€ë§Œ, ì„ë² ë”© ê³µê°„ì˜ ìœ í´ë¦¬ë“œ ê¸°í•˜í•™ì´ í‘œí˜„ë ¥ì„ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” DEê°€ ê³„ì¸µì˜ ê¹Šì´ì— ì„ í˜•ì ì´ê³  ë¬¸ì„œ ìˆ˜ì— ë¡œê·¸ì ì¼ ë•Œ HRì— ì í•©í•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ê³„ì¸µ êµ¬ì¡°ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ ë¬¸ì„œì˜ ê²€ìƒ‰ ì •í™•ë„ê°€ ë–¨ì–´ì§€ëŠ” í˜„ìƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ì „ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì„ ë„ì…í•˜ì—¬ ì¥ê±°ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. WordNetì˜ ê³„ì¸µ êµ¬ì¡° ì‹¤í—˜ì—ì„œ ì¥ê±°ë¦¬ ìŒì˜ ì¬í˜„ìœ¨ì´ 19%ì—ì„œ 76%ë¡œ ì¦ê°€í–ˆìœ¼ë©°, ì‡¼í•‘ ì¿¼ë¦¬ ë°ì´í„°ì…‹ì—ì„œë„ ê´€ë ¨ ì œí’ˆ ê²€ìƒ‰ì´ ê°œì„ ë¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ì¤‘ ì¸ì½”ë” ëª¨ë¸(DE)ì€ ì •ë³´ ê²€ìƒ‰ì—ì„œ ê°„ë‹¨í•˜ê³  í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ì§€ë§Œ, ì„ë² ë”© ê³µê°„ì˜ ìœ í´ë¦¬ë“œ ê¸°í•˜í•™ì´ í‘œí˜„ë ¥ì„ ì œí•œí•˜ì—¬ í’ˆì§ˆì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.
- 2. ë¬¸ì„œ ì§‘í•©ì´ ê³„ì¸µ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ê³„ì¸µì  ê²€ìƒ‰(HR)ì—ì„œ DEì˜ í•œê³„ë¥¼ ì¡°ì‚¬í•˜ê³ , ì„ë² ë”© ì°¨ì›ì´ ê³„ì¸µì˜ ê¹Šì´ì— ì„ í˜•ì´ê³  ë¬¸ì„œ ìˆ˜ì— ëŒ€í•´ ë¡œê·¸í˜•ì¼ ë•Œ DEê°€ HRì— ì í•©í•˜ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í–ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, ê³„ì¸µ êµ¬ì¡°ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ ë¬¸ì„œì— ëŒ€í•œ ê²€ìƒ‰ ì •í™•ë„ê°€ ì €í•˜ë˜ëŠ” 'ì¥ê±°ë¦¬ ì†ì‹¤' í˜„ìƒì´ ë‚˜íƒ€ë‚¬ë‹¤.
- 4. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì‚¬ì „ í•™ìŠµ-ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì„ ë„ì…í•˜ì—¬ ê°€ê¹Œìš´ ë¬¸ì„œì˜ ì„±ëŠ¥ì„ í¬ìƒí•˜ì§€ ì•Šìœ¼ë©´ì„œ ì¥ê±°ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ë‹¤.
- 5. WordNetì˜ í˜„ì‹¤ì ì¸ ê³„ì¸µ êµ¬ì¡°ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ì‚¬ì „ í•™ìŠµ-ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì´ ì¥ê±°ë¦¬ ìŒì— ëŒ€í•œ ì¬í˜„ìœ¨ì„ 19%ì—ì„œ 76%ë¡œ ì¦ê°€ì‹œì¼°ë‹¤.


---

*Generated on 2025-09-24 02:08:14*