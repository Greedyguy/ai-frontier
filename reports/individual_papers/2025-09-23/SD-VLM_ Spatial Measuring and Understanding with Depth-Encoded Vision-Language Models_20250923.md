---
keywords:
  - Vision-Language Model
  - SD-VLM
  - Depth Positional Encoding
  - MSMU Dataset
  - Spatial Generalization
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17664
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:05:19.825959",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "SD-VLM",
    "Depth Positional Encoding",
    "MSMU Dataset",
    "Spatial Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "SD-VLM": 0.78,
    "Depth Positional Encoding": 0.82,
    "MSMU Dataset": 0.79,
    "Spatial Generalization": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision language models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's theme and connect to the trending concept of multimodal learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "SD-VLM",
        "canonical": "SD-VLM",
        "aliases": [
          "Spatial Depth Vision-Language Model"
        ],
        "category": "unique_technical",
        "rationale": "SD-VLM is a novel framework introduced in the paper, representing a unique contribution to spatial understanding in VLMs.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Depth positional encoding",
        "canonical": "Depth Positional Encoding",
        "aliases": [
          "Depth Encoding"
        ],
        "category": "specific_connectable",
        "rationale": "Depth positional encoding is a key technique that enhances spatial awareness, linking to concepts in spatial and 3D modeling.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "MSMU dataset",
        "canonical": "MSMU Dataset",
        "aliases": [
          "Massive Spatial Measuring and Understanding Dataset"
        ],
        "category": "unique_technical",
        "rationale": "The MSMU dataset is a significant contribution that supports spatial tasks, making it a unique technical resource.",
        "novelty_score": 0.87,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "Spatial generalization",
        "canonical": "Spatial Generalization",
        "aliases": [
          "Spatial Understanding"
        ],
        "category": "specific_connectable",
        "rationale": "Spatial generalization is crucial for understanding 3D relationships, linking to broader spatial reasoning concepts.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "2D images",
      "3D spatial relationships",
      "state-of-the-art performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision language models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "SD-VLM",
      "resolved_canonical": "SD-VLM",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Depth positional encoding",
      "resolved_canonical": "Depth Positional Encoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "MSMU dataset",
      "resolved_canonical": "MSMU Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.87,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Spatial generalization",
      "resolved_canonical": "Spatial Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17664.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17664](https://arxiv.org/abs/2509.17664)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation_20250922|Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation]] (89.1% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (87.6% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (86.8% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (85.6% similar)
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Depth Positional Encoding|Depth Positional Encoding]], [[keywords/Spatial Generalization|Spatial Generalization]]
**âš¡ Unique Technical**: [[keywords/SD-VLM|SD-VLM]], [[keywords/MSMU Dataset|MSMU Dataset]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17664v1 Announce Type: cross 
Abstract: While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under-explored, due to the deficiency of 2D images' spatial representation ability. In this paper, we analyze the problem hindering VLMs' spatial understanding abilities and propose SD-VLM, a novel framework that significantly enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs' spatial awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples. We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. Code and models are released at https://github.com/cpystan/SD-VLM.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹œê° ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ 3D ê³µê°„ ê´€ê³„ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ SD-VLMì´ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë‘ ê°€ì§€ë¡œ, ì²«ì§¸, ì •ë°€í•œ ê³µê°„ ì£¼ì„ì„ í¬í•¨í•œ Massive Spatial Measuring and Understanding (MSMU) ë°ì´í„°ì…‹ì„ ì œì•ˆí•˜ê³ , ë‘˜ì§¸, VLMsì˜ ê³µê°„ ì¸ì‹ì„ ê°•í™”í•˜ëŠ” ê°„ë‹¨í•œ ê¹Šì´ ìœ„ì¹˜ ì¸ì½”ë”© ë°©ë²•ì„ ë„ì…í•œ ê²ƒì…ë‹ˆë‹¤. SD-VLMì€ MSMU-Benchì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¤ë¥¸ ê³µê°„ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ë›°ì–´ë‚œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SD-VLMì€ MSMU-Benchì—ì„œ GPT-4oì™€ Intern-VL3-78Bë¥¼ ê°ê° 26.91%ì™€ 25.56% ì´ˆê³¼í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SD-VLMì€ VLMsì˜ ê³µê°„ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ MSMU ë°ì´í„°ì…‹ê³¼ ê¹Šì´ ìœ„ì¹˜ ì¸ì½”ë”© ë°©ë²•ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 2. MSMU ë°ì´í„°ì…‹ì€ 70ë§Œ ê°œì˜ QA ìŒ, 250ë§Œ ê°œì˜ ë¬¼ë¦¬ì  ìˆ˜ì¹˜ ì£¼ì„, 1ë§Œ ê°œì˜ ì‚¬ê³  ê³¼ì • ì¦ê°• ìƒ˜í”Œì„ í¬í•¨í•˜ì—¬ ë°©ëŒ€í•œ ì–‘ì˜ ì •ëŸ‰ì  ê³µê°„ ì‘ì—…ì„ ë‹¤ë£¹ë‹ˆë‹¤.
- 3. SD-VLMì€ ì œì•ˆëœ MSMU-Benchì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, Q-Spatial ë° SpatialRGPT-Benchë¥¼ í¬í•¨í•œ ë‹¤ë¥¸ ê³µê°„ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ê³µê°„ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, SD-VLMì€ MSMU-Benchì—ì„œ GPT-4oì™€ Intern-VL3-78Bë¥¼ ê°ê° 26.91%ì™€ 25.56% ì´ˆê³¼í•˜ì—¬ ì„±ëŠ¥ì„ ë°œíœ˜í–ˆìŠµë‹ˆë‹¤.
- 5. SD-VLMì˜ ì½”ë“œì™€ ëª¨ë¸ì€ https://github.com/cpystan/SD-VLMì—ì„œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:05:19*