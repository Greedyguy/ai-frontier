---
keywords:
  - Large Language Model
  - Quantization
  - Walsh-Hadamard Transform
  - Fourier Transform
  - Parameter-Efficient Fine-Tuning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17428
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:26:03.089951",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Quantization",
    "Walsh-Hadamard Transform",
    "Fourier Transform",
    "Parameter-Efficient Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Quantization": 0.8,
    "Walsh-Hadamard Transform": 0.78,
    "Fourier Transform": 0.75,
    "Parameter-Efficient Fine-Tuning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on quantization and fine-tuning, linking to broader discussions in NLP and AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Quantization",
        "canonical": "Quantization",
        "aliases": [
          "Quantized Models"
        ],
        "category": "unique_technical",
        "rationale": "Quantization is a key technical focus of the paper, crucial for efficient model deployment and fine-tuning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Walsh-Hadamard Transform",
        "canonical": "Walsh-Hadamard Transform",
        "aliases": [
          "WHT"
        ],
        "category": "unique_technical",
        "rationale": "The Walsh-Hadamard Transform is a novel approach proposed in the paper, enhancing the representational capacity of quantized models.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Fourier-related Transform",
        "canonical": "Fourier Transform",
        "aliases": [
          "FT-based Adapters"
        ],
        "category": "specific_connectable",
        "rationale": "Fourier-related Transforms are discussed as a comparative method, linking to broader mathematical techniques in signal processing.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Parameter-Efficient Fine-Tuning",
        "canonical": "Parameter-Efficient Fine-Tuning",
        "aliases": [
          "PEFT"
        ],
        "category": "unique_technical",
        "rationale": "PEFT is a central theme in the paper, addressing the need for efficient model adaptation with minimal parameter updates.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Quantization",
      "resolved_canonical": "Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Walsh-Hadamard Transform",
      "resolved_canonical": "Walsh-Hadamard Transform",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Fourier-related Transform",
      "resolved_canonical": "Fourier Transform",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Parameter-Efficient Fine-Tuning",
      "resolved_canonical": "Parameter-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17428.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17428](https://arxiv.org/abs/2509.17428)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/How Can Quantum Deep Learning Improve Large Language Models?_20250923|How Can Quantum Deep Learning Improve Large Language Models?]] (88.0% similar)
- [[2025-09-23/Does quantization affect models' performance on long-context tasks?_20250923|Does quantization affect models' performance on long-context tasks?]] (87.3% similar)
- [[2025-09-23/GuidedQuant_ Large Language Model Quantization via Exploiting End Loss Guidance_20250923|GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance]] (87.1% similar)
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (87.0% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (86.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Fourier Transform|Fourier Transform]]
**âš¡ Unique Technical**: [[keywords/Quantization|Quantization]], [[keywords/Walsh-Hadamard Transform|Walsh-Hadamard Transform]], [[keywords/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17428v1 Announce Type: new 
Abstract: The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì ì¸ ë°°í¬ë¥¼ ìœ„í•œ ì–‘ìí™” ë° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT)ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì €ìë“¤ì€ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì¤„ì´ê³  ëª¨ë¸ì˜ ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•´ Walsh-Hadamard Transform(WHT)ì„ í™œìš©í•œ QWHA ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì €ì°¨ì› ì ì‘ ë°©ì‹ì€ í‘œí˜„ë ¥ì´ ì œí•œì ì´ë©°, ìµœê·¼ì˜ í‘¸ë¦¬ì— ë³€í™˜ ê¸°ë°˜ ì–´ëŒ‘í„°ëŠ” ì–‘ìí™” ëª¨ë¸ì— ì§ì ‘ í†µí•© ì‹œ ì˜¤ë¥˜ ê°ì†Œì— ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. QWHAëŠ” ì ì‘ì  íŒŒë¼ë¯¸í„° ì„ íƒ ë° ê°’ ì •ì œë¥¼ í¬í•¨í•œ ìƒˆë¡œìš´ ì–´ëŒ‘í„° ì´ˆê¸°í™” ë°©ì‹ì„ í†µí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, QWHAëŠ” ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™” ì •í™•ë„ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, í›ˆë ¨ ì†ë„ë„ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤. ì½”ë“œ ë§í¬ëŠ” https://github.com/vantaa89/qwhaì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì  ë°°í¬ë¥¼ ìœ„í•´ ì–‘ìí™”ì™€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT)ì´ ì¤‘ìš”í•˜ë‹¤.
- 2. ê¸°ì¡´ì˜ ì €ë­í¬ ì ì‘ ë°©ì‹ì€ í‘œí˜„ë ¥ì˜ í•œê³„ë¡œ ì¸í•´ ì–‘ìí™” ì˜¤ë¥˜ ê°ì†Œì— ë¹„íš¨ìœ¨ì ì´ë‹¤.
- 3. QWHAëŠ” Walsh-Hadamard ë³€í™˜(WHT)ì„ ì‚¬ìš©í•˜ì—¬ FT ê¸°ë°˜ ì–´ëŒ‘í„°ë¥¼ ì–‘ìí™”ëœ ëª¨ë¸ì— í†µí•©í•˜ì—¬ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•œë‹¤.
- 4. QWHAëŠ” ì ì‘í˜• íŒŒë¼ë¯¸í„° ì„ íƒê³¼ ê°’ ì •ì œë¥¼ í¬í•¨í•œ ìƒˆë¡œìš´ ì–´ëŒ‘í„° ì´ˆê¸°í™” ë°©ì‹ì„ í†µí•´ ì—°ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¤„ì¸ë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, QWHAëŠ” ì €ë¹„íŠ¸ ì–‘ìí™” ì •í™•ë„ì—ì„œ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ê¸°ì¡´ FT ê¸°ë°˜ ì–´ëŒ‘í„° ëŒ€ë¹„ í›ˆë ¨ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-24 03:26:03*