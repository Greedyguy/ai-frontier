---
keywords:
  - Large Language Model
  - Cross-Lingual Transfer
  - Low-Resource Languages
  - Reversible Transliteration
  - Huffman Coding
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17493
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:29:22.060965",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Lingual Transfer",
    "Low-Resource Languages",
    "Reversible Transliteration",
    "Huffman Coding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cross-Lingual Transfer": 0.9,
    "Low-Resource Languages": 0.8,
    "Reversible Transliteration": 0.85,
    "Huffman Coding": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader concept of language models, relevant for cross-lingual tasks.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-Lingual Transfer",
        "canonical": "Cross-Lingual Transfer",
        "aliases": [
          "Cross-Language Transfer"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on transferring capabilities across languages.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Low-Resource Languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "Under-Resourced Languages"
        ],
        "category": "unique_technical",
        "rationale": "Key to understanding the challenges and solutions proposed in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reversible Transliteration",
        "canonical": "Reversible Transliteration",
        "aliases": [
          "Bidirectional Transliteration"
        ],
        "category": "unique_technical",
        "rationale": "Describes the novel approach for handling script conversion in low-resource languages.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Huffman Coding",
        "canonical": "Huffman Coding",
        "aliases": [
          "Huffman Compression"
        ],
        "category": "specific_connectable",
        "rationale": "Integral to the paper's transliteration framework, offering compression benefits.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "framework",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-Lingual Transfer",
      "resolved_canonical": "Cross-Lingual Transfer",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Low-Resource Languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reversible Transliteration",
      "resolved_canonical": "Reversible Transliteration",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Huffman Coding",
      "resolved_canonical": "Huffman Coding",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17493.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17493](https://arxiv.org/abs/2509.17493)

## 🔗 유사한 논문
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (84.8% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (84.7% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (84.5% similar)
- [[2025-09-23/Leveraging Multilingual Training for Authorship Representation_ Enhancing Generalization across Languages and Domains_20250923|Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains]] (83.6% similar)
- [[2025-09-19/ReCoVeR the Target Language_ Language Steering without Sacrificing Task Performance_20250919|ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Huffman Coding|Huffman Coding]]
**⚡ Unique Technical**: [[keywords/Cross-Lingual Transfer|Cross-Lingual Transfer]], [[keywords/Low-Resource Languages|Low-Resource Languages]], [[keywords/Reversible Transliteration|Reversible Transliteration]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17493v1 Announce Type: new 
Abstract: As large language models (LLMs) are trained on increasingly diverse and extensive multilingual corpora, they demonstrate cross-lingual transfer capabilities. However, these capabilities often fail to effectively extend to low-resource languages, particularly those utilizing non-Latin scripts. While transliterating low-resource languages into Latin script presents a natural solution, there currently lacks a comprehensive framework for integrating transliteration into LLMs training and deployment. Taking a pragmatic approach, this paper innovatively combines character transliteration with Huffman coding to design a complete transliteration framework. Our proposed framework offers the following advantages: 1) Compression: Reduces storage requirements for low-resource language content, achieving up to 50% reduction in file size and 50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless conversion from transliterated text back to the source language. 3) Efficiency: Eliminates the need for vocabulary expansion for low-resource languages, improving training and inference efficiency. 4) Scalability: The framework can be extended to other low-resource languages. We validate the effectiveness of our framework across multiple downstream tasks, including text classification, machine reading comprehension, and machine translation. Experimental results demonstrate that our method significantly enhances the model's capability to process low-resource languages while maintaining performance on high-resource languages. Our data and code are publicly available at https://github.com/CMLI-NLP/HuffmanTranslit.

## 📝 요약

이 논문은 대형 언어 모델(LLM)이 다국어 코퍼스를 학습하면서도 저자원 언어, 특히 비라틴 문자를 사용하는 언어에 대한 확장성이 부족한 문제를 해결하고자 합니다. 이를 위해 저자들은 문자 음역과 허프만 코딩을 결합한 음역 프레임워크를 제안합니다. 이 프레임워크는 저장 공간을 최대 50% 줄이고, 토큰 수를 50-80% 감소시키며, 100% 무손실 변환을 보장합니다. 또한, 저자원 언어의 어휘 확장을 필요로 하지 않아 효율성을 높이고, 다른 저자원 언어로의 확장성도 제공합니다. 다양한 다운스트림 작업에서 이 프레임워크의 효과를 검증했으며, 저자원 언어 처리 능력을 향상시키면서도 고자원 언어의 성능을 유지함을 실험적으로 입증했습니다. 데이터와 코드는 공개되어 있습니다.

## 🎯 주요 포인트

- 1. 대규모 언어 모델(LLMs)이 다양한 다국어 말뭉치로 훈련되면서 언어 간 전이 능력을 보이지만, 저자원 언어, 특히 비라틴 문자 언어에는 효과적으로 확장되지 못한다.
- 2. 본 논문은 문자 음역과 허프만 코딩을 결합하여 완전한 음역 프레임워크를 설계하고, 이를 통해 최대 50%의 파일 크기 감소와 50-80%의 토큰 수 감소를 달성한다.
- 3. 제안된 프레임워크는 100% 손실 없는 음역 텍스트의 원본 언어로의 변환을 보장하고, 저자원 언어에 대한 어휘 확장을 필요로 하지 않아 훈련 및 추론 효율성을 향상시킨다.
- 4. 이 프레임워크는 다른 저자원 언어로 확장 가능하며, 텍스트 분류, 기계 독해, 기계 번역 등 여러 다운스트림 작업에서 효과가 검증되었다.
- 5. 실험 결과, 제안된 방법이 저자원 언어 처리 능력을 크게 향상시키면서도 고자원 언어에 대한 성능을 유지함을 보여준다.


---

*Generated on 2025-09-24 03:29:22*