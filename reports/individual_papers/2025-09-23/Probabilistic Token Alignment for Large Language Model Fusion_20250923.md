---
keywords:
  - Large Language Model
  - Probabilistic Token Alignment
  - Optimal Transport
  - Distribution Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17276
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:49:55.108228",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Probabilistic Token Alignment",
    "Optimal Transport",
    "Distribution Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Probabilistic Token Alignment": 0.8,
    "Optimal Transport": 0.78,
    "Distribution Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Key concept in the paper, linking to existing work on language models.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Probabilistic Token Alignment",
        "canonical": "Probabilistic Token Alignment",
        "aliases": [
          "PTA",
          "PTA-LLM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for token alignment in model fusion.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Optimal Transport",
        "canonical": "Optimal Transport",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Connects to mathematical methods used in distribution learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Distribution Learning",
        "canonical": "Distribution Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Central to the proposed method's theoretical foundation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "model fusion",
      "vocabulary alignment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Probabilistic Token Alignment",
      "resolved_canonical": "Probabilistic Token Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Optimal Transport",
      "resolved_canonical": "Optimal Transport",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Distribution Learning",
      "resolved_canonical": "Distribution Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Probabilistic Token Alignment for Large Language Model Fusion

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17276.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17276](https://arxiv.org/abs/2509.17276)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (88.2% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (85.0% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (84.8% similar)
- [[2025-09-23/LifeAlign_ Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization_20250923|LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization]] (84.3% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Optimal Transport|Optimal Transport]], [[keywords/Distribution Learning|Distribution Learning]]
**âš¡ Unique Technical**: [[keywords/Probabilistic Token Alignment|Probabilistic Token Alignment]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17276v1 Announce Type: cross 
Abstract: Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities. Our code is avaliable at https://runjia.tech/neurips_pta-llm/.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ëŒ€ì‹ , ê¸°ì¡´ì˜ ì‚¬ì „ í›ˆë ¨ëœ LLMì„ ê²°í•©í•˜ì—¬ ë” ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ ê²°í•©ì˜ ë¬¸ì œì ì¸ ìˆ˜ë™ì  ì–´íœ˜ ì •ë ¬ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ í™•ë¥ ì  í† í° ì •ë ¬(PTA-LLM) ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ìµœì  ìˆ˜ì†¡ ë¬¸ì œë¥¼ í™œìš©í•˜ì—¬ ë” ì¼ê´€ëœ ëª¨ë¸ ê²°í•©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. PTA-LLMì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ë¶„í¬ ê´€ì ì—ì„œ í•´ì„ ê°€ëŠ¥í•œ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ì—¬ëŸ¬ í‰ê°€ì—ì„œ ëª©í‘œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ê²ƒì€ ë…íŠ¹í•œ ê¸°ëŠ¥ê³¼ ê°•ì ì„ ê°€ì§„ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆì§€ë§Œ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì¤‘ë³µëœ ê¸°ëŠ¥ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì‚¬ì „ í›ˆë ¨ëœ LLMì„ ê²°í•©í•˜ì—¬ ë” ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ ë¹„ìš© íš¨ìœ¨ì ì¸ ëŒ€ì•ˆì…ë‹ˆë‹¤.
- 3. ê¸°ì¡´ ëª¨ë¸ ê²°í•©ì˜ ì£¼ìš” ë¬¸ì œëŠ” ìˆ˜ë™ìœ¼ë¡œ ì •ì˜ëœ ì–´íœ˜ ì •ë ¬ì— ì˜ì¡´í•˜ì—¬ ë‹¤ì–‘í•œ ë§¥ë½ì—ì„œ ì¼ë°˜í™”ë˜ì§€ ëª»í•˜ê³  ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
- 4. ìš°ë¦¬ëŠ” í™•ë¥ ì  í† í° ì •ë ¬ ë°©ë²•(PTA-LLM)ì„ ì œì•ˆí•˜ì—¬ ìµœì  ìˆ˜ì†¡ ë¬¸ì œë¡œ í† í° ì •ë ¬ì„ ì¬êµ¬ì„±í•˜ê³ , ë¶„í¬ ì¸ì‹ í•™ìŠµì„ í™œìš©í•˜ì—¬ ëª¨ë¸ ê²°í•©ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, í™•ë¥ ì  í† í° ì •ë ¬ì€ ì—¬ëŸ¬ ê¸°ëŠ¥ì—ì„œ ëŒ€ìƒ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:49:55*