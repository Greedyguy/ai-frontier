---
keywords:
  - Heterogeneous Reinforcement Learning
  - Group Expectation Policy Optimization
  - Decentralized Training
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.17850
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:22:41.302361",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Heterogeneous Reinforcement Learning",
    "Group Expectation Policy Optimization",
    "Decentralized Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Heterogeneous Reinforcement Learning": 0.78,
    "Group Expectation Policy Optimization": 0.8,
    "Decentralized Training": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Heterogeneous Reinforcement Learning",
        "canonical": "Heterogeneous Reinforcement Learning",
        "aliases": [
          "HeteroRL"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's contribution and represents a novel approach to RL in decentralized environments.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Group Expectation Policy Optimization",
        "canonical": "Group Expectation Policy Optimization",
        "aliases": [
          "GEPO"
        ],
        "category": "unique_technical",
        "rationale": "GEPO is a novel algorithm introduced in the paper, crucial for addressing latency issues in distributed RL.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.8
      },
      {
        "surface": "Decentralized Training",
        "canonical": "Decentralized Training",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Decentralized training is a key context for the paper's methodology, linking it to broader trends in distributed computing.",
        "novelty_score": 0.55,
        "connectivity_score": 0.77,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Heterogeneous Reinforcement Learning",
      "resolved_canonical": "Heterogeneous Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Group Expectation Policy Optimization",
      "resolved_canonical": "Group Expectation Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Decentralized Training",
      "resolved_canonical": "Decentralized Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.77,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.17850.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.17850](https://arxiv.org/abs/2508.17850)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (83.1% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (82.5% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (82.2% similar)
- [[2025-09-22/RLinf_ Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation_20250922|RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation]] (82.1% similar)
- [[2025-09-23/Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation_20250923|Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Decentralized Training|Decentralized Training]]
**âš¡ Unique Technical**: [[keywords/Heterogeneous Reinforcement Learning|Heterogeneous Reinforcement Learning]], [[keywords/Group Expectation Policy Optimization|Group Expectation Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.17850v5 Announce Type: replace-cross 
Abstract: As single-center computing approaches power constraints, decentralized training becomes essential. However, traditional Reinforcement Learning (RL) methods, crucial for enhancing large model post-training, cannot adapt to decentralized distributed training due to the tight coupling between parameter learning and rollout sampling. For this, we propose HeteroRL, a heterogeneous RL architecture that decouples these processes, enabling stable training across geographically distributed nodes connected via the Internet. The core component is Group Expectation Policy Optimization (GEPO), an asynchronous RL algorithm robust to latency caused by network delays or heterogeneity in computational resources. Our study reveals that high latency significantly increases KL divergence, leading to higher variance in importance sampling weights and training instability. GEPO mitigates this issue by using group expectation weighting to exponentially reduce the variance of importance weights, with theoretical guarantees. Experiments show that GEPO achieves superior stability, with only a 3\% performance drop from online to 1800s latency, demonstrating strong potential for decentralized RL in geographically distributed, resource-heterogeneous computing environments.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¨ì¼ ì„¼í„° ì»´í“¨íŒ…ì˜ ì „ë ¥ ì œì•½ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë¶„ì‚°í˜• í›ˆë ¨ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ì „í†µì ì¸ ê°•í™” í•™ìŠµ(RL) ë°©ë²•ì´ ë¶„ì‚°í˜• í›ˆë ¨ì— ì í•©í•˜ì§€ ì•ŠìŒì„ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ HeteroRLì´ë¼ëŠ” ì´ì§ˆì  RL ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì•„í‚¤í…ì²˜ëŠ” ë§¤ê°œë³€ìˆ˜ í•™ìŠµê³¼ ë¡¤ì•„ì›ƒ ìƒ˜í”Œë§ì„ ë¶„ë¦¬í•˜ì—¬ ì¸í„°ë„·ìœ¼ë¡œ ì—°ê²°ëœ ì§€ë¦¬ì ìœ¼ë¡œ ë¶„ì‚°ëœ ë…¸ë“œì—ì„œ ì•ˆì •ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. í•µì‹¬ ìš”ì†Œì¸ ê·¸ë£¹ ê¸°ëŒ€ ì •ì±… ìµœì í™”(GEPO)ëŠ” ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì´ë‚˜ ê³„ì‚° ìì›ì˜ ì´ì§ˆì„±ì— ê°•í•œ ë¹„ë™ê¸°ì‹ RL ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë†’ì€ ì§€ì—°ì´ KL ë°œì‚°ì„ ì¦ê°€ì‹œì¼œ ì¤‘ìš”ë„ ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜ì˜ ë¶„ì‚°ì„ ë†’ì´ê³  í›ˆë ¨ì˜ ë¶ˆì•ˆì •ì„ ì´ˆë˜í•¨ì„ ë°í˜”ìŠµë‹ˆë‹¤. GEPOëŠ” ê·¸ë£¹ ê¸°ëŒ€ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, ì´ë¡ ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì˜ ë¶„ì‚°ì„ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, GEPOëŠ” ì˜¨ë¼ì¸ì—ì„œ 1800ì´ˆ ì§€ì—°ê¹Œì§€ 3%ì˜ ì„±ëŠ¥ ì €í•˜ë§Œì„ ë³´ì´ë©°, ì§€ë¦¬ì ìœ¼ë¡œ ë¶„ì‚°ë˜ê³  ìì›ì´ ì´ì§ˆì ì¸ í™˜ê²½ì—ì„œì˜ ë¶„ì‚°í˜• RLì— ê°•ë ¥í•œ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. HeteroRLì€ ë§¤ê°œë³€ìˆ˜ í•™ìŠµê³¼ ë¡¤ì•„ì›ƒ ìƒ˜í”Œë§ì„ ë¶„ë¦¬í•˜ì—¬ ë¶„ì‚°ëœ í™˜ê²½ì—ì„œ ì•ˆì •ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì´ì¢… ê°•í™” í•™ìŠµ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.
- 2. Group Expectation Policy Optimization (GEPO)ì€ ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì´ë‚˜ ìì› ì´ì§ˆì„±ìœ¼ë¡œ ì¸í•œ ì§€ì—°ì— ê°•í•œ ë¹„ë™ê¸°ì‹ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 3. ë†’ì€ ì§€ì—°ì€ KL ë°œì‚°ì„ ì¦ê°€ì‹œì¼œ ì¤‘ìš”ë„ ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜ì˜ ë¶„ì‚°ì„ ë†’ì´ê³  í›ˆë ¨ì˜ ë¶ˆì•ˆì •ì„ ì´ˆë˜í•˜ì§€ë§Œ, GEPOëŠ” ê·¸ë£¹ ê¸°ëŒ€ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ì™„í™”í•©ë‹ˆë‹¤.
- 4. GEPOëŠ” ì´ë¡ ì  ë³´ì¥ì„ í†µí•´ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ì˜ ë¶„ì‚°ì„ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¤„ì´ë©°, ì‹¤í—˜ ê²°ê³¼ ì˜¨ë¼ì¸ì—ì„œ 1800ì´ˆ ì§€ì—°ê¹Œì§€ ì„±ëŠ¥ì´ 3%ë§Œ ê°ì†Œí•˜ëŠ” ì•ˆì •ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. GEPOëŠ” ì§€ë¦¬ì ìœ¼ë¡œ ë¶„ì‚°ë˜ê³  ìì›ì´ ì´ì§ˆì ì¸ ì»´í“¨íŒ… í™˜ê²½ì—ì„œ ë¶„ì‚° ê°•í™” í•™ìŠµì˜ ê°•ë ¥í•œ ì ì¬ë ¥ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:22:41*