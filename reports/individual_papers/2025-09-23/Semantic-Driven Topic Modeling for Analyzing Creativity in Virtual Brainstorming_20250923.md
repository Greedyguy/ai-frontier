---
keywords:
  - Transformer
  - Dimensionality Reduction
  - Clustering
  - Topic Coherence
  - Embedding-Based Topic Modeling
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16835
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:35:33.718276",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Dimensionality Reduction",
    "Clustering",
    "Topic Coherence",
    "Embedding-Based Topic Modeling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Dimensionality Reduction": 0.72,
    "Clustering": 0.78,
    "Topic Coherence": 0.7,
    "Embedding-Based Topic Modeling": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "transformer-based embeddings",
        "canonical": "Transformer",
        "aliases": [
          "Sentence-BERT",
          "BERT embeddings"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in NLP, crucial for understanding the semantic-driven approach in the paper.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "dimensionality reduction",
        "canonical": "Dimensionality Reduction",
        "aliases": [
          "UMAP"
        ],
        "category": "specific_connectable",
        "rationale": "Dimensionality reduction is key for processing high-dimensional data, facilitating better topic modeling.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.68,
        "link_intent_score": 0.72
      },
      {
        "surface": "clustering",
        "canonical": "Clustering",
        "aliases": [
          "HDBSCAN"
        ],
        "category": "specific_connectable",
        "rationale": "Clustering is essential for grouping similar ideas, enhancing the framework's ability to extract coherent themes.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "topic coherence",
        "canonical": "Topic Coherence",
        "aliases": [
          "coherence score"
        ],
        "category": "unique_technical",
        "rationale": "Topic coherence is a unique metric used to evaluate the quality of topic models, pivotal for the study's findings.",
        "novelty_score": 0.7,
        "connectivity_score": 0.67,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "embedding-based topic modeling",
        "canonical": "Embedding-Based Topic Modeling",
        "aliases": [
          "semantic-driven topic modeling"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach combining embeddings with topic modeling, central to the paper's contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "manual coding",
      "structured Zoom brainstorming sessions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "transformer-based embeddings",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "dimensionality reduction",
      "resolved_canonical": "Dimensionality Reduction",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.68,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "clustering",
      "resolved_canonical": "Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "topic coherence",
      "resolved_canonical": "Topic Coherence",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.67,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "embedding-based topic modeling",
      "resolved_canonical": "Embedding-Based Topic Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16835.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16835](https://arxiv.org/abs/2509.16835)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (80.7% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (80.0% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (79.9% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (79.8% similar)
- [[2025-09-22/MuseScorer_ Idea Originality Scoring At Scale_20250922|MuseScorer: Idea Originality Scoring At Scale]] (79.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Dimensionality Reduction|Dimensionality Reduction]], [[keywords/Clustering|Clustering]]
**âš¡ Unique Technical**: [[keywords/Topic Coherence|Topic Coherence]], [[keywords/Embedding-Based Topic Modeling|Embedding-Based Topic Modeling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16835v1 Announce Type: cross 
Abstract: Virtual brainstorming sessions have become a central component of collaborative problem solving, yet the large volume and uneven distribution of ideas often make it difficult to extract valuable insights efficiently. Manual coding of ideas is time-consuming and subjective, underscoring the need for automated approaches to support the evaluation of group creativity. In this study, we propose a semantic-driven topic modeling framework that integrates four modular components: transformer-based embeddings (Sentence-BERT), dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction with refinement. The framework captures semantic similarity at the sentence level, enabling the discovery of coherent themes from brainstorming transcripts while filtering noise and identifying outliers. We evaluate our approach on structured Zoom brainstorming sessions involving student groups tasked with improving their university. Results demonstrate that our model achieves higher topic coherence compared to established methods such as LDA, ETM, and BERTopic, with an average coherence score of 0.687 (CV), outperforming baselines by a significant margin. Beyond improved performance, the model provides interpretable insights into the depth and diversity of topics explored, supporting both convergent and divergent dimensions of group creativity. This work highlights the potential of embedding-based topic modeling for analyzing collaborative ideation and contributes an efficient and scalable framework for studying creativity in synchronous virtual meetings.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ê°€ìƒ ë¸Œë ˆì¸ìŠ¤í† ë° ì„¸ì…˜ì—ì„œ ìƒì„±ëœ ì•„ì´ë””ì–´ì˜ íš¨ìœ¨ì ì¸ í‰ê°€ë¥¼ ìœ„í•´ ìë™í™”ëœ ì£¼ì œ ëª¨ë¸ë§ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” Sentence-BERT ê¸°ë°˜ ì„ë² ë”©, UMAP ì°¨ì› ì¶•ì†Œ, HDBSCAN í´ëŸ¬ìŠ¤í„°ë§, ì£¼ì œ ì¶”ì¶œ ë° ì •ì œë¥¼ í†µí•©í•˜ì—¬ ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ í¬ì°©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¸Œë ˆì¸ìŠ¤í† ë° ê¸°ë¡ì—ì„œ ì¼ê´€ëœ ì£¼ì œë¥¼ ë°œê²¬í•˜ê³  ì¡ìŒì„ ê±¸ëŸ¬ë‚´ë©° ì´ìƒì¹˜ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í•™ ê°œì„ ì„ ì£¼ì œë¡œ í•œ í•™ìƒ ê·¸ë£¹ì˜ Zoom ì„¸ì…˜ì„ í‰ê°€í•œ ê²°ê³¼, ì œì•ˆëœ ëª¨ë¸ì€ ê¸°ì¡´ ë°©ë²•ë“¤(LDA, ETM, BERTopic)ë³´ë‹¤ ë†’ì€ ì£¼ì œ ì¼ê´€ì„± ì ìˆ˜(í‰ê·  0.687 CV)ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì£¼ì œì˜ ê¹Šì´ì™€ ë‹¤ì–‘ì„±ì— ëŒ€í•œ í•´ì„ ê°€ëŠ¥í•œ í†µì°°ì„ ì œê³µí•˜ë©°, ê·¸ë£¹ ì°½ì˜ì„±ì˜ ìˆ˜ë ´ì  ë° ë°œì‚°ì  ì°¨ì›ì„ ì§€ì›í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ë™ê¸°ì‹ ê°€ìƒ íšŒì˜ì—ì„œ í˜‘ë ¥ì  ì•„ì´ë””ì–´ ì°½ì¶œ ë¶„ì„ì„ ìœ„í•œ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°€ìƒ ë¸Œë ˆì¸ìŠ¤í† ë° ì„¸ì…˜ì—ì„œ ì•„ì´ë””ì–´ì˜ ëŒ€ëŸ‰ ë° ë¶ˆê· ë“± ë¶„í¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìë™í™”ëœ í‰ê°€ ë°©ë²•ì˜ í•„ìš”ì„±ì´ ê°•ì¡°ë©ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” Sentence-BERT, UMAP, HDBSCANì„ í†µí•©í•œ ì˜ë¯¸ ê¸°ë°˜ ì£¼ì œ ëª¨ë¸ë§ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ ë¸Œë ˆì¸ìŠ¤í† ë° ê¸°ë¡ì—ì„œ ì¼ê´€ëœ ì£¼ì œë¥¼ ë°œê²¬í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ëª¨ë¸ì€ LDA, ETM, BERTopic ë“± ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í‰ê·  ì¼ê´€ì„± ì ìˆ˜ 0.687(CV)ë¡œ ë†’ì€ ì£¼ì œ ì¼ê´€ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. ëª¨ë¸ì€ ì£¼ì œì˜ ê¹Šì´ì™€ ë‹¤ì–‘ì„±ì— ëŒ€í•œ í•´ì„ ê°€ëŠ¥í•œ í†µì°°ë ¥ì„ ì œê³µí•˜ë©°, ê·¸ë£¹ ì°½ì˜ì„±ì˜ ìˆ˜ë ´ì  ë° ë°œì‚°ì  ì°¨ì›ì„ ì§€ì›í•©ë‹ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ë™ê¸°ì‹ ê°€ìƒ íšŒì˜ì—ì„œ í˜‘ì—…ì  ì•„ì´ë””ì–´ ìƒì„± ë¶„ì„ì„ ìœ„í•œ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:35:33*