---
keywords:
  - EpiCache
  - Long Conversational Question Answering
  - Key-Value Caching
  - Large Language Model
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17396
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:24:21.716796",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "EpiCache",
    "Long Conversational Question Answering",
    "Key-Value Caching",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "EpiCache": 0.8,
    "Long Conversational Question Answering": 0.78,
    "Key-Value Caching": 0.75,
    "Large Language Model": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "EpiCache",
        "canonical": "EpiCache",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "EpiCache is a novel framework specific to the paper's context, offering unique insights into KV cache management.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Long Conversational Question Answering",
        "canonical": "Long Conversational Question Answering",
        "aliases": [
          "LongConvQA"
        ],
        "category": "unique_technical",
        "rationale": "This term describes a specific application area that the paper addresses, relevant for linking related research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Key-Value Caching",
        "canonical": "Key-Value Caching",
        "aliases": [
          "KV Caching"
        ],
        "category": "specific_connectable",
        "rationale": "Key-Value Caching is central to the paper's discussion and connects to broader research on memory management in LLMs.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are foundational to the paper's context, providing a broad technical basis for linking.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "EpiCache",
      "resolved_canonical": "EpiCache",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Long Conversational Question Answering",
      "resolved_canonical": "Long Conversational Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Key-Value Caching",
      "resolved_canonical": "Key-Value Caching",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# EpiCache: Episodic KV Cache Management for Long Conversational Question Answering

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17396.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17396](https://arxiv.org/abs/2509.17396)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (86.7% similar)
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (85.5% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (83.5% similar)
- [[2025-09-23/MPIC_ Position-Independent Multimodal Context Caching System for Efficient MLLM Serving_20250923|MPIC: Position-Independent Multimodal Context Caching System for Efficient MLLM Serving]] (83.5% similar)
- [[2025-09-23/A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue_20250923|A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue]] (83.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Key-Value Caching|Key-Value Caching]]
**âš¡ Unique Technical**: [[keywords/EpiCache|EpiCache]], [[keywords/Long Conversational Question Answering|Long Conversational Question Answering]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17396v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ìœ¼ë¡œ ê¸´ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ë©° ì¼ê´€ë˜ê³  ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ëŒ€í™” ê¸¸ì´ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ ì•¼ê¸°í•˜ëŠ” Key-Value(KV) ìºì‹±ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ KV ìºì‹œ ì••ì¶•ì´ ì—°êµ¬ë˜ê³  ìˆì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¬¸ì œì™€ ì •í™•ë„ ì €í•˜ ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê³ ì •ëœ ë©”ëª¨ë¦¬ í™˜ê²½ì—ì„œ ê¸´ ëŒ€í™”í˜• ì§ˆë¬¸ ì‘ë‹µ(LongConvQA)ì„ ìœ„í•œ EpiCacheë¼ëŠ” ìƒˆë¡œìš´ KV ìºì‹œ ê´€ë¦¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. EpiCacheëŠ” ë¸”ë¡ ë‹¨ìœ„ì˜ í”„ë¦¬í•„ê³¼ ì—í”¼ì†Œë“œë³„ KV ì••ì¶•ì„ í†µí•´ ìºì‹œ ì„±ì¥ì„ ì œí•œí•˜ê³ , ëŒ€í™” ê¸°ë¡ì„ ì¼ê´€ëœ ì—í”¼ì†Œë“œë¡œ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì£¼ì œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ê° ë ˆì´ì–´ì˜ ë¯¼ê°ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì˜ˆì‚°ì„ ì ì ˆíˆ ë¶„ë°°í•˜ëŠ” ì ì‘í˜• ë ˆì´ì–´ ì˜ˆì‚° í• ë‹¹ ì „ëµì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ì„¸ ê°€ì§€ LongConvQA ë²¤ì¹˜ë§ˆí¬ì—ì„œ EpiCacheëŠ” ê¸°ì¡´ ê¸°ì¤€ë³´ë‹¤ ìµœëŒ€ 40% ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ê³ , 4-6ë°° ì••ì¶•ì—ì„œë„ ê±°ì˜ ì™„ì „í•œ KV ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, ì§€ì—° ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€ 2.4ë°° ë° 3.5ë°° ì¤„ì—¬ ìì› ì œì•½ í•˜ì—ì„œ íš¨ìœ¨ì ì¸ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ìœ¼ë¡œ ë§¥ë½ ê¸¸ì´ê°€ í™•ì¥ë˜ì–´, ê°œì¸í™”ëœ ì‘ë‹µì„ ìœ„í•œ ê¸´ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.
- 2. KV ìºì‹œ ì••ì¶•ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì—°êµ¬ë¡œ, ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ì§€ ëª»í•˜ê³  ìˆë‹¤.
- 3. EpiCacheëŠ” ê³ ì •ëœ ë©”ëª¨ë¦¬ ì˜ˆì‚° í•˜ì—ì„œ ê¸´ ëŒ€í™”í˜• ì§ˆë¬¸ ì‘ë‹µ(LongConvQA)ì„ ìœ„í•œ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” KV ìºì‹œ ê´€ë¦¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. EpiCacheëŠ” ë¸”ë¡ ë‹¨ìœ„ì˜ í”„ë¦¬í•„ê³¼ ì—í”¼ì†Œë“œë³„ KV ì••ì¶•ì„ í†µí•´ ìºì‹œ ì„±ì¥ì„ ì œí•œí•˜ê³ , ì£¼ì œ ê´€ë ¨ ë§¥ë½ì„ ìœ ì§€í•œë‹¤.
- 5. EpiCacheëŠ” ì„¸ ê°€ì§€ LongConvQA ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 40%ì˜ ì •í™•ë„ í–¥ìƒê³¼ 4-6ë°°ì˜ ì••ì¶• í•˜ì—ì„œë„ ê±°ì˜ ì™„ì „í•œ KV ì •í™•ì„±ì„ ìœ ì§€í•˜ë©°, ì§€ì—° ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€ 2.4ë°° ë° 3.5ë°°ê¹Œì§€ ì¤„ì¸ë‹¤.


---

*Generated on 2025-09-24 03:24:21*