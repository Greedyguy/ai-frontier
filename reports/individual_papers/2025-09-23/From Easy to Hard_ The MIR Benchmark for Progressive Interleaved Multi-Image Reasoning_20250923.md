---
keywords:
  - Large Language Model
  - Multi-image Interleaved Reasoning
  - Curriculum Learning
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17040
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:41:53.446839",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multi-image Interleaved Reasoning",
    "Curriculum Learning",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Multi-image Interleaved Reasoning": 0.82,
    "Curriculum Learning": 0.8,
    "Vision-Language Model": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-modal Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "MLLMs"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus on enhancing reasoning capabilities across multiple images and text, linking it to the broader field of language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multi-image Interleaved Reasoning",
        "canonical": "Multi-image Interleaved Reasoning",
        "aliases": [
          "MIR"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel concept introduced by the paper, essential for understanding its contribution and potential connections to other multi-modal reasoning research.",
        "novelty_score": 0.92,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "Curriculum Learning",
        "canonical": "Curriculum Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The paper's use of a stage-wise curriculum learning strategy is crucial for understanding the progressive approach to training models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is key to the paper's focus on reasoning across visual and textual data, linking to recent trends in multimodal AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.83,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "method",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-modal Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multi-image Interleaved Reasoning",
      "resolved_canonical": "Multi-image Interleaved Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Curriculum Learning",
      "resolved_canonical": "Curriculum Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.83,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17040.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17040](https://arxiv.org/abs/2509.17040)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (87.0% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (84.0% similar)
- [[2025-09-19/MARIC_ Multi-Agent Reasoning for Image Classification_20250919|MARIC: Multi-Agent Reasoning for Image Classification]] (83.9% similar)
- [[2025-09-22/Multi-Physics_ A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems_20250922|Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems]] (83.8% similar)
- [[2025-09-23/NUMINA_ A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities_20250923|NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Curriculum Learning|Curriculum Learning]]
**âš¡ Unique Technical**: [[keywords/Multi-image Interleaved Reasoning|Multi-image Interleaved Reasoning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17040v1 Announce Type: cross 
Abstract: Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language Models (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks. While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images. To enhance MLLMs ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an "easy to hard" approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks. Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models reasoning performance on MIR and other established benchmarks. We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs capability to handle complex inter-modal tasks.Our code and dataset are available at https://github.com/Shelly-coder239/MIRBench.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ ì´ë¯¸ì§€ êµì°¨ ì¶”ë¡ ì„ í†µí•´ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ë³µì¡í•œ ì¥ë©´ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ëŠ” ì´ë¯¸ì§€ ê°„ì˜ ìƒí˜¸ ì—°ê´€ëœ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ê°„ê³¼í•˜ê³ , ê°œë³„ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ê´€ê³„ë¥¼ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ MIRì„ ë„ì…í•˜ì—¬ ë‹¤ì¤‘ ì´ë¯¸ì§€ì™€ êµì°¨ëœ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ í•¨ê»˜ ì¶”ë¡ í•˜ë„ë¡ ìš”êµ¬í•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‹¨ê³„ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì „ëµì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ì‰¬ìš´ ê²ƒì—ì„œ ì–´ë ¤ìš´ ê²ƒìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ëª¨ë¸ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì´ MIR ë° ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” MLLMsì˜ ë³µì¡í•œ êµì°¨ ëª¨ë‹¬ ì‘ì—… ì²˜ë¦¬ ëŠ¥ë ¥ì„ ë°œì „ì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ ì´ë¯¸ì§€ êµì°¨ ì¶”ë¡ ì€ MLLMsì˜ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë° ê´€ë ¨ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ ì í•œë‹¤.
- 2. ê¸°ì¡´ì˜ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ëŠ” êµì°¨ëœ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ê°„ê³¼í•˜ê³  ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ê³ ìœ í•œ ê´€ê³„ë¥¼ ë¬´ì‹œí•œë‹¤.
- 3. ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ MIRì€ ë‹¤ì¤‘ ì´ë¯¸ì§€ì™€ êµì°¨ëœ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ í†µí•´ ì´ë¯¸ì§€ ì˜ì—­ê³¼ í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì—°ê²°í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë³´ë¥¼ ì—°ê²°í•˜ëŠ” ê²ƒì„ ìš”êµ¬í•œë‹¤.
- 4. ë‹¨ê³„ë³„ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì „ëµì„ í†µí•´ ëª¨ë¸ì´ ë‹¨ìˆœí•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ì ì§„ì ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•œë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ MIR ë° ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-23 23:41:53*