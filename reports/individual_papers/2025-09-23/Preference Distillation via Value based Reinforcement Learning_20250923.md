---
keywords:
  - Preference Distillation
  - Value-based Reinforcement Learning
  - Teacher Value-based Knowledge Distillation
  - Reward Modeling
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16965
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:21:01.537035",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Preference Distillation",
    "Value-based Reinforcement Learning",
    "Teacher Value-based Knowledge Distillation",
    "Reward Modeling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Preference Distillation": 0.85,
    "Value-based Reinforcement Learning": 0.8,
    "Teacher Value-based Knowledge Distillation": 0.9,
    "Reward Modeling": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Preference Distillation",
        "canonical": "Preference Distillation",
        "aliases": [
          "Preference Optimization"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's novel approach and offers a unique concept for linking.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Value-based Reinforcement Learning",
        "canonical": "Value-based Reinforcement Learning",
        "aliases": [
          "Value Function Learning"
        ],
        "category": "broad_technical",
        "rationale": "This is a foundational concept in reinforcement learning, relevant for connecting to broader technical discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Teacher Value-based Knowledge Distillation",
        "canonical": "Teacher Value-based Knowledge Distillation",
        "aliases": [
          "TVKD"
        ],
        "category": "unique_technical",
        "rationale": "This is the core method proposed in the paper, offering a unique linking opportunity for specific discussions.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.9
      },
      {
        "surface": "Reward Modeling",
        "canonical": "Reward Modeling",
        "aliases": [
          "Reward Shaping"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is crucial for understanding the paper's approach to reinforcement learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Preference Distillation",
      "resolved_canonical": "Preference Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Value-based Reinforcement Learning",
      "resolved_canonical": "Value-based Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Teacher Value-based Knowledge Distillation",
      "resolved_canonical": "Teacher Value-based Knowledge Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Reward Modeling",
      "resolved_canonical": "Reward Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Preference Distillation via Value based Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16965.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16965](https://arxiv.org/abs/2509.16965)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Delta Knowledge Distillation for Large Language Models_20250919|Delta Knowledge Distillation for Large Language Models]] (86.1% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (83.0% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (82.8% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (82.3% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Value-based Reinforcement Learning|Value-based Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Reward Modeling|Reward Modeling]]
**âš¡ Unique Technical**: [[keywords/Preference Distillation|Preference Distillation]], [[keywords/Teacher Value-based Knowledge Distillation|Teacher Value-based Knowledge Distillation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16965v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) is a powerful paradigm to align language models with human preferences using pairwise comparisons. However, its binary win-or-loss supervision often proves insufficient for training small models with limited capacity. Prior works attempt to distill information from large teacher models using behavior cloning or KL divergence. These methods often focus on mimicking current behavior and overlook distilling reward modeling. To address this issue, we propose \textit{Teacher Value-based Knowledge Distillation} (TVKD), which introduces an auxiliary reward from the value function of the teacher model to provide a soft guide. This auxiliary reward is formulated to satisfy potential-based reward shaping, ensuring that the global reward structure and optimal policy of DPO are preserved. TVKD can be integrated into the standard DPO training framework and does not require additional rollouts. Our experimental results show that TVKD consistently improves performance across various benchmarks and model sizes.

## ğŸ“ ìš”ì•½

Direct Preference Optimization (DPO)ëŠ” ì–¸ì–´ ëª¨ë¸ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ê¸° ìœ„í•´ ìŒë³„ ë¹„êµë¥¼ ì‚¬ìš©í•˜ëŠ” ê°•ë ¥í•œ ë°©ë²•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‘ì€ ëª¨ë¸ì—ì„œëŠ” ì´ì§„ ìŠ¹íŒ¨ ê°ë…ì´ ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ëŒ€í˜• ëª¨ë¸ì—ì„œ í–‰ë™ ë³µì œë‚˜ KL ë°œì‚°ì„ í†µí•´ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë ¤ í–ˆìœ¼ë‚˜, ë³´ìƒ ëª¨ë¸ë§ì˜ ì¦ë¥˜ë¥¼ ê°„ê³¼í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” êµì‚¬ ëª¨ë¸ì˜ ê°€ì¹˜ í•¨ìˆ˜ì—ì„œ ë³´ì¡° ë³´ìƒì„ ë„ì…í•˜ëŠ” 'êµì‚¬ ê°€ì¹˜ ê¸°ë°˜ ì§€ì‹ ì¦ë¥˜(TVKD)'ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë³´ì¡° ë³´ìƒì€ ì ì¬ ê¸°ë°˜ ë³´ìƒ í˜•ì„±ì„ ë§Œì¡±ì‹œì¼œ DPOì˜ ê¸€ë¡œë²Œ ë³´ìƒ êµ¬ì¡°ì™€ ìµœì  ì •ì±…ì„ ìœ ì§€í•©ë‹ˆë‹¤. TVKDëŠ” í‘œì¤€ DPO í›ˆë ¨ í”„ë ˆì„ì›Œí¬ì— í†µí•©ë  ìˆ˜ ìˆìœ¼ë©° ì¶”ê°€ì ì¸ ë¡¤ì•„ì›ƒì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TVKDëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ëª¨ë¸ í¬ê¸°ì—ì„œ ì„±ëŠ¥ì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Direct Preference Optimization(DPO)ëŠ” ì–¸ì–´ ëª¨ë¸ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ê¸° ìœ„í•œ ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ì´ì§€ë§Œ, ì‘ì€ ëª¨ë¸ì—ì„œëŠ” ì´ì§„ ìŠ¹íŒ¨ ê°ë…ì´ ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ëŒ€í˜• êµì‚¬ ëª¨ë¸ë¡œë¶€í„° í–‰ë™ ë³µì œë‚˜ KL ë°œì‚°ì„ í†µí•´ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë ¤ê³  ì‹œë„í–ˆìœ¼ë‚˜, ë³´ìƒ ëª¨ë¸ë§ì„ ê°„ê³¼í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìŠµë‹ˆë‹¤.
- 3. Teacher Value-based Knowledge Distillation(TVKD)ì€ êµì‚¬ ëª¨ë¸ì˜ ê°€ì¹˜ í•¨ìˆ˜ì—ì„œ ë³´ì¡° ë³´ìƒì„ ë„ì…í•˜ì—¬ ë¶€ë“œëŸ¬ìš´ ì§€ì¹¨ì„ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. TVKDëŠ” ì ì¬ ê¸°ë°˜ ë³´ìƒ í˜•ì„±ì„ ë§Œì¡±í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ DPOì˜ ì „ë°˜ì ì¸ ë³´ìƒ êµ¬ì¡°ì™€ ìµœì  ì •ì±…ì„ ë³´ì¡´í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, TVKDëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ëª¨ë¸ í¬ê¸°ì—ì„œ ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:21:01*