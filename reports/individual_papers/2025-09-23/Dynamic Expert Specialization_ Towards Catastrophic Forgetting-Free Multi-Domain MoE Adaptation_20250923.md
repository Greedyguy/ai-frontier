---
keywords:
  - Mixture-of-Experts
  - Catastrophic Forgetting
  - Dynamic Expert Specialization
  - Adaptive Fine-Tuning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16882
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:36:51.276537",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Catastrophic Forgetting",
    "Dynamic Expert Specialization",
    "Adaptive Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.85,
    "Catastrophic Forgetting": 0.88,
    "Dynamic Expert Specialization": 0.9,
    "Adaptive Fine-Tuning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's methodology, offering a unique approach to model adaptation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Catastrophic Forgetting",
        "canonical": "Catastrophic Forgetting",
        "aliases": [
          "Forgetting"
        ],
        "category": "specific_connectable",
        "rationale": "A critical challenge in multi-domain adaptation, relevant for linking to related research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.88
      },
      {
        "surface": "Dynamic Expert Specialization",
        "canonical": "Dynamic Expert Specialization",
        "aliases": [
          "DES"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework that is central to the paper's contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "Adaptive Fine-Tuning",
        "canonical": "Adaptive Fine-Tuning",
        "aliases": [
          "Fine-Tuning"
        ],
        "category": "specific_connectable",
        "rationale": "A method for optimizing model performance, relevant to many adaptation strategies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Catastrophic Forgetting",
      "resolved_canonical": "Catastrophic Forgetting",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Dynamic Expert Specialization",
      "resolved_canonical": "Dynamic Expert Specialization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Adaptive Fine-Tuning",
      "resolved_canonical": "Adaptive Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16882.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16882](https://arxiv.org/abs/2509.16882)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (86.5% similar)
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (85.6% similar)
- [[2025-09-22/TrueMoE_ Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection_20250922|TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection]] (85.1% similar)
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (84.7% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (83.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Catastrophic Forgetting|Catastrophic Forgetting]], [[keywords/Adaptive Fine-Tuning|Adaptive Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Dynamic Expert Specialization|Dynamic Expert Specialization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16882v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation.

## ğŸ“ ìš”ì•½

Mixture-of-Experts (MoE) ëª¨ë¸ì€ ì „ë¬¸ê°€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ë†’ì€ ìš©ëŸ‰ì„ ì œê³µí•˜ì§€ë§Œ, ì—¬ëŸ¬ ë„ë©”ì¸ì— ì ì‘í•˜ë©´ì„œ ë§ê°ì„ ë°©ì§€í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ì–´ë ¤ìš´ ë¬¸ì œì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ DES-MoEëŠ” ì„¸ ê°€ì§€ í˜ì‹ ì„ í†µí•´ ë§ê°ì„ ë°©ì§€í•©ë‹ˆë‹¤: (1) ì‚¬ì „ í•™ìŠµëœ ì§€ì‹ ìœ ì§€ì™€ ê³¼ì œë³„ ì—…ë°ì´íŠ¸ë¥¼ ê· í˜• ìˆê²Œ ì¡°ì •í•˜ëŠ” ì ì‘í˜• ë¼ìš°í„°, (2) ì‹¤ì‹œê°„ ì „ë¬¸ê°€-ë„ë©”ì¸ ìƒê´€ê´€ê³„ ë§¤í•‘ì„ í†µí•œ ë„ë©”ì¸ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ë¦¬, (3) ë¹„ì „ë¬¸ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ê³ ì •í•˜ëŠ” ì„¸ ë‹¨ê³„ì˜ ì ì‘í˜• ë¯¸ì„¸ ì¡°ì • ì¼ì •ì…ë‹ˆë‹¤. DES-MoEëŠ” ë‹¨ì¼ ë„ë©”ì¸ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ë§ê°ì„ 89% ì¤„ì´ê³ , ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ 68% ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë‹¤ì¤‘ ì‘ì—… MoE ì ì‘ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DES-MoEëŠ” Mixture-of-Experts ëª¨ë¸ì˜ ë‹¤ì¤‘ ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ ë™ì  ì „ë¬¸ê°€ íŠ¹í™” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì ì‘í˜• ë¼ìš°í„°ë¥¼ í†µí•´ ì‚¬ì „ í•™ìŠµëœ ì§€ì‹ì˜ ìœ ì§€ì™€ ì‘ì—…ë³„ ì—…ë°ì´íŠ¸ë¥¼ ê· í˜• ìˆê²Œ ì¡°ì ˆí•©ë‹ˆë‹¤.
- 3. ì‹¤ì‹œê°„ ì „ë¬¸ê°€-ë„ë©”ì¸ ìƒê´€ ë§¤í•‘ì„ í†µí•´ ë„ë©”ì¸ë³„ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë§ê°ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- 4. ì„¸ ë‹¨ê³„ì˜ ì ì‘í˜• ë¯¸ì„¸ ì¡°ì • ìŠ¤ì¼€ì¤„ì„ í†µí•´ ë¹„ì „ë¬¸ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
- 5. DES-MoEëŠ” ë‹¨ì¼ ë„ë©”ì¸ ESFT ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ í†µí•© ëª¨ë¸ì„ í›ˆë ¨í•˜ë©°, ë„ë©”ì¸ì´ ì¦ê°€í• ìˆ˜ë¡ ë§ê°ì„ 89% ì¤„ì´ê³  ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ 68% ë¹ ë¥¸ ìˆ˜ë ´ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:36:51*