---
keywords:
  - Large Language Model
  - Cross-Lingual Transfer Learning
  - Low-Resource Languages
  - Machine Translation
  - Zero-Shot Learning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16914
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:20:22.777743",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Lingual Transfer Learning",
    "Low-Resource Languages",
    "Machine Translation",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cross-Lingual Transfer Learning": 0.88,
    "Low-Resource Languages": 0.82,
    "Machine Translation": 0.8,
    "Zero-Shot Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus on enhancing cross-lingual capabilities in NLP, linking it to existing research on LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "cross-lingual transfer learning",
        "canonical": "Cross-Lingual Transfer Learning",
        "aliases": [
          "cross-lingual knowledge transfer"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is crucial for understanding the paper's contribution to improving language models for low-resource languages.",
        "novelty_score": 0.68,
        "connectivity_score": 0.79,
        "specificity_score": 0.82,
        "link_intent_score": 0.88
      },
      {
        "surface": "low-resource languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "under-resourced languages"
        ],
        "category": "unique_technical",
        "rationale": "The focus on low-resource languages is a unique aspect of the dataset and its application in the paper.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "machine translation",
        "canonical": "Machine Translation",
        "aliases": [
          "MT"
        ],
        "category": "specific_connectable",
        "rationale": "Machine translation is a key method used to create the dataset, linking it to broader translation research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "zero-shot capabilities",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a trending concept relevant to the paper's discussion on LLM capabilities.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "corpus",
      "languages"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "cross-lingual transfer learning",
      "resolved_canonical": "Cross-Lingual Transfer Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.79,
        "specificity": 0.82,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "low-resource languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "machine translation",
      "resolved_canonical": "Machine Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "zero-shot capabilities",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16914.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16914](https://arxiv.org/abs/2509.16914)

## 🔗 유사한 논문
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (83.0% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (82.9% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (82.1% similar)
- [[2025-09-22/UPRPRC_ Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations_20250922|UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations]] (81.9% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Cross-Lingual Transfer Learning|Cross-Lingual Transfer Learning]], [[keywords/Machine Translation|Machine Translation]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Low-Resource Languages|Low-Resource Languages]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16914v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities in various NLP tasks, significantly enhancing user experience and efficiency. However, this advantage is primarily limited to resource-rich languages. For the diverse array of low-resource languages, support remains inadequate, with the scarcity of training corpora considered the primary cause. We construct and open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two 25GB sets of four-language corpora (one parallel and one non-parallel), obtained through machine translation. CUTE encompasses two resource-rich languages (Chinese and English) and two low-resource languages (Uyghur and Tibetan). Prior to constructing CUTE, human assessment validates that the machine translation quality between Chinese-Uyghur and Chinese-Tibetan approaches that of Chinese-English translation. CUTE represents the largest open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate its effectiveness in enhancing LLMs' ability to process low-resource languages while investigating the role of corpus parallelism in cross-lingual transfer learning. The CUTE corpus and related models are made publicly available to the research community.

## 📝 요약

대형 언어 모델(LLM)은 다양한 자연어 처리 작업에서 뛰어난 제로샷 능력을 보여주지만, 주로 자원이 풍부한 언어에 국한됩니다. 저자들은 중국어, 위구르어, 티베트어, 영어로 구성된 25GB의 병렬 및 비병렬 코퍼스를 포함한 CUTE 데이터를 구축하고 공개했습니다. CUTE는 위구르어와 티베트어 같은 저자원 언어에 대한 가장 큰 공개 코퍼스로, LLM의 저자원 언어 처리 능력을 향상시키는 데 효과적임을 입증했습니다. 또한, 코퍼스 병렬성이 언어 간 전이 학습에 미치는 역할을 조사했습니다. CUTE 코퍼스와 관련 모델은 연구 커뮤니티에 공개됩니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 다양한 자연어 처리 작업에서 뛰어난 제로샷 능력을 보여주지만, 주로 자원이 풍부한 언어에 한정됩니다.
- 2. 저자들은 중국어, 위구르어, 티베트어, 영어로 구성된 CUTE 데이터셋을 구축하여 공개했으며, 이는 병렬 및 비병렬 25GB의 네 언어 코퍼스로 이루어져 있습니다.
- 3. CUTE는 위구르어와 티베트어에 대한 가장 큰 오픈 소스 코퍼스로, LLM의 저자원 언어 처리 능력을 향상시키는 데 효과적임을 입증했습니다.
- 4. CUTE 데이터셋은 연구 커뮤니티에 공개되어 있으며, 코퍼스 병렬성이 교차 언어 전이 학습에 미치는 역할을 조사합니다.
- 5. 중국어-위구르어 및 중국어-티베트어 간 기계 번역 품질이 중국어-영어 번역에 근접함을 인간 평가를 통해 검증했습니다.


---

*Generated on 2025-09-24 03:20:22*