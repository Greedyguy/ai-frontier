---
keywords:
  - Large Language Model
  - Cross-Lingual Transfer Learning
  - Low-Resource Languages
  - Machine Translation
  - Zero-Shot Learning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16914
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:20:22.777743",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Lingual Transfer Learning",
    "Low-Resource Languages",
    "Machine Translation",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cross-Lingual Transfer Learning": 0.88,
    "Low-Resource Languages": 0.82,
    "Machine Translation": 0.8,
    "Zero-Shot Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus on enhancing cross-lingual capabilities in NLP, linking it to existing research on LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "cross-lingual transfer learning",
        "canonical": "Cross-Lingual Transfer Learning",
        "aliases": [
          "cross-lingual knowledge transfer"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is crucial for understanding the paper's contribution to improving language models for low-resource languages.",
        "novelty_score": 0.68,
        "connectivity_score": 0.79,
        "specificity_score": 0.82,
        "link_intent_score": 0.88
      },
      {
        "surface": "low-resource languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "under-resourced languages"
        ],
        "category": "unique_technical",
        "rationale": "The focus on low-resource languages is a unique aspect of the dataset and its application in the paper.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "machine translation",
        "canonical": "Machine Translation",
        "aliases": [
          "MT"
        ],
        "category": "specific_connectable",
        "rationale": "Machine translation is a key method used to create the dataset, linking it to broader translation research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "zero-shot capabilities",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a trending concept relevant to the paper's discussion on LLM capabilities.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "corpus",
      "languages"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "cross-lingual transfer learning",
      "resolved_canonical": "Cross-Lingual Transfer Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.79,
        "specificity": 0.82,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "low-resource languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "machine translation",
      "resolved_canonical": "Machine Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "zero-shot capabilities",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16914.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16914](https://arxiv.org/abs/2509.16914)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (83.0% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (82.9% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (82.1% similar)
- [[2025-09-22/UPRPRC_ Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations_20250922|UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations]] (81.9% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Cross-Lingual Transfer Learning|Cross-Lingual Transfer Learning]], [[keywords/Machine Translation|Machine Translation]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Low-Resource Languages|Low-Resource Languages]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16914v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities in various NLP tasks, significantly enhancing user experience and efficiency. However, this advantage is primarily limited to resource-rich languages. For the diverse array of low-resource languages, support remains inadequate, with the scarcity of training corpora considered the primary cause. We construct and open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two 25GB sets of four-language corpora (one parallel and one non-parallel), obtained through machine translation. CUTE encompasses two resource-rich languages (Chinese and English) and two low-resource languages (Uyghur and Tibetan). Prior to constructing CUTE, human assessment validates that the machine translation quality between Chinese-Uyghur and Chinese-Tibetan approaches that of Chinese-English translation. CUTE represents the largest open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate its effectiveness in enhancing LLMs' ability to process low-resource languages while investigating the role of corpus parallelism in cross-lingual transfer learning. The CUTE corpus and related models are made publicly available to the research community.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì œë¡œìƒ· ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ì£¼ë¡œ ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì— êµ­í•œë©ë‹ˆë‹¤. ì €ìë“¤ì€ ì¤‘êµ­ì–´, ìœ„êµ¬ë¥´ì–´, í‹°ë² íŠ¸ì–´, ì˜ì–´ë¡œ êµ¬ì„±ëœ 25GBì˜ ë³‘ë ¬ ë° ë¹„ë³‘ë ¬ ì½”í¼ìŠ¤ë¥¼ í¬í•¨í•œ CUTE ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ê³  ê³µê°œí–ˆìŠµë‹ˆë‹¤. CUTEëŠ” ìœ„êµ¬ë¥´ì–´ì™€ í‹°ë² íŠ¸ì–´ ê°™ì€ ì €ìì› ì–¸ì–´ì— ëŒ€í•œ ê°€ì¥ í° ê³µê°œ ì½”í¼ìŠ¤ë¡œ, LLMì˜ ì €ìì› ì–¸ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì½”í¼ìŠ¤ ë³‘ë ¬ì„±ì´ ì–¸ì–´ ê°„ ì „ì´ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì—­í• ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. CUTE ì½”í¼ìŠ¤ì™€ ê´€ë ¨ ëª¨ë¸ì€ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— ê³µê°œë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì œë¡œìƒ· ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ì£¼ë¡œ ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì— í•œì •ë©ë‹ˆë‹¤.
- 2. ì €ìë“¤ì€ ì¤‘êµ­ì–´, ìœ„êµ¬ë¥´ì–´, í‹°ë² íŠ¸ì–´, ì˜ì–´ë¡œ êµ¬ì„±ëœ CUTE ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ ê³µê°œí–ˆìœ¼ë©°, ì´ëŠ” ë³‘ë ¬ ë° ë¹„ë³‘ë ¬ 25GBì˜ ë„¤ ì–¸ì–´ ì½”í¼ìŠ¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
- 3. CUTEëŠ” ìœ„êµ¬ë¥´ì–´ì™€ í‹°ë² íŠ¸ì–´ì— ëŒ€í•œ ê°€ì¥ í° ì˜¤í”ˆ ì†ŒìŠ¤ ì½”í¼ìŠ¤ë¡œ, LLMì˜ ì €ìì› ì–¸ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 4. CUTE ë°ì´í„°ì…‹ì€ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— ê³µê°œë˜ì–´ ìˆìœ¼ë©°, ì½”í¼ìŠ¤ ë³‘ë ¬ì„±ì´ êµì°¨ ì–¸ì–´ ì „ì´ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì—­í• ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.
- 5. ì¤‘êµ­ì–´-ìœ„êµ¬ë¥´ì–´ ë° ì¤‘êµ­ì–´-í‹°ë² íŠ¸ì–´ ê°„ ê¸°ê³„ ë²ˆì—­ í’ˆì§ˆì´ ì¤‘êµ­ì–´-ì˜ì–´ ë²ˆì—­ì— ê·¼ì ‘í•¨ì„ ì¸ê°„ í‰ê°€ë¥¼ í†µí•´ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:20:22*