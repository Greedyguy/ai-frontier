---
keywords:
  - Dual Encoder Architecture
  - Self-supervised Learning
  - Large Language Model
  - Data Augmentation
  - Clustering
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16649
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:30:13.611579",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Dual Encoder Architecture",
    "Self-supervised Learning",
    "Large Language Model",
    "Data Augmentation",
    "Clustering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Dual Encoder Architecture": 0.7,
    "Self-supervised Learning": 0.8,
    "Large Language Model": 0.85,
    "Data Augmentation": 0.7,
    "Clustering": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "dual encoder architecture",
        "canonical": "Dual Encoder Architecture",
        "aliases": [
          "dual encoders"
        ],
        "category": "unique_technical",
        "rationale": "This architecture is central to the system's design, enabling separate encoding of audio and text modalities.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "contrastive learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "contrastive methods"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive learning is a key technique in self-supervised learning, enhancing connectivity with related concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are pivotal for data augmentation, linking to broader AI and NLP research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "data augmentation techniques",
        "canonical": "Data Augmentation",
        "aliases": [
          "augmentation methods"
        ],
        "category": "specific_connectable",
        "rationale": "Data augmentation is crucial for improving model robustness, connecting to various machine learning applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "clustering",
        "canonical": "Clustering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Clustering introduces an auxiliary task, enhancing model fine-tuning and linking to unsupervised learning methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "system",
      "task",
      "approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "dual encoder architecture",
      "resolved_canonical": "Dual Encoder Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "contrastive learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "data augmentation techniques",
      "resolved_canonical": "Data Augmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "clustering",
      "resolved_canonical": "Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16649.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16649](https://arxiv.org/abs/2509.16649)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Omni-CLST_ Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering_20250918|Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (82.0% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (81.2% similar)
- [[2025-09-17/DSpAST_ Disentangled Representations for Spatial Audio Reasoning with Large Language Models_20250917|DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models]] (80.8% similar)
- [[2025-09-22/Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training_20250922|Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training]] (80.8% similar)
- [[2025-09-22/Frustratingly Easy Data Augmentation for Low-Resource ASR_20250922|Frustratingly Easy Data Augmentation for Low-Resource ASR]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Data Augmentation|Data Augmentation]], [[keywords/Clustering|Clustering]]
**âš¡ Unique Technical**: [[keywords/Dual Encoder Architecture|Dual Encoder Architecture]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16649v1 Announce Type: cross 
Abstract: This report presents the AISTAT team's submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year's challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.

## ğŸ“ ìš”ì•½

ì´ ë³´ê³ ì„œëŠ” DCASE 2025 Task 6ì˜ ì–¸ì–´ ê¸°ë°˜ ì˜¤ë””ì˜¤ ê²€ìƒ‰ ê³¼ì œì— ëŒ€í•œ AISTAT íŒ€ì˜ ì œì¶œì‘ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì œì•ˆëœ ì‹œìŠ¤í…œì€ ì˜¤ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê°ê° ì¸ì½”ë”©í•˜ê³ , ì´ë“¤ì˜ í‘œí˜„ì„ ëŒ€ì¡° í•™ìŠµì„ í†µí•´ ì •ë ¬í•˜ëŠ” ì´ì¤‘ ì¸ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ì „ ì—°ë„ì˜ ë„ì „ ê³¼ì œì—ì„œ ì˜ê°ì„ ë°›ì•„, ì¦ë¥˜ ì ‘ê·¼ë²•ì„ êµ¬í˜„í•˜ê³  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ ë°±ë²ˆì—­ ë° LLM ë¯¹ìŠ¤ë¥¼ í¬í•¨í•œ ë°ì´í„° ì¦ê°• ê¸°ë²•ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ë³´ì¡° ë¶„ë¥˜ ì‘ì—…ì„ ë„ì…í•˜ì—¬ ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ë‹¨ì¼ ì‹œìŠ¤í…œì—ì„œ mAP@16 46.62ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ë„¤ ê°œì˜ ì‹œìŠ¤í…œì„ ì•™ìƒë¸”í•˜ì—¬ mAP@16 48.83ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AISTAT íŒ€ì€ DCASE 2025 Task 6ì˜ ì–¸ì–´ ê¸°ë°˜ ì˜¤ë””ì˜¤ ê²€ìƒ‰ ê³¼ì œì— ë“€ì–¼ ì¸ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ë³„ë„ë¡œ ì¸ì½”ë”©í•˜ê³  ëŒ€ì¡° í•™ìŠµì„ í†µí•´ ì •ë ¬í–ˆìŠµë‹ˆë‹¤.
- 2. ë°ì´í„° ì¦ê°•ì„ ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ ë°±ë²ˆì—­ê³¼ LLM ë¯¹ìŠ¤ë¥¼ í¬í•¨í•œ ê¸°ë²•ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
- 3. ë³´ì¡° ë¶„ë¥˜ ì‘ì—…ì„ ë„ì…í•˜ê¸° ìœ„í•´ í´ëŸ¬ìŠ¤í„°ë§ì„ í¬í•¨í•˜ì—¬ ì¶”ê°€ì ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
- 4. ë‹¨ì¼ ì‹œìŠ¤í…œì—ì„œ mAP@16 46.62ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ë„¤ ê°œì˜ ì‹œìŠ¤í…œì„ ì•™ìƒë¸”í•˜ì—¬ mAP@16 48.83ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:30:13*