---
keywords:
  - Dual Encoder Architecture
  - Self-supervised Learning
  - Large Language Model
  - Data Augmentation
  - Clustering
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16649
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:30:13.611579",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Dual Encoder Architecture",
    "Self-supervised Learning",
    "Large Language Model",
    "Data Augmentation",
    "Clustering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Dual Encoder Architecture": 0.7,
    "Self-supervised Learning": 0.8,
    "Large Language Model": 0.85,
    "Data Augmentation": 0.7,
    "Clustering": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "dual encoder architecture",
        "canonical": "Dual Encoder Architecture",
        "aliases": [
          "dual encoders"
        ],
        "category": "unique_technical",
        "rationale": "This architecture is central to the system's design, enabling separate encoding of audio and text modalities.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "contrastive learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "contrastive methods"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive learning is a key technique in self-supervised learning, enhancing connectivity with related concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are pivotal for data augmentation, linking to broader AI and NLP research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "data augmentation techniques",
        "canonical": "Data Augmentation",
        "aliases": [
          "augmentation methods"
        ],
        "category": "specific_connectable",
        "rationale": "Data augmentation is crucial for improving model robustness, connecting to various machine learning applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "clustering",
        "canonical": "Clustering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Clustering introduces an auxiliary task, enhancing model fine-tuning and linking to unsupervised learning methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "system",
      "task",
      "approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "dual encoder architecture",
      "resolved_canonical": "Dual Encoder Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "contrastive learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "data augmentation techniques",
      "resolved_canonical": "Data Augmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "clustering",
      "resolved_canonical": "Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16649.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16649](https://arxiv.org/abs/2509.16649)

## 🔗 유사한 논문
- [[2025-09-18/Omni-CLST_ Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering_20250918|Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (82.0% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (81.2% similar)
- [[2025-09-17/DSpAST_ Disentangled Representations for Spatial Audio Reasoning with Large Language Models_20250917|DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models]] (80.8% similar)
- [[2025-09-22/Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training_20250922|Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training]] (80.8% similar)
- [[2025-09-22/Frustratingly Easy Data Augmentation for Low-Resource ASR_20250922|Frustratingly Easy Data Augmentation for Low-Resource ASR]] (80.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Data Augmentation|Data Augmentation]], [[keywords/Clustering|Clustering]]
**⚡ Unique Technical**: [[keywords/Dual Encoder Architecture|Dual Encoder Architecture]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16649v1 Announce Type: cross 
Abstract: This report presents the AISTAT team's submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year's challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.

## 📝 요약

이 보고서는 DCASE 2025 Task 6의 언어 기반 오디오 검색 과제에 대한 AISTAT 팀의 제출작을 소개합니다. 제안된 시스템은 오디오와 텍스트 모달리티를 각각 인코딩하고, 이들의 표현을 대조 학습을 통해 정렬하는 이중 인코더 아키텍처를 사용합니다. 이전 연도의 도전 과제에서 영감을 받아, 증류 접근법을 구현하고 대규모 언어 모델(LLM)을 활용하여 백번역 및 LLM 믹스를 포함한 데이터 증강 기법을 적용했습니다. 또한, 클러스터링을 통해 보조 분류 작업을 도입하여 추가 미세 조정을 수행했습니다. 단일 시스템에서 mAP@16 46.62를 기록했으며, 네 개의 시스템을 앙상블하여 mAP@16 48.83을 달성했습니다.

## 🎯 주요 포인트

- 1. AISTAT 팀은 DCASE 2025 Task 6의 언어 기반 오디오 검색 과제에 듀얼 인코더 아키텍처를 사용하여 오디오와 텍스트 모달리티를 별도로 인코딩하고 대조 학습을 통해 정렬했습니다.
- 2. 데이터 증강을 위해 대형 언어 모델(LLM)을 활용하여 백번역과 LLM 믹스를 포함한 기법을 구현했습니다.
- 3. 보조 분류 작업을 도입하기 위해 클러스터링을 포함하여 추가적인 미세 조정을 수행했습니다.
- 4. 단일 시스템에서 mAP@16 46.62를 달성했으며, 네 개의 시스템을 앙상블하여 mAP@16 48.83을 기록했습니다.


---

*Generated on 2025-09-23 23:30:13*