---
keywords:
  - COMTAIL
  - Neural Translation Metric
  - Human Evaluation Dataset
  - BLEU
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17667
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:30:44.100935",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "COMTAIL",
    "Neural Translation Metric",
    "Human Evaluation Dataset",
    "BLEU"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "COMTAIL": 0.88,
    "Neural Translation Metric": 0.72,
    "Human Evaluation Dataset": 0.78,
    "BLEU": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Cross-lingual Optimized Metric for Translation Assessment of Indian Languages",
        "canonical": "COMTAIL",
        "aliases": [
          "Cross-lingual Optimized Metric",
          "Translation Assessment Metric"
        ],
        "category": "unique_technical",
        "rationale": "COMTAIL is a novel metric specifically designed for evaluating translations involving Indian languages, offering unique insights and connections in multilingual NLP research.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "neural translation evaluation metric",
        "canonical": "Neural Translation Metric",
        "aliases": [
          "Neural Evaluation Metric"
        ],
        "category": "broad_technical",
        "rationale": "This connects to broader discussions on neural approaches in translation evaluation, which is a significant area in NLP.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "human evaluation ratings dataset",
        "canonical": "Human Evaluation Dataset",
        "aliases": [
          "Human Ratings Dataset"
        ],
        "category": "specific_connectable",
        "rationale": "Datasets are critical for training and evaluating models, and this specific dataset enhances connectivity in translation evaluation research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "BLEU",
        "canonical": "BLEU",
        "aliases": [
          "BLEU Score"
        ],
        "category": "specific_connectable",
        "rationale": "BLEU is a widely recognized metric in translation evaluation, providing strong connectivity to existing research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "automatic evaluation",
      "translation pairs",
      "state-of-the-art"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Cross-lingual Optimized Metric for Translation Assessment of Indian Languages",
      "resolved_canonical": "COMTAIL",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "neural translation evaluation metric",
      "resolved_canonical": "Neural Translation Metric",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "human evaluation ratings dataset",
      "resolved_canonical": "Human Evaluation Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "BLEU",
      "resolved_canonical": "BLEU",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Crosslingual Optimized Metric for Translation Assessment of Indian Languages

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17667.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17667](https://arxiv.org/abs/2509.17667)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India_ Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context_20250923|DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context]] (81.1% similar)
- [[2025-09-23/Fluent but Foreign_ Even Regional LLMs Lack Cultural Alignment_20250923|Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment]] (79.8% similar)
- [[2025-09-17/Long-context Reference-based MT Quality Estimation_20250917|Long-context Reference-based MT Quality Estimation]] (79.6% similar)
- [[2025-09-22/Translationese-index_ Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese_20250922|Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese]] (79.5% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (79.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Translation Metric|Neural Translation Metric]]
**ğŸ”— Specific Connectable**: [[keywords/Human Evaluation Dataset|Human Evaluation Dataset]], [[keywords/BLEU|BLEU]]
**âš¡ Unique Technical**: [[keywords/COMTAIL|COMTAIL]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17667v1 Announce Type: new 
Abstract: Automatic evaluation of translation remains a challenging task owing to the orthographic, morphological, syntactic and semantic richness and divergence observed across languages. String-based metrics such as BLEU have previously been extensively used for automatic evaluation tasks, but their limitations are now increasingly recognized. Although learned neural metrics have helped mitigate some of the limitations of string-based approaches, they remain constrained by a paucity of gold evaluation data in most languages beyond the usual high-resource pairs. In this present work we address some of these gaps. We create a large human evaluation ratings dataset for 13 Indian languages covering 21 translation directions and then train a neural translation evaluation metric named Cross-lingual Optimized Metric for Translation Assessment of Indian Languages (COMTAIL) on this dataset. The best performing metric variants show significant performance gains over previous state-of-the-art when adjudging translation pairs with at least one Indian language. Furthermore, we conduct a series of ablation studies to highlight the sensitivities of such a metric to changes in domain, translation quality, and language groupings. We release both the COMTAIL dataset and the accompanying metric models.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” 13ê°œì˜ ì¸ë„ ì–¸ì–´ì™€ 21ê°œì˜ ë²ˆì—­ ë°©í–¥ì„ í¬í•¨í•œ ëŒ€ê·œëª¨ ì¸ê°„ í‰ê°€ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ë„ ì–¸ì–´ ë²ˆì—­ í‰ê°€ë¥¼ ìœ„í•œ ì‹ ê²½ë§ ê¸°ë°˜ í‰ê°€ ì§€í‘œì¸ COMTAILì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. COMTAILì€ ê¸°ì¡´ì˜ ìµœê³  ì„±ëŠ¥ ì§€í‘œë³´ë‹¤ ì¸ë„ ì–¸ì–´ê°€ í¬í•¨ëœ ë²ˆì—­ ìŒ í‰ê°€ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ë„ë©”ì¸, ë²ˆì—­ í’ˆì§ˆ, ì–¸ì–´ ê·¸ë£¹ ë³€í™”ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ì†Œê±° ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ë¡œ COMTAIL ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë²ˆì—­ì˜ ìë™ í‰ê°€ì—ì„œ BLEUì™€ ê°™ì€ ë¬¸ìì—´ ê¸°ë°˜ ì§€í‘œì˜ í•œê³„ê°€ ì¸ì‹ë˜ê³  ìˆìœ¼ë©°, ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ì‹ ê²½ë§ ê¸°ë°˜ ì§€í‘œê°€ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ëŒ€ë¶€ë¶„ì˜ ì–¸ì–´ì—ì„œ ê¸ˆ í‰ê°€ ë°ì´í„°ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì‹ ê²½ë§ ê¸°ë°˜ ì§€í‘œì˜ ì œì•½ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” 13ê°œì˜ ì¸ë„ ì–¸ì–´ì™€ 21ê°œì˜ ë²ˆì—­ ë°©í–¥ì„ í¬í•¨í•˜ëŠ” ëŒ€ê·œëª¨ ì¸ê°„ í‰ê°€ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ COMTAILì´ë¼ëŠ” ì‹ ê²½ë§ ë²ˆì—­ í‰ê°€ ì§€í‘œë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- 4. COMTAILì€ ìµœì†Œ í•˜ë‚˜ì˜ ì¸ë„ ì–¸ì–´ê°€ í¬í•¨ëœ ë²ˆì—­ ìŒ í‰ê°€ì—ì„œ ì´ì „ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ë„ë©”ì¸, ë²ˆì—­ í’ˆì§ˆ, ì–¸ì–´ ê·¸ë£¹ì˜ ë³€í™”ì— ëŒ€í•œ ë¯¼ê°ì„±ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ ì¼ë ¨ì˜ ì†Œê±° ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:30:44*