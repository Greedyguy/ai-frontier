---
keywords:
  - Graph Neural Network
  - Constrained Optimization
  - Dual Ascent Algorithm
  - Lagrangian Saddle Point
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17156
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:47:38.666965",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Neural Network",
    "Constrained Optimization",
    "Dual Ascent Algorithm",
    "Lagrangian Saddle Point"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Graph Neural Network": 0.88,
    "Constrained Optimization": 0.75,
    "Dual Ascent Algorithm": 0.78,
    "Lagrangian Saddle Point": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Graph Neural Networks",
        "canonical": "Graph Neural Network",
        "aliases": [
          "GNN",
          "Graph NN"
        ],
        "category": "specific_connectable",
        "rationale": "Graph Neural Networks are central to the paper's approach, facilitating connections with other works using GNNs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Constrained Optimization",
        "canonical": "Constrained Optimization",
        "aliases": [
          "Optimization under Constraints"
        ],
        "category": "unique_technical",
        "rationale": "The paper focuses on solving constrained optimization problems, which is a specific technical challenge.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Dual Ascent Algorithm",
        "canonical": "Dual Ascent Algorithm",
        "aliases": [
          "DA Algorithm"
        ],
        "category": "unique_technical",
        "rationale": "The dual ascent algorithm is a unique approach discussed in the paper, relevant for optimization techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Lagrangian Saddle Point",
        "canonical": "Lagrangian Saddle Point",
        "aliases": [
          "Saddle Point of Lagrangian"
        ],
        "category": "unique_technical",
        "rationale": "Finding a Lagrangian saddle point is key to the optimization process described in the paper.",
        "novelty_score": 0.6,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Graph Neural Networks",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Constrained Optimization",
      "resolved_canonical": "Constrained Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Dual Ascent Algorithm",
      "resolved_canonical": "Dual Ascent Algorithm",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Lagrangian Saddle Point",
      "resolved_canonical": "Lagrangian Saddle Point",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Unrolled Graph Neural Networks for Constrained Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17156.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17156](https://arxiv.org/abs/2509.17156)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Adversarial generalization of unfolding (model-based) networks_20250922|Adversarial generalization of unfolding (model-based) networks]] (81.4% similar)
- [[2025-09-22/GIN-Graph_ A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks_20250922|GIN-Graph: A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks]] (80.8% similar)
- [[2025-09-18/GraphTorque_ Torque-Driven Rewiring Graph Neural Network_20250918|GraphTorque: Torque-Driven Rewiring Graph Neural Network]] (80.8% similar)
- [[2025-09-18/Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (80.4% similar)
- [[2025-09-17/A Neural Network for the Identical Kuramoto Equation_ Architectural Considerations and Performance Evaluation_20250917|A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation]] (80.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]]
**âš¡ Unique Technical**: [[keywords/Constrained Optimization|Constrained Optimization]], [[keywords/Dual Ascent Algorithm|Dual Ascent Algorithm]], [[keywords/Lagrangian Saddle Point|Lagrangian Saddle Point]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17156v1 Announce Type: new 
Abstract: In this paper, we unroll the dynamics of the dual ascent (DA) algorithm in two coupled graph neural networks (GNNs) to solve constrained optimization problems. The two networks interact with each other at the layer level to find a saddle point of the Lagrangian. The primal GNN finds a stationary point for a given dual multiplier, while the dual network iteratively refines its estimates to reach an optimal solution. We force the primal and dual networks to mirror the dynamics of the DA algorithm by imposing descent and ascent constraints. We propose a joint training scheme that alternates between updating the primal and dual networks. Our numerical experiments demonstrate that our approach yields near-optimal near-feasible solutions and generalizes well to out-of-distribution (OOD) problems.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì œì•½ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‘ ê°œì˜ ê²°í•©ëœ ê·¸ë˜í”„ ì‹ ê²½ë§(GNN)ì—ì„œ ì´ì¤‘ ìƒìŠ¹(DA) ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ì„ í¼ì¹©ë‹ˆë‹¤. ë‘ ë„¤íŠ¸ì›Œí¬ëŠ” ê³„ì¸µ ìˆ˜ì¤€ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë¼ê·¸ë‘ì§€ì•ˆì˜ ì•ˆì¥ì ì„ ì°¾ìŠµë‹ˆë‹¤. ê¸°ë³¸ GNNì€ ì£¼ì–´ì§„ ì´ì¤‘ ìŠ¹ìˆ˜ì— ëŒ€í•´ ì •ì§€ì ì„ ì°¾ê³ , ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ëŠ” ìµœì ì˜ í•´ë¥¼ ì°¾ê¸° ìœ„í•´ ì¶”ì •ì¹˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” DA ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ì„ ë°˜ì˜í•˜ë„ë¡ ê¸°ë³¸ ë° ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ì— í•˜ê°• ë° ìƒìŠ¹ ì œì•½ì„ ë¶€ê³¼í•©ë‹ˆë‹¤. ê¸°ë³¸ ë° ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ë¥¼ ë²ˆê°ˆì•„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³µë™ í•™ìŠµ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìˆ˜ì¹˜ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ê±°ì˜ ìµœì ì— ê°€ê¹Œìš´ í•´ë¥¼ ì œê³µí•˜ë©° ë¶„í¬ ì™¸ ë¬¸ì œì—ë„ ì˜ ì¼ë°˜í™”ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°œì˜ ê·¸ë˜í”„ ì‹ ê²½ë§(GNN)ì„ í™œìš©í•˜ì—¬ ì œì•½ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì´ì¤‘ ìƒìŠ¹(DA) ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
- 2. ë‘ ë„¤íŠ¸ì›Œí¬ëŠ” ê³„ì¸µ ìˆ˜ì¤€ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë¼ê·¸ë‘ì§€ì•ˆì˜ ì•ˆì¥ì ì„ ì°¾ìŠµë‹ˆë‹¤.
- 3. ê¸°ë³¸ GNNì€ ì£¼ì–´ì§„ ì´ì¤‘ ìŠ¹ìˆ˜ì— ëŒ€í•´ ì •ì§€ì ì„ ì°¾ê³ , ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ëŠ” ìµœì ì˜ í•´ë¥¼ ì°¾ê¸° ìœ„í•´ ì¶”ì •ì¹˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.
- 4. ê¸°ë³¸ ë° ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ê°€ DA ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ì„ ë°˜ì˜í•˜ë„ë¡ í•˜ê°• ë° ìƒìŠ¹ ì œì•½ì„ ë¶€ê³¼í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ê³µë™ í•™ìŠµ ë°©ì‹ì€ ê¸°ë³¸ ë° ì´ì¤‘ ë„¤íŠ¸ì›Œí¬ë¥¼ ë²ˆê°ˆì•„ ì—…ë°ì´íŠ¸í•˜ë©°, ì‹¤í—˜ ê²°ê³¼ëŠ” ê±°ì˜ ìµœì ì— ê°€ê¹Œìš´ í•´ë¥¼ ì œê³µí•˜ê³  ë¶„í¬ ì™¸ ë¬¸ì œì— ì˜ ì¼ë°˜í™”ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:47:38*