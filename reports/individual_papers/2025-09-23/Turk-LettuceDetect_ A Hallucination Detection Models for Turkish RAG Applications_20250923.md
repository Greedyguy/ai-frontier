---
keywords:
  - Turk-LettuceDetect
  - Large Language Model
  - Retrieval Augmented Generation
  - ModernBERT
  - RAGTruth Benchmark
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17671
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:05:53.689419",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Turk-LettuceDetect",
    "Large Language Model",
    "Retrieval Augmented Generation",
    "ModernBERT",
    "RAGTruth Benchmark"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Turk-LettuceDetect": 0.8,
    "Large Language Model": 0.85,
    "Retrieval Augmented Generation": 0.82,
    "ModernBERT": 0.75,
    "RAGTruth Benchmark": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Turk-LettuceDetect",
        "canonical": "Turk-LettuceDetect",
        "aliases": [
          "Turkish Hallucination Detection"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel model specifically for Turkish RAG applications, filling a gap in multilingual NLP.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental concept in NLP, crucial for understanding the context of hallucination in language models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's approach in addressing hallucination, providing a key link to recent advancements.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "ModernBERT",
        "canonical": "ModernBERT",
        "aliases": [
          "Turkish BERT"
        ],
        "category": "unique_technical",
        "rationale": "A specialized model variant tailored for Turkish, enhancing the paper's focus on language-specific solutions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "RAGTruth benchmark dataset",
        "canonical": "RAGTruth Benchmark",
        "aliases": [
          "RAGTruth"
        ],
        "category": "unique_technical",
        "rationale": "Provides a critical dataset for evaluating hallucination detection, linking to empirical validation efforts.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "hallucination",
      "dataset",
      "model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Turk-LettuceDetect",
      "resolved_canonical": "Turk-LettuceDetect",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "ModernBERT",
      "resolved_canonical": "ModernBERT",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "RAGTruth benchmark dataset",
      "resolved_canonical": "RAGTruth Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17671.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17671](https://arxiv.org/abs/2509.17671)

## 🔗 유사한 논문
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (81.4% similar)
- [[2025-09-23/How Large Language Models are Designed to Hallucinate_20250923|How Large Language Models are Designed to Hallucinate]] (81.3% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (81.3% similar)
- [[2025-09-22/Automatic Lexical Simplification for Turkish_20250922|Automatic Lexical Simplification for Turkish]] (81.0% similar)
- [[2025-09-22/KatFishNet_ Detecting LLM-Generated Korean Text through Linguistic Feature Analysis_20250922|KatFishNet: Detecting LLM-Generated Korean Text through Linguistic Feature Analysis]] (80.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**⚡ Unique Technical**: [[keywords/Turk-LettuceDetect|Turk-LettuceDetect]], [[keywords/ModernBERT|ModernBERT]], [[keywords/RAGTruth Benchmark|RAGTruth Benchmark]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17671v1 Announce Type: cross 
Abstract: The widespread adoption of Large Language Models (LLMs) has been hindered by their tendency to hallucinate, generating plausible but factually incorrect information. While Retrieval-Augmented Generation (RAG) systems attempt to address this issue by grounding responses in external knowledge, hallucination remains a persistent challenge, particularly for morphologically complex, low-resource languages like Turkish. This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. Building on the LettuceDetect framework, we formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. Our experimental results show that the ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. Comparative analysis reveals that while state-of-the-art LLMs demonstrate high recall, they suffer from low precision due to over-generation of hallucinated content, underscoring the necessity of specialized detection mechanisms. By releasing our models and translated dataset, this work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages.

## 📝 요약

이 논문은 터키어와 같은 형태적으로 복잡하고 자원이 부족한 언어에서 발생하는 환각 문제를 해결하기 위해 터키어 RAG(Retrieval-Augmented Generation) 응용 프로그램에 특화된 환각 탐지 모델인 Turk-LettuceDetect를 소개합니다. LettuceDetect 프레임워크를 기반으로 환각 탐지를 토큰 수준의 분류 작업으로 정의하고, 터키어에 특화된 ModernBERT, TurkEmbed4STS, 다국어 EuroBERT 등 세 가지 인코더 아키텍처를 미세 조정했습니다. 이 모델들은 17,790개의 인스턴스를 포함한 RAGTruth 벤치마크 데이터셋의 기계 번역 버전으로 훈련되었습니다. 실험 결과, ModernBERT 기반 모델이 전체 테스트 세트에서 F1 점수 0.7266을 기록하며 특히 구조화된 작업에서 강력한 성능을 보였습니다. 모델들은 최대 8,192 토큰의 긴 문맥을 지원하면서도 계산 효율성을 유지하여 실시간 배포에 적합합니다. 비교 분석 결과, 최신 LLM들이 높은 재현율을 보이지만 환각된 콘텐츠의 과잉 생성으로 인해 낮은 정밀도를 나타내어, 특화된 탐지 메커니즘의 필요성을 강조합니다. 이 연구는 다국어 NLP의 중요한 격차를 해결하고, 터키어 및 다른 언어를 위한 신뢰할 수 있는 AI 응용 프로그램 개발의 기초를 마련합니다.

## 🎯 주요 포인트

- 1. 터키어 RAG 애플리케이션을 위한 최초의 환각 탐지 모델 모음인 Turk-LettuceDetect를 소개합니다.
- 2. 환각 탐지를 토큰 수준의 분류 작업으로 정의하고, 세 가지 인코더 아키텍처를 미세 조정했습니다: 터키어 전용 ModernBERT, TurkEmbed4STS, 다국어 EuroBERT.
- 3. ModernBERT 기반 모델은 전체 테스트 세트에서 F1 점수 0.7266을 달성하며, 특히 구조화된 작업에서 강력한 성능을 보였습니다.
- 4. 모델들은 최대 8,192 토큰의 긴 컨텍스트를 지원하면서도 계산 효율성을 유지하여 실시간 배포에 적합합니다.
- 5. 이 연구는 다국어 NLP에서 중요한 격차를 해결하고, 터키어 및 기타 언어를 위한 더 신뢰할 수 있는 AI 애플리케이션 개발의 기초를 마련합니다.


---

*Generated on 2025-09-24 00:05:53*