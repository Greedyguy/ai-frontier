---
keywords:
  - Neural Network
  - Excitation Pullbacks
  - Path Stability Hypothesis
  - Batch Normalization
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2507.22832
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:48:42.276596",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Network",
    "Excitation Pullbacks",
    "Path Stability Hypothesis",
    "Batch Normalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Network": 0.8,
    "Excitation Pullbacks": 0.78,
    "Path Stability Hypothesis": 0.75,
    "Batch Normalization": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "ReLU Networks",
        "canonical": "Neural Network",
        "aliases": [
          "Rectified Linear Unit Networks"
        ],
        "category": "broad_technical",
        "rationale": "ReLU Networks are a fundamental component of many neural network architectures, providing a strong link to general neural network discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "excitation pullbacks",
        "canonical": "Excitation Pullbacks",
        "aliases": [
          "modified gradients"
        ],
        "category": "unique_technical",
        "rationale": "This novel concept introduced in the paper provides a new perspective on gradient alignment and interpretability in neural networks.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "path stability hypothesis",
        "canonical": "Path Stability Hypothesis",
        "aliases": [
          "binary activation pattern stability"
        ],
        "category": "unique_technical",
        "rationale": "The hypothesis offers a theoretical framework that could link to discussions on neural network training dynamics.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Batch Normalization",
        "canonical": "Batch Normalization",
        "aliases": [
          "BN"
        ],
        "category": "specific_connectable",
        "rationale": "Batch Normalization is a widely used technique in deep learning, providing a strong connection to discussions on network training and generalization.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "piecewise affine",
      "intrinsic noise",
      "soft gating"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "ReLU Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "excitation pullbacks",
      "resolved_canonical": "Excitation Pullbacks",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "path stability hypothesis",
      "resolved_canonical": "Path Stability Hypothesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Batch Normalization",
      "resolved_canonical": "Batch Normalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Pulling Back the Curtain on ReLU Networks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2507.22832.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2507.22832](https://arxiv.org/abs/2507.22832)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Circuit realization and hardware linearization of monotone operator equilibrium networks_20250917|Circuit realization and hardware linearization of monotone operator equilibrium networks]] (83.2% similar)
- [[2025-09-23/Checking extracted rules in Neural Networks_20250923|Checking extracted rules in Neural Networks]] (82.7% similar)
- [[2025-09-23/Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks_20250923|Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks]] (82.1% similar)
- [[2025-09-22/Adversarial generalization of unfolding (model-based) networks_20250922|Adversarial generalization of unfolding (model-based) networks]] (82.0% similar)
- [[2025-09-23/Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs_20250923|Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Batch Normalization|Batch Normalization]]
**âš¡ Unique Technical**: [[keywords/Excitation Pullbacks|Excitation Pullbacks]], [[keywords/Path Stability Hypothesis|Path Stability Hypothesis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.22832v4 Announce Type: replace 
Abstract: Since any ReLU network is piecewise affine, its hidden units can be characterized by their pullbacks through the active subnetwork, i.e., by their gradients (up to bias terms). However, gradients of deeper neurons are notoriously misaligned, which obscures the network's internal representations. We posit that models do align gradients with data, yet this is concealed by the intrinsic noise of the ReLU hard gating. We validate this intuition by applying soft gating in the backward pass only, reducing the local impact of weakly excited neurons. The resulting modified gradients, which we call "excitation pullbacks", exhibit striking perceptual alignment on a number of ImageNet-pretrained architectures, while the rudimentary pixel-space gradient ascent quickly produces easily interpretable input- and target-specific features. Inspired by these findings, we formulate the "path stability" hypothesis, claiming that the binary activation patterns largely stabilize during training and get encoded in the pre-activation distribution of the final model. When true, excitation pullbacks become aligned with the gradients of a kernel machine that mainly determines the network's decision. This provides a theoretical justification for the apparent faithfulness of the feature attributions based on excitation pullbacks, potentially even leading to mechanistic interpretability of deep models. Incidentally, we give a possible explanation for the effectiveness of Batch Normalization and Deep Features, together with a novel perspective on the network's internal memory and generalization properties. We release the code and an interactive app for easier exploration of the excitation pullbacks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ReLU ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ í‘œí˜„ì„ ì´í•´í•˜ê¸° ìœ„í•´ "excitation pullbacks"ë¼ëŠ” ìˆ˜ì •ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ReLUì˜ í•˜ë“œ ê²Œì´íŒ…ìœ¼ë¡œ ì¸í•´ ê¹Šì€ ì¸µì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì˜ëª» ì •ë ¬ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì†Œí”„íŠ¸ ê²Œì´íŒ…ì„ ì ìš©í•˜ì—¬ ì•½í•˜ê²Œ í™œì„±í™”ëœ ë‰´ëŸ°ì˜ ì˜í–¥ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ImageNet ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì—ì„œ ëª…í™•í•œ ì‹œê°ì  ì •ë ¬ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, "ê²½ë¡œ ì•ˆì •ì„±" ê°€ì„¤ì„ ì œì‹œí•˜ì—¬ ì´ì§„ í™œì„±í™” íŒ¨í„´ì´ í›ˆë ¨ ì¤‘ ì•ˆì •í™”ë˜ë©°, ìµœì¢… ëª¨ë¸ì˜ ì‚¬ì „ í™œì„±í™” ë¶„í¬ì— ì¸ì½”ë”©ëœë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” Batch Normalizationê³¼ ë”¥ í”¼ì²˜ì˜ íš¨ê³¼ì„±ì„ ì„¤ëª…í•˜ê³ , ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ ë©”ëª¨ë¦¬ ë° ì¼ë°˜í™” íŠ¹ì„±ì— ëŒ€í•œ ìƒˆë¡œìš´ ê´€ì ì„ ì œê³µí•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ì½”ë“œì™€ ì¸í„°ë™í‹°ë¸Œ ì•±ë„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ReLU ë„¤íŠ¸ì›Œí¬ì˜ ìˆ¨ê²¨ì§„ ìœ ë‹›ì€ í™œì„± ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ í’€ë°±, ì¦‰ ê¸°ìš¸ê¸°ë¡œ íŠ¹ì§•ì§€ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ê¹Šì€ ì‹ ê²½ë§ì˜ ê¸°ìš¸ê¸°ëŠ” ì˜ëª» ì •ë ¬ë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ í‘œí˜„ì„ ëª¨í˜¸í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
- 3. ì†Œí”„íŠ¸ ê²Œì´íŒ…ì„ ì—­ì „íŒŒì—ë§Œ ì ìš©í•˜ì—¬ ì•½í•˜ê²Œ í™œì„±í™”ëœ ë‰´ëŸ°ì˜ ì˜í–¥ì„ ì¤„ì´ê³ , "í¥ë¶„ í’€ë°±"ì´ë¼ëŠ” ìˆ˜ì •ëœ ê¸°ìš¸ê¸°ë¥¼ í†µí•´ ì´ë¯¸ì§€ë„· ì‚¬ì „ í•™ìŠµ ì•„í‚¤í…ì²˜ì—ì„œ ì¸ì§€ì  ì •ë ¬ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- 4. "ê²½ë¡œ ì•ˆì •ì„±" ê°€ì„¤ì„ ì œì•ˆí•˜ì—¬ ì´ì§„ í™œì„±í™” íŒ¨í„´ì´ í›ˆë ¨ ì¤‘ì— ì•ˆì •í™”ë˜ê³  ìµœì¢… ëª¨ë¸ì˜ ì‚¬ì „ í™œì„±í™” ë¶„í¬ì— ì¸ì½”ë”©ëœë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.
- 5. ë°°ì¹˜ ì •ê·œí™”ì™€ ì‹¬ì¸µ íŠ¹ì§•ì˜ íš¨ê³¼ì„±ì— ëŒ€í•œ ì„¤ëª…ì„ ì œê³µí•˜ê³ , ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ ë©”ëª¨ë¦¬ ë° ì¼ë°˜í™” íŠ¹ì„±ì— ëŒ€í•œ ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:48:42*