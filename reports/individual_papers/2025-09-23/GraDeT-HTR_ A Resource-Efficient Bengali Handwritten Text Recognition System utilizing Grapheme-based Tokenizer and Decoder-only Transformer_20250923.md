---
keywords:
  - Grapheme-aware Decoder-only Transformer
  - Bengali Handwritten Text Recognition
  - Grapheme-based Tokenizer
  - Synthetic Data Pretraining
  - Attention Mechanism
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.18081
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:08:11.124558",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Grapheme-aware Decoder-only Transformer",
    "Bengali Handwritten Text Recognition",
    "Grapheme-based Tokenizer",
    "Synthetic Data Pretraining",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Grapheme-aware Decoder-only Transformer": 0.8,
    "Bengali Handwritten Text Recognition": 0.78,
    "Grapheme-based Tokenizer": 0.77,
    "Synthetic Data Pretraining": 0.72,
    "Attention Mechanism": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Grapheme-aware Decoder-only Transformer",
        "canonical": "Grapheme-aware Decoder-only Transformer",
        "aliases": [
          "GraDeT-HTR"
        ],
        "category": "unique_technical",
        "rationale": "This specific architecture addresses the unique challenges of Bengali script, providing a novel approach to handwritten text recognition.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Bengali Handwritten Text Recognition",
        "canonical": "Bengali Handwritten Text Recognition",
        "aliases": [
          "Bengali HTR"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specific application of handwritten text recognition tailored for Bengali, a language with unique script challenges.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Grapheme-based Tokenizer",
        "canonical": "Grapheme-based Tokenizer",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specialized tokenizer that improves recognition accuracy by addressing the complexity of Bengali script.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Synthetic Data Pretraining",
        "canonical": "Synthetic Data Pretraining",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Utilizing synthetic data for pretraining is a growing trend in machine learning, enhancing model performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A fundamental component in transformer models, crucial for understanding the architecture's operation.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "handwriting styles",
      "annotated datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Grapheme-aware Decoder-only Transformer",
      "resolved_canonical": "Grapheme-aware Decoder-only Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Bengali Handwritten Text Recognition",
      "resolved_canonical": "Bengali Handwritten Text Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Grapheme-based Tokenizer",
      "resolved_canonical": "Grapheme-based Tokenizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Synthetic Data Pretraining",
      "resolved_canonical": "Synthetic Data Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18081.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.18081](https://arxiv.org/abs/2509.18081)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime_20250923|Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime]] (84.5% similar)
- [[2025-09-23/Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation_20250923|Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation]] (80.6% similar)
- [[2025-09-23/Learning to Align_ Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition_20250923|Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition]] (79.4% similar)
- [[2025-09-19/BST_ Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports_20250919|BST: Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports]] (79.3% similar)
- [[2025-09-23/Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation_20250923|Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Synthetic Data Pretraining|Synthetic Data Pretraining]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Grapheme-aware Decoder-only Transformer|Grapheme-aware Decoder-only Transformer]], [[keywords/Bengali Handwritten Text Recognition|Bengali Handwritten Text Recognition]], [[keywords/Grapheme-based Tokenizer|Grapheme-based Tokenizer]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18081v1 Announce Type: new 
Abstract: Despite Bengali being the sixth most spoken language in the world, handwritten text recognition (HTR) systems for Bengali remain severely underdeveloped. The complexity of Bengali script--featuring conjuncts, diacritics, and highly variable handwriting styles--combined with a scarcity of annotated datasets makes this task particularly challenging. We present GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system based on a Grapheme-aware Decoder-only Transformer architecture. To address the unique challenges of Bengali script, we augment the performance of a decoder-only transformer by integrating a grapheme-based tokenizer and demonstrate that it significantly improves recognition accuracy compared to conventional subword tokenizers. Our model is pretrained on large-scale synthetic data and fine-tuned on real human-annotated samples, achieving state-of-the-art performance on multiple benchmark datasets.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì„¸ê³„ì—ì„œ ì—¬ì„¯ ë²ˆì§¸ë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ì¸ ë²µê³¨ì–´ì˜ í•„ê¸°ì²´ ì¸ì‹ ì‹œìŠ¤í…œ ê°œë°œì˜ ì–´ë ¤ì›€ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ë²µê³¨ì–´ì˜ ë³µì¡í•œ ë¬¸ì êµ¬ì¡°ì™€ ì œí•œëœ ì£¼ì„ ë°ì´í„°ì…‹ì´ ì£¼ìš” ë„ì „ ê³¼ì œë¡œ ì‘ìš©í•©ë‹ˆë‹¤. ì €ìë“¤ì€ Grapheme-aware Decoder-only Transformer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ GraDeT-HTR ì‹œìŠ¤í…œì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ìì†Œ ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•©í•˜ì—¬ ì¸ì‹ ì •í™•ë„ë¥¼ í–¥ìƒì‹œì¼°ìœ¼ë©°, ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ í›„ ì‹¤ì œ ì£¼ì„ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë²µê³¨ì–´ í•„ê¸°ì²´ ì¸ì‹ ì‹œìŠ¤í…œì€ ì—¬ì „íˆ ê°œë°œì´ ë¯¸í¡í•˜ì—¬, ë²µê³¨ì–´ í•„ê¸°ì²´ ì¸ì‹ì˜ ë³µì¡ì„±ê³¼ ë°ì´í„° ë¶€ì¡±ì´ ì£¼ìš” ë„ì „ ê³¼ì œë¡œ ì‘ìš©í•©ë‹ˆë‹¤.
- 2. GraDeT-HTRì€ Grapheme-aware Decoder-only Transformer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìì› íš¨ìœ¨ì ì¸ ë²µê³¨ì–´ í•„ê¸°ì²´ ì¸ì‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- 3. ìëª¨ ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•©í•˜ì—¬ ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, ì´ëŠ” ê¸°ì¡´ì˜ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ì— ë¹„í•´ ì¸ì‹ ì •í™•ë„ë¥¼ í¬ê²Œ ê°œì„ í•©ë‹ˆë‹¤.
- 4. ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„°ë¥¼ ì‚¬ì „ í•™ìŠµí•˜ê³  ì‹¤ì œ ì¸ê°„ ì£¼ì„ ìƒ˜í”Œë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:08:11*