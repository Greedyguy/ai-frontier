---
keywords:
  - Large Language Model
  - Zeroth-Order Optimization
  - Multi-Perturbed LoRA
  - Inference Engine
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2409.15520
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:35:06.721176",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Zeroth-Order Optimization",
    "Multi-Perturbed LoRA",
    "Inference Engine"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Zeroth-Order Optimization": 0.78,
    "Multi-Perturbed LoRA": 0.82,
    "Inference Engine": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on fine-tuning and edge deployment, linking to broader discussions on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "zeroth-order optimization",
        "canonical": "Zeroth-Order Optimization",
        "aliases": [
          "ZO optimization"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel optimization method crucial for the proposed framework, enhancing technical specificity.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multi-Perturbed LoRA",
        "canonical": "Multi-Perturbed LoRA",
        "aliases": [
          "MP-LoRA"
        ],
        "category": "unique_technical",
        "rationale": "A specific module innovation that is key to the framework's efficiency, offering a unique technical link.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "inference engines",
        "canonical": "Inference Engine",
        "aliases": [
          "ExecuTorch"
        ],
        "category": "specific_connectable",
        "rationale": "Inference engines are repurposed for fine-tuning, connecting to broader discussions on model deployment.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "zeroth-order optimization",
      "resolved_canonical": "Zeroth-Order Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multi-Perturbed LoRA",
      "resolved_canonical": "Multi-Perturbed LoRA",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "inference engines",
      "resolved_canonical": "Inference Engine",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2409.15520.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2409.15520](https://arxiv.org/abs/2409.15520)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (85.5% similar)
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (84.0% similar)
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (83.9% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (83.8% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Inference Engine|Inference Engine]]
**âš¡ Unique Technical**: [[keywords/Zeroth-Order Optimization|Zeroth-Order Optimization]], [[keywords/Multi-Perturbed LoRA|Multi-Perturbed LoRA]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2409.15520v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are currently pre-trained and fine-tuned on large cloud servers. The next frontier is LLM personalization, where a foundation model can be fine-tuned with user/task-specific data. Given the sensitive nature of such private data, it is desirable to fine-tune these models on edge devices to improve user trust. However, fine-tuning on resource-constrained edge devices presents significant challenges due to substantial memory and computational demands, as well as limited infrastructure support. We observe that inference engines (e.g., ExecuTorch) can be repurposed for fine-tuning by leveraging zeroth-order (ZO) optimization, which uses multiple forward passes to approximate gradients. While promising, direct application of ZO methods on edge devices is inefficient due to the high computational cost of multiple forward passes required for accurate gradient estimation, and their deployment has been largely unexplored in practice. We introduce MobiZO, a resource-efficient fine-tuning framework for LLMs specifically designed for edge devices. MobiZO combines three key innovations: (1) a parallelized randomized gradient estimator that employs both outer-loop and inner-loop parallelism to eliminate sequential forward passes, (2) a specialized Multi-Perturbed LoRA (MP-LoRA) module that enables efficient realization of both inner and outer loop parallelism, and (3) a seamless integration with ExecuTorch for on-device training, requiring no modifications to the runtime. Experiments demonstrate that MobiZO achieves substantial runtime speedups and memory savings while improving fine-tuning accuracy, paving the way for practical deployment of LLMs in real-time, on-device applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê°œì¸í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ëŒ€í˜• ì„œë²„ì—ì„œ ì‚¬ì „ í›ˆë ¨ ë° ë¯¸ì„¸ ì¡°ì •ì´ ì´ë£¨ì–´ì¡Œìœ¼ë‚˜, ì‚¬ìš©ì ì‹ ë¢°ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—£ì§€ ë””ë°”ì´ìŠ¤ì˜ ìì› í•œê³„ë¡œ ì¸í•´ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ MobiZOëŠ” ìì› íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì • í”„ë ˆì„ì›Œí¬ë¡œ, ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ì‹¤í–‰ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. MobiZOëŠ” ë³‘ë ¬í™”ëœ ë¬´ì‘ìœ„ ê¸°ìš¸ê¸° ì¶”ì •ê¸°, Multi-Perturbed LoRA ëª¨ë“ˆ, ê·¸ë¦¬ê³  ExecuTorchì™€ì˜ í†µí•©ì„ í†µí•´ ìì› ì†Œëª¨ë¥¼ ì¤„ì´ê³  ì •í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MobiZOëŠ” ì‹¤í–‰ ì†ë„ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ê°œì„ í•˜ë©°, ì‹¤ì‹œê°„ ë””ë°”ì´ìŠ¤ ìƒì˜ LLM í™œìš© ê°€ëŠ¥ì„±ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê°œì¸í™”ëŠ” ì‚¬ìš©ì/ì‘ì—…ë³„ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒìœ¼ë¡œ, ë¯¼ê°í•œ ë°ì´í„°ë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•´ ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
- 2. ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ë¯¸ì„¸ ì¡°ì •ì€ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ìš”êµ¬ëŸ‰ì´ í¬ê³  ì¸í”„ë¼ ì§€ì›ì´ ì œí•œì ì´ì–´ì„œ ë„ì „ ê³¼ì œê°€ ë©ë‹ˆë‹¤.
- 3. MobiZOëŠ” ì—£ì§€ ë””ë°”ì´ìŠ¤ì— íŠ¹í™”ëœ ìì› íš¨ìœ¨ì ì¸ LLM ë¯¸ì„¸ ì¡°ì • í”„ë ˆì„ì›Œí¬ë¡œ, ë³‘ë ¬í™”ëœ ë¬´ì‘ìœ„ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ê¸°ì™€ MP-LoRA ëª¨ë“ˆì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 4. MobiZOëŠ” ExecuTorchì™€ì˜ í†µí•©ì„ í†µí•´ ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ì‹¤ì‹œê°„, ì˜¨ë””ë°”ì´ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ëŸ°íƒ€ì„ ìˆ˜ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, MobiZOëŠ” ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•ê³¼ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ë¯¸ì„¸ ì¡°ì • ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:35:06*