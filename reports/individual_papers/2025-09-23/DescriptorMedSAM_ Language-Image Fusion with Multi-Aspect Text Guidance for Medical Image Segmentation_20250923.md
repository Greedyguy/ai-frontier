---
keywords:
  - DescriptorMedSAM
  - Zero-Shot Learning
  - Few-Shot Learning
  - Vision-Language Model
  - Attention Mechanism
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2503.13806
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:51:00.870471",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "DescriptorMedSAM",
    "Zero-Shot Learning",
    "Few-Shot Learning",
    "Vision-Language Model",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "DescriptorMedSAM": 0.88,
    "Zero-Shot Learning": 0.85,
    "Few-Shot Learning": 0.82,
    "Vision-Language Model": 0.8,
    "Attention Mechanism": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "DescriptorMedSAM",
        "canonical": "DescriptorMedSAM",
        "aliases": [
          "MedSAM with Descriptors"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel extension of MedSAM with structured text prompts for medical image segmentation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for evaluating the model's performance on unseen data.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "few-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the model's adaptability with minimal data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "vision-language"
        ],
        "category": "evolved_concepts",
        "rationale": "Highlights the integration of visual and textual data in the model.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cross-Attention Block",
        "canonical": "Attention Mechanism",
        "aliases": [
          "cross-attention"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a specific attention mechanism used for fusing text and visual data.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "FLARE 2022 dataset",
      "Dice score"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "DescriptorMedSAM",
      "resolved_canonical": "DescriptorMedSAM",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cross-Attention Block",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance for Medical Image Segmentation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.13806.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2503.13806](https://arxiv.org/abs/2503.13806)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/pFedSAM_ Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation_20250922|pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation]] (83.8% similar)
- [[2025-09-22/ENSAM_ an efficient foundation model for interactive segmentation of 3D medical images_20250922|ENSAM: an efficient foundation model for interactive segmentation of 3D medical images]] (83.6% similar)
- [[2025-09-22/RegionMed-CLIP_ A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding_20250922|RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding]] (82.7% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.6% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/DescriptorMedSAM|DescriptorMedSAM]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.13806v2 Announce Type: replace-cross 
Abstract: Accurate organ segmentation is essential for clinical tasks such as radiotherapy planning and disease monitoring. Recent foundation models like MedSAM achieve strong results using point or bounding-box prompts but still require manual interaction. We propose DescriptorMedSAM, a lightweight extension of MedSAM that incorporates structured text prompts, ranging from simple organ names to combined shape and location descriptors to enable click-free segmentation. DescriptorMedSAM employs a CLIP text encoder to convert radiology-style descriptors into dense embeddings, which are fused with visual tokens via a cross-attention block and a multi-scale feature extractor. We designed four descriptor types: Name (N), Name + Shape (NS), Name + Location (NL), and Name + Shape + Location (NSL), and evaluated them on the FLARE 2022 dataset under zero-shot and few-shot settings, where organs unseen during training must be segmented with minimal additional data. NSL prompts achieved the highest performance, with a Dice score of 0.9405 under full supervision, a 76.31% zero-shot retention ratio, and a 97.02% retention ratio after fine-tuning with only 50 labeled slices per unseen organ. Adding shape and location cues consistently improved segmentation accuracy, especially for small or morphologically complex structures. We demonstrate that structured language prompts can effectively replace spatial interactions, delivering strong zero-shot performance and rapid few-shot adaptation. By quantifying the role of descriptor, this work lays the groundwork for scalable, prompt-aware segmentation models that generalize across diverse anatomical targets with minimal annotation effort.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì„ìƒ ì‘ì—…ì—ì„œ ì¤‘ìš”í•œ ì¥ê¸° ë¶„í• ì„ ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ DescriptorMedSAMì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ MedSAM ëª¨ë¸ì„ í™•ì¥í•˜ì—¬, í´ë¦­ ì—†ì´ë„ ë¶„í• ì´ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë„ì…í•œ ê²ƒì…ë‹ˆë‹¤. DescriptorMedSAMì€ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°©ì‚¬ì„ í•™ ìŠ¤íƒ€ì¼ì˜ ì„¤ëª…ì„ ë°€ì§‘ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ì‹œê°ì  í† í°ê³¼ ê²°í•©í•˜ì—¬ ì¥ê¸° ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë„¤ ê°€ì§€ ì„¤ëª…ì ìœ í˜•(N, NS, NL, NSL)ì„ ì„¤ê³„í•˜ì—¬ FLARE 2022 ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•œ ê²°ê³¼, NSL í”„ë¡¬í”„íŠ¸ê°€ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ, í˜•íƒœì™€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ë©´ ë¶„í•  ì •í™•ë„ê°€ í–¥ìƒë˜ì—ˆìœ¼ë©°, ì´ëŠ” ì‘ì€ êµ¬ì¡°ë‚˜ ë³µì¡í•œ í˜•íƒœì—ì„œ ë‘ë“œëŸ¬ì¡ŒìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” êµ¬ì¡°í™”ëœ ì–¸ì–´ í”„ë¡¬í”„íŠ¸ê°€ ê³µê°„ì  ìƒí˜¸ì‘ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ë‹¤ì–‘í•œ í•´ë¶€í•™ì  ëª©í‘œì— ëŒ€í•´ ìµœì†Œí•œì˜ ì£¼ì„ ì‘ì—…ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ê°œë°œì˜ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DescriptorMedSAMì€ MedSAMì˜ ê²½ëŸ‰ í™•ì¥ ëª¨ë¸ë¡œ, êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ í´ë¦­ ì—†ì´ ì¥ê¸° ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 2. CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í™œìš©í•˜ì—¬ ë°©ì‚¬ì„ í•™ ìŠ¤íƒ€ì¼ì˜ ì„¤ëª…ì„ ë°€ì§‘ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì‹œê°ì  í† í°ê³¼ ê²°í•©í•˜ì—¬ ë¶„í•  ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
- 3. ë„¤ ê°€ì§€ ì„¤ëª…ì ìœ í˜•(N, NS, NL, NSL)ì„ ì„¤ê³„í–ˆìœ¼ë©°, íŠ¹íˆ NSL í”„ë¡¬í”„íŠ¸ê°€ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- 4. ëª¨ì–‘ ë° ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ë©´ ë¶„í•  ì •í™•ë„ê°€ í–¥ìƒë˜ë©°, íŠ¹íˆ ì‘ì€ êµ¬ì¡°ë¬¼ì´ë‚˜ í˜•íƒœê°€ ë³µì¡í•œ êµ¬ì¡°ë¬¼ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤.
- 5. êµ¬ì¡°í™”ëœ ì–¸ì–´ í”„ë¡¬í”„íŠ¸ëŠ” ê³µê°„ì  ìƒí˜¸ì‘ìš©ì„ ëŒ€ì²´í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ í•´ë¶€í•™ì  ëª©í‘œì— ëŒ€í•´ ìµœì†Œí•œì˜ ì£¼ì„ ì‘ì—…ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ê°œë°œì˜ ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:51:00*