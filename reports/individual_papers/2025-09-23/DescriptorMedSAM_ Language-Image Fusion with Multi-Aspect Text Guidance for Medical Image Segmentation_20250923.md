---
keywords:
  - DescriptorMedSAM
  - Zero-Shot Learning
  - Few-Shot Learning
  - Vision-Language Model
  - Attention Mechanism
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2503.13806
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:51:00.870471",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "DescriptorMedSAM",
    "Zero-Shot Learning",
    "Few-Shot Learning",
    "Vision-Language Model",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "DescriptorMedSAM": 0.88,
    "Zero-Shot Learning": 0.85,
    "Few-Shot Learning": 0.82,
    "Vision-Language Model": 0.8,
    "Attention Mechanism": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "DescriptorMedSAM",
        "canonical": "DescriptorMedSAM",
        "aliases": [
          "MedSAM with Descriptors"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel extension of MedSAM with structured text prompts for medical image segmentation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for evaluating the model's performance on unseen data.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "few-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the model's adaptability with minimal data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "vision-language"
        ],
        "category": "evolved_concepts",
        "rationale": "Highlights the integration of visual and textual data in the model.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cross-Attention Block",
        "canonical": "Attention Mechanism",
        "aliases": [
          "cross-attention"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a specific attention mechanism used for fusing text and visual data.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "FLARE 2022 dataset",
      "Dice score"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "DescriptorMedSAM",
      "resolved_canonical": "DescriptorMedSAM",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cross-Attention Block",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance for Medical Image Segmentation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.13806.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2503.13806](https://arxiv.org/abs/2503.13806)

## 🔗 유사한 논문
- [[2025-09-22/pFedSAM_ Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation_20250922|pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation]] (83.8% similar)
- [[2025-09-22/ENSAM_ an efficient foundation model for interactive segmentation of 3D medical images_20250922|ENSAM: an efficient foundation model for interactive segmentation of 3D medical images]] (83.6% similar)
- [[2025-09-22/RegionMed-CLIP_ A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding_20250922|RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding]] (82.7% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.6% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (82.1% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/DescriptorMedSAM|DescriptorMedSAM]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2503.13806v2 Announce Type: replace-cross 
Abstract: Accurate organ segmentation is essential for clinical tasks such as radiotherapy planning and disease monitoring. Recent foundation models like MedSAM achieve strong results using point or bounding-box prompts but still require manual interaction. We propose DescriptorMedSAM, a lightweight extension of MedSAM that incorporates structured text prompts, ranging from simple organ names to combined shape and location descriptors to enable click-free segmentation. DescriptorMedSAM employs a CLIP text encoder to convert radiology-style descriptors into dense embeddings, which are fused with visual tokens via a cross-attention block and a multi-scale feature extractor. We designed four descriptor types: Name (N), Name + Shape (NS), Name + Location (NL), and Name + Shape + Location (NSL), and evaluated them on the FLARE 2022 dataset under zero-shot and few-shot settings, where organs unseen during training must be segmented with minimal additional data. NSL prompts achieved the highest performance, with a Dice score of 0.9405 under full supervision, a 76.31% zero-shot retention ratio, and a 97.02% retention ratio after fine-tuning with only 50 labeled slices per unseen organ. Adding shape and location cues consistently improved segmentation accuracy, especially for small or morphologically complex structures. We demonstrate that structured language prompts can effectively replace spatial interactions, delivering strong zero-shot performance and rapid few-shot adaptation. By quantifying the role of descriptor, this work lays the groundwork for scalable, prompt-aware segmentation models that generalize across diverse anatomical targets with minimal annotation effort.

## 📝 요약

이 논문은 임상 작업에서 중요한 장기 분할을 위한 새로운 방법론인 DescriptorMedSAM을 제안합니다. 이는 기존의 MedSAM 모델을 확장하여, 클릭 없이도 분할이 가능하도록 구조화된 텍스트 프롬프트를 도입한 것입니다. DescriptorMedSAM은 CLIP 텍스트 인코더를 사용하여 방사선학 스타일의 설명을 밀집 임베딩으로 변환하고, 이를 시각적 토큰과 결합하여 장기 분할을 수행합니다. 네 가지 설명자 유형(N, NS, NL, NSL)을 설계하여 FLARE 2022 데이터셋에서 평가한 결과, NSL 프롬프트가 가장 높은 성능을 보였습니다. 특히, 형태와 위치 정보를 추가하면 분할 정확도가 향상되었으며, 이는 작은 구조나 복잡한 형태에서 두드러졌습니다. 이 연구는 구조화된 언어 프롬프트가 공간적 상호작용을 효과적으로 대체할 수 있음을 보여주며, 다양한 해부학적 목표에 대해 최소한의 주석 작업으로 일반화할 수 있는 모델 개발의 기초를 마련합니다.

## 🎯 주요 포인트

- 1. DescriptorMedSAM은 MedSAM의 경량 확장 모델로, 구조화된 텍스트 프롬프트를 통해 클릭 없이 장기 분할을 수행합니다.
- 2. CLIP 텍스트 인코더를 활용하여 방사선학 스타일의 설명을 밀집 임베딩으로 변환하고, 시각적 토큰과 결합하여 분할 정확도를 높입니다.
- 3. 네 가지 설명자 유형(N, NS, NL, NSL)을 설계했으며, 특히 NSL 프롬프트가 가장 높은 성능을 보여주었습니다.
- 4. 모양 및 위치 정보를 추가하면 분할 정확도가 향상되며, 특히 작은 구조물이나 형태가 복잡한 구조물에서 효과적입니다.
- 5. 구조화된 언어 프롬프트는 공간적 상호작용을 대체할 수 있으며, 다양한 해부학적 목표에 대해 최소한의 주석 작업으로 일반화할 수 있는 모델 개발의 기초를 제공합니다.


---

*Generated on 2025-09-24 00:51:00*