---
keywords:
  - Neural Attention Search
  - Transformer
  - Attention Mechanism
  - Key-Value Cache
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2502.13251
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:47:40.502354",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Attention Search",
    "Transformer",
    "Attention Mechanism",
    "Key-Value Cache"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Attention Search": 0.88,
    "Transformer": 0.8,
    "Attention Mechanism": 0.78,
    "Key-Value Cache": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Attention Search",
        "canonical": "Neural Attention Search",
        "aliases": [
          "NAtS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework for token importance evaluation in sequences, which is central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Core technology used in the framework, linking to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.95,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Central to the framework's operation, facilitating connections with other attention-based methods.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "KV cache",
        "canonical": "Key-Value Cache",
        "aliases": [
          "KV cache"
        ],
        "category": "unique_technical",
        "rationale": "Key component in reducing inference costs, relevant for efficiency-focused research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "sequence",
      "token",
      "step"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Attention Search",
      "resolved_canonical": "Neural Attention Search",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.95,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "KV cache",
      "resolved_canonical": "Key-Value Cache",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Neural Attention Search

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.13251.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2502.13251](https://arxiv.org/abs/2502.13251)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (82.4% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (81.0% similar)
- [[2025-09-22/Walk and Read Less_ Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning_20250922|Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning]] (80.8% similar)
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers]] (80.4% similar)
- [[2025-09-17/Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions_20250917|Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Neural Attention Search|Neural Attention Search]], [[keywords/Key-Value Cache|Key-Value Cache]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.13251v3 Announce Type: replace-cross 
Abstract: We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.

## ğŸ“ ìš”ì•½

Neural Attention Search (NAtS)ëŠ” ì‹œí€€ìŠ¤ ë‚´ ê° í† í°ì˜ ì¤‘ìš”ì„±ì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ê³ , íŠ¹ì • ë‹¨ê³„ ì´í›„ í•´ë‹¹ í† í°ì„ ì‚­ì œí•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ ì¶”ë¡  ì‹œ í•„ìš”í•œ KV ìºì‹œ í¬ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¤„ì—¬ ì¶”ë¡  ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. NAtSëŠ” ê¸€ë¡œë²Œ í† í°, ë¡œì»¬ í† í°, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í† í°ì˜ ì„¸ ê°€ì§€ í† í° ìœ í˜•ì„ í¬í•¨í•˜ëŠ” ê²€ìƒ‰ ê³µê°„ì„ ì„¤ê³„í•˜ì—¬, í•™ìŠµ ê°€ëŠ¥í•œ ì£¼ì˜ ë§ˆìŠ¤í¬ë¥¼ í†µí•´ ì•„í‚¤í…ì²˜ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ í† í° ìœ í˜• ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ê±°ë‚˜ ê¸°ì¡´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ì‹¤í—˜ì—ì„œ NAtSëŠ” ëª¨ë¸ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ í•„ìš”í•œ KV ìºì‹œ í¬ê¸°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Neural Attention Search (NAtS)ëŠ” ì‹œí€€ìŠ¤ ë‚´ ê° í† í°ì˜ ì¤‘ìš”ì„±ì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ë¶ˆí•„ìš”í•œ í† í°ì„ ì œê±°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. NAtSëŠ” ì¶”ë¡  ì‹œ í•„ìš”í•œ KV ìºì‹œ í¬ê¸°ë¥¼ ì¤„ì—¬ ì¶”ë¡  ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 3. ì„¸ ê°€ì§€ í† í° ìœ í˜•(ê¸€ë¡œë²Œ, ë¡œì»¬, ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)ì„ í¬í•¨í•œ ê²€ìƒ‰ ê³µê°„ì„ ì„¤ê³„í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
- 4. í•™ìŠµ ê°€ëŠ¥í•œ ì£¼ì˜ ë§ˆìŠ¤í¬ë¥¼ í†µí•´ í† í° ìœ í˜• ì •ë³´ë¥¼ ì•„í‚¤í…ì²˜ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, NAtSëŠ” ëª¨ë¸ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ KV ìºì‹œ í¬ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:47:40*