---
keywords:
  - Large Language Model
  - Irrelevant Context
  - Symbolic Reasoning Graphs
  - Stepwise Tree Search
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.18761
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:01:08.179623",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Irrelevant Context",
    "Symbolic Reasoning Graphs",
    "Stepwise Tree Search"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Irrelevant Context": 0.7,
    "Symbolic Reasoning Graphs": 0.65,
    "Stepwise Tree Search": 0.6
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus, linking to a broad technical category that connects with various NLP concepts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "irrelevant context",
        "canonical": "Irrelevant Context",
        "aliases": [
          "IC",
          "distracting context"
        ],
        "category": "unique_technical",
        "rationale": "A unique technical term introduced in the paper, critical for understanding the specific challenge addressed.",
        "novelty_score": 0.7,
        "connectivity_score": 0.5,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "symbolic reasoning graphs",
        "canonical": "Symbolic Reasoning Graphs",
        "aliases": [
          "reasoning graphs"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific method used in the paper, enhancing understanding of the experimental setup.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.65
      },
      {
        "surface": "stepwise tree search",
        "canonical": "Stepwise Tree Search",
        "aliases": [
          "tree search"
        ],
        "category": "unique_technical",
        "rationale": "Describes a novel approach proposed in the paper, crucial for linking to related search algorithms.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.7,
        "link_intent_score": 0.6
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "training models"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "irrelevant context",
      "resolved_canonical": "Irrelevant Context",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.5,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "symbolic reasoning graphs",
      "resolved_canonical": "Symbolic Reasoning Graphs",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "stepwise tree search",
      "resolved_canonical": "Stepwise Tree Search",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.7,
        "link_intent": 0.6
      }
    }
  ]
}
-->

# How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.18761.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.18761](https://arxiv.org/abs/2505.18761)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (86.8% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (86.5% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (86.3% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (86.0% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (86.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Irrelevant Context|Irrelevant Context]], [[keywords/Symbolic Reasoning Graphs|Symbolic Reasoning Graphs]], [[keywords/Stepwise Tree Search|Stepwise Tree Search]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.18761v2 Announce Type: replace-cross 
Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ê°•ê±´ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ GSM-DCë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. GSM-DCëŠ” ì²´ê³„ì ìœ¼ë¡œ ì œì–´ëœ ë¬´ê´€í•œ ë§¥ë½ì„ í¬í•¨í•˜ì—¬ ìƒì§•ì  ì¶”ë¡  ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•¨ìœ¼ë¡œì¨ ì—„ë°€í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, LLMì´ ë¬´ê´€í•œ ë§¥ë½ì— ë¯¼ê°í•˜ì—¬ ì¶”ë¡  ê²½ë¡œ ì„ íƒê³¼ ì‚°ìˆ  ì •í™•ë„ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°•ë ¥í•œ ë°©í•´ ìš”ì†Œë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒë˜ë©°, íŠ¹íˆ ë¶„í¬ ë‚´ ë° ë¶„í¬ ì™¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤. ë˜í•œ, í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ë‹¨ê³„ë³„ íŠ¸ë¦¬ ê²€ìƒ‰ì„ ì œì•ˆí•˜ì—¬ ë¶„í¬ ì™¸ ì¡°ê±´ì—ì„œì˜ ê°•ê±´ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GSM-DCëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ê°•ê±´ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ í•©ì„± ë²¤ì¹˜ë§ˆí¬ë¡œ, ì²´ê³„ì ìœ¼ë¡œ ì œì–´ëœ ë¬´ê´€í•œ ë§¥ë½ì„ í¬í•¨í•©ë‹ˆë‹¤.
- 2. GSM-DCëŠ” ì •í™•í•œ ë°©í•´ ìš”ì†Œ ì£¼ì…ì„ í†µí•´ ìƒì§•ì  ì¶”ë¡  ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ì—¬ ì—„ê²©í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ë¬´ê´€í•œ ë§¥ë½ì— ìƒë‹¹íˆ ë¯¼ê°í•˜ë©°, ì´ëŠ” ì¶”ë¡  ê²½ë¡œ ì„ íƒê³¼ ì‚°ìˆ  ì •í™•ë„ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.
- 4. ê°•ë ¥í•œ ë°©í•´ ìš”ì†Œë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ë©´, ë¶„í¬ ë‚´ ë° ë¶„í¬ ì™¸ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.
- 5. í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ë‹¨ê³„ë³„ íŠ¸ë¦¬ ê²€ìƒ‰ì„ ì œì•ˆí•˜ì—¬, ë¶„í¬ ì™¸ ì¡°ê±´ì—ì„œì˜ ê°•ê±´ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:01:08*