---
keywords:
  - Text-to-Video Generation
  - Continual Learning
  - Diffusion Models
  - Temporal Consistency Loss
  - Video Retrieval Module
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16956
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:39:46.633071",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text-to-Video Generation",
    "Continual Learning",
    "Diffusion Models",
    "Temporal Consistency Loss",
    "Video Retrieval Module"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text-to-Video Generation": 0.78,
    "Continual Learning": 0.8,
    "Diffusion Models": 0.77,
    "Temporal Consistency Loss": 0.75,
    "Video Retrieval Module": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Text-to-Video Generation",
        "canonical": "Text-to-Video Generation",
        "aliases": [
          "Text2Video",
          "T2V"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific and emerging field in generative AI, crucial for linking related works.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "Incremental Learning",
          "Lifelong Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for linking to works on adaptive learning systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Diffusion-based Models",
        "canonical": "Diffusion Models",
        "aliases": [
          "Diffusion Processes",
          "Stochastic Diffusion"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the proposed framework, connecting to stochastic processes in generative models.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Temporal Consistency Loss",
        "canonical": "Temporal Consistency Loss",
        "aliases": [
          "Time Consistency Loss",
          "Temporal Smoothness"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel mechanism for enhancing video generation quality.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Video Retrieval Module",
        "canonical": "Video Retrieval Module",
        "aliases": [
          "Video Retrieval System",
          "Video Search Module"
        ],
        "category": "unique_technical",
        "rationale": "Provides structural guidance, linking to retrieval systems in AI.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Text-to-Video Generation",
      "resolved_canonical": "Text-to-Video Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Diffusion-based Models",
      "resolved_canonical": "Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Temporal Consistency Loss",
      "resolved_canonical": "Temporal Consistency Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Video Retrieval Module",
      "resolved_canonical": "Video Retrieval Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# VidCLearn: A Continual Learning Approach for Text-to-Video Generation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16956.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16956](https://arxiv.org/abs/2509.16956)

## 🔗 유사한 논문
- [[2025-09-18/Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations_20250918|Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations]] (84.1% similar)
- [[2025-09-23/Learning Primitive Embodied World Models_ Towards Scalable Robotic Learning_20250923|Learning Primitive Embodied World Models: Towards Scalable Robotic Learning]] (82.0% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (81.7% similar)
- [[2025-09-22/CLIPTTA_ Robust Contrastive Vision-Language Test-Time Adaptation_20250922|CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation]] (81.0% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (81.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Continual Learning|Continual Learning]], [[keywords/Diffusion Models|Diffusion Models]]
**⚡ Unique Technical**: [[keywords/Text-to-Video Generation|Text-to-Video Generation]], [[keywords/Temporal Consistency Loss|Temporal Consistency Loss]], [[keywords/Video Retrieval Module|Video Retrieval Module]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16956v1 Announce Type: new 
Abstract: Text-to-video generation is an emerging field in generative AI, enabling the creation of realistic, semantically accurate videos from text prompts. While current models achieve impressive visual quality and alignment with input text, they typically rely on static knowledge, making it difficult to incorporate new data without retraining from scratch. To address this limitation, we propose VidCLearn, a continual learning framework for diffusion-based text-to-video generation. VidCLearn features a student-teacher architecture where the student model is incrementally updated with new text-video pairs, and the teacher model helps preserve previously learned knowledge through generative replay. Additionally, we introduce a novel temporal consistency loss to enhance motion smoothness and a video retrieval module to provide structural guidance at inference. Our architecture is also designed to be more computationally efficient than existing models while retaining satisfactory generation performance. Experimental results show VidCLearn's superiority over baseline methods in terms of visual quality, semantic alignment, and temporal coherence.

## 📝 요약

이 논문은 텍스트-비디오 생성 분야에서 새로운 접근법인 VidCLearn을 제안합니다. VidCLearn은 지속적인 학습을 통해 새로운 데이터 통합이 가능하며, 학생-교사 구조를 활용하여 학생 모델이 새로운 텍스트-비디오 쌍으로 점진적으로 업데이트됩니다. 교사 모델은 생성적 재생을 통해 이전에 학습한 지식을 보존합니다. 또한, 모션의 부드러움을 향상시키는 새로운 시간적 일관성 손실과 추론 시 구조적 지침을 제공하는 비디오 검색 모듈을 도입했습니다. 이 아키텍처는 기존 모델보다 계산 효율성이 높으면서도 우수한 생성 성능을 유지합니다. 실험 결과, VidCLearn은 시각적 품질, 의미적 정렬, 시간적 일관성 면에서 기존 방법들보다 뛰어남을 보여줍니다.

## 🎯 주요 포인트

- 1. 텍스트-비디오 생성은 텍스트 프롬프트로부터 현실적이고 의미적으로 정확한 비디오를 생성하는 생성 AI의 신흥 분야입니다.
- 2. VidCLearn은 확산 기반 텍스트-비디오 생성에 대한 지속 학습 프레임워크로, 학생-교사 구조를 통해 새로운 텍스트-비디오 쌍을 점진적으로 업데이트합니다.
- 3. 새로운 시간 일관성 손실을 도입하여 동작의 부드러움을 향상시키고, 비디오 검색 모듈을 통해 추론 시 구조적 지침을 제공합니다.
- 4. VidCLearn은 기존 모델보다 계산 효율성이 높으면서도 만족스러운 생성 성능을 유지하도록 설계되었습니다.
- 5. 실험 결과, VidCLearn은 시각적 품질, 의미적 정렬, 시간적 일관성 측면에서 기존 방법들보다 우수한 성능을 보였습니다.


---

*Generated on 2025-09-24 04:39:46*