---
keywords:
  - Text-to-Video Generation
  - Continual Learning
  - Diffusion Models
  - Temporal Consistency Loss
  - Video Retrieval Module
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16956
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:39:46.633071",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text-to-Video Generation",
    "Continual Learning",
    "Diffusion Models",
    "Temporal Consistency Loss",
    "Video Retrieval Module"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text-to-Video Generation": 0.78,
    "Continual Learning": 0.8,
    "Diffusion Models": 0.77,
    "Temporal Consistency Loss": 0.75,
    "Video Retrieval Module": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Text-to-Video Generation",
        "canonical": "Text-to-Video Generation",
        "aliases": [
          "Text2Video",
          "T2V"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific and emerging field in generative AI, crucial for linking related works.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "Incremental Learning",
          "Lifelong Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for linking to works on adaptive learning systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Diffusion-based Models",
        "canonical": "Diffusion Models",
        "aliases": [
          "Diffusion Processes",
          "Stochastic Diffusion"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the proposed framework, connecting to stochastic processes in generative models.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Temporal Consistency Loss",
        "canonical": "Temporal Consistency Loss",
        "aliases": [
          "Time Consistency Loss",
          "Temporal Smoothness"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel mechanism for enhancing video generation quality.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Video Retrieval Module",
        "canonical": "Video Retrieval Module",
        "aliases": [
          "Video Retrieval System",
          "Video Search Module"
        ],
        "category": "unique_technical",
        "rationale": "Provides structural guidance, linking to retrieval systems in AI.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Text-to-Video Generation",
      "resolved_canonical": "Text-to-Video Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Diffusion-based Models",
      "resolved_canonical": "Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Temporal Consistency Loss",
      "resolved_canonical": "Temporal Consistency Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Video Retrieval Module",
      "resolved_canonical": "Video Retrieval Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# VidCLearn: A Continual Learning Approach for Text-to-Video Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16956.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16956](https://arxiv.org/abs/2509.16956)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations_20250918|Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations]] (84.1% similar)
- [[2025-09-23/Learning Primitive Embodied World Models_ Towards Scalable Robotic Learning_20250923|Learning Primitive Embodied World Models: Towards Scalable Robotic Learning]] (82.0% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (81.7% similar)
- [[2025-09-22/CLIPTTA_ Robust Contrastive Vision-Language Test-Time Adaptation_20250922|CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation]] (81.0% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Continual Learning|Continual Learning]], [[keywords/Diffusion Models|Diffusion Models]]
**âš¡ Unique Technical**: [[keywords/Text-to-Video Generation|Text-to-Video Generation]], [[keywords/Temporal Consistency Loss|Temporal Consistency Loss]], [[keywords/Video Retrieval Module|Video Retrieval Module]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16956v1 Announce Type: new 
Abstract: Text-to-video generation is an emerging field in generative AI, enabling the creation of realistic, semantically accurate videos from text prompts. While current models achieve impressive visual quality and alignment with input text, they typically rely on static knowledge, making it difficult to incorporate new data without retraining from scratch. To address this limitation, we propose VidCLearn, a continual learning framework for diffusion-based text-to-video generation. VidCLearn features a student-teacher architecture where the student model is incrementally updated with new text-video pairs, and the teacher model helps preserve previously learned knowledge through generative replay. Additionally, we introduce a novel temporal consistency loss to enhance motion smoothness and a video retrieval module to provide structural guidance at inference. Our architecture is also designed to be more computationally efficient than existing models while retaining satisfactory generation performance. Experimental results show VidCLearn's superiority over baseline methods in terms of visual quality, semantic alignment, and temporal coherence.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„± ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ VidCLearnì„ ì œì•ˆí•©ë‹ˆë‹¤. VidCLearnì€ ì§€ì†ì ì¸ í•™ìŠµì„ í†µí•´ ìƒˆë¡œìš´ ë°ì´í„° í†µí•©ì´ ê°€ëŠ¥í•˜ë©°, í•™ìƒ-êµì‚¬ êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ í•™ìƒ ëª¨ë¸ì´ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìŒìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. êµì‚¬ ëª¨ë¸ì€ ìƒì„±ì  ì¬ìƒì„ í†µí•´ ì´ì „ì— í•™ìŠµí•œ ì§€ì‹ì„ ë³´ì¡´í•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ì…˜ì˜ ë¶€ë“œëŸ¬ì›€ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì‹œê°„ì  ì¼ê´€ì„± ì†ì‹¤ê³¼ ì¶”ë¡  ì‹œ êµ¬ì¡°ì  ì§€ì¹¨ì„ ì œê³µí•˜ëŠ” ë¹„ë””ì˜¤ ê²€ìƒ‰ ëª¨ë“ˆì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ ì•„í‚¤í…ì²˜ëŠ” ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ìš°ìˆ˜í•œ ìƒì„± ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, VidCLearnì€ ì‹œê°ì  í’ˆì§ˆ, ì˜ë¯¸ì  ì •ë ¬, ì‹œê°„ì  ì¼ê´€ì„± ë©´ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ë›°ì–´ë‚¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±ì€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í˜„ì‹¤ì ì´ê³  ì˜ë¯¸ì ìœ¼ë¡œ ì •í™•í•œ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„± AIì˜ ì‹ í¥ ë¶„ì•¼ì…ë‹ˆë‹¤.
- 2. VidCLearnì€ í™•ì‚° ê¸°ë°˜ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±ì— ëŒ€í•œ ì§€ì† í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, í•™ìƒ-êµì‚¬ êµ¬ì¡°ë¥¼ í†µí•´ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìŒì„ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
- 3. ìƒˆë¡œìš´ ì‹œê°„ ì¼ê´€ì„± ì†ì‹¤ì„ ë„ì…í•˜ì—¬ ë™ì‘ì˜ ë¶€ë“œëŸ¬ì›€ì„ í–¥ìƒì‹œí‚¤ê³ , ë¹„ë””ì˜¤ ê²€ìƒ‰ ëª¨ë“ˆì„ í†µí•´ ì¶”ë¡  ì‹œ êµ¬ì¡°ì  ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. VidCLearnì€ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìƒì„± ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, VidCLearnì€ ì‹œê°ì  í’ˆì§ˆ, ì˜ë¯¸ì  ì •ë ¬, ì‹œê°„ì  ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:39:46*