---
keywords:
  - Few-Shot Learning
  - Bayesian Scaling Law
  - Large Language Model
  - Safety Alignment
  - Instruction-Tuned Large Language Models
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2410.16531
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:40:48.983772",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "Bayesian Scaling Law",
    "Large Language Model",
    "Safety Alignment",
    "Instruction-Tuned Large Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.82,
    "Bayesian Scaling Law": 0.79,
    "Large Language Model": 0.88,
    "Safety Alignment": 0.81,
    "Instruction-Tuned Large Language Models": 0.83
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "In-context learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "In-context learning is closely related to Few-Shot Learning, enhancing connectivity with existing literature on model adaptability.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Bayesian scaling law",
        "canonical": "Bayesian Scaling Law",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term introduces a novel concept specific to the paper, offering a unique perspective on scaling laws.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "As a fundamental concept in the paper, it connects to a wide range of existing research on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      },
      {
        "surface": "Safety alignment",
        "canonical": "Safety Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term is crucial for linking discussions on ethical AI and model safety, a growing area of interest.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.81
      },
      {
        "surface": "Instruction-tuned LLMs",
        "canonical": "Instruction-Tuned Large Language Models",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "This concept represents an evolution in LLM training techniques, connecting to recent advancements in model tuning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.83
      }
    ],
    "ban_list_suggestions": [
      "task priors",
      "learning efficiency",
      "per-example probabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "In-context learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Bayesian scaling law",
      "resolved_canonical": "Bayesian Scaling Law",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Safety alignment",
      "resolved_canonical": "Safety Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Instruction-tuned LLMs",
      "resolved_canonical": "Instruction-Tuned Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.83
      }
    }
  ]
}
-->

# Bayesian scaling laws for in-context learning

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2410.16531.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2410.16531](https://arxiv.org/abs/2410.16531)

## 🔗 유사한 논문
- [[2025-09-22/Disentangling Latent Shifts of In-Context Learning with Weak Supervision_20250922|Disentangling Latent Shifts of In-Context Learning with Weak Supervision]] (82.9% similar)
- [[2025-09-22/KITE_ Kernelized and Information Theoretic Exemplars for In-Context Learning_20250922|KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning]] (81.0% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (80.7% similar)
- [[2025-09-17/Online Bayesian Risk-Averse Reinforcement Learning_20250917|Online Bayesian Risk-Averse Reinforcement Learning]] (80.1% similar)
- [[2025-09-23/Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling_20250923|Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling]] (80.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Bayesian Scaling Law|Bayesian Scaling Law]], [[keywords/Safety Alignment|Safety Alignment]]
**🚀 Evolved Concepts**: [[keywords/Instruction-Tuned Large Language Models|Instruction-Tuned Large Language Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2410.16531v4 Announce Type: replace-cross 
Abstract: In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a novel Bayesian scaling law for ICL. In experiments with \mbox{GPT-2} models of different sizes, our scaling law matches existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT or DPO to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then study ICL on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause suppressed behaviors to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.

## 📝 요약

이 논문은 인컨텍스트 학습(ICL)이 베이지안 학습자와 유사하게 작동함을 보여주어, ICL의 성능과 인컨텍스트 예시 수 사이의 상관관계를 설명합니다. 이를 통해 새로운 베이지안 스케일링 법칙을 제안하며, 이는 기존의 스케일링 법칙과 정확도에서 일치하면서도 과제 우선순위, 학습 효율성, 예시별 확률에 대한 해석 가능한 용어를 제공합니다. 실험에서는 GPT-2 모델을 사용하여 이 법칙을 검증하고, 안전성 정렬 연구를 위한 합성 데이터셋 실험을 수행합니다. 또한, 실제 세계의 명령 조정된 대형 언어 모델(LLM)에서 ICL의 효과를 분석하여, 억제된 행동이 다시 나타나는 조건을 정확히 예측함으로써 훈련 후 안전성 향상의 비효과성을 밝힙니다.

## 🎯 주요 포인트

- 1. 인컨텍스트 학습(ICL)은 훈련 업데이트 없이 복잡한 작업을 수행할 수 있는 강력한 기법이다.
- 2. ICL은 베이지안 학습자를 근사화하며, 이를 통해 새로운 베이지안 스케일링 법칙을 제시한다.
- 3. 다양한 크기의 GPT-2 모델 실험에서, 제안된 스케일링 법칙은 기존의 스케일링 법칙과 정확도가 일치하며, 해석 가능한 용어를 제공한다.
- 4. 제안된 스케일링 법칙은 억제된 모델 능력이 재출현하는 조건을 정확히 예측하여, LLM 안전성 향상에 대한 사후 훈련의 비효과성을 설명한다.
- 5. 실험 프로토콜에서는 SFT나 DPO를 사용해 기존 모델의 원치 않는 능력을 억제한 후, ICL을 통해 그 능력을 복원하는 실험을 수행한다.


---

*Generated on 2025-09-24 00:40:48*