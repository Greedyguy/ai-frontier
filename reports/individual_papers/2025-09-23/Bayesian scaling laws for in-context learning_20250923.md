---
keywords:
  - Few-Shot Learning
  - Bayesian Scaling Law
  - Large Language Model
  - Safety Alignment
  - Instruction-Tuned Large Language Models
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2410.16531
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:40:48.983772",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "Bayesian Scaling Law",
    "Large Language Model",
    "Safety Alignment",
    "Instruction-Tuned Large Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.82,
    "Bayesian Scaling Law": 0.79,
    "Large Language Model": 0.88,
    "Safety Alignment": 0.81,
    "Instruction-Tuned Large Language Models": 0.83
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "In-context learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "In-context learning is closely related to Few-Shot Learning, enhancing connectivity with existing literature on model adaptability.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Bayesian scaling law",
        "canonical": "Bayesian Scaling Law",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term introduces a novel concept specific to the paper, offering a unique perspective on scaling laws.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "As a fundamental concept in the paper, it connects to a wide range of existing research on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      },
      {
        "surface": "Safety alignment",
        "canonical": "Safety Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term is crucial for linking discussions on ethical AI and model safety, a growing area of interest.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.81
      },
      {
        "surface": "Instruction-tuned LLMs",
        "canonical": "Instruction-Tuned Large Language Models",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "This concept represents an evolution in LLM training techniques, connecting to recent advancements in model tuning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.83
      }
    ],
    "ban_list_suggestions": [
      "task priors",
      "learning efficiency",
      "per-example probabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "In-context learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Bayesian scaling law",
      "resolved_canonical": "Bayesian Scaling Law",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Safety alignment",
      "resolved_canonical": "Safety Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Instruction-tuned LLMs",
      "resolved_canonical": "Instruction-Tuned Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.83
      }
    }
  ]
}
-->

# Bayesian scaling laws for in-context learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2410.16531.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2410.16531](https://arxiv.org/abs/2410.16531)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Disentangling Latent Shifts of In-Context Learning with Weak Supervision_20250922|Disentangling Latent Shifts of In-Context Learning with Weak Supervision]] (82.9% similar)
- [[2025-09-22/KITE_ Kernelized and Information Theoretic Exemplars for In-Context Learning_20250922|KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning]] (81.0% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (80.7% similar)
- [[2025-09-17/Online Bayesian Risk-Averse Reinforcement Learning_20250917|Online Bayesian Risk-Averse Reinforcement Learning]] (80.1% similar)
- [[2025-09-23/Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling_20250923|Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling]] (80.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Bayesian Scaling Law|Bayesian Scaling Law]], [[keywords/Safety Alignment|Safety Alignment]]
**ğŸš€ Evolved Concepts**: [[keywords/Instruction-Tuned Large Language Models|Instruction-Tuned Large Language Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.16531v4 Announce Type: replace-cross 
Abstract: In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a novel Bayesian scaling law for ICL. In experiments with \mbox{GPT-2} models of different sizes, our scaling law matches existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT or DPO to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then study ICL on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause suppressed behaviors to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ(ICL)ì´ ë² ì´ì§€ì•ˆ í•™ìŠµìì™€ ìœ ì‚¬í•˜ê²Œ ì‘ë™í•¨ì„ ë³´ì—¬ì£¼ì–´, ICLì˜ ì„±ëŠ¥ê³¼ ì¸ì»¨í…ìŠ¤íŠ¸ ì˜ˆì‹œ ìˆ˜ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë² ì´ì§€ì•ˆ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ê¸°ì¡´ì˜ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ê³¼ ì •í™•ë„ì—ì„œ ì¼ì¹˜í•˜ë©´ì„œë„ ê³¼ì œ ìš°ì„ ìˆœìœ„, í•™ìŠµ íš¨ìœ¨ì„±, ì˜ˆì‹œë³„ í™•ë¥ ì— ëŒ€í•œ í•´ì„ ê°€ëŠ¥í•œ ìš©ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ì—ì„œëŠ” GPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ ë²•ì¹™ì„ ê²€ì¦í•˜ê³ , ì•ˆì „ì„± ì •ë ¬ ì—°êµ¬ë¥¼ ìœ„í•œ í•©ì„± ë°ì´í„°ì…‹ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë˜í•œ, ì‹¤ì œ ì„¸ê³„ì˜ ëª…ë ¹ ì¡°ì •ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ICLì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•˜ì—¬, ì–µì œëœ í–‰ë™ì´ ë‹¤ì‹œ ë‚˜íƒ€ë‚˜ëŠ” ì¡°ê±´ì„ ì •í™•íˆ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ í›ˆë ¨ í›„ ì•ˆì „ì„± í–¥ìƒì˜ ë¹„íš¨ê³¼ì„±ì„ ë°í™ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ(ICL)ì€ í›ˆë ¨ ì—…ë°ì´íŠ¸ ì—†ì´ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ê¸°ë²•ì´ë‹¤.
- 2. ICLì€ ë² ì´ì§€ì•ˆ í•™ìŠµìë¥¼ ê·¼ì‚¬í™”í•˜ë©°, ì´ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë² ì´ì§€ì•ˆ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì œì‹œí•œë‹¤.
- 3. ë‹¤ì–‘í•œ í¬ê¸°ì˜ GPT-2 ëª¨ë¸ ì‹¤í—˜ì—ì„œ, ì œì•ˆëœ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì€ ê¸°ì¡´ì˜ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ê³¼ ì •í™•ë„ê°€ ì¼ì¹˜í•˜ë©°, í•´ì„ ê°€ëŠ¥í•œ ìš©ì–´ë¥¼ ì œê³µí•œë‹¤.
- 4. ì œì•ˆëœ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì€ ì–µì œëœ ëª¨ë¸ ëŠ¥ë ¥ì´ ì¬ì¶œí˜„í•˜ëŠ” ì¡°ê±´ì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ì—¬, LLM ì•ˆì „ì„± í–¥ìƒì— ëŒ€í•œ ì‚¬í›„ í›ˆë ¨ì˜ ë¹„íš¨ê³¼ì„±ì„ ì„¤ëª…í•œë‹¤.
- 5. ì‹¤í—˜ í”„ë¡œí† ì½œì—ì„œëŠ” SFTë‚˜ DPOë¥¼ ì‚¬ìš©í•´ ê¸°ì¡´ ëª¨ë¸ì˜ ì›ì¹˜ ì•ŠëŠ” ëŠ¥ë ¥ì„ ì–µì œí•œ í›„, ICLì„ í†µí•´ ê·¸ ëŠ¥ë ¥ì„ ë³µì›í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•œë‹¤.


---

*Generated on 2025-09-24 00:40:48*