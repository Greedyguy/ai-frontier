---
keywords:
  - Large Language Model
  - Retrieval Augmented Generation
  - Few-Shot Learning
  - Zero-Shot Learning
  - Multimodal Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17197
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:48:24.674702",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Retrieval Augmented Generation",
    "Few-Shot Learning",
    "Zero-Shot Learning",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Retrieval Augmented Generation": 0.79,
    "Few-Shot Learning": 0.78,
    "Zero-Shot Learning": 0.77,
    "Multimodal Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the framework, LLMs offer general-purpose knowledge and reasoning capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Retrieval Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "RAG is a key component in the modular architecture for adaptive retrieval and generation.",
        "novelty_score": 0.58,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is highlighted for its superior performance in experimental results.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is emphasized for its effectiveness in the framework's evaluation.",
        "novelty_score": 0.52,
        "connectivity_score": 0.79,
        "specificity_score": 0.73,
        "link_intent_score": 0.77
      },
      {
        "surface": "Cross-Modal Reasoning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Cross-Modal",
          "Cross-Modal Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-modal reasoning is crucial for the framework's ability to handle different signal modalities.",
        "novelty_score": 0.6,
        "connectivity_score": 0.83,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "signal processing",
      "framework",
      "tasks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Retrieval Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.79,
        "specificity": 0.73,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Cross-Modal Reasoning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.83,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17197.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17197](https://arxiv.org/abs/2509.17197)

## 🔗 유사한 논문
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (84.9% similar)
- [[2025-09-23/Large Language Models as End-to-end Combinatorial Optimization Solvers_20250923|Large Language Models as End-to-end Combinatorial Optimization Solvers]] (83.8% similar)
- [[2025-09-23/LLMs as Layout Designers_ A Spatial Reasoning Perspective_20250923|LLMs as Layout Designers: A Spatial Reasoning Perspective]] (83.8% similar)
- [[2025-09-23/Question Answering with LLMs and Learning from Answer Sets_20250923|Question Answering with LLMs and Learning from Answer Sets]] (83.3% similar)
- [[2025-09-19/An LLM-based multi-agent framework for agile effort estimation_20250919|An LLM-based multi-agent framework for agile effort estimation]] (83.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17197v1 Announce Type: cross 
Abstract: Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.

## 📝 요약

이 논문은 신호 처리(SP) 작업의 자동화와 일반화를 위해 대형 언어 모델(LLM)을 활용한 SignalLLM 프레임워크를 제안합니다. 기존의 SP 파이프라인은 복잡한 워크플로우와 제한된 데이터로 인해 적응성과 일반화에 어려움을 겪습니다. SignalLLM은 LLM의 강력한 추론 능력과 다양한 지식 기반을 활용하여 SP 목표를 구조화된 하위 작업으로 분해하고, 적응형 검색 증강 생성(RAG)과 정제를 통해 계층적으로 계획합니다. 이를 통해 다양한 신호 모달리티와 데이터 조건에 맞는 문제 해결 전략을 유연하게 선택할 수 있습니다. 실험 결과, SignalLLM은 통신 및 감지 분야의 다섯 가지 대표 작업에서 기존 방법보다 뛰어난 성능을 보였으며, 특히 적은 데이터 환경에서도 우수한 성과를 나타냈습니다.

## 🎯 주요 포인트

- 1. SignalLLM은 일반 신호 처리 작업을 위한 최초의 범용 LLM 기반 에이전트 프레임워크로 소개됩니다.
- 2. SignalLLM은 고수준의 신호 처리 목표를 구조화된 하위 작업으로 분해하고, 적응형 검색 보강 생성(RAG) 및 정제를 통해 계층적 계획을 수행합니다.
- 3. 이 프레임워크는 다양한 신호 모달리티, 작업 유형 및 데이터 조건에 걸쳐 문제 해결 전략을 유연하게 선택할 수 있는 일반화된 설계를 제공합니다.
- 4. 실험 결과, SignalLLM은 특히 소수 샷 및 제로 샷 설정에서 기존의 전통적 및 LLM 기반 방법들보다 우수한 성능을 보였습니다.
- 5. SignalLLM의 다재다능함과 효과는 레이더 목표 탐지, 인간 활동 인식, 텍스트 압축 등 통신 및 센싱 분야의 다섯 가지 대표 작업을 통해 입증되었습니다.


---

*Generated on 2025-09-23 23:48:24*