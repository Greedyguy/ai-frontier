---
keywords:
  - RealBench Dataset
  - Multimodal Learning
  - Closed-Source Models
  - Open-Source Visual Models
  - Multi-Image Understanding
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17421
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:25:47.017690",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "RealBench Dataset",
    "Multimodal Learning",
    "Closed-Source Models",
    "Open-Source Visual Models",
    "Multi-Image Understanding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "RealBench Dataset": 0.8,
    "Multimodal Learning": 0.78,
    "Closed-Source Models": 0.72,
    "Open-Source Visual Models": 0.74,
    "Multi-Image Understanding": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "RealBench",
        "canonical": "RealBench Dataset",
        "aliases": [
          "Chinese Multi-image Dataset"
        ],
        "category": "unique_technical",
        "rationale": "RealBench is a novel dataset specifically designed for Chinese multi-image understanding, providing a unique resource for research in this area.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "multimodal LLMs",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal models"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is crucial for understanding and processing multiple forms of data, linking to existing research on integrating different modalities.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "closed-source models",
        "canonical": "Closed-Source Models",
        "aliases": [
          "proprietary models"
        ],
        "category": "unique_technical",
        "rationale": "Closed-source models are significant in the context of performance comparisons with open-source alternatives, highlighting differences in capabilities.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "open-source visual/video models",
        "canonical": "Open-Source Visual Models",
        "aliases": [
          "open-source video models"
        ],
        "category": "unique_technical",
        "rationale": "Open-source visual models are important for community-driven research and development, providing a contrast to closed-source models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.74
      },
      {
        "surface": "multi-image understanding",
        "canonical": "Multi-Image Understanding",
        "aliases": [
          "multi-image analysis"
        ],
        "category": "specific_connectable",
        "rationale": "Multi-Image Understanding is a key challenge in computer vision, linking to broader research on image processing and analysis.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "performance gap"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "RealBench",
      "resolved_canonical": "RealBench Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "multimodal LLMs",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "closed-source models",
      "resolved_canonical": "Closed-Source Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "open-source visual/video models",
      "resolved_canonical": "Open-Source Visual Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "multi-image understanding",
      "resolved_canonical": "Multi-Image Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17421.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17421](https://arxiv.org/abs/2509.17421)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Multi-Physics_ A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems_20250922|Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems]] (84.0% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (81.1% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (81.1% similar)
- [[2025-09-23/NUMINA_ A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities_20250923|NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities]] (80.8% similar)
- [[2025-09-22/A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs_ Experiments with OpenAI Models_20250922|A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Multi-Image Understanding|Multi-Image Understanding]]
**âš¡ Unique Technical**: [[keywords/RealBench Dataset|RealBench Dataset]], [[keywords/Closed-Source Models|Closed-Source Models]], [[keywords/Open-Source Visual Models|Open-Source Visual Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17421v1 Announce Type: new 
Abstract: While various multimodal multi-image evaluation datasets have been emerged, but these datasets are primarily based on English, and there has yet to be a Chinese multi-image dataset. To fill this gap, we introduce RealBench, the first Chinese multimodal multi-image dataset, which contains 9393 samples and 69910 images. RealBench distinguishes itself by incorporating real user-generated content, ensuring high relevance to real-world applications. Additionally, the dataset covers a wide variety of scenes, image resolutions, and image structures, further increasing the difficulty of multi-image understanding. Ultimately, we conduct a comprehensive evaluation of RealBench using 21 multimodal LLMs of different sizes, including closed-source models that support multi-image inputs as well as open-source visual and video models. The experimental results indicate that even the most powerful closed-source models still face challenges when handling multi-image Chinese scenarios. Moreover, there remains a noticeable performance gap of around 71.8\% on average between open-source visual/video models and closed-source models. These results show that RealBench provides an important research foundation for further exploring multi-image understanding capabilities in the Chinese context.

## ğŸ“ ìš”ì•½

RealBenchëŠ” ì¤‘êµ­ì–´ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ, 9393ê°œì˜ ìƒ˜í”Œê³¼ 69910ê°œì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì‹¤ì œ ì‚¬ìš©ì ìƒì„± ì½˜í…ì¸ ë¥¼ í¬í•¨í•˜ì—¬ í˜„ì‹¤ ì„¸ê³„ ì‘ìš©ì— ë†’ì€ ê´€ë ¨ì„±ì„ ì§€ë‹ˆë©°, ë‹¤ì–‘í•œ ì¥ë©´, ì´ë¯¸ì§€ í•´ìƒë„, ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ ë‹¤ë£¨ì–´ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´ì˜ ë‚œì´ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. 21ê°œì˜ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë©€í‹°ëª¨ë‹¬ LLMì„ ì‚¬ìš©í•œ í‰ê°€ ê²°ê³¼, ê°€ì¥ ê°•ë ¥í•œ íì‡„í˜• ëª¨ë¸ì¡°ì°¨ ì¤‘êµ­ì–´ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ ë¹„ì£¼ì–¼/ë¹„ë””ì˜¤ ëª¨ë¸ê³¼ íì‡„í˜• ëª¨ë¸ ê°„ í‰ê·  ì•½ 71.8%ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ ì¡´ì¬í•¨ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. RealBenchëŠ” ì¤‘êµ­ì–´ ë§¥ë½ì—ì„œ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ì„ íƒêµ¬í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—°êµ¬ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RealBenchëŠ” ì¤‘êµ­ì–´ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ, 9393ê°œì˜ ìƒ˜í”Œê³¼ 69910ê°œì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ì´ ë°ì´í„°ì…‹ì€ ì‹¤ì œ ì‚¬ìš©ì ìƒì„± ì½˜í…ì¸ ë¥¼ í¬í•¨í•˜ì—¬ í˜„ì‹¤ ì„¸ê³„ ì‘ìš©ì— ë†’ì€ ê´€ë ¨ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ ì¥ë©´, ì´ë¯¸ì§€ í•´ìƒë„, ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ í¬í•¨í•˜ì—¬ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´ì˜ ë‚œì´ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.
- 4. 21ê°œì˜ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë©€í‹°ëª¨ë‹¬ LLMì„ ì‚¬ìš©í•˜ì—¬ RealBenchë¥¼ í‰ê°€í•œ ê²°ê³¼, ê°•ë ¥í•œ íì‡„í˜• ëª¨ë¸ë„ ì¤‘êµ­ì–´ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì—¬ì „íˆ ë„ì „ì— ì§ë©´í•˜ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. ê°œë°©í˜• ë¹„ì£¼ì–¼/ë¹„ë””ì˜¤ ëª¨ë¸ê³¼ íì‡„í˜• ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ê²©ì°¨ëŠ” í‰ê·  ì•½ 71.8%ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:25:47*