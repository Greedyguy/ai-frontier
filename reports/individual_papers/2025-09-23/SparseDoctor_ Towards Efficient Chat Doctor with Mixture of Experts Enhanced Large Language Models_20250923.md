---
keywords:
  - Large Language Model
  - SparseDoctor
  - Contrastive Learning
  - Mixture of Experts
  - Low Rank Adaptation-Mixture of Experts
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.14269
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:31:32.467546",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "SparseDoctor",
    "Contrastive Learning",
    "Mixture of Experts",
    "Low Rank Adaptation-Mixture of Experts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "SparseDoctor": 0.7,
    "Contrastive Learning": 0.75,
    "Mixture of Experts": 0.7,
    "Low Rank Adaptation-Mixture of Experts": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus on enhancing language models for medical applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      },
      {
        "surface": "SparseDoctor",
        "canonical": "SparseDoctor",
        "aliases": [
          "Sparse Doctor"
        ],
        "category": "unique_technical",
        "rationale": "A novel model introduced in the paper, representing a unique contribution to the field.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      },
      {
        "surface": "Contrastive Learning",
        "canonical": "Contrastive Learning",
        "aliases": [
          "Contrastive"
        ],
        "category": "specific_connectable",
        "rationale": "This technique is crucial for enhancing the model's performance and is a key component of the proposed architecture.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Mixture of Experts",
        "canonical": "Mixture of Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "A significant architectural element that enhances the model's efficiency and effectiveness.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      },
      {
        "surface": "LoRA-MoE",
        "canonical": "Low Rank Adaptation-Mixture of Experts",
        "aliases": [
          "LoRA-MoE"
        ],
        "category": "unique_technical",
        "rationale": "A specific architecture proposed in the paper, combining low-rank adaptation with mixture of experts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "training cost",
      "memory overflow"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "SparseDoctor",
      "resolved_canonical": "SparseDoctor",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Contrastive Learning",
      "resolved_canonical": "Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Mixture of Experts",
      "resolved_canonical": "Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "LoRA-MoE",
      "resolved_canonical": "Low Rank Adaptation-Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.14269.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.14269](https://arxiv.org/abs/2509.14269)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (87.5% similar)
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (86.9% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (86.9% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (85.3% similar)
- [[2025-09-18/Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes_20250918|Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Contrastive Learning|Contrastive Learning]], [[keywords/Mixture of Experts|Mixture of Experts]]
**âš¡ Unique Technical**: [[keywords/SparseDoctor|SparseDoctor]], [[keywords/Low Rank Adaptation-Mixture of Experts|Low Rank Adaptation-Mixture of Experts]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14269v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tuning strategies on LLM require the updates of billions of parameters, substantially increasing the training cost, including the training time and utility cost. To enhance the efficiency and effectiveness of the current medical LLMs and explore the boundary of the representation capability of the LLMs on the medical domain, apart from the traditional fine-tuning strategies from the data perspective (i.e., supervised fine-tuning or reinforcement learning from human feedback), we instead craft a novel sparse medical LLM named SparseDoctor armed with contrastive learning enhanced LoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end, the crafted automatic routing mechanism can scientifically allocate the computational resources among different LoRA experts supervised by the contrastive learning. Additionally, we also introduce a novel expert memory queue mechanism to further boost the efficiency of the overall framework and prevent the memory overflow during training. We conduct comprehensive evaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med. Experimental results demonstrate that the proposed LLM can consistently outperform the strong baselines such as the HuatuoGPT series.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ SparseDoctorë¼ëŠ” ìƒˆë¡œìš´ í¬ì†Œ ì˜ë£Œ LLMì„ ì œì•ˆí•©ë‹ˆë‹¤. ì „í†µì ì¸ ë¯¸ì„¸ ì¡°ì • ë°©ì‹ì´ ì•„ë‹Œ, ëŒ€ì¡° í•™ìŠµì´ ê°•í™”ëœ LoRA-MoE(ì €ì°¨ì› ì ì‘-ì „ë¬¸ê°€ í˜¼í•©) ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìë™ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì´ ë‹¤ì–‘í•œ LoRA ì „ë¬¸ê°€ë“¤ ê°„ì˜ ê³„ì‚° ìì›ì„ ê³¼í•™ì ìœ¼ë¡œ ë°°ë¶„í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ ì „ë¬¸ê°€ ë©”ëª¨ë¦¬ í ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ í›ˆë ¨ ì¤‘ ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œë¥¼ ë°©ì§€í•˜ê³  íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. CMB, CMExam, CMMLU-Med ë“± ì„¸ ê°€ì§€ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ í‰ê°€ ê²°ê³¼, ì œì•ˆëœ ëª¨ë¸ì´ ê¸°ì¡´ ê°•ë ¥í•œ ëª¨ë¸ë“¤ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ì§ˆë¬¸ ì‘ë‹µ ë° ì„ìƒ ì˜ì‚¬ ê²°ì •ì— í° ì„±ê³µì„ ê±°ë‘ì—ˆìœ¼ë‚˜, ì „í†µì ì¸ ë¯¸ì„¸ ì¡°ì • ì „ëµì€ ë§‰ëŒ€í•œ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¡œ ì¸í•´ ë†’ì€ í›ˆë ¨ ë¹„ìš©ì„ ì´ˆë˜í•œë‹¤.
- 2. SparseDoctorë¼ëŠ” ìƒˆë¡œìš´ í¬ì†Œ ì˜ë£Œ LLMì€ ëŒ€ì¡° í•™ìŠµì´ ê°•í™”ëœ LoRA-MoE ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ ë†’ì´ê³ ì í•œë‹¤.
- 3. ìë™ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì€ ëŒ€ì¡° í•™ìŠµì— ì˜í•´ ê°ë…ë˜ëŠ” ë‹¤ì–‘í•œ LoRA ì „ë¬¸ê°€ë“¤ ê°„ì˜ ê³„ì‚° ìì›ì„ ê³¼í•™ì ìœ¼ë¡œ í• ë‹¹í•œë‹¤.
- 4. ìƒˆë¡œìš´ ì „ë¬¸ê°€ ë©”ëª¨ë¦¬ í ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ì „ì²´ í”„ë ˆì„ì›Œí¬ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê³  í›ˆë ¨ ì¤‘ ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œë¥¼ ë°©ì§€í•œë‹¤.
- 5. ì„¸ ê°€ì§€ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬(CMB, CMExam, CMMLU-Med)ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ LLMì€ HuatuoGPT ì‹œë¦¬ì¦ˆì™€ ê°™ì€ ê°•ë ¥í•œ ê¸°ì¤€ ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•œë‹¤.


---

*Generated on 2025-09-24 01:31:32*