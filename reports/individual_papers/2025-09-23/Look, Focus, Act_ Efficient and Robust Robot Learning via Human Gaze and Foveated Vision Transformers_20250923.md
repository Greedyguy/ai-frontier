---
keywords:
  - Transformer
  - Foveated Vision
  - Machine Learning
  - Human Gaze
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2507.15833
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:14:30.193612",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Foveated Vision",
    "Machine Learning",
    "Human Gaze"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Foveated Vision": 0.8,
    "Machine Learning": 0.7,
    "Human Gaze": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformers",
        "canonical": "Transformer",
        "aliases": [
          "ViTs"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in machine learning and are directly applicable to the discussed foveated vision approach.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "foveated vision",
        "canonical": "Foveated Vision",
        "aliases": [
          "foveated processing"
        ],
        "category": "unique_technical",
        "rationale": "Foveated vision is a unique approach to visual processing that mirrors human vision and is central to the paper's methodology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "robot learning",
        "canonical": "Machine Learning",
        "aliases": [
          "robotic learning"
        ],
        "category": "broad_technical",
        "rationale": "Machine learning is the broader category under which robot learning falls, linking to a wide range of related research.",
        "novelty_score": 0.2,
        "connectivity_score": 0.88,
        "specificity_score": 0.5,
        "link_intent_score": 0.7
      },
      {
        "surface": "human gaze",
        "canonical": "Human Gaze",
        "aliases": [
          "gaze tracking"
        ],
        "category": "unique_technical",
        "rationale": "Human gaze is a specific aspect of the study that informs the robot's visual processing, offering a novel approach to vision systems.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "robot vision system",
      "simulation benchmark"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "foveated vision",
      "resolved_canonical": "Foveated Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "robot learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.88,
        "specificity": 0.5,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "human gaze",
      "resolved_canonical": "Human Gaze",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.15833.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2507.15833](https://arxiv.org/abs/2507.15833)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (84.7% similar)
- [[2025-09-19/GAF_ Gaussian Action Field as a Dynamic World Model for Robotic Manipulation_20250919|GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation]] (84.4% similar)
- [[2025-09-23/KungfuBot2_ Learning Versatile Motion Skills for Humanoid Whole-Body Control_20250923|KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control]] (84.3% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (83.9% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Machine Learning|Machine Learning]]
**âš¡ Unique Technical**: [[keywords/Foveated Vision|Foveated Vision]], [[keywords/Human Gaze|Human Gaze]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.15833v2 Announce Type: replace-cross 
Abstract: Human vision is a highly active process driven by gaze, which directs attention to task-relevant regions through foveation, dramatically reducing visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance efficiency and robustness. We develop GIAVA (Gaze Integrated Active-Vision ALOHA), a robot vision system that emulates human head and neck movement, and gaze adjustment for foveated processing. Extending the AV-ALOHA robot platform, we introduce a framework for simultaneously collecting eye-tracking, perspective control, and robot manipulation demonstration data from a human operator. We also open-source a simulation benchmark and dataset for training robot policies that incorporate human gaze. Inspired by recent work in foveated image segmentation and given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme. Compared to uniform patch tokenization, this significantly reduces the number of tokens, and thus computation. Our results show that our method for foveated robot vision drastically reduces computational overhead, and enhances robustness to background distractors. Notably, on certain high-precision tasks, foveated vision also improves performance, as reflected in higher success rates. Together, these findings suggest that human-inspired foveated visual processing offers untapped potential and should be further considered as a useful inductive bias in robotic vision systems. https://ian-chuang.github.io/gaze-av-aloha/

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ì˜ ì‹œê°ì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë¡œë´‡ ë¹„ì „ ì‹œìŠ¤í…œì— ì ìš©í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—°êµ¬íŒ€ì€ ì¸ê°„ì˜ ì‹œì„  ì¡°ì ˆì„ ëª¨ë°©í•œ GIAVA ì‹œìŠ¤í…œì„ ê°œë°œí•˜ì—¬, AV-ALOHA í”Œë«í¼ì„ í™•ì¥í–ˆìŠµë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì¸ê°„ì˜ ì‹œì„  ì¶”ì  ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë¡œë´‡ ì •ì±…ì„ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬ì™€ ë°ì´í„°ì…‹ì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, Vision Transformers(ViTs)ì— ì‹œì„  ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê³ , ë°°ê²½ ë°©í•´ ìš”ì†Œì— ëŒ€í•œ ê²¬ê³ ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì€ ê³ ì •ë°€ ì‘ì—…ì—ì„œ ì„±ê³µë¥ ì„ ë†’ì´ë©°, ì¸ê°„ì˜ ì‹œê° ì²˜ë¦¬ ë°©ì‹ì´ ë¡œë´‡ ë¹„ì „ ì‹œìŠ¤í…œì— ìœ ìš©í•œ ê·€ë‚©ì  í¸í–¥ìœ¼ë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê°„ì˜ ì‹œì„  ê¸°ë°˜ ëŠ¥ë™ì  ì‹œê° ì²˜ë¦¬ë¥¼ ë¡œë´‡ ì •ì±…ì— í†µí•©í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤.
- 2. ì¸ê°„ì˜ ë¨¸ë¦¬ì™€ ëª© ì›€ì§ì„, ì‹œì„  ì¡°ì •ì„ ëª¨ë°©í•˜ëŠ” ë¡œë´‡ ë¹„ì „ ì‹œìŠ¤í…œ GIAVAë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- 3. ì¸ê°„ ì‹œì„ ì„ í†µí•©í•œ Vision Transformers(ViTs)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ëŸ‰ì„ í¬ê²Œ ì¤„ì´ê³ , ë°°ê²½ ë°©í•´ ìš”ì†Œì— ëŒ€í•œ ê²¬ê³ ì„±ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.
- 4. íŠ¹ì • ê³ ì •ë°€ ì‘ì—…ì—ì„œ ì‹œì„  ê¸°ë°˜ ì‹œê° ì²˜ë¦¬ê°€ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ ì„±ê³µë¥ ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
- 5. ì¸ê°„ ì˜ê°ì„ ë°›ì€ ì‹œì„  ê¸°ë°˜ ì‹œê° ì²˜ë¦¬ê°€ ë¡œë´‡ ë¹„ì „ ì‹œìŠ¤í…œì—ì„œ ìœ ìš©í•œ ê·€ë‚©ì  í¸í–¥ìœ¼ë¡œ ê³ ë ¤ë˜ì–´ì•¼ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:14:30*