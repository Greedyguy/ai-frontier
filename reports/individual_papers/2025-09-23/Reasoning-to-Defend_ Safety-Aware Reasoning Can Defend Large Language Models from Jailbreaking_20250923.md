---
keywords:
  - Large Language Model
  - Reasoning-to-Defend
  - Safety-aware Reasoning
  - Contrastive Pivot Optimization
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2502.12970
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:49:36.480772",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reasoning-to-Defend",
    "Safety-aware Reasoning",
    "Contrastive Pivot Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reasoning-to-Defend": 0.78,
    "Safety-aware Reasoning": 0.77,
    "Contrastive Pivot Optimization": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on enhancing safety through reasoning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reasoning-to-Defend",
        "canonical": "Reasoning-to-Defend",
        "aliases": [
          "R2D"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel paradigm specific to the paper's methodology.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Safety-aware Reasoning",
        "canonical": "Safety-aware Reasoning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A key concept that integrates safety into the reasoning process of LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Contrastive Pivot Optimization",
        "canonical": "Contrastive Pivot Optimization",
        "aliases": [
          "CPO"
        ],
        "category": "unique_technical",
        "rationale": "A specific method proposed to enhance the model's safety perception.",
        "novelty_score": 0.82,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "jailbreak",
      "safety",
      "attack"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reasoning-to-Defend",
      "resolved_canonical": "Reasoning-to-Defend",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Safety-aware Reasoning",
      "resolved_canonical": "Safety-aware Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Contrastive Pivot Optimization",
      "resolved_canonical": "Contrastive Pivot Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.12970.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2502.12970](https://arxiv.org/abs/2502.12970)

## 🔗 유사한 논문
- [[2025-09-23/D-REX_ A Benchmark for Detecting Deceptive Reasoning in Large Language Models_20250923|D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models]] (88.1% similar)
- [[2025-09-18/Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs_20250918|Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs]] (86.8% similar)
- [[2025-09-22/AdaSteer_ Your Aligned LLM is Inherently an Adaptive Jailbreak Defender_20250922|AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender]] (86.5% similar)
- [[2025-09-23/AdaptiveGuard_ Towards Adaptive Runtime Safety for LLM-Powered Software_20250923|AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software]] (86.4% similar)
- [[2025-09-23/Targeting Alignment_ Extracting Safety Classifiers of Aligned LLMs_20250923|Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs]] (86.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Reasoning-to-Defend|Reasoning-to-Defend]], [[keywords/Safety-aware Reasoning|Safety-aware Reasoning]], [[keywords/Contrastive Pivot Optimization|Contrastive Pivot Optimization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.12970v3 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) have recently demonstrated impressive performances across diverse domains. However, how the safety of Large Language Models (LLMs) benefits from enhanced reasoning capabilities against jailbreak queries remains unexplored. To bridge this gap, in this paper, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates a safety-aware reasoning mechanism into LLMs' generation process. This enables self-evaluation at each step of the reasoning process, forming safety pivot tokens as indicators of the safety status of responses. Furthermore, in order to improve the accuracy of predicting pivot tokens, we propose Contrastive Pivot Optimization (CPO), which enhances the model's perception of the safety status of given dialogues. LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their safety capabilities defending jailbreak attacks. Extensive experiments demonstrate that R2D effectively mitigates various attacks and improves overall safety, while maintaining the original performances. This highlights the substantial potential of safety-aware reasoning in improving robustness of LRMs and LLMs against various jailbreaks.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 안전성을 향상시키기 위해 Reasoning-to-Defend (R2D)라는 새로운 훈련 패러다임을 제안합니다. R2D는 안전 인식 추론 메커니즘을 LLM의 생성 과정에 통합하여 각 추론 단계에서 자체 평가를 수행하고, 응답의 안전 상태를 나타내는 피벗 토큰을 형성합니다. 또한, 대조 피벗 최적화(CPO)를 통해 피벗 토큰의 예측 정확성을 높여 대화의 안전 상태에 대한 모델의 인식을 개선합니다. 이를 통해 LLM은 추론 중 응답 전략을 동적으로 조정하여 탈옥 공격에 대한 방어 능력을 크게 강화합니다. 실험 결과, R2D는 다양한 공격을 효과적으로 완화하고 전반적인 안전성을 향상시키면서도 기존 성능을 유지함을 보여줍니다. 이는 LLM과 LRM의 견고성을 높이는 데 있어 안전 인식 추론의 잠재력을 강조합니다.

## 🎯 주요 포인트

- 1. 대규모 추론 모델(LRMs)은 다양한 분야에서 뛰어난 성능을 보여주고 있지만, 대규모 언어 모델(LLMs)의 안전성이 강화된 추론 능력으로 인해 어떻게 향상되는지는 아직 탐구되지 않았습니다.
- 2. 본 논문에서는 Reasoning-to-Defend (R2D)라는 새로운 훈련 패러다임을 제안하여, LLMs의 생성 과정에 안전 인식 추론 메커니즘을 통합합니다.
- 3. R2D는 각 추론 단계에서 자기 평가를 가능하게 하여, 응답의 안전 상태를 나타내는 안전 피벗 토큰을 형성합니다.
- 4. 대조 피벗 최적화(CPO)를 제안하여 주어진 대화의 안전 상태에 대한 모델의 인식을 향상시키고, 피벗 토큰 예측의 정확성을 높입니다.
- 5. R2D는 다양한 공격을 효과적으로 완화하고, 원래의 성능을 유지하면서 전반적인 안전성을 향상시킵니다.


---

*Generated on 2025-09-24 03:49:36*