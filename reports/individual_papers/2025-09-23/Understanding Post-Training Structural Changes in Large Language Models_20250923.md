---
keywords:
  - Large Language Model
  - Instruction Tuning
  - Long-CoT Distillation
  - Singular Value Decomposition
  - Attention Mechanism
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17866
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:11:24.370883",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Instruction Tuning",
    "Long-CoT Distillation",
    "Singular Value Decomposition",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Instruction Tuning": 0.78,
    "Long-CoT Distillation": 0.82,
    "Singular Value Decomposition": 0.77,
    "Attention Mechanism": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to foundational concepts in NLP and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Instruction Tuning",
        "canonical": "Instruction Tuning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific post-training method analyzed in the paper, relevant for understanding LLM adaptation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Long-Chain-of-Thought Distillation",
        "canonical": "Long-CoT Distillation",
        "aliases": [
          "Long-CoT"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel post-training technique critical to the paper's findings.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Singular Value Decomposition",
        "canonical": "Singular Value Decomposition",
        "aliases": [
          "SVD"
        ],
        "category": "specific_connectable",
        "rationale": "Key analytical method used to understand structural changes in LLMs.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Fundamental to understanding the modulation of attention scores in LLMs.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "post-training",
      "parameter space",
      "performance degradation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Instruction Tuning",
      "resolved_canonical": "Instruction Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Long-Chain-of-Thought Distillation",
      "resolved_canonical": "Long-CoT Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Singular Value Decomposition",
      "resolved_canonical": "Singular Value Decomposition",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Understanding Post-Training Structural Changes in Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17866.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17866](https://arxiv.org/abs/2509.17866)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Natural Fingerprints of Large Language Models_20250922|Natural Fingerprints of Large Language Models]] (83.4% similar)
- [[2025-09-23/Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels_20250923|Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels]] (83.3% similar)
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (82.6% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (82.6% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Singular Value Decomposition|Singular Value Decomposition]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Instruction Tuning|Instruction Tuning]], [[keywords/Long-CoT Distillation|Long-CoT Distillation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17866v1 Announce Type: cross 
Abstract: Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two consistent and unexpected structural changes:(1) a near-uniform geometric scaling of singular values across layers, which theoretically modulates attention scores; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Disrupting this orthogonal consistency leads to catastrophic performance degradation. Based on these findings, we propose a simple yet effective framework that interprets post-training as a reparameterization of fixed subspaces in the pretrained parameter space. Further experiments reveal that singular value scaling behaves as a secondary effect, analogous to a temperature adjustment, whereas the core functional transformation lies in the coordinated rotation of singular vectors. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬í›„ í›ˆë ¨ì´ ë‚´ë¶€ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì‚¬í›„ í›ˆë ¨ ë°©ë²•ì¸ ì§€ì‹œ ì¡°ì •ê³¼ ê¸´ ì‚¬ê³  ì²´ì¸(Long-CoT) ì¦ë¥˜ì— ì´ˆì ì„ ë§ì¶°, ì£¼ìš” ì„ í˜• ê³„ì¸µì˜ íŠ¹ì´ê°’ ë¶„í•´(SVD)ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, ê³„ì¸µ ì „ë°˜ì— ê±¸ì¹œ íŠ¹ì´ê°’ì˜ ê±°ì˜ ê· ì¼í•œ ê¸°í•˜í•™ì  ìŠ¤ì¼€ì¼ë§ê³¼ ê° í–‰ë ¬ì˜ ì¢Œìš° íŠ¹ì´ ë²¡í„°ì— ì¼ê´€ëœ ì§êµ ë³€í™˜ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì§êµ ì¼ê´€ì„±ì„ ë°©í•´í•˜ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì‚¬í›„ í›ˆë ¨ì„ ì‚¬ì „ í›ˆë ¨ëœ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì˜ ê³ ì •ëœ ë¶€ë¶„ ê³µê°„ì˜ ì¬ë§¤ê°œë³€ìˆ˜í™”ë¡œ í•´ì„í•˜ëŠ” ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, íŠ¹ì´ê°’ ìŠ¤ì¼€ì¼ë§ì€ ì˜¨ë„ ì¡°ì •ê³¼ ìœ ì‚¬í•œ 2ì°¨ íš¨ê³¼ì´ë©°, í•µì‹¬ ê¸°ëŠ¥ì  ë³€í™˜ì€ íŠ¹ì´ ë²¡í„°ì˜ ì¡°ì •ëœ íšŒì „ì— ìˆìŒì„ ë°í˜”ìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì„ ë¸”ë™ë°•ìŠ¤ë¡œ ë³´ëŠ” ê¸°ì¡´ ê´€ì ì„ ë„ì „í•˜ë©°, ë§¤ê°œë³€ìˆ˜ ë³€í™”ì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‚¬í›„ í›ˆë ¨ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‚´ë¶€ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì— ì˜ˆìƒì¹˜ ëª»í•œ êµ¬ì¡°ì  ë³€í™”ë¥¼ ì´ˆë˜í•œë‹¤.
- 2. íŠ¹ì´ê°’ ë¶„í•´(SVD) ë¶„ì„ ê²°ê³¼, ì¸µ ì „ë°˜ì— ê±¸ì³ ê±°ì˜ ê· ì¼í•œ ê¸°í•˜í•™ì  ìŠ¤ì¼€ì¼ë§ì´ ë°œìƒí•˜ë©° ì´ëŠ” ì£¼ì˜(attention) ì ìˆ˜ë¥¼ ì¡°ì ˆí•œë‹¤.
- 3. ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ íŠ¹ì´ ë²¡í„°ì— ì¼ê´€ëœ ì§êµ ë³€í™˜ì´ ì ìš©ë˜ë©°, ì´ ì¼ê´€ì„±ì´ ê¹¨ì§€ë©´ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ëœë‹¤.
- 4. ì‚¬í›„ í›ˆë ¨ì€ ì‚¬ì „ í›ˆë ¨ëœ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì˜ ê³ ì •ëœ ë¶€ë¶„ ê³µê°„ì„ ì¬ë§¤ê°œë³€ìˆ˜í™”í•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤.
- 5. íŠ¹ì´ê°’ ìŠ¤ì¼€ì¼ë§ì€ ë¶€ì°¨ì  íš¨ê³¼ë¡œ ì‘ìš©í•˜ë©°, íŠ¹ì´ ë²¡í„°ì˜ íšŒì „ì´ í•µì‹¬ ê¸°ëŠ¥ì  ë³€í™”ë¥¼ ì´ëˆë‹¤.


---

*Generated on 2025-09-24 00:11:24*