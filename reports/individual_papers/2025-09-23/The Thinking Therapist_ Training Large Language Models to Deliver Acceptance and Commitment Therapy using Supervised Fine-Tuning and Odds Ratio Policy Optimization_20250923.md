---
keywords:
  - Acceptance and Commitment Therapy
  - Large Language Model
  - Odds Ratio Policy Optimization
  - Supervised Fine-Tuning
  - Chain-of-Thought reasoning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.09712
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:27:30.197832",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Acceptance and Commitment Therapy",
    "Large Language Model",
    "Odds Ratio Policy Optimization",
    "Supervised Fine-Tuning",
    "Chain-of-Thought reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Acceptance and Commitment Therapy": 0.85,
    "Large Language Model": 0.8,
    "Odds Ratio Policy Optimization": 0.78,
    "Supervised Fine-Tuning": 0.72,
    "Chain-of-Thought reasoning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Acceptance and Commitment Therapy",
        "canonical": "Acceptance and Commitment Therapy",
        "aliases": [
          "ACT"
        ],
        "category": "unique_technical",
        "rationale": "Central to the study, linking to psychological therapy concepts.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Key technology used in the study, relevant for linking to AI concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Odds Ratio Policy Optimization",
        "canonical": "Odds Ratio Policy Optimization",
        "aliases": [
          "ORPO"
        ],
        "category": "unique_technical",
        "rationale": "A novel training method explored in the study, significant for linking to machine learning optimization techniques.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "specific_connectable",
        "rationale": "A common technique in machine learning, relevant for connecting to training methodologies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Chain-of-Thought reasoning",
        "canonical": "Chain-of-Thought reasoning",
        "aliases": [
          "COT reasoning"
        ],
        "category": "unique_technical",
        "rationale": "A reasoning method evaluated in the study, important for linking to cognitive processing techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "therapy sessions",
      "performance evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Acceptance and Commitment Therapy",
      "resolved_canonical": "Acceptance and Commitment Therapy",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Odds Ratio Policy Optimization",
      "resolved_canonical": "Odds Ratio Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Chain-of-Thought reasoning",
      "resolved_canonical": "Chain-of-Thought reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.09712.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.09712](https://arxiv.org/abs/2509.09712)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression_ A Feasibility Study_20250923|Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression: A Feasibility Study]] (85.9% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (83.6% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (82.9% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (82.5% similar)
- [[2025-09-23/Correlation or Causation_ Analyzing the Causal Structures of LLM and LRM Reasoning Process_20250923|Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Acceptance and Commitment Therapy|Acceptance and Commitment Therapy]], [[keywords/Odds Ratio Policy Optimization|Odds Ratio Policy Optimization]], [[keywords/Chain-of-Thought reasoning|Chain-of-Thought reasoning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.09712v2 Announce Type: replace-cross 
Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral therapy with emerging evidence of efficacy in several psychiatric conditions. This study investigates the impact of post-training methodology and explicit reasoning on the ability of a small open-weight large language model (LLM) to deliver ACT. Using synthetic ACT transcripts generated by Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches, supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each with and without an explicit chain-of-thought (COT) reasoning step. Performance was evaluated by comparing these four post-trained variants against the base Instruct model. These models were benchmarked in simulated therapy sessions, with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned on human evaluations. Our findings demonstrate that the ORPO-trained models significantly outperformed both their SFT and Instruct counterparts on ACT fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) = 140.37, p < .001$). The effect of COT was conditional as it provided a significant benefit to SFT models, improving ACT-FM scores by an average of 2.68 points ($p < .001$), while offering no discernible advantage to the superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO stems from its ability to learn the therapeutic `process' over imitating `content,' a key aspect of ACT, while COT acts as a necessary scaffold for models trained only via imitation. This study establishes that preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and that the utility of explicit reasoning is highly dependent on the underlying training paradigm.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” Acceptance and Commitment Therapy(ACT)ë¥¼ ì „ë‹¬í•˜ëŠ” ë° ìˆì–´ ì†Œí˜• ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. Mistral-Largeê°€ ìƒì„±í•œ í•©ì„± ACT ëŒ€ë³¸ì„ ì‚¬ìš©í•˜ì—¬ Llama-3.2-3b-Instructë¥¼ ê°ë…ëœ ë¯¸ì„¸ ì¡°ì •(SFT)ê³¼ ë¹„ìœ¨ ìµœì í™”(ORPO) ë°©ë²•ìœ¼ë¡œ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ë‘ ë°©ë²• ëª¨ë‘ ëª…ì‹œì ì¸ ì‚¬ê³  ê³¼ì •(COT) ìœ ë¬´ì— ë”°ë¼ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ACT ì¶©ì‹¤ë„ì™€ ì¹˜ë£Œì  ê³µê° ëŠ¥ë ¥ì„ ì¸¡ì •í•œ ê²°ê³¼, ORPOë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ SFTì™€ ê¸°ë³¸ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. COTëŠ” SFT ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìœ¼ë‚˜, ORPO ëª¨ë¸ì—ëŠ” ë³„ë‹¤ë¥¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” ORPOê°€ ACTì˜ 'ê³¼ì •' í•™ìŠµì— íš¨ê³¼ì ì´ë©°, COTëŠ” ëª¨ë°© í•™ìŠµ ëª¨ë¸ì— í•„ìš”í•˜ë‹¤ê³  ê²°ë¡ ì§€ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Acceptance and Commitment Therapy (ACT)ëŠ” ì—¬ëŸ¬ ì •ì‹ ê³¼ì  ìƒíƒœì—ì„œ íš¨ê³¼ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚œ ì œ3ì„¸ëŒ€ ì¸ì§€ í–‰ë™ ì¹˜ë£Œë²•ì…ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ì†Œí˜• ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ACTë¥¼ ì „ë‹¬í•˜ëŠ” ëŠ¥ë ¥ì— ëŒ€í•œ í›ˆë ¨ í›„ ë°©ë²•ë¡ ê³¼ ëª…ì‹œì  ì¶”ë¡ ì˜ ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.
- 3. ORPO(odds ratio policy optimization)ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ACT ì¶©ì‹¤ë„ì™€ ì¹˜ë£Œì  ê³µê°ì—ì„œ SFT(supervised fine-tuning) ë° Instruct ëª¨ë¸ë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. ëª…ì‹œì  ì‚¬ê³  ê³¼ì •(COT)ì€ SFT ëª¨ë¸ì— ìœ ì˜ë¯¸í•œ ì´ì ì„ ì œê³µí–ˆìœ¼ë‚˜, ORPOë‚˜ Instruct ëª¨ë¸ì—ëŠ” ëšœë ·í•œ ì´ì ì„ ì œê³µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ì„ í˜¸ ì •ë ¬ ì •ì±… ìµœì í™”ê°€ ì†Œí˜• LLMì— ACT ì—­ëŸ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì£¼ì…í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•˜ë©°, ëª…ì‹œì  ì¶”ë¡ ì˜ ìœ ìš©ì„±ì€ í›ˆë ¨ íŒ¨ëŸ¬ë‹¤ì„ì— í¬ê²Œ ì˜ì¡´í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:27:30*