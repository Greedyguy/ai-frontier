---
keywords:
  - Acceptance and Commitment Therapy
  - Large Language Model
  - Odds Ratio Policy Optimization
  - Supervised Fine-Tuning
  - Chain-of-Thought reasoning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.09712
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:27:30.197832",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Acceptance and Commitment Therapy",
    "Large Language Model",
    "Odds Ratio Policy Optimization",
    "Supervised Fine-Tuning",
    "Chain-of-Thought reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Acceptance and Commitment Therapy": 0.85,
    "Large Language Model": 0.8,
    "Odds Ratio Policy Optimization": 0.78,
    "Supervised Fine-Tuning": 0.72,
    "Chain-of-Thought reasoning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Acceptance and Commitment Therapy",
        "canonical": "Acceptance and Commitment Therapy",
        "aliases": [
          "ACT"
        ],
        "category": "unique_technical",
        "rationale": "Central to the study, linking to psychological therapy concepts.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Key technology used in the study, relevant for linking to AI concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Odds Ratio Policy Optimization",
        "canonical": "Odds Ratio Policy Optimization",
        "aliases": [
          "ORPO"
        ],
        "category": "unique_technical",
        "rationale": "A novel training method explored in the study, significant for linking to machine learning optimization techniques.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "specific_connectable",
        "rationale": "A common technique in machine learning, relevant for connecting to training methodologies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Chain-of-Thought reasoning",
        "canonical": "Chain-of-Thought reasoning",
        "aliases": [
          "COT reasoning"
        ],
        "category": "unique_technical",
        "rationale": "A reasoning method evaluated in the study, important for linking to cognitive processing techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "therapy sessions",
      "performance evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Acceptance and Commitment Therapy",
      "resolved_canonical": "Acceptance and Commitment Therapy",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Odds Ratio Policy Optimization",
      "resolved_canonical": "Odds Ratio Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Chain-of-Thought reasoning",
      "resolved_canonical": "Chain-of-Thought reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.09712.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.09712](https://arxiv.org/abs/2509.09712)

## 🔗 유사한 논문
- [[2025-09-23/Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression_ A Feasibility Study_20250923|Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression: A Feasibility Study]] (85.9% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (83.6% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (82.9% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (82.5% similar)
- [[2025-09-23/Correlation or Causation_ Analyzing the Causal Structures of LLM and LRM Reasoning Process_20250923|Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process]] (82.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]]
**⚡ Unique Technical**: [[keywords/Acceptance and Commitment Therapy|Acceptance and Commitment Therapy]], [[keywords/Odds Ratio Policy Optimization|Odds Ratio Policy Optimization]], [[keywords/Chain-of-Thought reasoning|Chain-of-Thought reasoning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.09712v2 Announce Type: replace-cross 
Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral therapy with emerging evidence of efficacy in several psychiatric conditions. This study investigates the impact of post-training methodology and explicit reasoning on the ability of a small open-weight large language model (LLM) to deliver ACT. Using synthetic ACT transcripts generated by Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches, supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each with and without an explicit chain-of-thought (COT) reasoning step. Performance was evaluated by comparing these four post-trained variants against the base Instruct model. These models were benchmarked in simulated therapy sessions, with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned on human evaluations. Our findings demonstrate that the ORPO-trained models significantly outperformed both their SFT and Instruct counterparts on ACT fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) = 140.37, p < .001$). The effect of COT was conditional as it provided a significant benefit to SFT models, improving ACT-FM scores by an average of 2.68 points ($p < .001$), while offering no discernible advantage to the superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO stems from its ability to learn the therapeutic `process' over imitating `content,' a key aspect of ACT, while COT acts as a necessary scaffold for models trained only via imitation. This study establishes that preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and that the utility of explicit reasoning is highly dependent on the underlying training paradigm.

## 📝 요약

이 연구는 Acceptance and Commitment Therapy(ACT)를 전달하는 데 있어 소형 대규모 언어 모델(LLM)의 성능을 조사했습니다. Mistral-Large가 생성한 합성 ACT 대본을 사용하여 Llama-3.2-3b-Instruct를 감독된 미세 조정(SFT)과 비율 최적화(ORPO) 방법으로 훈련했습니다. 두 방법 모두 명시적인 사고 과정(COT) 유무에 따라 평가되었습니다. ACT 충실도와 치료적 공감 능력을 측정한 결과, ORPO로 훈련된 모델이 SFT와 기본 모델보다 우수한 성능을 보였습니다. COT는 SFT 모델의 성능을 향상시켰으나, ORPO 모델에는 별다른 영향을 미치지 않았습니다. 연구는 ORPO가 ACT의 '과정' 학습에 효과적이며, COT는 모방 학습 모델에 필요하다고 결론지었습니다.

## 🎯 주요 포인트

- 1. Acceptance and Commitment Therapy (ACT)는 여러 정신과적 상태에서 효과가 있는 것으로 나타난 제3세대 인지 행동 치료법입니다.
- 2. 본 연구는 소형 대규모 언어 모델(LLM)이 ACT를 전달하는 능력에 대한 훈련 후 방법론과 명시적 추론의 영향을 조사합니다.
- 3. ORPO(odds ratio policy optimization)로 훈련된 모델이 ACT 충실도와 치료적 공감에서 SFT(supervised fine-tuning) 및 Instruct 모델보다 유의미하게 우수한 성과를 보였습니다.
- 4. 명시적 사고 과정(COT)은 SFT 모델에 유의미한 이점을 제공했으나, ORPO나 Instruct 모델에는 뚜렷한 이점을 제공하지 않았습니다.
- 5. 본 연구는 선호 정렬 정책 최적화가 소형 LLM에 ACT 역량을 효과적으로 주입할 수 있음을 입증하며, 명시적 추론의 유용성은 훈련 패러다임에 크게 의존함을 보여줍니다.


---

*Generated on 2025-09-24 01:27:30*