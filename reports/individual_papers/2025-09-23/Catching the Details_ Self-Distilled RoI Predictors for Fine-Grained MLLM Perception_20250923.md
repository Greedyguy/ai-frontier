---
keywords:
  - Multimodal Learning
  - Attention Mechanism
  - Self-Distilled Region Proposal Network
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16944
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:39:14.284943",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Attention Mechanism",
    "Self-Distilled Region Proposal Network",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Attention Mechanism": 0.8,
    "Self-Distilled Region Proposal Network": 0.78,
    "Vision-Language Model": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of integrating multiple modalities in language models, enhancing connectivity with related multimodal research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Region-of-Interest mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "RoI mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the broader concept of attention mechanisms, which are crucial in focusing on specific parts of input data.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Self-Distilled Region Proposal Network",
        "canonical": "Self-Distilled Region Proposal Network",
        "aliases": [
          "SD-RPN"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach specific to this paper, useful for linking to future works that build on this method.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "LLaVA-1.5 architecture",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LLaVA"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents an evolved concept in vision-language models, facilitating connections with related architectures.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "high-resolution",
      "fine-grained perception",
      "pseudo-RoI labels"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Region-of-Interest mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Self-Distilled Region Proposal Network",
      "resolved_canonical": "Self-Distilled Region Proposal Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LLaVA-1.5 architecture",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16944.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16944](https://arxiv.org/abs/2509.16944)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (88.2% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (86.1% similar)
- [[2025-09-23/ProReason_ Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom_20250923|ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom]] (85.9% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (85.8% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (85.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Self-Distilled Region Proposal Network|Self-Distilled Region Proposal Network]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16944v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass operations.To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at https://github.com/YuHengsss/SD-RPN.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³ í•´ìƒë„ ì‹œê° ì •ë³´ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´, ì£¼ì„ì´ í•„ìš” ì—†ëŠ” Self-Distilled Region Proposal Network (SD-RPN)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì£¼ì„ì´ í•„ìš”í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ê±°ë‚˜, ë¹„íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ìš”êµ¬í•˜ëŠ” ë°˜ë©´, SD-RPNì€ MLLMì˜ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ìƒì„±ëœ ì£¼ì˜ ë§µì„ ê³ í’ˆì§ˆì˜ ê°€ìƒ RoI ë ˆì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ê²½ëŸ‰í™”ëœ RPNì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì´ RPNì€ ë‹¨ì¼ ì „ë°© íŒ¨ìŠ¤ë¥¼ í†µí•´ RoIë¥¼ ì˜ˆì¸¡í•˜ë©°, LLaVA-1.5 ì•„í‚¤í…ì²˜ì— í†µí•©ë˜ì–´ TextVQA ë“±ì—ì„œ 10% ì´ìƒì˜ ì •í™•ë„ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ê³ ë¹„ìš©ì˜ ê°ë… ì—†ì´ MLLMì˜ ì„¸ë°€í•œ ì¸ì‹ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MLLMsì˜ ë¯¸ì„¸í•œ ì¸ì‹ì„ ìœ„í•´ ê³ í•´ìƒë„ ì‹œê° ì •ë³´ê°€ í•„ìš”í•˜ì§€ë§Œ, ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ê³„ì‚°ì ìœ¼ë¡œ ë¶€ë‹´ì´ í½ë‹ˆë‹¤.
- 2. RoI ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•œ ê¸°ì¡´ ë°©ë²•ë“¤ì€ ëŒ€ê·œëª¨ ì£¼ì„ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ê±°ë‚˜ ê³„ì‚° íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì£¼ì„ ì—†ì´ íš¨ìœ¨ì ì¸ Self-Distilled Region Proposal Network (SD-RPN)ì„ ì œì•ˆí•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 4. SD-RPNì€ MLLMì˜ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ìƒì„±ëœ ì£¼ì˜ ë§µì„ ê³ í’ˆì§ˆì˜ ì˜ì‚¬ RoI ë ˆì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ê²½ëŸ‰ì˜ RPNì„ í›ˆë ¨í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ LLaVA-1.5 ì•„í‚¤í…ì²˜ì— í†µí•©ë˜ì–´ ë°ì´í„° íš¨ìœ¨ì„±ê³¼ ì¼ë°˜í™”ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, TextVQA, DocVQA, V-Star ë“±ì—ì„œ 10% ì´ìƒì˜ ì •í™•ë„ í–¥ìƒì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:39:14*