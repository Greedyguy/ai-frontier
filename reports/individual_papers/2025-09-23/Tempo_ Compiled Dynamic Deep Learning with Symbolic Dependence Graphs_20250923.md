---
keywords:
  - Deep Learning
  - Symbolic Dependence Graph
  - Temporal Dimensions
  - Recurrent Tensors
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2501.05408
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:42:52.799262",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Symbolic Dependence Graph",
    "Temporal Dimensions",
    "Recurrent Tensors"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.85,
    "Symbolic Dependence Graph": 0.8,
    "Temporal Dimensions": 0.78,
    "Recurrent Tensors": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Learning",
        "canonical": "Deep Learning",
        "aliases": [
          "DL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is a foundational concept that connects various aspects of the paper's methodology.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "symbolic dependence graph",
        "canonical": "Symbolic Dependence Graph",
        "aliases": [
          "SDG"
        ],
        "category": "unique_technical",
        "rationale": "The symbolic dependence graph is a unique technical concept introduced in the paper that underpins its novel approach.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "temporal dimensions",
        "canonical": "Temporal Dimensions",
        "aliases": [
          "time dimensions"
        ],
        "category": "specific_connectable",
        "rationale": "Temporal dimensions are crucial for understanding dynamic dependencies in the proposed system.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "recurrent tensors",
        "canonical": "Recurrent Tensors",
        "aliases": [
          "recurrent tensor structures"
        ],
        "category": "unique_technical",
        "rationale": "Recurrent tensors are a novel concept in the paper that enable dynamic execution.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "eager execution",
      "graph-based systems",
      "whole-program optimizations"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "symbolic dependence graph",
      "resolved_canonical": "Symbolic Dependence Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "temporal dimensions",
      "resolved_canonical": "Temporal Dimensions",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "recurrent tensors",
      "resolved_canonical": "Recurrent Tensors",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Tempo: Compiled Dynamic Deep Learning with Symbolic Dependence Graphs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2501.05408.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2501.05408](https://arxiv.org/abs/2501.05408)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/(P)rior(D)yna(F)low_ A Priori Dynamic Workflow Construction via Multi-Agent Collaboration_20250919|(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration]] (79.6% similar)
- [[2025-09-22/DebFlow_ Automating Agent Creation via Agent Debate_20250922|DebFlow: Automating Agent Creation via Agent Debate]] (79.6% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (79.4% similar)
- [[2025-09-23/LightCode_ Compiling LLM Inference for Photonic-Electronic Systems_20250923|LightCode: Compiling LLM Inference for Photonic-Electronic Systems]] (79.1% similar)
- [[2025-09-23/Adaptive Overclocking_ Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals_20250923|Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals]] (78.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Temporal Dimensions|Temporal Dimensions]]
**âš¡ Unique Technical**: [[keywords/Symbolic Dependence Graph|Symbolic Dependence Graph]], [[keywords/Recurrent Tensors|Recurrent Tensors]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.05408v2 Announce Type: replace-cross 
Abstract: Deep learning (DL) algorithms are often defined in terms of \emph{temporal relationships}: a tensor at one timestep may depend on tensors from earlier or later timesteps. Such \emph{dynamic} dependencies (and corresponding dynamic tensor shapes) are difficult to express and optimize: while \emph{eager} DL systems support such dynamism, they cannot apply compiler-based optimizations; \emph{graph-based} systems require static tensor shapes, which forces users to pad tensors or break-up programs into multiple static graphs.
  We describe Tempo, a new DL system that combines the dynamism of eager execution with the whole-program optimizations of graph-based compilation. Tempo achieves this through a declarative programming model with \emph{recurrent tensors}, which include explicit \emph{temporal dimensions}. Temporal dimensions can be indexed using \emph{symbolic expressions} to express dynamic dependencies on past and future tensors. Based on this, Tempo constructs a \emph{symbolic dependence graph}, which concisely encodes dynamic dependencies between operators, and applies whole-program optimizations, such as algebraic simplifications, vectorization, tiling, and fusion. By tiling dynamic dependencies into static-size blocks, Tempo can also reuse existing static code-generators. It then uses a polyhedral model to find a feasible execution schedule, which includes memory management operations. We show that Tempo achieves a 7$\times$ speedup over JAX for Llama-3.2-3B decoding; for reinforcement learning algorithms, Tempo achieves a 54$\times$ speedup, with 16$\times$ lower peak memory usage.

## ğŸ“ ìš”ì•½

TempoëŠ” ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œìœ¼ë¡œ, ì¦‰ì‹œ ì‹¤í–‰ì˜ ìœ ì—°ì„±ê³¼ ê·¸ë˜í”„ ê¸°ë°˜ ì»´íŒŒì¼ì˜ ìµœì í™”ë¥¼ ê²°í•©í•©ë‹ˆë‹¤. ì´ëŠ” ëª…ë ¹í˜• í”„ë¡œê·¸ë˜ë° ëª¨ë¸ê³¼ ëª…ì‹œì  ì‹œê°„ ì°¨ì›ì„ í¬í•¨í•˜ëŠ” ì¬ê·€ í…ì„œë¥¼ í†µí•´ êµ¬í˜„ë©ë‹ˆë‹¤. TempoëŠ” ë™ì  ì˜ì¡´ì„±ì„ ìƒì§•ì  í‘œí˜„ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´, ì—°ì‚°ì ê°„ì˜ ì˜ì¡´ì„±ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³ , ì „ì—­ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ TempoëŠ” ê¸°ì¡´ì˜ ì •ì  ì½”ë“œ ìƒì„±ê¸°ë¥¼ ì¬í™œìš©í•˜ë©°, ë©”ëª¨ë¦¬ ê´€ë¦¬ ë“±ì„ í¬í•¨í•œ ì‹¤í–‰ ì¼ì •ì„ ìµœì í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TempoëŠ” JAX ëŒ€ë¹„ Llama-3.2-3B ë””ì½”ë”©ì—ì„œ 7ë°°, ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì—ì„œ 54ë°°ì˜ ì†ë„ í–¥ìƒê³¼ 16ë°°ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš© ê°ì†Œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TempoëŠ” ë™ì  ì‹¤í–‰ì˜ ìœ ì—°ì„±ê³¼ ê·¸ë˜í”„ ê¸°ë°˜ ì»´íŒŒì¼ì˜ ì „ì²´ í”„ë¡œê·¸ë¨ ìµœì í™”ë¥¼ ê²°í•©í•œ ìƒˆë¡œìš´ ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- 2. TempoëŠ” ëª…ì‹œì ì¸ ì‹œê°„ ì°¨ì›ì„ í¬í•¨í•œ ì¬ê·€ í…ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  ì˜ì¡´ì„±ì„ í‘œí˜„í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤.
- 3. TempoëŠ” ì—°ì‚°ì ê°„ì˜ ë™ì  ì˜ì¡´ì„±ì„ ê°„ê²°í•˜ê²Œ ì¸ì½”ë”©í•˜ëŠ” ìƒì§•ì  ì˜ì¡´ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê³ , ì „ì—­ í”„ë¡œê·¸ë¨ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 4. TempoëŠ” ë™ì  ì˜ì¡´ì„±ì„ ì •ì  í¬ê¸°ì˜ ë¸”ë¡ìœ¼ë¡œ íƒ€ì¼ë§í•˜ì—¬ ê¸°ì¡´ì˜ ì •ì  ì½”ë“œ ìƒì„±ê¸°ë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. TempoëŠ” Llama-3.2-3B ë””ì½”ë”©ì—ì„œ JAX ëŒ€ë¹„ 7ë°°, ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì—ì„œ 54ë°°ì˜ ì†ë„ í–¥ìƒê³¼ 16ë°° ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:42:52*