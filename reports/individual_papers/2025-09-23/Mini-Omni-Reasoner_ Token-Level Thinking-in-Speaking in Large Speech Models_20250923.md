---
keywords:
  - Thinking-in-Speaking
  - Mini-Omni-Reasoner
  - Spoken-Math-Problems-3M
  - Large Speech Models
  - Thinker-Talker Architecture
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.15827
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:20:49.879077",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Thinking-in-Speaking",
    "Mini-Omni-Reasoner",
    "Spoken-Math-Problems-3M",
    "Large Speech Models",
    "Thinker-Talker Architecture"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Thinking-in-Speaking": 0.78,
    "Mini-Omni-Reasoner": 0.8,
    "Spoken-Math-Problems-3M": 0.72,
    "Large Speech Models": 0.75,
    "Thinker-Talker Architecture": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Thinking-in-Speaking",
        "canonical": "Thinking-in-Speaking",
        "aliases": [
          "Token-Level Thinking-in-Speaking"
        ],
        "category": "unique_technical",
        "rationale": "This concept introduces a novel approach to reasoning in speech models, enhancing real-time interaction.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mini-Omni-Reasoner",
        "canonical": "Mini-Omni-Reasoner",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a new framework for integrating reasoning within speech, crucial for linking to related speech model innovations.",
        "novelty_score": 0.9,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spoken-Math-Problems-3M",
        "canonical": "Spoken-Math-Problems-3M",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specialized dataset supporting interleaved reasoning, useful for connecting to dataset-focused research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      },
      {
        "surface": "Large Speech Models",
        "canonical": "Large Speech Models",
        "aliases": [
          "LSMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to broader discussions on advancements in speech model technologies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Thinker-Talker architecture",
        "canonical": "Thinker-Talker Architecture",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a specific architecture that supports the framework, relevant for architectural comparisons.",
        "novelty_score": 0.8,
        "connectivity_score": 0.68,
        "specificity_score": 0.83,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "reasoning",
      "communication efficiency",
      "real-time interaction"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Thinking-in-Speaking",
      "resolved_canonical": "Thinking-in-Speaking",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mini-Omni-Reasoner",
      "resolved_canonical": "Mini-Omni-Reasoner",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spoken-Math-Problems-3M",
      "resolved_canonical": "Spoken-Math-Problems-3M",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Large Speech Models",
      "resolved_canonical": "Large Speech Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Thinker-Talker architecture",
      "resolved_canonical": "Thinker-Talker Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.68,
        "specificity": 0.83,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.15827.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.15827](https://arxiv.org/abs/2508.15827)

## 🔗 유사한 논문
- [[2025-09-22/Think, Verbalize, then Speak_ Bridging Complex Thoughts and Comprehensible Speech_20250922|Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech]] (88.2% similar)
- [[2025-09-23/ProReason_ Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom_20250923|ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom]] (85.0% similar)
- [[2025-09-23/Audio-Reasoner_ Improving Reasoning Capability in Large Audio Language Models_20250923|Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models]] (84.3% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory]] (84.1% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (84.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Speech Models|Large Speech Models]]
**⚡ Unique Technical**: [[keywords/Thinking-in-Speaking|Thinking-in-Speaking]], [[keywords/Mini-Omni-Reasoner|Mini-Omni-Reasoner]], [[keywords/Spoken-Math-Problems-3M|Spoken-Math-Problems-3M]], [[keywords/Thinker-Talker Architecture|Thinker-Talker Architecture]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2508.15827v2 Announce Type: replace-cross 
Abstract: Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.

## 📝 요약

이 논문은 음성 모델(LSM)에서의 추론을 개선하기 위해 Mini-Omni-Reasoner라는 새로운 프레임워크를 제안합니다. 기존의 "생각 후 말하기" 방식은 실시간 상호작용에 지연을 초래했으나, Mini-Omni-Reasoner는 "말하면서 생각하기" 방식을 도입하여 음성 생성 중에도 추론을 병행합니다. 이를 위해 고빈도 토큰 처리 능력을 활용하여 추론 토큰과 응답 토큰을 교차 배치하고, 지역적 의미 정렬을 통해 각 응답 토큰이 이전의 추론에 기반하도록 합니다. 또한, 대규모 데이터셋 Spoken-Math-Problems-3M을 소개하여 이 프레임워크를 지원합니다. 결과적으로, Mini-Omni-Reasoner는 자연스러우면서도 논리적으로 일관된 음성 응답을 제공하며, Spoken-MQA 벤치마크에서 산술 추론 19.1%, 맥락 이해 6.4% 개선을 달성했습니다.

## 🎯 주요 포인트

- 1. Mini-Omni-Reasoner는 "Thinking-in-Speaking" 공식을 통해 발화 중 추론을 가능하게 하여 실시간 상호작용과 의사소통 효율성을 개선합니다.
- 2. 이 프레임워크는 발화 응답 토큰과 침묵 추론 토큰을 토큰 수준에서 교차 배치하여 연속적인 발화 생성과 구조화된 내부 추론을 동시에 수행합니다.
- 3. Spoken-Math-Problems-3M 데이터셋은 교차된 추론과 응답을 위한 대규모 데이터셋으로, 발화 토큰이 관련 추론 내용을 일관되게 따르도록 설계되었습니다.
- 4. Mini-Omni-Reasoner는 계층적 Thinker-Talker 아키텍처를 기반으로 하여 자연스러우면서도 논리적으로 근거 있는 발화 응답을 제공합니다.
- 5. Spoken-MQA 벤치마크에서 산술 추론에서 +19.1%, 맥락 이해에서 +6.4%의 성능 향상을 달성하며, 출력이 더 짧고 디코딩 지연이 없습니다.


---

*Generated on 2025-09-24 01:20:49*