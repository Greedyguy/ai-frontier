---
keywords:
  - Thinking-in-Speaking
  - Mini-Omni-Reasoner
  - Spoken-Math-Problems-3M
  - Large Speech Models
  - Thinker-Talker Architecture
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.15827
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:20:49.879077",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Thinking-in-Speaking",
    "Mini-Omni-Reasoner",
    "Spoken-Math-Problems-3M",
    "Large Speech Models",
    "Thinker-Talker Architecture"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Thinking-in-Speaking": 0.78,
    "Mini-Omni-Reasoner": 0.8,
    "Spoken-Math-Problems-3M": 0.72,
    "Large Speech Models": 0.75,
    "Thinker-Talker Architecture": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Thinking-in-Speaking",
        "canonical": "Thinking-in-Speaking",
        "aliases": [
          "Token-Level Thinking-in-Speaking"
        ],
        "category": "unique_technical",
        "rationale": "This concept introduces a novel approach to reasoning in speech models, enhancing real-time interaction.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mini-Omni-Reasoner",
        "canonical": "Mini-Omni-Reasoner",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a new framework for integrating reasoning within speech, crucial for linking to related speech model innovations.",
        "novelty_score": 0.9,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spoken-Math-Problems-3M",
        "canonical": "Spoken-Math-Problems-3M",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specialized dataset supporting interleaved reasoning, useful for connecting to dataset-focused research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      },
      {
        "surface": "Large Speech Models",
        "canonical": "Large Speech Models",
        "aliases": [
          "LSMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to broader discussions on advancements in speech model technologies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Thinker-Talker architecture",
        "canonical": "Thinker-Talker Architecture",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a specific architecture that supports the framework, relevant for architectural comparisons.",
        "novelty_score": 0.8,
        "connectivity_score": 0.68,
        "specificity_score": 0.83,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "reasoning",
      "communication efficiency",
      "real-time interaction"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Thinking-in-Speaking",
      "resolved_canonical": "Thinking-in-Speaking",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mini-Omni-Reasoner",
      "resolved_canonical": "Mini-Omni-Reasoner",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spoken-Math-Problems-3M",
      "resolved_canonical": "Spoken-Math-Problems-3M",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Large Speech Models",
      "resolved_canonical": "Large Speech Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Thinker-Talker architecture",
      "resolved_canonical": "Thinker-Talker Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.68,
        "specificity": 0.83,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.15827.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.15827](https://arxiv.org/abs/2508.15827)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Think, Verbalize, then Speak_ Bridging Complex Thoughts and Comprehensible Speech_20250922|Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech]] (88.2% similar)
- [[2025-09-23/ProReason_ Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom_20250923|ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom]] (85.0% similar)
- [[2025-09-23/Audio-Reasoner_ Improving Reasoning Capability in Large Audio Language Models_20250923|Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models]] (84.3% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory]] (84.1% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Speech Models|Large Speech Models]]
**âš¡ Unique Technical**: [[keywords/Thinking-in-Speaking|Thinking-in-Speaking]], [[keywords/Mini-Omni-Reasoner|Mini-Omni-Reasoner]], [[keywords/Spoken-Math-Problems-3M|Spoken-Math-Problems-3M]], [[keywords/Thinker-Talker Architecture|Thinker-Talker Architecture]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.15827v2 Announce Type: replace-cross 
Abstract: Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìŒì„± ëª¨ë¸(LSM)ì—ì„œì˜ ì¶”ë¡ ì„ ê°œì„ í•˜ê¸° ìœ„í•´ Mini-Omni-Reasonerë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ "ìƒê° í›„ ë§í•˜ê¸°" ë°©ì‹ì€ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì— ì§€ì—°ì„ ì´ˆë˜í–ˆìœ¼ë‚˜, Mini-Omni-ReasonerëŠ” "ë§í•˜ë©´ì„œ ìƒê°í•˜ê¸°" ë°©ì‹ì„ ë„ì…í•˜ì—¬ ìŒì„± ìƒì„± ì¤‘ì—ë„ ì¶”ë¡ ì„ ë³‘í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ê³ ë¹ˆë„ í† í° ì²˜ë¦¬ ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ì¶”ë¡  í† í°ê³¼ ì‘ë‹µ í† í°ì„ êµì°¨ ë°°ì¹˜í•˜ê³ , ì§€ì—­ì  ì˜ë¯¸ ì •ë ¬ì„ í†µí•´ ê° ì‘ë‹µ í† í°ì´ ì´ì „ì˜ ì¶”ë¡ ì— ê¸°ë°˜í•˜ë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ Spoken-Math-Problems-3Mì„ ì†Œê°œí•˜ì—¬ ì´ í”„ë ˆì„ì›Œí¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, Mini-Omni-ReasonerëŠ” ìì—°ìŠ¤ëŸ¬ìš°ë©´ì„œë„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœ ìŒì„± ì‘ë‹µì„ ì œê³µí•˜ë©°, Spoken-MQA ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‚°ìˆ  ì¶”ë¡  19.1%, ë§¥ë½ ì´í•´ 6.4% ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Mini-Omni-ReasonerëŠ” "Thinking-in-Speaking" ê³µì‹ì„ í†µí•´ ë°œí™” ì¤‘ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ê³¼ ì˜ì‚¬ì†Œí†µ íš¨ìœ¨ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë°œí™” ì‘ë‹µ í† í°ê³¼ ì¹¨ë¬µ ì¶”ë¡  í† í°ì„ í† í° ìˆ˜ì¤€ì—ì„œ êµì°¨ ë°°ì¹˜í•˜ì—¬ ì—°ì†ì ì¸ ë°œí™” ìƒì„±ê³¼ êµ¬ì¡°í™”ëœ ë‚´ë¶€ ì¶”ë¡ ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 3. Spoken-Math-Problems-3M ë°ì´í„°ì…‹ì€ êµì°¨ëœ ì¶”ë¡ ê³¼ ì‘ë‹µì„ ìœ„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ, ë°œí™” í† í°ì´ ê´€ë ¨ ì¶”ë¡  ë‚´ìš©ì„ ì¼ê´€ë˜ê²Œ ë”°ë¥´ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. Mini-Omni-ReasonerëŠ” ê³„ì¸µì  Thinker-Talker ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš°ë©´ì„œë„ ë…¼ë¦¬ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ë°œí™” ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. Spoken-MQA ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‚°ìˆ  ì¶”ë¡ ì—ì„œ +19.1%, ë§¥ë½ ì´í•´ì—ì„œ +6.4%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•˜ë©°, ì¶œë ¥ì´ ë” ì§§ê³  ë””ì½”ë”© ì§€ì—°ì´ ì—†ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:20:49*