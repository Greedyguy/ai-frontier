---
keywords:
  - Large Language Model
  - Attention Mechanism
  - Multiple-Instance Learning
  - Emotion, Logic, and Behavior components
  - Cognitive Distortion Detection
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17292
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:51:15.606498",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Attention Mechanism",
    "Multiple-Instance Learning",
    "Emotion, Logic, and Behavior components",
    "Cognitive Distortion Detection"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Attention Mechanism": 0.82,
    "Multiple-Instance Learning": 0.78,
    "Emotion, Logic, and Behavior components": 0.8,
    "Cognitive Distortion Detection": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the proposed framework and connect well with existing NLP concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multi-View Gated Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Gated Attention"
        ],
        "category": "specific_connectable",
        "rationale": "This mechanism is a specific application of attention, enhancing its connectivity with attention-based models.",
        "novelty_score": 0.58,
        "connectivity_score": 0.87,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multiple-Instance Learning",
        "canonical": "Multiple-Instance Learning",
        "aliases": [
          "MIL"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique approach within the framework, providing specific insights into learning methods.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "Emotion, Logic, and Behavior components",
        "canonical": "Emotion, Logic, and Behavior components",
        "aliases": [
          "ELB components"
        ],
        "category": "unique_technical",
        "rationale": "These components are unique to the paper's methodology, offering a novel perspective on cognitive distortion detection.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cognitive Distortion Detection",
        "canonical": "Cognitive Distortion Detection",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is the core application of the paper, crucial for linking mental health and NLP research.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "automatic detection",
      "classification performance",
      "psychologically grounded"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multi-View Gated Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.87,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multiple-Instance Learning",
      "resolved_canonical": "Multiple-Instance Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Emotion, Logic, and Behavior components",
      "resolved_canonical": "Emotion, Logic, and Behavior components",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cognitive Distortion Detection",
      "resolved_canonical": "Cognitive Distortion Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17292.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17292](https://arxiv.org/abs/2509.17292)

## 🔗 유사한 논문
- [[2025-09-22/A Layered Multi-Expert Framework for Long-Context Mental Health Assessments_20250922|A Layered Multi-Expert Framework for Long-Context Mental Health Assessments]] (85.7% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (83.2% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (83.1% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (82.5% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (82.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Multiple-Instance Learning|Multiple-Instance Learning]], [[keywords/Emotion, Logic, and Behavior components|Emotion, Logic, and Behavior components]], [[keywords/Cognitive Distortion Detection|Cognitive Distortion Detection]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17292v1 Announce Type: cross 
Abstract: Cognitive distortions have been closely linked to mental health disorders, yet their automatic detection remained challenging due to contextual ambiguity, co-occurrence, and semantic overlap. We proposed a novel framework that combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL) architecture to enhance interpretability and expression-level reasoning. Each utterance was decomposed into Emotion, Logic, and Behavior (ELB) components, which were processed by LLMs to infer multiple distortion instances, each with a predicted type, expression, and model-assigned salience score. These instances were integrated via a Multi-View Gated Attention mechanism for final classification. Experiments on Korean (KoACD) and English (Therapist QA) datasets demonstrate that incorporating ELB and LLM-inferred salience scores improves classification performance, especially for distortions with high interpretive ambiguity. Our results suggested a psychologically grounded and generalizable approach for fine-grained reasoning in mental health NLP.

## 📝 요약

이 논문은 정신 건강 장애와 관련된 인지 왜곡의 자동 탐지를 개선하기 위해 대형 언어 모델(LLMs)과 다중 인스턴스 학습(MIL) 아키텍처를 결합한 새로운 프레임워크를 제안합니다. 각 발화를 감정, 논리, 행동(ELB) 요소로 분해하여 LLMs가 다양한 왜곡 인스턴스를 추론하도록 하였으며, 이를 통해 예측 유형, 표현, 중요도 점수를 할당했습니다. 최종 분류는 다중 뷰 게이트 주의 메커니즘을 통해 이루어졌습니다. 한국어(KoACD)와 영어(Therapist QA) 데이터셋 실험 결과, ELB와 LLM 기반 중요도 점수를 활용하면 해석적 모호성이 높은 왜곡의 분류 성능이 향상됨을 보여주었습니다. 이는 정신 건강 자연어 처리(NLP)에서 심리적으로 근거 있는 세밀한 추론 접근법을 제시합니다.

## 🎯 주요 포인트

- 1. 인지 왜곡의 자동 탐지가 문맥적 모호성, 동시 발생, 의미적 중첩 때문에 어려움을 겪고 있습니다.
- 2. 본 연구는 대형 언어 모델(LLMs)과 다중 인스턴스 학습(MIL) 아키텍처를 결합한 새로운 프레임워크를 제안합니다.
- 3. 발화는 감정, 논리, 행동(ELB) 구성 요소로 분해되어 LLMs에 의해 다양한 왜곡 인스턴스를 추론합니다.
- 4. 한국어(KoACD)와 영어(Therapist QA) 데이터셋 실험에서 ELB와 LLM 추론 중요도 점수를 포함하면 분류 성능이 향상됨을 보였습니다.
- 5. 본 연구는 정신 건강 NLP에서 세밀한 추론을 위한 심리학적으로 근거 있고 일반화 가능한 접근법을 제시합니다.


---

*Generated on 2025-09-23 23:51:15*