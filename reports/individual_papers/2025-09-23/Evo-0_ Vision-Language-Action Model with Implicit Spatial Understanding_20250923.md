---
keywords:
  - Vision-Language-Action Model
  - Vision-Language Model
  - 3D Geometry Features
  - Depth-Aware Visual Representations
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2507.00416
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:35:53.701944",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language-Action Model",
    "Vision-Language Model",
    "3D Geometry Features",
    "Depth-Aware Visual Representations"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language-Action Model": 0.8,
    "Vision-Language Model": 0.85,
    "3D Geometry Features": 0.78,
    "Depth-Aware Visual Representations": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language-Action",
        "canonical": "Vision-Language-Action Model",
        "aliases": [
          "VLA"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a specific framework that integrates vision, language, and action, which is central to the paper's contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "This is a foundational concept for the discussed framework, linking to broader multimodal research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "3D geometry features",
        "canonical": "3D Geometry Features",
        "aliases": [
          "3D spatial features"
        ],
        "category": "specific_connectable",
        "rationale": "These features are crucial for the implicit spatial understanding discussed in the paper.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "depth-aware visual representations",
        "canonical": "Depth-Aware Visual Representations",
        "aliases": [
          "depth-aware visuals"
        ],
        "category": "unique_technical",
        "rationale": "This concept is key to the paper's novel approach to spatial understanding without explicit depth inputs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language-Action",
      "resolved_canonical": "Vision-Language-Action Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "3D geometry features",
      "resolved_canonical": "3D Geometry Features",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "depth-aware visual representations",
      "resolved_canonical": "Depth-Aware Visual Representations",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2507.00416.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2507.00416](https://arxiv.org/abs/2507.00416)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/GeoAware-VLA_ Implicit Geometry Aware Vision-Language-Action Model_20250918|GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model]] (90.0% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (87.1% similar)
- [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (86.5% similar)
- [[2025-09-23/SD-VLM_ Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models_20250923|SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models]] (86.4% similar)
- [[2025-09-19/CollabVLA_ Self-Reflective Vision-Language-Action Model Dreaming Together with Human_20250919|CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human]] (86.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/3D Geometry Features|3D Geometry Features]]
**âš¡ Unique Technical**: [[keywords/Vision-Language-Action Model|Vision-Language-Action Model]], [[keywords/Depth-Aware Visual Representations|Depth-Aware Visual Representations]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.00416v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale image and text pretraining. However, existing VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or pre-trained depth estimation models, which may yield defective results. In contrast, our work introduces a plug-and-play module that implicitly incorporates 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation model. This integration provides the model with depth-aware visual representations, improving its ability to understand the geometric structure of the scene and the spatial relationships among objects from RGB images alone. We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.

## ğŸ“ ìš”ì•½

Vision-Language-Action(VLA) ëª¨ë¸ì€ ë¡œë´‡ì´ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì¸ì‹, ì¶”ë¡ , í–‰ë™í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ìœ ë§í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ Vision-Language Models(VLMs)ì€ ì£¼ë¡œ 2D ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì— ë§ì¶°ì ¸ ìˆì–´ 3D ê³µê°„ ì´í•´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¼ë¶€ ì ‘ê·¼ë²•ì€ 3D ì…ë ¥ì„ ì¶”ê°€í•˜ì§€ë§Œ, ì´ëŠ” ì¶”ê°€ì ì¸ ì„¼ì„œë‚˜ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³„ë„ì˜ ì„¼ì„œ ì—†ì´ë„ 3D ê¸°í•˜í•™ì  íŠ¹ì§•ì„ VLA ëª¨ë¸ì— í†µí•©í•  ìˆ˜ ìˆëŠ” ëª¨ë“ˆì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë“ˆì€ RGB ì´ë¯¸ì§€ì—ì„œ ê¹Šì´ ì¸ì‹ ì‹œê° í‘œí˜„ì„ ì œê³µí•˜ì—¬ ì¥ë©´ì˜ ê¸°í•˜í•™ì  êµ¬ì¡°ì™€ ê°ì²´ ê°„ ê³µê°„ ê´€ê³„ ì´í•´ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ë° ì‹¤ì œ í™˜ê²½ì—ì„œì˜ í‰ê°€ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ VLA ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision-Language-Action(VLA) ëª¨ë¸ì€ ì¼ë°˜ì ì¸ ë¡œë´‡ì´ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì¸ì§€, ì¶”ë¡ , í–‰ë™í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ìœ ë§í•œ í”„ë ˆì„ì›Œí¬ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ Vision-Language Models(VLMs)ì€ ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì‚¬ì „ í•™ìŠµì„ í†µí•´ ì˜ë¯¸ ì´í•´ì— ë›°ì–´ë‚˜ì§€ë§Œ, 3D ê°ë… ì—†ì´ 2D ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì— ì£¼ë¡œ ë§ì¶°ì ¸ ìˆì–´ ì •ë°€í•œ ê³µê°„ ì´í•´ ëŠ¥ë ¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” 3D ì…ë ¥ì„ ëª…ì‹œì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ëŒ€ì‹ , ê¸°ì„±ì˜ ì‹œê° ê¸°í•˜í•™ ê¸°ë°˜ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ VLA ëª¨ë¸ì— 3D ê¸°í•˜í•™ì  íŠ¹ì§•ì„ ì•”ë¬µì ìœ¼ë¡œ í†µí•©í•˜ëŠ” í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ëª¨ë“ˆì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 4. ì´ í†µí•©ì€ RGB ì´ë¯¸ì§€ ë§Œìœ¼ë¡œë„ ì¥ë©´ì˜ ê¸°í•˜í•™ì  êµ¬ì¡°ì™€ ê°ì²´ ê°„ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¹Šì´ ì¸ì‹ ì‹œê° í‘œí˜„ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ê´‘ë²”ìœ„í•œ í‰ê°€ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ìµœì²¨ë‹¨ VLA ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:35:53*