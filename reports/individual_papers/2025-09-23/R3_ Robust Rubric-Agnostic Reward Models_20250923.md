---
keywords:
  - Reward Models
  - Large Language Model
  - Rubric-Agnostic Evaluation
  - Interpretable Score Assignments
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.13388
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:58:53.065850",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reward Models",
    "Large Language Model",
    "Rubric-Agnostic Evaluation",
    "Interpretable Score Assignments"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reward Models": 0.75,
    "Large Language Model": 0.8,
    "Rubric-Agnostic Evaluation": 0.7,
    "Interpretable Score Assignments": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "reward models",
        "canonical": "Reward Models",
        "aliases": [
          "reward modeling"
        ],
        "category": "unique_technical",
        "rationale": "Reward models are central to the paper's framework and are crucial for linking to discussions on model alignment and evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "language model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Language models are a fundamental component of the study, linking to broader discussions in NLP and AI.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "rubric-agnostic",
        "canonical": "Rubric-Agnostic Evaluation",
        "aliases": [
          "rubric-free evaluation"
        ],
        "category": "unique_technical",
        "rationale": "The concept of rubric-agnostic evaluation is novel and central to the paper's contribution, enhancing model interpretability and generalizability.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      },
      {
        "surface": "interpretable score assignments",
        "canonical": "Interpretable Score Assignments",
        "aliases": [
          "interpretable scoring"
        ],
        "category": "unique_technical",
        "rationale": "Interpretable score assignments are key to understanding model outputs, linking to transparency and accountability in AI.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "model",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "reward models",
      "resolved_canonical": "Reward Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "language model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "rubric-agnostic",
      "resolved_canonical": "Rubric-Agnostic Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "interpretable score assignments",
      "resolved_canonical": "Interpretable Score Assignments",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# R3: Robust Rubric-Agnostic Reward Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.13388.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.13388](https://arxiv.org/abs/2505.13388)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/reWordBench_ Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs_20250922|reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs]] (88.3% similar)
- [[2025-09-23/Post-hoc Reward Calibration_ A Case Study on Length Bias_20250923|Post-hoc Reward Calibration: A Case Study on Length Bias]] (87.1% similar)
- [[2025-09-22/BaseReward_ A Strong Baseline for Multimodal Reward Model_20250922|BaseReward: A Strong Baseline for Multimodal Reward Model]] (87.0% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (86.3% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Reward Models|Reward Models]], [[keywords/Rubric-Agnostic Evaluation|Rubric-Agnostic Evaluation]], [[keywords/Interpretable Score Assignments|Interpretable Score Assignments]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.13388v3 Announce Type: replace-cross 
Abstract: Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce $\shortmethodname$, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. $\shortmethodname$ enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ê¸° ìœ„í•œ ë³´ìƒ ëª¨ë¸ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì€ ì œì–´ ê°€ëŠ¥ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì´ ë¶€ì¡±í•˜ë©°, ì¢ì€ ëª©í‘œì— ìµœì í™”ë˜ì–´ ìˆì–´ ì¼ë°˜í™”ê°€ ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ í•´ì„ ê°€ëŠ¥í•˜ê³  ë‹¤ì–‘í•œ í‰ê°€ ì°¨ì›ì— ì¼ë°˜í™” ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ë³´ìƒ ëª¨ë¸ë§ í”„ë ˆì„ì›Œí¬ì¸ $\shortmethodname$ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€ë¥¼ ë³´ë‹¤ íˆ¬ëª…í•˜ê³  ìœ ì—°í•˜ê²Œ í•˜ì—¬ ë‹¤ì–‘í•œ ì¸ê°„ ê°€ì¹˜ì™€ ì‚¬ìš© ì‚¬ë¡€ì— ê°•ë ¥í•˜ê²Œ ë§ì¶œ ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤. ì—°êµ¬ì˜ ëª¨ë¸, ë°ì´í„°, ì½”ë“œëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ ë³´ìƒ ëª¨ë¸ì€ ì œì–´ ê°€ëŠ¥ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì´ ë¶€ì¡±í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ ì¶œë ¥ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ëŠ” ë° í•œê³„ê°€ ìˆë‹¤.
- 2. ê¸°ì¡´ ëª¨ë¸ì€ ì¢ì€ ëª©í‘œì— ìµœì í™”ë˜ì–´ ìˆì–´ ë” ë„“ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì¼ë°˜í™”í•˜ê¸° ì–´ë µë‹¤.
- 3. $\shortmethodname$ëŠ” í•´ì„ ê°€ëŠ¥í•œ ì´ìœ  ê¸°ë°˜ ì ìˆ˜ í• ë‹¹ì„ ì œê³µí•˜ì—¬ í‰ê°€ ì°¨ì› ì „ë°˜ì— ê±¸ì³ ì¼ë°˜í™” ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ë³´ìƒ ëª¨ë¸ë§ í”„ë ˆì„ì›Œí¬ì´ë‹¤.
- 4. $\shortmethodname$ëŠ” ë‹¤ì–‘í•œ ì¸ê°„ ê°€ì¹˜ì™€ ì‚¬ìš© ì‚¬ë¡€ì— ê°•ë ¥í•˜ê²Œ ë§ì¶œ ìˆ˜ ìˆë„ë¡ ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€ë¥¼ ë” íˆ¬ëª…í•˜ê³  ìœ ì—°í•˜ê²Œ ë§Œë“ ë‹¤.
- 5. ì—°êµ¬ì˜ ëª¨ë¸, ë°ì´í„°, ì½”ë“œëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µëœë‹¤.


---

*Generated on 2025-09-24 00:58:53*