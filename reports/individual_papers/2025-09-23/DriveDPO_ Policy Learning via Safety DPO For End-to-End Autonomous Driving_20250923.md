---
keywords:
  - End-to-End Autonomous Driving
  - Imitation Learning
  - Safety Direct Preference Optimization
  - Trajectory-Level Preference Alignment
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17940
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:13:05.258500",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "End-to-End Autonomous Driving",
    "Imitation Learning",
    "Safety Direct Preference Optimization",
    "Trajectory-Level Preference Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "End-to-End Autonomous Driving": 0.78,
    "Imitation Learning": 0.8,
    "Safety Direct Preference Optimization": 0.82,
    "Trajectory-Level Preference Alignment": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "End-to-end autonomous driving",
        "canonical": "End-to-End Autonomous Driving",
        "aliases": [
          "E2E autonomous driving"
        ],
        "category": "unique_technical",
        "rationale": "This represents a specific approach in autonomous vehicle technology, offering unique insights into direct policy learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Imitation learning",
        "canonical": "Imitation Learning",
        "aliases": [
          "behavior cloning"
        ],
        "category": "specific_connectable",
        "rationale": "This is a key method in machine learning for training autonomous systems, facilitating connections to broader learning paradigms.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Safety Direct Preference Optimization",
        "canonical": "Safety Direct Preference Optimization",
        "aliases": [
          "Safety DPO"
        ],
        "category": "unique_technical",
        "rationale": "This novel framework is central to the paper's contribution, offering a new approach to policy learning with safety considerations.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Trajectory-level preference alignment",
        "canonical": "Trajectory-Level Preference Alignment",
        "aliases": [
          "trajectory preference alignment"
        ],
        "category": "unique_technical",
        "rationale": "This concept is crucial for understanding the optimization process in the proposed framework, linking to trajectory-based learning methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "End-to-end autonomous driving",
      "resolved_canonical": "End-to-End Autonomous Driving",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Imitation learning",
      "resolved_canonical": "Imitation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Safety Direct Preference Optimization",
      "resolved_canonical": "Safety Direct Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Trajectory-level preference alignment",
      "resolved_canonical": "Trajectory-Level Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17940.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17940](https://arxiv.org/abs/2509.17940)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Safe Guaranteed Dynamics Exploration with Probabilistic Models_20250923|Safe Guaranteed Dynamics Exploration with Probabilistic Models]] (86.1% similar)
- [[2025-09-19/FlowDrive_ Energy Flow Field for End-to-End Autonomous Driving_20250919|FlowDrive: Energy Flow Field for End-to-End Autonomous Driving]] (85.7% similar)
- [[2025-09-17/MAP_ End-to-End Autonomous Driving with Map-Assisted Planning_20250917|MAP: End-to-End Autonomous Driving with Map-Assisted Planning]] (84.5% similar)
- [[2025-09-23/ReasonPlan_ Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving_20250923|ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving]] (84.2% similar)
- [[2025-09-19/Digital Twin-based Cooperative Autonomous Driving in Smart Intersections_ A Multi-Agent Reinforcement Learning Approach_20250919|Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Imitation Learning|Imitation Learning]]
**âš¡ Unique Technical**: [[keywords/End-to-End Autonomous Driving|End-to-End Autonomous Driving]], [[keywords/Safety Direct Preference Optimization|Safety Direct Preference Optimization]], [[keywords/Trajectory-Level Preference Alignment|Trajectory-Level Preference Alignment]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17940v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has substantially progressed by directly predicting future trajectories from raw perception inputs, which bypasses traditional modular pipelines. However, mainstream methods trained via imitation learning suffer from critical safety limitations, as they fail to distinguish between trajectories that appear human-like but are potentially unsafe. Some recent approaches attempt to address this by regressing multiple rule-driven scores but decoupling supervision from policy optimization, resulting in suboptimal performance. To tackle these challenges, we propose DriveDPO, a Safety Direct Preference Optimization Policy Learning framework. First, we distill a unified policy distribution from human imitation similarity and rule-based safety scores for direct policy optimization. Further, we introduce an iterative Direct Preference Optimization stage formulated as trajectory-level preference alignment. Extensive experiments on the NAVSIM benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of 90.0. Furthermore, qualitative results across diverse challenging scenarios highlight DriveDPO's ability to produce safer and more reliable driving behaviors.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ììœ¨ì£¼í–‰ì˜ ì•ˆì „ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ DriveDPOë¼ëŠ” ìƒˆë¡œìš´ ì •ì±… í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ëª¨ë°© í•™ìŠµ ê¸°ë°˜ ë°©ë²•ë¡ ì€ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ê²½ë¡œë¥¼ ì˜ˆì¸¡í•˜ì§€ë§Œ ì•ˆì „ì„± ì¸¡ë©´ì—ì„œ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. DriveDPOëŠ” ì¸ê°„ ëª¨ë°© ìœ ì‚¬ì„±ê³¼ ê·œì¹™ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ ì •ì±… ìµœì í™”ë¥¼ ì§ì ‘ ìˆ˜í–‰í•˜ë©°, ê²½ë¡œ ìˆ˜ì¤€ì˜ ì„ í˜¸ë„ ì •ë ¬ì„ í†µí•´ ì •ì±…ì„ ê°œì„ í•©ë‹ˆë‹¤. NAVSIM ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, DriveDPOëŠ” 90.0ì˜ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ PDMSë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ë‹¤ì–‘í•œ ë„ì „ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì£¼í–‰ í–‰ë™ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. End-to-end ììœ¨ ì£¼í–‰ì€ ì „í†µì ì¸ ëª¨ë“ˆì‹ íŒŒì´í”„ë¼ì¸ì„ ìš°íšŒí•˜ì—¬ ì›ì‹œ ì¸ì‹ ì…ë ¥ì—ì„œ ì§ì ‘ ë¯¸ë˜ ê²½ë¡œë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ ìƒë‹¹í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.
- 2. ëª¨ë°© í•™ìŠµìœ¼ë¡œ í›ˆë ¨ëœ ì£¼ë¥˜ ë°©ë²•ì€ ì¸ê°„ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì ì¬ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ ì•Šì€ ê²½ë¡œë¥¼ êµ¬ë³„í•˜ì§€ ëª»í•´ ì•ˆì „ì„±ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
- 3. DriveDPOëŠ” ì¸ê°„ ëª¨ë°© ìœ ì‚¬ì„±ê³¼ ê·œì¹™ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ ì§ì ‘ ì •ì±… ìµœì í™”ë¥¼ ìœ„í•œ í†µí•© ì •ì±… ë¶„í¬ë¥¼ ì¦ë¥˜í•©ë‹ˆë‹¤.
- 4. DriveDPOëŠ” NAVSIM ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ PDMS 90.0ì„ ë‹¬ì„±í•˜ì˜€ìœ¼ë©°, ë‹¤ì–‘í•œ ë„ì „ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì£¼í–‰ í–‰ë™ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. DriveDPOëŠ” ê²½ë¡œ ìˆ˜ì¤€ì˜ ì„ í˜¸ë„ ì •ë ¬ë¡œ êµ¬ì„±ëœ ë°˜ë³µì ì¸ Direct Preference Optimization ë‹¨ê³„ë¥¼ ë„ì…í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:13:05*