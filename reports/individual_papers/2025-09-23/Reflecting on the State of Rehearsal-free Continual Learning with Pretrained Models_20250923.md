---
keywords:
  - Continual Learning
  - Pretrained Models
  - Parameter-efficient Finetuning
  - Rehearsal-free Continual Learning
  - Elastic Weight Consolidation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2406.09384
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:33:06.061207",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Continual Learning",
    "Pretrained Models",
    "Parameter-efficient Finetuning",
    "Rehearsal-free Continual Learning",
    "Elastic Weight Consolidation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Continual Learning": 0.85,
    "Pretrained Models": 0.8,
    "Parameter-efficient Finetuning": 0.78,
    "Rehearsal-free Continual Learning": 0.77,
    "Elastic Weight Consolidation": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "CL"
        ],
        "category": "broad_technical",
        "rationale": "Continual Learning is a central theme of the paper and connects to various machine learning paradigms.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Pretrained Models",
        "canonical": "Pretrained Models",
        "aliases": [
          "Pretrained Networks"
        ],
        "category": "specific_connectable",
        "rationale": "Pretrained Models are crucial for understanding the shift in continual learning methods discussed.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Parameter-efficient Finetuning",
        "canonical": "Parameter-efficient Finetuning",
        "aliases": [
          "PEFT"
        ],
        "category": "unique_technical",
        "rationale": "This technique is pivotal in the paper's analysis of continual learning approaches.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Rehearsal-free Continual Learning",
        "canonical": "Rehearsal-free Continual Learning",
        "aliases": [
          "RFCL"
        ],
        "category": "unique_technical",
        "rationale": "RFCL is a specific focus of the study, highlighting a novel approach in the field.",
        "novelty_score": 0.7,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Elastic Weight Consolidation",
        "canonical": "Elastic Weight Consolidation",
        "aliases": [
          "EWC"
        ],
        "category": "specific_connectable",
        "rationale": "EWC is a standard technique in continual learning, providing a baseline for comparison.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "methodological choices",
      "benchmark scores"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Pretrained Models",
      "resolved_canonical": "Pretrained Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Parameter-efficient Finetuning",
      "resolved_canonical": "Parameter-efficient Finetuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Rehearsal-free Continual Learning",
      "resolved_canonical": "Rehearsal-free Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Elastic Weight Consolidation",
      "resolved_canonical": "Elastic Weight Consolidation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Reflecting on the State of Rehearsal-free Continual Learning with Pretrained Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2406.09384.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2406.09384](https://arxiv.org/abs/2406.09384)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Global Pre-fixing, Local Adjusting_ A Simple yet Effective Contrastive Strategy for Continual Learning_20250922|Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning]] (83.5% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (81.5% similar)
- [[2025-09-23/AIMMerging_ Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning_20250923|AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning]] (81.0% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (80.5% similar)
- [[2025-09-22/RLinf_ Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation_20250922|RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation]] (80.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Continual Learning|Continual Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Pretrained Models|Pretrained Models]], [[keywords/Elastic Weight Consolidation|Elastic Weight Consolidation]]
**âš¡ Unique Technical**: [[keywords/Parameter-efficient Finetuning|Parameter-efficient Finetuning]], [[keywords/Rehearsal-free Continual Learning|Rehearsal-free Continual Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2406.09384v2 Announce Type: replace 
Abstract: With the advent and recent ubiquity of foundation models, continual learning (CL) has recently shifted from continual training from scratch to the continual adaptation of pretrained models, seeing particular success on rehearsal-free CL benchmarks (RFCL). To achieve this, most proposed methods adapt and restructure parameter-efficient finetuning techniques (PEFT) to suit the continual nature of the problem. Based most often on input-conditional query-mechanisms or regularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL (P-RFCL) approaches report peak performances; often convincingly outperforming existing CL techniques. However, on the other end, critical studies have recently highlighted competitive results by training on just the first task or via simple non-parametric baselines. Consequently, questions arise about the relationship between methodological choices in P-RFCL and their reported high benchmark scores. In this work, we tackle these questions to better understand the true drivers behind strong P-RFCL performances, their placement w.r.t. recent first-task adaptation studies, and their relation to preceding CL standards such as EWC or SI. In particular, we show: (1) P-RFCL techniques relying on input-conditional query mechanisms work not because, but rather despite them by collapsing towards standard PEFT shortcut solutions. (2) Indeed, we show how most often, P-RFCL techniques can be matched by a simple and lightweight PEFT baseline. (3) Using this baseline, we identify the implicit bound on tunable parameters when deriving RFCL approaches from PEFT methods as a potential denominator behind P-RFCL efficacy. Finally, we (4) better disentangle continual versus first-task adaptation, and (5) motivate standard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì§€ì†ì  ì ì‘ì„ ì¤‘ì ìœ¼ë¡œ í•˜ëŠ” ì§€ì† í•™ìŠµ(CL) ë¶„ì•¼ì—ì„œì˜ ìµœê·¼ ë°œì „ì„ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, ë¦¬í—ˆì„¤ì´ í•„ìš” ì—†ëŠ” ì§€ì† í•™ìŠµ(RFCL) ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ê³µì„ ê±°ë‘” PEFT ìŠ¤íƒ€ì¼ì˜ RFCL(P-RFCL) ì ‘ê·¼ë²•ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ìœ¼ë¡œëŠ”, P-RFCL ê¸°ë²•ì´ ì…ë ¥ ì¡°ê±´ ì¿¼ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì— ì˜ì¡´í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” í‘œì¤€ PEFT í•´ê²°ì±…ìœ¼ë¡œ ìˆ˜ë ´í•˜ì—¬ íš¨ê³¼ë¥¼ ë°œíœ˜í•œë‹¤ëŠ” ì ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ê°„ë‹¨í•œ PEFT ê¸°ë°˜ì´ P-RFCL ê¸°ë²•ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì¡°ì • ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ì•”ë¬µì  ê²½ê³„ê°€ P-RFCLì˜ íš¨ìœ¨ì„±ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŒì„ ë°í™ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì§€ì†ì  ì ì‘ê³¼ ì²« ë²ˆì§¸ ê³¼ì œ ì ì‘ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ê³ , EWCë‚˜ SI ê°™ì€ ê¸°ì¡´ RFCL ê¸°ë²•ì˜ ì¤‘ìš”ì„±ì„ ì¬ì¡°ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ì˜ ì—°êµ¬ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì§€ì†ì  ì ì‘ìœ¼ë¡œ ì´ˆì ì„ ì „í™˜í•˜ì—¬ ë¦¬í—ˆì„¤ ì—†ëŠ” ì§€ì†ì  í•™ìŠµ(RFCL) ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ê³µì„ ê±°ë‘ê³  ìˆìŠµë‹ˆë‹¤.
- 2. PEFT ìŠ¤íƒ€ì¼ì˜ RFCL ì ‘ê·¼ë²•ì€ ì…ë ¥ ì¡°ê±´ ì¿¼ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì´ë‚˜ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ PEFT ìœ„ì˜ ì •ê·œí™”ë¥¼ í™œìš©í•˜ì—¬ ê¸°ì¡´ ì§€ì†ì  í•™ìŠµ ê¸°ë²•ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ê³ í•©ë‹ˆë‹¤.
- 3. ì—°êµ¬ëŠ” P-RFCL ê¸°ë²•ì´ ì…ë ¥ ì¡°ê±´ ì¿¼ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì— ì˜ì¡´í•˜ì§€ ì•Šê³ ë„ í‘œì¤€ PEFT í•´ê²°ì±…ìœ¼ë¡œ ìˆ˜ë ´í•˜ì—¬ ì‘ë™í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. ê°„ë‹¨í•˜ê³  ê²½ëŸ‰í™”ëœ PEFT ê¸°ì¤€ì„ ì´ ëŒ€ë¶€ë¶„ì˜ P-RFCL ê¸°ë²•ê³¼ ì„±ëŠ¥ì„ ë§ì¶œ ìˆ˜ ìˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ëŠ” ì§€ì†ì  í•™ìŠµê³¼ ì²« ë²ˆì§¸ ê³¼ì œ ì ì‘ì„ ë” ì˜ êµ¬ë¶„í•˜ê³ , ìµœê·¼ P-RFCL ë°©ë²•ì— ë¹„ì¶”ì–´ EWCë‚˜ SIì™€ ê°™ì€ í‘œì¤€ RFCL ê¸°ë²•ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:33:06*