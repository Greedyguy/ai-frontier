---
keywords:
  - Large Language Model
  - Supervised Fine-Tuning
  - Closed-Book Question Answering
  - Parameter Updates
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16596
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:27:28.135538",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Supervised Fine-Tuning",
    "Closed-Book Question Answering",
    "Parameter Updates"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Supervised Fine-Tuning": 0.82,
    "Closed-Book Question Answering": 0.79,
    "Parameter Updates": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large-scale language models"
        ],
        "category": "broad_technical",
        "rationale": "This term is essential for linking discussions about advanced NLP systems and their capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.92,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "supervised fine-tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT",
          "fine-tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Critical for understanding the post-training modifications applied to models, impacting their performance.",
        "novelty_score": 0.58,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "closed-book question answering",
        "canonical": "Closed-Book Question Answering",
        "aliases": [
          "CBQA"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific task used to evaluate model knowledge without external information.",
        "novelty_score": 0.72,
        "connectivity_score": 0.67,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "parameter updates",
        "canonical": "Parameter Updates",
        "aliases": [
          "model updates",
          "weight adjustments"
        ],
        "category": "unique_technical",
        "rationale": "Understanding parameter updates is crucial for analyzing how fine-tuning impacts model knowledge.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "model behavior",
      "knowledge enhancement"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.92,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "supervised fine-tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "closed-book question answering",
      "resolved_canonical": "Closed-Book Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.67,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "parameter updates",
      "resolved_canonical": "Parameter Updates",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16596.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16596](https://arxiv.org/abs/2509.16596)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (87.0% similar)
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (84.9% similar)
- [[2025-09-22/A method for improving multilingual quality and diversity of instruction fine-tuning datasets_20250922|A method for improving multilingual quality and diversity of instruction fine-tuning datasets]] (84.1% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (83.9% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (83.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Closed-Book Question Answering|Closed-Book Question Answering]], [[keywords/Parameter Updates|Parameter Updates]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16596v1 Announce Type: cross 
Abstract: Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì „ í›ˆë ¨ í›„ ê°ë…ëœ ë¯¸ì„¸ ì¡°ì •(SFT)ì´ ëª¨ë¸ì˜ ì§€ì‹ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. LLaMA-2 ë° LLaMA-3 ê³„ì—´ì˜ ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ íì‡„í˜• ì§ˆë¬¸ ì‘ë‹µ(CBQA) ì„±ëŠ¥ì„ í‰ê°€í•œ ê²°ê³¼, 1,920ê°œì˜ ìƒ˜í”Œë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì´ 240ê°œì˜ ìƒ˜í”Œë¡œ ì¡°ì •ëœ ëª¨ë¸ë³´ë‹¤ ìµœëŒ€ 14% ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë¯¸ì„¸ ì¡°ì • ë°ì´í„°ì˜ ì§€ì‹ ìˆ˜ì¤€ì— ë”°ë¼ ì„±ëŠ¥ì´ 12% ì´ìƒ ë³€ë™í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, SFT ì¤‘ ìµœëŒ€ 90%ì˜ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ê°€ ì§€ì‹ í–¥ìƒì— ê¸°ì—¬í•˜ì§€ ì•Šìœ¼ë©°, ì´ë¥¼ ë³µì›í•˜ë©´ CBQA ì„±ëŠ¥ì´ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ ëª¨ë¸ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°•í™”í•˜ëŠ” ë¯¸ì„¸ ì¡°ì • ì „ëµ ê°œë°œì— ì‹¤ì§ˆì ì¸ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì‚¬ì „ í›ˆë ¨ ë™ì•ˆ ìƒë‹¹í•œ ì„¸ê³„ ì§€ì‹ì„ ìŠµë“í•˜ë©°, ì´ëŠ” ê°ë…ëœ ë¯¸ì„¸ ì¡°ì •(SFT)ê³¼ ê°™ì€ í›„ì† í›ˆë ¨ ê¸°ë²•ì— ì˜í•´ ì¶”ê°€ë¡œ í˜•ì„±ëœë‹¤.
- 2. 1,920ê°œì˜ ìƒ˜í”Œë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì´ 240ê°œì˜ ìƒ˜í”Œë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ë³´ë‹¤ ìµœëŒ€ 14% ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” í˜„ìƒì´ ê´€ì°°ë˜ì—ˆë‹¤.
- 3. ë¯¸ì„¸ ì¡°ì • ë°ì´í„°ì˜ ì§€ì‹ ìˆ™ë‹¬ ìˆ˜ì¤€ì— ë”°ë¼ ì„±ëŠ¥ ë³€ë™ì´ 12% ì´ìƒ ë°œìƒí•œë‹¤.
- 4. SFT ë™ì•ˆì˜ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ ì¤‘ ìµœëŒ€ 90%ê°€ ì§€ì‹ í–¥ìƒì— ê¸°ì—¬í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŸ¬í•œ ì—…ë°ì´íŠ¸ë¥¼ ë³µì›í•˜ë©´ CBQA ì‘ì—… ì„±ëŠ¥ì´ ê°œì„ ë  ìˆ˜ ìˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ëª¨ë¸ ì§€ì‹ì„ ë” íš¨ê³¼ì ìœ¼ë¡œ ê°•í™”í•˜ëŠ” ë¯¸ì„¸ ì¡°ì • ì „ëµ ê°œë°œì— ì‹¤ì§ˆì ì¸ ì§€ì¹¨ì„ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-23 23:27:28*