---
keywords:
  - Contrastive Learning
  - Dual-Probe Evaluation
  - Embedding Space
  - Linear Probe
  - k-Nearest Neighbours
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2309.11895
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:37:25.568219",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Contrastive Learning",
    "Dual-Probe Evaluation",
    "Embedding Space",
    "Linear Probe",
    "k-Nearest Neighbours"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Contrastive Learning": 0.82,
    "Dual-Probe Evaluation": 0.79,
    "Embedding Space": 0.77,
    "Linear Probe": 0.78,
    "k-Nearest Neighbours": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "contrastive-tuning",
        "canonical": "Contrastive Learning",
        "aliases": [
          "contrastive tuning",
          "contrastive fine-tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive learning is a critical method for improving representation quality, making it a strong link for understanding model refinement processes.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "dual-probe evaluation",
        "canonical": "Dual-Probe Evaluation",
        "aliases": [
          "dual probe",
          "dual-probing"
        ],
        "category": "unique_technical",
        "rationale": "This novel evaluation method provides insights into representation quality, offering a unique perspective on model assessment.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "embedding space",
        "canonical": "Embedding Space",
        "aliases": [
          "geometric space",
          "latent space"
        ],
        "category": "broad_technical",
        "rationale": "Understanding the structure of embedding spaces is fundamental to representation learning, linking to various model analysis techniques.",
        "novelty_score": 0.48,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "linear probe",
        "canonical": "Linear Probe",
        "aliases": [
          "linear evaluation",
          "linear classifier"
        ],
        "category": "specific_connectable",
        "rationale": "Linear probes are widely used to assess the linear separability of learned representations, connecting to evaluation methods in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "k-Nearest Neighbours probe",
        "canonical": "k-Nearest Neighbours",
        "aliases": [
          "k-NN",
          "kNN probe"
        ],
        "category": "specific_connectable",
        "rationale": "The k-NN probe is a standard technique for analyzing local structure in data, relevant for understanding class clusters in embedding spaces.",
        "novelty_score": 0.52,
        "connectivity_score": 0.82,
        "specificity_score": 0.76,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "fine-tuning",
      "accuracy",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "contrastive-tuning",
      "resolved_canonical": "Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "dual-probe evaluation",
      "resolved_canonical": "Dual-Probe Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "embedding space",
      "resolved_canonical": "Embedding Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "linear probe",
      "resolved_canonical": "Linear Probe",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "k-Nearest Neighbours probe",
      "resolved_canonical": "k-Nearest Neighbours",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.82,
        "specificity": 0.76,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Audio Contrastive-based Fine-tuning: Decoupling Representation Learning and Classification

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2309.11895.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2309.11895](https://arxiv.org/abs/2309.11895)

## 🔗 유사한 논문
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (81.4% similar)
- [[2025-09-23/Evolution of Concepts in Language Model Pre-Training_20250923|Evolution of Concepts in Language Model Pre-Training]] (81.3% similar)
- [[2025-09-23/Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment_20250923|Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment]] (81.2% similar)
- [[2025-09-19/Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior_20250919|Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior]] (81.1% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (80.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Embedding Space|Embedding Space]]
**🔗 Specific Connectable**: [[keywords/Contrastive Learning|Contrastive Learning]], [[keywords/Linear Probe|Linear Probe]], [[keywords/k-Nearest Neighbours|k-Nearest Neighbours]]
**⚡ Unique Technical**: [[keywords/Dual-Probe Evaluation|Dual-Probe Evaluation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2309.11895v4 Announce Type: replace-cross 
Abstract: Standard fine-tuning of pre-trained audio models couples representation learning with classifier training, which can obscure the true quality of the learned representations. In this work, we advocate for a disentangled two-stage framework that separates representation refinement from downstream evaluation. First, we employ a "contrastive-tuning" stage to explicitly improve the geometric structure of the model's embedding space. Subsequently, we introduce a dual-probe evaluation protocol to assess the quality of these refined representations from a geometric perspective. This protocol uses a linear probe to measure global linear separability and a k-Nearest Neighbours probe to investigate the local structure of class clusters. Our experiments on a diverse set of audio classification tasks show that our framework provides a better foundation for classification, leading to improved accuracy. Our newly proposed dual-probing framework acts as a powerful analytical lens, demonstrating why contrastive learning is more effective by revealing a superior embedding space. It significantly outperforms vanilla fine-tuning, particularly on single-label datasets with a large number of classes, and also surpasses strong baselines on multi-label tasks using a Jaccard-weighted loss. Our findings demonstrate that decoupling representation refinement from classifier training is a broadly effective strategy for unlocking the full potential of pre-trained audio models. Our code will be publicly available.

## 📝 요약

이 논문은 사전 학습된 오디오 모델의 표현 학습과 분류기 훈련을 분리하는 두 단계 프레임워크를 제안합니다. 첫 번째 단계에서는 "대조 조정"을 통해 모델의 임베딩 공간의 기하학적 구조를 개선하고, 두 번째 단계에서는 이 개선된 표현의 품질을 평가하기 위해 이중 프로브 평가 프로토콜을 도입합니다. 실험 결과, 이 프레임워크는 다양한 오디오 분류 작업에서 정확도를 향상시키며, 특히 클래스 수가 많은 단일 레이블 데이터셋에서 기존의 미세 조정 방법보다 우수한 성능을 보였습니다. 이 연구는 표현 개선과 분류기 훈련의 분리가 사전 학습된 오디오 모델의 잠재력을 극대화하는 데 효과적임을 보여줍니다.

## 🎯 주요 포인트

- 1. 사전 학습된 오디오 모델의 표현 학습과 분류기 훈련을 분리하는 두 단계 프레임워크를 제안합니다.
- 2. "대조적 튜닝" 단계로 모델의 임베딩 공간의 기하학적 구조를 개선합니다.
- 3. 이중 프로브 평가 프로토콜을 도입하여 개선된 표현의 품질을 기하학적 관점에서 평가합니다.
- 4. 제안된 프레임워크는 다양한 오디오 분류 작업에서 분류의 정확도를 향상시킵니다.
- 5. 표현 개선과 분류기 훈련의 분리는 사전 학습된 오디오 모델의 잠재력을 최대한 발휘하는 효과적인 전략임을 입증합니다.


---

*Generated on 2025-09-24 00:37:25*