---
keywords:
  - Vision-Language Model
  - Backdoor Attacks
  - Self-supervised Learning
  - Prompt Tuning
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2502.19269
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:17:24.531844",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Backdoor Attacks",
    "Self-supervised Learning",
    "Prompt Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.8,
    "Backdoor Attacks": 0.78,
    "Self-supervised Learning": 0.77,
    "Prompt Tuning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on CLIP and backdoor defenses, linking to recent trends in multimodal learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Backdoor Attacks",
        "canonical": "Backdoor Attacks",
        "aliases": [
          "Backdoor Threats",
          "Poisoning Attacks"
        ],
        "category": "unique_technical",
        "rationale": "Backdoor Attacks are a specific threat addressed by the proposed method, crucial for understanding the paper's contribution.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Contrastive Learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "Contrastive Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive Learning is used in the paper to invert backdoor triggers, connecting to broader self-supervised learning techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Prompt Tuning",
        "canonical": "Prompt Tuning",
        "aliases": [
          "Text Prompt Optimization"
        ],
        "category": "unique_technical",
        "rationale": "Prompt Tuning is a novel method proposed in the paper, directly related to modifying model decision boundaries.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "fine-tuning",
      "model parameters",
      "clean accuracy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Backdoor Attacks",
      "resolved_canonical": "Backdoor Attacks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Contrastive Learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Prompt Tuning",
      "resolved_canonical": "Prompt Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in CLIP

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2502.19269.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2502.19269](https://arxiv.org/abs/2502.19269)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Test-Time Multimodal Backdoor Detection by Contrastive Prompting_20250923|Test-Time Multimodal Backdoor Detection by Contrastive Prompting]] (92.1% similar)
- [[2025-09-23/Revisiting Backdoor Attacks on LLMs_ A Stealthy and Practical Poisoning Framework via Harmless Inputs_20250923|Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs]] (85.3% similar)
- [[2025-09-22/Backdoor Mitigation via Invertible Pruning Masks_20250922|Backdoor Mitigation via Invertible Pruning Masks]] (85.2% similar)
- [[2025-09-23/Rethinking Backdoor Detection Evaluation for Language Models_20250923|Rethinking Backdoor Detection Evaluation for Language Models]] (84.7% similar)
- [[2025-09-23/Sugar-Coated Poison_ Benign Generation Unlocks LLM Jailbreaking_20250923|Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Backdoor Attacks|Backdoor Attacks]], [[keywords/Prompt Tuning|Prompt Tuning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.19269v2 Announce Type: replace 
Abstract: While pre-trained Vision-Language Models (VLMs) such as CLIP exhibit impressive representational capabilities for multimodal data, recent studies have revealed their vulnerability to backdoor attacks. To alleviate the threat, existing defense strategies primarily focus on fine-tuning the entire suspicious model. However, the substantial model parameters increase the difficulty of reaching a stable and consistent optimization direction, limiting their resistance against state-of-the-art attacks and often resulting in a degradation of clean accuracy. To address this challenge, we propose Class-wise Backdoor Prompt Tuning (CBPT), an efficient and effective defense mechanism that operates on text prompts to indirectly purify poisoned CLIP. Specifically, we first employ the advanced contrastive learning via carefully crafted positive and negative samples, to effectively invert the backdoor triggers that are potentially adopted by the attacker. Once the dummy trigger is established, we leverage three well-designed loss functions to optimize these class-wise text prompts, modifying the model's decision boundary and further reclassifying the feature regions affected by backdoor triggers. Extensive experiments demonstrate that CBPT significantly mitigates backdoor threats while preserving model utility, e.g. an average Clean Accuracy (CA) of 58.83% and an Attack Success Rate (ASR) of 0.39% across seven mainstream backdoor attacks. These results underscore the superiority of our prompt purifying design to strengthen CLIP's robustness against backdoor attacks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ CLIPê³¼ ê°™ì€ ì‚¬ì „ í•™ìŠµëœ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ë°±ë„ì–´ ê³µê²© ì·¨ì•½ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ Class-wise Backdoor Prompt Tuning (CBPT) ë°©ë²•ë¡ ì„ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ì–´ ì „ëµì€ ì „ì²´ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì— ì§‘ì¤‘í•˜ì§€ë§Œ, ì´ëŠ” ëª¨ë¸ì˜ ì•ˆì •ì  ìµœì í™”ë¥¼ ì–´ë µê²Œ í•˜ê³  ì •í™•ë„ë¥¼ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. CBPTëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ì ‘ì ìœ¼ë¡œ ì˜¤ì—¼ëœ CLIPì„ ì •í™”í•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ì–´ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ, ëŒ€ì¡° í•™ìŠµì„ í†µí•´ ê³µê²©ìê°€ ì‚¬ìš©í•˜ëŠ” ë°±ë„ì–´ íŠ¸ë¦¬ê±°ë¥¼ ë°˜ì „ì‹œí‚µë‹ˆë‹¤. ì„¸ ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í´ë˜ìŠ¤ë³„ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ê³ , ëª¨ë¸ì˜ ê²°ì • ê²½ê³„ë¥¼ ìˆ˜ì •í•˜ì—¬ ë°±ë„ì–´ íŠ¸ë¦¬ê±°ì— ì˜í–¥ì„ ë°›ì€ íŠ¹ì§• ì˜ì—­ì„ ì¬ë¶„ë¥˜í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CBPTëŠ” ë°±ë„ì–´ ìœ„í˜‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•˜ë©´ì„œë„ ëª¨ë¸ì˜ ìœ ìš©ì„±ì„ ìœ ì§€í•˜ë©°, í‰ê·  58.83%ì˜ ê¹¨ë—í•œ ì •í™•ë„(CA)ì™€ 0.39%ì˜ ê³µê²© ì„±ê³µë¥ (ASR)ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ë°±ë„ì–´ ê³µê²©ì— ëŒ€í•œ CLIPì˜ ê°•ê±´ì„±ì„ ê°•í™”í•˜ëŠ” CBPTì˜ ìš°ìˆ˜ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ë°±ë„ì–´ ê³µê²©ì— ì·¨ì•½í•˜ë©°, ì´ë¥¼ ë°©ì–´í•˜ê¸° ìœ„í•œ ê¸°ì¡´ ì „ëµì€ ì£¼ë¡œ ì „ì²´ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤.
- 2. Class-wise Backdoor Prompt Tuning (CBPT)ì€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•˜ì—¬ CLIP ëª¨ë¸ì˜ ë°±ë„ì–´ ê³µê²©ì„ ê°„ì ‘ì ìœ¼ë¡œ ì •í™”í•˜ëŠ” íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ë°©ì–´ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•œë‹¤.
- 3. CBPTëŠ” ê³ ê¸‰ ëŒ€ì¡° í•™ìŠµì„ í†µí•´ ê³µê²©ìê°€ ì‚¬ìš©í•˜ëŠ” ë°±ë„ì–´ íŠ¸ë¦¬ê±°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì—­ì „ì‹œí‚¤ê³ , ì„¸ ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í´ë˜ìŠ¤ë³„ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•œë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, CBPTëŠ” ë°±ë„ì–´ ìœ„í˜‘ì„ í¬ê²Œ ì™„í™”í•˜ë©´ì„œ ëª¨ë¸ì˜ ìœ ìš©ì„±ì„ ìœ ì§€í•˜ë©°, í‰ê·  58.83%ì˜ ê¹¨ë—í•œ ì •í™•ë„(CA)ì™€ 0.39%ì˜ ê³µê²© ì„±ê³µë¥ (ASR)ì„ ê¸°ë¡í–ˆë‹¤.
- 5. CBPTì˜ í”„ë¡¬í”„íŠ¸ ì •í™” ì„¤ê³„ëŠ” CLIPì˜ ë°±ë„ì–´ ê³µê²©ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ê°•í™”í•˜ëŠ” ë° ìˆì–´ ìš°ìˆ˜ì„±ì„ ì…ì¦í•œë‹¤.


---

*Generated on 2025-09-24 05:17:24*