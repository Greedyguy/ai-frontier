---
keywords:
  - Artificial Intelligence Generated Content
  - Song Generation
  - Large Language Model
  - Diarization Error Rate
  - Word Error Rate
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17404
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:55:12.020280",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Artificial Intelligence Generated Content",
    "Song Generation",
    "Large Language Model",
    "Diarization Error Rate",
    "Word Error Rate"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Artificial Intelligence Generated Content": 0.78,
    "Song Generation": 0.77,
    "Large Language Model": 0.82,
    "Diarization Error Rate": 0.74,
    "Word Error Rate": 0.73
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Artificial Intelligence Generated Content",
        "canonical": "Artificial Intelligence Generated Content",
        "aliases": [
          "AIGC"
        ],
        "category": "unique_technical",
        "rationale": "AIGC is a specific and emerging area within AI research, relevant for linking to content generation topics.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "song generation",
        "canonical": "Song Generation",
        "aliases": [
          "music generation"
        ],
        "category": "unique_technical",
        "rationale": "Song generation is a specialized application of AI in music, facilitating connections to creative AI.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "pretrained language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "pretrained models"
        ],
        "category": "broad_technical",
        "rationale": "Pretrained language models are foundational in NLP and connect to various AI applications.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Diarization Error Rate",
        "canonical": "Diarization Error Rate",
        "aliases": [
          "DER"
        ],
        "category": "specific_connectable",
        "rationale": "DER is a specific metric in audio processing, relevant for linking to speech and audio analysis.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.74
      },
      {
        "surface": "Word Error Rate",
        "canonical": "Word Error Rate",
        "aliases": [
          "WER"
        ],
        "category": "specific_connectable",
        "rationale": "WER is a common metric in speech recognition, facilitating connections to NLP and audio processing.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.76,
        "link_intent_score": 0.73
      }
    ],
    "ban_list_suggestions": [
      "preprocessing",
      "framework",
      "model",
      "dataset"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Artificial Intelligence Generated Content",
      "resolved_canonical": "Artificial Intelligence Generated Content",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "song generation",
      "resolved_canonical": "Song Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "pretrained language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Diarization Error Rate",
      "resolved_canonical": "Diarization Error Rate",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Word Error Rate",
      "resolved_canonical": "Word Error Rate",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.76,
        "link_intent": 0.73
      }
    }
  ]
}
-->

# SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17404.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17404](https://arxiv.org/abs/2509.17404)

## 🔗 유사한 논문
- [[2025-09-18/MAVL_ A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation_20250918|MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation]] (80.1% similar)
- [[2025-09-22/SuPreME_ A Supervised Pre-training Framework for Multimodal ECG Representation Learning_20250922|SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning]] (80.0% similar)
- [[2025-09-19/SpeechOp_ Inference-Time Task Composition for Generative Speech Processing_20250919|SpeechOp: Inference-Time Task Composition for Generative Speech Processing]] (79.6% similar)
- [[2025-09-22/Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization_20250922|Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization]] (79.5% similar)
- [[2025-09-22/Frustratingly Easy Data Augmentation for Low-Resource ASR_20250922|Frustratingly Easy Data Augmentation for Low-Resource ASR]] (79.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Diarization Error Rate|Diarization Error Rate]], [[keywords/Word Error Rate|Word Error Rate]]
**⚡ Unique Technical**: [[keywords/Artificial Intelligence Generated Content|Artificial Intelligence Generated Content]], [[keywords/Song Generation|Song Generation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17404v1 Announce Type: cross 
Abstract: Artificial Intelligence Generated Content (AIGC) is currently a popular research area. Among its various branches, song generation has attracted growing interest. Despite the abundance of available songs, effective data preparation remains a significant challenge. Converting these songs into training-ready datasets typically requires extensive manual labeling, which is both time consuming and costly. To address this issue, we propose SongPrep, an automated preprocessing pipeline designed specifically for song data. This framework streamlines key processes such as source separation, structure analysis, and lyric recognition, producing structured data that can be directly used to train song generation models. Furthermore, we introduce SongPrepE2E, an end-to-end structured lyrics recognition model based on pretrained language models. Without the need for additional source separation, SongPrepE2E is able to analyze the structure and lyrics of entire songs and provide precise timestamps. By leveraging context from the whole song alongside pretrained semantic knowledge, SongPrepE2E achieves low Diarization Error Rate (DER) and Word Error Rate (WER) on the proposed SSLD-200 dataset. Downstream tasks demonstrate that training song generation models with the data output by SongPrepE2E enables the generated songs to closely resemble those produced by humans.

## 📝 요약

이 논문은 노래 생성 분야에서 효과적인 데이터 준비의 어려움을 해결하기 위해 SongPrep이라는 자동화된 전처리 파이프라인을 제안합니다. SongPrep은 소스 분리, 구조 분석, 가사 인식을 자동화하여 노래 생성 모델 훈련에 직접 사용할 수 있는 구조화된 데이터를 생성합니다. 또한, SongPrepE2E라는 사전 학습된 언어 모델 기반의 종단 간 구조화 가사 인식 모델을 소개합니다. SongPrepE2E는 추가적인 소스 분리 없이 전체 노래의 구조와 가사를 분석하고 정확한 타임스탬프를 제공합니다. 이 모델은 SSLD-200 데이터셋에서 낮은 발화 오류율(DER)과 단어 오류율(WER)을 달성하며, 생성된 노래가 인간이 만든 것과 유사하게 만드는 데 기여합니다.

## 🎯 주요 포인트

- 1. SongPrep은 노래 데이터를 위한 자동화된 전처리 파이프라인으로, 소스 분리, 구조 분석, 가사 인식을 통해 구조화된 데이터를 생성합니다.
- 2. SongPrepE2E는 사전 학습된 언어 모델을 기반으로 한 엔드 투 엔드 구조의 가사 인식 모델로, 추가적인 소스 분리 없이 전체 노래의 구조와 가사를 분석하고 정확한 타임스탬프를 제공합니다.
- 3. SongPrepE2E는 전체 노래의 문맥과 사전 학습된 의미적 지식을 활용하여 SSLD-200 데이터셋에서 낮은 Diarization Error Rate (DER)와 Word Error Rate (WER)를 달성합니다.
- 4. SongPrepE2E의 데이터로 훈련된 노래 생성 모델은 인간이 만든 노래와 유사한 결과물을 생성할 수 있습니다.


---

*Generated on 2025-09-23 23:55:12*