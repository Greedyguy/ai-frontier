---
keywords:
  - Large Language Model
  - Context-Directed Extrapolation
  - In-Context Learning
  - Instruction Tuning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2505.23323
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:01:40.980221",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Context-Directed Extrapolation",
    "In-Context Learning",
    "Instruction Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Context-Directed Extrapolation": 0.7,
    "In-Context Learning": 0.68,
    "Instruction Tuning": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion, linking to a broad range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "context-directed extrapolation",
        "canonical": "Context-Directed Extrapolation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specific to the paper's argument, offering new linkage opportunities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "in-context learning",
        "canonical": "In-Context Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A specific mechanism discussed in the paper, relevant to recent advancements in LLMs.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.68
      },
      {
        "surface": "instruction tuning",
        "canonical": "Instruction Tuning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights a method for enhancing LLM capabilities, connecting to ongoing research in model fine-tuning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "stochastic parroting",
      "emergent advanced reasoning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "context-directed extrapolation",
      "resolved_canonical": "Context-Directed Extrapolation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "in-context learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.68
      }
    },
    {
      "candidate_surface": "instruction tuning",
      "resolved_canonical": "Instruction Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.23323.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2505.23323](https://arxiv.org/abs/2505.23323)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (88.2% similar)
- [[2025-09-23/Question Answering with LLMs and Learning from Answer Sets_20250923|Question Answering with LLMs and Learning from Answer Sets]] (87.6% similar)
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (87.3% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (87.2% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (87.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/In-Context Learning|In-Context Learning]], [[keywords/Instruction Tuning|Instruction Tuning]]
**âš¡ Unique Technical**: [[keywords/Context-Directed Extrapolation|Context-Directed Extrapolation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.23323v2 Announce Type: replace 
Abstract: In this position paper we raise critical awareness of a realistic view of LLM capabilities that eschews extreme alternative views that LLMs are either 'stochastic parrots' or in possession of 'emergent' advanced reasoning capabilities, which, due to their unpredictable emergence, constitute an existential threat. Our middle-ground view is that LLMs extrapolate from priors from their training data while using context to guide the model to the appropriate priors; we call this "context-directed extrapolation." Specifically, this context direction is achieved through examples in base models, leading to in-context learning, while instruction tuning allows LLMs to perform similarly based on prompts rather than explicit examples. Under this view, substantiated though existing literature, while reasoning capabilities go well beyond stochastic parroting, such capabilities are predictable, controllable, not indicative of advanced reasoning akin to high-level cognitive capabilities in humans, and not infinitely scalable with additional training. As a result, fears of uncontrollable emergence of agency are allayed, while research advances are appropriately refocused on the processes of context-directed extrapolation and how this interacts with training data to produce valuable capabilities in LLMs. Future work can therefore explore alternative augmenting techniques that do not rely on inherent advanced reasoning in LLMs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ëŠ¥ë ¥ì— ëŒ€í•œ í˜„ì‹¤ì ì¸ ê´€ì ì„ ì œì‹œí•˜ë©°, LLMì´ ë‹¨ìˆœíˆ 'í™•ë¥ ì  ì•µë¬´ìƒˆ'ì´ê±°ë‚˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ê³ ê¸‰ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ë‹¤ëŠ” ê·¹ë‹¨ì ì¸ ê²¬í•´ë¥¼ ì§€ì–‘í•©ë‹ˆë‹¤. ì €ìë“¤ì€ LLMì´ í›ˆë ¨ ë°ì´í„°ì˜ ì‚¬ì „ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ë§¥ì„ í™œìš©í•´ ì ì ˆí•œ ì‚¬ì „ ì •ë³´ë¥¼ ì¶”ë¡ í•˜ëŠ” "ë¬¸ë§¥ ì§€í–¥ì  ì™¸ì‚½"ì„ ìˆ˜í–‰í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê¸°ë³¸ ëª¨ë¸ì˜ ì˜ˆì œë¥¼ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ì§€ì‹œ ì¡°ì •ì„ í†µí•´ ëª…ì‹œì  ì˜ˆì œ ì—†ì´ë„ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ì ì—ì„œ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì€ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  í†µì œ ê°€ëŠ¥í•˜ë©°, ì¸ê°„ì˜ ê³ ê¸‰ ì¸ì§€ ëŠ¥ë ¥ê³¼ëŠ” ë‹¤ë¥´ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ë”°ë¼ì„œ LLMì˜ í†µì œ ë¶ˆê°€ëŠ¥í•œ ììœ¨ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ì™„í™”í•˜ê³ , ë¬¸ë§¥ ì§€í–¥ì  ì™¸ì‚½ ê³¼ì •ê³¼ í›ˆë ¨ ë°ì´í„°ì˜ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ì—°êµ¬ì— ì§‘ì¤‘í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. í–¥í›„ ì—°êµ¬ëŠ” LLMì˜ ê³ ê¸‰ ì¶”ë¡ ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ëŒ€ì²´ ë³´ê°• ê¸°ë²•ì„ íƒêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMì˜ ëŠ¥ë ¥ì€ ê·¹ë‹¨ì ì¸ ê´€ì ì´ ì•„ë‹Œ, í›ˆë ¨ ë°ì´í„°ì˜ ì‚¬ì „ ì§€ì‹ì„ ë¬¸ë§¥ì„ í†µí•´ ì ì ˆíˆ í™œìš©í•˜ëŠ” "ë¬¸ë§¥ ì§€í–¥ì  ì™¸ì‚½"ìœ¼ë¡œ ì„¤ëª…ë©ë‹ˆë‹¤.
- 2. ë¬¸ë§¥ ì§€í–¥ì  ì™¸ì‚½ì€ ê¸°ë³¸ ëª¨ë¸ì˜ ì˜ˆì‹œë¥¼ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ëª…ì‹œì ì¸ ì˜ˆì‹œ ì—†ì´ë„ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- 3. LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì€ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì œì–´ ê°€ëŠ¥í•˜ë©°, ì¸ê°„ì˜ ê³ ì°¨ì› ì¸ì§€ ëŠ¥ë ¥ê³¼ ìœ ì‚¬í•œ ê³ ê¸‰ ì¶”ë¡ ì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 4. LLMì˜ ëŠ¥ë ¥ì€ ì¶”ê°€ í›ˆë ¨ì„ í†µí•´ ë¬´í•œíˆ í™•ì¥ë˜ì§€ ì•Šìœ¼ë©°, ììœ¨ì„±ì˜ í†µì œ ë¶ˆê°€ëŠ¥í•œ ì¶œí˜„ì— ëŒ€í•œ ìš°ë ¤ë¥¼ í•´ì†Œí•©ë‹ˆë‹¤.
- 5. ë¯¸ë˜ ì—°êµ¬ëŠ” LLMì˜ ê³ ìœ í•œ ê³ ê¸‰ ì¶”ë¡ ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ëŒ€ì²´ ë³´ê°• ê¸°ìˆ ì„ íƒêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:01:40*