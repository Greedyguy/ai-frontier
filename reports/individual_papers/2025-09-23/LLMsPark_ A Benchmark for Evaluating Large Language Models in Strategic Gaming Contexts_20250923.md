---
keywords:
  - Large Language Model
  - Game Theory Evaluation
  - Strategic Intelligence
  - Multi-Agent Systems
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16610
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:15:29.740802",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Game Theory Evaluation",
    "Strategic Intelligence",
    "Multi-Agent Systems"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Game Theory Evaluation": 0.78,
    "Strategic Intelligence": 0.74,
    "Multi-Agent Systems": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's evaluation focus, providing a broad technical context for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "game theory-based evaluation platform",
        "canonical": "Game Theory Evaluation",
        "aliases": [
          "game-theoretic evaluation"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel evaluation method specific to strategic contexts, enhancing connectivity with game theory concepts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "strategic intelligence",
        "canonical": "Strategic Intelligence",
        "aliases": [
          "strategic reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a unique aspect of LLM evaluation, linking to cognitive and decision-making research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.74
      },
      {
        "surface": "multi-agent environment",
        "canonical": "Multi-Agent Systems",
        "aliases": [
          "multi-agent setting"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to research on interactions and dynamics in systems with multiple agents.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "interactive dynamics",
      "leaderboard rankings"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "game theory-based evaluation platform",
      "resolved_canonical": "Game Theory Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "strategic intelligence",
      "resolved_canonical": "Strategic Intelligence",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "multi-agent environment",
      "resolved_canonical": "Multi-Agent Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16610.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16610](https://arxiv.org/abs/2509.16610)

## 🔗 유사한 논문
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (87.5% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (86.8% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (86.5% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.5% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (86.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multi-Agent Systems|Multi-Agent Systems]]
**⚡ Unique Technical**: [[keywords/Game Theory Evaluation|Game Theory Evaluation]], [[keywords/Strategic Intelligence|Strategic Intelligence]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16610v1 Announce Type: new 
Abstract: As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 지능을 평가하기 위해 게임 이론 기반의 평가 플랫폼인 LLMsPark를 소개합니다. LLMsPark는 LLM의 의사 결정 전략과 사회적 행동을 고전적인 게임 이론 환경에서 측정하며, 15개의 주요 LLM을 상호 평가하여 전략적 깊이를 탐구합니다. 이 시스템은 리더보드 순위와 점수 메커니즘을 통해 모델 간의 성능 차이와 행동 패턴을 드러냅니다. 이 연구는 LLM의 전략적 지능을 평가하는 새로운 관점을 제시하며, 기존 벤치마크를 풍부하게 하고 상호작용적 게임 이론 시나리오에서의 평가를 확장합니다. 벤치마크와 순위는 공개적으로 제공됩니다.

## 🎯 주요 포인트

- 1. LLMsPark는 게임 이론 기반 평가 플랫폼으로, 대형 언어 모델(LLM)의 의사 결정 전략과 사회적 행동을 측정합니다.
- 2. 이 시스템은 15개의 주요 LLM을 리더보드 순위와 점수 메커니즘을 통해 교차 평가합니다.
- 3. 높은 점수는 강력한 추론 및 전략적 능력을 반영하며, 모델 간의 뚜렷한 행동 패턴과 성능 차이를 드러냅니다.
- 4. 이 연구는 LLM의 전략적 지능을 평가하는 새로운 관점을 제시하여 기존 벤치마크를 풍부하게 하고 평가 범위를 확장합니다.
- 5. 벤치마크와 순위는 https://llmsparks.github.io/에서 공개적으로 제공됩니다.


---

*Generated on 2025-09-24 03:15:29*