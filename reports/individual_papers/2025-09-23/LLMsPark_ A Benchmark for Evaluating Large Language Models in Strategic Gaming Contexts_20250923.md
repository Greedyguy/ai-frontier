---
keywords:
  - Large Language Model
  - Game Theory Evaluation
  - Strategic Intelligence
  - Multi-Agent Systems
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16610
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:15:29.740802",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Game Theory Evaluation",
    "Strategic Intelligence",
    "Multi-Agent Systems"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Game Theory Evaluation": 0.78,
    "Strategic Intelligence": 0.74,
    "Multi-Agent Systems": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's evaluation focus, providing a broad technical context for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "game theory-based evaluation platform",
        "canonical": "Game Theory Evaluation",
        "aliases": [
          "game-theoretic evaluation"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel evaluation method specific to strategic contexts, enhancing connectivity with game theory concepts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "strategic intelligence",
        "canonical": "Strategic Intelligence",
        "aliases": [
          "strategic reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a unique aspect of LLM evaluation, linking to cognitive and decision-making research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.74
      },
      {
        "surface": "multi-agent environment",
        "canonical": "Multi-Agent Systems",
        "aliases": [
          "multi-agent setting"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to research on interactions and dynamics in systems with multiple agents.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "interactive dynamics",
      "leaderboard rankings"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "game theory-based evaluation platform",
      "resolved_canonical": "Game Theory Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "strategic intelligence",
      "resolved_canonical": "Strategic Intelligence",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "multi-agent environment",
      "resolved_canonical": "Multi-Agent Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16610.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16610](https://arxiv.org/abs/2509.16610)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (87.5% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (86.8% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (86.5% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.5% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (86.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multi-Agent Systems|Multi-Agent Systems]]
**âš¡ Unique Technical**: [[keywords/Game Theory Evaluation|Game Theory Evaluation]], [[keywords/Strategic Intelligence|Strategic Intelligence]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16610v1 Announce Type: new 
Abstract: As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì§€ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê²Œì„ ì´ë¡  ê¸°ë°˜ì˜ í‰ê°€ í”Œë«í¼ì¸ LLMsParkë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. LLMsParkëŠ” LLMì˜ ì˜ì‚¬ ê²°ì • ì „ëµê³¼ ì‚¬íšŒì  í–‰ë™ì„ ê³ ì „ì ì¸ ê²Œì„ ì´ë¡  í™˜ê²½ì—ì„œ ì¸¡ì •í•˜ë©°, 15ê°œì˜ ì£¼ìš” LLMì„ ìƒí˜¸ í‰ê°€í•˜ì—¬ ì „ëµì  ê¹Šì´ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ë¦¬ë”ë³´ë“œ ìˆœìœ„ì™€ ì ìˆ˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ì™€ í–‰ë™ íŒ¨í„´ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ì „ëµì  ì§€ëŠ¥ì„ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ë©°, ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë¥¼ í’ë¶€í•˜ê²Œ í•˜ê³  ìƒí˜¸ì‘ìš©ì  ê²Œì„ ì´ë¡  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ í‰ê°€ë¥¼ í™•ì¥í•©ë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ì™€ ìˆœìœ„ëŠ” ê³µê°œì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMsParkëŠ” ê²Œì„ ì´ë¡  ê¸°ë°˜ í‰ê°€ í”Œë«í¼ìœ¼ë¡œ, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì˜ì‚¬ ê²°ì • ì „ëµê³¼ ì‚¬íšŒì  í–‰ë™ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
- 2. ì´ ì‹œìŠ¤í…œì€ 15ê°œì˜ ì£¼ìš” LLMì„ ë¦¬ë”ë³´ë“œ ìˆœìœ„ì™€ ì ìˆ˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ êµì°¨ í‰ê°€í•©ë‹ˆë‹¤.
- 3. ë†’ì€ ì ìˆ˜ëŠ” ê°•ë ¥í•œ ì¶”ë¡  ë° ì „ëµì  ëŠ¥ë ¥ì„ ë°˜ì˜í•˜ë©°, ëª¨ë¸ ê°„ì˜ ëšœë ·í•œ í–‰ë™ íŒ¨í„´ê³¼ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.
- 4. ì´ ì—°êµ¬ëŠ” LLMì˜ ì „ëµì  ì§€ëŠ¥ì„ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ì—¬ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë¥¼ í’ë¶€í•˜ê²Œ í•˜ê³  í‰ê°€ ë²”ìœ„ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.
- 5. ë²¤ì¹˜ë§ˆí¬ì™€ ìˆœìœ„ëŠ” https://llmsparks.github.io/ì—ì„œ ê³µê°œì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:15:29*