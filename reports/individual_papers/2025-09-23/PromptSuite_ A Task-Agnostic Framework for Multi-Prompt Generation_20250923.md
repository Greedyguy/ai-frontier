---
keywords:
  - Large Language Model
  - PromptSuite
  - Multi-Prompt Evaluation
  - Modular Prompt Design
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2507.14913
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:08:08.647430",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "PromptSuite",
    "Multi-Prompt Evaluation",
    "Modular Prompt Design"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "PromptSuite": 0.8,
    "Multi-Prompt Evaluation": 0.78,
    "Modular Prompt Design": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating language models with multiple prompts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "PromptSuite",
        "canonical": "PromptSuite",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique framework introduced in the paper, essential for understanding its contributions.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multi-Prompt Evaluation",
        "canonical": "Multi-Prompt Evaluation",
        "aliases": [
          "Multi-Prompt Testing"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for evaluating the robustness of language models, connecting to evaluation methodologies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Modular Prompt Design",
        "canonical": "Modular Prompt Design",
        "aliases": [
          "Prompt Modularity"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific design approach in the framework, highlighting its flexibility.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "framework",
      "tasks",
      "benchmarks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "PromptSuite",
      "resolved_canonical": "PromptSuite",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multi-Prompt Evaluation",
      "resolved_canonical": "Multi-Prompt Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Modular Prompt Design",
      "resolved_canonical": "Modular Prompt Design",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2507.14913.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2507.14913](https://arxiv.org/abs/2507.14913)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Prompt-with-Me_ in-IDE Structured Prompt Management for LLM-Driven Software Engineering_20250923|Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering]] (89.3% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (84.6% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (83.1% similar)
- [[2025-09-23/QA-prompting_ Improving Summarization with Large Language Models using Question-Answering_20250923|QA-prompting: Improving Summarization with Large Language Models using Question-Answering]] (83.0% similar)
- [[2025-09-17/A Taxonomy of Prompt Defects in LLM Systems_20250917|A Taxonomy of Prompt Defects in LLM Systems]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multi-Prompt Evaluation|Multi-Prompt Evaluation]]
**âš¡ Unique Technical**: [[keywords/PromptSuite|PromptSuite]], [[keywords/Modular Prompt Design|Modular Prompt Design]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.14913v4 Announce Type: replace 
Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice. To address this, we introduce PromptSuite, a framework that enables the automatic generation of various prompts. PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types. Through a series of case studies, we show that PromptSuite provides meaningful variations to support strong evaluation practices. All resources, including the Python API, source code, user-friendly web interface, and demonstration video, are available at: https://eliyahabba.github.io/PromptSuite/.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í‰ê°€í•˜ëŠ” ê²ƒì´ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ PromptSuiteë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. PromptSuiteëŠ” ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ë³´ë‹¤ ê²¬ê³ í•œ ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì–‘í•œ ì‘ì—…ê³¼ ë²¤ì¹˜ë§ˆí¬ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ëª¨ë“ˆì‹ ì„¤ê³„ë¥¼ í†µí•´ ê° êµ¬ì„± ìš”ì†Œë¥¼ ì œì–´ëœ ë°©ì‹ìœ¼ë¡œ ë³€í˜•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ìƒˆë¡œìš´ êµ¬ì„± ìš”ì†Œì™€ ë³€í˜• ìœ í˜•ì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‚¬ë¡€ ì—°êµ¬ë¥¼ í†µí•´ PromptSuiteê°€ ê°•ë ¥í•œ í‰ê°€ë¥¼ ì§€ì›í•˜ëŠ” ìœ ì˜ë¯¸í•œ ë³€í™”ë¥¼ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë“  ë¦¬ì†ŒìŠ¤ëŠ” ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ LLMì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ì‹ ë¢°ì„±ì´ ë‚®ìœ¼ë©°, ì‘ì€ ë³€í™”ì—ë„ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ê²Œ ë°œìƒí•œë‹¤.
- 2. PromptSuiteëŠ” ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ì—¬ëŸ¬ ì‘ì—…ê³¼ ë²¤ì¹˜ë§ˆí¬ì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•˜ë‹¤.
- 3. ëª¨ë“ˆì‹ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ë¥¼ í†µí•´ ê° êµ¬ì„± ìš”ì†Œì— ëŒ€í•œ ì œì–´ëœ ë³€í™”ë¥¼ í—ˆìš©í•˜ë©°, ìƒˆë¡œìš´ êµ¬ì„± ìš”ì†Œì™€ ë³€í™” ìœ í˜•ì˜ ì¶”ê°€ë¥¼ ì§€ì›í•œë‹¤.
- 4. ì‚¬ë¡€ ì—°êµ¬ë¥¼ í†µí•´ PromptSuiteê°€ ê°•ë ¥í•œ í‰ê°€ë¥¼ ì§€ì›í•˜ëŠ” ì˜ë¯¸ ìˆëŠ” ë³€í™”ë¥¼ ì œê³µí•¨ì„ ì…ì¦í•˜ì˜€ë‹¤.
- 5. PromptSuiteì˜ ëª¨ë“  ë¦¬ì†ŒìŠ¤ëŠ” ì›¹ ì¸í„°í˜ì´ìŠ¤ì™€ ë°ëª¨ ë¹„ë””ì˜¤ë¥¼ í¬í•¨í•˜ì—¬ ê³µê°œì ìœ¼ë¡œ ì œê³µëœë‹¤.


---

*Generated on 2025-09-24 04:08:08*