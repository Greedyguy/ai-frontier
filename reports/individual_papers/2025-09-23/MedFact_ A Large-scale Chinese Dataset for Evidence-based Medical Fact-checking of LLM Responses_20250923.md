---
keywords:
  - Large Language Model
  - Medical Fact-checking
  - In-context Learning
  - Fine-tuning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17436
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:26:20.063150",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Medical Fact-checking",
    "In-context Learning",
    "Fine-tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Medical Fact-checking": 0.78,
    "In-context Learning": 0.74,
    "Fine-tuning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus on evaluating LLM-generated content, providing a strong link to existing discussions on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Medical Fact-checking",
        "canonical": "Medical Fact-checking",
        "aliases": [
          "Medical Verification",
          "Healthcare Fact-checking"
        ],
        "category": "unique_technical",
        "rationale": "This is a key concept introduced by the paper, focusing on the verification of medical information, which is crucial for linking to related datasets and methodologies.",
        "novelty_score": 0.72,
        "connectivity_score": 0.67,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "In-context Learning",
        "canonical": "In-context Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "This learning approach is significant for understanding how LLMs can be adapted to new tasks, facilitating connections to broader machine learning strategies.",
        "novelty_score": 0.58,
        "connectivity_score": 0.75,
        "specificity_score": 0.79,
        "link_intent_score": 0.74
      },
      {
        "surface": "Fine-tuning",
        "canonical": "Fine-tuning",
        "aliases": [
          "Model Fine-tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Fine-tuning is a critical process in adapting models to specific tasks, linking to discussions on model optimization and performance enhancement.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "experiments",
      "error analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Medical Fact-checking",
      "resolved_canonical": "Medical Fact-checking",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.67,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "In-context Learning",
      "resolved_canonical": "In-context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.75,
        "specificity": 0.79,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Fine-tuning",
      "resolved_canonical": "Fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17436.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17436](https://arxiv.org/abs/2509.17436)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/MedFact-R1_ Towards Factual Medical Reasoning via Pseudo-Label Augmentation_20250919|MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation]] (84.6% similar)
- [[2025-09-23/ReasonMed_ A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning_20250923|ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning]] (83.9% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (83.8% similar)
- [[2025-09-17/Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification_20250917|Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification]] (82.5% similar)
- [[2025-09-17/Combining Evidence and Reasoning for Biomedical Fact-Checking_20250917|Combining Evidence and Reasoning for Biomedical Fact-Checking]] (82.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/In-context Learning|In-context Learning]], [[keywords/Fine-tuning|Fine-tuning]]
**âš¡ Unique Technical**: [[keywords/Medical Fact-checking|Medical Fact-checking]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17436v1 Announce Type: new 
Abstract: Medical fact-checking has become increasingly critical as more individuals seek medical information online. However, existing datasets predominantly focus on human-generated content, leaving the verification of content generated by large language models (LLMs) relatively unexplored. To address this gap, we introduce MedFact, the first evidence-based Chinese medical fact-checking dataset of LLM-generated medical content. It consists of 1,321 questions and 7,409 claims, mirroring the complexities of real-world medical scenarios. We conduct comprehensive experiments in both in-context learning (ICL) and fine-tuning settings, showcasing the capability and challenges of current LLMs on this task, accompanied by an in-depth error analysis to point out key directions for future research. Our dataset is publicly available at https://github.com/AshleyChenNLP/MedFact.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì´ ìƒì„±í•œ ì˜ë£Œ ì½˜í…ì¸ ì˜ ì‚¬ì‹¤ ê²€ì¦ì„ ìœ„í•œ ìµœì´ˆì˜ ì¦ê±° ê¸°ë°˜ ì¤‘êµ­ì–´ ë°ì´í„°ì…‹ì¸ MedFactë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 1,321ê°œì˜ ì§ˆë¬¸ê³¼ 7,409ê°œì˜ ì£¼ì¥ì„ í¬í•¨í•˜ë©°, ì‹¤ì œ ì˜ë£Œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ë³µì¡ì„±ì„ ë°˜ì˜í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ë¬¸ë§¥ í•™ìŠµê³¼ ë¯¸ì„¸ ì¡°ì • í™˜ê²½ì—ì„œ LLMì˜ ëŠ¥ë ¥ê³¼ ë„ì „ ê³¼ì œë¥¼ ì‹¤í—˜í•˜ê³ , ì‹¬ì¸µ ì˜¤ë¥˜ ë¶„ì„ì„ í†µí•´ í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì€ ê³µê°œë˜ì–´ ìˆìœ¼ë©°, ì—°êµ¬ëŠ” LLM ê¸°ë°˜ ì˜ë£Œ ì •ë³´ ê²€ì¦ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MedFactëŠ” LLMì´ ìƒì„±í•œ ì˜ë£Œ ì½˜í…ì¸ ì˜ ì‚¬ì‹¤ ê²€ì¦ì„ ìœ„í•œ ìµœì´ˆì˜ ì¦ê±° ê¸°ë°˜ ì¤‘êµ­ì–´ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
- 2. ë°ì´í„°ì…‹ì€ 1,321ê°œì˜ ì§ˆë¬¸ê³¼ 7,409ê°œì˜ ì£¼ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ì‹¤ì œ ì˜ë£Œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ë³µì¡ì„±ì„ ë°˜ì˜í•©ë‹ˆë‹¤.
- 3. ì—°êµ¬ëŠ” in-context learning(ICL)ê³¼ íŒŒì¸íŠœë‹ ì„¤ì •ì—ì„œ LLMì˜ ëŠ¥ë ¥ê³¼ ë„ì „ ê³¼ì œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. ì‹¬ì¸µ ì˜¤ë¥˜ ë¶„ì„ì„ í†µí•´ í–¥í›„ ì—°êµ¬ì˜ ì£¼ìš” ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 5. MedFact ë°ì´í„°ì…‹ì€ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•˜ë©°, GitHubì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:26:20*