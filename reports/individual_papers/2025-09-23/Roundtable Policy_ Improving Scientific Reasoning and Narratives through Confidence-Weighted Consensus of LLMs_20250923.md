---
keywords:
  - Large Language Model
  - Roundtable Policy
  - Consensus of Multiple LLMs
  - Scientific Narratives
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16839
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:52:21.422638",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Roundtable Policy",
    "Consensus of Multiple LLMs",
    "Scientific Narratives"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "Roundtable Policy": 0.85,
    "Consensus of Multiple LLMs": 0.78,
    "Scientific Narratives": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology and connects to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Roundtable Policy",
        "canonical": "Roundtable Policy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel framework introduced in the paper, enhancing reasoning through LLM consensus.",
        "novelty_score": 0.9,
        "connectivity_score": 0.5,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Consensus of multiple LLMs",
        "canonical": "Consensus of Multiple LLMs",
        "aliases": [
          "LLM Consensus"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for linking multi-agent reasoning and decision-making strategies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Scientific narratives",
        "canonical": "Scientific Narratives",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Important for linking discussions on improving narrative quality in scientific contexts.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "inference",
      "framework",
      "approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Roundtable Policy",
      "resolved_canonical": "Roundtable Policy",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.5,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Consensus of multiple LLMs",
      "resolved_canonical": "Consensus of Multiple LLMs",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Scientific narratives",
      "resolved_canonical": "Scientific Narratives",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16839.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16839](https://arxiv.org/abs/2509.16839)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (85.9% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (85.5% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (85.5% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (85.3% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Consensus of Multiple LLMs|Consensus of Multiple LLMs]], [[keywords/Scientific Narratives|Scientific Narratives]]
**âš¡ Unique Technical**: [[keywords/Roundtable Policy|Roundtable Policy]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16839v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities not only in language generation but also in advancing scientific discovery. A growing body of work has explored ways to improve their reasoning, from self-consistency and chain-of-thought to multi-agent debate. Inspired by the dynamics of scientific committees and the "Society of Mind," we introduce Roundtable Policy, a complementary inference-time reasoning framework that performs inference through the weighted consensus of multiple LLMs. Our findings indicate that this approach significantly enhances reasoning in complex heterogeneous scientific tasks and improves scientific narratives in terms of creativity, rigor, and logical coherence, while reducing hallucinations that single models are prone to. Our approach emphasizes structured and interpretable consensus rather than opaque convergence, while requiring only black-box access and uniform procedures, making it broadly applicable to multi-LLM reasoning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ 'ë¼ìš´ë“œí…Œì´ë¸” ì •ì±…'ì´ë¼ëŠ” ìƒˆë¡œìš´ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì—¬ëŸ¬ LLMì˜ ê°€ì¤‘ í•©ì˜ë¥¼ í†µí•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë©°, ê³¼í•™ì  ìœ„ì›íšŒì˜ ì—­í•™ê³¼ 'ë§ˆìŒì˜ ì‚¬íšŒ' ê°œë…ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì´ ì ‘ê·¼ë²•ì€ ë³µì¡í•˜ê³  ì´ì§ˆì ì¸ ê³¼í•™ì  ê³¼ì œì—ì„œ ì¶”ë¡  ëŠ¥ë ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³ , ì°½ì˜ì„±, ì—„ë°€ì„±, ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ê°œì„ í•˜ë©°, ë‹¨ì¼ ëª¨ë¸ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” í™˜ê°ì„ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë°©ë²•ì€ êµ¬ì¡°í™”ëœ í•´ì„ ê°€ëŠ¥í•œ í•©ì˜ë¥¼ ê°•ì¡°í•˜ë©°, ë¸”ë™ë°•ìŠ¤ ì ‘ê·¼ê³¼ ì¼ê´€ëœ ì ˆì°¨ë§Œ í•„ìš”ë¡œ í•˜ì—¬ ë‹¤ì¤‘ LLM ì¶”ë¡ ì— í­ë„“ê²Œ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì–¸ì–´ ìƒì„±ë¿ë§Œ ì•„ë‹ˆë¼ ê³¼í•™ì  ë°œê²¬ì„ ë°œì „ì‹œí‚¤ëŠ” ë°ë„ ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.
- 2. Roundtable PolicyëŠ” ì—¬ëŸ¬ LLMì˜ ê°€ì¤‘ì¹˜ í•©ì˜ë¥¼ í†µí•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ë³´ì™„ì  ì¶”ë¡  í”„ë ˆì„ì›Œí¬ë¡œ ì œì•ˆë˜ì—ˆë‹¤.
- 3. ì´ ì ‘ê·¼ ë°©ì‹ì€ ë³µì¡í•œ ì´ì§ˆì  ê³¼í•™ ê³¼ì œì—ì„œì˜ ì¶”ë¡ ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³ , ì°½ì˜ì„±, ì—„ë°€ì„±, ë…¼ë¦¬ì  ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ê³¼í•™ì  ì„œì‚¬ë¥¼ ê°œì„ í•œë‹¤.
- 4. Roundtable PolicyëŠ” ë‹¨ì¼ ëª¨ë¸ì´ ìì£¼ ê²ªëŠ” í™˜ê°ì„ ì¤„ì´ë©°, êµ¬ì¡°ì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ í•©ì˜ë¥¼ ê°•ì¡°í•œë‹¤.
- 5. ì´ ë°©ë²•ì€ ë¸”ë™ë°•ìŠ¤ ì ‘ê·¼ê³¼ ê· ì¼í•œ ì ˆì°¨ë§Œì„ ìš”êµ¬í•˜ì—¬, ë‹¤ì¤‘ LLM ì¶”ë¡ ì— í­ë„“ê²Œ ì ìš© ê°€ëŠ¥í•˜ë‹¤.


---

*Generated on 2025-09-23 22:52:21*