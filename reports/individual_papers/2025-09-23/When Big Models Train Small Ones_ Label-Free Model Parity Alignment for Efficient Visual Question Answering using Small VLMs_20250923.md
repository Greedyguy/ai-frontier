---
keywords:
  - Vision-Language Model
  - Model Parity Aligner
  - Knowledge Transfer
  - Visual Question Answering
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16633
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:29:22.292991",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Model Parity Aligner",
    "Knowledge Transfer",
    "Visual Question Answering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Model Parity Aligner": 0.78,
    "Knowledge Transfer": 0.8,
    "Visual Question Answering": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper and connects to the broader field of multimodal learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Model Parity Aligner",
        "canonical": "Model Parity Aligner",
        "aliases": [
          "MPA"
        ],
        "category": "unique_technical",
        "rationale": "A novel framework introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Knowledge Transfer",
        "canonical": "Knowledge Transfer",
        "aliases": [
          "Transfer Learning"
        ],
        "category": "specific_connectable",
        "rationale": "A key process in the paper, linking it to broader machine learning techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.68,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual Question Answering",
        "canonical": "Visual Question Answering",
        "aliases": [
          "VQA"
        ],
        "category": "specific_connectable",
        "rationale": "The primary application domain of the study, providing context for the research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "efficiency",
      "performance gap"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Model Parity Aligner",
      "resolved_canonical": "Model Parity Aligner",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Knowledge Transfer",
      "resolved_canonical": "Knowledge Transfer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.68,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual Question Answering",
      "resolved_canonical": "Visual Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16633.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16633](https://arxiv.org/abs/2509.16633)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (85.3% similar)
- [[2025-09-22/VLA-Mark_ A cross modal watermark for large vision-language alignment model_20250922|VLA-Mark: A cross modal watermark for large vision-language alignment model]] (85.1% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (85.0% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (84.6% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Transfer|Knowledge Transfer]], [[keywords/Visual Question Answering|Visual Question Answering]]
**âš¡ Unique Technical**: [[keywords/Model Parity Aligner|Model Parity Aligner]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16633v1 Announce Type: cross 
Abstract: Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì†Œí˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(S-VLM)ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ ëª¨ë¸ íŒ¨ë¦¬í‹° ì–¼ë¼ì´ë„ˆ(MPA)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì§€ì‹ ì¦ë¥˜ ë°©ë²•ì´ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ë°˜ë©´, MPAëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(L-VLM)ê³¼ì˜ ì§€ì‹ ê²©ì°¨ë¥¼ ì‹ë³„í•˜ê³  ì´ë¥¼ ëª©í‘œë¡œ ìµœì í™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ S-VLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” TextVQA, ST-VQA, ChartQA, OKVQA ë“± ë‹¤ì–‘í•œ VQA ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìœ¼ë©°, MPAê°€ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ S-VLMì˜ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚´ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì½”ë“œë„ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(L-VLMs)ì€ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ì¸í•´ ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œëŠ” ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.
- 2. ì†Œí˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(S-VLMs)ì€ íš¨ìœ¨ì ì´ì§€ë§Œ, ì„±ëŠ¥ ë©´ì—ì„œ L-VLMsì™€ í° ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” MPA(Model Parity Aligner)ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬, ë¼ë²¨ì´ ì—†ëŠ” ì´ë¯¸ì§€ì™€ L-VLMsì˜ íš¨ê³¼ì ì¸ ì§€ì‹ ì „ì´ë¥¼ í™œìš©í•´ S-VLMsì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.
- 4. MPAëŠ” ì „í†µì ì¸ ì§€ì‹ ì¦ë¥˜ ë°©ë²• ëŒ€ì‹ , S-VLMsì™€ L-VLMs ê°„ì˜ ì§€ì‹ ê²©ì°¨ë¥¼ ì •í™•íˆ ì‹ë³„í•˜ê³  ì´ë¥¼ ëª©í‘œë¡œ ìµœì í™”í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. ë‹¤ì–‘í•œ VQA ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, MPAëŠ” S-VLMsì˜ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:29:22*