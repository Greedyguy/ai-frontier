---
keywords:
  - Multimodal Learning
  - Functionally Equivalent Sampling
  - Uncertainty Quantification
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16648
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:51:00.284967",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Functionally Equivalent Sampling",
    "Uncertainty Quantification",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Functionally Equivalent Sampling": 0.78,
    "Uncertainty Quantification": 0.8,
    "Vision-Language Model": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal LLMs",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Large Language Models"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the trending concept of integrating multiple modalities in learning systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Functionally Equivalent Sampling",
        "canonical": "Functionally Equivalent Sampling",
        "aliases": [
          "FESTA"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel technique for trust assessment in multimodal models.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Uncertainty Quantification",
        "canonical": "Uncertainty Quantification",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for assessing model predictions and linking to other trust assessment methods.",
        "novelty_score": 0.58,
        "connectivity_score": 0.83,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-LLMs",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents an evolved concept in integrating vision and language processing.",
        "novelty_score": 0.6,
        "connectivity_score": 0.87,
        "specificity_score": 0.79,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "Trust Assessment",
      "Selective Prediction"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal LLMs",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Functionally Equivalent Sampling",
      "resolved_canonical": "Functionally Equivalent Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Uncertainty Quantification",
      "resolved_canonical": "Uncertainty Quantification",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.83,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-LLMs",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.87,
        "specificity": 0.79,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16648.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16648](https://arxiv.org/abs/2509.16648)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (82.9% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (82.5% similar)
- [[2025-09-22/Calibrating LLM Confidence by Probing Perturbed Representation Stability_20250922|Calibrating LLM Confidence by Probing Perturbed Representation Stability]] (82.3% similar)
- [[2025-09-19/Estimating Semantic Alphabet Size for LLM Uncertainty Quantification_20250919|Estimating Semantic Alphabet Size for LLM Uncertainty Quantification]] (81.3% similar)
- [[2025-09-22/Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding_20250922|Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Uncertainty Quantification|Uncertainty Quantification]]
**âš¡ Unique Technical**: [[keywords/Functionally Equivalent Sampling|Functionally Equivalent Sampling]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16648v1 Announce Type: new 
Abstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ì˜ˆì¸¡ì— ëŒ€í•œ ì‹ ë¢° í‰ê°€ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ FESTAë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. FESTAëŠ” ëª¨ë¸ì˜ ì…ë ¥ ê³µê°„ì„ í™•ì¥í•˜ì—¬ ì¼ê´€ì„±ê³¼ ë¯¼ê°ë„ë¥¼ í‰ê°€í•˜ëŠ” ì…ë ¥ ìƒ˜í”Œë§ ê¸°ë²•ìœ¼ë¡œ, ëª¨ë¸ì˜ ì…ë ¥-ì¶œë ¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, FESTAëŠ” ì‹œê° ë° ì˜¤ë””ì˜¤ ì¶”ë¡  ì‘ì—…ì—ì„œ AUROC ê¸°ì¤€ìœ¼ë¡œ ê°ê° 33.3%ì™€ 29.6%ì˜ ìƒëŒ€ì  ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê°ë… ì—†ì´ë„ ì‹ ë¢°ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìœ¼ë©°, ì½”ë“œê°€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. FESTAëŠ” ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì‹ ë¢° í‰ê°€ë¥¼ ìœ„í•œ ì…ë ¥ ìƒ˜í”Œë§ ê¸°ë²•ìœ¼ë¡œ, ë“±ê°€ ë° ë³´ì™„ì  ì…ë ¥ ìƒ˜í”Œë§ì„ í†µí•´ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ ìƒ˜í”Œë§ ê¸°ë²•ì€ ëª¨ë¸ì˜ ì¼ê´€ì„±ê³¼ ë¯¼ê°ì„±ì„ íƒìƒ‰í•˜ê¸° ìœ„í•´ ì…ë ¥ ê³µê°„ì„ í™•ì¥í•˜ë©°, ëª¨ë¸ì˜ ì…ë ¥-ì¶œë ¥ ì ‘ê·¼ë§Œì„ ì‚¬ìš©í•˜ê³  ì§€ë„ í•™ìŠµì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
- 3. FESTAëŠ” ì‹œê° ë° ì˜¤ë””ì˜¤ ì¶”ë¡  ì‘ì—…ì—ì„œ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ LLMì„ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìœ¼ë©°, ì„ íƒì  ì˜ˆì¸¡ ì„±ëŠ¥ì—ì„œ AUROC ê¸°ì¤€ìœ¼ë¡œ ì‹œê°-LLMì—ì„œ 33.3%, ì˜¤ë””ì˜¤-LLMì—ì„œ 29.6%ì˜ ìƒëŒ€ì  ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 4. ì½”ë“œ êµ¬í˜„ì€ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 22:51:00*