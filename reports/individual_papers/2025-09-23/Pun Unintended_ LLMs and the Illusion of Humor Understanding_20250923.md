---
keywords:
  - Large Language Model
  - Pun Detection Benchmarks
  - Human Evaluation
  - Robustness Challenges
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.12158
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:29:47.405297",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Pun Detection Benchmarks",
    "Human Evaluation",
    "Robustness Challenges"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Pun Detection Benchmarks": 0.7,
    "Human Evaluation": 0.65,
    "Robustness Challenges": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on humor understanding and pun detection.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Pun Detection Benchmarks",
        "canonical": "Pun Detection Benchmarks",
        "aliases": [
          "Pun Benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new set of benchmarks that are critical for evaluating LLMs' humor understanding.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Human Evaluation",
        "canonical": "Human Evaluation",
        "aliases": [
          "Human Assessment"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the comparison between human and LLM performance, essential for understanding model limitations.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.65
      },
      {
        "surface": "Robustness Challenges",
        "canonical": "Robustness Challenges",
        "aliases": [
          "Model Robustness Issues"
        ],
        "category": "unique_technical",
        "rationale": "Addresses the difficulties LLMs face with subtle changes in puns, a key focus of the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "humor understanding",
      "nuanced grasp",
      "subtle changes"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Pun Detection Benchmarks",
      "resolved_canonical": "Pun Detection Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Human Evaluation",
      "resolved_canonical": "Human Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Robustness Challenges",
      "resolved_canonical": "Robustness Challenges",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Pun Unintended: LLMs and the Illusion of Humor Understanding

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.12158.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.12158](https://arxiv.org/abs/2509.12158)

## 🔗 유사한 논문
- [[2025-09-18/Humor in Pixels_ Benchmarking Large Multimodal Models Understanding of Online Comics_20250918|Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics]] (85.3% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (84.4% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (84.0% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (83.6% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Human Evaluation|Human Evaluation]]
**⚡ Unique Technical**: [[keywords/Pun Detection Benchmarks|Pun Detection Benchmarks]], [[keywords/Robustness Challenges|Robustness Challenges]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.12158v2 Announce Type: replace-cross 
Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic similarity. While LLMs have shown promise in detecting puns, we show in this paper that their understanding often remains shallow, lacking the nuanced grasp typical of human interpretation. By systematically analyzing and reformulating existing pun benchmarks, we demonstrate how subtle changes in puns are sufficient to mislead LLMs. Our contributions include comprehensive and nuanced pun detection benchmarks, human evaluation across recent LLMs, and an analysis of the robustness challenges these models face in processing puns.

## 📝 요약

이 논문은 다의성과 음성 유사성을 활용한 유머인 말장난(pun)에 대한 연구를 다룹니다. 대형 언어 모델(LLM)이 말장난을 감지하는 데 유망한 성과를 보였으나, 인간의 해석에 비해 깊이가 부족하다는 점을 지적합니다. 기존의 말장난 벤치마크를 체계적으로 분석하고 재구성하여, 미세한 변화만으로도 LLM을 혼란스럽게 할 수 있음을 보여줍니다. 주요 기여로는 포괄적이고 세밀한 말장난 감지 벤치마크, 최신 LLM에 대한 인간 평가, 그리고 이러한 모델들이 말장난을 처리하는 데 직면한 강건성 문제 분석이 포함됩니다.

## 🎯 주요 포인트

- 1. LLMs는 말장난을 탐지하는 데 유망한 성과를 보였지만, 인간의 해석에 비해 깊이 있는 이해가 부족함을 보인다.
- 2. 기존의 말장난 벤치마크를 체계적으로 분석하고 재구성하여, 미세한 변화만으로도 LLMs를 혼란스럽게 할 수 있음을 입증한다.
- 3. 포괄적이고 세밀한 말장난 탐지 벤치마크를 제시하고, 최근 LLMs에 대한 인간 평가를 수행한다.
- 4. LLMs가 말장난을 처리할 때 직면하는 강건성 문제를 분석한다.


---

*Generated on 2025-09-24 01:29:47*