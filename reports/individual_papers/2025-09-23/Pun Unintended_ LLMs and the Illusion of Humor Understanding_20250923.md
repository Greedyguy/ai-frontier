---
keywords:
  - Large Language Model
  - Pun Detection Benchmarks
  - Human Evaluation
  - Robustness Challenges
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.12158
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:29:47.405297",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Pun Detection Benchmarks",
    "Human Evaluation",
    "Robustness Challenges"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Pun Detection Benchmarks": 0.7,
    "Human Evaluation": 0.65,
    "Robustness Challenges": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on humor understanding and pun detection.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Pun Detection Benchmarks",
        "canonical": "Pun Detection Benchmarks",
        "aliases": [
          "Pun Benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new set of benchmarks that are critical for evaluating LLMs' humor understanding.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Human Evaluation",
        "canonical": "Human Evaluation",
        "aliases": [
          "Human Assessment"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the comparison between human and LLM performance, essential for understanding model limitations.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.65
      },
      {
        "surface": "Robustness Challenges",
        "canonical": "Robustness Challenges",
        "aliases": [
          "Model Robustness Issues"
        ],
        "category": "unique_technical",
        "rationale": "Addresses the difficulties LLMs face with subtle changes in puns, a key focus of the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "humor understanding",
      "nuanced grasp",
      "subtle changes"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Pun Detection Benchmarks",
      "resolved_canonical": "Pun Detection Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Human Evaluation",
      "resolved_canonical": "Human Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Robustness Challenges",
      "resolved_canonical": "Robustness Challenges",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Pun Unintended: LLMs and the Illusion of Humor Understanding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.12158.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.12158](https://arxiv.org/abs/2509.12158)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Humor in Pixels_ Benchmarking Large Multimodal Models Understanding of Online Comics_20250918|Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics]] (85.3% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (84.4% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (84.0% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (83.6% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Human Evaluation|Human Evaluation]]
**âš¡ Unique Technical**: [[keywords/Pun Detection Benchmarks|Pun Detection Benchmarks]], [[keywords/Robustness Challenges|Robustness Challenges]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.12158v2 Announce Type: replace-cross 
Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic similarity. While LLMs have shown promise in detecting puns, we show in this paper that their understanding often remains shallow, lacking the nuanced grasp typical of human interpretation. By systematically analyzing and reformulating existing pun benchmarks, we demonstrate how subtle changes in puns are sufficient to mislead LLMs. Our contributions include comprehensive and nuanced pun detection benchmarks, human evaluation across recent LLMs, and an analysis of the robustness challenges these models face in processing puns.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì˜ì„±ê³¼ ìŒì„± ìœ ì‚¬ì„±ì„ í™œìš©í•œ ìœ ë¨¸ì¸ ë§ì¥ë‚œ(pun)ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë§ì¥ë‚œì„ ê°ì§€í•˜ëŠ” ë° ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ì¸ê°„ì˜ í•´ì„ì— ë¹„í•´ ê¹Šì´ê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ì ì„ ì§€ì í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë§ì¥ë‚œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì¬êµ¬ì„±í•˜ì—¬, ë¯¸ì„¸í•œ ë³€í™”ë§Œìœ¼ë¡œë„ LLMì„ í˜¼ë€ìŠ¤ëŸ½ê²Œ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” í¬ê´„ì ì´ê³  ì„¸ë°€í•œ ë§ì¥ë‚œ ê°ì§€ ë²¤ì¹˜ë§ˆí¬, ìµœì‹  LLMì— ëŒ€í•œ ì¸ê°„ í‰ê°€, ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì´ ë§ì¥ë‚œì„ ì²˜ë¦¬í•˜ëŠ” ë° ì§ë©´í•œ ê°•ê±´ì„± ë¬¸ì œ ë¶„ì„ì´ í¬í•¨ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMsëŠ” ë§ì¥ë‚œì„ íƒì§€í•˜ëŠ” ë° ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ì§€ë§Œ, ì¸ê°„ì˜ í•´ì„ì— ë¹„í•´ ê¹Šì´ ìˆëŠ” ì´í•´ê°€ ë¶€ì¡±í•¨ì„ ë³´ì¸ë‹¤.
- 2. ê¸°ì¡´ì˜ ë§ì¥ë‚œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì¬êµ¬ì„±í•˜ì—¬, ë¯¸ì„¸í•œ ë³€í™”ë§Œìœ¼ë¡œë„ LLMsë¥¼ í˜¼ë€ìŠ¤ëŸ½ê²Œ í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•œë‹¤.
- 3. í¬ê´„ì ì´ê³  ì„¸ë°€í•œ ë§ì¥ë‚œ íƒì§€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì‹œí•˜ê³ , ìµœê·¼ LLMsì— ëŒ€í•œ ì¸ê°„ í‰ê°€ë¥¼ ìˆ˜í–‰í•œë‹¤.
- 4. LLMsê°€ ë§ì¥ë‚œì„ ì²˜ë¦¬í•  ë•Œ ì§ë©´í•˜ëŠ” ê°•ê±´ì„± ë¬¸ì œë¥¼ ë¶„ì„í•œë‹¤.


---

*Generated on 2025-09-24 01:29:47*