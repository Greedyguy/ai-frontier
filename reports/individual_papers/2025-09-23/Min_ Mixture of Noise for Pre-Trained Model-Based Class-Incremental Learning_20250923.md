---
keywords:
  - Class Incremental Learning
  - Pre-trained Models
  - Mixture of Noise
  - Information Theory
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16738
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:14:53.848116",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Class Incremental Learning",
    "Pre-trained Models",
    "Mixture of Noise",
    "Information Theory"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Class Incremental Learning": 0.8,
    "Pre-trained Models": 0.7,
    "Mixture of Noise": 0.85,
    "Information Theory": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Class Incremental Learning",
        "canonical": "Class Incremental Learning",
        "aliases": [
          "CIL"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's focus on learning new categories while retaining old knowledge, which is crucial for linking related research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Pre-trained Models",
        "canonical": "Pre-trained Models",
        "aliases": [
          "PTMs"
        ],
        "category": "broad_technical",
        "rationale": "Pre-trained models are a foundational concept in the paper, providing a basis for understanding the proposed method.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Mixture of Noise",
        "canonical": "Mixture of Noise",
        "aliases": [
          "Min"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel method proposed by the authors, making it a unique concept for linking within the domain.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Information Theory",
        "canonical": "Information Theory",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Information theory is used to guide the learning of beneficial noise, linking it to a broader theoretical framework.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "parameter drift",
      "visual patterns"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Class Incremental Learning",
      "resolved_canonical": "Class Incremental Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Pre-trained Models",
      "resolved_canonical": "Pre-trained Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Mixture of Noise",
      "resolved_canonical": "Mixture of Noise",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Information Theory",
      "resolved_canonical": "Information Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16738.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16738](https://arxiv.org/abs/2509.16738)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Intra-Cluster Mixup_ An Effective Data Augmentation Technique for Complementary-Label Learning_20250923|Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning]] (84.5% similar)
- [[2025-09-22/Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection_20250922|Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection]] (83.1% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (82.8% similar)
- [[2025-09-23/AIMMerging_ Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning_20250923|AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning]] (81.6% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Pre-trained Models|Pre-trained Models]]
**ğŸ”— Specific Connectable**: [[keywords/Information Theory|Information Theory]]
**âš¡ Unique Technical**: [[keywords/Class Incremental Learning|Class Incremental Learning]], [[keywords/Mixture of Noise|Mixture of Noise]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16738v1 Announce Type: cross 
Abstract: Class Incremental Learning (CIL) aims to continuously learn new categories while retaining the knowledge of old ones. Pre-trained models (PTMs) show promising capabilities in CIL. However, existing approaches that apply lightweight fine-tuning to backbones still induce parameter drift, thereby compromising the generalization capability of pre-trained models. Parameter drift can be conceptualized as a form of noise that obscures critical patterns learned for previous tasks. However, recent researches have shown that noise is not always harmful. For example, the large number of visual patterns learned from pre-training can be easily abused by a single task, and introducing appropriate noise can suppress some low-correlation features, thus leaving a margin for future tasks. To this end, we propose learning beneficial noise for CIL guided by information theory and propose Mixture of Noise (Min), aiming to mitigate the degradation of backbone generalization from adapting new tasks. Specifically, task-specific noise is learned from high-dimension features of new tasks. Then, a set of weights is adjusted dynamically for optimal mixture of different task noise. Finally, Min embeds the beneficial noise into the intermediate features to mask the response of inefficient patterns. Extensive experiments on six benchmark datasets demonstrate that Min achieves state-of-the-art performance in most incremental settings, with particularly outstanding results in 50-steps incremental settings. This shows the significant potential for beneficial noise in continual learning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í´ë˜ìŠ¤ ì¦ë¶„ í•™ìŠµ(CIL)ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(PTM)ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„° ë“œë¦¬í”„íŠ¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ ìœ ìµí•œ ë…¸ì´ì¦ˆ í•™ìŠµ ë°©ë²•ì¸ Mixture of Noise (Min)ì„ ì œì•ˆí•©ë‹ˆë‹¤. Minì€ ìƒˆë¡œìš´ ì‘ì—…ì˜ ê³ ì°¨ì› íŠ¹ì§•ì—ì„œ ì‘ì—…ë³„ ë…¸ì´ì¦ˆë¥¼ í•™ìŠµí•˜ê³ , ë‹¤ì–‘í•œ ì‘ì—… ë…¸ì´ì¦ˆì˜ ìµœì  í˜¼í•©ì„ ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¹„íš¨ìœ¨ì ì¸ íŒ¨í„´ì˜ ë°˜ì‘ì„ ì–µì œí•˜ì—¬ ë°±ë³¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ë¥¼ ì™„í™”í•©ë‹ˆë‹¤. ì—¬ì„¯ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìˆ˜í–‰í•œ ì‹¤í—˜ ê²°ê³¼, Minì€ ëŒ€ë¶€ë¶„ì˜ ì¦ë¶„ ì„¤ì •ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, íŠ¹íˆ 50ë‹¨ê³„ ì¦ë¶„ ì„¤ì •ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì§€ì† í•™ìŠµì—ì„œ ìœ ìµí•œ ë…¸ì´ì¦ˆì˜ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í´ë˜ìŠ¤ ì¦ë¶„ í•™ìŠµ(CIL)ì€ ìƒˆë¡œìš´ ë²”ì£¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ ê¸°ì¡´ ì§€ì‹ì„ ìœ ì§€í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
- 2. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(PTM)ì€ CILì—ì„œ ìœ ë§í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ê²½ëŸ‰í™”ëœ ë¯¸ì„¸ ì¡°ì •ì´ íŒŒë¼ë¯¸í„° ë“œë¦¬í”„íŠ¸ë¥¼ ìœ ë°œí•˜ì—¬ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì €í•˜ì‹œí‚¨ë‹¤.
- 3. ì •ë³´ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ CILì— ìœ ìµí•œ ë…¸ì´ì¦ˆë¥¼ í•™ìŠµí•˜ëŠ” Mixture of Noise(Min) ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ìƒˆë¡œìš´ ì‘ì—… ì ì‘ ì‹œ ë°±ë³¸ì˜ ì¼ë°˜í™” ì €í•˜ë¥¼ ì™„í™”í•œë‹¤.
- 4. Minì€ ìƒˆë¡œìš´ ì‘ì—…ì˜ ê³ ì°¨ì› íŠ¹ì§•ì—ì„œ ì‘ì—…ë³„ ë…¸ì´ì¦ˆë¥¼ í•™ìŠµí•˜ê³ , ë‹¤ì–‘í•œ ì‘ì—… ë…¸ì´ì¦ˆì˜ ìµœì  í˜¼í•©ì„ ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•œë‹¤.
- 5. ì—¬ì„¯ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, Minì€ ëŒ€ë¶€ë¶„ì˜ ì¦ë¶„ ì„¤ì •ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, íŠ¹íˆ 50ë‹¨ê³„ ì¦ë¶„ ì„¤ì •ì—ì„œ ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.


---

*Generated on 2025-09-24 02:14:53*