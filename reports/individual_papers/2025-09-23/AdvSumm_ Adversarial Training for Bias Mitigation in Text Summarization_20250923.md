---
keywords:
  - Large Language Model
  - Adversarial Summarization
  - Bias Mitigation
  - Sequence-to-Sequence Model
  - Gradient-guided Perturbation
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2506.06273
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:04:08.051603",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Adversarial Summarization",
    "Bias Mitigation",
    "Sequence-to-Sequence Model",
    "Gradient-guided Perturbation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Adversarial Summarization": 0.82,
    "Bias Mitigation": 0.79,
    "Sequence-to-Sequence Model": 0.77,
    "Gradient-guided Perturbation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on summarization and bias mitigation.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Adversarial Summarization",
        "canonical": "Adversarial Summarization",
        "aliases": [
          "AdvSumm"
        ],
        "category": "unique_technical",
        "rationale": "Adversarial Summarization is a novel approach introduced in the paper, crucial for understanding its contribution.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "Bias Mitigation",
        "canonical": "Bias Mitigation",
        "aliases": [
          "Bias Reduction"
        ],
        "category": "specific_connectable",
        "rationale": "Bias Mitigation is a key theme of the paper, relevant for linking to broader discussions on fairness in AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Sequence-to-Sequence models",
        "canonical": "Sequence-to-Sequence Model",
        "aliases": [
          "Seq2Seq"
        ],
        "category": "specific_connectable",
        "rationale": "Sequence-to-Sequence models are fundamental to the summarization process discussed in the paper.",
        "novelty_score": 0.48,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "Gradient-guided perturbations",
        "canonical": "Gradient-guided Perturbation",
        "aliases": [
          "Gradient Perturbation"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a novel aspect of the paper's methodology, enhancing model robustness.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Adversarial Summarization",
      "resolved_canonical": "Adversarial Summarization",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Bias Mitigation",
      "resolved_canonical": "Bias Mitigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Sequence-to-Sequence models",
      "resolved_canonical": "Sequence-to-Sequence Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Gradient-guided perturbations",
      "resolved_canonical": "Gradient-guided Perturbation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2506.06273.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2506.06273](https://arxiv.org/abs/2506.06273)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (85.4% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (84.6% similar)
- [[2025-09-23/QA-prompting_ Improving Summarization with Large Language Models using Question-Answering_20250923|QA-prompting: Improving Summarization with Large Language Models using Question-Answering]] (84.5% similar)
- [[2025-09-22/Re-FRAME the Meeting Summarization SCOPE_ Fact-Based Summarization and Personalization via Questions_20250922|Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions]] (83.3% similar)
- [[2025-09-23/Auto-Search and Refinement_ An Automated Framework for Gender Bias Mitigation in Large Language Models_20250923|Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Bias Mitigation|Bias Mitigation]], [[keywords/Sequence-to-Sequence Model|Sequence-to-Sequence Model]]
**âš¡ Unique Technical**: [[keywords/Adversarial Summarization|Adversarial Summarization]], [[keywords/Gradient-guided Perturbation|Gradient-guided Perturbation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.06273v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved impressive performance in text summarization and are increasingly deployed in real-world applications. However, these systems often inherit associative and framing biases from pre-training data, leading to inappropriate or unfair outputs in downstream tasks. In this work, we present AdvSumm (Adversarial Summarization), a domain-agnostic training framework designed to mitigate bias in text summarization through improved generalization. Inspired by adversarial robustness, AdvSumm introduces a novel Perturber component that applies gradient-guided perturbations at the embedding level of Sequence-to-Sequence models, enhancing the model's robustness to input variations. We empirically demonstrate that AdvSumm effectively reduces different types of bias in summarization-specifically, name-nationality bias and political framing bias-without compromising summarization quality. Compared to standard transformers and data augmentation techniques like back-translation, AdvSumm achieves stronger bias mitigation performance across benchmark datasets.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ í…ìŠ¤íŠ¸ ìš”ì•½ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì‚¬ì „ í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° ì—°ê´€ ë° í”„ë ˆì´ë° í¸í–¥ì„ ë¬¼ë ¤ë°›ì•„ ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¶ˆê³µì •í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒì„ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ AdvSummì´ë¼ëŠ” í¸í–¥ ì™„í™” í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. AdvSummëŠ” ì ëŒ€ì  ê²¬ê³ ì„±ì—ì„œ ì˜ê°ì„ ë°›ì•„, ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸ì˜ ì„ë² ë”© ë‹¨ê³„ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ì˜ ë³€í˜•ì„ ì ìš©í•˜ëŠ” Perturber ì»´í¬ë„ŒíŠ¸ë¥¼ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ ì…ë ¥ ë³€ë™ì— ëŒ€í•œ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, AdvSummëŠ” ìš”ì•½ í’ˆì§ˆì„ ìœ ì§€í•˜ë©´ì„œ ì´ë¦„-êµ­ì  í¸í–¥ê³¼ ì •ì¹˜ì  í”„ë ˆì´ë° í¸í–¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë°ì´í„° ì¦ê°• ê¸°ë²•ë³´ë‹¤ ë›°ì–´ë‚œ í¸í–¥ ì™„í™” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ í…ìŠ¤íŠ¸ ìš”ì•½ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì‚¬ì „ í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° í¸í–¥ì„ ë¬¼ë ¤ë°›ì•„ ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¶ˆê³µì •í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
- 2. AdvSummì€ í…ìŠ¤íŠ¸ ìš”ì•½ì—ì„œì˜ í¸í–¥ì„ ì™„í™”í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ë„ë©”ì¸ ë…ë¦½ì ì¸ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¡œ, ì¼ë°˜í™”ë¥¼ ê°œì„ í•˜ì—¬ í¸í–¥ì„ ì¤„ì¸ë‹¤.
- 3. AdvSummì€ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸ì˜ ì„ë² ë”© ìˆ˜ì¤€ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ì˜ êµë€ì„ ì ìš©í•˜ëŠ” Perturber ì»´í¬ë„ŒíŠ¸ë¥¼ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ ì…ë ¥ ë³€ì´ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 4. AdvSummì€ ìš”ì•½ì˜ ì§ˆì„ ì €í•˜ì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ì´ë¦„-êµ­ì  í¸í–¥ê³¼ ì •ì¹˜ì  í”„ë ˆì´ë° í¸í–¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¸ë‹¤.
- 5. AdvSummì€ í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ì—­ë²ˆì—­ ê°™ì€ ë°ì´í„° ì¦ê°• ê¸°ë²•ì— ë¹„í•´ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ë” ê°•ë ¥í•œ í¸í–¥ ì™„í™” ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-24 04:04:08*