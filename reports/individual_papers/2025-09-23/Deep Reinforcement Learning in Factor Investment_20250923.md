---
keywords:
  - Deep Reinforcement Learning
  - Conditional Auto-encoded Factor-based Portfolio Optimisation
  - Proximal Policy Optimization
  - Deep Deterministic Policy Gradient
  - Factor-based Portfolio Optimisation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16206
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:02:30.476563",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Reinforcement Learning",
    "Conditional Auto-encoded Factor-based Portfolio Optimisation",
    "Proximal Policy Optimization",
    "Deep Deterministic Policy Gradient",
    "Factor-based Portfolio Optimisation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Reinforcement Learning": 0.85,
    "Conditional Auto-encoded Factor-based Portfolio Optimisation": 0.8,
    "Proximal Policy Optimization": 0.78,
    "Deep Deterministic Policy Gradient": 0.78,
    "Factor-based Portfolio Optimisation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Reinforcement Learning",
        "canonical": "Deep Reinforcement Learning",
        "aliases": [
          "DRL"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology, linking to broader machine learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
        "canonical": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
        "aliases": [
          "CAFPO"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method specific to the paper, crucial for understanding its contribution.",
        "novelty_score": 0.92,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "specific_connectable",
        "rationale": "A key algorithm used in the paper, important for linking reinforcement learning techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Deep Deterministic Policy Gradient",
        "canonical": "Deep Deterministic Policy Gradient",
        "aliases": [
          "DDPG"
        ],
        "category": "specific_connectable",
        "rationale": "Another core algorithm in the paper, enhancing connections within reinforcement learning literature.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Factor-based Portfolio Optimisation",
        "canonical": "Factor-based Portfolio Optimisation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific to the paper's approach, linking investment strategies with machine learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "trade execution",
      "portfolio management"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Reinforcement Learning",
      "resolved_canonical": "Deep Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
      "resolved_canonical": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Deep Deterministic Policy Gradient",
      "resolved_canonical": "Deep Deterministic Policy Gradient",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Factor-based Portfolio Optimisation",
      "resolved_canonical": "Factor-based Portfolio Optimisation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Deep Reinforcement Learning in Factor Investment

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16206.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16206](https://arxiv.org/abs/2509.16206)

## 🔗 유사한 논문
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (80.6% similar)
- [[2025-09-23/Improving After-sales Service_ Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences_20250923|Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences]] (78.6% similar)
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (78.6% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (78.4% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (78.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Deep Reinforcement Learning|Deep Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]], [[keywords/Deep Deterministic Policy Gradient|Deep Deterministic Policy Gradient]]
**⚡ Unique Technical**: [[keywords/Conditional Auto-encoded Factor-based Portfolio Optimisation|Conditional Auto-encoded Factor-based Portfolio Optimisation]], [[keywords/Factor-based Portfolio Optimisation|Factor-based Portfolio Optimisation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16206v1 Announce Type: cross 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.

## 📝 요약

이 논문은 저주파수 요인 포트폴리오 구성에 있어 심층 강화 학습(DRL)의 활용 가능성을 탐구합니다. 주식의 진입과 퇴출로 인해 발생하는 고차원 불균형 상태 공간이 주요 장애물로 작용하는데, 이를 해결하기 위해 94개의 기업 특성에 기반한 조건부 오토인코딩 요인 기반 포트폴리오 최적화(CAFPO)를 제안합니다. CAFPO는 주식 수익률을 잠재 요인으로 압축하고, 이를 PPO와 DDPG를 활용한 DRL 에이전트에 입력하여 연속적인 롱숏 비중을 생성합니다. 2000년부터 2020년까지의 미국 주식 데이터를 통해 CAFPO는 기존 방법들보다 우수한 성과를 보였으며, 24.6%의 복합 수익률과 0.94의 샤프 비율을 기록했습니다. SHAP 분석은 경제적으로 직관적인 요인 기여도를 보여주며, 요인 인식 표현 학습이 DRL을 기관 투자자용 저회전 포트폴리오 관리에 실용적으로 만들 수 있음을 입증합니다.

## 🎯 주요 포인트

- 1. CAFPO는 94개의 기업 특성에 기반한 잠재 요인으로 주식 수익률을 압축하여 포트폴리오 최적화를 수행합니다.
- 2. PPO와 DDPG를 활용한 DRL 에이전트가 연속적인 롱-숏 비중을 생성합니다.
- 3. 2000년부터 2020년까지의 미국 주식 데이터를 통해 CAFPO는 24.6%의 복합 수익률과 0.94의 샤프 비율을 기록하며 기존 방법들을 능가합니다.
- 4. SHAP 분석을 통해 경제적으로 직관적인 요인 기여도를 확인할 수 있습니다.
- 5. 요인 인식 표현 학습이 DRL을 기관의 저회전율 포트폴리오 관리에 실용적으로 활용할 수 있음을 보여줍니다.


---

*Generated on 2025-09-24 02:02:30*