---
keywords:
  - Deep Reinforcement Learning
  - Conditional Auto-encoded Factor-based Portfolio Optimisation
  - Proximal Policy Optimization
  - Deep Deterministic Policy Gradient
  - Factor-based Portfolio Optimisation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16206
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:02:30.476563",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Reinforcement Learning",
    "Conditional Auto-encoded Factor-based Portfolio Optimisation",
    "Proximal Policy Optimization",
    "Deep Deterministic Policy Gradient",
    "Factor-based Portfolio Optimisation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Reinforcement Learning": 0.85,
    "Conditional Auto-encoded Factor-based Portfolio Optimisation": 0.8,
    "Proximal Policy Optimization": 0.78,
    "Deep Deterministic Policy Gradient": 0.78,
    "Factor-based Portfolio Optimisation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Reinforcement Learning",
        "canonical": "Deep Reinforcement Learning",
        "aliases": [
          "DRL"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology, linking to broader machine learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
        "canonical": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
        "aliases": [
          "CAFPO"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method specific to the paper, crucial for understanding its contribution.",
        "novelty_score": 0.92,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "specific_connectable",
        "rationale": "A key algorithm used in the paper, important for linking reinforcement learning techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Deep Deterministic Policy Gradient",
        "canonical": "Deep Deterministic Policy Gradient",
        "aliases": [
          "DDPG"
        ],
        "category": "specific_connectable",
        "rationale": "Another core algorithm in the paper, enhancing connections within reinforcement learning literature.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Factor-based Portfolio Optimisation",
        "canonical": "Factor-based Portfolio Optimisation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific to the paper's approach, linking investment strategies with machine learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "trade execution",
      "portfolio management"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Reinforcement Learning",
      "resolved_canonical": "Deep Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
      "resolved_canonical": "Conditional Auto-encoded Factor-based Portfolio Optimisation",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Deep Deterministic Policy Gradient",
      "resolved_canonical": "Deep Deterministic Policy Gradient",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Factor-based Portfolio Optimisation",
      "resolved_canonical": "Factor-based Portfolio Optimisation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Deep Reinforcement Learning in Factor Investment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16206.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16206](https://arxiv.org/abs/2509.16206)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (80.6% similar)
- [[2025-09-23/Improving After-sales Service_ Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences_20250923|Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences]] (78.6% similar)
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (78.6% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (78.4% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (78.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Reinforcement Learning|Deep Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]], [[keywords/Deep Deterministic Policy Gradient|Deep Deterministic Policy Gradient]]
**âš¡ Unique Technical**: [[keywords/Conditional Auto-encoded Factor-based Portfolio Optimisation|Conditional Auto-encoded Factor-based Portfolio Optimisation]], [[keywords/Factor-based Portfolio Optimisation|Factor-based Portfolio Optimisation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16206v1 Announce Type: cross 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì €ì£¼íŒŒìˆ˜ ìš”ì¸ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ì— ìˆì–´ ì‹¬ì¸µ ê°•í™” í•™ìŠµ(DRL)ì˜ í™œìš© ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì£¼ì‹ì˜ ì§„ì…ê³¼ í‡´ì¶œë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ê³ ì°¨ì› ë¶ˆê· í˜• ìƒíƒœ ê³µê°„ì´ ì£¼ìš” ì¥ì• ë¬¼ë¡œ ì‘ìš©í•˜ëŠ”ë°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 94ê°œì˜ ê¸°ì—… íŠ¹ì„±ì— ê¸°ë°˜í•œ ì¡°ê±´ë¶€ ì˜¤í† ì¸ì½”ë”© ìš”ì¸ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”(CAFPO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CAFPOëŠ” ì£¼ì‹ ìˆ˜ìµë¥ ì„ ì ì¬ ìš”ì¸ìœ¼ë¡œ ì••ì¶•í•˜ê³ , ì´ë¥¼ PPOì™€ DDPGë¥¼ í™œìš©í•œ DRL ì—ì´ì „íŠ¸ì— ì…ë ¥í•˜ì—¬ ì—°ì†ì ì¸ ë¡±ìˆ ë¹„ì¤‘ì„ ìƒì„±í•©ë‹ˆë‹¤. 2000ë…„ë¶€í„° 2020ë…„ê¹Œì§€ì˜ ë¯¸êµ­ ì£¼ì‹ ë°ì´í„°ë¥¼ í†µí•´ CAFPOëŠ” ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©°, 24.6%ì˜ ë³µí•© ìˆ˜ìµë¥ ê³¼ 0.94ì˜ ìƒ¤í”„ ë¹„ìœ¨ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. SHAP ë¶„ì„ì€ ê²½ì œì ìœ¼ë¡œ ì§ê´€ì ì¸ ìš”ì¸ ê¸°ì—¬ë„ë¥¼ ë³´ì—¬ì£¼ë©°, ìš”ì¸ ì¸ì‹ í‘œí˜„ í•™ìŠµì´ DRLì„ ê¸°ê´€ íˆ¬ìììš© ì €íšŒì „ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ì— ì‹¤ìš©ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CAFPOëŠ” 94ê°œì˜ ê¸°ì—… íŠ¹ì„±ì— ê¸°ë°˜í•œ ì ì¬ ìš”ì¸ìœ¼ë¡œ ì£¼ì‹ ìˆ˜ìµë¥ ì„ ì••ì¶•í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 2. PPOì™€ DDPGë¥¼ í™œìš©í•œ DRL ì—ì´ì „íŠ¸ê°€ ì—°ì†ì ì¸ ë¡±-ìˆ ë¹„ì¤‘ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 3. 2000ë…„ë¶€í„° 2020ë…„ê¹Œì§€ì˜ ë¯¸êµ­ ì£¼ì‹ ë°ì´í„°ë¥¼ í†µí•´ CAFPOëŠ” 24.6%ì˜ ë³µí•© ìˆ˜ìµë¥ ê³¼ 0.94ì˜ ìƒ¤í”„ ë¹„ìœ¨ì„ ê¸°ë¡í•˜ë©° ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 4. SHAP ë¶„ì„ì„ í†µí•´ ê²½ì œì ìœ¼ë¡œ ì§ê´€ì ì¸ ìš”ì¸ ê¸°ì—¬ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. ìš”ì¸ ì¸ì‹ í‘œí˜„ í•™ìŠµì´ DRLì„ ê¸°ê´€ì˜ ì €íšŒì „ìœ¨ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ì— ì‹¤ìš©ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:02:30*