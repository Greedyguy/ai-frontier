---
keywords:
  - Large Language Model
  - Linear Probes
  - Training-Order Encoding
  - Named Entity Recognition
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.14223
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:31:11.787706",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Linear Probes",
    "Training-Order Encoding",
    "Named Entity Recognition"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Linear Probes": 0.7,
    "Training-Order Encoding": 0.75,
    "Named Entity Recognition": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "language model"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader field of study and facilitates linking with other works on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "linear probes",
        "canonical": "Linear Probes",
        "aliases": [
          "linear classifier"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific technique used in the study, relevant for linking with similar probing methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "training-order encoding",
        "canonical": "Training-Order Encoding",
        "aliases": [
          "order encoding",
          "temporal encoding"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a novel concept introduced in the paper, useful for linking with future research on temporal learning patterns.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "named entities",
        "canonical": "Named Entity Recognition",
        "aliases": [
          "NER",
          "named entity"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to a well-established task in NLP, facilitating links with related entity recognition studies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "model",
      "information",
      "dataset",
      "entity"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "linear probes",
      "resolved_canonical": "Linear Probes",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "training-order encoding",
      "resolved_canonical": "Training-Order Encoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "named entities",
      "resolved_canonical": "Named Entity Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Fresh in memory: Training-order recency is linearly encoded in language model activations

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.14223.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.14223](https://arxiv.org/abs/2509.14223)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Language models' activations linearly encode training-order recency_20250917|Language models' activations linearly encode training-order recency]] (96.2% similar)
- [[2025-09-22/Natural Fingerprints of Large Language Models_20250922|Natural Fingerprints of Large Language Models]] (83.8% similar)
- [[2025-09-23/Evolution of Concepts in Language Model Pre-Training_20250923|Evolution of Concepts in Language Model Pre-Training]] (83.4% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (82.8% similar)
- [[2025-09-23/Understanding Post-Training Structural Changes in Large Language Models_20250923|Understanding Post-Training Structural Changes in Large Language Models]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Named Entity Recognition|Named Entity Recognition]]
**âš¡ Unique Technical**: [[keywords/Linear Probes|Linear Probes]], [[keywords/Training-Order Encoding|Training-Order Encoding]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14223v2 Announce Type: replace-cross 
Abstract: We show that language models' activations linearly encode when information was learned during training. Our setup involves creating a model with a known training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but otherwise similar datasets about named entities. We find that the average activations of test samples corresponding to the six training datasets encode the training order: when projected into a 2D subspace, these centroids are arranged exactly in the order of training and lie on a straight line. Further, we show that linear probes can accurately (~90%) distinguish "early" vs. "late" entities, generalizing to entities unseen during the probes' own training. The model can also be fine-tuned to explicitly report an unseen entity's training stage (~80% accuracy). Interestingly, the training-order encoding does not seem attributable to simple differences in activation magnitudes, losses, or model confidence. Our paper demonstrates that models are capable of differentiating information by its acquisition time, and carries significant implications for how they might manage conflicting data and respond to knowledge modifications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì˜ í™œì„±í™”ê°€ í›ˆë ¨ ì¤‘ ì •ë³´ë¥¼ í•™ìŠµí•œ ì‹œì ì„ ì„ í˜•ì ìœ¼ë¡œ ì¸ì½”ë”©í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” Llama-3.2-1B ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ëª…ëª…ëœ ì—”í‹°í‹°ì— ê´€í•œ ì—¬ì„¯ ê°œì˜ ë¶„ë¦¬ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì˜ í‰ê·  í™œì„±í™”ê°€ í›ˆë ¨ ìˆœì„œë¥¼ ì¸ì½”ë”©í•˜ë©°, 2D ê³µê°„ì— íˆ¬ì˜í–ˆì„ ë•Œ ì´ ì¤‘ì‹¬ì ë“¤ì´ í›ˆë ¨ ìˆœì„œëŒ€ë¡œ ì§ì„ ìƒì— ë°°ì—´ë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„ í˜• í”„ë¡œë¸Œë¥¼ í†µí•´ "ì´ˆê¸°"ì™€ "í›„ê¸°" ì—”í‹°í‹°ë¥¼ ì•½ 90% ì •í™•ë„ë¡œ êµ¬ë³„í•  ìˆ˜ ìˆìœ¼ë©°, ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì˜ í›ˆë ¨ ë‹¨ê³„ë„ ì•½ 80% ì •í™•ë„ë¡œ ë³´ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¸ì½”ë”©ì€ ë‹¨ìˆœí•œ í™œì„±í™” í¬ê¸°, ì†ì‹¤, ëª¨ë¸ ì‹ ë¢°ë„ ì°¨ì´ë¡œ ì„¤ëª…ë˜ì§€ ì•Šìœ¼ë©°, ëª¨ë¸ì´ ì •ë³´ íšë“ ì‹œì ì— ë”°ë¼ ë°ì´í„°ë¥¼ êµ¬ë³„í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ìƒì¶©ë˜ëŠ” ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê³  ì§€ì‹ ìˆ˜ì •ì— ë°˜ì‘í•˜ëŠ” ë°©ì‹ì— ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸ì˜ í™œì„±í™”ëŠ” í›ˆë ¨ ì¤‘ ì •ë³´ê°€ í•™ìŠµëœ ì‹œì ì„ ì„ í˜•ì ìœ¼ë¡œ ì¸ì½”ë”©í•œë‹¤.
- 2. ëª¨ë¸ì˜ í‰ê·  í™œì„±í™”ëŠ” í›ˆë ¨ ìˆœì„œë¥¼ ì¸ì½”ë”©í•˜ë©°, 2D ì„œë¸ŒìŠ¤í˜ì´ìŠ¤ì— íˆ¬ì˜ë  ë•Œ í›ˆë ¨ ìˆœì„œëŒ€ë¡œ ë°°ì—´ëœë‹¤.
- 3. ì„ í˜• í”„ë¡œë¸ŒëŠ” "ì´ˆê¸°"ì™€ "í›„ê¸°" ì—”í‹°í‹°ë¥¼ ì•½ 90%ì˜ ì •í™•ë„ë¡œ êµ¬ë³„í•  ìˆ˜ ìˆë‹¤.
- 4. ëª¨ë¸ì€ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì˜ í›ˆë ¨ ë‹¨ê³„ë¥¼ ì•½ 80%ì˜ ì •í™•ë„ë¡œ ë³´ê³ í•˜ë„ë¡ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìˆë‹¤.
- 5. í›ˆë ¨ ìˆœì„œ ì¸ì½”ë”©ì€ í™œì„±í™” í¬ê¸°, ì†ì‹¤, ëª¨ë¸ì˜ ì‹ ë¢°ë„ì™€ ê°™ì€ ë‹¨ìˆœí•œ ì°¨ì´ì— ê¸°ì¸í•˜ì§€ ì•ŠëŠ”ë‹¤.


---

*Generated on 2025-09-24 01:31:11*