---
keywords:
  - Temporal Question Answering
  - Large Language Model
  - Exact Match
  - Symmetric Mean Absolute Percentage Error
  - Mean Absolute Scaled Error
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16720
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:17:46.102405",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Temporal Question Answering",
    "Large Language Model",
    "Exact Match",
    "Symmetric Mean Absolute Percentage Error",
    "Mean Absolute Scaled Error"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Temporal Question Answering": 0.78,
    "Large Language Model": 0.82,
    "Exact Match": 0.75,
    "Symmetric Mean Absolute Percentage Error": 0.72,
    "Mean Absolute Scaled Error": 0.71
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Temporal Question Answering",
        "canonical": "Temporal Question Answering",
        "aliases": [
          "Temporal QA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized task that evaluates temporal reasoning in language models, which is central to the paper's focus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "The paper evaluates temporal reasoning capabilities specifically within large language models, linking it to broader AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Exact Match",
        "canonical": "Exact Match",
        "aliases": [
          "EM"
        ],
        "category": "unique_technical",
        "rationale": "Exact match is a critical evaluation metric discussed in the paper, highlighting its limitations in temporal QA.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Symmetric Mean Absolute Percentage Error",
        "canonical": "Symmetric Mean Absolute Percentage Error",
        "aliases": [
          "sMAPE"
        ],
        "category": "unique_technical",
        "rationale": "sMAPE is introduced as an alternative metric to exact match, providing a nuanced evaluation of model performance.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Mean Absolute Scaled Error",
        "canonical": "Mean Absolute Scaled Error",
        "aliases": [
          "MASE"
        ],
        "category": "unique_technical",
        "rationale": "MASE is another metric proposed to better assess temporal reasoning, offering insights into model ranking changes.",
        "novelty_score": 0.67,
        "connectivity_score": 0.58,
        "specificity_score": 0.77,
        "link_intent_score": 0.71
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Temporal Question Answering",
      "resolved_canonical": "Temporal Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Exact Match",
      "resolved_canonical": "Exact Match",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Symmetric Mean Absolute Percentage Error",
      "resolved_canonical": "Symmetric Mean Absolute Percentage Error",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Mean Absolute Scaled Error",
      "resolved_canonical": "Mean Absolute Scaled Error",
      "decision": "linked",
      "scores": {
        "novelty": 0.67,
        "connectivity": 0.58,
        "specificity": 0.77,
        "link_intent": 0.71
      }
    }
  ]
}
-->

# Time to Revist Exact Match

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16720.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16720](https://arxiv.org/abs/2509.16720)

## 🔗 유사한 논문
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (81.8% similar)
- [[2025-09-23/Beyond Prompting_ An Efficient Embedding Framework for Open-Domain Question Answering_20250923|Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering]] (81.0% similar)
- [[2025-09-19/SWE-QA_ Can Language Models Answer Repository-level Code Questions?_20250919|SWE-QA: Can Language Models Answer Repository-level Code Questions?]] (80.6% similar)
- [[2025-09-23/Time Is a Feature_ Exploiting Temporal Dynamics in Diffusion Language Models_20250923|Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models]] (80.3% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (79.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Temporal Question Answering|Temporal Question Answering]], [[keywords/Exact Match|Exact Match]], [[keywords/Symmetric Mean Absolute Percentage Error|Symmetric Mean Absolute Percentage Error]], [[keywords/Mean Absolute Scaled Error|Mean Absolute Scaled Error]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16720v1 Announce Type: new 
Abstract: Temporal question answering is an established method for evaluating temporal reasoning in large language models. Expected answers are often numeric (e.g., dates or durations), yet model responses are evaluated like regular text with exact match (EM), unable to distinguish small from large errors. In this investigative work, we frame temporal question answering as a numerical estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a benchmark distilled from Test of Time and TempTabQA, where all questions require a numerical, temporal answer, allowing us to evaluate models beyond EM. We use the forecasting metrics symmetric mean absolute percentage error (sMAPE) and mean absolute scaled error (MASE). With sMAPE, we find that error size and EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some models have high sMAPE despite high EM. Scaling errors by the deviation of the ground truth data with MASE reshuffles model rankings compared to EM, revealing gaps in models' understanding of temporal domain knowledge, especially when trained with synthetic data. Lastly, the models' most frequent error is to deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM, adequately weight these errors. Our findings underscore the need for specialised metrics for temporal QA tasks. Code and data are available on https://github.com/aauss/temporal-answer-qa.

## 📝 요약

이 논문은 대형 언어 모델의 시간적 추론 능력을 평가하는 방법으로서의 시간적 질문 응답(Temporal Question Answering)을 다룹니다. 기존의 정확한 일치(EM) 평가 방식은 작은 오류와 큰 오류를 구분하지 못하는 한계가 있습니다. 이를 해결하기 위해, 저자들은 TempAnswerQA라는 벤치마크를 소개하여 모든 질문이 숫자적, 시간적 답변을 요구하도록 했습니다. 이 연구는 대체 평가 지표로 대칭 평균 절대 백분율 오차(sMAPE)와 평균 절대 스케일링 오차(MASE)를 사용하여 모델의 성능을 평가했습니다. 연구 결과, sMAPE는 EM과 오류 크기를 분리하며, MASE는 모델의 시간적 도메인 지식 이해도 차이를 드러냅니다. 특히, 모델들은 종종 정답에서 ±1만큼 벗어난 오류를 자주 범하는데, sMAPE와 MASE는 이러한 오류를 적절히 평가합니다. 이 연구는 시간적 질문 응답 과제에 특화된 평가 지표의 필요성을 강조합니다.

## 🎯 주요 포인트

- 1. Temporal question answering는 대형 언어 모델의 시간적 추론 능력을 평가하는 방법으로, 기존의 정확한 일치(EM) 평가 방식은 작은 오류와 큰 오류를 구분하지 못한다.
- 2. TempAnswerQA 벤치마크를 도입하여 EM을 넘어서는 모델 평가를 가능하게 하며, 모든 질문이 수치적, 시간적 답변을 요구한다.
- 3. 대칭 평균 절대 백분율 오차(sMAPE)와 평균 절대 스케일링 오차(MASE)를 사용하여 EM과 오류 크기의 분리를 발견하고, 모델의 시간적 도메인 지식 이해의 격차를 드러낸다.
- 4. 모델의 가장 빈번한 오류는 정답에서 ±1만큼 벗어나는 것이며, sMAPE와 MASE는 이러한 오류를 적절히 가중치로 평가한다.
- 5. 우리의 연구 결과는 시간적 QA 작업을 위한 특화된 평가 지표의 필요성을 강조하며, 관련 코드와 데이터를 공개한다.


---

*Generated on 2025-09-24 03:17:46*