---
keywords:
  - Retrieval Augmented Generation
  - Reinforcement Learning from Verifiable Rewards
  - Large Language Model
  - Open-domain Question Answering
  - Hybrid Knowledge Access Strategy
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.06165
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:18:02.152656",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval Augmented Generation",
    "Reinforcement Learning from Verifiable Rewards",
    "Large Language Model",
    "Open-domain Question Answering",
    "Hybrid Knowledge Access Strategy"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Retrieval Augmented Generation": 0.85,
    "Reinforcement Learning from Verifiable Rewards": 0.7,
    "Large Language Model": 0.8,
    "Open-domain Question Answering": 0.78,
    "Hybrid Knowledge Access Strategy": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "RAG is a trending concept that enhances knowledge grounding in LLMs, making it highly relevant for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning from Verifiable Rewards",
        "canonical": "Reinforcement Learning from Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach that combines reinforcement learning with verifiable rewards, offering a unique perspective for linking.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are foundational in this research, providing a broad technical context for linking.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Open-domain QA",
        "canonical": "Open-domain Question Answering",
        "aliases": [
          "Open-domain QA"
        ],
        "category": "specific_connectable",
        "rationale": "Open-domain QA is a specific application area for RAG and RLVR, enhancing connectivity.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Hybrid Knowledge Access Strategy",
        "canonical": "Hybrid Knowledge Access Strategy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This strategy is a unique contribution of the paper, combining offline corpora with LLM-generated summaries.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning from Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Open-domain QA",
      "resolved_canonical": "Open-domain Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Hybrid Knowledge Access Strategy",
      "resolved_canonical": "Hybrid Knowledge Access Strategy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# UR$^2$: Unify RAG and Reasoning through Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.06165.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.06165](https://arxiv.org/abs/2508.06165)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/HydraRAG_ Structured Cross-Source Enhanced Large Language Model Reasoning_20250922|HydraRAG: Structured Cross-Source Enhanced Large Language Model Reasoning]] (88.1% similar)
- [[2025-09-19/Causal-Counterfactual RAG_ The Integration of Causal-Counterfactual Reasoning into RAG_20250919|Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG]] (86.9% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (86.2% similar)
- [[2025-09-22/Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models_20250922|Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models]] (86.1% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility: Process-Supervised Rewrite for RAG]] (85.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Open-domain Question Answering|Open-domain Question Answering]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning from Verifiable Rewards|Reinforcement Learning from Verifiable Rewards]], [[keywords/Hybrid Knowledge Access Strategy|Hybrid Knowledge Access Strategy]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.06165v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope -- typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‘ ê°€ì§€ ì£¼ìš” ê¸°ëŠ¥ì¸ ê²€ìƒ‰ ë³´ê°• ìƒì„±(RAG)ê³¼ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì— ì˜í•œ ê°•í™” í•™ìŠµ(RLVR)ì„ í†µí•©í•˜ëŠ” UR2(í†µí•© RAG ë° ì¶”ë¡ )ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. UR2ëŠ” ë‚œì´ë„ ì¸ì‹ ì»¤ë¦¬í˜ëŸ¼ í›ˆë ¨ê³¼ í•˜ì´ë¸Œë¦¬ë“œ ì§€ì‹ ì ‘ê·¼ ì „ëµì„ ë„ì…í•˜ì—¬ ë‹¤ì–‘í•œ ê³¼ì œì— ì ì‘ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, UR2ëŠ” ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ RAG ë° RL ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©°, GPT-4o-mini ë° GPT-4.1-miniì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ëª¨ë“  ì½”ë“œì™€ ëª¨ë¸ì€ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‘ ê°€ì§€ ì£¼ìš” íŒ¨ëŸ¬ë‹¤ì„ì¸ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ê³¼ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ê¸°ë°˜ ê°•í™” í•™ìŠµ(RLVR)ì„ í†µí•©í•˜ëŠ” UR2 í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. UR2ëŠ” ë‚œì´ë„ ì¸ì‹ ì»¤ë¦¬í˜ëŸ¼ í›ˆë ¨ì„ í†µí•´ ì–´ë ¤ìš´ ë¬¸ì œì—ë§Œ ê²€ìƒ‰ì„ í™œìš©í•˜ê³ , ë„ë©”ì¸ íŠ¹í™” ì˜¤í”„ë¼ì¸ ì½”í¼ìŠ¤ì™€ LLM ìƒì„± ìš”ì•½ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì§€ì‹ ì ‘ê·¼ ì „ëµì„ ë„ì…í•©ë‹ˆë‹¤.
- 3. UR2ëŠ” ê²€ìƒ‰ê³¼ ì¶”ë¡  ê°„ì˜ ë™ì  ì¡°ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œì˜ ì ì‘ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, UR2ëŠ” ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ RAG ë° RL ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©°, GPT-4o-mini ë° GPT-4.1-miniì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. ëª¨ë“  ì½”ë“œ, ëª¨ë¸, ë°ì´í„°ëŠ” ê³µê°œë˜ì–´ ìˆìœ¼ë©°, GitHubì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:18:02*