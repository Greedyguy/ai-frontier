---
keywords:
  - Feature Scaling
  - Ensemble Methods
  - Random Forest
  - Gradient Boosting
  - Logistic Regression
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.08274
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:46:16.320444",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Feature Scaling",
    "Ensemble Methods",
    "Random Forest",
    "Gradient Boosting",
    "Logistic Regression"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Feature Scaling": 0.78,
    "Ensemble Methods": 0.8,
    "Random Forest": 0.77,
    "Gradient Boosting": 0.79,
    "Logistic Regression": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Feature Scaling",
        "canonical": "Feature Scaling",
        "aliases": [
          "Data Normalization",
          "Data Scaling"
        ],
        "category": "unique_technical",
        "rationale": "Feature scaling is crucial for optimizing model performance and is a key consideration in machine learning workflows.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Ensemble Methods",
        "canonical": "Ensemble Methods",
        "aliases": [
          "Ensemble Models"
        ],
        "category": "broad_technical",
        "rationale": "Ensemble methods like Random Forest and gradient boosting are widely used and relevant for linking various machine learning strategies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.89,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Random Forest",
        "canonical": "Random Forest",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Random Forest is a popular ensemble learning method that is often discussed in relation to feature scaling.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Gradient Boosting",
        "canonical": "Gradient Boosting",
        "aliases": [
          "XGBoost",
          "CatBoost",
          "LightGBM"
        ],
        "category": "specific_connectable",
        "rationale": "Gradient boosting techniques are critical for understanding the impact of scaling on predictive performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Logistic Regression",
        "canonical": "Logistic Regression",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Logistic Regression is a fundamental algorithm that is sensitive to feature scaling, making it a key concept for linking.",
        "novelty_score": 0.4,
        "connectivity_score": 0.83,
        "specificity_score": 0.75,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Predictive Performance",
      "Computational Costs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Feature Scaling",
      "resolved_canonical": "Feature Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Ensemble Methods",
      "resolved_canonical": "Ensemble Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.89,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Random Forest",
      "resolved_canonical": "Random Forest",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Gradient Boosting",
      "resolved_canonical": "Gradient Boosting",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Logistic Regression",
      "resolved_canonical": "Logistic Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.83,
        "specificity": 0.75,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.08274.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.08274](https://arxiv.org/abs/2506.08274)

## 🔗 유사한 논문
- [[2025-09-23/Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling_20250923|Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling]] (81.7% similar)
- [[2025-09-23/Elucidating the Design Space of FP4 training_20250923|Elucidating the Design Space of FP4 training]] (81.6% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (80.1% similar)
- [[2025-09-23/Conv-like Scale-Fusion Time Series Transformer_ A Multi-Scale Representation for Variable-Length Long Time Series_20250923|Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series]] (79.7% similar)
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (79.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Ensemble Methods|Ensemble Methods]]
**🔗 Specific Connectable**: [[keywords/Random Forest|Random Forest]], [[keywords/Gradient Boosting|Gradient Boosting]], [[keywords/Logistic Regression|Logistic Regression]]
**⚡ Unique Technical**: [[keywords/Feature Scaling|Feature Scaling]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.08274v4 Announce Type: replace 
Abstract: This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.

## 📝 요약

이 연구는 12가지 특성 스케일링 기법을 14개의 머신러닝 알고리즘과 16개의 데이터셋에 적용하여 분류 및 회귀 작업에서의 성능과 계산 비용에 미치는 영향을 체계적으로 평가했습니다. 주요 발견으로는 앙상블 방법(예: 랜덤 포레스트, XGBoost 등)은 스케일링에 크게 영향을 받지 않는 반면, 로지스틱 회귀, SVM, TabNet, MLP 등은 선택한 스케일러에 따라 성능 차이가 크다는 점이 있습니다. 이 연구는 모든 소스 코드와 실험 결과를 공개하여 투명성과 재현성을 보장하며, 최적의 특성 스케일링 기법 선택에 대한 중요한 지침을 제공합니다.

## 🎯 주요 포인트

- 1. 12개의 스케일링 기법을 14개의 머신러닝 알고리즘과 16개의 데이터셋에 대해 체계적으로 평가하여 특징 스케일링에 대한 포괄적인 연구 부족 문제를 해결했습니다.
- 2. 앙상블 방법(예: 랜덤 포레스트, XGBoost, CatBoost, LightGBM)은 스케일링에 크게 의존하지 않고 견고한 성능을 보였습니다.
- 3. 로지스틱 회귀, SVM, TabNet, MLP와 같은 모델은 선택한 스케일러에 따라 성능 변동이 크게 나타났습니다.
- 4. 예측 성능(정확도, MAE, MSE, $R^2$)과 계산 비용(훈련 시간, 추론 시간, 메모리 사용량)에 미치는 영향을 면밀히 분석했습니다.
- 5. 모든 소스 코드, 실험 결과 및 모델 매개변수를 공개하여 완전한 투명성과 재현성을 보장하고, 실무자들에게 최적의 특징 스케일링 기법 선택에 대한 모델별 중요한 지침을 제공합니다.


---

*Generated on 2025-09-24 02:46:16*