---
keywords:
  - Feature Scaling
  - Ensemble Methods
  - Random Forest
  - Gradient Boosting
  - Logistic Regression
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.08274
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:46:16.320444",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Feature Scaling",
    "Ensemble Methods",
    "Random Forest",
    "Gradient Boosting",
    "Logistic Regression"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Feature Scaling": 0.78,
    "Ensemble Methods": 0.8,
    "Random Forest": 0.77,
    "Gradient Boosting": 0.79,
    "Logistic Regression": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Feature Scaling",
        "canonical": "Feature Scaling",
        "aliases": [
          "Data Normalization",
          "Data Scaling"
        ],
        "category": "unique_technical",
        "rationale": "Feature scaling is crucial for optimizing model performance and is a key consideration in machine learning workflows.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Ensemble Methods",
        "canonical": "Ensemble Methods",
        "aliases": [
          "Ensemble Models"
        ],
        "category": "broad_technical",
        "rationale": "Ensemble methods like Random Forest and gradient boosting are widely used and relevant for linking various machine learning strategies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.89,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Random Forest",
        "canonical": "Random Forest",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Random Forest is a popular ensemble learning method that is often discussed in relation to feature scaling.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Gradient Boosting",
        "canonical": "Gradient Boosting",
        "aliases": [
          "XGBoost",
          "CatBoost",
          "LightGBM"
        ],
        "category": "specific_connectable",
        "rationale": "Gradient boosting techniques are critical for understanding the impact of scaling on predictive performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Logistic Regression",
        "canonical": "Logistic Regression",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Logistic Regression is a fundamental algorithm that is sensitive to feature scaling, making it a key concept for linking.",
        "novelty_score": 0.4,
        "connectivity_score": 0.83,
        "specificity_score": 0.75,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Predictive Performance",
      "Computational Costs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Feature Scaling",
      "resolved_canonical": "Feature Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Ensemble Methods",
      "resolved_canonical": "Ensemble Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.89,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Random Forest",
      "resolved_canonical": "Random Forest",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Gradient Boosting",
      "resolved_canonical": "Gradient Boosting",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Logistic Regression",
      "resolved_canonical": "Logistic Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.83,
        "specificity": 0.75,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.08274.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.08274](https://arxiv.org/abs/2506.08274)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling_20250923|Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling]] (81.7% similar)
- [[2025-09-23/Elucidating the Design Space of FP4 training_20250923|Elucidating the Design Space of FP4 training]] (81.6% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (80.1% similar)
- [[2025-09-23/Conv-like Scale-Fusion Time Series Transformer_ A Multi-Scale Representation for Variable-Length Long Time Series_20250923|Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series]] (79.7% similar)
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Ensemble Methods|Ensemble Methods]]
**ğŸ”— Specific Connectable**: [[keywords/Random Forest|Random Forest]], [[keywords/Gradient Boosting|Gradient Boosting]], [[keywords/Logistic Regression|Logistic Regression]]
**âš¡ Unique Technical**: [[keywords/Feature Scaling|Feature Scaling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.08274v4 Announce Type: replace 
Abstract: This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” 12ê°€ì§€ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì„ 14ê°œì˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ 16ê°œì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ì—¬ ë¶„ë¥˜ ë° íšŒê·€ ì‘ì—…ì—ì„œì˜ ì„±ëŠ¥ê³¼ ê³„ì‚° ë¹„ìš©ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ìœ¼ë¡œëŠ” ì•™ìƒë¸” ë°©ë²•(ì˜ˆ: ëœë¤ í¬ë ˆìŠ¤íŠ¸, XGBoost ë“±)ì€ ìŠ¤ì¼€ì¼ë§ì— í¬ê²Œ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” ë°˜ë©´, ë¡œì§€ìŠ¤í‹± íšŒê·€, SVM, TabNet, MLP ë“±ì€ ì„ íƒí•œ ìŠ¤ì¼€ì¼ëŸ¬ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ë‹¤ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œì™€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ê³µê°œí•˜ì—¬ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ë©°, ìµœì ì˜ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ê¸°ë²• ì„ íƒì— ëŒ€í•œ ì¤‘ìš”í•œ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. 12ê°œì˜ ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì„ 14ê°œì˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ 16ê°œì˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ì— ëŒ€í•œ í¬ê´„ì ì¸ ì—°êµ¬ ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
- 2. ì•™ìƒë¸” ë°©ë²•(ì˜ˆ: ëœë¤ í¬ë ˆìŠ¤íŠ¸, XGBoost, CatBoost, LightGBM)ì€ ìŠ¤ì¼€ì¼ë§ì— í¬ê²Œ ì˜ì¡´í•˜ì§€ ì•Šê³  ê²¬ê³ í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 3. ë¡œì§€ìŠ¤í‹± íšŒê·€, SVM, TabNet, MLPì™€ ê°™ì€ ëª¨ë¸ì€ ì„ íƒí•œ ìŠ¤ì¼€ì¼ëŸ¬ì— ë”°ë¼ ì„±ëŠ¥ ë³€ë™ì´ í¬ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
- 4. ì˜ˆì¸¡ ì„±ëŠ¥(ì •í™•ë„, MAE, MSE, $R^2$)ê³¼ ê³„ì‚° ë¹„ìš©(í›ˆë ¨ ì‹œê°„, ì¶”ë¡  ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë©´ë°€íˆ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
- 5. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œ, ì‹¤í—˜ ê²°ê³¼ ë° ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³µê°œí•˜ì—¬ ì™„ì „í•œ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ê³ , ì‹¤ë¬´ìë“¤ì—ê²Œ ìµœì ì˜ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ê¸°ë²• ì„ íƒì— ëŒ€í•œ ëª¨ë¸ë³„ ì¤‘ìš”í•œ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:46:16*