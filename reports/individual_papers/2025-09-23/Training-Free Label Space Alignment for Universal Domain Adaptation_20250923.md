---
keywords:
  - Universal Domain Adaptation
  - Vision-Language Model
  - Zero-Shot Learning
  - Label Space Alignment
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17452
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:56:44.676762",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Universal Domain Adaptation",
    "Vision-Language Model",
    "Zero-Shot Learning",
    "Label Space Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Universal Domain Adaptation": 0.78,
    "Vision-Language Model": 0.82,
    "Zero-Shot Learning": 0.8,
    "Label Space Alignment": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Universal Domain Adaptation",
        "canonical": "Universal Domain Adaptation",
        "aliases": [
          "UniDA"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a specific adaptation challenge that connects to domain adaptation literature.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Foundation Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "These models are crucial for the proposed method and connect to recent advances in multimodal learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Zero-Shot Capabilities",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a key feature leveraged by the method, linking to broader machine learning capabilities.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Label Space Alignment",
        "canonical": "Label Space Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel approach proposed in the paper, crucial for understanding the method's contribution.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Universal Domain Adaptation",
      "resolved_canonical": "Universal Domain Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Foundation Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Zero-Shot Capabilities",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Label Space Alignment",
      "resolved_canonical": "Label Space Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Training-Free Label Space Alignment for Universal Domain Adaptation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17452.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17452](https://arxiv.org/abs/2509.17452)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses_20250919|Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses]] (83.1% similar)
- [[2025-09-22/CoDoL_ Conditional Domain Prompt Learning for Out-of-Distribution Generalization_20250922|CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization]] (83.0% similar)
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (82.8% similar)
- [[2025-09-22/Advances in Multimodal Adaptation and Generalization_ From Traditional Approaches to Foundation Models_20250922|Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models]] (82.8% similar)
- [[2025-09-22/Domain-invariant feature learning in brain MR imaging for content-based image retrieval_20250922|Domain-invariant feature learning in brain MR imaging for content-based image retrieval]] (82.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Universal Domain Adaptation|Universal Domain Adaptation]], [[keywords/Label Space Alignment|Label Space Alignment]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17452v1 Announce Type: cross 
Abstract: Universal domain adaptation (UniDA) transfers knowledge from a labeled source domain to an unlabeled target domain, where label spaces may differ and the target domain may contain private classes. Previous UniDA methods primarily focused on visual space alignment but often struggled with visual ambiguities due to content differences, which limited their robustness and generalizability. To overcome this, we introduce a novel approach that leverages the strong \textit{zero-shot capabilities} of recent vision-language foundation models (VLMs) like CLIP, concentrating solely on label space alignment to enhance adaptation stability. CLIP can generate task-specific classifiers based only on label names. However, adapting CLIP to UniDA is challenging because the label space is not fully known in advance. In this study, we first utilize generative vision-language models to identify unknown categories in the target domain. Noise and semantic ambiguities in the discovered labels -- such as those similar to source labels (e.g., synonyms, hypernyms, hyponyms) -- complicate label alignment. To address this, we propose a training-free label-space alignment method for UniDA (\ours). Our method aligns label spaces instead of visual spaces by filtering and refining noisy labels between the domains. We then construct a \textit{universal classifier} that integrates both shared knowledge and target-private class information, thereby improving generalizability under domain shifts. The results reveal that the proposed method considerably outperforms existing UniDA techniques across key DomainBed benchmarks, delivering an average improvement of \textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score. Furthermore, incorporating self-training further enhances performance and achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and H$^3$-scores.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë³´í¸ì  ë„ë©”ì¸ ì ì‘(UniDA)ì—ì„œ ë ˆì´ë¸” ê³µê°„ ì •ë ¬ì„ í†µí•´ ì ì‘ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì£¼ë¡œ ì‹œê°ì  ê³µê°„ ì •ë ¬ì— ì§‘ì¤‘í–ˆìœ¼ë‚˜, ì‹œê°ì  ëª¨í˜¸ì„± ë¬¸ì œë¡œ ì¸í•´ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìµœê·¼ì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì¸ CLIPì˜ ê°•ë ¥í•œ ì œë¡œìƒ· ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë ˆì´ë¸” ê³µê°„ ì •ë ¬ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. CLIPì€ ë ˆì´ë¸” ì´ë¦„ë§Œìœ¼ë¡œë„ ì‘ì—…ë³„ ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, ë ˆì´ë¸” ê³µê°„ì´ ì‚¬ì „ì— ì™„ì „íˆ ì•Œë ¤ì§€ì§€ ì•Šì•„ UniDAì— ì ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ìƒì„±ì  ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ ëŒ€ìƒ ë„ë©”ì¸ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ë¥¼ ì‹ë³„í•˜ê³ , ë°œê²¬ëœ ë ˆì´ë¸”ì˜ ì¡ìŒê³¼ ì˜ë¯¸ì  ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” ë ˆì´ë¸” ê³µê°„ ì •ë ¬ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë„ë©”ì¸ ê°„ì˜ ì¡ìŒ ìˆëŠ” ë ˆì´ë¸”ì„ ì •ì œí•˜ì—¬ ë ˆì´ë¸” ê³µê°„ì„ ì •ë ¬í•˜ê³ , ê³µìœ  ì§€ì‹ê³¼ ëŒ€ìƒ ì „ìš© í´ë˜ìŠ¤ ì •ë³´ë¥¼ í†µí•©í•œ ë³´í¸ì  ë¶„ë¥˜ê¸°ë¥¼ êµ¬ì¶•í•˜ì—¬ ë„ë©”ì¸ ë³€í™”ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì£¼ìš” DomainBed ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ UniDA ê¸°ë²•ë³´ë‹¤ í‰ê·  7.9%ì˜ H-scoreì™€ 6.1%ì˜ HÂ³-score ê°œì„ ì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, ìê¸° í›ˆë ¨ì„ ì¶”ê°€í•˜ë©´ ì„±ëŠ¥ì´ ì¶”ê°€ë¡œ 1.6% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ìµœê·¼ì˜ ë¹„ì „-ì–¸ì–´ ê¸°ë°˜ ëª¨ë¸ì¸ CLIPì˜ ê°•ë ¥í•œ ì œë¡œìƒ· ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë ˆì´ë¸” ê³µê°„ ì •ë ¬ì— ì§‘ì¤‘í•¨ìœ¼ë¡œì¨ ë³´í¸ì  ë„ë©”ì¸ ì ì‘ì˜ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. ì œì•ˆëœ ë°©ë²•ì€ ë„ë©”ì¸ ê°„ì˜ ì‹œê°ì  ê³µê°„ì´ ì•„ë‹Œ ë ˆì´ë¸” ê³µê°„ì„ ì •ë ¬í•˜ì—¬, ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë ˆì´ë¸”ì„ í•„í„°ë§í•˜ê³  ì •ì œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë ˆì´ë¸” ê³µê°„ì„ ì •ë ¬í•©ë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” ê³µìœ  ì§€ì‹ê³¼ íƒ€ê²Ÿ ì „ìš© í´ë˜ìŠ¤ ì •ë³´ë¥¼ í†µí•©í•œ ë³´í¸ì  ë¶„ë¥˜ê¸°ë¥¼ êµ¬ì¶•í•˜ì—¬ ë„ë©”ì¸ ë³€í™”ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ì£¼ìš” DomainBed ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ì˜ ë³´í¸ì  ë„ë©”ì¸ ì ì‘ ê¸°ë²•ì„ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, H-ì ìˆ˜ì—ì„œ í‰ê·  +7.9%, HÂ³-ì ìˆ˜ì—ì„œ +6.1%ì˜ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. ìê¸° í›ˆë ¨ì„ í†µí•©í•˜ë©´ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë˜ì–´ H- ë° HÂ³-ì ìˆ˜ì—ì„œ ì¶”ê°€ë¡œ +1.6%ì˜ ì¦ê°€ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:56:44*