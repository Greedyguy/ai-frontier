---
keywords:
  - Multimodal Learning
  - Pixel-level Visual Reasoning
  - Vision-Language Model
  - Referring Expression Segmentation
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.18094
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:21:40.182725",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Pixel-level Visual Reasoning",
    "Vision-Language Model",
    "Referring Expression Segmentation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.8,
    "Pixel-level Visual Reasoning": 0.78,
    "Vision-Language Model": 0.79,
    "Referring Expression Segmentation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Multi-modal Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "LMMs",
          "Large Multimodal Models"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the broader field of integrating multiple data types, which is central to the paper's approach.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Pixel-level Visual Reasoning",
        "canonical": "Pixel-level Visual Reasoning",
        "aliases": [
          "Pixel-level Reasoning",
          "Fine-grained Visual Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel approach within the field, focusing on detailed visual analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Understanding",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Integration"
        ],
        "category": "evolved_concepts",
        "rationale": "Highlights the intersection of visual and language processing, crucial for the paper's methodology.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      },
      {
        "surface": "Referring Expression Segmentation",
        "canonical": "Referring Expression Segmentation",
        "aliases": [
          "Referring Segmentation"
        ],
        "category": "specific_connectable",
        "rationale": "A specific task that aligns with the paper's focus on integrating segmentation and language.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.77,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Multi-modal Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Pixel-level Visual Reasoning",
      "resolved_canonical": "Pixel-level Visual Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Understanding",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Referring Expression Segmentation",
      "resolved_canonical": "Referring Expression Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.77,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18094.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.18094](https://arxiv.org/abs/2509.18094)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (83.8% similar)
- [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (83.7% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (83.4% similar)
- [[2025-09-22/UniMRSeg_ Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation_20250922|UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation]] (83.4% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Referring Expression Segmentation|Referring Expression Segmentation]]
**âš¡ Unique Technical**: [[keywords/Pixel-level Visual Reasoning|Pixel-level Visual Reasoning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18094v1 Announce Type: cross 
Abstract: Recent advances in Large Multi-modal Models (LMMs) have demonstrated their remarkable success as general-purpose multi-modal assistants, with particular focuses on holistic image- and video-language understanding. Conversely, less attention has been given to scaling fine-grained pixel-level understanding capabilities, where the models are expected to realize pixel-level alignment between visual signals and language semantics. Some previous studies have applied LMMs to related tasks such as region-level captioning and referring expression segmentation. However, these models are limited to performing either referring or segmentation tasks independently and fail to integrate these fine-grained perception capabilities into visual reasoning. To bridge this gap, we propose UniPixel, a large multi-modal model capable of flexibly comprehending visual prompt inputs and generating mask-grounded responses. Our model distinguishes itself by seamlessly integrating pixel-level perception with general visual understanding capabilities. Specifically, UniPixel processes visual prompts and generates relevant masks on demand, and performs subsequent reasoning conditioning on these intermediate pointers during inference, thereby enabling fine-grained pixel-level reasoning. The effectiveness of our approach has been verified on 10 benchmarks across a diverse set of tasks, including pixel-level referring/segmentation and object-centric understanding in images/videos. A novel PixelQA task that jointly requires referring, segmentation, and question answering is also designed to verify the flexibility of our method.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(LMMs)ì˜ ë°œì „ì€ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤-ì–¸ì–´ ì´í•´ì—ì„œ í° ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ì„¸ë°€í•œ í”½ì…€ ìˆ˜ì¤€ì˜ ì´í•´ ëŠ¥ë ¥ í™•ì¥ì—ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ëœ ì£¼ëª©ë°›ì•˜ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ UniPixelì´ë¼ëŠ” ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. UniPixelì€ ì‹œê°ì  í”„ë¡¬í”„íŠ¸ë¥¼ ìœ ì—°í•˜ê²Œ ì´í•´í•˜ê³  ë§ˆìŠ¤í¬ ê¸°ë°˜ì˜ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ, í”½ì…€ ìˆ˜ì¤€ì˜ ì¸ì‹ê³¼ ì¼ë°˜ì ì¸ ì‹œê° ì´í•´ ëŠ¥ë ¥ì„ í†µí•©í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì‹œê°ì  í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  í•„ìš”í•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ë©°, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì—¬ ì„¸ë°€í•œ í”½ì…€ ìˆ˜ì¤€ì˜ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ 10ê°œì˜ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íš¨ê³¼ê°€ ê²€ì¦ë˜ì—ˆìœ¼ë©°, PixelQAë¼ëŠ” ìƒˆë¡œìš´ ê³¼ì œë¥¼ í†µí•´ ëª¨ë¸ì˜ ìœ ì—°ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(LMMs)ì€ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì–¸ì–´ ì´í•´ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ì„¸ë°€í•œ í”½ì…€ ìˆ˜ì¤€ì˜ ì´í•´ ëŠ¥ë ¥ í™•ì¥ì—ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ê´€ì‹¬ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ LMMsë¥¼ ì˜ì—­ ìˆ˜ì¤€ ìº¡ì…˜ ìƒì„± ë° ì§€ì‹œ í‘œí˜„ ë¶„í• ê³¼ ê°™ì€ ê´€ë ¨ ì‘ì—…ì— ì ìš©í–ˆì§€ë§Œ, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ì§€ì‹œ ë˜ëŠ” ë¶„í•  ì‘ì—…ì„ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë° í•œì •ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. UniPixelì€ ì‹œê°ì  í”„ë¡¬í”„íŠ¸ ì…ë ¥ì„ ìœ ì—°í•˜ê²Œ ì´í•´í•˜ê³  ë§ˆìŠ¤í¬ ê¸°ë°˜ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ, í”½ì…€ ìˆ˜ì¤€ì˜ ì¸ì‹ê³¼ ì¼ë°˜ì ì¸ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ í†µí•©í•©ë‹ˆë‹¤.
- 4. UniPixelì€ ì‹œê°ì  í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  í•„ìš”ì— ë”°ë¼ ê´€ë ¨ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ë©°, ì¶”ë¡  ì¤‘ ì´ëŸ¬í•œ ì¤‘ê°„ í¬ì¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë°€í•œ í”½ì…€ ìˆ˜ì¤€ì˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 5. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ 10ê°œì˜ ë‹¤ì–‘í•œ ì‘ì—… ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê·¸ íš¨ê³¼ê°€ ê²€ì¦ë˜ì—ˆìœ¼ë©°, PixelQAë¼ëŠ” ìƒˆë¡œìš´ ì‘ì—…ì„ í†µí•´ ì§€ì‹œ, ë¶„í• , ì§ˆë¬¸ ì‘ë‹µì„ í†µí•©ì ìœ¼ë¡œ ìš”êµ¬í•˜ëŠ” ë°©ë²•ì˜ ìœ ì—°ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:21:40*