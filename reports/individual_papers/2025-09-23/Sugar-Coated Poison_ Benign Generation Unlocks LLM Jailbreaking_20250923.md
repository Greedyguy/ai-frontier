---
keywords:
  - Large Language Model
  - Defense Threshold Decay
  - Sugar-Coated Poison
  - Part-of-Speech Defense
  - Prompt Engineering
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2504.05652
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:15:59.418069",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Defense Threshold Decay",
    "Sugar-Coated Poison",
    "Part-of-Speech Defense",
    "Prompt Engineering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Defense Threshold Decay": 0.78,
    "Sugar-Coated Poison": 0.82,
    "Part-of-Speech Defense": 0.8,
    "Prompt Engineering": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on safety mechanisms and jailbreak attacks.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Defense Threshold Decay",
        "canonical": "Defense Threshold Decay",
        "aliases": [
          "DTD"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept affecting LLM safety, pivotal for understanding the proposed attack.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sugar-Coated Poison",
        "canonical": "Sugar-Coated Poison",
        "aliases": [
          "SCP"
        ],
        "category": "unique_technical",
        "rationale": "Represents a new attack paradigm that is central to the paper's contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Part-of-Speech Defense",
        "canonical": "Part-of-Speech Defense",
        "aliases": [
          "POSD"
        ],
        "category": "unique_technical",
        "rationale": "Proposed defense mechanism leveraging syntactic analysis, crucial for enhancing LLM safety.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Prompt Engineering",
        "canonical": "Prompt Engineering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A key technique in jailbreak attacks, relevant for linking discussions on LLM vulnerabilities.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "jailbreak attacks",
      "safety mechanisms"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Defense Threshold Decay",
      "resolved_canonical": "Defense Threshold Decay",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sugar-Coated Poison",
      "resolved_canonical": "Sugar-Coated Poison",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Part-of-Speech Defense",
      "resolved_canonical": "Part-of-Speech Defense",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Prompt Engineering",
      "resolved_canonical": "Prompt Engineering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2504.05652.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2504.05652](https://arxiv.org/abs/2504.05652)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Targeting Alignment_ Extracting Safety Classifiers of Aligned LLMs_20250923|Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs]] (88.9% similar)
- [[2025-09-23/Adaptive Distraction_ Probing LLM Contextual Robustness with Automated Tree Search_20250923|Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search]] (88.8% similar)
- [[2025-09-23/Revisiting Backdoor Attacks on LLMs_ A Stealthy and Practical Poisoning Framework via Harmless Inputs_20250923|Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs]] (88.5% similar)
- [[2025-09-23/MIST_ Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning_20250923|MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning]] (88.3% similar)
- [[2025-09-22/SABER_ Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection_20250922|SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection]] (88.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Prompt Engineering|Prompt Engineering]]
**âš¡ Unique Technical**: [[keywords/Defense Threshold Decay|Defense Threshold Decay]], [[keywords/Sugar-Coated Poison|Sugar-Coated Poison]], [[keywords/Part-of-Speech Defense|Part-of-Speech Defense]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.05652v3 Announce Type: replace-cross 
Abstract: With the increasingly deep integration of large language models (LLMs) across diverse domains, the effectiveness of their safety mechanisms is encountering severe challenges. Currently, jailbreak attacks based on prompt engineering have become a major safety threat. However, existing methods primarily rely on black-box manipulation of prompt templates, resulting in poor interpretability and limited generalization. To break through the bottleneck, this study first introduces the concept of Defense Threshold Decay (DTD), revealing the potential safety impact caused by LLMs' benign generation: as benign content generation in LLMs increases, the model's focus on input instructions progressively diminishes. Building on this insight, we propose the Sugar-Coated Poison (SCP) attack paradigm, which uses a "semantic reversal" strategy to craft benign inputs that are opposite in meaning to malicious intent. This strategy induces the models to generate extensive benign content, thereby enabling adversarial reasoning to bypass safety mechanisms. Experiments show that SCP outperforms existing baselines. Remarkably, it achieves an average attack success rate of 87.23% across six LLMs. For defense, we propose Part-of-Speech Defense (POSD), leveraging verb-noun dependencies for syntactic analysis to enhance safety of LLMs while preserving their generalization ability.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì´ ì§ë©´í•œ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë°˜ íƒˆì˜¥ ê³µê²©ì€ í•´ì„ ê°€ëŠ¥ì„±ê³¼ ì¼ë°˜í™”ê°€ ì œí•œì ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì—°êµ¬ëŠ” 'ë°©ì–´ ì„ê³„ê°’ ê°ì‡ (DTD)' ê°œë…ì„ ë„ì…í•˜ì—¬ LLMì˜ ì•ˆì „ì„±ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì„¤íƒ• ì½”íŒ… ë…(SCP)' ê³µê²© íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ì˜ë¯¸ ë°˜ì „ ì „ëµì„ ì‚¬ìš©í•´ ì•…ì˜ì  ì˜ë„ë¥¼ ê°€ì§„ ì…ë ¥ì„ ë¬´í•´í•œ ê²ƒìœ¼ë¡œ ìœ„ì¥í•˜ì—¬ ëª¨ë¸ì˜ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ SCPëŠ” ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì´ë©°, í‰ê·  87.23%ì˜ ê³µê²© ì„±ê³µë¥ ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ë°©ì–´ ì¸¡ë©´ì—ì„œëŠ” ë™ì‚¬-ëª…ì‚¬ ì˜ì¡´ì„±ì„ í™œìš©í•œ 'í’ˆì‚¬ ë°©ì–´(POSD)'ë¥¼ ì œì•ˆí•˜ì—¬ LLMì˜ ì•ˆì „ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì´ ì ì  ë” ì‹¬ê°í•œ ë„ì „ì— ì§ë©´í•˜ê³  ìˆìœ¼ë©°, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ íƒˆì˜¥ ê³µê²©ì´ ì£¼ìš” ìœ„í˜‘ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ì€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì˜ ë¸”ë™ë°•ìŠ¤ ì¡°ì‘ì— ì˜ì¡´í•˜ì—¬ í•´ì„ ê°€ëŠ¥ì„±ê³¼ ì¼ë°˜í™”ê°€ ì œí•œì ì´ë‹¤.
- 3. ì—°êµ¬ì—ì„œëŠ” ë°©ì–´ ì„ê³„ê°’ ê°ì‡ (DTD) ê°œë…ì„ ë„ì…í•˜ì—¬ LLMì˜ ë¬´í•´í•œ ìƒì„±ì´ ì•ˆì „ì— ë¯¸ì¹˜ëŠ” ì ì¬ì  ì˜í–¥ì„ ë°í˜€ëƒˆë‹¤.
- 4. "ì˜ë¯¸ ë°˜ì „" ì „ëµì„ ì‚¬ìš©í•˜ëŠ” Sugar-Coated Poison(SCP) ê³µê²© íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•˜ì—¬, ëª¨ë¸ì´ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ë„ë¡ ìœ ë„í•œë‹¤.
- 5. SCPëŠ” ê¸°ì¡´ ê¸°ì¤€ì„ ëŠ¥ê°€í•˜ë©°, ì—¬ì„¯ ê°œì˜ LLMì—ì„œ í‰ê·  87.23%ì˜ ê³µê²© ì„±ê³µë¥ ì„ ë‹¬ì„±í–ˆë‹¤. ë°©ì–´ë¥¼ ìœ„í•´ ë™ì‚¬-ëª…ì‚¬ ì˜ì¡´ì„±ì„ í™œìš©í•œ í’ˆì‚¬ ë°©ì–´(POSD)ë¥¼ ì œì•ˆí•œë‹¤.


---

*Generated on 2025-09-24 04:15:59*