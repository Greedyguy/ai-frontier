---
keywords:
  - Diffusion Transformers
  - Adaptive Context Enrichment
  - Attention Mechanism
  - Training-free Video Editing
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17818
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:04:34.871599",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Transformers",
    "Adaptive Context Enrichment",
    "Attention Mechanism",
    "Training-free Video Editing"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Transformers": 0.8,
    "Adaptive Context Enrichment": 0.85,
    "Attention Mechanism": 0.78,
    "Training-free Video Editing": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformers",
        "canonical": "Diffusion Transformers",
        "aliases": [
          "DiT"
        ],
        "category": "unique_technical",
        "rationale": "A novel concept specific to this paper, providing a unique approach to video object editing.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adaptive Context Enrichment",
        "canonical": "Adaptive Context Enrichment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Central to the paper's methodology, addressing contextual conflicts in video editing.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Self-attention"
        ],
        "category": "specific_connectable",
        "rationale": "A key component in the proposed method, linking to broader attention-based models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Training-free video object editing",
        "canonical": "Training-free Video Editing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes the paper's primary focus, offering a new perspective on video editing without training.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformers",
      "resolved_canonical": "Diffusion Transformers",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adaptive Context Enrichment",
      "resolved_canonical": "Adaptive Context Enrichment",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Training-free video object editing",
      "resolved_canonical": "Training-free Video Editing",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17818.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17818](https://arxiv.org/abs/2509.17818)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (82.7% similar)
- [[2025-09-17/RFM-Editing_ Rectified Flow Matching for Text-guided Audio Editing_20250917|RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing]] (81.9% similar)
- [[2025-09-23/Prompt-Driven Agentic Video Editing System_ Autonomous Comprehension of Long-Form, Story-Driven Media_20250923|Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media]] (81.8% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (81.6% similar)
- [[2025-09-17/BWCache_ Accelerating Video Diffusion Transformers through Block-Wise Caching_20250917|BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Diffusion Transformers|Diffusion Transformers]], [[keywords/Adaptive Context Enrichment|Adaptive Context Enrichment]], [[keywords/Training-free Video Editing|Training-free Video Editing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17818v1 Announce Type: new 
Abstract: Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude "hard" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í›ˆë ¨ ì—†ì´ ë¹„ë””ì˜¤ ê°ì²´ í¸ì§‘ì„ ìˆ˜í–‰í•˜ëŠ” ContextFlowë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ì •í™•í•œ ë°˜ì „ê³¼ ë¬¸ë§¥ì  ì¶©ëŒ ë¬¸ì œë¥¼ ê²ªëŠ” ë°˜ë©´, ContextFlowëŠ” ê³ ì°¨ì› Rectified Flow ì†”ë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²¬ê³ í•œ í¸ì§‘ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤. í•µì‹¬ ê¸°ì—¬ëŠ” Adaptive Context Enrichment ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ, ì´ëŠ” ê°ì²´ í¸ì§‘ ì‹œ ë¬¸ë§¥ì  ì¶©ëŒì„ í•´ê²°í•˜ê³ , ì…€í”„ ì–´í…ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°•í™”í•˜ì—¬ ì •ë³´ ìœµí•©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì„ í†µí•´ ì‘ì—…ë³„ë¡œ ì¤‘ìš”í•œ DiT ë¸”ë¡ì„ ì‹ë³„í•˜ì—¬ íš¨ê³¼ì ì¸ ê°€ì´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ContextFlowëŠ” ê¸°ì¡´ì˜ í›ˆë ¨ ì—†ëŠ” ë°©ë²•ë“¤ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ëª‡ëª‡ ìµœì²¨ë‹¨ í›ˆë ¨ ê¸°ë°˜ ë°©ë²•ë“¤ë³´ë‹¤ë„ ìš°ìˆ˜í•œ ì‹œê°„ì  ì¼ê´€ì„±ê³¼ ê³ í’ˆì§ˆ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ContextFlowëŠ” DiT ê¸°ë°˜ ë¹„ë””ì˜¤ ê°ì²´ í¸ì§‘ì„ ìœ„í•œ ìƒˆë¡œìš´ í›ˆë ¨ ì—†ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ê°ì²´ ì‚½ì…, êµì²´, ì‚­ì œ ë“±ì˜ ì‘ì—…ì—ì„œ ë†’ì€ ì •í™•ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. Adaptive Context Enrichment ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ê¸°ì¡´ì˜ "í•˜ë“œ" í”¼ì²˜ êµì²´ë¡œ ì¸í•œ ë¬¸ë§¥ì  ì¶©ëŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , Key-Value ìŒì„ ë³‘ë ¬ ê²½ë¡œì—ì„œ ê²°í•©í•˜ì—¬ ì •ë³´ ìœµí•©ì„ ê°•í™”í•©ë‹ˆë‹¤.
- 3. ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì„ í†µí•´ ì‘ì—…ë³„ë¡œ ì¤‘ìš”í•œ DiT ë¸”ë¡ì„ ì‹ë³„í•˜ê³ , ìƒˆë¡œìš´ Guidance Responsiveness Metricì„ í™œìš©í•˜ì—¬ íš¨ê³¼ì ì¸ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. ContextFlowëŠ” ê¸°ì¡´ì˜ í›ˆë ¨ ì—†ëŠ” ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©°, ì—¬ëŸ¬ ìµœì‹  í›ˆë ¨ ê¸°ë°˜ ì ‘ê·¼ë²•ë³´ë‹¤ë„ ë›°ì–´ë‚œ ì‹œê°„ì  ì¼ê´€ì„±ê³¼ ê³ í’ˆì§ˆ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:04:34*