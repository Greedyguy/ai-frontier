---
keywords:
  - Multimodal Learning
  - Group Relative Policy Optimization
  - Visual Fidelity Reward
  - Table to LaTeX Conversion
  - TEDS-Structure
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17589
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:02:14.063948",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Group Relative Policy Optimization",
    "Visual Fidelity Reward",
    "Table to LaTeX Conversion",
    "TEDS-Structure"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Group Relative Policy Optimization": 0.78,
    "Visual Fidelity Reward": 0.71,
    "Table to LaTeX Conversion": 0.75,
    "TEDS-Structure": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "reinforced multimodal large language model",
        "canonical": "Multimodal Learning",
        "aliases": [
          "reinforced MLLM",
          "multimodal language model"
        ],
        "category": "specific_connectable",
        "rationale": "This concept bridges language and vision tasks, enhancing connectivity in multimodal research.",
        "novelty_score": 0.58,
        "connectivity_score": 0.87,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "This novel reinforcement learning strategy is central to the paper's methodology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.64,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "visual fidelity reward",
        "canonical": "Visual Fidelity Reward",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This specific reward mechanism is crucial for optimizing visual output quality.",
        "novelty_score": 0.68,
        "connectivity_score": 0.59,
        "specificity_score": 0.79,
        "link_intent_score": 0.71
      },
      {
        "surface": "table image to LaTeX code generation",
        "canonical": "Table to LaTeX Conversion",
        "aliases": [
          "table image conversion",
          "table to LaTeX"
        ],
        "category": "unique_technical",
        "rationale": "This task-specific process is the focus of the paper and links to document conversion research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.66,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "TEDS-Structure",
        "canonical": "TEDS-Structure",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This evaluation protocol is key to assessing structural accuracy in the paper's context.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation protocol"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "reinforced multimodal large language model",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.87,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.64,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "visual fidelity reward",
      "resolved_canonical": "Visual Fidelity Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.59,
        "specificity": 0.79,
        "link_intent": 0.71
      }
    },
    {
      "candidate_surface": "table image to LaTeX code generation",
      "resolved_canonical": "Table to LaTeX Conversion",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.66,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "TEDS-Structure",
      "resolved_canonical": "TEDS-Structure",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17589.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17589](https://arxiv.org/abs/2509.17589)

## 🔗 유사한 논문
- [[2025-09-19/TableDART_ Dynamic Adaptive Multi-Modal Routing for Table Understanding_20250919|TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding]] (84.9% similar)
- [[2025-09-22/Structured Information for Improving Spatial Relationships in Text-to-Image Generation_20250922|Structured Information for Improving Spatial Relationships in Text-to-Image Generation]] (81.3% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (80.7% similar)
- [[2025-09-19/Generalizable Geometric Image Caption Synthesis_20250919|Generalizable Geometric Image Caption Synthesis]] (80.6% similar)
- [[2025-09-22/AcT2I_ Evaluating and Improving Action Depiction in Text-to-Image Models_20250922|AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models]] (80.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]], [[keywords/Visual Fidelity Reward|Visual Fidelity Reward]], [[keywords/Table to LaTeX Conversion|Table to LaTeX Conversion]], [[keywords/TEDS-Structure|TEDS-Structure]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17589v1 Announce Type: new 
Abstract: In this work, we address the task of table image to LaTeX code generation, with the goal of automating the reconstruction of high-quality, publication-ready tables from visual inputs. A central challenge of this task lies in accurately handling complex tables -- those with large sizes, deeply nested structures, and semantically rich or irregular cell content -- where existing methods often fail. We begin with a comprehensive analysis, identifying key challenges and highlighting the limitations of current evaluation protocols. To overcome these issues, we propose a reinforced multimodal large language model (MLLM) framework, where a pre-trained MLLM is fine-tuned on a large-scale table-to-LaTeX dataset. To further improve generation quality, we introduce a dual-reward reinforcement learning strategy based on Group Relative Policy Optimization (GRPO). Unlike standard approaches that optimize purely over text outputs, our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality. We adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and show that our method achieves state-of-the-art performance, particularly on structurally complex tables, demonstrating the effectiveness and robustness of our approach.

## 📝 요약

이 연구는 테이블 이미지를 LaTeX 코드로 변환하는 작업을 자동화하여 고품질의 출판 준비가 된 테이블을 시각적 입력으로부터 재구성하는 것을 목표로 합니다. 복잡한 테이블을 정확하게 처리하는 것이 주요 과제로, 기존 방법들이 종종 실패하는 부분입니다. 이를 해결하기 위해, 우리는 대규모 테이블-대-LaTeX 데이터셋에 대해 사전 학습된 다중 모달 대형 언어 모델(MLLM)을 미세 조정하는 강화된 MLLM 프레임워크를 제안합니다. 생성 품질을 향상시키기 위해, Group Relative Policy Optimization(GRPO)에 기반한 이중 보상 강화 학습 전략을 도입하였습니다. 이 방법은 LaTeX 코드의 구조적 보상과 렌더링된 출력에서 계산된 시각적 충실도 보상을 포함하여 시각적 출력 품질을 직접 최적화합니다. TEDS-Structure와 CW-SSIM을 결합한 하이브리드 평가 프로토콜을 채택하여, 특히 구조적으로 복잡한 테이블에서 최첨단 성능을 달성함을 보여주었습니다.

## 🎯 주요 포인트

- 1. 본 연구는 테이블 이미지에서 LaTeX 코드로의 변환 작업을 자동화하여 고품질의 출판 준비가 된 테이블을 시각적 입력으로부터 재구성하는 것을 목표로 한다.
- 2. 기존 방법들이 자주 실패하는 복잡한 테이블을 정확히 처리하는 것이 주요 과제로, 이를 해결하기 위해 강화된 다중 모달 대형 언어 모델(MLLM) 프레임워크를 제안한다.
- 3. LaTeX 코드의 구조적 보상과 렌더링된 출력으로부터 계산된 시각적 충실도 보상을 포함하는 이중 보상 강화 학습 전략을 도입하여 생성 품질을 향상시킨다.
- 4. TEDS-Structure와 CW-SSIM을 결합한 하이브리드 평가 프로토콜을 채택하여 구조적으로 복잡한 테이블에서 최첨단 성능을 달성했음을 보여준다.
- 5. 제안된 방법은 시각적 출력 품질의 직접적인 최적화를 가능하게 하여 접근 방식의 효과성과 견고성을 입증한다.


---

*Generated on 2025-09-23 23:02:14*