---
keywords:
  - Large Language Model
  - Cultural Awareness Evaluation
  - Multilingual Evaluation
  - Wikidata
  - Open-ended Text Generation
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2505.21693
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:59:42.479775",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cultural Awareness Evaluation",
    "Multilingual Evaluation",
    "Wikidata",
    "Open-ended Text Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cultural Awareness Evaluation": 0.78,
    "Multilingual Evaluation": 0.8,
    "Wikidata": 0.82,
    "Open-ended Text Generation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on evaluating cultural awareness across languages.",
        "novelty_score": 0.45,
        "connectivity_score": 0.89,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cultural Awareness Evaluation",
        "canonical": "Cultural Awareness Evaluation",
        "aliases": [
          "Cultural Evaluation"
        ],
        "category": "unique_technical",
        "rationale": "The concept is unique to the framework introduced in the paper, focusing on assessing cultural knowledge in LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multilingual Evaluation",
        "canonical": "Multilingual Evaluation",
        "aliases": [
          "Cross-lingual Evaluation"
        ],
        "category": "specific_connectable",
        "rationale": "Multilingual evaluation is a key aspect of the framework, linking to broader discussions on cross-lingual model performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Wikidata",
        "canonical": "Wikidata",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Wikidata is used as a cross-lingual anchor in the framework, crucial for linking cultural entities to structured knowledge.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.82
      },
      {
        "surface": "Open-ended Text Generation",
        "canonical": "Open-ended Text Generation",
        "aliases": [
          "Text Generation"
        ],
        "category": "unique_technical",
        "rationale": "This process is central to evaluating how models express culturally grounded knowledge.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "English-centric",
      "Translation Quality"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.89,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cultural Awareness Evaluation",
      "resolved_canonical": "Cultural Awareness Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multilingual Evaluation",
      "resolved_canonical": "Multilingual Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Wikidata",
      "resolved_canonical": "Wikidata",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Open-ended Text Generation",
      "resolved_canonical": "Open-ended Text Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.21693.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2505.21693](https://arxiv.org/abs/2505.21693)

## 🔗 유사한 논문
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (90.2% similar)
- [[2025-09-23/Fluent but Foreign_ Even Regional LLMs Lack Cultural Alignment_20250923|Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment]] (88.0% similar)
- [[2025-09-23/DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India_ Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context_20250923|DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context]] (86.9% similar)
- [[2025-09-22/A method for improving multilingual quality and diversity of instruction fine-tuning datasets_20250922|A method for improving multilingual quality and diversity of instruction fine-tuning datasets]] (86.7% similar)
- [[2025-09-22/MUG-Eval_ A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language_20250922|MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language]] (86.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multilingual Evaluation|Multilingual Evaluation]], [[keywords/Wikidata|Wikidata]]
**⚡ Unique Technical**: [[keywords/Cultural Awareness Evaluation|Cultural Awareness Evaluation]], [[keywords/Open-ended Text Generation|Open-ended Text Generation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.21693v3 Announce Type: replace 
Abstract: Large language models (LLMs) are used globally across many languages, but their English-centric pretraining raises concerns about cross-lingual disparities for cultural awareness, often resulting in biased outputs. However, comprehensive multilingual evaluation remains challenging due to limited benchmarks and questionable translation quality. To better assess these disparities, we introduce MAKIEval, an automatic multilingual framework for evaluating cultural awareness in LLMs across languages, regions, and topics. MAKIEval evaluates open-ended text generation, capturing how models express culturally grounded knowledge in natural language. Leveraging Wikidata's multilingual structure as a cross-lingual anchor, it automatically identifies cultural entities in model outputs and links them to structured knowledge, enabling scalable, language-agnostic evaluation without manual annotation or translation. We then introduce four metrics that capture complementary dimensions of cultural awareness: granularity, diversity, cultural specificity, and consensus across languages. We assess 7 LLMs developed from different parts of the world, encompassing both open-source and proprietary systems, across 13 languages, 19 countries and regions, and 6 culturally salient topics (e.g., food, clothing). Notably, we find that models tend to exhibit stronger cultural awareness in English, suggesting that English prompts more effectively activate culturally grounded knowledge.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 문화적 인식 차이를 평가하기 위한 자동 다국어 프레임워크인 MAKIEval을 소개합니다. LLM의 영어 중심 사전 학습이 문화적 편향을 초래할 수 있다는 문제를 해결하기 위해, 이 프레임워크는 위키데이터의 다국어 구조를 활용하여 모델 출력에서 문화적 요소를 자동으로 식별하고 구조화된 지식과 연결합니다. 이를 통해 수작업 주석이나 번역 없이 확장 가능하고 언어에 구애받지 않는 평가가 가능합니다. 또한, 문화적 인식을 평가하는 네 가지 지표(세분성, 다양성, 문화적 특수성, 언어 간 합의)를 도입하여 7개의 LLM을 13개 언어, 19개 국가 및 지역, 6개의 문화적 주제에서 평가했습니다. 연구 결과, 영어에서 문화적 인식이 더 강하게 나타나는 경향이 있음을 발견했습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)의 영어 중심 사전 학습은 문화적 인식의 언어 간 격차를 초래할 수 있으며, 이는 편향된 출력으로 이어질 수 있다.
- 2. MAKIEval은 LLM의 문화적 인식을 다국어로 평가하기 위한 자동화된 프레임워크로, 언어, 지역, 주제에 걸쳐 모델의 문화적 지식을 자연어로 표현하는 방식을 평가한다.
- 3. Wikidata의 다국어 구조를 활용하여 모델 출력에서 문화적 엔티티를 자동으로 식별하고 구조화된 지식과 연결함으로써, 수작업 주석이나 번역 없이 확장 가능하고 언어에 구애받지 않는 평가를 가능하게 한다.
- 4. 문화적 인식을 평가하기 위해 세분성, 다양성, 문화적 특수성, 언어 간 합의를 포착하는 네 가지 지표를 도입하였다.
- 5. 연구 결과, 모델들은 영어에서 더 강한 문화적 인식을 나타내는 경향이 있으며, 이는 영어가 문화적으로 기반이 된 지식을 더 효과적으로 활성화한다는 것을 시사한다.


---

*Generated on 2025-09-24 03:59:42*