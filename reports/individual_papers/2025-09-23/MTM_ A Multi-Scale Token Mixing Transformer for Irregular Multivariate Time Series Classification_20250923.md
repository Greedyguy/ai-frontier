---
keywords:
  - Transformer
  - Irregular Multivariate Time Series
  - Token Mixing
  - Attention Mechanism
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17809
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:57:23.513126",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Irregular Multivariate Time Series",
    "Token Mixing",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Irregular Multivariate Time Series": 0.8,
    "Token Mixing": 0.78,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in deep learning, relevant for understanding the MTM architecture.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Irregular Multivariate Time Series",
        "canonical": "Irregular Multivariate Time Series",
        "aliases": [
          "IMTS"
        ],
        "category": "unique_technical",
        "rationale": "IMTS is a specific problem domain addressed by the MTM model, crucial for understanding its application.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Token Mixing",
        "canonical": "Token Mixing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Token Mixing is a novel mechanism in MTM that enhances channel-wise learning, central to the paper's contribution.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are integral to the MTM model's ability to handle channel-wise asynchrony.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "classification",
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Irregular Multivariate Time Series",
      "resolved_canonical": "Irregular Multivariate Time Series",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Token Mixing",
      "resolved_canonical": "Token Mixing",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17809.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17809](https://arxiv.org/abs/2509.17809)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/TSGym_ Design Choices for Deep Multivariate Time-Series Forecasting_20250923|TSGym: Design Choices for Deep Multivariate Time-Series Forecasting]] (84.5% similar)
- [[2025-09-19/Improving Internet Traffic Matrix Prediction via Time Series Clustering_20250919|Improving Internet Traffic Matrix Prediction via Time Series Clustering]] (82.8% similar)
- [[2025-09-22/MTS-DMAE_ Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning_20250922|MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning]] (82.8% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (82.0% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Irregular Multivariate Time Series|Irregular Multivariate Time Series]], [[keywords/Token Mixing|Token Mixing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17809v1 Announce Type: new 
Abstract: Irregular multivariate time series (IMTS) is characterized by the lack of synchronized observations across its different channels. In this paper, we point out that this channel-wise asynchrony can lead to poor channel-wise modeling of existing deep learning methods. To overcome this limitation, we propose MTM, a multi-scale token mixing transformer for the classification of IMTS. We find that the channel-wise asynchrony can be alleviated by down-sampling the time series to coarser timescales, and propose to incorporate a masked concat pooling in MTM that gradually down-samples IMTS to enhance the channel-wise attention modules. Meanwhile, we propose a novel channel-wise token mixing mechanism which proactively chooses important tokens from one channel and mixes them with other channels, to further boost the channel-wise learning of our model. Through extensive experiments on real-world datasets and comparison with state-of-the-art methods, we demonstrate that MTM consistently achieves the best performance on all the benchmarks, with improvements of up to 3.8% in AUPRC for classification.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë¹„ë™ê¸°ì ì¸ ê´€ì¸¡ìœ¼ë¡œ íŠ¹ì§•ì§€ì–´ì§€ëŠ” ë¶ˆê·œì¹™ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´(IMTS)ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ MTMì´ë¼ëŠ” ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í† í° ë¯¹ì‹± íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë”¥ëŸ¬ë‹ ë°©ë²•ì˜ ì±„ë„ë³„ ëª¨ë¸ë§ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì‹œê³„ì—´ì„ ë” ê±°ì¹œ ì‹œê°„ ì²™ë„ë¡œ ë‹¤ìš´ìƒ˜í”Œë§í•˜ê³ , ë§ˆìŠ¤í‚¹ëœ ì»¨ìº£ í’€ë§ì„ ë„ì…í•˜ì—¬ ì±„ë„ë³„ ì£¼ì˜ ëª¨ë“ˆì„ ê°•í™”í•©ë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ ì±„ë„ë³„ í† í° ë¯¹ì‹± ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•œ ì±„ë„ì˜ ì¤‘ìš”í•œ í† í°ì„ ë‹¤ë¥¸ ì±„ë„ê³¼ í˜¼í•©í•˜ì—¬ í•™ìŠµ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ì˜ ë¹„êµ ì‹¤í—˜ì—ì„œ MTMì€ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©°, AUPRC ê¸°ì¤€ ìµœëŒ€ 3.8%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¶ˆê·œì¹™ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´(IMTS)ì€ ì±„ë„ ê°„ ë¹„ë™ê¸°í™”ëœ ê´€ì¸¡ìœ¼ë¡œ ì¸í•´ ê¸°ì¡´ ë”¥ëŸ¬ë‹ ë°©ë²•ì˜ ì±„ë„ë³„ ëª¨ë¸ë§ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤.
- 2. MTMì€ IMTS ë¶„ë¥˜ë¥¼ ìœ„í•œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í† í° ë¯¹ì‹± íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ, ì±„ë„ë³„ ë¹„ë™ê¸°í™”ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì‹œê³„ì—´ì„ ë” ê±°ì¹œ ì‹œê°„ ì²™ë„ë¡œ ë‹¤ìš´ìƒ˜í”Œë§í•œë‹¤.
- 3. MTMì€ ë§ˆìŠ¤í¬ë“œ ì½˜ìº£ í’€ë§ì„ ë„ì…í•˜ì—¬ IMTSë¥¼ ì ì§„ì ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§í•˜ê³  ì±„ë„ë³„ ì£¼ì˜ ëª¨ë“ˆì„ ê°•í™”í•œë‹¤.
- 4. ìƒˆë¡œìš´ ì±„ë„ë³„ í† í° ë¯¹ì‹± ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•œ ì±„ë„ì—ì„œ ì¤‘ìš”í•œ í† í°ì„ ì„ íƒí•˜ê³  ë‹¤ë¥¸ ì±„ë„ê³¼ í˜¼í•©í•˜ì—¬ ì±„ë„ë³„ í•™ìŠµì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 5. ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ì˜ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ MTMì´ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ë˜ê²Œ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, AUPRC ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ 3.8%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 01:57:23*