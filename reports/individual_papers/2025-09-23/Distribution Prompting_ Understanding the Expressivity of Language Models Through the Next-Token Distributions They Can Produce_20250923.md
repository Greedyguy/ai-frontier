---
keywords:
  - Large Language Model
  - Prompt Tuning
  - Entropy
  - Outlier Tokens
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2505.12244
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:55:00.169253",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Prompt Tuning",
    "Entropy",
    "Outlier Tokens"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Prompt Tuning": 0.78,
    "Entropy": 0.72,
    "Outlier Tokens": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Autoregressive neural language models",
        "canonical": "Large Language Model",
        "aliases": [
          "Autoregressive LMs",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Links to a fundamental concept in NLP and connects with other works on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "prompt tuning",
        "canonical": "Prompt Tuning",
        "aliases": [
          "Prompt Engineering"
        ],
        "category": "specific_connectable",
        "rationale": "A specific technique relevant to optimizing language model outputs, facilitating connections with related research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "entropy",
        "canonical": "Entropy",
        "aliases": [
          "Information Entropy"
        ],
        "category": "unique_technical",
        "rationale": "A unique measure used to understand distribution complexity, aiding in linking to statistical analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "outlier tokens",
        "canonical": "Outlier Tokens",
        "aliases": [
          "Anomalous Tokens"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific phenomenon in token distribution, useful for niche research connections.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "probability distribution",
      "target distribution",
      "next-token distribution"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Autoregressive neural language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "prompt tuning",
      "resolved_canonical": "Prompt Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "entropy",
      "resolved_canonical": "Entropy",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "outlier tokens",
      "resolved_canonical": "Outlier Tokens",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.12244.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2505.12244](https://arxiv.org/abs/2505.12244)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Learning to vary_ Teaching LMs to reproduce human linguistic variability in next-word prediction_20250923|Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction]] (83.8% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (83.1% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (83.0% similar)
- [[2025-09-23/CIE_ Controlling Language Model Text Generations Using Continuous Signals_20250923|CIE: Controlling Language Model Text Generations Using Continuous Signals]] (82.8% similar)
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Prompt Tuning|Prompt Tuning]]
**âš¡ Unique Technical**: [[keywords/Entropy|Entropy]], [[keywords/Outlier Tokens|Outlier Tokens]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.12244v2 Announce Type: replace 
Abstract: Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce, showing that some distributions are significantly harder to elicit than others. Specifically, for any target next-token distribution over the vocabulary, we attempt to find a prompt that induces the LM to output a distribution as close as possible to the target, using either soft or hard gradient-based prompt tuning. We find that (1) in general, distributions with very low or very high entropy are easier to approximate than those with moderate entropy; (2) among distributions with the same entropy, those containing ''outlier tokens'' are easier to approximate; (3) target distributions generated by LMs -- even LMs with different tokenizers -- are easier to approximate than randomly chosen targets. These results offer insights into the expressiveness of LMs and the challenges of using them as probability distribution proposers.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìê¸°íšŒê·€ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸(LM)ì´ ìƒì„±í•  ìˆ˜ ìˆëŠ” í™•ë¥  ë¶„í¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì´í•´í•˜ë ¤ëŠ” ì‹œë„ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” ì£¼ì–´ì§„ ëª©í‘œ ë‹¤ìŒ í† í° ë¶„í¬ì— ìµœëŒ€í•œ ê·¼ì ‘í•œ ë¶„í¬ë¥¼ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ê¸° ìœ„í•´ ì†Œí”„íŠ¸ ë˜ëŠ” í•˜ë“œ ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ íŠœë‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ ì‚¬í•­ìœ¼ë¡œëŠ” (1) ë§¤ìš° ë‚®ê±°ë‚˜ ë†’ì€ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§„ ë¶„í¬ê°€ ì¤‘ê°„ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§„ ë¶„í¬ë³´ë‹¤ ê·¼ì‚¬í•˜ê¸° ì‰½ë‹¤ëŠ” ì , (2) ë™ì¼í•œ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§„ ë¶„í¬ ì¤‘ 'ì´ìƒì¹˜ í† í°'ì„ í¬í•¨í•œ ë¶„í¬ê°€ ë” ê·¼ì‚¬í•˜ê¸° ì‰½ë‹¤ëŠ” ì , (3) LMì´ ìƒì„±í•œ ëª©í‘œ ë¶„í¬ê°€ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ëª©í‘œë³´ë‹¤ ê·¼ì‚¬í•˜ê¸° ì‰½ë‹¤ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LMì˜ í‘œí˜„ë ¥ê³¼ í™•ë¥  ë¶„í¬ ì œì•ˆìë¡œì„œì˜ ì‚¬ìš©ì— ëŒ€í•œ ë„ì „ ê³¼ì œë¥¼ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìíšŒê·€ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì€ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±í•œë‹¤.
- 2. ë§¤ìš° ë‚®ê±°ë‚˜ ë†’ì€ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§„ ë¶„í¬ëŠ” ì¤‘ê°„ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§„ ë¶„í¬ë³´ë‹¤ ê·¼ì‚¬í•˜ê¸° ì‰½ë‹¤.
- 3. ê°™ì€ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì§„ ë¶„í¬ ì¤‘ 'ì´ìƒì¹˜ í† í°'ì„ í¬í•¨í•œ ë¶„í¬ëŠ” ê·¼ì‚¬í•˜ê¸° ì‰½ë‹¤.
- 4. LMsê°€ ìƒì„±í•œ ëª©í‘œ ë¶„í¬ëŠ” ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ëª©í‘œë³´ë‹¤ ê·¼ì‚¬í•˜ê¸° ì‰½ë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” LMsì˜ í‘œí˜„ë ¥ê³¼ í™•ë¥  ë¶„í¬ ì œì•ˆìë¡œì„œì˜ ì‚¬ìš©ì— ëŒ€í•œ ë„ì „ ê³¼ì œë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤.


---

*Generated on 2025-09-24 03:55:00*