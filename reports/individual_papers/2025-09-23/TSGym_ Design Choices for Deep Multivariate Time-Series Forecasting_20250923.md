---
keywords:
  - Multivariate Time Series Forecasting
  - Attention Mechanism
  - Large Language Model
  - Time-series Foundation Model
  - Automated Model Construction
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17063
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:46:38.701134",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multivariate Time Series Forecasting",
    "Attention Mechanism",
    "Large Language Model",
    "Time-series Foundation Model",
    "Automated Model Construction"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multivariate Time Series Forecasting": 0.78,
    "Attention Mechanism": 0.8,
    "Large Language Model": 0.82,
    "Time-series Foundation Model": 0.77,
    "Automated Model Construction": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Multivariate Time Series Forecasting",
        "canonical": "Multivariate Time Series Forecasting",
        "aliases": [
          "MTSF"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized area of forecasting that benefits from deep learning advancements, making it a unique technical concept.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Attention Modules",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Modules"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial for enhancing model performance in time series forecasting, linking to broader machine learning concepts.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are increasingly being applied beyond NLP, including time series forecasting, highlighting their broad applicability.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Time-series Foundation Models",
        "canonical": "Time-series Foundation Model",
        "aliases": [
          "TS Foundation Models"
        ],
        "category": "unique_technical",
        "rationale": "This represents a novel approach in the field of time series analysis, offering a new avenue for research and application.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Automated Model Construction",
        "canonical": "Automated Model Construction",
        "aliases": [
          "Auto Model Construction"
        ],
        "category": "unique_technical",
        "rationale": "This process is pivotal for creating adaptable models, enhancing transferability and robustness in time series forecasting.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "hyperparameter tuning",
      "neural architecture searching",
      "fixed model selection"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Multivariate Time Series Forecasting",
      "resolved_canonical": "Multivariate Time Series Forecasting",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Attention Modules",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Time-series Foundation Models",
      "resolved_canonical": "Time-series Foundation Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Automated Model Construction",
      "resolved_canonical": "Automated Model Construction",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# TSGym: Design Choices for Deep Multivariate Time-Series Forecasting

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17063.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17063](https://arxiv.org/abs/2509.17063)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Less is More_ Unlocking Specialization of Time Series Foundation Models via Structured Pruning_20250923|Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning]] (82.9% similar)
- [[2025-09-18/Super-Linear_ A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting_20250918|Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting]] (82.6% similar)
- [[2025-09-22/Tsururu_ A Python-based Time Series Forecasting Strategies Library_20250922|Tsururu: A Python-based Time Series Forecasting Strategies Library]] (82.5% similar)
- [[2025-09-22/MTS-DMAE_ Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning_20250922|MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning]] (82.3% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Multivariate Time Series Forecasting|Multivariate Time Series Forecasting]], [[keywords/Time-series Foundation Model|Time-series Foundation Model]], [[keywords/Automated Model Construction|Automated Model Construction]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17063v1 Announce Type: new 
Abstract: Recently, deep learning has driven significant advancements in multivariate time series forecasting (MTSF) tasks. However, much of the current research in MTSF tends to evaluate models from a holistic perspective, which obscures the individual contributions and leaves critical issues unaddressed. Adhering to the current modeling paradigms, this work bridges these gaps by systematically decomposing deep MTSF methods into their core, fine-grained components like series-patching tokenization, channel-independent strategy, attention modules, or even Large Language Models and Time-series Foundation Models. Through extensive experiments and component-level analysis, our work offers more profound insights than previous benchmarks that typically discuss models as a whole.
  Furthermore, we propose a novel automated solution called TSGym for MTSF tasks. Unlike traditional hyperparameter tuning, neural architecture searching or fixed model selection, TSGym performs fine-grained component selection and automated model construction, which enables the creation of more effective solutions tailored to diverse time series data, therefore enhancing model transferability across different data sources and robustness against distribution shifts. Extensive experiments indicate that TSGym significantly outperforms existing state-of-the-art MTSF and AutoML methods. All code is publicly available on https://github.com/SUFE-AILAB/TSGym.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì˜ˆì¸¡(MTSF)ì—ì„œ ì‹¬ì¸µ í•™ìŠµì˜ ë°œì „ì„ ë‹¤ë£¨ë©°, ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ MTSF ë°©ë²•ì„ ì„¸ë¶„í™”ëœ êµ¬ì„± ìš”ì†Œë¡œ ë¶„í•´í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” ì‹œê³„ì—´ íŒ¨ì¹˜ í† í°í™”, ì±„ë„ ë…ë¦½ ì „ëµ, ì£¼ì˜ ëª¨ë“ˆ ë“±ì„ í¬í•¨í•œ êµ¬ì„± ìš”ì†Œ ìˆ˜ì¤€ì˜ ë¶„ì„ì„ í†µí•´ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë³´ë‹¤ ê¹Šì´ ìˆëŠ” í†µì°°ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ ìë™í™” ì†”ë£¨ì…˜ì¸ TSGymì„ ì œì•ˆí•˜ì—¬, ì„¸ë¶„í™”ëœ êµ¬ì„± ìš”ì†Œ ì„ íƒê³¼ ìë™ ëª¨ë¸ êµ¬ì¶•ì„ í†µí•´ ë‹¤ì–‘í•œ ì‹œê³„ì—´ ë°ì´í„°ì— ë§ì¶¤í˜• ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TSGymì€ ê¸°ì¡´ ìµœì‹  MTSF ë° AutoML ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì½”ë“œì™€ ê´€ë ¨ ìë£ŒëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ ì‹¬ì¸µ í•™ìŠµì€ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì˜ˆì¸¡(MTSF) ì‘ì—…ì—ì„œ ìƒë‹¹í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ì‹¬ì¸µ MTSF ë°©ë²•ì„ ì‹œë¦¬ì¦ˆ íŒ¨ì¹­ í† í°í™”, ì±„ë„ ë…ë¦½ ì „ëµ, ì£¼ì˜ ëª¨ë“ˆ ë“± ì„¸ë¶€ êµ¬ì„± ìš”ì†Œë¡œ ì²´ê³„ì ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.
- 3. TSGymì´ë¼ëŠ” ìƒˆë¡œìš´ ìë™í™” ì†”ë£¨ì…˜ì„ ì œì•ˆí•˜ì—¬, ì„¸ë°€í•œ êµ¬ì„± ìš”ì†Œ ì„ íƒê³¼ ìë™í™”ëœ ëª¨ë¸ êµ¬ì¶•ì„ í†µí•´ ë‹¤ì–‘í•œ ì‹œê³„ì—´ ë°ì´í„°ì— ë§ì¶˜ íš¨ê³¼ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. TSGymì€ ê¸°ì¡´ ìµœì²¨ë‹¨ MTSF ë° AutoML ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨ì„ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.
- 5. TSGymì˜ ëª¨ë“  ì½”ë“œëŠ” https://github.com/SUFE-AILAB/TSGymì—ì„œ ê³µê°œì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:46:38*