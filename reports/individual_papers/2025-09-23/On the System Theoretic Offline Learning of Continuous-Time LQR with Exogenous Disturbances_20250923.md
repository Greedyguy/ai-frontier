---
keywords:
  - Linear Quadratic Regulator
  - Adaptive Dynamic Programming
  - Lyapunov Method
  - Markov Decision Process
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16746
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:15:09.130525",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Linear Quadratic Regulator",
    "Adaptive Dynamic Programming",
    "Lyapunov Method",
    "Markov Decision Process"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Linear Quadratic Regulator": 0.85,
    "Adaptive Dynamic Programming": 0.82,
    "Lyapunov Method": 0.78,
    "Markov Decision Process": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "linear quadratic regulator",
        "canonical": "Linear Quadratic Regulator",
        "aliases": [
          "LQR"
        ],
        "category": "specific_connectable",
        "rationale": "LQR is a fundamental concept in control theory, providing strong links to related adaptive control and optimization techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "adaptive dynamic programming",
        "canonical": "Adaptive Dynamic Programming",
        "aliases": [
          "ADP"
        ],
        "category": "specific_connectable",
        "rationale": "ADP is a key method in reinforcement learning, connecting to broader machine learning frameworks.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Lyapunov-based analytical methodology",
        "canonical": "Lyapunov Method",
        "aliases": [
          "Lyapunov Analysis"
        ],
        "category": "specific_connectable",
        "rationale": "Lyapunov methods are crucial for stability analysis in control systems, linking to theoretical and practical applications.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Markov decision process",
        "canonical": "Markov Decision Process",
        "aliases": [
          "MDP"
        ],
        "category": "broad_technical",
        "rationale": "MDPs are foundational in decision-making models, connecting to various stochastic and dynamic programming approaches.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "offline designs",
      "controlled environment",
      "numerical experiments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "linear quadratic regulator",
      "resolved_canonical": "Linear Quadratic Regulator",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "adaptive dynamic programming",
      "resolved_canonical": "Adaptive Dynamic Programming",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Lyapunov-based analytical methodology",
      "resolved_canonical": "Lyapunov Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Markov decision process",
      "resolved_canonical": "Markov Decision Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# On the System Theoretic Offline Learning of Continuous-Time LQR with Exogenous Disturbances

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16746.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16746](https://arxiv.org/abs/2509.16746)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Safe Guaranteed Dynamics Exploration with Probabilistic Models_20250923|Safe Guaranteed Dynamics Exploration with Probabilistic Models]] (81.8% similar)
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (81.0% similar)
- [[2025-09-22/Online Robust Planning under Model Uncertainty_ A Sample-Based Approach_20250922|Online Robust Planning under Model Uncertainty: A Sample-Based Approach]] (80.1% similar)
- [[2025-09-19/Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (79.7% similar)
- [[2025-09-23/Control Disturbance Rejection in Neural ODEs_20250923|Control Disturbance Rejection in Neural ODEs]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Markov Decision Process|Markov Decision Process]]
**ğŸ”— Specific Connectable**: [[keywords/Linear Quadratic Regulator|Linear Quadratic Regulator]], [[keywords/Adaptive Dynamic Programming|Adaptive Dynamic Programming]], [[keywords/Lyapunov Method|Lyapunov Method]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16746v1 Announce Type: cross 
Abstract: We analyze offline designs of linear quadratic regulator (LQR) strategies with uncertain disturbances. First, we consider the scenario where the exogenous variable can be estimated in a controlled environment, and subsequently, consider a more practical and challenging scenario where it is unknown in a stochastic setting. Our approach builds on the fundamental learning-based framework of adaptive dynamic programming (ADP), combined with a Lyapunov-based analytical methodology to design the algorithms and derive sample-based approximations motivated from the Markov decision process (MDP)-based approaches. For the scenario involving non-measurable disturbances, we further establish stability and convergence guarantees for the learned control gains under sample-based approximations. The overall methodology emphasizes simplicity while providing rigorous guarantees. Finally, numerical experiments focus on the intricacies and validations for the design of offline continuous-time LQR with exogenous disturbances.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¶ˆí™•ì‹¤í•œ ì™¸ë€ì´ ìˆëŠ” ì„ í˜• ì´ì°¨ ì¡°ì ˆê¸°(LQR) ì „ëµì˜ ì˜¤í”„ë¼ì¸ ì„¤ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ë¨¼ì €, ì™¸ìƒ ë³€ìˆ˜ë¥¼ ì œì–´ëœ í™˜ê²½ì—ì„œ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ê²½ìš°ë¥¼ ë‹¤ë£¨ê³ , ì´í›„ í™•ë¥ ì  í™˜ê²½ì—ì„œ ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì ì‘í˜• ë™ì  í”„ë¡œê·¸ë˜ë°(ADP)ê³¼ Lyapunov ê¸°ë°˜ ë¶„ì„ ë°©ë²•ë¡ ì„ ê²°í•©í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ê³„í•˜ê³ , ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP) ê¸°ë°˜ ì ‘ê·¼ë²•ì—ì„œ ì˜ê°ì„ ë°›ì€ ìƒ˜í”Œ ê¸°ë°˜ ê·¼ì‚¬ì¹˜ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤. ì¸¡ì • ë¶ˆê°€ëŠ¥í•œ ì™¸ë€ì´ ìˆëŠ” ê²½ìš°, í•™ìŠµëœ ì œì–´ ì´ë“ì˜ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì´ ë°©ë²•ë¡ ì€ ë‹¨ìˆœì„±ì„ ê°•ì¡°í•˜ë©´ì„œë„ ì—„ê²©í•œ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì™¸ìƒ ì™¸ë€ì´ ìˆëŠ” ì˜¤í”„ë¼ì¸ ì—°ì† ì‹œê°„ LQR ì„¤ê³„ì˜ ë³µì¡ì„±ê³¼ ê²€ì¦ì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¶ˆí™•ì‹¤í•œ ì™¸ë€ì´ ìˆëŠ” ì„ í˜• ì´ì°¨ ì¡°ì ˆê¸°(LQR) ì „ëµì˜ ì˜¤í”„ë¼ì¸ ì„¤ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- 2. ì œì–´ëœ í™˜ê²½ì—ì„œ ì™¸ìƒ ë³€ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì™€ í™•ë¥ ì  ì„¤ì •ì—ì„œ ì™¸ìƒ ë³€ìˆ˜ê°€ ì•Œë ¤ì§€ì§€ ì•Šì€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
- 3. ì ì‘í˜• ë™ì  í”„ë¡œê·¸ë˜ë°(ADP)ê³¼ Lyapunov ê¸°ë°˜ ë¶„ì„ ë°©ë²•ë¡ ì„ ê²°í•©í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ê³„í•˜ê³ , ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP) ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì—ì„œ ì˜ê°ì„ ì–»ì€ ìƒ˜í”Œ ê¸°ë°˜ ê·¼ì‚¬ì¹˜ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
- 4. ë¹„ì¸¡ì • ê°€ëŠ¥í•œ ì™¸ë€ì´ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ í•™ìŠµëœ ì œì–´ ì´ë“ì˜ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 5. ì˜¤í”„ë¼ì¸ ì—°ì† ì‹œê°„ LQR ì„¤ê³„ì˜ ë³µì¡ì„±ê³¼ ê²€ì¦ì„ ìœ„í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:15:09*