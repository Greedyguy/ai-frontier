---
keywords:
  - Vision-Language Model
  - Entity-centric Multimodal Preference Optimization
  - Hallucinations in AI
  - Modality Alignment
  - Preference Alignment
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2506.04039
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:05:01.834515",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Entity-centric Multimodal Preference Optimization",
    "Hallucinations in AI",
    "Modality Alignment",
    "Preference Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.9,
    "Entity-centric Multimodal Preference Optimization": 0.85,
    "Hallucinations in AI": 0.82,
    "Modality Alignment": 0.79,
    "Preference Alignment": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Visual Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a key focus of the paper, addressing modality alignment issues.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.9
      },
      {
        "surface": "Entity-centric Multimodal Preference Optimization",
        "canonical": "Entity-centric Multimodal Preference Optimization",
        "aliases": [
          "EMPO"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel method proposed in the paper to improve modality alignment and reduce hallucinations.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.91,
        "link_intent_score": 0.85
      },
      {
        "surface": "hallucinations",
        "canonical": "Hallucinations in AI",
        "aliases": [
          "AI hallucinations"
        ],
        "category": "specific_connectable",
        "rationale": "Addressing hallucinations is central to the paper's objectives, linking to broader AI reliability concerns.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "modality alignment",
        "canonical": "Modality Alignment",
        "aliases": [
          "image-text alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Modality alignment is crucial for reducing hallucinations in multimodal models.",
        "novelty_score": 0.48,
        "connectivity_score": 0.82,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      },
      {
        "surface": "preference alignment",
        "canonical": "Preference Alignment",
        "aliases": [
          "human preference alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Preference alignment is a key technique discussed for aligning model responses with human expectations.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Visual Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Entity-centric Multimodal Preference Optimization",
      "resolved_canonical": "Entity-centric Multimodal Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.91,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "hallucinations",
      "resolved_canonical": "Hallucinations in AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "modality alignment",
      "resolved_canonical": "Modality Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.82,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "preference alignment",
      "resolved_canonical": "Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.04039.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2506.04039](https://arxiv.org/abs/2506.04039)

## 🔗 유사한 논문
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (85.2% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (84.7% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.4% similar)
- [[2025-09-23/LifeAlign_ Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization_20250923|LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization]] (83.9% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (83.7% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Hallucinations in AI|Hallucinations in AI]], [[keywords/Modality Alignment|Modality Alignment]], [[keywords/Preference Alignment|Preference Alignment]]
**⚡ Unique Technical**: [[keywords/Entity-centric Multimodal Preference Optimization|Entity-centric Multimodal Preference Optimization]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.04039v2 Announce Type: replace-cross 
Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive capabilities across multiple tasks. However, their trustworthiness is often challenged by hallucinations, which can be attributed to the modality misalignment and the inherent hallucinations of their underlying Large Language Models (LLMs) backbone. Existing preference alignment methods focus on aligning model responses with human preferences while neglecting image-text modality alignment, resulting in over-reliance on LLMs and hallucinations. In this paper, we propose Entity-centric Multimodal Preference Optimization (EMPO), which achieves enhanced modality alignment compared to existing human preference alignment methods. Besides, to overcome the scarcity of high-quality multimodal preference data, we utilize open-source instruction datasets to automatically construct high-quality preference data across three aspects: image, instruction, and response. Experiments on two human preference datasets and five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO, e.g., reducing hallucination rates by 85.9\% on Object-HalBench and 49.8\% on MM-HalBench.

## 📝 요약

이 논문은 대형 시각 언어 모델(LVLM)의 신뢰성을 높이기 위해 엔티티 중심의 다중 모달 선호 최적화(EMPO)를 제안합니다. 기존 방법들은 인간의 선호에 맞춰 모델 응답을 조정하지만 이미지-텍스트 모달 정렬을 간과하여 환각 현상을 초래합니다. EMPO는 이러한 문제를 해결하기 위해 고품질의 다중 모달 선호 데이터를 생성하고, 실험 결과 환각률을 크게 줄이는 데 성공했습니다. 예를 들어, Object-HalBench에서 85.9%, MM-HalBench에서 49.8%의 환각률 감소를 달성했습니다.

## 🎯 주요 포인트

- 1. 대형 시각 언어 모델(LVLMs)은 여러 작업에서 뛰어난 성능을 보이지만, 환각 문제로 인해 신뢰성이 도전받고 있습니다.
- 2. 기존의 선호도 정렬 방법은 이미지-텍스트 모달리티 정렬을 간과하여 LLMs에 과도하게 의존하게 되고 환각을 유발합니다.
- 3. 본 논문에서는 기존의 인간 선호도 정렬 방법보다 향상된 모달리티 정렬을 달성하는 엔티티 중심 다중 모달 선호도 최적화(EMPO)를 제안합니다.
- 4. 고품질 다중 모달 선호도 데이터의 부족을 극복하기 위해, 오픈 소스 지시 데이터셋을 활용하여 이미지, 지시, 응답의 세 가지 측면에서 고품질 선호도 데이터를 자동으로 구축합니다.
- 5. 두 개의 인간 선호도 데이터셋과 다섯 개의 다중 모달 환각 벤치마크 실험에서 EMPO의 효과가 입증되었으며, 예를 들어 Object-HalBench에서 환각률을 85.9%, MM-HalBench에서 49.8% 감소시켰습니다.


---

*Generated on 2025-09-24 01:05:01*