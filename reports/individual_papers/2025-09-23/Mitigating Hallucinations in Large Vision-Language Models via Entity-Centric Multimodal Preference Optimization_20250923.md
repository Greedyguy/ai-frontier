---
keywords:
  - Vision-Language Model
  - Entity-centric Multimodal Preference Optimization
  - Hallucinations in AI
  - Modality Alignment
  - Preference Alignment
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2506.04039
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:05:01.834515",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Entity-centric Multimodal Preference Optimization",
    "Hallucinations in AI",
    "Modality Alignment",
    "Preference Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.9,
    "Entity-centric Multimodal Preference Optimization": 0.85,
    "Hallucinations in AI": 0.82,
    "Modality Alignment": 0.79,
    "Preference Alignment": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Visual Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a key focus of the paper, addressing modality alignment issues.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.9
      },
      {
        "surface": "Entity-centric Multimodal Preference Optimization",
        "canonical": "Entity-centric Multimodal Preference Optimization",
        "aliases": [
          "EMPO"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel method proposed in the paper to improve modality alignment and reduce hallucinations.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.91,
        "link_intent_score": 0.85
      },
      {
        "surface": "hallucinations",
        "canonical": "Hallucinations in AI",
        "aliases": [
          "AI hallucinations"
        ],
        "category": "specific_connectable",
        "rationale": "Addressing hallucinations is central to the paper's objectives, linking to broader AI reliability concerns.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "modality alignment",
        "canonical": "Modality Alignment",
        "aliases": [
          "image-text alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Modality alignment is crucial for reducing hallucinations in multimodal models.",
        "novelty_score": 0.48,
        "connectivity_score": 0.82,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      },
      {
        "surface": "preference alignment",
        "canonical": "Preference Alignment",
        "aliases": [
          "human preference alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Preference alignment is a key technique discussed for aligning model responses with human expectations.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Visual Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Entity-centric Multimodal Preference Optimization",
      "resolved_canonical": "Entity-centric Multimodal Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.91,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "hallucinations",
      "resolved_canonical": "Hallucinations in AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "modality alignment",
      "resolved_canonical": "Modality Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.82,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "preference alignment",
      "resolved_canonical": "Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.04039.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2506.04039](https://arxiv.org/abs/2506.04039)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (85.2% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (84.7% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.4% similar)
- [[2025-09-23/LifeAlign_ Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization_20250923|LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization]] (83.9% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (83.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Hallucinations in AI|Hallucinations in AI]], [[keywords/Modality Alignment|Modality Alignment]], [[keywords/Preference Alignment|Preference Alignment]]
**âš¡ Unique Technical**: [[keywords/Entity-centric Multimodal Preference Optimization|Entity-centric Multimodal Preference Optimization]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.04039v2 Announce Type: replace-cross 
Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive capabilities across multiple tasks. However, their trustworthiness is often challenged by hallucinations, which can be attributed to the modality misalignment and the inherent hallucinations of their underlying Large Language Models (LLMs) backbone. Existing preference alignment methods focus on aligning model responses with human preferences while neglecting image-text modality alignment, resulting in over-reliance on LLMs and hallucinations. In this paper, we propose Entity-centric Multimodal Preference Optimization (EMPO), which achieves enhanced modality alignment compared to existing human preference alignment methods. Besides, to overcome the scarcity of high-quality multimodal preference data, we utilize open-source instruction datasets to automatically construct high-quality preference data across three aspects: image, instruction, and response. Experiments on two human preference datasets and five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO, e.g., reducing hallucination rates by 85.9\% on Object-HalBench and 49.8\% on MM-HalBench.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì‹œê° ì–¸ì–´ ëª¨ë¸(LVLM)ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì—”í‹°í‹° ì¤‘ì‹¬ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ì„ í˜¸ ìµœì í™”(EMPO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶° ëª¨ë¸ ì‘ë‹µì„ ì¡°ì •í•˜ì§€ë§Œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ëª¨ë‹¬ ì •ë ¬ì„ ê°„ê³¼í•˜ì—¬ í™˜ê° í˜„ìƒì„ ì´ˆë˜í•©ë‹ˆë‹¤. EMPOëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ í’ˆì§ˆì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ì„ í˜¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ì‹¤í—˜ ê²°ê³¼ í™˜ê°ë¥ ì„ í¬ê²Œ ì¤„ì´ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Object-HalBenchì—ì„œ 85.9%, MM-HalBenchì—ì„œ 49.8%ì˜ í™˜ê°ë¥  ê°ì†Œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì‹œê° ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, í™˜ê° ë¬¸ì œë¡œ ì¸í•´ ì‹ ë¢°ì„±ì´ ë„ì „ë°›ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì„ í˜¸ë„ ì •ë ¬ ë°©ë²•ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹° ì •ë ¬ì„ ê°„ê³¼í•˜ì—¬ LLMsì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ê²Œ ë˜ê³  í™˜ê°ì„ ìœ ë°œí•©ë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ì˜ ì¸ê°„ ì„ í˜¸ë„ ì •ë ¬ ë°©ë²•ë³´ë‹¤ í–¥ìƒëœ ëª¨ë‹¬ë¦¬í‹° ì •ë ¬ì„ ë‹¬ì„±í•˜ëŠ” ì—”í‹°í‹° ì¤‘ì‹¬ ë‹¤ì¤‘ ëª¨ë‹¬ ì„ í˜¸ë„ ìµœì í™”(EMPO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ê³ í’ˆì§ˆ ë‹¤ì¤‘ ëª¨ë‹¬ ì„ í˜¸ë„ ë°ì´í„°ì˜ ë¶€ì¡±ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì˜¤í”ˆ ì†ŒìŠ¤ ì§€ì‹œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€, ì§€ì‹œ, ì‘ë‹µì˜ ì„¸ ê°€ì§€ ì¸¡ë©´ì—ì„œ ê³ í’ˆì§ˆ ì„ í˜¸ë„ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.
- 5. ë‘ ê°œì˜ ì¸ê°„ ì„ í˜¸ë„ ë°ì´í„°ì…‹ê³¼ ë‹¤ì„¯ ê°œì˜ ë‹¤ì¤‘ ëª¨ë‹¬ í™˜ê° ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ EMPOì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìœ¼ë©°, ì˜ˆë¥¼ ë“¤ì–´ Object-HalBenchì—ì„œ í™˜ê°ë¥ ì„ 85.9%, MM-HalBenchì—ì„œ 49.8% ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:05:01*