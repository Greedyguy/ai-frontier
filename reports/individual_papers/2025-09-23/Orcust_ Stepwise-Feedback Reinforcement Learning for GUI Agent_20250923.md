---
keywords:
  - Principle-Constrained Reward Modeling
  - Online VM-Grounded Trajectory Construction
  - Large Language Model
  - ScreenSpot
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17917
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:04:10.664108",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Principle-Constrained Reward Modeling",
    "Online VM-Grounded Trajectory Construction",
    "Large Language Model",
    "ScreenSpot"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Principle-Constrained Reward Modeling": 0.78,
    "Online VM-Grounded Trajectory Construction": 0.75,
    "Large Language Model": 0.8,
    "ScreenSpot": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Principle-Constrained Reward Modeling",
        "canonical": "Principle-Constrained Reward Modeling",
        "aliases": [
          "PCRM"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's novel approach to enhancing GUI agent performance and is not covered by existing canonical terms.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Online VM-Grounded Trajectory Construction",
        "canonical": "Online VM-Grounded Trajectory Construction",
        "aliases": [
          "OVTC"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique method introduced in the paper for generating GUI interaction trajectories, which is crucial for understanding the paper's contribution.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are integral to the paper's approach for deriving principles and enhancing reward modeling.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "ScreenSpot",
        "canonical": "ScreenSpot",
        "aliases": [
          "ScreenSpot-Pro"
        ],
        "category": "unique_technical",
        "rationale": "ScreenSpot is a benchmark used to demonstrate the effectiveness of the proposed framework, highlighting its practical application.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "reward signals",
      "task execution"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Principle-Constrained Reward Modeling",
      "resolved_canonical": "Principle-Constrained Reward Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Online VM-Grounded Trajectory Construction",
      "resolved_canonical": "Online VM-Grounded Trajectory Construction",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "ScreenSpot",
      "resolved_canonical": "ScreenSpot",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17917.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17917](https://arxiv.org/abs/2509.17917)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/GUI-ARP_ Enhancing Grounding with Adaptive Region Perception for GUI Agents_20250922|GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents]] (86.4% similar)
- [[2025-09-22/GUI-ReWalk_ Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning_20250922|GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning]] (84.7% similar)
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (83.3% similar)
- [[2025-09-22/BTL-UI_ Blink-Think-Link Reasoning Model for GUI Agent_20250922|BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent]] (81.2% similar)
- [[2025-09-18/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250918|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (80.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Principle-Constrained Reward Modeling|Principle-Constrained Reward Modeling]], [[keywords/Online VM-Grounded Trajectory Construction|Online VM-Grounded Trajectory Construction]], [[keywords/ScreenSpot|ScreenSpot]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17917v1 Announce Type: new 
Abstract: Recent advances in GUI agents have achieved remarkable grounding and action-prediction performance, yet existing models struggle with unreliable reward signals and limited online trajectory generation. In this paper, we introduce Orcust, a framework that integrates Principle-Constrained Reward Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to enhance reasoning reliability and data efficiency in interactive GUI tasks. We leverages environment-verifiable and LLM-derived principle to enforce interpretable reward signals that constrain long chain-of-thought reasoning and rule-based feedback. OVTC spins up instrumented virtual machines to autonomously collect structured GUI interaction trajectories with explicit procedural and structural objectives, enabling the training of a stepwise reward model that robustly captures human preferences and adheres to task-specific constraints. Extensive experiments on standard GUI benchmarks covering perceptual grounding, foundational operations, and end-to-end task execution reveal that Orcust achieves state-of-the-art performance, improving by 22.2\% on ScreenSpot and 23.9\% on ScreenSpot-Pro over the base model (i.e. Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the reasoning, adaptability and scalability of GUI agents across various environments and task complexities.

## ğŸ“ ìš”ì•½

OrcustëŠ” GUI ì—ì´ì „íŠ¸ì˜ ì‹ ë¢°ì„±ê³¼ ë°ì´í„° íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ Principle-Constrained Reward Modeling(PCRM)ê³¼ Online VM-Grounded Trajectory Construction(OVTC)ì„ í†µí•©í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ í•´ì„ ê°€ëŠ¥í•œ ë³´ìƒ ì‹ í˜¸ë¥¼ í†µí•´ ë³µì¡í•œ ì¶”ë¡ ê³¼ ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°±ì„ ê°•í™”í•˜ë©°, ê°€ìƒ ë¨¸ì‹ ì„ í™œìš©í•´ êµ¬ì¡°í™”ëœ GUI ìƒí˜¸ì‘ìš© ê²½ë¡œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¸ê°„ì˜ ì„ í˜¸ë¥¼ ë°˜ì˜í•˜ê³  ê³¼ì œë³„ ì œì•½ì„ ì¤€ìˆ˜í•˜ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, OrcustëŠ” ScreenSpotê³¼ ScreenSpot-Proì—ì„œ ê°ê° 22.2%ì™€ 23.9% ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬, ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ê³¼ì œ ë³µì¡ì„±ì—ì„œ GUI ì—ì´ì „íŠ¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ê³¼ ì ì‘ì„±, í™•ì¥ì„±ì„ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. OrcustëŠ” Principle-Constrained Reward Modeling(PCRM)ê³¼ Online VM-Grounded Trajectory Construction(OVTC)ì„ í†µí•©í•˜ì—¬ GUI ì‘ì—…ì—ì„œ ì¶”ë¡  ì‹ ë¢°ì„±ê³¼ ë°ì´í„° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. í™˜ê²½ ê²€ì¦ ê°€ëŠ¥í•˜ê³  LLMì—ì„œ íŒŒìƒëœ ì›ì¹™ì„ í™œìš©í•˜ì—¬ í•´ì„ ê°€ëŠ¥í•œ ë³´ìƒ ì‹ í˜¸ë¥¼ ê°•í™”í•˜ê³ , ì¥ê¸°ì ì¸ ì‚¬ê³  ê³¼ì •ì„ ì œí•œí•˜ë©° ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 3. OVTCëŠ” ê°€ìƒ ë¨¸ì‹ ì„ í™œìš©í•˜ì—¬ ëª…ì‹œì ì¸ ì ˆì°¨ì  ë° êµ¬ì¡°ì  ëª©í‘œë¥¼ ê°€ì§„ GUI ìƒí˜¸ì‘ìš© ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì¸ê°„ì˜ ì„ í˜¸ë¥¼ í¬ì°©í•˜ê³  ì‘ì—…ë³„ ì œì•½ì„ ì¤€ìˆ˜í•˜ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
- 4. OrcustëŠ” í‘œì¤€ GUI ë²¤ì¹˜ë§ˆí¬ì—ì„œ 22.2% (ScreenSpot) ë° 23.9% (ScreenSpot-Pro)ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, GUI ì—ì´ì „íŠ¸ì˜ ì¶”ë¡ , ì ì‘ì„± ë° í™•ì¥ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:04:10*