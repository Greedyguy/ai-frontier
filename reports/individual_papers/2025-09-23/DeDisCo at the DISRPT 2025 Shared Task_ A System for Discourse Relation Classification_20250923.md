---
keywords:
  - Discourse Relation Classification
  - Transformer
  - Qwen Model
  - Augmented Dataset
  - Low-Resource Languages
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.11498
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:13:34.190069",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Discourse Relation Classification",
    "Transformer",
    "Qwen Model",
    "Augmented Dataset",
    "Low-Resource Languages"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Discourse Relation Classification": 0.8,
    "Transformer": 0.85,
    "Qwen Model": 0.78,
    "Augmented Dataset": 0.7,
    "Low-Resource Languages": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "discourse relation classification",
        "canonical": "Discourse Relation Classification",
        "aliases": [
          "discourse relations",
          "relation classification"
        ],
        "category": "unique_technical",
        "rationale": "This is the primary focus of the paper and a unique technical term that can connect to related discourse analysis research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "mt5-based encoder",
        "canonical": "Transformer",
        "aliases": [
          "mt5 encoder",
          "mt5"
        ],
        "category": "broad_technical",
        "rationale": "mt5 is a variant of the Transformer model, which is a fundamental concept in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Qwen model",
        "canonical": "Qwen Model",
        "aliases": [
          "Qwen"
        ],
        "category": "unique_technical",
        "rationale": "The Qwen model is a specific model used in the paper, potentially linking to other works using the same model.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "augmented dataset",
        "canonical": "Augmented Dataset",
        "aliases": [
          "data augmentation"
        ],
        "category": "specific_connectable",
        "rationale": "Augmented datasets are crucial for improving model performance, especially in low-resource settings.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "low-resource languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "resource-scarce languages"
        ],
        "category": "specific_connectable",
        "rationale": "This term connects to research focused on language processing for less commonly supported languages.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "system",
      "approach",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "discourse relation classification",
      "resolved_canonical": "Discourse Relation Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "mt5-based encoder",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Qwen model",
      "resolved_canonical": "Qwen Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "augmented dataset",
      "resolved_canonical": "Augmented Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "low-resource languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.11498.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.11498](https://arxiv.org/abs/2509.11498)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/CLaC at DISRPT 2025_ Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification_20250923|CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification]] (85.0% similar)
- [[2025-09-23/CorPipe at CRAC 2025_ Evaluating Multilingual Encoders for Multilingual Coreference Resolution_20250923|CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution]] (79.6% similar)
- [[2025-09-23/Findings of the Fourth Shared Task on Multilingual Coreference Resolution_ Can LLMs Dethrone Traditional Approaches?_20250923|Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?]] (79.6% similar)
- [[2025-09-23/DiscoSG_ Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement_20250923|DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement]] (79.4% similar)
- [[2025-09-23/AISTAT lab system for DCASE2025 Task6_ Language-based audio retrieval_20250923|AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval]] (79.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Augmented Dataset|Augmented Dataset]], [[keywords/Low-Resource Languages|Low-Resource Languages]]
**âš¡ Unique Technical**: [[keywords/Discourse Relation Classification|Discourse Relation Classification]], [[keywords/Qwen Model|Qwen Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.11498v4 Announce Type: replace 
Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025 shared task on discourse relation classification. We test two approaches, using an mt5-based encoder and a decoder based approach using the openly available Qwen model. We also experiment on training with augmented dataset for low-resource languages using matched data translated automatically from English, as well as using some additional linguistic features inspired by entries in previous editions of the Shared Task. Our system achieves a macro-accuracy score of 71.28, and we provide some interpretation and error analysis for our results.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Georgetown Universityì˜ DISRPT 2025 ë‹´í™” ê´€ê³„ ë¶„ë¥˜ ê³µìœ  ê³¼ì œ ì°¸ê°€ì‘ì¸ DeDisCoë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. mt5 ê¸°ë°˜ ì¸ì½”ë”ì™€ Qwen ëª¨ë¸ì„ í™œìš©í•œ ë””ì½”ë” ì ‘ê·¼ë²•ì„ í…ŒìŠ¤íŠ¸í•˜ì˜€ìœ¼ë©°, ì €ìë“¤ì€ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì–¸ì–´ì— ëŒ€í•´ ì˜ì–´ì—ì„œ ìë™ ë²ˆì—­ëœ ë°ì´í„°ë¥¼ í™œìš©í•œ ì¦ê°• ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì´ì „ ëŒ€íšŒ ì°¸ê°€ì‘ì—ì„œ ì˜ê°ì„ ì–»ì€ ì¶”ê°€ì ì¸ ì–¸ì–´ì  íŠ¹ì§•ë„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì€ 71.28ì˜ ë§¤í¬ë¡œ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ê²°ê³¼ì— ëŒ€í•œ í•´ì„ê³¼ ì˜¤ë¥˜ ë¶„ì„ë„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DeDisCoëŠ” Georgetown Universityê°€ DISRPT 2025 ë‹´í™” ê´€ê³„ ë¶„ë¥˜ ê³µìœ  ê³¼ì œì— ì œì¶œí•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- 2. ë‘ ê°€ì§€ ì ‘ê·¼ë²•ì„ í…ŒìŠ¤íŠ¸í–ˆìœ¼ë©°, mt5 ê¸°ë°˜ ì¸ì½”ë”ì™€ ê³µê°œëœ Qwen ëª¨ë¸ì„ ì‚¬ìš©í•œ ë””ì½”ë” ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- 3. ì €ìë“¤ì€ ì˜ì–´ì—ì„œ ìë™ ë²ˆì—­ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ìì› ì–¸ì–´ì— ëŒ€í•œ ì¦ê°• ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤.
- 4. ì´ì „ ê³µìœ  ê³¼ì œì˜ ì°¸ê°€ì‘ì—ì„œ ì˜ê°ì„ ë°›ì€ ì¶”ê°€ì ì¸ ì–¸ì–´ì  íŠ¹ì§•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- 5. ì‹œìŠ¤í…œì€ ë§¤í¬ë¡œ ì •í™•ë„ ì ìˆ˜ 71.28ì„ ë‹¬ì„±í–ˆìœ¼ë©°, ê²°ê³¼ì— ëŒ€í•œ í•´ì„ ë° ì˜¤ë¥˜ ë¶„ì„ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:13:34*