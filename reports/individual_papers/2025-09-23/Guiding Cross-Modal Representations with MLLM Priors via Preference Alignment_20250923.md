---
keywords:
  - Multimodal Large Language Model
  - Cross-Modal Representation Learning
  - Preference Alignment
  - Relative Preference Alignment
  - CLIP
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.06970
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:24:22.190994",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Large Language Model",
    "Cross-Modal Representation Learning",
    "Preference Alignment",
    "Relative Preference Alignment",
    "CLIP"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Large Language Model": 0.88,
    "Cross-Modal Representation Learning": 0.82,
    "Preference Alignment": 0.79,
    "Relative Preference Alignment": 0.77,
    "CLIP": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "MLLM",
        "canonical": "Multimodal Large Language Model",
        "aliases": [
          "Multimodal LLM",
          "Multimodal Language Model"
        ],
        "category": "specific_connectable",
        "rationale": "MLLMs are central to the paper's approach, offering a bridge between modalities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Cross-Modal Representation",
        "canonical": "Cross-Modal Representation Learning",
        "aliases": [
          "Cross-Modal Learning"
        ],
        "category": "evolved_concepts",
        "rationale": "The paper focuses on enhancing cross-modal representation, a key concept in multimodal learning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Preference Alignment",
        "canonical": "Preference Alignment",
        "aliases": [
          "Preference Learning"
        ],
        "category": "unique_technical",
        "rationale": "Preference alignment is a novel approach in the paper, crucial for guiding representation learning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "Relative Preference Alignment",
        "canonical": "Relative Preference Alignment",
        "aliases": [
          "RPA"
        ],
        "category": "unique_technical",
        "rationale": "RPA is a specific technique introduced in the paper, enhancing the precision of preference alignment.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Contrastive Language-Image Pretraining",
        "canonical": "CLIP",
        "aliases": [
          "Contrastive Learning"
        ],
        "category": "broad_technical",
        "rationale": "CLIP is a foundational model referenced in the paper, relevant for understanding modality gaps.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "MLLM",
      "resolved_canonical": "Multimodal Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Cross-Modal Representation",
      "resolved_canonical": "Cross-Modal Representation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Preference Alignment",
      "resolved_canonical": "Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Relative Preference Alignment",
      "resolved_canonical": "Relative Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Contrastive Language-Image Pretraining",
      "resolved_canonical": "CLIP",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    }
  ]
}
-->

# Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.06970.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.06970](https://arxiv.org/abs/2506.06970)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (85.8% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (85.6% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (85.6% similar)
- [[2025-09-23/Continual Multimodal Contrastive Learning_20250923|Continual Multimodal Contrastive Learning]] (85.0% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/CLIP|CLIP]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Large Language Model|Multimodal Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Preference Alignment|Preference Alignment]], [[keywords/Relative Preference Alignment|Relative Preference Alignment]]
**ğŸš€ Evolved Concepts**: [[keywords/Cross-Modal Representation Learning|Cross-Modal Representation Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.06970v2 Announce Type: replace 
Abstract: Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability to retrieve content across modalities, a substantial modality gap persists in its feature space. Intriguingly, we discover that off-the-shelf MLLMs (Multimodal Large Language Models) demonstrate powerful inherent modality alignment properties. While recent MLLM-based retrievers with unified architectures partially mitigate this gap, their reliance on coarse modality alignment mechanisms fundamentally limits their potential. In this work, We introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel framework that leverages the fine grained alignment priors inherent in MLLM to guide cross modal representation learning. MAPLE formulates the learning process as reinforcement learning with two key components: (1) Automatic preference data construction using off-the-shelf MLLM, and (2) a new Relative Preference Alignment (RPA) loss, which adapts Direct Preference Optimization (DPO) to the embedding learning setting. Experimental results show that our preference-guided alignment achieves substantial gains in fine-grained cross-modal retrieval, underscoring its effectiveness in handling nuanced semantic distinctions.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ CLIPì˜ ëª¨ë‹¬ë¦¬í‹° ê°„ ì½˜í…ì¸  ê²€ìƒ‰ ëŠ¥ë ¥ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ” ëª¨ë‹¬ë¦¬í‹° ê²©ì°¨ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì €ìë“¤ì€ MLLM(ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì´ ê°•ë ¥í•œ ëª¨ë‹¬ë¦¬í‹° ì •ë ¬ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ MLLM ê¸°ë°˜ ê²€ìƒ‰ê¸°ëŠ” ì´ ê²©ì°¨ë¥¼ ì¼ë¶€ ì¤„ì˜€ì§€ë§Œ, ì¡°ì¡í•œ ì •ë ¬ ë©”ì»¤ë‹ˆì¦˜ì— ì˜ì¡´í•˜ì—¬ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ MAPLEì´ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. MAPLEì€ MLLMì˜ ì„¸ë°€í•œ ì •ë ¬ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ êµì°¨ ëª¨ë‹¬ í‘œí˜„ í•™ìŠµì„ ê°•í™”í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” (1) MLLMì„ ì‚¬ìš©í•œ ìë™ ì„ í˜¸ ë°ì´í„° êµ¬ì¶•ê³¼ (2) ìƒˆë¡œìš´ ìƒëŒ€ì  ì„ í˜¸ ì •ë ¬(RPA) ì†ì‹¤ì„ í¬í•¨í•˜ì—¬ ê°•í™” í•™ìŠµìœ¼ë¡œ í•™ìŠµ ê³¼ì •ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MAPLEì€ ì„¸ë°€í•œ êµì°¨ ëª¨ë‹¬ ê²€ìƒ‰ì—ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, ë¯¸ì„¸í•œ ì˜ë¯¸ì  ì°¨ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CLIPì˜ ê¸°ëŠ¥ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì—¬ì „íˆ ê¸°ëŠ¥ ê³µê°„ì—ì„œ ìƒë‹¹í•œ ëª¨ë‹¬ë¦¬í‹° ê²©ì°¨ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
- 2. MLLMì€ ê°•ë ¥í•œ ë‚´ì¬ì  ëª¨ë‹¬ë¦¬í‹° ì •ë ¬ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 3. MAPLEì€ MLLMì˜ ì„¸ë°€í•œ ì •ë ¬ ì„ í–‰ ì¡°ê±´ì„ í™œìš©í•˜ì—¬ êµì°¨ ëª¨ë‹¬ í‘œí˜„ í•™ìŠµì„ ì•ˆë‚´í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 4. MAPLEì€ ìë™ ì„ í˜¸ ë°ì´í„° êµ¬ì„±ê³¼ ìƒˆë¡œìš´ ìƒëŒ€ì  ì„ í˜¸ ì •ë ¬ ì†ì‹¤ì„ í†µí•´ í•™ìŠµ ê³¼ì •ì„ ê°•í™” í•™ìŠµìœ¼ë¡œ ê³µì‹í™”í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì„ í˜¸ ê¸°ë°˜ ì •ë ¬ì´ ë¯¸ì„¸í•œ êµì°¨ ëª¨ë‹¬ ê²€ìƒ‰ì—ì„œ ìƒë‹¹í•œ í–¥ìƒì„ ì´ë£¨ì–´ëƒˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:24:22*