---
keywords:
  - Multimodal Large Language Model
  - Cross-Modal Representation Learning
  - Preference Alignment
  - Relative Preference Alignment
  - CLIP
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.06970
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:24:22.190994",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Large Language Model",
    "Cross-Modal Representation Learning",
    "Preference Alignment",
    "Relative Preference Alignment",
    "CLIP"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Large Language Model": 0.88,
    "Cross-Modal Representation Learning": 0.82,
    "Preference Alignment": 0.79,
    "Relative Preference Alignment": 0.77,
    "CLIP": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "MLLM",
        "canonical": "Multimodal Large Language Model",
        "aliases": [
          "Multimodal LLM",
          "Multimodal Language Model"
        ],
        "category": "specific_connectable",
        "rationale": "MLLMs are central to the paper's approach, offering a bridge between modalities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Cross-Modal Representation",
        "canonical": "Cross-Modal Representation Learning",
        "aliases": [
          "Cross-Modal Learning"
        ],
        "category": "evolved_concepts",
        "rationale": "The paper focuses on enhancing cross-modal representation, a key concept in multimodal learning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Preference Alignment",
        "canonical": "Preference Alignment",
        "aliases": [
          "Preference Learning"
        ],
        "category": "unique_technical",
        "rationale": "Preference alignment is a novel approach in the paper, crucial for guiding representation learning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "Relative Preference Alignment",
        "canonical": "Relative Preference Alignment",
        "aliases": [
          "RPA"
        ],
        "category": "unique_technical",
        "rationale": "RPA is a specific technique introduced in the paper, enhancing the precision of preference alignment.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Contrastive Language-Image Pretraining",
        "canonical": "CLIP",
        "aliases": [
          "Contrastive Learning"
        ],
        "category": "broad_technical",
        "rationale": "CLIP is a foundational model referenced in the paper, relevant for understanding modality gaps.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "MLLM",
      "resolved_canonical": "Multimodal Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Cross-Modal Representation",
      "resolved_canonical": "Cross-Modal Representation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Preference Alignment",
      "resolved_canonical": "Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Relative Preference Alignment",
      "resolved_canonical": "Relative Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Contrastive Language-Image Pretraining",
      "resolved_canonical": "CLIP",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    }
  ]
}
-->

# Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.06970.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.06970](https://arxiv.org/abs/2506.06970)

## 🔗 유사한 논문
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (85.8% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (85.6% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (85.6% similar)
- [[2025-09-23/Continual Multimodal Contrastive Learning_20250923|Continual Multimodal Contrastive Learning]] (85.0% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (84.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/CLIP|CLIP]]
**🔗 Specific Connectable**: [[keywords/Multimodal Large Language Model|Multimodal Large Language Model]]
**⚡ Unique Technical**: [[keywords/Preference Alignment|Preference Alignment]], [[keywords/Relative Preference Alignment|Relative Preference Alignment]]
**🚀 Evolved Concepts**: [[keywords/Cross-Modal Representation Learning|Cross-Modal Representation Learning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.06970v2 Announce Type: replace 
Abstract: Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability to retrieve content across modalities, a substantial modality gap persists in its feature space. Intriguingly, we discover that off-the-shelf MLLMs (Multimodal Large Language Models) demonstrate powerful inherent modality alignment properties. While recent MLLM-based retrievers with unified architectures partially mitigate this gap, their reliance on coarse modality alignment mechanisms fundamentally limits their potential. In this work, We introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel framework that leverages the fine grained alignment priors inherent in MLLM to guide cross modal representation learning. MAPLE formulates the learning process as reinforcement learning with two key components: (1) Automatic preference data construction using off-the-shelf MLLM, and (2) a new Relative Preference Alignment (RPA) loss, which adapts Direct Preference Optimization (DPO) to the embedding learning setting. Experimental results show that our preference-guided alignment achieves substantial gains in fine-grained cross-modal retrieval, underscoring its effectiveness in handling nuanced semantic distinctions.

## 📝 요약

이 논문은 CLIP의 모달리티 간 콘텐츠 검색 능력에도 불구하고 여전히 존재하는 모달리티 격차 문제를 다룹니다. 저자들은 MLLM(다중 모달 대형 언어 모델)이 강력한 모달리티 정렬 특성을 가지고 있음을 발견했습니다. 기존의 MLLM 기반 검색기는 이 격차를 일부 줄였지만, 조잡한 정렬 메커니즘에 의존하여 한계가 있었습니다. 이를 해결하기 위해, 저자들은 MAPLE이라는 새로운 프레임워크를 제안했습니다. MAPLE은 MLLM의 세밀한 정렬 특성을 활용하여 교차 모달 표현 학습을 강화합니다. 이 프레임워크는 (1) MLLM을 사용한 자동 선호 데이터 구축과 (2) 새로운 상대적 선호 정렬(RPA) 손실을 포함하여 강화 학습으로 학습 과정을 구성합니다. 실험 결과, MAPLE은 세밀한 교차 모달 검색에서 상당한 성능 향상을 보여주며, 미세한 의미적 차이를 효과적으로 처리할 수 있음을 입증했습니다.

## 🎯 주요 포인트

- 1. CLIP의 기능에도 불구하고, 여전히 기능 공간에서 상당한 모달리티 격차가 존재합니다.
- 2. MLLM은 강력한 내재적 모달리티 정렬 특성을 가지고 있습니다.
- 3. MAPLE은 MLLM의 세밀한 정렬 선행 조건을 활용하여 교차 모달 표현 학습을 안내하는 새로운 프레임워크입니다.
- 4. MAPLE은 자동 선호 데이터 구성과 새로운 상대적 선호 정렬 손실을 통해 학습 과정을 강화 학습으로 공식화합니다.
- 5. 실험 결과, 선호 기반 정렬이 미세한 교차 모달 검색에서 상당한 향상을 이루어냈습니다.


---

*Generated on 2025-09-24 05:24:22*