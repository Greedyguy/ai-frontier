---
keywords:
  - Graph Aligned Large Language Models
  - Graph Neural Network
  - Large Language Model
  - Cross-modal Alignment
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2409.04183
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:39:18.520412",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Aligned Large Language Models",
    "Graph Neural Network",
    "Large Language Model",
    "Cross-modal Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Graph Aligned Large Language Models": 0.78,
    "Graph Neural Network": 0.82,
    "Large Language Model": 0.75,
    "Cross-modal Alignment": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Graph Aligned Large Language Models",
        "canonical": "Graph Aligned Large Language Models",
        "aliases": [
          "GALLa"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel framework combining graph neural networks with large language models, enhancing source code understanding.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Graph Neural Networks",
        "canonical": "Graph Neural Network",
        "aliases": [
          "GNN"
        ],
        "category": "specific_connectable",
        "rationale": "Graph neural networks are crucial for encoding structural information in source code, linking to existing graph-based methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are central to the study, providing a basis for integrating graph-based enhancements.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Cross-modal alignment",
        "canonical": "Cross-modal Alignment",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Cross-modal alignment is essential for integrating structural graph information with language models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "source code",
      "baseline LLM",
      "training time"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Graph Aligned Large Language Models",
      "resolved_canonical": "Graph Aligned Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Graph Neural Networks",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Cross-modal alignment",
      "resolved_canonical": "Cross-modal Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2409.04183.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2409.04183](https://arxiv.org/abs/2409.04183)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Generalizable End-to-End Tool-Use RL with Synthetic CodeGym_20250923|Generalizable End-to-End Tool-Use RL with Synthetic CodeGym]] (85.1% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (84.6% similar)
- [[2025-09-22/A Survey of Large Language Models for Data Challenges in Graphs_20250922|A Survey of Large Language Models for Data Challenges in Graphs]] (84.2% similar)
- [[2025-09-23/MapCoder-Lite_ Squeezing Multi-Agent Coding into a Single Small LLM_20250923|MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM]] (84.0% similar)
- [[2025-09-19/Automated and Context-Aware Code Documentation Leveraging Advanced LLMs_20250919|Automated and Context-Aware Code Documentation Leveraging Advanced LLMs]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]], [[keywords/Cross-modal Alignment|Cross-modal Alignment]]
**âš¡ Unique Technical**: [[keywords/Graph Aligned Large Language Models|Graph Aligned Large Language Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2409.04183v3 Announce Type: replace-cross 
Abstract: Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” GALLa(ê·¸ë˜í”„ ì •ë ¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì½”ë“œ ì–¸ì–´ ëª¨ë¸ì€ ì†ŒìŠ¤ ì½”ë“œë¥¼ í…ìŠ¤íŠ¸ í† í°ìœ¼ë¡œë§Œ ì²˜ë¦¬í•˜ì§€ë§Œ, GALLaëŠ” ê·¸ë˜í”„ ì‹ ê²½ë§ê³¼ êµì°¨ ëª¨ë‹¬ ì •ë ¬ ê¸°ìˆ ì„ ì‚¬ìš©í•´ ì½”ë“œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì— ì£¼ì…í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ê³¼ ì‘ì—…ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, í•™ìŠµ ì‹œì—ë§Œ êµ¬ì¡°ì  ê·¸ë˜í”„ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ê³  ì¶”ë¡  ì‹œì—ëŠ” ì¶”ê°€ ë¹„ìš©ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 350Mì—ì„œ 14Bê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ 7ê°œ ê¸°ë³¸ LLMì„ ì‚¬ìš©í•œ 5ê°€ì§€ ì½”ë“œ ì‘ì—… ì‹¤í—˜ì—ì„œ GALLaëŠ” ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, LLaMA3ì™€ Qwen2.5-Coderì™€ ê°™ì€ ê°•ë ¥í•œ ëª¨ë¸ì—ì„œë„ íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ í’ë¶€í•œ ì˜ë¯¸ ì •ë³´ë¥¼ ê·¸ë˜í”„ í˜•íƒœë¡œ í‘œí˜„í•˜ì—¬ ì½”ë“œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” GALLa ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. GALLaëŠ” ê·¸ë˜í”„ ì‹ ê²½ë§ê³¼ í¬ë¡œìŠ¤ ëª¨ë‹¬ ì •ë ¬ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ LLMì— ì£¼ì…í•©ë‹ˆë‹¤.
- 3. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ëª¨ë¸ ë° ì‘ì—…ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ì½”ë“œ LLMì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. GALLaëŠ” í•™ìŠµ ì‹œì—ë§Œ êµ¬ì¡°ì  ê·¸ë˜í”„ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ë©°, ì¶”ë¡  ì‹œì—ëŠ” ì¶”ê°€ ë¹„ìš©ì´ ë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, GALLaëŠ” ë‹¤ì–‘í•œ í¬ê¸°ì˜ LLMì„ ì‚¬ìš©í•œ ë‹¤ì„¯ ê°€ì§€ ì½”ë“œ ì‘ì—…ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:39:18*