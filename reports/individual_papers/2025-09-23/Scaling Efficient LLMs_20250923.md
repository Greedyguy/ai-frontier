---
keywords:
  - Recurrent Transformers
  - AI Scaling Law
  - Kullback-Leibler Divergence
  - Efficient Large Language Models
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2402.14746
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:54:52.665718",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Recurrent Transformers",
    "AI Scaling Law",
    "Kullback-Leibler Divergence",
    "Efficient Large Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Recurrent Transformers": 0.78,
    "AI Scaling Law": 0.77,
    "Kullback-Leibler Divergence": 0.81,
    "Efficient Large Language Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "recurrent transformers",
        "canonical": "Recurrent Transformers",
        "aliases": [
          "recurrent transformer",
          "recurrent transformer networks"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel architecture combining transformers and recurrent networks, enhancing efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "AI scaling law",
        "canonical": "AI Scaling Law",
        "aliases": [
          "scaling law for AI",
          "transformer scaling law"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a fundamental principle affecting the design and efficiency of large language models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Kullback-Liebler divergence",
        "canonical": "Kullback-Leibler Divergence",
        "aliases": [
          "KL divergence"
        ],
        "category": "specific_connectable",
        "rationale": "A key statistical measure used in the paper to derive efficiency metrics for LLMs.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.78,
        "link_intent_score": 0.81
      },
      {
        "surface": "efficient LLMs",
        "canonical": "Efficient Large Language Models",
        "aliases": [
          "efficient LLM",
          "optimized LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Focuses on optimizing large language models for parameter efficiency and performance.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "transformer architecture",
      "training corpus"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "recurrent transformers",
      "resolved_canonical": "Recurrent Transformers",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "AI scaling law",
      "resolved_canonical": "AI Scaling Law",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Kullback-Liebler divergence",
      "resolved_canonical": "Kullback-Leibler Divergence",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.78,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "efficient LLMs",
      "resolved_canonical": "Efficient Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Scaling Efficient LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2402.14746.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2402.14746](https://arxiv.org/abs/2402.14746)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (86.5% similar)
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (85.0% similar)
- [[2025-09-23/Conv-like Scale-Fusion Time Series Transformer_ A Multi-Scale Representation for Variable-Length Long Time Series_20250923|Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series]] (84.0% similar)
- [[2025-09-23/Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model_20250923|Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model]] (83.7% similar)
- [[2025-09-23/How Large Language Models are Designed to Hallucinate_20250923|How Large Language Models are Designed to Hallucinate]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Efficient Large Language Models|Efficient Large Language Models]]
**ğŸ”— Specific Connectable**: [[keywords/AI Scaling Law|AI Scaling Law]], [[keywords/Kullback-Leibler Divergence|Kullback-Leibler Divergence]]
**âš¡ Unique Technical**: [[keywords/Recurrent Transformers|Recurrent Transformers]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2402.14746v4 Announce Type: replace-cross 
Abstract: Trained LLMs in the transformer architecture are typically sparse in that most of the parameters are negligible, raising questions on efficiency. Furthermore, the so called "AI scaling law" for transformers suggests that the number of parameters must scale linearly with the size of the data. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, by comparing theoretical and empirical estimates of the Kullback-Liebler divergence, we derive a natural AI scaling law that the number of parameters in an efficient LLM scales as $D^{\gamma}$ where $D$ is the size of the training data and $ \gamma \in [0.44, 0.72]$, suggesting the existence of more efficient architectures. Against this backdrop, we propose recurrent transformers, combining the efficacy of transformers with the efficiency of recurrent networks, progressively applying a single transformer layer to a fixed-width sliding window across the input sequence. Recurrent transformers (a) run in linear time in the sequence length, (b) are memory-efficient and amenable to parallel processing in large batches, (c) learn to forget history for language tasks, or accumulate history for long range tasks like copy and selective copy, and (d) are amenable to curriculum training to overcome vanishing gradients. In our experiments, we find that recurrent transformers perform favorably on benchmark tests.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íš¨ìœ¨ì ì¸ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„¤ê³„ì— ëŒ€í•´ ì—°êµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ëŠ” ëŒ€ë¶€ë¶„ì˜ ë§¤ê°œë³€ìˆ˜ê°€ ì¤‘ìš”í•˜ì§€ ì•Šì•„ ë¹„íš¨ìœ¨ì ì´ë©°, AI ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì— ë”°ë¥´ë©´ ë§¤ê°œë³€ìˆ˜ ìˆ˜ëŠ” ë°ì´í„° í¬ê¸°ì— ë¹„ë¡€í•´ì•¼ í•©ë‹ˆë‹¤. ì €ìë“¤ì€ íš¨ìœ¨ì ì¸ LLMì˜ ë§¤ê°œë³€ìˆ˜ ìˆ˜ê°€ ë°ì´í„° í¬ê¸° $D$ì— ëŒ€í•´ $D^{\gamma}$ë¡œ ìŠ¤ì¼€ì¼ë§ëœë‹¤ëŠ” ë²•ì¹™ì„ ì œì•ˆí•˜ë©°, $\gamma$ëŠ” [0.44, 0.72] ë²”ìœ„ì— ì†í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ìˆœí™˜ ì‹ ê²½ë§ì˜ ì¥ì ì„ ê²°í•©í•œ 'ìˆœí™˜ íŠ¸ëœìŠ¤í¬ë¨¸'ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„ í˜• ì‹œê°„ ë³µì¡ë„ë¥¼ ê°€ì§€ë©°, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë³‘ë ¬ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ìˆœí™˜ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ LLMì€ ëŒ€ë¶€ë¶„ì˜ ë§¤ê°œë³€ìˆ˜ê°€ ë¯¸ë¯¸í•˜ì—¬ íš¨ìœ¨ì„±ì— ëŒ€í•œ ì˜ë¬¸ì„ ì œê¸°í•©ë‹ˆë‹¤.
- 2. íš¨ìœ¨ì ì¸ LLMì€ í›ˆë ¨ ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë§¤ê°œë³€ìˆ˜ê°€ $D^{\gamma}$ë¡œ ìŠ¤ì¼€ì¼ë§ë˜ë©°, $\gamma$ëŠ” [0.44, 0.72] ë²”ìœ„ì— ìˆìŠµë‹ˆë‹¤.
- 3. ë°˜ë³µ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íš¨ê³¼ì™€ ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ì˜ íš¨ìœ¨ì„±ì„ ê²°í•©í•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ì— ê³ ì • í­ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
- 4. ë°˜ë³µ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ì— ëŒ€í•´ ì„ í˜• ì‹œê°„ì— ì‹¤í–‰ë˜ë©°, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ëŒ€ëŸ‰ ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- 5. ë°˜ë³µ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:54:52*