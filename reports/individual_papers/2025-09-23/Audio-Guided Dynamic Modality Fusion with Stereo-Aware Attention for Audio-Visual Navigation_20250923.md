---
keywords:
  - Stereo-Aware Attention Module
  - Audio-Guided Dynamic Fusion Module
  - Audio-Visual Navigation
  - Stereo Audio Cues
  - Multimodal Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16924
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:54:14.603448",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Stereo-Aware Attention Module",
    "Audio-Guided Dynamic Fusion Module",
    "Audio-Visual Navigation",
    "Stereo Audio Cues",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Stereo-Aware Attention Module": 0.78,
    "Audio-Guided Dynamic Fusion Module": 0.77,
    "Audio-Visual Navigation": 0.8,
    "Stereo Audio Cues": 0.72,
    "Multimodal Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Stereo-Aware Attention Module",
        "canonical": "Stereo-Aware Attention Module",
        "aliases": [
          "SAM"
        ],
        "category": "unique_technical",
        "rationale": "This module introduces a novel approach to leveraging stereo audio cues, which is crucial for enhancing directional sound perception in audio-visual navigation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Audio-Guided Dynamic Fusion Module",
        "canonical": "Audio-Guided Dynamic Fusion Module",
        "aliases": [
          "AGDF"
        ],
        "category": "unique_technical",
        "rationale": "This module dynamically adjusts audio-visual fusion, which is key for adapting to environmental changes in navigation tasks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Audio-Visual Navigation",
        "canonical": "Audio-Visual Navigation",
        "aliases": [
          "AVN"
        ],
        "category": "specific_connectable",
        "rationale": "This is a central task in the paper, linking to broader research in multi-modal navigation systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Stereo Audio Cues",
        "canonical": "Stereo Audio Cues",
        "aliases": [
          "Stereo Sound Cues"
        ],
        "category": "specific_connectable",
        "rationale": "Stereo audio cues are pivotal for spatial awareness in navigation, connecting to research on audio signal processing.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Deep Multi-Modal Fusion",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Deep Multi-Modal Fusion"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is crucial for integrating audio and visual data, aligning with recent trends in multimodal learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "end-to-end",
      "reinforcement learning",
      "3D environments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Stereo-Aware Attention Module",
      "resolved_canonical": "Stereo-Aware Attention Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Audio-Guided Dynamic Fusion Module",
      "resolved_canonical": "Audio-Guided Dynamic Fusion Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Audio-Visual Navigation",
      "resolved_canonical": "Audio-Visual Navigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Stereo Audio Cues",
      "resolved_canonical": "Stereo Audio Cues",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Deep Multi-Modal Fusion",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16924.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16924](https://arxiv.org/abs/2509.16924)

## 🔗 유사한 논문
- [[2025-09-18/Spatial Audio Motion Understanding and Reasoning_20250918|Spatial Audio Motion Understanding and Reasoning]] (84.7% similar)
- [[2025-09-18/Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing_20250918|Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing]] (83.4% similar)
- [[2025-09-17/DSpAST_ Disentangled Representations for Spatial Audio Reasoning with Large Language Models_20250917|DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models]] (83.1% similar)
- [[2025-09-18/Embodied Navigation Foundation Model_20250918|Embodied Navigation Foundation Model]] (81.9% similar)
- [[2025-09-22/SightSound-R1_ Cross-Modal Reasoning Distillation from Vision to Audio Language Models_20250922|SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Audio-Visual Navigation|Audio-Visual Navigation]], [[keywords/Stereo Audio Cues|Stereo Audio Cues]], [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Stereo-Aware Attention Module|Stereo-Aware Attention Module]], [[keywords/Audio-Guided Dynamic Fusion Module|Audio-Guided Dynamic Fusion Module]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16924v1 Announce Type: new 
Abstract: In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention \textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.

## 📝 요약

이 논문은 복잡한 3D 환경에서 소리의 출처를 자율적으로 찾는 오디오-비주얼 내비게이션(AVN) 과제를 다룹니다. 기존 방법들이 정적 모달리티 융합 전략에 의존하고 스테레오 오디오의 공간적 단서를 무시하는 문제를 해결하기 위해, 저자들은 두 가지 주요 혁신을 제안합니다. 첫째, 스테레오 인식 주의 모듈(SAM)은 좌우 오디오 채널 간의 공간적 차이를 학습하여 방향성 있는 소리 인식을 향상시킵니다. 둘째, 오디오-유도 동적 융합 모듈(AGDF)은 오디오 단서에 따라 시각 및 청각 특징의 융합 비율을 동적으로 조정하여 환경 변화에 대한 강인성을 높입니다. Replica와 Matterport3D 데이터셋에서의 실험 결과, 제안된 방법이 기존 접근법보다 내비게이션 성공률과 경로 효율성 면에서 뛰어난 성능을 보였으며, 특히 오디오만 사용하는 조건에서 40% 이상의 성능 향상을 달성했습니다. 이는 스테레오 채널의 공간적 단서를 명시적으로 모델링하고 깊이 있는 다중 모달 융합을 수행하는 것이 중요함을 강조합니다.

## 🎯 주요 포인트

- 1. 제안된 AVN 프레임워크는 스테레오 오디오의 공간적 차이를 활용하여 방향성 있는 소리 인식을 강화하는 스테레오-인식 주의 모듈(SAM)을 포함합니다.
- 2. 오디오-유도 동적 융합 모듈(AGDF)은 오디오 신호에 기반하여 시각 및 청각 특징의 융합 비율을 동적으로 조정하여 환경 변화에 대한 강인성을 향상시킵니다.
- 3. 제안된 방법은 두 가지 현실적인 3D 장면 데이터셋(Replica와 Matterport3D)에서 기존 방법들보다 내비게이션 성공률과 경로 효율성 면에서 크게 뛰어난 성능을 보였습니다.
- 4. 오디오만 사용한 조건에서도 제안된 모델은 기존 최고의 성능을 보인 기준 모델들보다 40% 이상의 성능 향상을 달성했습니다.
- 5. 스테레오 채널의 공간적 단서를 명시적으로 모델링하고 깊이 있는 다중 모달 융합을 수행하는 것이 강력하고 효율적인 오디오-비주얼 내비게이션에 중요함을 강조합니다.


---

*Generated on 2025-09-23 22:54:14*