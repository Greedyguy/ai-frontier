---
keywords:
  - Large Language Model
  - Retrieval Augmented Generation
  - MedRaC
  - Error Analysis Framework
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16584
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:26:24.890925",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Retrieval Augmented Generation",
    "MedRaC",
    "Error Analysis Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Retrieval Augmented Generation": 0.82,
    "MedRaC": 0.7,
    "Error Analysis Framework": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "As a foundational technology, it connects to various subfields and applications in NLP and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "It represents a trending method that enhances LLM capabilities, relevant for linking to recent advancements.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "MedRaC",
        "canonical": "MedRaC",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique framework introduced in the paper, pivotal for understanding the proposed methodology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Error Analysis Framework",
        "canonical": "Error Analysis Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel approach for diagnosing LLM performance, crucial for linking to evaluation methodologies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "clinical decision-making",
      "final answer",
      "human evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "MedRaC",
      "resolved_canonical": "MedRaC",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Error Analysis Framework",
      "resolved_canonical": "Error Analysis Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16584.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16584](https://arxiv.org/abs/2509.16584)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (87.6% similar)
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (86.9% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (86.8% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (86.3% similar)
- [[2025-09-22/Calibrating LLM Confidence by Probing Perturbed Representation Stability_20250922|Calibrating LLM Confidence by Probing Perturbed Representation Stability]] (85.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**âš¡ Unique Technical**: [[keywords/MedRaC|MedRaC]], [[keywords/Error Analysis Framework|Error Analysis Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16584v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated promising performance on medical benchmarks; however, their ability to perform medical calculations, a crucial aspect of clinical decision-making, remains underexplored and poorly evaluated. Existing benchmarks often assess only the final answer with a wide numerical tolerance, overlooking systematic reasoning failures and potentially causing serious clinical misjudgments. In this work, we revisit medical calculation evaluation with a stronger focus on clinical trustworthiness. First, we clean and restructure the MedCalc-Bench dataset and propose a new step-by-step evaluation pipeline that independently assesses formula selection, entity extraction, and arithmetic computation. Under this granular framework, the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by prior evaluations. Second, we introduce an automatic error analysis framework that generates structured attribution for each failure mode. Human evaluation confirms its alignment with expert judgment, enabling scalable and explainable diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that combines retrieval-augmented generation and Python-based code execution. Without any fine-tuning, MedRaC improves the accuracy of different LLMs from 16.35% up to 53.19%. Our work highlights the limitations of current benchmark practices and proposes a more clinically faithful methodology. By enabling transparent and transferable reasoning evaluation, we move closer to making LLM-based systems trustworthy for real-world medical applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì˜ë£Œ ê³„ì‚° ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ëŠ” ìµœì¢… ë‹µë³€ë§Œ í‰ê°€í•˜ì—¬ ì²´ê³„ì ì¸ ì¶”ë¡  ì˜¤ë¥˜ë¥¼ ê°„ê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ MedCalc-Bench ë°ì´í„°ì…‹ì„ ì •ë¦¬í•˜ê³ , ê³µì‹ ì„ íƒ, ì—”í‹°í‹° ì¶”ì¶œ, ì‚°ìˆ  ê³„ì‚°ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë‹¨ê³„ë³„ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ GPT-4oì˜ ì •í™•ë„ê°€ 62.7%ì—ì„œ 43.6%ë¡œ ê°ì†Œí•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìë™ ì˜¤ë¥˜ ë¶„ì„ í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ ì˜¤ë¥˜ ëª¨ë“œì— ëŒ€í•œ êµ¬ì¡°ì  ê·€ì†ì„ ìƒì„±í•˜ê³ , ì¸ê°„ í‰ê°€ë¥¼ í†µí•´ ì „ë¬¸ê°€ íŒë‹¨ê³¼ì˜ ì¼ì¹˜ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, MedRaCë¼ëŠ” ëª¨ë“ˆí˜• ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•˜ì—¬ LLMì˜ ì •í™•ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” í˜„ì¬ ë²¤ì¹˜ë§ˆí¬ì˜ í•œê³„ë¥¼ ê°•ì¡°í•˜ê³ , ì„ìƒì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ ë°©ë²•ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì˜ë£Œ ê³„ì‚° ìˆ˜í–‰ ëŠ¥ë ¥ì€ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì²´ê³„ì ì¸ ì¶”ë¡  ì‹¤íŒ¨ë¥¼ ê°„ê³¼í•  ìˆ˜ ìˆë‹¤.
- 2. MedCalc-Bench ë°ì´í„°ì…‹ì„ ì •ë¦¬í•˜ê³  ì¬êµ¬ì„±í•˜ì—¬ ê³µì‹ ì„ íƒ, ì—”í‹°í‹° ì¶”ì¶œ, ì‚°ìˆ  ê³„ì‚°ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë‹¨ê³„ë³„ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•œë‹¤.
- 3. ìƒˆë¡œìš´ í‰ê°€ í”„ë ˆì„ì›Œí¬ í•˜ì—ì„œ GPT-4oì˜ ì •í™•ë„ê°€ 62.7%ì—ì„œ 43.6%ë¡œ ê°ì†Œí•˜ì—¬ ì´ì „ í‰ê°€ì—ì„œ ê°€ë ¤ì¡Œë˜ ì˜¤ë¥˜ë¥¼ ë“œëŸ¬ë‚¸ë‹¤.
- 4. ì‹¤íŒ¨ ëª¨ë“œì— ëŒ€í•œ êµ¬ì¡°í™”ëœ ê·€ì†ì„ ìƒì„±í•˜ëŠ” ìë™ ì˜¤ë¥˜ ë¶„ì„ í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ ì „ë¬¸ê°€ íŒë‹¨ê³¼ì˜ ì •ë ¬ì„ í™•ì¸í•œë‹¤.
- 5. MedRaCë¼ëŠ” ëª¨ë“ˆí˜• ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•˜ì—¬, ë‹¤ì–‘í•œ LLMì˜ ì •í™•ë„ë¥¼ 16.35%ì—ì„œ ìµœëŒ€ 53.19%ê¹Œì§€ í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-23 23:26:24*