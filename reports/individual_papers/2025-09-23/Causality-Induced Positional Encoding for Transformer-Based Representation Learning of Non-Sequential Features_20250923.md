---
keywords:
  - Transformer
  - Causality-Induced Positional Encoding
  - Directed Acyclic Graph
  - Hyperbolic Space
  - Attention Mechanism
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16629
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:41:26.613191",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Causality-Induced Positional Encoding",
    "Directed Acyclic Graph",
    "Hyperbolic Space",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Causality-Induced Positional Encoding": 0.8,
    "Directed Acyclic Graph": 0.78,
    "Hyperbolic Space": 0.72,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are central to the paper's methodology, linking to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Causality-Induced Positional Encoding",
        "canonical": "Causality-Induced Positional Encoding",
        "aliases": [
          "CAPE"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel method introduced by the paper, crucial for understanding its unique contribution.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Weighted Directed Acyclic Graph",
        "canonical": "Directed Acyclic Graph",
        "aliases": [
          "DAG"
        ],
        "category": "specific_connectable",
        "rationale": "DAGs are essential for representing causal structures, linking to graph theory and causal inference.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Hyperbolic Space",
        "canonical": "Hyperbolic Space",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Hyperbolic space is used for embedding the DAG, connecting to geometric representations in machine learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Self-Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Self-Attention"
        ],
        "category": "specific_connectable",
        "rationale": "The self-attention mechanism is a key component of transformers, facilitating connections to neural network architectures.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Causality-Induced Positional Encoding",
      "resolved_canonical": "Causality-Induced Positional Encoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Weighted Directed Acyclic Graph",
      "resolved_canonical": "Directed Acyclic Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Hyperbolic Space",
      "resolved_canonical": "Hyperbolic Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Self-Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16629.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16629](https://arxiv.org/abs/2509.16629)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Positional Encoding in Transformer-Based Time Series Models_ A Survey_20250922|Positional Encoding in Transformer-Based Time Series Models: A Survey]] (85.3% similar)
- [[2025-09-19/DyWPE_ Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers_20250919|DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers]] (80.3% similar)
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (79.9% similar)
- [[2025-09-23/ViTCAE_ ViT-based Class-conditioned Autoencoder_20250923|ViTCAE: ViT-based Class-conditioned Autoencoder]] (79.4% similar)
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Directed Acyclic Graph|Directed Acyclic Graph]], [[keywords/Hyperbolic Space|Hyperbolic Space]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Causality-Induced Positional Encoding|Causality-Induced Positional Encoding]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16629v1 Announce Type: new 
Abstract: Positional encoding is essential for supplementing transformer with positional information of tokens. Existing positional encoding methods demand predefined token/feature order, rendering them unsuitable for real-world data with non-sequential yet causally-related features. To address this limitation, we propose CAPE, a novel method that identifies underlying causal structure over non-sequential features as a weighted directed acyclic graph (DAG) using generalized structural equation modeling. The DAG is then embedded in hyperbolic space where its geometric structure is well-preserved using a hyperboloid model-based approach that effectively captures two important causal graph properties (causal strength & causal specificity). This step yields causality-aware positional encodings for the features, which are converted into their rotary form for integrating with transformer's self-attention mechanism. Theoretical analysis reveals that CAPE-generated rotary positional encodings possess three valuable properties for enhanced self-attention, including causal distance-induced attenuation, causal generality-induced attenuation, and robustness to positional disturbances. We evaluate CAPE over both synthetic and real-word datasets, empirically demonstrating its theoretical properties and effectiveness in enhancing transformer for data with non-sequential features. Our code is available at https://github.com/Catchxu/CAPE.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ìˆœì°¨ì ì´ì§€ë§Œ ì¸ê³¼ì ìœ¼ë¡œ ê´€ë ¨ëœ íŠ¹ì§•ì„ ê°€ì§„ ì‹¤ì œ ë°ì´í„°ì— ì í•©í•œ ìƒˆë¡œìš´ ìœ„ì¹˜ ì¸ì½”ë”© ë°©ë²•ì¸ CAPEë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CAPEëŠ” ì¼ë°˜í™”ëœ êµ¬ì¡° ë°©ì •ì‹ ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ìˆœì°¨ì  íŠ¹ì§•ì˜ ì¸ê³¼ êµ¬ì¡°ë¥¼ ê°€ì¤‘ ë°©í–¥ ë¹„ìˆœí™˜ ê·¸ë˜í”„ë¡œ ì‹ë³„í•˜ê³ , ì´ë¥¼ ìŒê³¡ ê³µê°„ì— ì„ë² ë”©í•˜ì—¬ ì¸ê³¼ ê°•ë„ì™€ íŠ¹ì´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ìƒì„±ëœ ì¸ê³¼ì„± ì¸ì‹ ìœ„ì¹˜ ì¸ì½”ë”©ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— í†µí•©ë©ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ì„ í†µí•´ CAPEê°€ ìƒì„±í•œ íšŒì „ ìœ„ì¹˜ ì¸ì½”ë”©ì´ ì¸ê³¼ ê±°ë¦¬ì™€ ì¼ë°˜ì„±ì— ë”°ë¥¸ ê°ì‡  ë° ìœ„ì¹˜ êµë€ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í¬í•¨í•œ ì„¸ ê°€ì§€ ì¤‘ìš”í•œ ì†ì„±ì„ ê°€ì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. CAPEëŠ” í•©ì„± ë° ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ë˜ì–´ ì´ë¡ ì  ì†ì„±ê³¼ íŠ¸ëœìŠ¤í¬ë¨¸ ì„±ëŠ¥ í–¥ìƒì— íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CAPEëŠ” ë¹„ìˆœì°¨ì  íŠ¹ì§•ì˜ ì¸ê³¼ êµ¬ì¡°ë¥¼ ê°€ì¤‘ì¹˜ ë°©í–¥ ë¹„ìˆœí™˜ ê·¸ë˜í”„ë¡œ ì‹ë³„í•˜ì—¬ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì— ì„ë² ë”©í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì…ë‹ˆë‹¤.
- 2. í•˜ì´í¼ë³¼ë¡œì´ë“œ ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¸ê³¼ ê·¸ë˜í”„ì˜ ë‘ ê°€ì§€ ì¤‘ìš”í•œ ì†ì„±ì¸ ì¸ê³¼ ê°•ë„ì™€ ì¸ê³¼ íŠ¹ì´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤.
- 3. CAPEê°€ ìƒì„±í•œ íšŒì „í˜• ìœ„ì¹˜ ì¸ì½”ë”©ì€ ì¸ê³¼ ê±°ë¦¬ ìœ ë„ ê°ì‡ , ì¸ê³¼ ì¼ë°˜ì„± ìœ ë„ ê°ì‡ , ìœ„ì¹˜ êµë€ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í¬í•¨í•œ ì„¸ ê°€ì§€ ì¤‘ìš”í•œ ì†ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.
- 4. CAPEëŠ” í•©ì„± ë° ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ë¹„ìˆœì°¨ì  íŠ¹ì§•ì´ ìˆëŠ” ë°ì´í„°ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ì‹¤ì¦ì ìœ¼ë¡œ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:41:26*