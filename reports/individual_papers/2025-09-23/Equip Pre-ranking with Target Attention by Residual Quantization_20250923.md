---
keywords:
  - Attention Mechanism
  - Residual Quantization
  - Pre-ranking Framework
  - Generative Models
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16931
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:38:24.310564",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "Residual Quantization",
    "Pre-ranking Framework",
    "Generative Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.85,
    "Residual Quantization": 0.78,
    "Pre-ranking Framework": 0.72,
    "Generative Models": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Target Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "TA"
        ],
        "category": "specific_connectable",
        "rationale": "Target Attention is a specific application of the Attention Mechanism, which is crucial for linking complex feature interactions in recommendation systems.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Residual Quantization",
        "canonical": "Residual Quantization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Residual Quantization is a novel technique proposed in the paper, enabling efficient pre-ranking with high accuracy, thus offering strong linking potential.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Pre-ranking Framework",
        "canonical": "Pre-ranking Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The pre-ranking framework is a unique concept introduced in the paper, crucial for understanding the proposed system's architecture.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Generative Models",
        "canonical": "Generative Models",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Generative models inspire the proposed framework, providing a broad technical context for linking related concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.5,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "efficiency",
      "effectiveness",
      "performance bottleneck",
      "state-of-the-art"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Target Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Residual Quantization",
      "resolved_canonical": "Residual Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Pre-ranking Framework",
      "resolved_canonical": "Pre-ranking Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Generative Models",
      "resolved_canonical": "Generative Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.5,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# Equip Pre-ranking with Target Attention by Residual Quantization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16931.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16931](https://arxiv.org/abs/2509.16931)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Database-Augmented Query Representation for Information Retrieval_20250922|Database-Augmented Query Representation for Information Retrieval]] (78.2% similar)
- [[2025-09-22/BiRQ_ Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition_20250922|BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition]] (78.2% similar)
- [[2025-09-18/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250918|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (78.0% similar)
- [[2025-09-22/Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings_20250922|Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings]] (78.0% similar)
- [[2025-09-19/Chain-of-Thought Re-ranking for Image Retrieval Tasks_20250919|Chain-of-Thought Re-ranking for Image Retrieval Tasks]] (77.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Generative Models|Generative Models]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Residual Quantization|Residual Quantization]], [[keywords/Pre-ranking Framework|Pre-ranking Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16931v1 Announce Type: cross 
Abstract: The pre-ranking stage in industrial recommendation systems faces a fundamental conflict between efficiency and effectiveness. While powerful models like Target Attention (TA) excel at capturing complex feature interactions in the ranking stage, their high computational cost makes them infeasible for pre-ranking, which often relies on simplistic vector-product models. This disparity creates a significant performance bottleneck for the entire system. To bridge this gap, we propose TARQ, a novel pre-ranking framework. Inspired by generative models, TARQ's key innovation is to equip pre-ranking with an architecture approximate to TA by Residual Quantization. This allows us to bring the modeling power of TA into the latency-critical pre-ranking stage for the first time, establishing a new state-of-the-art trade-off between accuracy and efficiency. Extensive offline experiments and large-scale online A/B tests at Taobao demonstrate TARQ's significant improvements in ranking performance. Consequently, our model has been fully deployed in production, serving tens of millions of daily active users and yielding substantial business improvements.

## ğŸ“ ìš”ì•½

ì‚°ì—… ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì‚¬ì „ ìˆœìœ„ ë‹¨ê³„ëŠ” íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„± ê°„ì˜ ê·¼ë³¸ì ì¸ ê°ˆë“±ì„ ê²ªìŠµë‹ˆë‹¤. ê°•ë ¥í•œ ëª¨ë¸ì¸ Target Attention(TA)ì€ ë³µì¡í•œ íŠ¹ì§• ìƒí˜¸ì‘ìš©ì„ ì˜ í¬ì°©í•˜ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ì¸í•´ ì‚¬ì „ ìˆœìœ„ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ì´ ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” TARQë¼ëŠ” ìƒˆë¡œìš´ ì‚¬ì „ ìˆœìœ„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. TARQëŠ” ìƒì„± ëª¨ë¸ì—ì„œ ì˜ê°ì„ ë°›ì•„ ì”ì—¬ ì–‘ìí™”ë¥¼ í†µí•´ TAì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ì‚¬ì „ ìˆœìœ„ì— ë„ì…í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ê°„ì˜ ìƒˆë¡œìš´ ìµœì  ê· í˜•ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. Taobaoì—ì„œì˜ ëŒ€ê·œëª¨ ì˜¨ë¼ì¸ A/B í…ŒìŠ¤íŠ¸ì™€ ì˜¤í”„ë¼ì¸ ì‹¤í—˜ì„ í†µí•´ TARQì˜ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í–ˆìœ¼ë©°, í˜„ì¬ ìˆ˜ì²œë§Œ ëª…ì˜ ì¼ì¼ í™œì„± ì‚¬ìš©ìì—ê²Œ ì ìš©ë˜ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ë¥¼ ê°œì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‚°ì—… ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì‚¬ì „ ìˆœìœ„ ë‹¨ê³„ëŠ” íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„± ê°„ì˜ ê·¼ë³¸ì ì¸ ê°ˆë“±ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
- 2. TARQëŠ” Residual Quantizationì„ í†µí•´ Target Attention(TA)ì˜ ëª¨ë¸ë§ ëŠ¥ë ¥ì„ ì‚¬ì „ ìˆœìœ„ ë‹¨ê³„ì— ë„ì…í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 3. TARQëŠ” ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ê°„ì˜ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê· í˜•ì„ í™•ë¦½í•˜ì—¬ ì„±ëŠ¥ ë³‘ëª© í˜„ìƒì„ í•´ê²°í•©ë‹ˆë‹¤.
- 4. Taobaoì—ì„œì˜ ëŒ€ê·œëª¨ ì˜¨ë¼ì¸ A/B í…ŒìŠ¤íŠ¸ì™€ ì˜¤í”„ë¼ì¸ ì‹¤í—˜ì„ í†µí•´ TARQì˜ ì„±ëŠ¥ í–¥ìƒì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. TARQ ëª¨ë¸ì€ ìƒì‚° í™˜ê²½ì— ì™„ì „íˆ ë°°í¬ë˜ì–´ ìˆ˜ì²œë§Œ ëª…ì˜ ì¼ì¼ í™œì„± ì‚¬ìš©ìì—ê²Œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©° ìƒë‹¹í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°œì„ ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:38:24*