---
keywords:
  - Natural Language Processing
  - Self-supervised Learning
  - Transformer
  - Named Entity Recognition
  - Maithili Language Corpus
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.15048
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:15:00.586755",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Natural Language Processing",
    "Self-supervised Learning",
    "Transformer",
    "Named Entity Recognition",
    "Maithili Language Corpus"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Natural Language Processing": 0.81,
    "Self-supervised Learning": 0.78,
    "Transformer": 0.8,
    "Named Entity Recognition": 0.77,
    "Maithili Language Corpus": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Natural Language Understanding",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLU"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader field of NLP, which encompasses NLU.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.81
      },
      {
        "surface": "Masked Language Modeling",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "MLM"
        ],
        "category": "specific_connectable",
        "rationale": "MLM is a common technique in self-supervised learning, relevant for language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "BERT-based language model",
        "canonical": "Transformer",
        "aliases": [
          "BERT"
        ],
        "category": "broad_technical",
        "rationale": "BERT is a specific implementation of the Transformer architecture.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Named Entity Recognition",
        "canonical": "Named Entity Recognition",
        "aliases": [
          "NER"
        ],
        "category": "specific_connectable",
        "rationale": "NER is a common downstream task in NLP, linking to practical applications.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Maithili corpus",
        "canonical": "Maithili Language Corpus",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique dataset specific to the Maithili language, crucial for language-specific research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "news classification task",
      "overall accuracy gain"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Natural Language Understanding",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Masked Language Modeling",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "BERT-based language model",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Named Entity Recognition",
      "resolved_canonical": "Named Entity Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Maithili corpus",
      "resolved_canonical": "Maithili Language Corpus",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Can maiBERT Speak for Maithili?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15048.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.15048](https://arxiv.org/abs/2509.15048)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Patent Language Model Pretraining with ModernBERT_20250918|Patent Language Model Pretraining with ModernBERT]] (80.9% similar)
- [[2025-09-23/KuBERT_ Central Kurdish BERT Model and Its Application for Sentiment Analysis_20250923|KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis]] (80.1% similar)
- [[2025-09-19/MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs]] (79.3% similar)
- [[2025-09-18/BabyHuBERT_ Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings_20250918|BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings]] (78.9% similar)
- [[2025-09-22/Tag&Tab_ Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack_20250922|Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack]] (78.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]], [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Named Entity Recognition|Named Entity Recognition]]
**âš¡ Unique Technical**: [[keywords/Maithili Language Corpus|Maithili Language Corpus]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15048v2 Announce Type: replace 
Abstract: Natural Language Understanding (NLU) for low-resource languages remains a major challenge in NLP due to the scarcity of high-quality data and language-specific models. Maithili, despite being spoken by millions, lacks adequate computational resources, limiting its inclusion in digital and AI-driven applications. To address this gap, we introducemaiBERT, a BERT-based language model pre-trained specifically for Maithili using the Masked Language Modeling (MLM) technique. Our model is trained on a newly constructed Maithili corpus and evaluated through a news classification task. In our experiments, maiBERT achieved an accuracy of 87.02%, outperforming existing regional models like NepBERTa and HindiBERT, with a 0.13% overall accuracy gain and 5-7% improvement across various classes. We have open-sourced maiBERT on Hugging Face enabling further fine-tuning for downstream tasks such as sentiment analysis and Named Entity Recognition (NER).

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì›ì´ ë¶€ì¡±í•œ ì–¸ì–´ì˜ ìì—°ì–´ ì´í•´(NLU) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Maithili ì–¸ì–´ì— íŠ¹í™”ëœ BERT ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì¸ maiBERTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MaithiliëŠ” ìˆ˜ë°±ë§Œ ëª…ì´ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ë””ì§€í„¸ ë° AI ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œì˜ í™œìš©ì´ ì œí•œì ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Masked Language Modeling(MLM) ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ Maithili ì½”í¼ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ maiBERTë¥¼ ì‚¬ì „ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ë¶„ë¥˜ ì‘ì—…ì—ì„œ maiBERTëŠ” 87.02%ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©° NepBERTaì™€ HindiBERT ë“± ê¸°ì¡´ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. maiBERTëŠ” Hugging Faceì— ê³µê°œë˜ì–´ ê°ì • ë¶„ì„ ë° ê°œì²´ëª… ì¸ì‹(NER) ë“±ì˜ í›„ì† ì‘ì—…ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Maithili ì–¸ì–´ëŠ” ìˆ˜ë°±ë§Œ ëª…ì´ ì‚¬ìš©í•˜ì§€ë§Œ, ê³ í’ˆì§ˆ ë°ì´í„°ì™€ ì–¸ì–´ ëª¨ë¸ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë””ì§€í„¸ ë° AI ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œì˜ í™œìš©ì´ ì œí•œì ì…ë‹ˆë‹¤.
- 2. maiBERTëŠ” Maithili ì–¸ì–´ë¥¼ ìœ„í•œ BERT ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ë¡œ, Masked Language Modeling ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. maiBERTëŠ” ìƒˆë¡œìš´ Maithili ì½”í¼ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, ë‰´ìŠ¤ ë¶„ë¥˜ ì‘ì—…ì—ì„œ 87.02%ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ì—¬ ê¸°ì¡´ì˜ ì§€ì—­ ëª¨ë¸ì¸ NepBERTaì™€ HindiBERTë¥¼ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.
- 4. maiBERTëŠ” Hugging Faceì— ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´, ê°ì • ë¶„ì„ ë° ê°œì²´ëª… ì¸ì‹(NER)ê³¼ ê°™ì€ í›„ì† ì‘ì—…ì„ ìœ„í•œ ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:15:00*