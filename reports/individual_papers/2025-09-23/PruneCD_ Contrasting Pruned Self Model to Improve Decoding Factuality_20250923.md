---
keywords:
  - PruneCD
  - Large Language Model
  - Contrastive Decoding
  - Hallucination Problem
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16598
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:27:48.566631",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "PruneCD",
    "Large Language Model",
    "Contrastive Decoding",
    "Hallucination Problem"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "PruneCD": 0.78,
    "Large Language Model": 0.8,
    "Contrastive Decoding": 0.77,
    "Hallucination Problem": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "PruneCD",
        "canonical": "PruneCD",
        "aliases": [
          "Pruned Contrastive Decoding"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for improving factuality in language models, offering a unique approach to contrastive decoding.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on improving factuality, connecting to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Contrastive Decoding",
        "canonical": "Contrastive Decoding",
        "aliases": [
          "Contrastive Prior"
        ],
        "category": "specific_connectable",
        "rationale": "Key technique discussed in the paper, relevant for linking to contrastive learning methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "Hallucination Problem",
        "canonical": "Hallucination Problem",
        "aliases": [
          "Model Hallucinations"
        ],
        "category": "specific_connectable",
        "rationale": "Addresses a significant challenge in language models, relevant for discussions on model reliability.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "PruneCD",
      "resolved_canonical": "PruneCD",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Contrastive Decoding",
      "resolved_canonical": "Contrastive Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Hallucination Problem",
      "resolved_canonical": "Hallucination Problem",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16598.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16598](https://arxiv.org/abs/2509.16598)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (85.1% similar)
- [[2025-09-22/Backdoor Mitigation via Invertible Pruning Masks_20250922|Backdoor Mitigation via Invertible Pruning Masks]] (82.3% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (82.2% similar)
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (81.7% similar)
- [[2025-09-22/World Modelling Improves Language Model Agents_20250922|World Modelling Improves Language Model Agents]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Contrastive Decoding|Contrastive Decoding]], [[keywords/Hallucination Problem|Hallucination Problem]]
**âš¡ Unique Technical**: [[keywords/PruneCD|PruneCD]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16598v1 Announce Type: cross 
Abstract: To mitigate the hallucination problem in large language models, DoLa exploits early exit logits from the same model as a contrastive prior. However, we found that these early exit logits tend to be flat, low in magnitude, and fail to reflect meaningful contrasts. To address this, we propose PruneCD, a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit. This design leads to more informative and well-aligned logits, enabling more effective contrastive decoding. Through qualitative and quantitative analyses, we demonstrate that PruneCD consistently improves factuality with minimal inference overhead, offering a robust and practical approach to mitigating hallucinations in LLMs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í™˜ê° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ PruneCDë¼ëŠ” ìƒˆë¡œìš´ ëŒ€ì¡°ì  ë””ì½”ë”© ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ DoLa ë°©ë²•ì€ ì¡°ê¸° ì¢…ë£Œ ë¡œì§“ì„ ëŒ€ì¡°ì  ì‚¬ì „ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ ë¡œì§“ì€ í‰íƒ„í•˜ê³  ì˜ë¯¸ ìˆëŠ” ëŒ€ì¡°ë¥¼ ì œê³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤. PruneCDëŠ” ì¡°ê¸° ì¢…ë£Œ ëŒ€ì‹  ë ˆì´ì–´ í”„ë£¨ë‹ì„ í†µí•´ ì•„ë§ˆì¶”ì–´ ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ ë” ì •ë³´ê°€ í’ë¶€í•˜ê³  ì˜ ì •ë ¬ëœ ë¡œì§“ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëŒ€ì¡°ì  ë””ì½”ë”©ì´ íš¨ê³¼ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë©°, ì§ˆì  ë° ì–‘ì  ë¶„ì„ì„ í†µí•´ PruneCDê°€ ìµœì†Œí•œì˜ ì¶”ë¡  ì˜¤ë²„í—¤ë“œë¡œ ì‚¬ì‹¤ì„±ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DoLaëŠ” ëŒ€ì¡°ì  ì‚¬ì „ìœ¼ë¡œ ì´ˆê¸° ì¢…ë£Œ ë¡œì§“ì„ í™œìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í™˜ê° ë¬¸ì œë¥¼ ì™„í™”í•˜ë ¤ê³  ì‹œë„í•˜ì˜€ìœ¼ë‚˜, ì´ˆê¸° ì¢…ë£Œ ë¡œì§“ì´ í‰íƒ„í•˜ê³  ì˜ë¯¸ ìˆëŠ” ëŒ€ì¡°ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ ë°œê²¬í•˜ì˜€ë‹¤.
- 2. PruneCDëŠ” ì´ˆê¸° ì¢…ë£Œ ëŒ€ì‹  ë ˆì´ì–´ ê°€ì§€ì¹˜ê¸°ë¥¼ í†µí•´ ì•„ë§ˆì¶”ì–´ ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ìƒˆë¡œìš´ ëŒ€ì¡°ì  ë””ì½”ë”© ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
- 3. PruneCDëŠ” ë” ì •ë³´ê°€ í’ë¶€í•˜ê³  ì˜ ì •ë ¬ëœ ë¡œì§“ì„ ìƒì„±í•˜ì—¬ ëŒ€ì¡°ì  ë””ì½”ë”©ì„ ë” íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•œë‹¤.
- 4. ì§ˆì  ë° ì–‘ì  ë¶„ì„ì„ í†µí•´ PruneCDê°€ ìµœì†Œí•œì˜ ì¶”ë¡  ì˜¤ë²„í—¤ë“œë¡œ ì‚¬ì‹¤ì„±ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ì…ì¦í•˜ì˜€ë‹¤.
- 5. PruneCDëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í™˜ê° ë¬¸ì œë¥¼ ì™„í™”í•˜ëŠ” ë° ìˆì–´ ê²¬ê³ í•˜ê³  ì‹¤ìš©ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-23 23:27:48*