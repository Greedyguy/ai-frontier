---
keywords:
  - Large Language Model
  - Trie-Based Decoding
  - Attention Mechanism
  - Memory-Constrained Environments
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2502.00085
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:47:26.475473",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Trie-Based Decoding",
    "Attention Mechanism",
    "Memory-Constrained Environments"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Trie-Based Decoding": 0.8,
    "Attention Mechanism": 0.78,
    "Memory-Constrained Environments": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on improving decoding efficiency for large-scale models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Trie-Based Decoding",
        "canonical": "Trie-Based Decoding",
        "aliases": [
          "Prefix-Tree Decoding"
        ],
        "category": "unique_technical",
        "rationale": "Describes the novel method introduced in the paper, crucial for understanding the proposed efficiency improvements.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multi-Head Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "MHA"
        ],
        "category": "specific_connectable",
        "rationale": "A key component of the architectures evaluated, linking to broader attention mechanism studies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Memory-Constrained Environments",
        "canonical": "Memory-Constrained Environments",
        "aliases": [
          "Low-Memory Settings"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the practical application context of the method, relevant for deployment scenarios.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Trie-Based Decoding",
      "resolved_canonical": "Trie-Based Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multi-Head Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Memory-Constrained Environments",
      "resolved_canonical": "Memory-Constrained Environments",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Efficient Beam Search for Large Language Models Using Trie-Based Decoding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.00085.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2502.00085](https://arxiv.org/abs/2502.00085)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (84.8% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (84.8% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (83.1% similar)
- [[2025-09-23/SEDM_ Scalable Self-Evolving Distributed Memory for Agents_20250923|SEDM: Scalable Self-Evolving Distributed Memory for Agents]] (82.3% similar)
- [[2025-09-23/LightRetriever_ A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference_20250923|LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Trie-Based Decoding|Trie-Based Decoding]], [[keywords/Memory-Constrained Environments|Memory-Constrained Environments]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.00085v2 Announce Type: replace 
Abstract: This work presents a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache across beams with common prefixes, our approach dramatically reduces memory usage and enables efficient decoding. We evaluated our method across three attention architectures, Multi-Head Attention (Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and Sliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail for abstractive summarization and HumanEval for code generation. Our experiments demonstrate substantial memory savings (4--8$\times$) and up to 2.4$\times$ faster decoding, without compromising generation quality. These results highlight our method's suitability for memory-constrained environments and large-scale deployments.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë°°ì¹˜ ê¸°ë°˜ ë¹” ì„œì¹˜ì˜ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ íŠ¸ë¼ì´ ê¸°ë°˜ ë³‘ë ¬ ë””ì½”ë”© ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê³µí†µ ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ ë¹”ë“¤ ê°„ì— ë‹¨ì¼ KV ìºì‹œë¥¼ ê³µìœ í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ ë””ì½”ë”©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. Multi-Head Attention, Grouped Query Attention, Sliding Window Attention ë“± ì„¸ ê°€ì§€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ CNN/DailyMailê³¼ HumanEval ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 4~8ë°° ì ˆê°í•˜ê³  ë””ì½”ë”© ì†ë„ë¥¼ ìµœëŒ€ 2.4ë°° í–¥ìƒì‹œì¼°ìœ¼ë©°, ìƒì„± í’ˆì§ˆì€ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ì œì•½ì´ ìˆëŠ” í™˜ê²½ê³¼ ëŒ€ê·œëª¨ ë°°í¬ì— ì í•©í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ë°°ì¹˜ ê¸°ë°˜ ë¹” ì„œì¹˜ì˜ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ íŠ¸ë¼ì´ ê¸°ë°˜ ë³‘ë ¬ ë””ì½”ë”© ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ê³µí†µ ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ ë¹” ê°„ì— ë‹¨ì¼ KV ìºì‹œë¥¼ ê³µìœ í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ ë””ì½”ë”©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. Multi-Head Attention, Grouped Query Attention, Sliding Window Attention ë“± ì„¸ ê°€ì§€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 4~8ë°° ì ˆê°í•˜ê³  ë””ì½”ë”© ì†ë„ë¥¼ ìµœëŒ€ 2.4ë°° í–¥ìƒì‹œí‚¤ë©´ì„œë„ ìƒì„± í’ˆì§ˆì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.
- 5. ì´ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ì œì•½ì´ ìˆëŠ” í™˜ê²½ê³¼ ëŒ€ê·œëª¨ ë°°í¬ì— ì í•©í•˜ë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:47:26*