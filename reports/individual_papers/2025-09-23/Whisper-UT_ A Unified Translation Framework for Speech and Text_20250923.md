---
keywords:
  - Whisper-UT Framework
  - Multimodal Machine Translation
  - Speech Translation
  - Cross-Modal Fine-Tuning
  - Lightweight Adapters
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16375
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:11:26.142101",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Whisper-UT Framework",
    "Multimodal Machine Translation",
    "Speech Translation",
    "Cross-Modal Fine-Tuning",
    "Lightweight Adapters"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Whisper-UT Framework": 0.85,
    "Multimodal Machine Translation": 0.82,
    "Speech Translation": 0.78,
    "Cross-Modal Fine-Tuning": 0.79,
    "Lightweight Adapters": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Whisper-UT",
        "canonical": "Whisper-UT Framework",
        "aliases": [
          "Whisper Unified Translation"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel framework introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "multi-modal machine translation",
        "canonical": "Multimodal Machine Translation",
        "aliases": [
          "MMT"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in the paper that connects speech and text translation, linking to multimodal learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "speech translation",
        "canonical": "Speech Translation",
        "aliases": [
          "ST"
        ],
        "category": "specific_connectable",
        "rationale": "Central task discussed in the paper, essential for linking to related translation technologies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "cross-modal fine-tuning",
        "canonical": "Cross-Modal Fine-Tuning",
        "aliases": [
          "cross-modal tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a specific technique used to enhance model performance across different modalities.",
        "novelty_score": 0.68,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "lightweight adapters",
        "canonical": "Lightweight Adapters",
        "aliases": [
          "adapters"
        ],
        "category": "broad_technical",
        "rationale": "Technical component that facilitates model adaptation, relevant to model architecture discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "system"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Whisper-UT",
      "resolved_canonical": "Whisper-UT Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "multi-modal machine translation",
      "resolved_canonical": "Multimodal Machine Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "speech translation",
      "resolved_canonical": "Speech Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "cross-modal fine-tuning",
      "resolved_canonical": "Cross-Modal Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "lightweight adapters",
      "resolved_canonical": "Lightweight Adapters",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Whisper-UT: A Unified Translation Framework for Speech and Text

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16375.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16375](https://arxiv.org/abs/2509.16375)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/AS-ASR_ A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition_20250922|AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition]] (83.7% similar)
- [[2025-09-23/Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation_20250923|Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation]] (83.1% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (81.7% similar)
- [[2025-09-23/TMD-TTS_ A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation_20250923|TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation]] (81.5% similar)
- [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Lightweight Adapters|Lightweight Adapters]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Machine Translation|Multimodal Machine Translation]], [[keywords/Speech Translation|Speech Translation]], [[keywords/Cross-Modal Fine-Tuning|Cross-Modal Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Whisper-UT Framework|Whisper-UT Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16375v1 Announce Type: new 
Abstract: Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì–‘í•œ ë‹¨ì¼ ë° ë‹¤ì¤‘ ëª¨ë‹¬ ì‹œë‚˜ë¦¬ì˜¤ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆëŠ” Whisper-UTë¼ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ê°€ë²¼ìš´ ì–´ëŒ‘í„°ë¥¼ í™œìš©í•˜ì—¬ ìŒì„± ë° í…ìŠ¤íŠ¸ ì…ë ¥ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ê³„ ë²ˆì—­(MMT) ì‘ì—…ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤. ASR ê°€ì„¤ ë˜ëŠ” ì‹¤ì œ ì „ì‚¬ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ 2ë‹¨ê³„ ë””ì½”ë”© ì „ëµì„ í†µí•´ ìŒì„± ë²ˆì—­(ST) ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°©ë²•ë¡ ì„ ê²€ì¦í–ˆìœ¼ë©°, ìœ ì‚¬í•œ ë©€í‹°íƒœìŠ¤í¬ ëª¨ë¸ì—ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. 3ë°©í–¥ ë³‘ë ¬ ë°ì´í„° ì—†ì´ë„ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” êµì°¨ ëª¨ë‹¬ ë° êµì°¨ ì‘ì—… ë¯¸ì„¸ ì¡°ì •ì˜ íš¨ê³¼ë¥¼ ê°•ì¡°í•˜ë©°, ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ì˜ ìœ ì—°ì„±ê³¼ íš¨ìœ¨ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Whisper-UTëŠ” ê²½ëŸ‰ ì–´ëŒ‘í„°ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë‹¨ì¼/ë‹¤ì¤‘ ëª¨ë‹¬ ì‹œë‚˜ë¦¬ì˜¤ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì´ ì ‘ê·¼ë²•ì€ ASR ê°€ì„¤ì´ë‚˜ ì‹¤ì œ ì „ì‚¬ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•˜ê³ , 2ë‹¨ê³„ ë””ì½”ë”© ì „ëµì„ í†µí•´ ìŒì„± ë²ˆì—­ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œì—°ë˜ì—ˆìœ¼ë©°, ìœ ì‚¬í•œ ë©€í‹°íƒœìŠ¤í¬ ëª¨ë¸ì—ë„ ì¼ë°˜ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. 3ë°©í–¥ ë³‘ë ¬ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ ì•Šì€ í¬ë¡œìŠ¤ ëª¨ë‹¬ ë° í¬ë¡œìŠ¤ íƒœìŠ¤í¬ íŒŒì¸íŠœë‹ì˜ íš¨ê³¼ê°€ ê°•ì¡°ë©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ë²ˆì—­ì— ëŒ€í•œ ìœ ì—°ì„±, íš¨ìœ¨ì„± ë° ì¼ë°˜ì  ì ìš© ê°€ëŠ¥ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:11:26*