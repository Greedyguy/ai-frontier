---
keywords:
  - Whisper-UT Framework
  - Multimodal Machine Translation
  - Speech Translation
  - Cross-Modal Fine-Tuning
  - Lightweight Adapters
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16375
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:11:26.142101",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Whisper-UT Framework",
    "Multimodal Machine Translation",
    "Speech Translation",
    "Cross-Modal Fine-Tuning",
    "Lightweight Adapters"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Whisper-UT Framework": 0.85,
    "Multimodal Machine Translation": 0.82,
    "Speech Translation": 0.78,
    "Cross-Modal Fine-Tuning": 0.79,
    "Lightweight Adapters": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Whisper-UT",
        "canonical": "Whisper-UT Framework",
        "aliases": [
          "Whisper Unified Translation"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel framework introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "multi-modal machine translation",
        "canonical": "Multimodal Machine Translation",
        "aliases": [
          "MMT"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in the paper that connects speech and text translation, linking to multimodal learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "speech translation",
        "canonical": "Speech Translation",
        "aliases": [
          "ST"
        ],
        "category": "specific_connectable",
        "rationale": "Central task discussed in the paper, essential for linking to related translation technologies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "cross-modal fine-tuning",
        "canonical": "Cross-Modal Fine-Tuning",
        "aliases": [
          "cross-modal tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a specific technique used to enhance model performance across different modalities.",
        "novelty_score": 0.68,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "lightweight adapters",
        "canonical": "Lightweight Adapters",
        "aliases": [
          "adapters"
        ],
        "category": "broad_technical",
        "rationale": "Technical component that facilitates model adaptation, relevant to model architecture discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "system"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Whisper-UT",
      "resolved_canonical": "Whisper-UT Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "multi-modal machine translation",
      "resolved_canonical": "Multimodal Machine Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "speech translation",
      "resolved_canonical": "Speech Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "cross-modal fine-tuning",
      "resolved_canonical": "Cross-Modal Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "lightweight adapters",
      "resolved_canonical": "Lightweight Adapters",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Whisper-UT: A Unified Translation Framework for Speech and Text

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16375.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16375](https://arxiv.org/abs/2509.16375)

## 🔗 유사한 논문
- [[2025-09-22/AS-ASR_ A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition_20250922|AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition]] (83.7% similar)
- [[2025-09-23/Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation_20250923|Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation]] (83.1% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (81.7% similar)
- [[2025-09-23/TMD-TTS_ A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation_20250923|TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation]] (81.5% similar)
- [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (81.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Lightweight Adapters|Lightweight Adapters]]
**🔗 Specific Connectable**: [[keywords/Multimodal Machine Translation|Multimodal Machine Translation]], [[keywords/Speech Translation|Speech Translation]], [[keywords/Cross-Modal Fine-Tuning|Cross-Modal Fine-Tuning]]
**⚡ Unique Technical**: [[keywords/Whisper-UT Framework|Whisper-UT Framework]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16375v1 Announce Type: new 
Abstract: Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.

## 📝 요약

이 논문은 다양한 단일 및 다중 모달 시나리오에 효율적으로 적응할 수 있는 Whisper-UT라는 통합 프레임워크를 제안합니다. 이 프레임워크는 가벼운 어댑터를 활용하여 음성 및 텍스트 입력을 동시에 처리할 수 있으며, 특히 다중 모달 기계 번역(MMT) 작업에서 효과적입니다. ASR 가설 또는 실제 전사를 프롬프트로 사용하여 2단계 디코딩 전략을 통해 음성 번역(ST) 성능을 향상시킵니다. Whisper 모델을 사용하여 방법론을 검증했으며, 유사한 멀티태스크 모델에도 적용 가능합니다. 3방향 병렬 데이터 없이도 성능을 개선하는 교차 모달 및 교차 작업 미세 조정의 효과를 강조하며, 제안된 프레임워크의 유연성과 효율성을 입증합니다.

## 🎯 주요 포인트

- 1. Whisper-UT는 경량 어댑터를 활용하여 다양한 단일/다중 모달 시나리오에 효율적으로 적응할 수 있는 통합 프레임워크입니다.
- 2. 이 접근법은 ASR 가설이나 실제 전사를 프롬프트로 사용하여 두 가지 모달리티를 동시에 처리할 수 있게 하고, 2단계 디코딩 전략을 통해 음성 번역 성능을 향상시킵니다.
- 3. 제안된 방법은 Whisper 모델을 사용하여 시연되었으며, 유사한 멀티태스크 모델에도 일반적으로 적용될 수 있습니다.
- 4. 3방향 병렬 데이터가 필요하지 않은 크로스 모달 및 크로스 태스크 파인튜닝의 효과가 강조됩니다.
- 5. 제안된 프레임워크는 다중 모달 번역에 대한 유연성, 효율성 및 일반적 적용 가능성을 갖추고 있습니다.


---

*Generated on 2025-09-24 03:11:26*