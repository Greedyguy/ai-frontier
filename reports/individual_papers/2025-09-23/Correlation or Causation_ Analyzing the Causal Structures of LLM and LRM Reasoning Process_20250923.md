---
keywords:
  - Large Language Model
  - Reinforcement Learning
  - Structural Causal Model
  - Causal Reasoning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17380
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:59:20.487646",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning",
    "Structural Causal Model",
    "Causal Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning": 0.8,
    "Structural Causal Model": 0.78,
    "Causal Reasoning": 0.84
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the paper's focus on reasoning processes and is a key concept in AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "specific_connectable",
        "rationale": "Reinforcement Learning is a significant method discussed in the paper for enhancing causal reasoning in models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Structural Causal Models",
        "canonical": "Structural Causal Model",
        "aliases": [
          "SCM"
        ],
        "category": "unique_technical",
        "rationale": "SCMs are a unique technical concept used in the paper to analyze causal structures, offering new insights.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Causal Reasoning",
        "canonical": "Causal Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Causal reasoning is a core theme of the paper, crucial for understanding model improvements discussed.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.84
      }
    ],
    "ban_list_suggestions": [
      "unfaithfulness",
      "bias",
      "inconsistency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Structural Causal Models",
      "resolved_canonical": "Structural Causal Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Causal Reasoning",
      "resolved_canonical": "Causal Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.84
      }
    }
  ]
}
-->

# Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17380.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17380](https://arxiv.org/abs/2509.17380)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (87.4% similar)
- [[2025-09-19/Causal-Counterfactual RAG_ The Integration of Causal-Counterfactual Reasoning into RAG_20250919|Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG]] (85.6% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.2% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (85.1% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Causal Reasoning|Causal Reasoning]]
**âš¡ Unique Technical**: [[keywords/Structural Causal Model|Structural Causal Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17380v1 Announce Type: new 
Abstract: LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ì—°ì†ì  í•™ìŠµ ëª¨ë¸(LRM)ì˜ ì¸ê³¼ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. LLMì€ í”¼ìƒì  ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ë¶ˆì¶©ì‹¤ì„±, í¸í–¥, ë¹„ì¼ê´€ì„± ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. ì´ì— ë¹„í•´ ê°•í™” í•™ìŠµ ë° ì¦ë¥˜ ê¸°ë²•ì„ ì‚¬ìš©í•œ RLVR í›ˆë ¨ LRMì€ ì¸ê³¼ì  êµ¬ì¡°ì™€ ë” ì˜ ì¼ì¹˜í•˜ë©°, ë¶ˆí•„ìš”í•œ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ê³  ì§„ì •í•œ ì¸ê³¼ íŒ¨í„´ì„ ê°•í™”í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì™„í™”í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” RLVRì˜ ì¸ê³¼ì  ì¶”ë¡  í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ ê°•ì¡°í•˜ë©°, í–¥í›„ AI ì‹œìŠ¤í…œ ì„¤ê³„ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMsëŠ” í”¼ìƒì ì¸ ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ì‹ ë¢°ì„±, í¸í–¥, ì¼ê´€ì„± ë¬¸ì œë¥¼ ê²ªê³  ìˆë‹¤.
- 2. ê°•í™” í•™ìŠµ(RL)ê³¼ ì¦ë¥˜ ê¸°ë²•ì„ í™œìš©í•œ LRMsëŠ” ê³¼ì œ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ë©° ìœ ë§í•œ ëŒ€ì•ˆìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼, RLVRë¡œ í›ˆë ¨ëœ LRMsëŠ” ì´ìƒì ì¸ ì¸ê³¼ êµ¬ì¡°ì— ë” ê°€ê¹ê²Œ ì •ë ¬ë˜ì–´ ì¸ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì´ í–¥ìƒë˜ì—ˆë‹¤.
- 4. RLVRëŠ” ê±°ì§“ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ê³  ì§„ì •í•œ ì¸ê³¼ íŒ¨í„´ì„ ê°•í™”í•˜ì—¬ ì‹ ë¢°ì„± ë¶€ì¡±ê³¼ í¸í–¥ì„ ì™„í™”í•œë‹¤.
- 5. ì—°êµ¬ëŠ” ì¸ê³¼ ê´€ê³„ ì´í•´ì— ê¸°ì—¬í•˜ë©°, RLVRì˜ ì¸ê³¼ ì¶”ë¡  ê°•í™”ì—ì„œì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ìˆë‹¤.


---

*Generated on 2025-09-23 22:59:20*