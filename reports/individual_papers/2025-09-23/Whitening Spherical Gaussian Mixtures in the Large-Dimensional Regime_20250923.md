---
keywords:
  - Whitening
  - Gaussian Mixture Model
  - Random Matrix Theory
  - Latent Variable Models
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17636
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:24:44.682879",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Whitening",
    "Gaussian Mixture Model",
    "Random Matrix Theory",
    "Latent Variable Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Whitening": 0.78,
    "Gaussian Mixture Model": 0.8,
    "Random Matrix Theory": 0.77,
    "Latent Variable Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Whitening",
        "canonical": "Whitening",
        "aliases": [
          "Data Whitening",
          "Whitening Transformation"
        ],
        "category": "unique_technical",
        "rationale": "Whitening is a key technique discussed in the paper, crucial for understanding the transformation of data in high-dimensional spaces.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Spherical Gaussian Mixture Model",
        "canonical": "Gaussian Mixture Model",
        "aliases": [
          "GMM",
          "Spherical GMM"
        ],
        "category": "broad_technical",
        "rationale": "Gaussian Mixture Models are fundamental in statistics and machine learning, providing a basis for linking to broader concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Random Matrix Theory",
        "canonical": "Random Matrix Theory",
        "aliases": [
          "RMT"
        ],
        "category": "specific_connectable",
        "rationale": "Random Matrix Theory is essential for the mathematical framework used in the paper, facilitating connections to theoretical foundations.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Latent Variable Models",
        "canonical": "Latent Variable Models",
        "aliases": [
          "LVM"
        ],
        "category": "specific_connectable",
        "rationale": "Latent Variable Models are central to the paper's discussion on estimation techniques, linking to a wide range of statistical methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Large-Dimensional Regime",
      "Sample Covariance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Whitening",
      "resolved_canonical": "Whitening",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Spherical Gaussian Mixture Model",
      "resolved_canonical": "Gaussian Mixture Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Random Matrix Theory",
      "resolved_canonical": "Random Matrix Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Latent Variable Models",
      "resolved_canonical": "Latent Variable Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17636.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17636](https://arxiv.org/abs/2509.17636)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (81.8% similar)
- [[2025-09-23/Regularizing Extrapolation in Causal Inference_20250923|Regularizing Extrapolation in Causal Inference]] (79.8% similar)
- [[2025-09-22/Manifold Dimension Estimation_ An Empirical Study_20250922|Manifold Dimension Estimation: An Empirical Study]] (79.8% similar)
- [[2025-09-18/Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis_20250918|Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis]] (79.6% similar)
- [[2025-09-22/MCGA_ Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention_20250922|MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention]] (79.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Gaussian Mixture Model|Gaussian Mixture Model]]
**ğŸ”— Specific Connectable**: [[keywords/Random Matrix Theory|Random Matrix Theory]], [[keywords/Latent Variable Models|Latent Variable Models]]
**âš¡ Unique Technical**: [[keywords/Whitening|Whitening]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17636v1 Announce Type: cross 
Abstract: Whitening is a classical technique in unsupervised learning that can facilitate estimation tasks by standardizing data. An important application is the estimation of latent variable models via the decomposition of tensors built from high-order moments. In particular, whitening orthogonalizes the means of a spherical Gaussian mixture model (GMM), thereby making the corresponding moment tensor orthogonally decomposable, hence easier to decompose. However, in the large-dimensional regime (LDR) where data are high-dimensional and scarce, the standard whitening matrix built from the sample covariance becomes ineffective because the latter is spectrally distorted. Consequently, whitened means of a spherical GMM are no longer orthogonal. Using random matrix theory, we derive exact limits for their dot products, which are generally nonzero in the LDR. As our main contribution, we then construct a corrected whitening matrix that restores asymptotic orthogonality, allowing for performance gains in spherical GMM estimation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³ ì°¨ ëª¨ë©˜íŠ¸ë¡œ êµ¬ì„±ëœ í…ì„œì˜ ë¶„í•´ë¥¼ í†µí•´ ì ì¬ ë³€ìˆ˜ ëª¨ë¸ì„ ì¶”ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê³ ì „ì ì¸ ê¸°ë²•ì¸ í™”ì´íŠ¸ë‹(Whitening)ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, ê³ ì°¨ì› ì €í‘œë³¸ í™˜ê²½ì—ì„œëŠ” í‘œë³¸ ê³µë¶„ì‚°ìœ¼ë¡œ êµ¬ì¶•ëœ ê¸°ì¡´ì˜ í™”ì´íŠ¸ë‹ í–‰ë ¬ì´ ë¹„íš¨ìœ¨ì ì´ë©°, ì´ë¡œ ì¸í•´ êµ¬í˜• ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(GMM)ì˜ í‰ê· ì´ ì§êµì„±ì„ ìƒê²Œ ë©ë‹ˆë‹¤. ì €ìëŠ” ëœë¤ í–‰ë ¬ ì´ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ í‰ê· ì˜ ë‚´ì ì— ëŒ€í•œ ì •í™•í•œ í•œê³„ë¥¼ ë„ì¶œí•˜ê³ , ë¹„ëŒ€ì¹­ì„±ì„ ë³µì›í•˜ì—¬ êµ¬í˜• GMM ì¶”ì • ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìˆ˜ì •ëœ í™”ì´íŠ¸ë‹ í–‰ë ¬ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Whiteningì€ ë°ì´í„° í‘œì¤€í™”ë¥¼ í†µí•´ ì¶”ì • ì‘ì—…ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµì˜ ê³ ì „ì ì¸ ê¸°ë²•ì´ë‹¤.
- 2. ê³ ì°¨ ëª¨ë©˜íŠ¸ë¡œ êµ¬ì„±ëœ í…ì„œì˜ ë¶„í•´ë¥¼ í†µí•´ ì ì¬ ë³€ìˆ˜ ëª¨ë¸ì„ ì¶”ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- 3. í‘œë³¸ ê³µë¶„ì‚°ìœ¼ë¡œ êµ¬ì„±ëœ í‘œì¤€ whitening í–‰ë ¬ì€ ê³ ì°¨ì› ë°ì´í„° í™˜ê²½ì—ì„œ ë¹„íš¨ê³¼ì ì´ë‹¤.
- 4. ëœë¤ í–‰ë ¬ ì´ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì°¨ì› í™˜ê²½ì—ì„œ ë¹„ì§êµì ì¸ ì ê³±ì˜ ì •í™•í•œ í•œê³„ë¥¼ ë„ì¶œí•˜ì˜€ë‹¤.
- 5. ìˆ˜ì •ëœ whitening í–‰ë ¬ì„ êµ¬ì„±í•˜ì—¬ êµ¬í˜• GMM ì¶”ì •ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.


---

*Generated on 2025-09-24 02:24:44*