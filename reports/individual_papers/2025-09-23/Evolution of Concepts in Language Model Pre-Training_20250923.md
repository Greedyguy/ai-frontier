---
keywords:
  - Large Language Model
  - Transformer
  - Crosscoders
  - Feature Evolution
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17196
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:48:05.944934",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Transformer",
    "Crosscoders",
    "Feature Evolution"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Transformer": 0.8,
    "Crosscoders": 0.7,
    "Feature Evolution": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LM",
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on pre-training and feature evolution.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Key architecture discussed in relation to feature learning phases.",
        "novelty_score": 0.3,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "crosscoders",
        "canonical": "Crosscoders",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific method introduced for tracking feature evolution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "feature evolution",
        "canonical": "Feature Evolution",
        "aliases": [
          "feature dynamics"
        ],
        "category": "unique_technical",
        "rationale": "Describes the process observed and analyzed in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "pre-training",
      "snapshot",
      "black box"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "crosscoders",
      "resolved_canonical": "Crosscoders",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "feature evolution",
      "resolved_canonical": "Feature Evolution",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Evolution of Concepts in Language Model Pre-Training

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17196.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17196](https://arxiv.org/abs/2509.17196)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (82.0% similar)
- [[2025-09-17/Language models' activations linearly encode training-order recency_20250917|Language models' activations linearly encode training-order recency]] (81.9% similar)
- [[2025-09-23/Rethinking the Role of Text Complexity in Language Model Pretraining_20250923|Rethinking the Role of Text Complexity in Language Model Pretraining]] (81.2% similar)
- [[2025-09-22/Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation_20250922|Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation]] (81.1% similar)
- [[2025-09-22/Modeling Transformers as complex networks to analyze learning dynamics_20250922|Modeling Transformers as complex networks to analyze learning dynamics]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**âš¡ Unique Technical**: [[keywords/Crosscoders|Crosscoders]], [[keywords/Feature Evolution|Feature Evolution]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17196v1 Announce Type: cross 
Abstract: Language models obtain extensive capabilities through pre-training. However, the pre-training process remains a black box. In this work, we track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders. We find that most features begin to form around a specific point, while more complex patterns emerge in later training stages. Feature attribution analyses reveal causal connections between feature evolution and downstream performance. Our feature-level observations are highly consistent with previous findings on Transformer's two-stage learning process, which we term a statistical learning phase and a feature learning phase. Our work opens up the possibility to track fine-grained representation progress during language model learning dynamics.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ ê³¼ì •ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ í¬ì†Œ ì‚¬ì „ í•™ìŠµ ë°©ë²•ì¸ crosscodersë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜• í•´ì„ ê°€ëŠ¥í•œ íŠ¹ì§•ì˜ ì§„í™”ë¥¼ ì¶”ì í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ëŒ€ë¶€ë¶„ì˜ íŠ¹ì§•ì€ íŠ¹ì • ì‹œì ì—ì„œ í˜•ì„±ë˜ê¸° ì‹œì‘í•˜ë©°, ë³µì¡í•œ íŒ¨í„´ì€ í›ˆë ¨ í›„ë°˜ë¶€ì— ë‚˜íƒ€ë‚©ë‹ˆë‹¤. íŠ¹ì§• ê·€ì† ë¶„ì„ì„ í†µí•´ íŠ¹ì§• ì§„í™”ì™€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ ê°„ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì§• ìˆ˜ì¤€ì˜ ê´€ì°°ì€ Transformerì˜ ë‘ ë‹¨ê³„ í•™ìŠµ ê³¼ì •ì¸ í†µê³„ì  í•™ìŠµ ë‹¨ê³„ì™€ íŠ¹ì§• í•™ìŠµ ë‹¨ê³„ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–¸ì–´ ëª¨ë¸ í•™ìŠµì˜ ì„¸ë°€í•œ í‘œí˜„ ì§„í–‰ì„ ì¶”ì í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸ì€ ì‚¬ì „ í›ˆë ¨ì„ í†µí•´ ê´‘ë²”ìœ„í•œ ëŠ¥ë ¥ì„ ì–»ì§€ë§Œ, ê·¸ ê³¼ì •ì€ ì—¬ì „íˆ ë¸”ë™ë°•ìŠ¤ë¡œ ë‚¨ì•„ ìˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” crosscodersë¼ëŠ” í¬ì†Œ ì‚¬ì „ í•™ìŠµ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ ìŠ¤ëƒ…ìƒ· ê°„ì˜ ì„ í˜• í•´ì„ ê°€ëŠ¥í•œ íŠ¹ì§• ì§„í™”ë¥¼ ì¶”ì í•œë‹¤.
- 3. ëŒ€ë¶€ë¶„ì˜ íŠ¹ì§•ì€ íŠ¹ì • ì‹œì ì—ì„œ í˜•ì„±ë˜ê¸° ì‹œì‘í•˜ë©°, ë” ë³µì¡í•œ íŒ¨í„´ì€ í›ˆë ¨ í›„ë°˜ ë‹¨ê³„ì—ì„œ ë‚˜íƒ€ë‚œë‹¤.
- 4. íŠ¹ì§• ê·€ì† ë¶„ì„ì„ í†µí•´ íŠ¹ì§• ì§„í™”ì™€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ ê°„ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ ë°í˜€ë‚¸ë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ì„¸ë°€í•œ í‘œí˜„ì˜ ë°œì „ì„ ì¶”ì í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤€ë‹¤.


---

*Generated on 2025-09-23 23:48:05*