---
keywords:
  - Large Language Model
  - AI Alignment
  - Human Benchmarking
  - Value Prioritization
  - Human-Centric Models
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2506.12617
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:27:41.128108",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "AI Alignment",
    "Human Benchmarking",
    "Value Prioritization",
    "Human-Centric Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "AI Alignment": 0.9,
    "Human Benchmarking": 0.82,
    "Value Prioritization": 0.8,
    "Human-Centric Models": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "A core technology discussed in the paper, crucial for linking to related AI alignment and evaluation research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "AI Alignment",
        "canonical": "AI Alignment",
        "aliases": [
          "Alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Central theme of the paper, connecting to broader discussions on aligning AI systems with human values.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Human Benchmarking",
        "canonical": "Human Benchmarking",
        "aliases": [
          "Human Evaluation"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the method of comparing AI outputs to human judgments, relevant for linking to evaluation methodologies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Value Prioritization",
        "canonical": "Value Prioritization",
        "aliases": [
          "Value Ranking"
        ],
        "category": "unique_technical",
        "rationale": "Describes the process of ranking values, which is key to understanding AI behavior in the study.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "Human-Centric Models",
        "canonical": "Human-Centric Models",
        "aliases": [
          "Human-Oriented Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Differentiates models that align closely with human values, important for discussions on model design.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "PAPERS",
      "Study 1",
      "Study 2",
      "Study 3"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "AI Alignment",
      "resolved_canonical": "AI Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Human Benchmarking",
      "resolved_canonical": "Human Benchmarking",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Value Prioritization",
      "resolved_canonical": "Value Prioritization",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Human-Centric Models",
      "resolved_canonical": "Human-Centric Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Evaluating AI Alignment in Eleven LLMs through Output-Based Analysis and Human Benchmarking

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.12617.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2506.12617](https://arxiv.org/abs/2506.12617)

## 🔗 유사한 논문
- [[2025-09-18/Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs_20250918|Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs]] (86.9% similar)
- [[2025-09-23/Evaluating Behavioral Alignment in Conflict Dialogue_ A Multi-Dimensional Comparison of LLM Agents and Humans_20250923|Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans]] (86.7% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (85.1% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (84.2% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (84.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/AI Alignment|AI Alignment]]
**⚡ Unique Technical**: [[keywords/Human Benchmarking|Human Benchmarking]], [[keywords/Value Prioritization|Value Prioritization]]
**🚀 Evolved Concepts**: [[keywords/Human-Centric Models|Human-Centric Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.12617v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used in psychological research and practice, yet traditional benchmarks reveal little about the values they express in real interaction. We introduce PAPERS, an output-based evaluation of the values LLMs prioritise in their text. Study 1 thematically analysed responses from eleven LLMs, identifying five recurring dimensions (Purposeful Contribution, Adaptive Growth, Positive Relationality, Ethical Integrity, and Robust Functionality) with Self-Actualised Autonomy appearing only under a hypothetical sentience prompt. These results suggest that LLMs are trained to prioritise humanistic and utility values as dual objectives of optimal functioning, a pattern supported by existing AI alignment and prioritisation frameworks. Study 2 operationalised PAPERS as a ranking instrument across the same eleven LLMs, yielding stable, non-random value priorities alongside systematic between-model differences. Hierarchical clustering distinguished "human-centric" models (e.g., ChatGPT-4o, Claude Sonnet 4) that prioritised relational/ethical values from "utility-driven" models (e.g., Llama 4, Gemini 2.5 Pro) that emphasised operational priorities. Study 3 benchmarked four LLMs against human judgements (N = 376) under matched prompts, finding near-perfect rank-order convergence (r = .97-.98) but moderate absolute agreement; among tested models, ChatGPT-4o showed the closest alignment with human ratings (ICC = .78). Humans also showed limited readiness to endorse sentient AI systems. Taken together, PAPERS enabled systematic value audits and revealed trade-offs with direct implications for deployment: human-centric models aligned more closely with human value judgments and appear better suited for humanistic psychological applications, whereas utility-driven models emphasised functional efficiency and may be more appropriate for instrumental or back-office tasks.

## 📝 요약

이 논문은 대형 언어 모델(LLM)이 실제 상호작용에서 표현하는 가치를 평가하기 위해 PAPERS라는 평가 방법을 제안합니다. 연구 1에서는 11개의 LLM을 분석하여 다섯 가지 가치 차원(목적 있는 기여, 적응적 성장, 긍정적 관계성, 윤리적 무결성, 견고한 기능성)을 식별했습니다. 연구 2에서는 PAPERS를 통해 모델 간의 가치 우선순위를 평가했으며, 인간 중심 모델(예: ChatGPT-4o)이 관계적/윤리적 가치를, 효용 중심 모델(예: Llama 4)이 기능적 우선순위를 강조하는 것으로 나타났습니다. 연구 3에서는 LLM과 인간의 가치 판단을 비교하여 ChatGPT-4o가 인간 평가와 가장 잘 일치함을 발견했습니다. 이 연구는 인간 중심 모델이 심리학적 응용에 적합하고, 효용 중심 모델이 기능적 효율성을 중시하는 업무에 적합함을 시사합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 심리학 연구 및 실무에서 점점 더 많이 사용되고 있으며, 이들의 텍스트에서 표현되는 가치 우선순위를 평가하기 위해 PAPERS라는 평가 방법이 도입되었습니다.
- 2. 연구 1에서는 11개의 LLM을 분석하여 다섯 가지 반복적인 가치 차원(목적 있는 기여, 적응적 성장, 긍정적 관계성, 윤리적 무결성, 견고한 기능성)을 식별하였으며, 자율성은 가상의 감성 프롬프트에서만 나타났습니다.
- 3. 연구 2에서는 PAPERS를 랭킹 도구로 사용하여 모델 간의 가치 우선순위를 평가했으며, 인간 중심 모델과 효용 중심 모델 간의 체계적인 차이를 발견했습니다.
- 4. 연구 3에서는 인간의 판단과 비교하여 LLM의 순위 일치도를 평가했으며, ChatGPT-4o가 인간의 가치 판단과 가장 가까운 일치를 보였습니다.
- 5. PAPERS를 통해 인간 중심 모델은 인간의 가치 판단과 더 잘 맞아 심리학적 응용에 적합하며, 효용 중심 모델은 기능적 효율성을 강조하여 도구적 또는 백오피스 작업에 적합하다는 결론을 도출했습니다.


---

*Generated on 2025-09-24 00:27:41*