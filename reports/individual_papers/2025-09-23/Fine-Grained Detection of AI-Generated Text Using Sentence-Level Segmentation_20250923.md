---
keywords:
  - Transformer
  - Conditional Random Fields
  - Neural Network
  - Zero-Shot Learning
  - Sentence-Level Segmentation
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17830
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:10:35.496253",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Conditional Random Fields",
    "Neural Network",
    "Zero-Shot Learning",
    "Sentence-Level Segmentation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Conditional Random Fields": 0.78,
    "Neural Network": 0.8,
    "Zero-Shot Learning": 0.82,
    "Sentence-Level Segmentation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer models",
        "canonical": "Transformer",
        "aliases": [
          "Transformers",
          "Transformer architecture"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the model's architecture, linking to a broad range of AI applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Conditional Random Fields",
        "canonical": "Conditional Random Fields",
        "aliases": [
          "CRFs"
        ],
        "category": "specific_connectable",
        "rationale": "CRFs enhance sequence recognition, crucial for linking to sequence labeling tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "NN"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are fundamental to the model's architecture, providing connectivity to various AI methodologies.",
        "novelty_score": 0.25,
        "connectivity_score": 0.85,
        "specificity_score": 0.55,
        "link_intent_score": 0.8
      },
      {
        "surface": "Zero-shot detectors",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot detection"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a trending approach in AI, linking to models that generalize without task-specific data.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Sentence-level segmentation",
        "canonical": "Sentence-Level Segmentation",
        "aliases": [
          "Sentence segmentation"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach for detecting AI-generated text, linking to text analysis and segmentation techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "AI-generated text",
      "human-written text",
      "document-level classification"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Conditional Random Fields",
      "resolved_canonical": "Conditional Random Fields",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.85,
        "specificity": 0.55,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Zero-shot detectors",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Sentence-level segmentation",
      "resolved_canonical": "Sentence-Level Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17830.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17830](https://arxiv.org/abs/2509.17830)

## 🔗 유사한 논문
- [[2025-09-22/DNA-DetectLLM_ Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm_20250922|DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm]] (88.2% similar)
- [[2025-09-22/Real, Fake, or Manipulated? Detecting Machine-Influenced Text_20250922|Real, Fake, or Manipulated? Detecting Machine-Influenced Text]] (83.1% similar)
- [[2025-09-22/SENTRA_ Selected-Next-Token Transformer for LLM Text Detection_20250922|SENTRA: Selected-Next-Token Transformer for LLM Text Detection]] (82.6% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (82.4% similar)
- [[2025-09-22/The Great AI Witch Hunt_ Reviewers Perception and (Mis)Conception of Generative AI in Research Writing_20250922|The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing]] (82.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Neural Network|Neural Network]]
**🔗 Specific Connectable**: [[keywords/Conditional Random Fields|Conditional Random Fields]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Sentence-Level Segmentation|Sentence-Level Segmentation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17830v1 Announce Type: cross 
Abstract: Generation of Artificial Intelligence (AI) texts in important works has become a common practice that can be used to misuse and abuse AI at various levels. Traditional AI detectors often rely on document-level classification, which struggles to identify AI content in hybrid or slightly edited texts designed to avoid detection, leading to concerns about the model's efficiency, which makes it hard to distinguish between human-written and AI-generated texts. A sentence-level sequence labeling model proposed to detect transitions between human- and AI-generated text, leveraging nuanced linguistic signals overlooked by document-level classifiers. By this method, detecting and segmenting AI and human-written text within a single document at the token-level granularity is achieved. Our model combines the state-of-the-art pre-trained Transformer models, incorporating Neural Networks (NN) and Conditional Random Fields (CRFs). This approach extends the power of transformers to extract semantic and syntactic patterns, and the neural network component to capture enhanced sequence-level representations, thereby improving the boundary predictions by the CRF layer, which enhances sequence recognition and further identification of the partition between Human- and AI-generated texts. The evaluation is performed on two publicly available benchmark datasets containing collaborative human and AI-generated texts. Our experimental comparisons are with zero-shot detectors and the existing state-of-the-art models, along with rigorous ablation studies to justify that this approach, in particular, can accurately detect the spans of AI texts in a completely collaborative text. All our source code and the processed datasets are available in our GitHub repository.

## 📝 요약

이 논문은 AI 생성 텍스트의 오용을 방지하기 위해 문서 내 인간과 AI 생성 텍스트의 전환을 문장 수준에서 감지하는 모델을 제안합니다. 기존의 문서 수준 AI 탐지기는 혼합된 텍스트에서 AI 콘텐츠를 식별하는 데 어려움을 겪습니다. 제안된 모델은 Transformer, 신경망(NN), 조건부 랜덤 필드(CRF)를 결합하여 문서 내에서 인간과 AI 텍스트를 토큰 수준으로 구분합니다. 두 개의 공개 데이터셋을 통해 평가한 결과, 이 방법이 기존 모델보다 정확하게 AI 텍스트를 식별할 수 있음을 입증했습니다. 소스 코드는 GitHub에서 제공됩니다.

## 🎯 주요 포인트

- 1. 전통적인 AI 탐지기는 문서 수준의 분류에 의존하여 혼합된 텍스트에서 AI 콘텐츠를 식별하는 데 어려움을 겪습니다.
- 2. 제안된 모델은 문장 수준의 시퀀스 레이블링을 통해 인간과 AI 생성 텍스트 간의 전환을 탐지합니다.
- 3. 이 모델은 최신 사전 학습된 Transformer 모델과 신경망, 조건부 랜덤 필드를 결합하여 문서 내 AI 및 인간 작성 텍스트를 토큰 수준에서 구분합니다.
- 4. 두 개의 공개 벤치마크 데이터셋을 사용한 평가에서 제안된 접근법이 AI 텍스트의 범위를 정확하게 탐지할 수 있음을 입증했습니다.
- 5. 모든 소스 코드와 처리된 데이터셋은 GitHub 저장소에서 공개되어 있습니다.


---

*Generated on 2025-09-24 00:10:35*