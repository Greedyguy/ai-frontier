---
keywords:
  - Transformer
  - Conditional Random Fields
  - Neural Network
  - Zero-Shot Learning
  - Sentence-Level Segmentation
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17830
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:10:35.496253",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Conditional Random Fields",
    "Neural Network",
    "Zero-Shot Learning",
    "Sentence-Level Segmentation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Conditional Random Fields": 0.78,
    "Neural Network": 0.8,
    "Zero-Shot Learning": 0.82,
    "Sentence-Level Segmentation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer models",
        "canonical": "Transformer",
        "aliases": [
          "Transformers",
          "Transformer architecture"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the model's architecture, linking to a broad range of AI applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Conditional Random Fields",
        "canonical": "Conditional Random Fields",
        "aliases": [
          "CRFs"
        ],
        "category": "specific_connectable",
        "rationale": "CRFs enhance sequence recognition, crucial for linking to sequence labeling tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "NN"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are fundamental to the model's architecture, providing connectivity to various AI methodologies.",
        "novelty_score": 0.25,
        "connectivity_score": 0.85,
        "specificity_score": 0.55,
        "link_intent_score": 0.8
      },
      {
        "surface": "Zero-shot detectors",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot detection"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a trending approach in AI, linking to models that generalize without task-specific data.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Sentence-level segmentation",
        "canonical": "Sentence-Level Segmentation",
        "aliases": [
          "Sentence segmentation"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach for detecting AI-generated text, linking to text analysis and segmentation techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "AI-generated text",
      "human-written text",
      "document-level classification"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Conditional Random Fields",
      "resolved_canonical": "Conditional Random Fields",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.85,
        "specificity": 0.55,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Zero-shot detectors",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Sentence-level segmentation",
      "resolved_canonical": "Sentence-Level Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17830.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17830](https://arxiv.org/abs/2509.17830)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DNA-DetectLLM_ Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm_20250922|DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm]] (88.2% similar)
- [[2025-09-22/Real, Fake, or Manipulated? Detecting Machine-Influenced Text_20250922|Real, Fake, or Manipulated? Detecting Machine-Influenced Text]] (83.1% similar)
- [[2025-09-22/SENTRA_ Selected-Next-Token Transformer for LLM Text Detection_20250922|SENTRA: Selected-Next-Token Transformer for LLM Text Detection]] (82.6% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (82.4% similar)
- [[2025-09-22/The Great AI Witch Hunt_ Reviewers Perception and (Mis)Conception of Generative AI in Research Writing_20250922|The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Conditional Random Fields|Conditional Random Fields]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Sentence-Level Segmentation|Sentence-Level Segmentation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17830v1 Announce Type: cross 
Abstract: Generation of Artificial Intelligence (AI) texts in important works has become a common practice that can be used to misuse and abuse AI at various levels. Traditional AI detectors often rely on document-level classification, which struggles to identify AI content in hybrid or slightly edited texts designed to avoid detection, leading to concerns about the model's efficiency, which makes it hard to distinguish between human-written and AI-generated texts. A sentence-level sequence labeling model proposed to detect transitions between human- and AI-generated text, leveraging nuanced linguistic signals overlooked by document-level classifiers. By this method, detecting and segmenting AI and human-written text within a single document at the token-level granularity is achieved. Our model combines the state-of-the-art pre-trained Transformer models, incorporating Neural Networks (NN) and Conditional Random Fields (CRFs). This approach extends the power of transformers to extract semantic and syntactic patterns, and the neural network component to capture enhanced sequence-level representations, thereby improving the boundary predictions by the CRF layer, which enhances sequence recognition and further identification of the partition between Human- and AI-generated texts. The evaluation is performed on two publicly available benchmark datasets containing collaborative human and AI-generated texts. Our experimental comparisons are with zero-shot detectors and the existing state-of-the-art models, along with rigorous ablation studies to justify that this approach, in particular, can accurately detect the spans of AI texts in a completely collaborative text. All our source code and the processed datasets are available in our GitHub repository.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ AI ìƒì„± í…ìŠ¤íŠ¸ì˜ ì˜¤ìš©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë¬¸ì„œ ë‚´ ì¸ê°„ê³¼ AI ìƒì„± í…ìŠ¤íŠ¸ì˜ ì „í™˜ì„ ë¬¸ì¥ ìˆ˜ì¤€ì—ì„œ ê°ì§€í•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¬¸ì„œ ìˆ˜ì¤€ AI íƒì§€ê¸°ëŠ” í˜¼í•©ëœ í…ìŠ¤íŠ¸ì—ì„œ AI ì½˜í…ì¸ ë¥¼ ì‹ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì œì•ˆëœ ëª¨ë¸ì€ Transformer, ì‹ ê²½ë§(NN), ì¡°ê±´ë¶€ ëœë¤ í•„ë“œ(CRF)ë¥¼ ê²°í•©í•˜ì—¬ ë¬¸ì„œ ë‚´ì—ì„œ ì¸ê°„ê³¼ AI í…ìŠ¤íŠ¸ë¥¼ í† í° ìˆ˜ì¤€ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. ë‘ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì„ í†µí•´ í‰ê°€í•œ ê²°ê³¼, ì´ ë°©ë²•ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì •í™•í•˜ê²Œ AI í…ìŠ¤íŠ¸ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì „í†µì ì¸ AI íƒì§€ê¸°ëŠ” ë¬¸ì„œ ìˆ˜ì¤€ì˜ ë¶„ë¥˜ì— ì˜ì¡´í•˜ì—¬ í˜¼í•©ëœ í…ìŠ¤íŠ¸ì—ì„œ AI ì½˜í…ì¸ ë¥¼ ì‹ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
- 2. ì œì•ˆëœ ëª¨ë¸ì€ ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§ì„ í†µí•´ ì¸ê°„ê³¼ AI ìƒì„± í…ìŠ¤íŠ¸ ê°„ì˜ ì „í™˜ì„ íƒì§€í•©ë‹ˆë‹¤.
- 3. ì´ ëª¨ë¸ì€ ìµœì‹  ì‚¬ì „ í•™ìŠµëœ Transformer ëª¨ë¸ê³¼ ì‹ ê²½ë§, ì¡°ê±´ë¶€ ëœë¤ í•„ë“œë¥¼ ê²°í•©í•˜ì—¬ ë¬¸ì„œ ë‚´ AI ë° ì¸ê°„ ì‘ì„± í…ìŠ¤íŠ¸ë¥¼ í† í° ìˆ˜ì¤€ì—ì„œ êµ¬ë¶„í•©ë‹ˆë‹¤.
- 4. ë‘ ê°œì˜ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ í‰ê°€ì—ì„œ ì œì•ˆëœ ì ‘ê·¼ë²•ì´ AI í…ìŠ¤íŠ¸ì˜ ë²”ìœ„ë¥¼ ì •í™•í•˜ê²Œ íƒì§€í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 5. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œì™€ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì€ GitHub ì €ì¥ì†Œì—ì„œ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:10:35*