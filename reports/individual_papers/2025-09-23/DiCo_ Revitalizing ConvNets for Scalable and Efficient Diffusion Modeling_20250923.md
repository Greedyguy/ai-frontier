---
keywords:
  - Diffusion Transformer
  - Neural Network
  - Attention Mechanism
  - Diffusion ConvNet
  - Text-to-Image Generation
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2505.11196
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:22:16.812231",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Transformer",
    "Neural Network",
    "Attention Mechanism",
    "Diffusion ConvNet",
    "Text-to-Image Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Transformer": 0.78,
    "Neural Network": 0.82,
    "Attention Mechanism": 0.8,
    "Diffusion ConvNet": 0.85,
    "Text-to-Image Generation": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformer",
        "canonical": "Diffusion Transformer",
        "aliases": [
          "DiT"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific model discussed in the paper, highlighting its importance in diffusion modeling.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Convolutional Neural Network",
        "canonical": "Neural Network",
        "aliases": [
          "ConvNet"
        ],
        "category": "broad_technical",
        "rationale": "ConvNets are revisited as a core component, linking to broader neural network discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Channel Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Channel Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Enhances feature diversity, connecting to discussions on attention mechanisms.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Diffusion ConvNet",
        "canonical": "Diffusion ConvNet",
        "aliases": [
          "DiCo"
        ],
        "category": "unique_technical",
        "rationale": "A novel model proposed in the paper, central to its contributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.85
      },
      {
        "surface": "Text-to-Image Generation",
        "canonical": "Text-to-Image Generation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the application potential of the discussed models in multimodal generation tasks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformer",
      "resolved_canonical": "Diffusion Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Convolutional Neural Network",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Channel Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Diffusion ConvNet",
      "resolved_canonical": "Diffusion ConvNet",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Text-to-Image Generation",
      "resolved_canonical": "Text-to-Image Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.11196.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2505.11196](https://arxiv.org/abs/2505.11196)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/AlignedGen_ Aligning Style Across Generated Images_20250923|AlignedGen: Aligning Style Across Generated Images]] (85.4% similar)
- [[2025-09-23/Optimizing Inference in Transformer-Based Models_ A Multi-Method Benchmark_20250923|Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark]] (85.4% similar)
- [[2025-09-17/BWCache_ Accelerating Video Diffusion Transformers through Block-Wise Caching_20250917|BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching]] (84.0% similar)
- [[2025-09-22/LowDiff_ Efficient Diffusion Sampling with Low-Resolution Condition_20250922|LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition]] (83.7% similar)
- [[2025-09-22/Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification_20250922|Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Text-to-Image Generation|Text-to-Image Generation]]
**âš¡ Unique Technical**: [[keywords/Diffusion Transformer|Diffusion Transformer]], [[keywords/Diffusion ConvNet|Diffusion ConvNet]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.11196v2 Announce Type: replace 
Abstract: Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet generation benchmarks, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively. Furthermore, experimental results on MS-COCO demonstrate that the purely convolutional DiCo exhibits strong potential for text-to-image generation. Code: https://github.com/shallowdream204/DiCo.

## ğŸ“ ìš”ì•½

Diffusion Transformer (DiT)ëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ì˜ ì‹œê° ìƒì„± ëª¨ë¸ì´ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ì´ ë¬¸ì œì…ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, DiTì˜ ê¸€ë¡œë²Œ ì…€í”„ ì–´í…ì…˜ì€ ì£¼ë¡œ ì§€ì—­ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” ë° ë¶ˆí•„ìš”í•˜ë©°, ë” íš¨ìœ¨ì ì¸ ëŒ€ì•ˆì´ ê°€ëŠ¥í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” íš¨ìœ¨ì ì´ê³  í‘œí˜„ë ¥ ìˆëŠ” í™•ì‚° ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ì»¨ë³¼ë£¨ì…˜ì„ ëŒ€ì•ˆìœ¼ë¡œ ì¬ê²€í† í•©ë‹ˆë‹¤. ì…€í”„ ì–´í…ì…˜ì„ ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ ë‹¨ìˆœíˆ ëŒ€ì²´í•˜ë©´ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•˜ëŠ”ë°, ì´ëŠ” ConvNetì˜ ì±„ë„ ì¤‘ë³µë„ê°€ Transformerë³´ë‹¤ ë†’ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì±„ë„ì˜ í™œì„±í™”ë¥¼ ì´‰ì§„í•˜ëŠ” ì»´íŒ©íŠ¸ ì±„ë„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ê¸°ëŠ¥ ë‹¤ì–‘ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, í‘œì¤€ ConvNet ëª¨ë“ˆë¡œë§Œ êµ¬ì„±ëœ Diffusion ConvNet (DiCo) ëª¨ë¸ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê°•ë ¥í•œ ìƒì„± ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ImageNet ìƒì„± ë²¤ì¹˜ë§ˆí¬ì—ì„œ DiCo-XLì€ 256x256 í•´ìƒë„ì—ì„œ FID 2.05, 512x512 í•´ìƒë„ì—ì„œ 2.53ì„ ê¸°ë¡í•˜ë©°, DiT-XL/2 ëŒ€ë¹„ ê°ê° 2.7ë°°, 3.1ë°°ì˜ ì†ë„ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. MS-COCO ì‹¤í—˜ ê²°ê³¼, ìˆœìˆ˜ ì»¨ë³¼ë£¨ì…˜ ê¸°ë°˜ DiCoëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì—ì„œë„ ê°•ë ¥í•œ ì ì¬ë ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DiT ëª¨ë¸ì˜ ê¸€ë¡œë²Œ ì…€í”„ ì–´í…ì…˜ì€ ì¢…ì¢… ë¶ˆí•„ìš”í•˜ë©°, ì£¼ë¡œ ì§€ì—­ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” ë° ì§‘ì¤‘ë˜ì–´ ìˆìŒ.
- 2. ì…€í”„ ì–´í…ì…˜ì„ ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•˜ë©°, ì´ëŠ” ConvNetì˜ ì±„ë„ ì¤‘ë³µì„±ì´ ì›ì¸ì„.
- 3. ë‹¤ì–‘í•œ ì±„ë„ í™œì„±í™”ë¥¼ ì´‰ì§„í•˜ëŠ” ì»´íŒ©íŠ¸ ì±„ë„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ì„±ëŠ¥ ê²©ì°¨ë¥¼ í•´ì†Œí•¨.
- 4. DiCo ëª¨ë¸ì€ í‘œì¤€ ConvNet ëª¨ë“ˆë¡œë§Œ êµ¬ì„±ë˜ì–´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê°•ë ¥í•œ ìƒì„± ì„±ëŠ¥ì„ ì œê³µí•¨.
- 5. DiCo-XLì€ ImageNet ë° MS-COCO ë²¤ì¹˜ë§ˆí¬ì—ì„œ DiT-XL/2 ëŒ€ë¹„ ìµœëŒ€ 3.1ë°°ì˜ ì†ë„ í–¥ìƒì„ ë³´ì´ë©°, ìš°ìˆ˜í•œ ì´ë¯¸ì§€ ìƒì„± ì„±ëŠ¥ì„ ì…ì¦í•¨.


---

*Generated on 2025-09-24 05:22:16*