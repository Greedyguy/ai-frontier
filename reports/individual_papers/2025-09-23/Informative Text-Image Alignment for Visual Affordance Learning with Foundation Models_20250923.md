---
keywords:
  - Visual Affordance Learning
  - Vision-Language Model
  - Text-Image Alignment
  - Mutual Information Constraint
  - Few-Shot Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17074
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:43:14.816390",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Visual Affordance Learning",
    "Vision-Language Model",
    "Text-Image Alignment",
    "Mutual Information Constraint",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Visual Affordance Learning": 0.78,
    "Vision-Language Model": 0.82,
    "Text-Image Alignment": 0.77,
    "Mutual Information Constraint": 0.75,
    "Few-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Affordance Learning",
        "canonical": "Visual Affordance Learning",
        "aliases": [
          "Affordance Learning"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's contribution and is specific to the domain of robotics and interaction with the physical world.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Foundation Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Foundation Models"
        ],
        "category": "evolved_concepts",
        "rationale": "This represents a key technological foundation leveraged in the paper, linking it to broader trends in multimodal AI.",
        "novelty_score": 0.58,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Text-Image Alignment",
        "canonical": "Text-Image Alignment",
        "aliases": [
          "Image-Text Alignment"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach discussed in the paper, crucial for achieving effective affordance learning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mutual Information Constraint",
        "canonical": "Mutual Information Constraint",
        "aliases": [
          "Information Constraint"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a specific methodological innovation introduced in the paper to improve learning outcomes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "One-Shot Affordance Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "One-Shot Learning"
        ],
        "category": "specific_connectable",
        "rationale": "This aligns with the trending concept of Few-Shot Learning, providing a connection to broader machine learning methodologies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Affordance Learning",
      "resolved_canonical": "Visual Affordance Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Foundation Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Text-Image Alignment",
      "resolved_canonical": "Text-Image Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mutual Information Constraint",
      "resolved_canonical": "Mutual Information Constraint",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "One-Shot Affordance Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17074.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17074](https://arxiv.org/abs/2509.17074)

## 🔗 유사한 논문
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (81.9% similar)
- [[2025-09-22/Dynamic Classifier-Free Diffusion Guidance via Online Feedback_20250922|Dynamic Classifier-Free Diffusion Guidance via Online Feedback]] (81.7% similar)
- [[2025-09-22/Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery_20250922|Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery]] (80.6% similar)
- [[2025-09-22/The Curious Case of Visual Grounding_ Different Effects for Speech- and Text-based Language Encoders_20250922|The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders]] (80.4% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (80.2% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Visual Affordance Learning|Visual Affordance Learning]], [[keywords/Text-Image Alignment|Text-Image Alignment]], [[keywords/Mutual Information Constraint|Mutual Information Constraint]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17074v1 Announce Type: cross 
Abstract: Visual affordance learning is crucial for robots to understand and interact effectively with the physical world. Recent advances in this field attempt to leverage pre-trained knowledge of vision-language foundation models to learn affordance properties with limited training data, providing a novel paradigm for visual affordance learning. However, these methods overlook the significance of maintaining feature alignment between visual images and language descriptions for identifying affordance areas with textual guidance, and thus may lead to suboptimal results. In this paper, we present an informative framework for text-guided affordance learning, which involves information-based constraints to achieve text-image alignment at feature level. Specifically, we design an affordance mutual information constraint that helps learn appropriate textual prompts and task-oriented visual features simultaneously by maximizing the mutual information between the features of the affordance areas in the input images and the corresponding textual prompts. In addition, we propose an object-level information constraint that maximizes the mutual information between the visual features of a given object and the text features of the category it belongs to. This enables the model to capture high-quality representations for the object, providing more reliable semantic priors for identifying affordance regions. Experimental results on the AGD20K dataset show that the proposed method outperforms existing approaches and achieves the new state-of-the-art in one-shot affordance learning.

## 📝 요약

이 논문은 로봇이 물리적 세계와 효과적으로 상호작용하기 위해 필요한 시각적 어포던스 학습을 다룹니다. 기존의 방법들은 시각-언어 모델의 사전 학습 지식을 활용하여 제한된 데이터로 어포던스를 학습하려 하지만, 시각 이미지와 언어 설명 간의 특징 정렬을 간과하여 최적의 결과를 얻지 못할 수 있습니다. 본 연구에서는 정보 기반 제약을 통해 텍스트와 이미지의 특징 정렬을 달성하는 텍스트 기반 어포던스 학습 프레임워크를 제안합니다. 특히, 어포던스 상호 정보 제약을 설계하여 입력 이미지의 어포던스 영역과 해당 텍스트 프롬프트 간의 상호 정보를 최대화함으로써 적절한 텍스트 프롬프트와 시각적 특징을 동시에 학습합니다. 또한, 객체 수준 정보 제약을 통해 객체의 시각적 특징과 해당 카테고리의 텍스트 특징 간의 상호 정보를 최대화하여 더 신뢰할 수 있는 의미적 기준을 제공합니다. AGD20K 데이터셋 실험 결과, 제안된 방법이 기존 방법들을 능가하며 새로운 최첨단 성과를 달성했습니다.

## 🎯 주요 포인트

- 1. 시각적 어포던스 학습은 로봇이 물리적 세계와 효과적으로 상호작용하기 위해 필수적이다.
- 2. 최근 연구들은 비전-언어 기반 모델의 사전 학습 지식을 활용하여 제한된 데이터로 어포던스 속성을 학습하는 새로운 패러다임을 제시한다.
- 3. 본 논문에서는 정보 기반 제약을 통해 텍스트-이미지 정렬을 달성하는 텍스트 안내 어포던스 학습 프레임워크를 제안한다.
- 4. 어포던스 상호 정보 제약을 설계하여 입력 이미지의 어포던스 영역과 해당 텍스트 프롬프트 간의 상호 정보를 최대화한다.
- 5. AGD20K 데이터셋 실험 결과, 제안된 방법이 기존 접근법을 능가하며 원샷 어포던스 학습에서 새로운 최첨단 성과를 달성했다.


---

*Generated on 2025-09-23 23:43:14*