---
keywords:
  - Visual Affordance Learning
  - Vision-Language Model
  - Text-Image Alignment
  - Mutual Information Constraint
  - Few-Shot Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17074
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:43:14.816390",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Visual Affordance Learning",
    "Vision-Language Model",
    "Text-Image Alignment",
    "Mutual Information Constraint",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Visual Affordance Learning": 0.78,
    "Vision-Language Model": 0.82,
    "Text-Image Alignment": 0.77,
    "Mutual Information Constraint": 0.75,
    "Few-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Affordance Learning",
        "canonical": "Visual Affordance Learning",
        "aliases": [
          "Affordance Learning"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's contribution and is specific to the domain of robotics and interaction with the physical world.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Foundation Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Foundation Models"
        ],
        "category": "evolved_concepts",
        "rationale": "This represents a key technological foundation leveraged in the paper, linking it to broader trends in multimodal AI.",
        "novelty_score": 0.58,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Text-Image Alignment",
        "canonical": "Text-Image Alignment",
        "aliases": [
          "Image-Text Alignment"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach discussed in the paper, crucial for achieving effective affordance learning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mutual Information Constraint",
        "canonical": "Mutual Information Constraint",
        "aliases": [
          "Information Constraint"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a specific methodological innovation introduced in the paper to improve learning outcomes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "One-Shot Affordance Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "One-Shot Learning"
        ],
        "category": "specific_connectable",
        "rationale": "This aligns with the trending concept of Few-Shot Learning, providing a connection to broader machine learning methodologies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Affordance Learning",
      "resolved_canonical": "Visual Affordance Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Foundation Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Text-Image Alignment",
      "resolved_canonical": "Text-Image Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mutual Information Constraint",
      "resolved_canonical": "Mutual Information Constraint",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "One-Shot Affordance Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17074.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17074](https://arxiv.org/abs/2509.17074)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (81.9% similar)
- [[2025-09-22/Dynamic Classifier-Free Diffusion Guidance via Online Feedback_20250922|Dynamic Classifier-Free Diffusion Guidance via Online Feedback]] (81.7% similar)
- [[2025-09-22/Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery_20250922|Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery]] (80.6% similar)
- [[2025-09-22/The Curious Case of Visual Grounding_ Different Effects for Speech- and Text-based Language Encoders_20250922|The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders]] (80.4% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Visual Affordance Learning|Visual Affordance Learning]], [[keywords/Text-Image Alignment|Text-Image Alignment]], [[keywords/Mutual Information Constraint|Mutual Information Constraint]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17074v1 Announce Type: cross 
Abstract: Visual affordance learning is crucial for robots to understand and interact effectively with the physical world. Recent advances in this field attempt to leverage pre-trained knowledge of vision-language foundation models to learn affordance properties with limited training data, providing a novel paradigm for visual affordance learning. However, these methods overlook the significance of maintaining feature alignment between visual images and language descriptions for identifying affordance areas with textual guidance, and thus may lead to suboptimal results. In this paper, we present an informative framework for text-guided affordance learning, which involves information-based constraints to achieve text-image alignment at feature level. Specifically, we design an affordance mutual information constraint that helps learn appropriate textual prompts and task-oriented visual features simultaneously by maximizing the mutual information between the features of the affordance areas in the input images and the corresponding textual prompts. In addition, we propose an object-level information constraint that maximizes the mutual information between the visual features of a given object and the text features of the category it belongs to. This enables the model to capture high-quality representations for the object, providing more reliable semantic priors for identifying affordance regions. Experimental results on the AGD20K dataset show that the proposed method outperforms existing approaches and achieves the new state-of-the-art in one-shot affordance learning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¡œë´‡ì´ ë¬¼ë¦¬ì  ì„¸ê³„ì™€ íš¨ê³¼ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì‹œê°ì  ì–´í¬ë˜ìŠ¤ í•™ìŠµì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ì‹œê°-ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì œí•œëœ ë°ì´í„°ë¡œ ì–´í¬ë˜ìŠ¤ë¥¼ í•™ìŠµí•˜ë ¤ í•˜ì§€ë§Œ, ì‹œê° ì´ë¯¸ì§€ì™€ ì–¸ì–´ ì„¤ëª… ê°„ì˜ íŠ¹ì§• ì •ë ¬ì„ ê°„ê³¼í•˜ì—¬ ìµœì ì˜ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì •ë³´ ê¸°ë°˜ ì œì•½ì„ í†µí•´ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ íŠ¹ì§• ì •ë ¬ì„ ë‹¬ì„±í•˜ëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–´í¬ë˜ìŠ¤ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ì–´í¬ë˜ìŠ¤ ìƒí˜¸ ì •ë³´ ì œì•½ì„ ì„¤ê³„í•˜ì—¬ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì–´í¬ë˜ìŠ¤ ì˜ì—­ê³¼ í•´ë‹¹ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê°„ì˜ ìƒí˜¸ ì •ë³´ë¥¼ ìµœëŒ€í™”í•¨ìœ¼ë¡œì¨ ì ì ˆí•œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì™€ ì‹œê°ì  íŠ¹ì§•ì„ ë™ì‹œì— í•™ìŠµí•©ë‹ˆë‹¤. ë˜í•œ, ê°ì²´ ìˆ˜ì¤€ ì •ë³´ ì œì•½ì„ í†µí•´ ê°ì²´ì˜ ì‹œê°ì  íŠ¹ì§•ê³¼ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í…ìŠ¤íŠ¸ íŠ¹ì§• ê°„ì˜ ìƒí˜¸ ì •ë³´ë¥¼ ìµœëŒ€í™”í•˜ì—¬ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ë¯¸ì  ê¸°ì¤€ì„ ì œê³µí•©ë‹ˆë‹¤. AGD20K ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©° ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹œê°ì  ì–´í¬ë˜ìŠ¤ í•™ìŠµì€ ë¡œë´‡ì´ ë¬¼ë¦¬ì  ì„¸ê³„ì™€ íš¨ê³¼ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì´ë‹¤.
- 2. ìµœê·¼ ì—°êµ¬ë“¤ì€ ë¹„ì „-ì–¸ì–´ ê¸°ë°˜ ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì œí•œëœ ë°ì´í„°ë¡œ ì–´í¬ë˜ìŠ¤ ì†ì„±ì„ í•™ìŠµí•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•œë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì •ë³´ ê¸°ë°˜ ì œì•½ì„ í†µí•´ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì •ë ¬ì„ ë‹¬ì„±í•˜ëŠ” í…ìŠ¤íŠ¸ ì•ˆë‚´ ì–´í¬ë˜ìŠ¤ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. ì–´í¬ë˜ìŠ¤ ìƒí˜¸ ì •ë³´ ì œì•½ì„ ì„¤ê³„í•˜ì—¬ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì–´í¬ë˜ìŠ¤ ì˜ì—­ê³¼ í•´ë‹¹ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê°„ì˜ ìƒí˜¸ ì •ë³´ë¥¼ ìµœëŒ€í™”í•œë‹¤.
- 5. AGD20K ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ì ‘ê·¼ë²•ì„ ëŠ¥ê°€í•˜ë©° ì›ìƒ· ì–´í¬ë˜ìŠ¤ í•™ìŠµì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆë‹¤.


---

*Generated on 2025-09-23 23:43:14*