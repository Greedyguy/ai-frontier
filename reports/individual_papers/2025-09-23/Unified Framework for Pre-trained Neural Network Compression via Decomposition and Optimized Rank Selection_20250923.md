---
keywords:
  - Machine Learning
  - Neural Network
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2409.03555
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:34:09.433679",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Machine Learning",
    "Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Machine Learning": 0.0,
    "Neural Network": 0.0
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Machine Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.0,
        "connectivity": 0.0,
        "specificity": 0.0,
        "link_intent": 0.0
      }
    },
    {
      "candidate_surface": "Neural Network",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.0,
        "connectivity": 0.0,
        "specificity": 0.0,
        "link_intent": 0.0
      }
    }
  ]
}
-->

# Unified Framework for Pre-trained Neural Network Compression via Decomposition and Optimized Rank Selection

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2409.03555.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2409.03555](https://arxiv.org/abs/2409.03555)

## 🔗 유사한 논문
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (83.2% similar)
- [[2025-09-23/Search-Optimized Quantization in Biomedical Ontology Alignment_20250923|Search-Optimized Quantization in Biomedical Ontology Alignment]] (83.2% similar)
- [[2025-09-23/Interpretability-Aware Pruning for Efficient Medical Image Analysis_20250923|Interpretability-Aware Pruning for Efficient Medical Image Analysis]] (83.0% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (82.5% similar)
- [[2025-09-18/A Novel Compression Framework for YOLOv8_ Achieving Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation_20250918|A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation]] (82.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Machine Learning|Machine Learning]], [[keywords/Neural Network|Neural Network]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2409.03555v2 Announce Type: replace 
Abstract: Despite their high accuracy, complex neural networks demand significant computational resources, posing challenges for deployment on resource constrained devices such as mobile phones and embedded systems. Compression algorithms have been developed to address these challenges by reducing model size and computational demands while maintaining accuracy. Among these approaches, factorization methods based on tensor decomposition are theoretically sound and effective. However, they face difficulties in selecting the appropriate rank for decomposition. This paper tackles this issue by presenting a unified framework that simultaneously applies decomposition and rank selection, employing a composite compression loss within defined rank constraints. Our method includes an automatic rank search in a continuous space, efficiently identifying optimal rank configurations for the pre-trained model by eliminating the need for additional training data and reducing computational overhead in the search step. Combined with a subsequent fine-tuning step, our approach maintains the performance of highly compressed models on par with their original counterparts. Using various benchmark datasets and models, we demonstrate the efficacy of our method through a comprehensive analysis.

## 📝 요약

복잡한 신경망의 높은 정확도에도 불구하고, 자원 제약이 있는 모바일 기기나 임베디드 시스템에 배포하기에는 많은 계산 자원이 필요합니다. 이를 해결하기 위해 모델 크기와 계산 요구를 줄이면서 정확성을 유지하는 압축 알고리즘이 개발되었습니다. 특히 텐서 분해 기반의 팩터라이제이션 방법은 이론적으로 타당하고 효과적이지만, 적절한 분해 랭크를 선택하는 데 어려움이 있습니다. 본 논문은 분해와 랭크 선택을 동시에 적용하는 통합 프레임워크를 제안하여 이 문제를 해결합니다. 제안된 방법은 연속 공간에서 자동으로 랭크를 탐색하여 추가 훈련 데이터 없이 최적의 랭크 구성을 효율적으로 식별하고, 탐색 단계의 계산 부담을 줄입니다. 이후의 미세 조정 단계를 통해 고도로 압축된 모델의 성능을 원본 모델과 동등하게 유지합니다. 다양한 벤치마크 데이터셋과 모델을 사용한 종합 분석을 통해 본 방법의 효능을 입증하였습니다.

## 🎯 주요 포인트

- 1. 복잡한 신경망의 높은 정확도에도 불구하고, 자원 제한 장치에서의 배포에 어려움이 있다.
- 2. 모델 크기와 계산 요구를 줄이면서 정확성을 유지하기 위해 압축 알고리즘이 개발되었다.
- 3. 텐서 분해 기반의 계수화 방법은 이론적으로 타당하고 효과적이지만 적절한 계수를 선택하는 데 어려움이 있다.
- 4. 본 논문은 계수 선택과 분해를 동시에 적용하는 통합 프레임워크를 제안하여 이 문제를 해결한다.
- 5. 제안된 방법은 추가 학습 데이터 없이도 최적의 계수 구성을 효율적으로 식별하며, 성능을 유지한다.


---

*Generated on 2025-09-24 02:34:09*