---
keywords:
  - TempFlow-GRPO
  - Text-to-Image Generation
  - Reinforcement Learning
  - Human Preference Alignment
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2508.04324
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:28:32.281463",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "TempFlow-GRPO",
    "Text-to-Image Generation",
    "Reinforcement Learning",
    "Human Preference Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "TempFlow-GRPO": 0.78,
    "Text-to-Image Generation": 0.79,
    "Reinforcement Learning": 0.75,
    "Human Preference Alignment": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "TempFlow-GRPO",
        "canonical": "TempFlow-GRPO",
        "aliases": [
          "Temporal Flow GRPO"
        ],
        "category": "unique_technical",
        "rationale": "TempFlow-GRPO is a novel framework specifically designed for improving GRPO training in flow models, offering unique insights into temporal optimization.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "text-to-image generation",
        "canonical": "Text-to-Image Generation",
        "aliases": [
          "text to image synthesis"
        ],
        "category": "specific_connectable",
        "rationale": "Text-to-Image Generation is a key application area for flow models, facilitating connections to multimodal learning and vision-language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "reinforcement learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is integral to the optimization process discussed, linking to broader machine learning strategies.",
        "novelty_score": 0.3,
        "connectivity_score": 0.92,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "human preference alignment",
        "canonical": "Human Preference Alignment",
        "aliases": [
          "preference alignment"
        ],
        "category": "evolved_concepts",
        "rationale": "Human Preference Alignment is crucial for ensuring models align with user expectations, linking to ethical AI and user-centric design.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "TempFlow-GRPO",
      "resolved_canonical": "TempFlow-GRPO",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "text-to-image generation",
      "resolved_canonical": "Text-to-Image Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "reinforcement learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.92,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "human preference alignment",
      "resolved_canonical": "Human Preference Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# TempFlow-GRPO: When Timing Matters for GRPO in Flow Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2508.04324.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2508.04324](https://arxiv.org/abs/2508.04324)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/GRPOformer_ Advancing Hyperparameter Optimization via Group Relative Policy Optimization_20250923|GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization]] (83.4% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (83.2% similar)
- [[2025-09-23/TempSamp-R1_ Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs_20250923|TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs]] (82.9% similar)
- [[2025-09-17/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250917|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (82.4% similar)
- [[2025-09-19/FlowRL_ Matching Reward Distributions for LLM Reasoning_20250919|FlowRL: Matching Reward Distributions for LLM Reasoning]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Text-to-Image Generation|Text-to-Image Generation]]
**âš¡ Unique Technical**: [[keywords/TempFlow-GRPO|TempFlow-GRPO]]
**ğŸš€ Evolved Concepts**: [[keywords/Human Preference Alignment|Human Preference Alignment]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.04324v2 Announce Type: replace 
Abstract: Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce \textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces three key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases; and (iii) a seed group strategy that controls for initialization effects to isolate exploration contributions. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and text-to-image benchmarks.

## ğŸ“ ìš”ì•½

ìµœê·¼ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì—ì„œ íë¦„ ë§¤ì¹­ ëª¨ë¸ì˜ í’ˆì§ˆì€ ë›°ì–´ë‚˜ì§€ë§Œ, ì¸ê°„ì˜ ì„ í˜¸ë„ì— ë§ì¶˜ ê°•í™” í•™ìŠµê³¼ì˜ í†µí•©ì€ ë¯¸í¡í•˜ì—¬ ì„¸ë°€í•œ ë³´ìƒ ê¸°ë°˜ ìµœì í™”ê°€ ì–´ë µìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì˜ ì‹œê°„ì  ê· ì¼ì„± ê°€ì •ì´ ë¹„íš¨ìœ¨ì ì¸ íƒìƒ‰ê³¼ ìµœì í™”ì˜ ì¥ì• ë¬¼ë¡œ ì‘ìš©í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” \textbf{TempFlow-GRPO}ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. TempFlow-GRPOëŠ” (i) ì§€ì •ëœ ë¶„ê¸°ì ì—ì„œ í™•ë¥ ì„±ì„ ì§‘ì¤‘ì‹œì¼œ ì •í™•í•œ ë³´ìƒ í• ë‹¹ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê²½ë¡œ ë¶„ê¸° ë©”ì»¤ë‹ˆì¦˜, (ii) ê° ì‹œê°„ ë‹¨ê³„ì˜ íƒìƒ‰ ì ì¬ë ¥ì— ë”°ë¼ ì •ì±… ìµœì í™”ë¥¼ ì¡°ì ˆí•˜ëŠ” ë…¸ì´ì¦ˆ ì¸ì‹ ê°€ì¤‘ì¹˜ ì²´ê³„, (iii) ì´ˆê¸°í™” íš¨ê³¼ë¥¼ í†µì œí•˜ì—¬ íƒìƒ‰ ê¸°ì—¬ë¥¼ ë¶„ë¦¬í•˜ëŠ” ì‹œë“œ ê·¸ë£¹ ì „ëµì„ ë„ì…í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í˜ì‹ ì€ ëª¨ë¸ì´ ìƒì„±ì˜ ì‹œê°„ì  êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ìµœì í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ì¸ê°„ ì„ í˜¸ë„ ì •ë ¬ê³¼ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± íë¦„ ëª¨ë¸ì€ ê°•í™” í•™ìŠµê³¼ì˜ í†µí•©ì—ì„œ ë¹„íš¨ìœ¨ì ì¸ íƒìƒ‰ê³¼ ìˆ˜ë ´ ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
- 2. TempFlow-GRPOëŠ” íë¦„ ê¸°ë°˜ ìƒì„±ì˜ ì‹œê°„ êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ ê°•í™” í•™ìŠµì„ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 3. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì§€ì •ëœ ë¶„ê¸°ì ì—ì„œ í™•ë¥ ì„±ì„ ì§‘ì¤‘ì‹œì¼œ ì •í™•í•œ í¬ë ˆë”§ í• ë‹¹ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê¶¤ì  ë¶„ê¸° ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•©ë‹ˆë‹¤.
- 4. ì‹œê°„ ë‹¨ê³„ë³„ íƒìƒ‰ ì ì¬ë ¥ì— ë”°ë¼ ì •ì±… ìµœì í™”ë¥¼ ì¡°ì ˆí•˜ëŠ” ë…¸ì´ì¦ˆ ì¸ì‹ ê°€ì¤‘ì¹˜ ì²´ê³„ë¥¼ í†µí•´ í•™ìŠµì„ ìš°ì„ ì‹œí•©ë‹ˆë‹¤.
- 5. ì´ˆê¸°í™” íš¨ê³¼ë¥¼ í†µì œí•˜ì—¬ íƒìƒ‰ ê¸°ì—¬ë¥¼ ë¶„ë¦¬í•˜ëŠ” ì‹œë“œ ê·¸ë£¹ ì „ëµì„ í†µí•´ ì¸ê°„ ì„ í˜¸ë„ ì •ë ¬ ë° í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:28:32*