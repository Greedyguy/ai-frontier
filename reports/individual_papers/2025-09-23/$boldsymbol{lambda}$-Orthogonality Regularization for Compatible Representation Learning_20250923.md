---
keywords:
  - Neural Network
  - Zero-Shot Learning
  - Orthogonal Transformation
  - Affine Transformation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16664
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:41:43.326478",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Network",
    "Zero-Shot Learning",
    "Orthogonal Transformation",
    "Affine Transformation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Network": 0.78,
    "Zero-Shot Learning": 0.79,
    "Orthogonal Transformation": 0.77,
    "Affine Transformation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "Neural Nets"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are central to the paper's discussion on representation learning and model compatibility.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Performance",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a key aspect of the paper's evaluation of model compatibility.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Orthogonal Transformations",
        "canonical": "Orthogonal Transformation",
        "aliases": [
          "Orthogonal Transform"
        ],
        "category": "unique_technical",
        "rationale": "Orthogonal Transformations are crucial for understanding the geometric constraints discussed in the paper.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Affine Transformations",
        "canonical": "Affine Transformation",
        "aliases": [
          "Affine Transform"
        ],
        "category": "unique_technical",
        "rationale": "Affine Transformations are a primary method discussed for adapting learned representations.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "retrieval systems",
      "extensive experiments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Performance",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Orthogonal Transformations",
      "resolved_canonical": "Orthogonal Transformation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Affine Transformations",
      "resolved_canonical": "Affine Transformation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# $\boldsymbol{\lambda}$-Orthogonality Regularization for Compatible Representation Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16664.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16664](https://arxiv.org/abs/2509.16664)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Probabilistic Token Alignment for Large Language Model Fusion_20250923|Probabilistic Token Alignment for Large Language Model Fusion]] (83.7% similar)
- [[2025-09-23/LifeAlign_ Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization_20250923|LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization]] (82.1% similar)
- [[2025-09-17/Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation_20250917|Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation]] (82.1% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (81.7% similar)
- [[2025-09-23/Bridging Past and Future_ Distribution-Aware Alignment for Time Series Forecasting_20250923|Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Orthogonal Transformation|Orthogonal Transformation]], [[keywords/Affine Transformation|Affine Transformation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16664v1 Announce Type: new 
Abstract: Retrieval systems rely on representations learned by increasingly powerful models. However, due to the high training cost and inconsistencies in learned representations, there is significant interest in facilitating communication between representations and ensuring compatibility across independently trained neural networks. In the literature, two primary approaches are commonly used to adapt different learned representations: affine transformations, which adapt well to specific distributions but can significantly alter the original representation, and orthogonal transformations, which preserve the original structure with strict geometric constraints but limit adaptability. A key challenge is adapting the latent spaces of updated models to align with those of previous models on downstream distributions while preserving the newly learned representation spaces. In this paper, we impose a relaxed orthogonality constraint, namely $\lambda$-orthogonality regularization, while learning an affine transformation, to obtain distribution-specific adaptation while retaining the original learned representations. Extensive experiments across various architectures and datasets validate our approach, demonstrating that it preserves the model's zero-shot performance and ensures compatibility across model updates. Code available at: https://github.com/miccunifi/lambda_orthogonality

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ í•™ìŠµ ëª¨ë¸ ê°„ì˜ í˜¸í™˜ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í‘œí˜„ í•™ìŠµ ëª¨ë¸ ê°„ì˜ ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ì™„í™”ëœ ì§êµì„± ì œì•½ ì¡°ê±´ì¸ $\lambda$-ì§êµì„± ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ìƒˆë¡œìš´ í‘œí˜„ ê³µê°„ì„ ìœ ì§€í•˜ë©´ì„œë„ ì´ì „ ëª¨ë¸ê³¼ì˜ í˜¸í™˜ì„±ì„ ë³´ì¥í•˜ëŠ” ë°©ë²•ë¡ ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì™€ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜ì„ í†µí•´ ê²€ì¦ë˜ì—ˆìœ¼ë©°, ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œì—ë„ ì œë¡œìƒ· ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³  í˜¸í™˜ì„±ì„ ë³´ì¥í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê²€ìƒ‰ ì‹œìŠ¤í…œì€ ì ì  ë” ê°•ë ¥í•´ì§€ëŠ” ëª¨ë¸ì— ì˜í•´ í•™ìŠµëœ í‘œí˜„ì— ì˜ì¡´í•˜ì§€ë§Œ, ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµëœ ì‹ ê²½ë§ ê°„ì˜ í˜¸í™˜ì„±ì„ ë³´ì¥í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ í‘œí˜„ì„ í¬ê²Œ ë³€ê²½í•  ìˆ˜ ìˆëŠ” ì•„í•€ ë³€í™˜ê³¼ ì—„ê²©í•œ ê¸°í•˜í•™ì  ì œì•½ì„ ê°€ì§€ëŠ” ì§êµ ë³€í™˜ì´ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•„í•€ ë³€í™˜ì„ í•™ìŠµí•˜ë©´ì„œ $\lambda$-ì§êµì„± ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ì›ë˜ì˜ í•™ìŠµëœ í‘œí˜„ì„ ìœ ì§€í•˜ë©´ì„œë„ ë¶„í¬ë³„ ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì™€ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì´ ëª¨ë¸ì˜ ì œë¡œìƒ· ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³  ëª¨ë¸ ì—…ë°ì´íŠ¸ ê°„ì˜ í˜¸í™˜ì„±ì„ ë³´ì¥í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì½”ë“œì™€ í•¨ê»˜ ê³µê°œë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ í˜¸í™˜ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:41:43*