---
keywords:
  - Transformer
  - Vision-Language Model
  - Visual Robustness Learning
  - Visual Instruction Pretraining
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17562
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:55:16.945616",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Vision-Language Model",
    "Visual Robustness Learning",
    "Visual Instruction Pretraining"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Vision-Language Model": 0.9,
    "Visual Robustness Learning": 0.8,
    "Visual Instruction Pretraining": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational technology in modern machine learning, and linking to them helps connect various related concepts.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are increasingly important in multimodal learning, connecting vision and language processing.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Visual Robustness Learning",
        "canonical": "Visual Robustness Learning",
        "aliases": [
          "VRL"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper, enhancing the robustness of visual models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual Instruction Pretraining",
        "canonical": "Visual Instruction Pretraining",
        "aliases": [
          "ViTP"
        ],
        "category": "unique_technical",
        "rationale": "This is a new pretraining paradigm proposed in the paper, crucial for understanding the paper's contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      }
    ],
    "ban_list_suggestions": [
      "perception",
      "reasoning",
      "generation",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Visual Robustness Learning",
      "resolved_canonical": "Visual Robustness Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual Instruction Pretraining",
      "resolved_canonical": "Visual Instruction Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    }
  ]
}
-->

# Visual Instruction Pretraining for Domain-Specific Foundation Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17562.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17562](https://arxiv.org/abs/2509.17562)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (84.8% similar)
- [[2025-09-23/Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute_20250923|Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute]] (84.8% similar)
- [[2025-09-22/Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks_20250922|Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks]] (83.8% similar)
- [[2025-09-23/Interpreting vision transformers via residual replacement model_20250923|Interpreting vision transformers via residual replacement model]] (83.7% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**âš¡ Unique Technical**: [[keywords/Visual Robustness Learning|Visual Robustness Learning]], [[keywords/Visual Instruction Pretraining|Visual Instruction Pretraining]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17562v1 Announce Type: new 
Abstract: Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at github.com/zcablii/ViTP.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³ ì°¨ì›ì  ì¶”ë¡ ì´ ì €ì°¨ì›ì  ì§€ê° íŠ¹ì§• í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íƒêµ¬í•˜ì§€ ì•Šì€ ê¸°ì¡´ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì, ìƒˆë¡œìš´ ì‚¬ì „ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì¸ Visual insTruction Pretraining (ViTP)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ViTPëŠ” Vision Transformer (ViT)ë¥¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì— ë‚´ì¥í•˜ê³ , ëª©í‘œ ë„ë©”ì¸ì—ì„œ ìˆ˜ì§‘í•œ ì‹œê°ì  ì§€ì‹œ ë°ì´í„°ë¥¼ í™œìš©í•´ ì¢…ë‹¨ ê°„ ì‚¬ì „ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. íŠ¹íˆ, ViTPëŠ” ì œì•ˆëœ Visual Robustness Learning (VRL)ì„ í†µí•´ ViTê°€ ë„ë©”ì¸ ê´€ë ¨ íŠ¹ì§•ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. 16ê°œì˜ ì›ê²© ê°ì§€ ë° ì˜ë£Œ ì˜ìƒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ViTPëŠ” ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ìµœì‹  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í˜„ëŒ€ ì»´í“¨í„° ë¹„ì „ì€ ì¸ì‹, ì¶”ë¡ , ìƒì„±ì´ ì„œë¡œ ê°•í™”í•˜ëŠ” íì‡„ ë£¨í”„ì— ìˆ˜ë ´í•˜ê³  ìˆì§€ë§Œ, ê³ ì°¨ì› ì¶”ë¡ ì´ ì €ì°¨ì› ì¸ì‹ íŠ¹ì§• í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.
- 2. ViTP(Visual insTruction Pretraining)ëŠ” ê³ ì°¨ì› ì¶”ë¡ ì„ í†µí•´ ì¸ì‹ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì‚¬ì „ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•œë‹¤.
- 3. ViTPëŠ” Vision Transformer(ViT) ë°±ë³¸ì„ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì— ë‚´ì¥í•˜ê³ , ëª©í‘œ í•˜ìœ„ ë„ë©”ì¸ì—ì„œ ìˆ˜ì§‘í•œ ì‹œê°ì  ì§€ì‹œ ë°ì´í„°ë¡œ ì¢…ë‹¨ ê°„ ì‚¬ì „ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.
- 4. ViTPëŠ” ì œì•ˆëœ Visual Robustness Learning(VRL)ì„ í†µí•´ ì†Œìˆ˜ì˜ ì‹œê°ì  í† í°ìœ¼ë¡œë¶€í„° ê²¬ê³ í•˜ê³  ë„ë©”ì¸ ê´€ë ¨ íŠ¹ì§•ì„ í•™ìŠµí•˜ë„ë¡ í•œë‹¤.
- 5. ViTPëŠ” 16ê°œì˜ ì›ê²© ê°ì§€ ë° ì˜ë£Œ ì˜ìƒ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 04:55:16*