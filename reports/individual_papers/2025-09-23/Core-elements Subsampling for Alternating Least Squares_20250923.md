---
keywords:
  - Alternating Least Squares
  - Low-Rank Matrix Factorization
  - Recommender Systems
  - Sparse Matrix Operations
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.18024
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:29:31.805113",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Alternating Least Squares",
    "Low-Rank Matrix Factorization",
    "Recommender Systems",
    "Sparse Matrix Operations"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Alternating Least Squares": 0.8,
    "Low-Rank Matrix Factorization": 0.75,
    "Recommender Systems": 0.7,
    "Sparse Matrix Operations": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "alternating least squares",
        "canonical": "Alternating Least Squares",
        "aliases": [
          "ALS"
        ],
        "category": "unique_technical",
        "rationale": "Alternating Least Squares is a specific algorithm central to the paper's proposed method, enhancing connectivity with related computational techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "low-rank matrix factorization",
        "canonical": "Low-Rank Matrix Factorization",
        "aliases": [
          "matrix factorization"
        ],
        "category": "specific_connectable",
        "rationale": "Low-Rank Matrix Factorization is a key concept in the paper, linking to broader topics in matrix computations and data science.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "recommender systems",
        "canonical": "Recommender Systems",
        "aliases": [
          "recommendation systems"
        ],
        "category": "broad_technical",
        "rationale": "Recommender Systems are a major application area for the discussed methods, providing a broad technical context for linking.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      },
      {
        "surface": "sparse matrix operations",
        "canonical": "Sparse Matrix Operations",
        "aliases": [
          "sparse operations"
        ],
        "category": "specific_connectable",
        "rationale": "Sparse Matrix Operations are crucial for the efficiency improvements in the proposed method, connecting to computational optimizations.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "efficiency",
      "applications"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "alternating least squares",
      "resolved_canonical": "Alternating Least Squares",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "low-rank matrix factorization",
      "resolved_canonical": "Low-Rank Matrix Factorization",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "recommender systems",
      "resolved_canonical": "Recommender Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "sparse matrix operations",
      "resolved_canonical": "Sparse Matrix Operations",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Core-elements Subsampling for Alternating Least Squares

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18024.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.18024](https://arxiv.org/abs/2509.18024)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (80.9% similar)
- [[2025-09-18/Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (79.4% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (78.7% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (78.5% similar)
- [[2025-09-22/Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization_20250922|Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization]] (78.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Recommender Systems|Recommender Systems]]
**ğŸ”— Specific Connectable**: [[keywords/Low-Rank Matrix Factorization|Low-Rank Matrix Factorization]], [[keywords/Sparse Matrix Operations|Sparse Matrix Operations]]
**âš¡ Unique Technical**: [[keywords/Alternating Least Squares|Alternating Least Squares]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18024v1 Announce Type: cross 
Abstract: In this paper, we propose a novel element-wise subset selection method for the alternating least squares (ALS) algorithm, focusing on low-rank matrix factorization involving matrices with missing values, as commonly encountered in recommender systems. While ALS is widely used for providing personalized recommendations based on user-item interaction data, its high computational cost, stemming from repeated regression operations, poses significant challenges for large-scale datasets. To enhance the efficiency of ALS, we propose a core-elements subsampling method that selects a representative subset of data and leverages sparse matrix operations to approximate ALS estimations efficiently. We establish theoretical guarantees for the approximation and convergence of the proposed approach, showing that it achieves similar accuracy with significantly reduced computational time compared to full-data ALS. Extensive simulations and real-world applications demonstrate the effectiveness of our method in various scenarios, emphasizing its potential in large-scale recommendation systems.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í–‰ë ¬ì˜ ê²°ì¸¡ê°’ì„ í¬í•¨í•œ ì €ë­í¬ í–‰ë ¬ ë¶„í•´ë¥¼ ìœ„í•œ êµëŒ€ ìµœì†ŒììŠ¹(ALS) ì•Œê³ ë¦¬ì¦˜ì˜ ìƒˆë¡œìš´ ìš”ì†Œë³„ ë¶€ë¶„ì§‘í•© ì„ íƒ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ALSëŠ” ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš© ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸í™”ëœ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ë˜ì§€ë§Œ, ë°˜ë³µì ì¸ íšŒê·€ ì—°ì‚°ìœ¼ë¡œ ì¸í•´ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ê³„ì‚° ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëŒ€í‘œì ì¸ ë°ì´í„° ë¶€ë¶„ì§‘í•©ì„ ì„ íƒí•˜ê³  í¬ì†Œ í–‰ë ¬ ì—°ì‚°ì„ í™œìš©í•˜ì—¬ ALS ì¶”ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” í•µì‹¬ ìš”ì†Œ ì„œë¸Œìƒ˜í”Œë§ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì˜ ê·¼ì‚¬ ë° ìˆ˜ë ´ì— ëŒ€í•œ ì´ë¡ ì  ë³´ì¥ì„ ì œê³µí•˜ë©°, ì „ì²´ ë°ì´í„° ALSì™€ ìœ ì‚¬í•œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê³„ì‚° ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ ì‘ìš©ì„ í†µí•´ ëŒ€ê·œëª¨ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œì˜ ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ALS ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ìš”ì†Œë³„ ë¶€ë¶„ì§‘í•© ì„ íƒ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ ë°©ë²•ì€ ë°ì´í„°ì˜ ëŒ€í‘œì ì¸ ë¶€ë¶„ì§‘í•©ì„ ì„ íƒí•˜ê³  í¬ì†Œ í–‰ë ¬ ì—°ì‚°ì„ í™œìš©í•˜ì—¬ ALS ì¶”ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê·¼ì‚¬í•©ë‹ˆë‹¤.
- 3. ì´ ì ‘ê·¼ë²•ì€ ì „ì²´ ë°ì´í„° ALSì™€ ìœ ì‚¬í•œ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê³„ì‚° ì‹œê°„ì„ í¬ê²Œ ì¤„ì´ëŠ” ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. ì´ë¡ ì  ë³´ì¥ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì˜ ê·¼ì‚¬ ë° ìˆ˜ë ´ì„±ì„ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.
- 5. ë‹¤ì–‘í•œ ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ ì‘ìš©ì„ í†µí•´ ëŒ€ê·œëª¨ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œì˜ íš¨ê³¼ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:29:31*