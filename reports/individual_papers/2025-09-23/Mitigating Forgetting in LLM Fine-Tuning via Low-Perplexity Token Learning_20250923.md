---
keywords:
  - Large Language Model
  - Token Perplexity
  - Catastrophic Forgetting
  - Cross-Domain Generalization
  - Fine-Tuning Strategies
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2501.14315
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:47:01.376278",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Token Perplexity",
    "Catastrophic Forgetting",
    "Cross-Domain Generalization",
    "Fine-Tuning Strategies"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Token Perplexity": 0.82,
    "Catastrophic Forgetting": 0.81,
    "Cross-Domain Generalization": 0.78,
    "Fine-Tuning Strategies": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, connecting to various aspects of language model fine-tuning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Token Perplexity",
        "canonical": "Token Perplexity",
        "aliases": [
          "Perplexity Reduction"
        ],
        "category": "unique_technical",
        "rationale": "Key concept introduced for mitigating forgetting in fine-tuning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Catastrophic Forgetting",
        "canonical": "Catastrophic Forgetting",
        "aliases": [
          "Forgetting Mitigation"
        ],
        "category": "specific_connectable",
        "rationale": "Addresses a fundamental issue in model fine-tuning, linking to broader discussions on model stability.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.82,
        "link_intent_score": 0.81
      },
      {
        "surface": "Cross-Domain Generalization",
        "canonical": "Cross-Domain Generalization",
        "aliases": [
          "Domain Generalization"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the challenge of maintaining performance across different domains, relevant to various ML applications.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Fine-Tuning Strategies",
        "canonical": "Fine-Tuning Strategies",
        "aliases": [
          "Model Fine-Tuning"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's contribution, offering insights into improving model robustness.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "model performance",
      "ground truth data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Token Perplexity",
      "resolved_canonical": "Token Perplexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Catastrophic Forgetting",
      "resolved_canonical": "Catastrophic Forgetting",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.82,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Cross-Domain Generalization",
      "resolved_canonical": "Cross-Domain Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Fine-Tuning Strategies",
      "resolved_canonical": "Fine-Tuning Strategies",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2501.14315.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2501.14315](https://arxiv.org/abs/2501.14315)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (84.9% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (84.6% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (84.6% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (84.1% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Catastrophic Forgetting|Catastrophic Forgetting]], [[keywords/Cross-Domain Generalization|Cross-Domain Generalization]], [[keywords/Fine-Tuning Strategies|Fine-Tuning Strategies]]
**âš¡ Unique Technical**: [[keywords/Token Perplexity|Token Perplexity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.14315v4 Announce Type: replace 
Abstract: Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë„ë©”ì¸ ê°„ ì¼ê´€ëœ ëª¨ë¸ ì„±ëŠ¥ ìœ ì§€ì˜ ì–´ë ¤ì›€ì„ ë‹¤ë£¨ë©°, LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸) ìƒì„± ë°ì´í„°ë¥¼ í™œìš©í•œ íŒŒì¸íŠœë‹ì´ êµì°¨ ë„ë©”ì¸ ì¼ë°˜í™”ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, LLM ìƒì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ íŒŒì¸íŠœë‹ì€ ëª©í‘œ ì‘ì—… ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë¹„ëª©í‘œ ì‘ì—…ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ë°í˜”ìŠµë‹ˆë‹¤. ì´ëŠ” LLM ìƒì„± ë°ì´í„°ì—ì„œ ë†’ì€ ë‹¹í˜¹ë„(perplexity)ë¥¼ ê°€ì§„ í† í°ì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë˜í•œ, ì›ë³¸ ë°ì´í„°ì—ì„œë„ ë†’ì€ ë‹¹í˜¹ë„ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ë©´ ìœ ì‚¬í•œ ë¹„ëª©í‘œ ì‘ì—… ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ê·œëª¨ì— ê±¸ì¹œ ì‹¤í—˜ì€ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•˜ë©°, ì´ëŠ” íŒŒì¸íŠœë‹ í›„ LLMì—ì„œì˜ ì¹˜ëª…ì  ë§ê°ì„ ì™„í™”í•˜ëŠ” ë° ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLM ìƒì„± ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì • ì‹œ, ëª©í‘œ ì‘ì—… ì„±ëŠ¥ í–¥ìƒê³¼ ë¹„ëª©í‘œ ì‘ì—… ì„±ëŠ¥ ì €í•˜ ê°ì†Œë¥¼ ë™ì‹œì— ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.
- 2. ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ ë°ì´í„° ì‹œí€€ìŠ¤ë¥¼ ë¶„ì„í•œ ê²°ê³¼, LLM ìƒì„± ë°ì´í„°ëŠ” ë†’ì€ ë‹¹í˜¹ë„ í† í°ì„ ì¤„ì—¬ ë¹„ëª©í‘œ ì‘ì—…ì˜ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 3. ë†’ì€ ë‹¹í˜¹ë„ í† í°ì„ ë§ˆìŠ¤í‚¹í•œ ì‹¤ì œ ë°ì´í„°ë¡œë„ LLM ìƒì„± ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë¹„ëª©í‘œ ì‘ì—… ì„±ëŠ¥ ë³´ì¡´ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
- 4. Gemma 2 IT 2B, Llama 3 8B Instruct ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ë‹¹í˜¹ë„ í† í° ê°ì†Œê°€ ë¯¸ì„¸ ì¡°ì • í›„ì˜ ë§ê° ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” LLMì˜ ë¯¸ì„¸ ì¡°ì • í›„ ë§ê° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í† í° ë‹¹í˜¹ë„ ê°ì†Œ ê¸°ë°˜ì˜ ê²½í—˜ì  ì„¤ëª…ì„ ìµœì´ˆë¡œ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-24 03:47:01*