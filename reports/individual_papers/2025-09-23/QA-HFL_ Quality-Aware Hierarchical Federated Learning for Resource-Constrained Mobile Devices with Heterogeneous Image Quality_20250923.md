---
keywords:
  - Hierarchical Federated Learning
  - Differential Privacy
  - Quality-Weighted Fusion
  - Knowledge Distillation
  - Adaptive Weighting
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.05411
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:35:19.432912",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hierarchical Federated Learning",
    "Differential Privacy",
    "Quality-Weighted Fusion",
    "Knowledge Distillation",
    "Adaptive Weighting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hierarchical Federated Learning": 0.78,
    "Differential Privacy": 0.82,
    "Quality-Weighted Fusion": 0.75,
    "Knowledge Distillation": 0.8,
    "Adaptive Weighting": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Hierarchical Federated Learning",
        "canonical": "Hierarchical Federated Learning",
        "aliases": [
          "HFL"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a novel approach in federated learning, offering a unique perspective on model training across devices.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Differential Privacy",
        "canonical": "Differential Privacy",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Differential privacy is a key aspect of the framework, ensuring privacy-preserving model training, which is critical for federated learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Quality-Weighted Fusion",
        "canonical": "Quality-Weighted Fusion",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is a novel contribution of the paper, enhancing model performance by adapting to image quality variations.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Knowledge Distillation",
        "canonical": "Knowledge Distillation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Knowledge distillation is used for efficient model training and is a well-known technique in machine learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adaptive Weighting",
        "canonical": "Adaptive Weighting",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Adaptive weighting is a specific method used in the framework to handle device-specific variations, crucial for performance improvement.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.77,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "resource-constrained",
      "image quality",
      "federation rounds"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Hierarchical Federated Learning",
      "resolved_canonical": "Hierarchical Federated Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Differential Privacy",
      "resolved_canonical": "Differential Privacy",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Quality-Weighted Fusion",
      "resolved_canonical": "Quality-Weighted Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Knowledge Distillation",
      "resolved_canonical": "Knowledge Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adaptive Weighting",
      "resolved_canonical": "Adaptive Weighting",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.77,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# QA-HFL: Quality-Aware Hierarchical Federated Learning for Resource-Constrained Mobile Devices with Heterogeneous Image Quality

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.05411.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.05411](https://arxiv.org/abs/2506.05411)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Hybrid Reputation Aggregation_ A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments_20250923|Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments]] (85.0% similar)
- [[2025-09-23/Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation_ A Stagewise Decision-Making Methodology_20250923|Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation: A Stagewise Decision-Making Methodology]] (83.0% similar)
- [[2025-09-23/FedEL_ Federated Elastic Learning for Heterogeneous Devices_20250923|FedEL: Federated Elastic Learning for Heterogeneous Devices]] (82.9% similar)
- [[2025-09-22/Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning_20250922|Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning]] (82.7% similar)
- [[2025-09-23/FROQ_ Observing Face Recognition Models for Efficient Quality Assessment_20250923|FROQ: Observing Face Recognition Models for Efficient Quality Assessment]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Differential Privacy|Differential Privacy]], [[keywords/Knowledge Distillation|Knowledge Distillation]]
**âš¡ Unique Technical**: [[keywords/Hierarchical Federated Learning|Hierarchical Federated Learning]], [[keywords/Quality-Weighted Fusion|Quality-Weighted Fusion]], [[keywords/Adaptive Weighting|Adaptive Weighting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.05411v2 Announce Type: replace-cross 
Abstract: This paper introduces QA-HFL, a quality-aware hierarchical federated learning framework that efficiently handles heterogeneous image quality across resource-constrained mobile devices. Our approach trains specialized local models for different image quality levels and aggregates their features using a quality-weighted fusion mechanism, while incorporating differential privacy protection. Experiments on MNIST demonstrate that QA-HFL achieves 92.31% accuracy after just three federation rounds, significantly outperforming state-of-the-art methods like FedRolex (86.42%). Under strict privacy constraints, our approach maintains 30.77% accuracy with formal differential privacy guarantees. Counter-intuitively, low-end devices contributed most significantly (63.5%) to the final model despite using 100 fewer parameters than high-end counterparts. Our quality-aware approach addresses accuracy decline through device-specific regularization, adaptive weighting, intelligent client selection, and server-side knowledge distillation, while maintaining efficient communication with a 4.71% compression ratio. Statistical analysis confirms that our approach significantly outperforms baseline methods (p 0.01) under both standard and privacy-constrained conditions.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ QA-HFLì´ë¼ëŠ” í’ˆì§ˆ ì¸ì‹ ê³„ì¸µì  ì—°í•© í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ìì›ì´ ì œí•œëœ ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í’ˆì§ˆì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê°ê¸° ë‹¤ë¥¸ ì´ë¯¸ì§€ í’ˆì§ˆ ìˆ˜ì¤€ì— ë§ì¶˜ ë¡œì»¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , í’ˆì§ˆ ê°€ì¤‘ì¹˜ ìœµí•© ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ íŠ¹ì§•ì„ ì§‘ê³„í•˜ë©°, ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. MNIST ì‹¤í—˜ì—ì„œ QA-HFLì€ ì„¸ ë²ˆì˜ ì—°í•© ë¼ìš´ë“œ í›„ 92.31%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì—¬ FedRolex(86.42%) ë“± ìµœì‹  ë°©ë²•ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. ì—„ê²©í•œ í”„ë¼ì´ë²„ì‹œ ì œì•½ í•˜ì—ì„œë„ 30.77%ì˜ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, ì €ê°€í˜• ê¸°ê¸°ê°€ ì „ì²´ ëª¨ë¸ ê¸°ì—¬ë„ì˜ 63.5%ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ê¸°ê¸°ë³„ ì •ê·œí™”, ì ì‘í˜• ê°€ì¤‘ì¹˜, í´ë¼ì´ì–¸íŠ¸ ì„ íƒ, ì„œë²„ ì§€ì‹ ì¦ë¥˜ë¥¼ í†µí•´ ì •í™•ë„ ì €í•˜ë¥¼ í•´ê²°í•˜ê³ , 4.71%ì˜ ì••ì¶•ë¥ ë¡œ íš¨ìœ¨ì ì¸ í†µì‹ ì„ ìœ ì§€í•©ë‹ˆë‹¤. í†µê³„ ë¶„ì„ ê²°ê³¼, ì´ ë°©ë²•ì€ í‘œì¤€ ë° í”„ë¼ì´ë²„ì‹œ ì œì•½ ì¡°ê±´ ëª¨ë‘ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ìš°ìˆ˜í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. QA-HFLì€ ìì› ì œí•œì´ ìˆëŠ” ëª¨ë°”ì¼ ì¥ì¹˜ì—ì„œ ì´ì§ˆì ì¸ ì´ë¯¸ì§€ í’ˆì§ˆì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í’ˆì§ˆ ì¸ì‹ ê³„ì¸µì  ì—°í•© í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì´ ì ‘ê·¼ë²•ì€ ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ í’ˆì§ˆ ìˆ˜ì¤€ì— ëŒ€í•´ íŠ¹í™”ëœ ë¡œì»¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , í’ˆì§ˆ ê°€ì¤‘ì¹˜ ìœµí•© ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ì„ ì§‘ê³„í•©ë‹ˆë‹¤.
- 3. MNIST ì‹¤í—˜ì—ì„œ QA-HFLì€ 3ë²ˆì˜ ì—°í•© ë¼ìš´ë“œ í›„ 92.31%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì—¬ FedRolex(86.42%)ì™€ ê°™ì€ ìµœì‹  ë°©ë²•ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.
- 4. ì—„ê²©í•œ í”„ë¼ì´ë²„ì‹œ ì œì•½ ì¡°ê±´ í•˜ì—ì„œë„ QA-HFLì€ 30.77%ì˜ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, í˜•ì‹ì ì¸ ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. í†µê³„ ë¶„ì„ ê²°ê³¼, QA-HFLì€ í‘œì¤€ ë° í”„ë¼ì´ë²„ì‹œ ì œì•½ ì¡°ê±´ ëª¨ë‘ì—ì„œ ê¸°ì¤€ ë°©ë²•ë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤ (p 0.01).


---

*Generated on 2025-09-24 05:35:19*