---
keywords:
  - BiPrompt-SAM
  - Segment Anything Model
  - Intersection over Union
  - Zero-Shot Learning
  - Multimodal Learning
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2503.19769
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:03:19.266790",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "BiPrompt-SAM",
    "Segment Anything Model",
    "Intersection over Union",
    "Zero-Shot Learning",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "BiPrompt-SAM": 0.88,
    "Segment Anything Model": 0.82,
    "Intersection over Union": 0.79,
    "Zero-Shot Learning": 0.8,
    "Multimodal Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "BiPrompt-SAM",
        "canonical": "BiPrompt-SAM",
        "aliases": [
          "BiPrompt SAM",
          "BiPrompt"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel dual-modal prompt segmentation framework introduced in the paper.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Segment Anything Model",
        "canonical": "Segment Anything Model",
        "aliases": [
          "SAM"
        ],
        "category": "unique_technical",
        "rationale": "A key model discussed in the paper, central to the segmentation task.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Intersection over Union",
        "canonical": "Intersection over Union",
        "aliases": [
          "IoU"
        ],
        "category": "specific_connectable",
        "rationale": "A critical metric for evaluating spatial alignment in segmentation tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "Zero-Shot Performance",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the model's ability to perform without domain-specific training, linking to broader zero-shot learning concepts.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Encoders",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Emphasizes the integration of multiple data types, relevant to the paper's dual-modal approach.",
        "novelty_score": 0.58,
        "connectivity_score": 0.84,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "BiPrompt-SAM",
      "resolved_canonical": "BiPrompt-SAM",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Segment Anything Model",
      "resolved_canonical": "Segment Anything Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Intersection over Union",
      "resolved_canonical": "Intersection over Union",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Zero-Shot Performance",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Encoders",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.84,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2503.19769.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2503.19769](https://arxiv.org/abs/2503.19769)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DescriptorMedSAM_ Language-Image Fusion with Multi-Aspect Text Guidance for Medical Image Segmentation_20250923|DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance for Medical Image Segmentation]] (86.8% similar)
- [[2025-09-22/ENSAM_ an efficient foundation model for interactive segmentation of 3D medical images_20250922|ENSAM: an efficient foundation model for interactive segmentation of 3D medical images]] (83.8% similar)
- [[2025-09-22/pFedSAM_ Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation_20250922|pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation]] (83.2% similar)
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (82.8% similar)
- [[2025-09-22/TASAM_ Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation_20250922|TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Intersection over Union|Intersection over Union]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/BiPrompt-SAM|BiPrompt-SAM]], [[keywords/Segment Anything Model|Segment Anything Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.19769v3 Announce Type: replace-cross 
Abstract: Segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. The Segment Anything Model (SAM) excels at point-prompted segmentation, while text-based models, often leveraging powerful multimodal encoders like BEIT-3, provide rich semantic understanding. However, effectively combining these complementary modalities remains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal prompt segmentation framework employing an explicit selection mechanism. We leverage SAM's ability to generate multiple mask candidates from a single point prompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select the point-generated mask that best aligns spatially, measured by Intersection over Union (IoU). This approach, interpretable as a simplified Mixture of Experts (MoE), effectively fuses spatial precision and semantic context without complex model modifications. Notably, our method achieves strong zero-shot performance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using only a single point prompt per instance. This significantly reduces annotation burden compared to bounding boxes and aligns better with practical clinical workflows, demonstrating the method's effectiveness without domain-specific training. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8% IoU, significantly outperforming existing approaches. Experiments show BiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic disambiguation, offering a simple, effective, and interpretable perspective on multi-modal prompt fusion.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ ìƒˆë¡œìš´ ì´ì¤‘ ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ í”„ë ˆì„ì›Œí¬ì¸ BiPrompt-SAMì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ SAMì˜ í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„± ëŠ¥ë ¥ê³¼ BEIT-3ë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§ˆìŠ¤í¬ë¥¼ ê²°í•©í•˜ì—¬, IoUë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì˜ ë§ëŠ” ë§ˆìŠ¤í¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë³µì¡í•œ ëª¨ë¸ ìˆ˜ì • ì—†ì´ ê³µê°„ì  ì •ë°€ë„ì™€ ì˜ë¯¸ì  ë§¥ë½ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœµí•©í•©ë‹ˆë‹¤. BiPrompt-SAMì€ Endovis17 ì˜ë£Œ ë°ì´í„°ì…‹ì—ì„œ 89.55% mDiceì™€ 81.46% mIoUë¥¼ ê¸°ë¡í•˜ë©°, ë„ë©”ì¸ íŠ¹í™” í›ˆë ¨ ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ RefCOCO ì‹œë¦¬ì¦ˆì—ì„œë„ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ê³µê°„ì  ì •í™•ì„±ê³¼ ì˜ë¯¸ì  êµ¬ë¶„ì´ í•„ìš”í•œ ìƒí™©ì—ì„œ íš¨ê³¼ì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ ë‹¤ì¤‘ ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ìœµí•©ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. BiPrompt-SAMì€ ë“€ì–¼ ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ì„¸ë¶„í™” í”„ë ˆì„ì›Œí¬ë¡œ, ëª…ì‹œì  ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê³µê°„ì  ì •ë°€ì„±ê³¼ ì˜ë¯¸ì  ë§¥ë½ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤.
- 2. SAMì˜ ë‹¨ì¼ í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒì„±ëœ ì—¬ëŸ¬ ë§ˆìŠ¤í¬ í›„ë³´ ì¤‘ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§ˆìŠ¤í¬ë¥¼ ì´ìš©í•´ ìµœì ì˜ ë§ˆìŠ¤í¬ë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì„ ì±„íƒí•©ë‹ˆë‹¤.
- 3. BiPrompt-SAMì€ Endovis17 ì˜ë£Œ ë°ì´í„°ì…‹ì—ì„œ ê°•ë ¥í•œ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‹¤ì§ˆì ì¸ ì„ìƒ ì›Œí¬í”Œë¡œìš°ì™€ ì˜ ë§ì•„ ë„ë©”ì¸ íŠ¹í™” í›ˆë ¨ ì—†ì´ë„ íš¨ê³¼ì ì…ë‹ˆë‹¤.
- 4. RefCOCO ì‹œë¦¬ì¦ˆì—ì„œ BiPrompt-SAMì€ ê¸°ì¡´ ë°©ë²•ë“¤ì„ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” IoU ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
- 5. ì´ ë°©ë²•ì€ ê³µê°„ì  ì •í™•ì„±ê³¼ ì˜ë¯¸ì  ëª…í™•ì„±ì´ ìš”êµ¬ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, ë³µì¡í•œ ëª¨ë¸ ìˆ˜ì • ì—†ì´ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ìœµí•©ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:03:19*