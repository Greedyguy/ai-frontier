---
keywords:
  - Large Language Model
  - seqBench
  - Sequential Reasoning
  - Logical Depth
  - Commonsense Reasoning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16866
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:53:37.618580",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "seqBench",
    "Sequential Reasoning",
    "Logical Depth",
    "Commonsense Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "seqBench": 0.8,
    "Sequential Reasoning": 0.78,
    "Logical Depth": 0.72,
    "Commonsense Reasoning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus, linking to existing discussions on LLM capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "seqBench",
        "canonical": "seqBench",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a new benchmark specific to the paper, useful for linking related research.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "sequential reasoning",
        "canonical": "Sequential Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key concept for understanding LLM limitations in the paper.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "logical depth",
        "canonical": "Logical Depth",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific metric introduced in the paper for evaluating reasoning complexity.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "commonsense reasoning",
        "canonical": "Commonsense Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights a critical area of LLM evaluation discussed in the paper.",
        "novelty_score": 0.45,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "accuracy",
      "evaluation metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "seqBench",
      "resolved_canonical": "seqBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "sequential reasoning",
      "resolved_canonical": "Sequential Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "logical depth",
      "resolved_canonical": "Logical Depth",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "commonsense reasoning",
      "resolved_canonical": "Commonsense Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16866.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16866](https://arxiv.org/abs/2509.16866)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (88.0% similar)
- [[2025-09-23/On LLM-Based Scientific Inductive Reasoning Beyond Equations_20250923|On LLM-Based Scientific Inductive Reasoning Beyond Equations]] (86.9% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (85.4% similar)
- [[2025-09-22/How Good are Foundation Models in Step-by-Step Embodied Reasoning?_20250922|How Good are Foundation Models in Step-by-Step Embodied Reasoning?]] (84.7% similar)
- [[2025-09-23/Roundtable Policy_ Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs_20250923|Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Sequential Reasoning|Sequential Reasoning]], [[keywords/Commonsense Reasoning|Commonsense Reasoning]]
**âš¡ Unique Technical**: [[keywords/seqBench|seqBench]], [[keywords/Logical Depth|Logical Depth]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16866v1 Announce Type: new 
Abstract: We introduce seqBench, a parametrized benchmark for probing sequential reasoning limits in Large Language Models (LLMs) through precise, multi-dimensional control over several key complexity dimensions. seqBench allows systematic variation of (1) the logical depth, defined as the number of sequential actions required to solve the task; (2) the number of backtracking steps along the optimal path, quantifying how often the agent must revisit prior states to satisfy deferred preconditions (e.g., retrieving a key after encountering a locked door); and (3) the noise ratio, defined as the ratio between supporting and distracting facts about the environment. Our evaluations on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses exponentially beyond a model-specific logical depth. Unlike existing benchmarks, seqBench's fine-grained control facilitates targeted analyses of these reasoning failures, illuminating universal scaling laws and statistical limits, as detailed in this paper alongside its generation methodology and evaluation metrics. We find that even top-performing models systematically fail on seqBench's structured reasoning tasks despite minimal search complexity, underscoring key limitations in their commonsense reasoning capabilities. Designed for future evolution to keep pace with advancing models, the seqBench datasets are publicly released to spur deeper scientific inquiry into LLM reasoning, aiming to establish a clearer understanding of their true potential and current boundaries for robust real-world application.

## ğŸ“ ìš”ì•½

seqBenchëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìˆœì°¨ì  ì¶”ë¡  í•œê³„ë¥¼ íƒêµ¬í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜í™”ëœ ë²¤ì¹˜ë§ˆí¬ë¡œ, ë…¼ë¦¬ì  ê¹Šì´, ìµœì  ê²½ë¡œì—ì„œì˜ ë°±íŠ¸ë˜í‚¹ ë‹¨ê³„ ìˆ˜, í™˜ê²½ì— ëŒ€í•œ ì§€ì› ë° ë°©í•´ ì‚¬ì‹¤ ë¹„ìœ¨ ë“±ì˜ ë³µì¡ì„± ì°¨ì›ì„ ì •ë°€í•˜ê²Œ ì œì–´í•©ë‹ˆë‹¤. ìµœì‹  LLM í‰ê°€ ê²°ê³¼, ëª¨ë¸ë³„ ë…¼ë¦¬ì  ê¹Šì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì •í™•ë„ê°€ ê¸‰ê²©íˆ ê°ì†Œí•˜ëŠ” ë³´í¸ì  ì‹¤íŒ¨ íŒ¨í„´ì´ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤. seqBenchëŠ” ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ì™€ ë‹¬ë¦¬ ì„¸ë¶„í™”ëœ ì œì–´ë¥¼ í†µí•´ ì´ëŸ¬í•œ ì¶”ë¡  ì‹¤íŒ¨ë¥¼ ë¶„ì„í•˜ê³ , ëª¨ë¸ì˜ ìƒì‹ ì¶”ë¡  í•œê³„ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. seqBench ë°ì´í„°ì…‹ì€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì— ëŒ€í•œ ì‹¬ì¸µì  ê³¼í•™ì  íƒêµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. seqBenchëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìˆœì°¨ì  ì¶”ë¡  í•œê³„ë¥¼ íƒêµ¬í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜í™”ëœ ë²¤ì¹˜ë§ˆí¬ë¡œ, ì—¬ëŸ¬ ë³µì¡ì„± ì°¨ì›ì„ ì •ë°€í•˜ê²Œ ì œì–´í•©ë‹ˆë‹¤.
- 2. seqBenchëŠ” ë…¼ë¦¬ì  ê¹Šì´, ìµœì  ê²½ë¡œì—ì„œì˜ ë°±íŠ¸ë˜í‚¹ ë‹¨ê³„ ìˆ˜, í™˜ê²½ì— ëŒ€í•œ ì§€ì› ë° ë°©í•´ ì‚¬ì‹¤ì˜ ë¹„ìœ¨ì„ ì²´ê³„ì ìœ¼ë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤.
- 3. ìµœì²¨ë‹¨ LLMì— ëŒ€í•œ í‰ê°€ ê²°ê³¼, ëª¨ë¸ë³„ ë…¼ë¦¬ì  ê¹Šì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì •í™•ë„ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ë³´í¸ì ì¸ ì‹¤íŒ¨ íŒ¨í„´ì´ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.
- 4. seqBenchëŠ” ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ì™€ ë‹¬ë¦¬ ì„¸ë°€í•œ ì œì–´ë¥¼ í†µí•´ ì´ëŸ¬í•œ ì¶”ë¡  ì‹¤íŒ¨ì— ëŒ€í•œ ëª©í‘œ ë¶„ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ë³´í¸ì ì¸ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ê³¼ í†µê³„ì  í•œê³„ë¥¼ ë°í˜€ëƒ…ë‹ˆë‹¤.
- 5. seqBench ë°ì´í„°ì…‹ì€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì— ëŒ€í•œ ë” ê¹Šì€ ê³¼í•™ì  íƒêµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ ê³µê°œë˜ì—ˆìœ¼ë©°, ì‹¤ì„¸ê³„ ì‘ìš©ì„ ìœ„í•œ ëª…í™•í•œ ì´í•´ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 22:53:37*