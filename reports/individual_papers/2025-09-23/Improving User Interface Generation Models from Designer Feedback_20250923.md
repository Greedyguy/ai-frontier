---
keywords:
  - Large Language Model
  - User Interface Generation
  - Designer Feedback
  - Reinforcement Learning from Human Feedback
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16779
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:15:23.280298",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "User Interface Generation",
    "Designer Feedback",
    "Reinforcement Learning from Human Feedback"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "User Interface Generation": 0.78,
    "Designer Feedback": 0.77,
    "Reinforcement Learning from Human Feedback": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Essential for linking as it is the core technology being improved in the study.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "UI Generation",
        "canonical": "User Interface Generation",
        "aliases": [
          "UI Design Generation"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on improving design models with feedback.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Designer Feedback",
        "canonical": "Designer Feedback",
        "aliases": [
          "Design Feedback"
        ],
        "category": "unique_technical",
        "rationale": "Key element in the study for enhancing model performance through human input.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Reinforcement Learning from Human Feedback",
        "canonical": "Reinforcement Learning from Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant for linking due to its role in training models with human input.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "study",
      "models"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "UI Generation",
      "resolved_canonical": "User Interface Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Designer Feedback",
      "resolved_canonical": "Designer Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Human Feedback",
      "resolved_canonical": "Reinforcement Learning from Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Improving User Interface Generation Models from Designer Feedback

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16779.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16779](https://arxiv.org/abs/2509.16779)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (85.2% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (85.0% similar)
- [[2025-09-23/Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues_20250923|Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues]] (84.7% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (84.4% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (84.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning from Human Feedback|Reinforcement Learning from Human Feedback]]
**âš¡ Unique Technical**: [[keywords/User Interface Generation|User Interface Generation]], [[keywords/Designer Feedback|Designer Feedback]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16779v1 Announce Type: cross 
Abstract: Despite being trained on vast amounts of data, most LLMs are unable to reliably generate well-designed UIs. Designer feedback is essential to improving performance on UI generation; however, we find that existing RLHF methods based on ratings or rankings are not well-aligned with designers' workflows and ignore the rich rationale used to critique and improve UI designs. In this paper, we investigate several approaches for designers to give feedback to UI generation models, using familiar interactions such as commenting, sketching and direct manipulation. We first perform a study with 21 designers where they gave feedback using these interactions, which resulted in ~1500 design annotations. We then use this data to finetune a series of LLMs to generate higher quality UIs. Finally, we evaluate these models with human judges, and we find that our designer-aligned approaches outperform models trained with traditional ranking feedback and all tested baselines, including GPT-5.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(UI)ë¥¼ ìƒì„±í•˜ëŠ” ë° ìˆì–´ ë””ìì´ë„ˆì˜ í”¼ë“œë°±ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìˆœìœ„ ê¸°ë°˜ ê°•í™” í•™ìŠµ(RLHF) ë°©ë²•ì€ ë””ìì´ë„ˆì˜ ì‘ì—… íë¦„ê³¼ ì˜ ë§ì§€ ì•Šìœ¼ë©°, UI ë””ìì¸ ê°œì„ ì— ì‚¬ìš©ë˜ëŠ” í’ë¶€í•œ ë…¼ë¦¬ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤. ì—°êµ¬ì§„ì€ ë””ìì´ë„ˆê°€ ëŒ“ê¸€, ìŠ¤ì¼€ì¹˜, ì§ì ‘ ì¡°ì‘ ë“± ìµìˆ™í•œ ë°©ì‹ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ì ‘ê·¼ë²•ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. 21ëª…ì˜ ë””ìì´ë„ˆì™€ì˜ ì—°êµ¬ë¥¼ í†µí•´ ì•½ 1500ê°œì˜ ë””ìì¸ ì£¼ì„ì„ ìˆ˜ì§‘í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë” ë†’ì€ í’ˆì§ˆì˜ UIë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ì¸ê°„ í‰ê°€ìì™€ì˜ í‰ê°€ì—ì„œ ì´ëŸ¬í•œ ë””ìì´ë„ˆ ì¤‘ì‹¬ ì ‘ê·¼ë²•ì´ ê¸°ì¡´ì˜ ìˆœìœ„ í”¼ë“œë°± ë° GPT-5ë¥¼ í¬í•¨í•œ ëª¨ë“  í…ŒìŠ¤íŠ¸ëœ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ë¶€ë¶„ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ ë°ì´í„°ë¡œ í›ˆë ¨ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì˜ ì„¤ê³„ëœ UIë¥¼ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ í‰ê°€ ê¸°ë°˜ ê°•í™” í•™ìŠµ(RLHF) ë°©ë²•ì€ ë””ìì´ë„ˆì˜ ì›Œí¬í”Œë¡œìš°ì™€ ì˜ ë§ì§€ ì•Šìœ¼ë©°, UI ë””ìì¸ì„ ê°œì„ í•˜ê¸° ìœ„í•œ í’ë¶€í•œ ê·¼ê±°ë¥¼ ë¬´ì‹œí•˜ê³  ìˆë‹¤.
- 3. ë””ìì´ë„ˆê°€ ì£¼ì„ ë‹¬ê¸°, ìŠ¤ì¼€ì¹˜, ì§ì ‘ ì¡°ì‘ê³¼ ê°™ì€ ìµìˆ™í•œ ìƒí˜¸ì‘ìš©ì„ í†µí•´ UI ìƒì„± ëª¨ë¸ì— í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì—¬ëŸ¬ ì ‘ê·¼ ë°©ì‹ì„ ì¡°ì‚¬í•˜ì˜€ë‹¤.
- 4. 21ëª…ì˜ ë””ìì´ë„ˆê°€ ì°¸ì—¬í•œ ì—°êµ¬ì—ì„œ ì•½ 1500ê°œì˜ ë””ìì¸ ì£¼ì„ì´ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©°, ì´ë¥¼ í†µí•´ LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë” ë†’ì€ í’ˆì§ˆì˜ UIë¥¼ ìƒì„±í•˜ì˜€ë‹¤.
- 5. ì¸ê°„ ì‹¬ì‚¬ìœ„ì›ì„ í†µí•´ í‰ê°€í•œ ê²°ê³¼, ë””ìì´ë„ˆì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ë°˜ì˜í•œ ì ‘ê·¼ ë°©ì‹ì´ ê¸°ì¡´ì˜ ìˆœìœ„ ê¸°ë°˜ í”¼ë“œë°± ë° GPT-5ë¥¼ í¬í•¨í•œ ëª¨ë“  í…ŒìŠ¤íŠ¸ëœ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 02:15:23*