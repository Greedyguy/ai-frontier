---
keywords:
  - Multimodal Learning
  - Computer Vision
  - Embedding Distillation
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2412.09585
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:16:37.829966",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Computer Vision",
    "Embedding Distillation",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.78,
    "Computer Vision": 0.72,
    "Embedding Distillation": 0.77,
    "Vision-Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal LLMs",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Large Language Models"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to recent trends in integrating multiple data modalities within language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Visual Perception",
        "canonical": "Computer Vision",
        "aliases": [
          "Visual Understanding"
        ],
        "category": "broad_technical",
        "rationale": "Links to the domain of computer vision which is integral to the paper's focus on visual data.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Embedding Distillation",
        "canonical": "Embedding Distillation",
        "aliases": [
          "Knowledge Distillation for Embeddings"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel technique specific to the paper's methodology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision Encoder",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision Encoder Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents the integration of vision and language processing, a key concept in the study.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.68,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "natural language supervision",
      "spatial reasoning",
      "embodied AI"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal LLMs",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Visual Perception",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Embedding Distillation",
      "resolved_canonical": "Embedding Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision Encoder",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.68,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2412.09585.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2412.09585](https://arxiv.org/abs/2412.09585)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (87.9% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (86.4% similar)
- [[2025-09-23/Catching the Details_ Self-Distilled RoI Predictors for Fine-Grained MLLM Perception_20250923|Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception]] (86.1% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (84.6% similar)
- [[2025-09-23/GeoPQA_ Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning_20250923|GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Computer Vision|Computer Vision]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Embedding Distillation|Embedding Distillation]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2412.09585v2 Announce Type: replace 
Abstract: In recent times, the standard practice for developing MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. This approach often causes models to lean towards language comprehension and undermine the rich visual perception signals present in the data, which are critical for tasks involving spatial reasoning in the domain of embodied AI and robotics. Is it possible to optimize both at the same time? In this work, we propose VisPer-LM, the first approach that infuses visual perception knowledge from expert vision encoders into the LLM's (of an MLLM) hidden representations. We start by investigating MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Given this insight, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next (text) token prediction. Moreover, through extensive probing, we observe improved visual representation quality due to embedding optimization, underscoring the effectiveness of our probing setup. We demonstrate that our VisPer-LM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. In particular, VisPer-LM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench.

## ğŸ“ ìš”ì•½

ìµœê·¼ MLLM ê°œë°œì—ì„œëŠ” ë¹„ì „ ì¸ì½”ë”ì˜ íŠ¹ì§•ì„ LLMì— ì…ë ¥í•˜ê³  ìì—°ì–´ ê°ë…ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ì–¸ì–´ ì´í•´ì— ì¹˜ìš°ì³ ê³µê°„ì  ì¶”ë¡ ì´ ì¤‘ìš”í•œ ì˜ì—­ì—ì„œ ì‹œê°ì  ì¸ì‹ ì‹ í˜¸ë¥¼ ê°„ê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” VisPer-LMì„ ì œì•ˆí•˜ì—¬ ì „ë¬¸ê°€ ë¹„ì „ ì¸ì½”ë”ì˜ ì‹œê°ì  ì¸ì‹ ì§€ì‹ì„ MLLMì˜ ìˆ¨ê²¨ì§„ í‘œí˜„ì— ì£¼ì…í•©ë‹ˆë‹¤. ìì—°ì–´ ê°ë…ìœ¼ë¡œë§Œ í•™ìŠµëœ MLLMì„ ë¶„ì„í•œ ê²°ê³¼, ì‹œê°ì  í‘œí˜„ì˜ ì§ˆê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ ê°„ì˜ ê¸ì •ì  ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ì‹œê° ì„ë² ë”©ê³¼ ë‹¤ìŒ í…ìŠ¤íŠ¸ í† í° ì˜ˆì¸¡ì˜ ê²°í•© ìµœì í™”ë¥¼ ëª©í‘œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. VisPer-LMì€ ë‹¨ì¼ ë° ë‹¤ì¤‘ ì¸ì½”ë” ê¸°ë°˜ì„ ëŠ¥ê°€í•˜ë©°, íŠ¹íˆ CV-Benchì˜ Depth ì‘ì—…ì—ì„œ 8.7%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VisPer-LMì€ ì „ë¬¸ ë¹„ì „ ì¸ì½”ë”ì˜ ì‹œê°ì  ì§€ì‹ì„ MLLMì˜ ìˆ¨ê²¨ì§„ í‘œí˜„ì— ì£¼ì…í•˜ëŠ” ìµœì´ˆì˜ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ìì—°ì–´ ê°ë…ë§Œìœ¼ë¡œ í›ˆë ¨ëœ MLLMì—ì„œ ì‹œê°ì  í‘œí˜„ì˜ í’ˆì§ˆê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ ê°„ì˜ ê¸ì •ì ì¸ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- 3. ì‚¬ì „ í›ˆë ¨ ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡ ì‹œê° ì„ë² ë”©ê³¼ ë‹¤ìŒ í…ìŠ¤íŠ¸ í† í° ì˜ˆì¸¡ì˜ ê²°í•© ìµœì í™”ë¥¼ ëª©í‘œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
- 4. VisPer-LMì€ ë‹¨ì¼ ë° ë‹¤ì¤‘ ì¸ì½”ë” ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë©°, CV-Benchì˜ Depth ê³¼ì œì—ì„œ 8.7%ì˜ ì„±ëŠ¥ í–¥ìƒì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê·  2.5%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì„ë² ë”© ìµœì í™”ë¥¼ í†µí•´ ì‹œê°ì  í‘œí˜„ì˜ í’ˆì§ˆì´ í–¥ìƒë˜ì—ˆìŒì„ ê´‘ë²”ìœ„í•œ í”„ë¡œë¹™ì„ í†µí•´ ê´€ì°°í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:16:37*