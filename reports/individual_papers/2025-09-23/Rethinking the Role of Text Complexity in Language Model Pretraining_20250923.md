---
keywords:
  - Text Complexity
  - Language Model Pretraining
  - Zero-Shot Learning
  - Simplified Texts
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16551
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:25:32.254480",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text Complexity",
    "Language Model Pretraining",
    "Zero-Shot Learning",
    "Simplified Texts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text Complexity": 0.75,
    "Language Model Pretraining": 0.8,
    "Zero-Shot Learning": 0.78,
    "Simplified Texts": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "text complexity",
        "canonical": "Text Complexity",
        "aliases": [
          "complexity of text",
          "reading difficulty"
        ],
        "category": "unique_technical",
        "rationale": "Text complexity is central to the study and impacts language model performance, making it a unique technical concept.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "language model pretraining",
        "canonical": "Language Model Pretraining",
        "aliases": [
          "pretraining of language models"
        ],
        "category": "broad_technical",
        "rationale": "Pretraining is a foundational process in NLP, crucial for understanding model development and performance.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "zero-shot evaluations",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot tasks"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a trending concept that connects to broader discussions on model evaluation and adaptation.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "simplified texts",
        "canonical": "Simplified Texts",
        "aliases": [
          "text simplification",
          "simpler text"
        ],
        "category": "unique_technical",
        "rationale": "The use of simplified texts in pretraining is a unique approach that could influence model performance and understanding.",
        "novelty_score": 0.65,
        "connectivity_score": 0.5,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "downstream performance",
      "sentence structure"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "text complexity",
      "resolved_canonical": "Text Complexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "language model pretraining",
      "resolved_canonical": "Language Model Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "zero-shot evaluations",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "simplified texts",
      "resolved_canonical": "Simplified Texts",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.5,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Rethinking the Role of Text Complexity in Language Model Pretraining

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16551.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16551](https://arxiv.org/abs/2509.16551)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (83.5% similar)
- [[2025-09-22/Once Upon a Time_ Interactive Learning for Storytelling with Small Language Models_20250922|Once Upon a Time: Interactive Learning for Storytelling with Small Language Models]] (80.7% similar)
- [[2025-09-22/Measuring Lexical Diversity of Synthetic Data Generated through Fine-Grained Persona Prompting_20250922|Measuring Lexical Diversity of Synthetic Data Generated through Fine-Grained Persona Prompting]] (80.3% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (80.1% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (79.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Language Model Pretraining|Language Model Pretraining]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Text Complexity|Text Complexity]], [[keywords/Simplified Texts|Simplified Texts]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16551v1 Announce Type: cross 
Abstract: Improving pretraining data quality and size is known to boost downstream performance, but the role of text complexity is less explored. Text complexity refers to how hard a text is to read, and is typically estimated from surface cues such as sentence length, word choice, and sentence structure. We reduce surface-level complexity--shorter sentences, simpler words, simpler structure--while keeping core text content close to constant, and ask: (1) How does complexity affect language modeling across model sizes? (2) Can useful representations be learned from simpler text alone? (3) How does pretraining text complexity influence downstream language understanding? To answer these questions, we simplify human-written texts using a large language model, then pretrain causal models (28M-500M) from scratch on both original and simplified data, and evaluate them in finetuning and zero-shot setups. We find that perplexity is sensitive to the interaction between model capacity and text complexity--smaller models degrade far less on simpler texts--while text complexity has little impact on finetuning evaluations, with zero-shot evaluations indicating that simpler texts benefit performance on linguistic knowledge tasks, whereas more complex texts favor tasks requiring world knowledge and entity tracking.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í…ìŠ¤íŠ¸ ë³µì¡ì„±ì´ ì‚¬ì „ í•™ìŠµê³¼ í›„ì† ì‘ì—… ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” ë¬¸ì¥ ê¸¸ì´, ë‹¨ì–´ ì„ íƒ, ë¬¸ì¥ êµ¬ì¡° ë“±ì„ ë‹¨ìˆœí™”í•˜ì—¬ í…ìŠ¤íŠ¸ ë³µì¡ì„±ì„ ì¤„ì´ê³ , ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸(28M-500M)ì„ ì‚¬ìš©í•´ ì›ë³¸ ë° ë‹¨ìˆœí™”ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ëª¨ë¸ì˜ ìš©ëŸ‰ê³¼ í…ìŠ¤íŠ¸ ë³µì¡ì„± ê°„ì˜ ìƒí˜¸ì‘ìš©ì— ë”°ë¼ ë‹¹í˜¹ë„ê°€ ë‹¬ë¼ì§€ë©°, ì‘ì€ ëª¨ë¸ì€ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ëœí•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ëŠ” ì–¸ì–´ ì§€ì‹ ê³¼ì œì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°˜ë©´, ë³µì¡í•œ í…ìŠ¤íŠ¸ëŠ” ì„¸ê³„ ì§€ì‹ ë° ì—”í‹°í‹° ì¶”ì ì´ í•„ìš”í•œ ê³¼ì œì—ì„œ ìœ ë¦¬í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í…ìŠ¤íŠ¸ ë³µì¡ì„±ì€ ë¬¸ì¥ ê¸¸ì´, ë‹¨ì–´ ì„ íƒ, ë¬¸ì¥ êµ¬ì¡°ì™€ ê°™ì€ í‘œë©´ì  ìš”ì†Œë¡œ ì¶”ì •ë˜ë©°, ì´ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ì–¸ì–´ ëª¨ë¸ë§ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì—°êµ¬í•œë‹¤.
- 2. ëª¨ë¸ í¬ê¸°ì™€ í…ìŠ¤íŠ¸ ë³µì¡ì„± ê°„ì˜ ìƒí˜¸ì‘ìš©ì— ë”°ë¼ í¼í”Œë ‰ì‹œí‹°ê°€ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ë©°, ì‘ì€ ëª¨ë¸ì€ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ì ë‹¤.
- 3. í…ìŠ¤íŠ¸ ë³µì¡ì„±ì€ íŒŒì¸íŠœë‹ í‰ê°€ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì§€ë§Œ, ì œë¡œìƒ· í‰ê°€ì—ì„œëŠ” ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ê°€ ì–¸ì–´ ì§€ì‹ ê³¼ì œì— ìœ ë¦¬í•˜ê³  ë³µì¡í•œ í…ìŠ¤íŠ¸ê°€ ì„¸ê³„ ì§€ì‹ ë° ì—”í‹°í‹° ì¶”ì  ê³¼ì œì— ìœ ë¦¬í•˜ë‹¤.
- 4. ì¸ê°„ì´ ì‘ì„±í•œ í…ìŠ¤íŠ¸ë¥¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ë¡œ ë‹¨ìˆœí™”í•˜ì—¬ ì›ë³¸ê³¼ ë‹¨ìˆœí™”ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ì¸ê³¼ ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•˜ê³  í‰ê°€í•œë‹¤.
- 5. ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œë„ ìœ ìš©í•œ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ íƒêµ¬í•œë‹¤.


---

*Generated on 2025-09-23 23:25:32*