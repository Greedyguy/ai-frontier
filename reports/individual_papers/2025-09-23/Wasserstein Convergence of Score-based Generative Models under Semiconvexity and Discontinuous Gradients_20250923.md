---
keywords:
  - Score-based Generative Models
  - Wasserstein Convergence
  - Semiconvexity
  - Gaussian Mixtures
  - Reverse Diffusion Process
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2505.03432
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:42:07.268362",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Score-based Generative Models",
    "Wasserstein Convergence",
    "Semiconvexity",
    "Gaussian Mixtures",
    "Reverse Diffusion Process"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Score-based Generative Models": 0.78,
    "Wasserstein Convergence": 0.77,
    "Semiconvexity": 0.75,
    "Gaussian Mixtures": 0.7,
    "Reverse Diffusion Process": 0.76
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Score-based Generative Models",
        "canonical": "Score-based Generative Models",
        "aliases": [
          "SGMs"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's contributions and represents a specific class of generative models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Wasserstein Convergence",
        "canonical": "Wasserstein Convergence",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The paper provides new theoretical insights into this concept, which is crucial for understanding convergence in generative models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Semiconvexity",
        "canonical": "Semiconvexity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Semiconvexity is a key mathematical property leveraged in the paper to establish convergence results.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Gaussian Mixtures",
        "canonical": "Gaussian Mixtures",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Gaussian mixtures are a common example of complex distributions discussed in the paper.",
        "novelty_score": 0.45,
        "connectivity_score": 0.72,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "Reverse Diffusion Process",
        "canonical": "Reverse Diffusion Process",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This process is fundamental to the functioning of SGMs and is a core concept in the paper.",
        "novelty_score": 0.66,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.76
      }
    ],
    "ban_list_suggestions": [
      "data distribution",
      "performance",
      "state-of-the-art"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Score-based Generative Models",
      "resolved_canonical": "Score-based Generative Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Wasserstein Convergence",
      "resolved_canonical": "Wasserstein Convergence",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Semiconvexity",
      "resolved_canonical": "Semiconvexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Gaussian Mixtures",
      "resolved_canonical": "Gaussian Mixtures",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.72,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Reverse Diffusion Process",
      "resolved_canonical": "Reverse Diffusion Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.66,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.76
      }
    }
  ]
}
-->

# Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.03432.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2505.03432](https://arxiv.org/abs/2505.03432)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (81.8% similar)
- [[2025-09-22/G2D2_ Gradient-Guided Discrete Diffusion for Inverse Problem Solving_20250922|G2D2: Gradient-Guided Discrete Diffusion for Inverse Problem Solving]] (81.6% similar)
- [[2025-09-19/A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation_20250919|A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation]] (79.8% similar)
- [[2025-09-23/DoubleGen_ Debiased Generative Modeling of Counterfactuals_20250923|DoubleGen: Debiased Generative Modeling of Counterfactuals]] (79.6% similar)
- [[2025-09-22/Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation_20250922|Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation]] (79.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Gaussian Mixtures|Gaussian Mixtures]]
**âš¡ Unique Technical**: [[keywords/Score-based Generative Models|Score-based Generative Models]], [[keywords/Wasserstein Convergence|Wasserstein Convergence]], [[keywords/Semiconvexity|Semiconvexity]], [[keywords/Reverse Diffusion Process|Reverse Diffusion Process]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.03432v2 Announce Type: replace 
Abstract: Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions-such as smoothness or strict log-concavity of the data distribution-that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Score-based Generative Models (SGMs)ì˜ Wasserstein-2 ìˆ˜ë ´ ë³´ì¥ì„ ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì—°ì†ì ì¸ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§„ ë°˜ë³¼ë¡ ë¶„í¬ì— ëŒ€í•´ ìµœì´ˆë¡œ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¶„ì„ì€ ë§¤ë„ëŸ¬ì›€ì´ë‚˜ ì—„ê²©í•œ ë¡œê·¸ ì˜¤ëª©ì„±ê³¼ ê°™ì€ ê°•í•œ ì •ê·œì„±ì„ ê°€ì •í–ˆìœ¼ë‚˜, ë³¸ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ê°€ì • ì—†ì´ë„ ëª…ì‹œì ì´ê³  ë‚ ì¹´ë¡œìš´ ìƒí•œì„ ì œê³µí•˜ì—¬ ë°ì´í„° ì°¨ì› $d$ì— ëŒ€í•´ $O(\sqrt{d})$ì˜ ìµœì  ì˜ì¡´ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ë¡œì¨ ëŒ€ì¹­ ìˆ˜ì • ë°˜ì •ê·œ ë¶„í¬, ê°€ìš°ì‹œì•ˆ í˜¼í•©, ì´ì¤‘ ìš°ë¬¼ í¬í…ì…œ, ì—˜ë¼ìŠ¤í‹± ë„· í¬í…ì…œ ë“± ë‹¤ì–‘í•œ ë¶„í¬ë¥¼ í¬ê´„í•˜ë©°, SGMsì˜ ì´ë¡ ì  ê¸°ë°˜ì„ í™•ì¥í•˜ì—¬ ë³µì¡í•œ ë°ì´í„° í™˜ê²½ì—ì„œì˜ ê²½í—˜ì  ì„±ê³µê³¼ ì—„ë°€í•œ ë³´ì¥ ê°„ì˜ ê°„ê·¹ì„ ì¢í™ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Score-based Generative Models(SGMs)ëŠ” ë³µì¡í•œ ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³  ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ë° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 2. ê¸°ì¡´ì˜ Wasserstein-2 ìˆ˜ë ´ ë¶„ì„ì€ ë°ì´í„° ë¶„í¬ì˜ ë§¤ë„ëŸ¬ì›€ì´ë‚˜ ì—„ê²©í•œ ë¡œê·¸ ë³¼ë¡ì„± ê°™ì€ ê°•í•œ ì •ê·œì„± ì¡°ê±´ì„ ê°€ì •í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë“œë¬¼ê²Œ ë§Œì¡±ëœë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” ì ì¬ì ìœ¼ë¡œ ë¶ˆì—°ì†ì ì¸ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§„ ë°˜ë³¼ë¡ ë¶„í¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” SGMsì— ëŒ€í•´ ìµœì´ˆì˜ ë¹„ì ê·¼ì  Wasserstein-2 ìˆ˜ë ´ ë³´ì¥ì„ í™•ë¦½í–ˆë‹¤.
- 4. ì œì•ˆëœ ìƒí•œì€ ë°ì´í„° ì°¨ì› $d$ì— ëŒ€í•´ $O(\sqrt{d})$ì˜ ìµœì  ì˜ì¡´ì„±ê³¼ 1ì°¨ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ëŒ€ì¹­ ìˆ˜ì • ë°˜ì •ê·œ ë¶„í¬, ê°€ìš°ì‹œì•ˆ í˜¼í•©, ì´ì¤‘ ìš°ë¬¼ í¬í…ì…œ, íƒ„ì„± ë„¤íŠ¸ í¬í…ì…œì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ë¶„í¬ë¥¼ í¬ê´„í•œë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ë§¤ë„ëŸ¬ì›€ ê°€ì • ì—†ì´ ë°˜ë³¼ë¡ì„±ì„ í™œìš©í•˜ì—¬ SGMsì˜ ì´ë¡ ì  ê¸°ì´ˆë¥¼ í™•ì¥í•˜ê³ , ë¹„ë§¤ë„ëŸ¬ìš´ ë³µì¡í•œ ë°ì´í„° í™˜ê²½ì—ì„œì˜ ê²½í—˜ì  ì„±ê³µê³¼ ì—„ê²©í•œ ë³´ì¥ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì¤„ì¸ë‹¤.


---

*Generated on 2025-09-24 02:42:07*