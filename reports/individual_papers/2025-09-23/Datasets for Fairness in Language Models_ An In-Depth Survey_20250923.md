---
keywords:
  - Fairness Benchmarks
  - Large Language Model
  - Demographic Disparities
  - Evaluation Framework
  - Social Contexts
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.23411
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:06:43.960051",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Fairness Benchmarks",
    "Large Language Model",
    "Demographic Disparities",
    "Evaluation Framework",
    "Social Contexts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Fairness Benchmarks": 0.85,
    "Large Language Model": 0.8,
    "Demographic Disparities": 0.77,
    "Evaluation Framework": 0.79,
    "Social Contexts": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "fairness benchmarks",
        "canonical": "Fairness Benchmarks",
        "aliases": [
          "equity benchmarks",
          "bias benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on evaluating language models for fairness, providing a unique perspective.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs",
          "language processing models"
        ],
        "category": "broad_technical",
        "rationale": "Key technology discussed in the paper, linking to broader technical discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "demographic disparities",
        "canonical": "Demographic Disparities",
        "aliases": [
          "demographic biases",
          "population disparities"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights a specific issue in fairness evaluation, crucial for understanding biases in datasets.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "evaluation framework",
        "canonical": "Evaluation Framework",
        "aliases": [
          "assessment framework",
          "evaluation model"
        ],
        "category": "unique_technical",
        "rationale": "Proposed framework in the paper that provides a structured approach to analyzing datasets.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "social contexts",
        "canonical": "Social Contexts",
        "aliases": [
          "societal contexts",
          "cultural contexts"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the broader implications of fairness in language models.",
        "novelty_score": 0.58,
        "connectivity_score": 0.7,
        "specificity_score": 0.73,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "datasets",
      "results",
      "code"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "fairness benchmarks",
      "resolved_canonical": "Fairness Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "demographic disparities",
      "resolved_canonical": "Demographic Disparities",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "evaluation framework",
      "resolved_canonical": "Evaluation Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "social contexts",
      "resolved_canonical": "Social Contexts",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.7,
        "specificity": 0.73,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Datasets for Fairness in Language Models: An In-Depth Survey

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.23411.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.23411](https://arxiv.org/abs/2506.23411)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Intrinsic Meets Extrinsic Fairness_ Assessing the Downstream Impact of Bias Mitigation in Large Language Models_20250923|Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models]] (85.6% similar)
- [[2025-09-23/Auto-Search and Refinement_ An Automated Framework for Gender Bias Mitigation in Large Language Models_20250923|Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models]] (84.6% similar)
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (84.1% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (83.2% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Demographic Disparities|Demographic Disparities]], [[keywords/Social Contexts|Social Contexts]]
**âš¡ Unique Technical**: [[keywords/Fairness Benchmarks|Fairness Benchmarks]], [[keywords/Evaluation Framework|Evaluation Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.23411v2 Announce Type: replace-cross 
Abstract: Despite the growing reliance on fairness benchmarks to evaluate language models, the datasets that underpin these benchmarks remain critically underexamined. This survey addresses that overlooked foundation by offering a comprehensive analysis of the most widely used fairness datasets in language model research. To ground this analysis, we characterize each dataset across key dimensions, including provenance, demographic scope, annotation design, and intended use, revealing the assumptions and limitations baked into current evaluation practices. Building on this foundation, we propose a unified evaluation framework that surfaces consistent patterns of demographic disparities across benchmarks and scoring metrics. Applying this framework to sixteen popular datasets, we uncover overlooked biases that may distort conclusions about model fairness and offer guidance on selecting, combining, and interpreting these resources more effectively and responsibly. Our findings highlight an urgent need for new benchmarks that capture a broader range of social contexts and fairness notions. To support future research, we release all data, code, and results at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets, fostering transparency and reproducibility in the evaluation of language model fairness.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì˜ ê³µì •ì„±ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, í˜„ì¬ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê³µì •ì„± ë°ì´í„°ì…‹ì„ í¬ê´„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì˜ ì¶œì²˜, ì¸êµ¬ í†µê³„ ë²”ìœ„, ì£¼ì„ ì„¤ê³„, ì‚¬ìš© ëª©ì  ë“±ì„ í†µí•´ í‰ê°€ ê´€í–‰ì— ë‚´ì¬ëœ ê°€ì •ê³¼ í•œê³„ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ê´€ëœ ì¸êµ¬ í†µê³„ì  ë¶ˆê· í˜• íŒ¨í„´ì„ ë“œëŸ¬ë‚´ëŠ” í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ê³ , 16ê°œì˜ ì¸ê¸° ë°ì´í„°ì…‹ì— ì´ë¥¼ ì ìš©í•˜ì—¬ ëª¨ë¸ ê³µì •ì„±ì— ëŒ€í•œ ê²°ë¡ ì„ ì™œê³¡í•  ìˆ˜ ìˆëŠ” ê°„ê³¼ëœ í¸í–¥ì„ ë°œê²¬í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” ë‹¤ì–‘í•œ ì‚¬íšŒì  ë§¥ë½ê³¼ ê³µì •ì„± ê°œë…ì„ í¬ê´„í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ëª¨ë“  ë°ì´í„°, ì½”ë“œ, ê²°ê³¼ë¥¼ ê³µê°œí•˜ì—¬ ì—°êµ¬ì˜ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸ ì—°êµ¬ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê³µì •ì„± ë°ì´í„°ì…‹ì„ í¬ê´„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë°ì´í„°ì…‹ì˜ ì¶œì²˜, ì¸êµ¬ í†µê³„ ë²”ìœ„, ì£¼ì„ ì„¤ê³„, ì˜ë„ëœ ì‚¬ìš© ë“± ì£¼ìš” ì°¨ì›ì„ í†µí•´ ê° ë°ì´í„°ì…‹ì„ íŠ¹ì„±í™”í–ˆìŠµë‹ˆë‹¤.
- 2. ì œì•ˆëœ í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë²¤ì¹˜ë§ˆí¬ì™€ ì ìˆ˜ ë©”íŠ¸ë¦­ ì „ë°˜ì— ê±¸ì³ ì¼ê´€ëœ ì¸êµ¬ í†µê³„ì  ê²©ì°¨ íŒ¨í„´ì„ ë“œëŸ¬ë‚´ê³ , 16ê°œì˜ ì¸ê¸° ìˆëŠ” ë°ì´í„°ì…‹ì— ì ìš©í•˜ì—¬ ëª¨ë¸ ê³µì •ì„±ì— ëŒ€í•œ ê²°ë¡ ì„ ì™œê³¡í•  ìˆ˜ ìˆëŠ” ê°„ê³¼ëœ í¸í–¥ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼ëŠ” ë” ë„“ì€ ì‚¬íšŒì  ë§¥ë½ê³¼ ê³µì •ì„± ê°œë…ì„ í¬ì°©í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ë°ì´í„°ì…‹ ì„ íƒ, ê²°í•© ë° í•´ì„ì— ëŒ€í•œ íš¨ê³¼ì ì´ê³  ì±…ì„ ìˆëŠ” ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 4. í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ëª¨ë“  ë°ì´í„°, ì½”ë“œ ë° ê²°ê³¼ë¥¼ ê³µê°œí•˜ì—¬ ì–¸ì–´ ëª¨ë¸ ê³µì •ì„± í‰ê°€ì˜ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ì´‰ì§„í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:06:43*