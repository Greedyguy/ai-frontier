---
keywords:
  - Fairness Benchmarks
  - Large Language Model
  - Demographic Disparities
  - Evaluation Framework
  - Social Contexts
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.23411
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:06:43.960051",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Fairness Benchmarks",
    "Large Language Model",
    "Demographic Disparities",
    "Evaluation Framework",
    "Social Contexts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Fairness Benchmarks": 0.85,
    "Large Language Model": 0.8,
    "Demographic Disparities": 0.77,
    "Evaluation Framework": 0.79,
    "Social Contexts": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "fairness benchmarks",
        "canonical": "Fairness Benchmarks",
        "aliases": [
          "equity benchmarks",
          "bias benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on evaluating language models for fairness, providing a unique perspective.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs",
          "language processing models"
        ],
        "category": "broad_technical",
        "rationale": "Key technology discussed in the paper, linking to broader technical discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "demographic disparities",
        "canonical": "Demographic Disparities",
        "aliases": [
          "demographic biases",
          "population disparities"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights a specific issue in fairness evaluation, crucial for understanding biases in datasets.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "evaluation framework",
        "canonical": "Evaluation Framework",
        "aliases": [
          "assessment framework",
          "evaluation model"
        ],
        "category": "unique_technical",
        "rationale": "Proposed framework in the paper that provides a structured approach to analyzing datasets.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "social contexts",
        "canonical": "Social Contexts",
        "aliases": [
          "societal contexts",
          "cultural contexts"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the broader implications of fairness in language models.",
        "novelty_score": 0.58,
        "connectivity_score": 0.7,
        "specificity_score": 0.73,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "datasets",
      "results",
      "code"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "fairness benchmarks",
      "resolved_canonical": "Fairness Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "demographic disparities",
      "resolved_canonical": "Demographic Disparities",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "evaluation framework",
      "resolved_canonical": "Evaluation Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "social contexts",
      "resolved_canonical": "Social Contexts",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.7,
        "specificity": 0.73,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Datasets for Fairness in Language Models: An In-Depth Survey

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.23411.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.23411](https://arxiv.org/abs/2506.23411)

## 🔗 유사한 논문
- [[2025-09-23/Intrinsic Meets Extrinsic Fairness_ Assessing the Downstream Impact of Bias Mitigation in Large Language Models_20250923|Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models]] (85.6% similar)
- [[2025-09-23/Auto-Search and Refinement_ An Automated Framework for Gender Bias Mitigation in Large Language Models_20250923|Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models]] (84.6% similar)
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (84.1% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (83.2% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (82.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Demographic Disparities|Demographic Disparities]], [[keywords/Social Contexts|Social Contexts]]
**⚡ Unique Technical**: [[keywords/Fairness Benchmarks|Fairness Benchmarks]], [[keywords/Evaluation Framework|Evaluation Framework]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.23411v2 Announce Type: replace-cross 
Abstract: Despite the growing reliance on fairness benchmarks to evaluate language models, the datasets that underpin these benchmarks remain critically underexamined. This survey addresses that overlooked foundation by offering a comprehensive analysis of the most widely used fairness datasets in language model research. To ground this analysis, we characterize each dataset across key dimensions, including provenance, demographic scope, annotation design, and intended use, revealing the assumptions and limitations baked into current evaluation practices. Building on this foundation, we propose a unified evaluation framework that surfaces consistent patterns of demographic disparities across benchmarks and scoring metrics. Applying this framework to sixteen popular datasets, we uncover overlooked biases that may distort conclusions about model fairness and offer guidance on selecting, combining, and interpreting these resources more effectively and responsibly. Our findings highlight an urgent need for new benchmarks that capture a broader range of social contexts and fairness notions. To support future research, we release all data, code, and results at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets, fostering transparency and reproducibility in the evaluation of language model fairness.

## 📝 요약

이 논문은 언어 모델의 공정성을 평가하는 데 사용되는 데이터셋의 중요성을 강조하며, 현재 널리 사용되는 공정성 데이터셋을 포괄적으로 분석합니다. 각 데이터셋의 출처, 인구 통계 범위, 주석 설계, 사용 목적 등을 통해 평가 관행에 내재된 가정과 한계를 드러냅니다. 이를 바탕으로 일관된 인구 통계적 불균형 패턴을 드러내는 통합 평가 프레임워크를 제안하고, 16개의 인기 데이터셋에 이를 적용하여 모델 공정성에 대한 결론을 왜곡할 수 있는 간과된 편향을 발견합니다. 연구 결과는 다양한 사회적 맥락과 공정성 개념을 포괄하는 새로운 벤치마크의 필요성을 강조하며, 모든 데이터, 코드, 결과를 공개하여 연구의 투명성과 재현성을 지원합니다.

## 🎯 주요 포인트

- 1. 언어 모델 연구에서 가장 널리 사용되는 공정성 데이터셋을 포괄적으로 분석하여 데이터셋의 출처, 인구 통계 범위, 주석 설계, 의도된 사용 등 주요 차원을 통해 각 데이터셋을 특성화했습니다.
- 2. 제안된 통합 평가 프레임워크를 통해 벤치마크와 점수 메트릭 전반에 걸쳐 일관된 인구 통계적 격차 패턴을 드러내고, 16개의 인기 있는 데이터셋에 적용하여 모델 공정성에 대한 결론을 왜곡할 수 있는 간과된 편향을 발견했습니다.
- 3. 연구 결과는 더 넓은 사회적 맥락과 공정성 개념을 포착하는 새로운 벤치마크의 필요성을 강조하며, 데이터셋 선택, 결합 및 해석에 대한 효과적이고 책임 있는 가이드를 제공합니다.
- 4. 향후 연구를 지원하기 위해 모든 데이터, 코드 및 결과를 공개하여 언어 모델 공정성 평가의 투명성과 재현성을 촉진합니다.


---

*Generated on 2025-09-24 03:06:43*