---
keywords:
  - Transformer
  - Ontology Alignment
  - Unified Medical Language System
  - Dynamic Quantization
  - Intel Neural Compressor
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2507.13742
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:14:07.299180",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Ontology Alignment",
    "Unified Medical Language System",
    "Dynamic Quantization",
    "Intel Neural Compressor"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Ontology Alignment": 0.78,
    "Unified Medical Language System": 0.8,
    "Dynamic Quantization": 0.75,
    "Intel Neural Compressor": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based models",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to modern AI and connect well with other machine learning concepts.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Ontology alignment",
        "canonical": "Ontology Alignment",
        "aliases": [
          "Ontology matching"
        ],
        "category": "unique_technical",
        "rationale": "Ontology alignment is a specialized task in AI, linking semantic structures across domains.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Unified Medical Language System",
        "canonical": "Unified Medical Language System",
        "aliases": [
          "UMLS"
        ],
        "category": "specific_connectable",
        "rationale": "UMLS is a critical resource in biomedical informatics, facilitating connections in medical ontology research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dynamic quantization",
        "canonical": "Dynamic Quantization",
        "aliases": [
          "Quantization"
        ],
        "category": "unique_technical",
        "rationale": "Dynamic quantization is a key optimization technique for deploying models efficiently.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Intel Neural Compressor",
        "canonical": "Intel Neural Compressor",
        "aliases": [
          "INC"
        ],
        "category": "unique_technical",
        "rationale": "This tool is specific to model optimization, relevant for linking technical resources in AI.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "edge devices",
      "resource-constrained environments",
      "energy consumption"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Ontology alignment",
      "resolved_canonical": "Ontology Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Unified Medical Language System",
      "resolved_canonical": "Unified Medical Language System",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dynamic quantization",
      "resolved_canonical": "Dynamic Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Intel Neural Compressor",
      "resolved_canonical": "Intel Neural Compressor",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Search-Optimized Quantization in Biomedical Ontology Alignment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.13742.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2507.13742](https://arxiv.org/abs/2507.13742)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (84.2% similar)
- [[2025-09-23/Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state_20250923|Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state]] (82.7% similar)
- [[2025-09-23/Interpretability-Aware Pruning for Efficient Medical Image Analysis_20250923|Interpretability-Aware Pruning for Efficient Medical Image Analysis]] (82.7% similar)
- [[2025-09-23/Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers_20250923|Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers]] (82.5% similar)
- [[2025-09-22/MEC-Quant_ Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training_20250922|MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Unified Medical Language System|Unified Medical Language System]]
**âš¡ Unique Technical**: [[keywords/Ontology Alignment|Ontology Alignment]], [[keywords/Dynamic Quantization|Dynamic Quantization]], [[keywords/Intel Neural Compressor|Intel Neural Compressor]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.13742v2 Announce Type: replace-cross 
Abstract: In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” AI ëª¨ë¸ì˜ í¬ê¸°ì™€ ê³„ì‚° ìš”êµ¬ë¡œ ì¸í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íš¨ìœ¨ì ì¸ ëª¨ë¸ ìµœì í™” ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ìƒì˜í•™ ìš©ì–´ì™€ UMLS ë©”íƒ€ì‹œì†ŒëŸ¬ìŠ¤ë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•´ ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ì„±ì„ í™œìš©í•©ë‹ˆë‹¤. Microsoft Oliveì™€ ONNX Runtimeì„ í†µí•´ ë‹¤ì–‘í•œ ì‹¤í–‰ ì œê³µì ê°„ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ê³ , Intel Neural Compressorì™€ IPEXë¥¼ ì‚¬ìš©í•œ ë™ì  ì–‘ìí™”ë¥¼ í†µí•´ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. DEFT 2020 í‰ê°€ ìº í˜ì¸ì˜ ë‘ ê³¼ì œì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, ì„±ëŠ¥ ì§€í‘œë¥¼ ìœ ì§€í•˜ë©´ì„œ í‰ê·  ì¶”ë¡  ì†ë„ë¥¼ 20ë°° í–¥ìƒì‹œí‚¤ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì•½ 70% ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AI ëª¨ë¸ì˜ í¬ê¸°ì™€ ê³„ì‚° ìš”êµ¬ë¡œ ì¸í•´ ì—£ì§€ ë””ë°”ì´ìŠ¤ë‚˜ ìì› ì œí•œ í™˜ê²½ì—ì„œì˜ ë°°í¬ê°€ ì—ë„ˆì§€ ì†Œë¹„, ë©”ëª¨ë¦¬ ì‚¬ìš© ë° ì§€ì—°ê³¼ ê´€ë ¨ëœ ì¶”ê°€ì ì¸ ë„ì „ì„ ì´ˆë˜í•©ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì„ í™œìš©í•˜ì—¬, ìƒë¬¼ì˜í•™ ì¼ë°˜ ìš©ì–´ì™€ UMLS ë©”íƒ€ì‹œì†ŒëŸ¬ìŠ¤ ê°„ì˜ ì½”ì‚¬ì¸ ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ì„±ì— ê¸°ì´ˆí•œ ì²´ê³„ì ì¸ ì˜¨í†¨ë¡œì§€ ì •ë ¬ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 3. Microsoft Oliveì™€ ONNX Runtime ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‹¤í–‰ ì œê³µì(EP) ê°„ì˜ ìµœì í™”ë¥¼ íƒìƒ‰í•˜ê³ , Intel Neural Compressorì™€ IPEXë¥¼ í™œìš©í•œ ë™ì  ì–‘ìí™”ë¥¼ í†µí•´ ìµœì í™” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 4. DEFT 2020 í‰ê°€ ìº í˜ì¸ì˜ ë‘ ê°€ì§€ ê³¼ì œì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, í‰ê·  ì¶”ë¡  ì†ë„ë¥¼ 20ë°° í–¥ìƒì‹œí‚¤ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì•½ 70% ì¤„ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:14:07*