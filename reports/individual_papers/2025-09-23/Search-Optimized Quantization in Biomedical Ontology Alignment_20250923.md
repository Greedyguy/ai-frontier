---
keywords:
  - Transformer
  - Ontology Alignment
  - Unified Medical Language System
  - Dynamic Quantization
  - Intel Neural Compressor
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2507.13742
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:14:07.299180",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Ontology Alignment",
    "Unified Medical Language System",
    "Dynamic Quantization",
    "Intel Neural Compressor"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Ontology Alignment": 0.78,
    "Unified Medical Language System": 0.8,
    "Dynamic Quantization": 0.75,
    "Intel Neural Compressor": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based models",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to modern AI and connect well with other machine learning concepts.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Ontology alignment",
        "canonical": "Ontology Alignment",
        "aliases": [
          "Ontology matching"
        ],
        "category": "unique_technical",
        "rationale": "Ontology alignment is a specialized task in AI, linking semantic structures across domains.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Unified Medical Language System",
        "canonical": "Unified Medical Language System",
        "aliases": [
          "UMLS"
        ],
        "category": "specific_connectable",
        "rationale": "UMLS is a critical resource in biomedical informatics, facilitating connections in medical ontology research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dynamic quantization",
        "canonical": "Dynamic Quantization",
        "aliases": [
          "Quantization"
        ],
        "category": "unique_technical",
        "rationale": "Dynamic quantization is a key optimization technique for deploying models efficiently.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Intel Neural Compressor",
        "canonical": "Intel Neural Compressor",
        "aliases": [
          "INC"
        ],
        "category": "unique_technical",
        "rationale": "This tool is specific to model optimization, relevant for linking technical resources in AI.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "edge devices",
      "resource-constrained environments",
      "energy consumption"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Ontology alignment",
      "resolved_canonical": "Ontology Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Unified Medical Language System",
      "resolved_canonical": "Unified Medical Language System",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dynamic quantization",
      "resolved_canonical": "Dynamic Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Intel Neural Compressor",
      "resolved_canonical": "Intel Neural Compressor",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Search-Optimized Quantization in Biomedical Ontology Alignment

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.13742.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2507.13742](https://arxiv.org/abs/2507.13742)

## 🔗 유사한 논문
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (84.2% similar)
- [[2025-09-23/Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state_20250923|Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state]] (82.7% similar)
- [[2025-09-23/Interpretability-Aware Pruning for Efficient Medical Image Analysis_20250923|Interpretability-Aware Pruning for Efficient Medical Image Analysis]] (82.7% similar)
- [[2025-09-23/Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers_20250923|Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers]] (82.5% similar)
- [[2025-09-22/MEC-Quant_ Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training_20250922|MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training]] (82.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Unified Medical Language System|Unified Medical Language System]]
**⚡ Unique Technical**: [[keywords/Ontology Alignment|Ontology Alignment]], [[keywords/Dynamic Quantization|Dynamic Quantization]], [[keywords/Intel Neural Compressor|Intel Neural Compressor]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.13742v2 Announce Type: replace-cross 
Abstract: In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.

## 📝 요약

이 연구는 AI 모델의 크기와 계산 요구로 인한 문제를 해결하기 위해 효율적인 모델 최적화 기법을 제안합니다. 특히, 생의학 용어와 UMLS 메타시소러스를 연결하기 위해 최신 트랜스포머 기반 모델을 사용하여 코사인 기반 의미 유사성을 활용합니다. Microsoft Olive와 ONNX Runtime을 통해 다양한 실행 제공자 간 최적화를 수행하고, Intel Neural Compressor와 IPEX를 사용한 동적 양자화를 통해 최적화를 진행합니다. DEFT 2020 평가 캠페인의 두 과제에서 새로운 최고 성능을 달성했으며, 성능 지표를 유지하면서 평균 추론 속도를 20배 향상시키고 메모리 사용량을 약 70% 줄였습니다.

## 🎯 주요 포인트

- 1. AI 모델의 크기와 계산 요구로 인해 엣지 디바이스나 자원 제한 환경에서의 배포가 에너지 소비, 메모리 사용 및 지연과 관련된 추가적인 도전을 초래합니다.
- 2. 본 연구는 최신 트랜스포머 기반 모델을 활용하여, 생물의학 일반 용어와 UMLS 메타시소러스 간의 코사인 기반 의미 유사성에 기초한 체계적인 온톨로지 정렬 방법을 소개합니다.
- 3. Microsoft Olive와 ONNX Runtime 백엔드를 사용하여 다양한 실행 제공자(EP) 간의 최적화를 탐색하고, Intel Neural Compressor와 IPEX를 활용한 동적 양자화를 통해 최적화 과정을 수행합니다.
- 4. DEFT 2020 평가 캠페인의 두 가지 과제에서 새로운 최첨단 성과를 달성하며, 평균 추론 속도를 20배 향상시키고 메모리 사용을 약 70% 줄였습니다.


---

*Generated on 2025-09-24 01:14:07*