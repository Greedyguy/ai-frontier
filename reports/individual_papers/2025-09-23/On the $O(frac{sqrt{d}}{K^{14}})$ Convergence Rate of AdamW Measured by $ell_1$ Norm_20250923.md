---
keywords:
  - AdamW Optimizer
  - Convergence Rate
  - Large Language Model
  - Stochastic Gradient Descent
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2505.11840
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:43:34.441659",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AdamW Optimizer",
    "Convergence Rate",
    "Large Language Model",
    "Stochastic Gradient Descent"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AdamW Optimizer": 0.78,
    "Convergence Rate": 0.7,
    "Large Language Model": 0.72,
    "Stochastic Gradient Descent": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AdamW",
        "canonical": "AdamW Optimizer",
        "aliases": [
          "AdamW"
        ],
        "category": "unique_technical",
        "rationale": "AdamW is a specific optimization algorithm crucial for linking discussions on convergence rates in deep learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "convergence rate",
        "canonical": "Convergence Rate",
        "aliases": [
          "rate of convergence"
        ],
        "category": "broad_technical",
        "rationale": "Convergence rate is a fundamental concept in optimization and machine learning, linking to performance analysis.",
        "novelty_score": 0.45,
        "connectivity_score": 0.8,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are central to modern AI research, facilitating links to various applications and studies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "SGD",
        "canonical": "Stochastic Gradient Descent",
        "aliases": [
          "SGD"
        ],
        "category": "specific_connectable",
        "rationale": "SGD is a widely used optimization method, providing a basis for comparing convergence rates with AdamW.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "iteration number",
      "model dimension",
      "Gaussian distribution"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AdamW",
      "resolved_canonical": "AdamW Optimizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "convergence rate",
      "resolved_canonical": "Convergence Rate",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.8,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "SGD",
      "resolved_canonical": "Stochastic Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.11840.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2505.11840](https://arxiv.org/abs/2505.11840)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization_20250922|Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization]] (81.2% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (80.0% similar)
- [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (79.9% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (79.8% similar)
- [[2025-09-17/On the Rate of Gaussian Approximation for Linear Regression Problems_20250917|On the Rate of Gaussian Approximation for Linear Regression Problems]] (79.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Convergence Rate|Convergence Rate]], [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Stochastic Gradient Descent|Stochastic Gradient Descent]]
**âš¡ Unique Technical**: [[keywords/AdamW Optimizer|AdamW Optimizer]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.11840v2 Announce Type: replace 
Abstract: As the default optimizer for training large language models, AdamW has achieved remarkable success in deep learning. However, its convergence behavior is not theoretically well-understood. This paper establishes the convergence rate $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_1\right]\leq O(\frac{\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\ell_1$ norm, where $K$ represents the iteration number, $d$ denotes the model dimension, and $C$ matches the constant in the optimal convergence rate of SGD. Theoretically, we have $||\nabla f(x)||_2\ll ||\nabla f(x)||_1\leq \sqrt{d}||\nabla f(x)||_2$ for any high-dimensional vector $x$ and $E\left[||\nabla f(x)||_1\right]\geq\sqrt{\frac{2d}{\pi}}E\left[||\nabla f(x)||_2\right]$ when each element of $\nabla f(x)$ is generated from Gaussian distribution $\mathcal N(0,1)$. Empirically, our experimental results on real-world deep learning tasks reveal $||\nabla f(x)||_1=\varTheta(\sqrt{d})||\nabla f(x)||_2$. Both support that our convergence rate can be considered to be analogous to the optimal $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_2\right]\leq O(\frac{C}{K^{1/4}})$ convergence rate of SGD.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ í•™ìŠµì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” AdamW ì˜µí‹°ë§ˆì´ì €ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. AdamWì˜ ìˆ˜ë ´ ì†ë„ê°€ $\ell_1$ ë…¸ë¦„ìœ¼ë¡œ ì¸¡ì •ë  ë•Œ $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_1\right]\leq O(\frac{\sqrt{d}C}{K^{1/4}})$ì„ì„ ì¦ëª…í•˜ì˜€ìœ¼ë©°, ì´ëŠ” SGDì˜ ìµœì  ìˆ˜ë ´ ì†ë„ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ $\ell_2$ ë…¸ë¦„ë³´ë‹¤ $\ell_1$ ë…¸ë¦„ì´ ë” í¬ë©°, ì‹¤í—˜ì ìœ¼ë¡œë„ ì‹¤ì œ ë”¥ëŸ¬ë‹ ì‘ì—…ì—ì„œ ì´ ê²°ê³¼ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì€ AdamWì˜ ìˆ˜ë ´ ì†ë„ê°€ SGDì˜ ìµœì  ìˆ˜ë ´ ì†ë„ì™€ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AdamWëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ í•™ìŠµì˜ ê¸°ë³¸ ìµœì í™” ë„êµ¬ë¡œì„œ ê¹Šì´ ìˆëŠ” í•™ìŠµì—ì„œ í° ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ê·¸ ìˆ˜ë ´ í–‰ë™ì€ ì´ë¡ ì ìœ¼ë¡œ ì˜ ì´í•´ë˜ì§€ ì•Šì•˜ë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì€ AdamWì˜ ìˆ˜ë ´ ì†ë„ë¥¼ $\ell_1$ ë…¸ë¦„ìœ¼ë¡œ ì¸¡ì •í•˜ì—¬ $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_1\right]\leq O(\frac{\sqrt{d}C}{K^{1/4}})$ë¡œ ì„¤ì •í•˜ì˜€ë‹¤.
- 3. ì´ë¡ ì ìœ¼ë¡œ ê³ ì°¨ì› ë²¡í„° $x$ì— ëŒ€í•´ $||\nabla f(x)||_2\ll ||\nabla f(x)||_1\leq \sqrt{d}||\nabla f(x)||_2$ì´ë©°, $\nabla f(x)$ì˜ ê° ìš”ì†Œê°€ Gaussian ë¶„í¬ $\mathcal N(0,1)$ì—ì„œ ìƒì„±ë  ë•Œ $E\left[||\nabla f(x)||_1\right]\geq\sqrt{\frac{2d}{\pi}}E\left[||\nabla f(x)||_2\right]$ì„ì„ ë³´ì˜€ë‹¤.
- 4. ì‹¤í—˜ì ìœ¼ë¡œ, ì‹¤ì œ ì‹¬ì¸µ í•™ìŠµ ì‘ì—…ì—ì„œ $||\nabla f(x)||_1=\varTheta(\sqrt{d})||\nabla f(x)||_2$ì„ì„ í™•ì¸í•˜ì˜€ë‹¤.
- 5. ì´ëŸ¬í•œ ê²°ê³¼ë“¤ì€ AdamWì˜ ìˆ˜ë ´ ì†ë„ê°€ SGDì˜ ìµœì  ìˆ˜ë ´ ì†ë„ì™€ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ë’·ë°›ì¹¨í•œë‹¤.


---

*Generated on 2025-09-24 02:43:34*