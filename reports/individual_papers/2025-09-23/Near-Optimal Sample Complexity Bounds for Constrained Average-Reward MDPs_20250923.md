---
keywords:
  - Constrained Average-Reward MDP
  - Sample Complexity
  - Generative Model
  - Feasibility Settings
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16586
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:40:44.704749",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Constrained Average-Reward MDP",
    "Sample Complexity",
    "Generative Model",
    "Feasibility Settings"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Constrained Average-Reward MDP": 0.9,
    "Sample Complexity": 0.7,
    "Generative Model": 0.8,
    "Feasibility Settings": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "constrained average-reward MDP",
        "canonical": "Constrained Average-Reward MDP",
        "aliases": [
          "CAMDP"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's focus and represents a specific type of Markov decision process with constraints, which is crucial for linking related works.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "sample complexity",
        "canonical": "Sample Complexity",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Sample complexity is a fundamental concept in learning theory, relevant for connecting with other works on learning efficiency.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "generative model",
        "canonical": "Generative Model",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Generative models are widely used in machine learning, providing a strong link to related methodologies and applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "feasibility settings",
        "canonical": "Feasibility Settings",
        "aliases": [
          "relaxed feasibility",
          "strict feasibility"
        ],
        "category": "unique_technical",
        "rationale": "These settings are specific to the paper's approach and are important for understanding the constraints applied in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "constrained average-reward MDP",
      "resolved_canonical": "Constrained Average-Reward MDP",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "sample complexity",
      "resolved_canonical": "Sample Complexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "generative model",
      "resolved_canonical": "Generative Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "feasibility settings",
      "resolved_canonical": "Feasibility Settings",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    }
  ]
}
-->

# Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16586.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16586](https://arxiv.org/abs/2509.16586)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain_20250918|Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain]] (84.9% similar)
- [[2025-09-19/Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks_20250919|Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks]] (82.8% similar)
- [[2025-09-22/Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses_20250922|Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses]] (82.1% similar)
- [[2025-09-22/Online Robust Planning under Model Uncertainty_ A Sample-Based Approach_20250922|Online Robust Planning under Model Uncertainty: A Sample-Based Approach]] (81.8% similar)
- [[2025-09-19/Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits_20250919|Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Sample Complexity|Sample Complexity]], [[keywords/Generative Model|Generative Model]]
**âš¡ Unique Technical**: [[keywords/Constrained Average-Reward MDP|Constrained Average-Reward MDP]], [[keywords/Feasibility Settings|Feasibility Settings]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16586v1 Announce Type: new 
Abstract: Recent advances have significantly improved our understanding of the sample complexity of learning in average-reward Markov decision processes (AMDPs) under the generative model. However, much less is known about the constrained average-reward MDP (CAMDP), where policies must satisfy long-run average constraints. In this work, we address this gap by studying the sample complexity of learning an $\epsilon$-optimal policy in CAMDPs under a generative model. We propose a model-based algorithm that operates under two settings: (i) relaxed feasibility, which allows small constraint violations, and (ii) strict feasibility, where the output policy satisfies the constraint. We show that our algorithm achieves sample complexities of $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$ and $\tilde{O} \left(\frac{S A (B+H)}{\epsilon^2 \zeta^2} \right)$ under the relaxed and strict feasibility settings, respectively. Here, $\zeta$ is the Slater constant indicating the size of the feasible region, $H$ is the span bound of the bias function, and $B$ is the transient time bound. Moreover, a matching lower bound of $\tilde{\Omega}\left(\frac{S A (B+H)}{ \epsilon^2\zeta^2}\right)$ for the strict feasibility case is established, thus providing the first minimax-optimal bounds for CAMDPs. Our results close the theoretical gap in understanding the complexity of constrained average-reward MDPs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìƒì„± ëª¨ë¸ í•˜ì—ì„œ ì œì•½ëœ í‰ê·  ë³´ìƒ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(CAMDP)ì˜ ìƒ˜í”Œ ë³µì¡ì„±ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì €ìë“¤ì€ $\epsilon$-ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ëª¨ë¸ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, (i) ì‘ì€ ì œì•½ ìœ„ë°˜ì„ í—ˆìš©í•˜ëŠ” ì™„í™”ëœ íƒ€ë‹¹ì„± ì„¤ì •ê³¼ (ii) ì œì•½ì„ ë§Œì¡±í•˜ëŠ” ì—„ê²©í•œ íƒ€ë‹¹ì„± ì„¤ì • ë‘ ê°€ì§€ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ê°ê° $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$ì™€ $\tilde{O} \left(\frac{S A (B+H)}{\epsilon^2 \zeta^2} \right)$ì˜ ìƒ˜í”Œ ë³µì¡ì„±ì„ ë‹¬ì„±í•˜ë©°, ì—¬ê¸°ì„œ $\zeta$ëŠ” ìŠ¬ë ˆì´í„° ìƒìˆ˜, $H$ëŠ” í¸í–¥ í•¨ìˆ˜ì˜ ë²”ìœ„, $B$ëŠ” ê³¼ë„ ì‹œê°„ í•œê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë˜í•œ, ì—„ê²©í•œ íƒ€ë‹¹ì„± ê²½ìš°ì— ëŒ€í•œ í•˜í•œì„ ì œì‹œí•˜ì—¬ CAMDPì˜ ë³µì¡ì„±ì— ëŒ€í•œ ì´ë¡ ì  ê²©ì°¨ë¥¼ í•´ì†Œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í‰ê·  ë³´ìƒ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(AMDP)ì˜ í‘œë³¸ ë³µì¡ì„±ì— ëŒ€í•œ ì´í•´ê°€ ìµœê·¼ í¬ê²Œ í–¥ìƒë˜ì—ˆë‹¤.
- 2. ì œí•œëœ í‰ê·  ë³´ìƒ MDP(CAMDP)ì˜ í•™ìŠµ í‘œë³¸ ë³µì¡ì„±ì„ ì—°êµ¬í•˜ì—¬ ì´ë¡ ì  ê²©ì°¨ë¥¼ í•´ì†Œí•˜ì˜€ë‹¤.
- 3. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ì™„í™”ëœ íƒ€ë‹¹ì„±ê³¼ ì—„ê²©í•œ íƒ€ë‹¹ì„± ë‘ ê°€ì§€ ì„¤ì •ì—ì„œ ì‘ë™í•˜ë©°, ê°ê°ì˜ í‘œë³¸ ë³µì¡ì„±ì„ ë‹¬ì„±í•œë‹¤.
- 4. ì—„ê²©í•œ íƒ€ë‹¹ì„± ì„¤ì •ì—ì„œì˜ í•˜í•œì„ ì œê³µí•˜ì—¬ CAMDPì— ëŒ€í•œ ì²« ë²ˆì§¸ ë¯¸ë‹ˆë§¥ìŠ¤ ìµœì  ê²½ê³„ë¥¼ ì œì‹œí•˜ì˜€ë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì œí•œëœ í‰ê·  ë³´ìƒ MDPì˜ ë³µì¡ì„±ì— ëŒ€í•œ ì´ë¡ ì  ì´í•´ë¥¼ ì™„ì„±í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 01:40:44*