---
keywords:
  - Personalized Question Answering
  - Large Language Model
  - LaMP-QA Benchmark
  - Personalized Context
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.00137
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:04:39.240939",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Personalized Question Answering",
    "Large Language Model",
    "LaMP-QA Benchmark",
    "Personalized Context"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Personalized Question Answering": 0.78,
    "Large Language Model": 0.82,
    "LaMP-QA Benchmark": 0.85,
    "Personalized Context": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "personalized question answering",
        "canonical": "Personalized Question Answering",
        "aliases": [
          "personalized QA"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's contribution and represents a niche area in question answering systems.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are a foundational technology for the paper's methodology and evaluation.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "LaMP-QA benchmark",
        "canonical": "LaMP-QA Benchmark",
        "aliases": [
          "LaMP-QA"
        ],
        "category": "unique_technical",
        "rationale": "The benchmark is a novel contribution of the paper, essential for evaluating personalized long-form answer generation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "personalized context",
        "canonical": "Personalized Context",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Personalized context is a key factor in the performance improvement reported in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "answer generation",
      "evaluation strategies",
      "performance improvements",
      "human preferences"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "personalized question answering",
      "resolved_canonical": "Personalized Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "LaMP-QA benchmark",
      "resolved_canonical": "LaMP-QA Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "personalized context",
      "resolved_canonical": "Personalized Context",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# LaMP-QA: A Benchmark for Personalized Long-form Question Answering

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.00137.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.00137](https://arxiv.org/abs/2506.00137)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (84.2% similar)
- [[2025-09-19/SWE-QA_ Can Language Models Answer Repository-level Code Questions?_20250919|SWE-QA: Can Language Models Answer Repository-level Code Questions?]] (82.6% similar)
- [[2025-09-23/Beyond Prompting_ An Efficient Embedding Framework for Open-Domain Question Answering_20250923|Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering]] (81.4% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (81.3% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Personalized Question Answering|Personalized Question Answering]], [[keywords/LaMP-QA Benchmark|LaMP-QA Benchmark]], [[keywords/Personalized Context|Personalized Context]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.00137v2 Announce Type: replace-cross 
Abstract: Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models. Our results show that incorporating the personalized context provided leads to up to 39% performance improvements. The benchmark is publicly released to support future research in this area.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ìš©ì ì¤‘ì‹¬ì˜ ì§ˆë¬¸ ì‘ë‹µ ì‹œìŠ¤í…œì—ì„œ ê°œì¸í™”ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ê°œì¸í™”ëœ ë‹µë³€ ìƒì„± ì—°êµ¬ì˜ ë¶€ì¡±ì„ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LaMP-QAë¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•˜ë©°, ì´ëŠ” ì˜ˆìˆ  ë° ì—”í„°í…Œì¸ë¨¼íŠ¸, ë¼ì´í”„ìŠ¤íƒ€ì¼ ë° ê°œì¸ ê°œë°œ, ì‚¬íšŒ ë° ë¬¸í™”ì˜ ì„¸ ê°€ì§€ ì£¼ìš” ì¹´í…Œê³ ë¦¬ì™€ 45ê°œ ì´ìƒì˜ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì¸ê°„ê³¼ ìë™ í‰ê°€ë¥¼ í†µí•´ ê°œì¸í™”ëœ ë‹µë³€ì˜ ì§ˆê³¼ ì˜í–¥ë ¥ì„ ë¹„êµí•˜ê³ , ì¸ê°„ ì„ í˜¸ë„ì™€ì˜ ì¼ì¹˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ë˜í•œ, ì˜¤í”ˆ ì†ŒìŠ¤ ë° ë…ì  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì„ ë²¤ì¹˜ë§ˆí‚¹í•˜ì—¬ ê°œì¸í™”ëœ ë¬¸ë§¥ì„ í†µí•©í–ˆì„ ë•Œ ìµœëŒ€ 39%ì˜ ì„±ëŠ¥ í–¥ìƒì´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LaMP-QAëŠ” ê°œì¸í™”ëœ ì¥ë¬¸ ë‹µë³€ ìƒì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ë¡œ, ì˜ˆìˆ  ë° ì—”í„°í…Œì¸ë¨¼íŠ¸, ë¼ì´í”„ìŠ¤íƒ€ì¼ ë° ê°œì¸ ê°œë°œ, ì‚¬íšŒ ë° ë¬¸í™”ì˜ ì„¸ ê°€ì§€ ì£¼ìš” ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
- 2. LaMP-QA ë²¤ì¹˜ë§ˆí¬ì˜ í’ˆì§ˆê³¼ ì ì¬ì  ì˜í–¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì¸ê°„ ë° ìë™ í‰ê°€ë¥¼ í†µí•´ ì—¬ëŸ¬ í‰ê°€ ì „ëµì„ ë¹„êµí•˜ê³  ì¸ê°„ ì„ í˜¸ë„ì™€ì˜ ì¼ì¹˜ë¥¼ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.
- 3. ê°œì¸í™”ëœ ë§¥ë½ì„ í¬í•¨í•˜ë©´ ì„±ëŠ¥ì´ ìµœëŒ€ 39% í–¥ìƒëœë‹¤ëŠ” ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.
- 4. LaMP-QA ë²¤ì¹˜ë§ˆí¬ëŠ” ì´ ë¶„ì•¼ì˜ í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:04:39*