---
keywords:
  - Large Language Model
  - Photonic Tensor Units
  - Hybrid Photonic-Electronic Systems
  - Stacked Graph
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16443
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:19:53.131252",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Photonic Tensor Units",
    "Hybrid Photonic-Electronic Systems",
    "Stacked Graph"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Photonic Tensor Units": 0.78,
    "Hybrid Photonic-Electronic Systems": 0.77,
    "Stacked Graph": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on inference optimization, providing a strong link to existing research in NLP and machine learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Photonic Tensor Units",
        "canonical": "Photonic Tensor Units",
        "aliases": [
          "PTUs"
        ],
        "category": "unique_technical",
        "rationale": "Photonic Tensor Units represent a novel hardware component discussed in the paper, crucial for linking to emerging photonic computing research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Hybrid Photonic-Electronic Systems",
        "canonical": "Hybrid Photonic-Electronic Systems",
        "aliases": [
          "Photonic-Electronic Systems"
        ],
        "category": "unique_technical",
        "rationale": "The hybrid systems are a unique focus of the paper, offering a new perspective on integrating different computational paradigms.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Stacked Graph",
        "canonical": "Stacked Graph",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The Stacked Graph is a novel intermediate representation introduced in the paper, essential for understanding the compilation strategy.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Photonic Tensor Units",
      "resolved_canonical": "Photonic Tensor Units",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Hybrid Photonic-Electronic Systems",
      "resolved_canonical": "Hybrid Photonic-Electronic Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Stacked Graph",
      "resolved_canonical": "Stacked Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# LightCode: Compiling LLM Inference for Photonic-Electronic Systems

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16443.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16443](https://arxiv.org/abs/2509.16443)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (84.6% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (83.4% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (82.1% similar)
- [[2025-09-22/MigGPT_ Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions_20250922|MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions]] (82.0% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Photonic Tensor Units|Photonic Tensor Units]], [[keywords/Hybrid Photonic-Electronic Systems|Hybrid Photonic-Electronic Systems]], [[keywords/Stacked Graph|Stacked Graph]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16443v1 Announce Type: cross 
Abstract: The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific accelerators like the Photonic Tensor Units (PTUs), which offer low-power, high-throughput linear computation. This motivates hybrid compilation strategies that combine photonic and electronic resources. We present LightCode, a compiler framework and simulator for mapping LLM inference workloads across hybrid photonic-electronic systems. LightCode introduces the Stacked Graph, an intermediate representation that encodes multiple hardware-specific realizations of each tensor operation. Hardware assignment is formulated as a constrained subgraph selection problem optimized for latency or energy under parametric cost models. We evaluate LightCode on the prefill stage of GPT-2 and Llama-7B showing that under our workload and hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our simulated workloads at maximum sequence length; (ii) multiplexing and assignment strategy yielded latency improvements exceeding 10x; and (iii) Optimizing for latency or energy resulted in distinct hardware mappings in our simulations. LightCode offers a module, foundational framework and simulator for compiling LLMs to emerging photonic accelerators.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ì‘ì—…ì—ì„œ ë‚®ì€ ì§€ì—° ì‹œê°„ê³¼ ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì´ì¢… ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì£¼ë¡œ GPUê°€ ì‚¬ìš©ë˜ì§€ë§Œ, ì €ì „ë ¥ ê³ ì²˜ë¦¬ëŸ‰ì„ ì œê³µí•˜ëŠ” í¬í† ë‹‰ í…ì„œ ìœ ë‹›(PTU)ê³¼ì˜ í†µí•©ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í¬í† ë‹‰ ë° ì „ì ìì›ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì»´íŒŒì¼ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. LightCodeëŠ” ì´ëŸ¬í•œ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì—ì„œ LLM ì¶”ë¡  ì‘ì—…ì„ ë§¤í•‘í•˜ëŠ” ì»´íŒŒì¼ëŸ¬ í”„ë ˆì„ì›Œí¬ ë° ì‹œë®¬ë ˆì´í„°ë¡œ, ê° í…ì„œ ì—°ì‚°ì˜ í•˜ë“œì›¨ì–´ë³„ êµ¬í˜„ì„ ì¸ì½”ë”©í•˜ëŠ” ì¤‘ê°„ í‘œí˜„ì¸ Stacked Graphë¥¼ ë„ì…í•©ë‹ˆë‹¤. í•˜ë“œì›¨ì–´ í• ë‹¹ì€ ì§€ì—° ì‹œê°„ì´ë‚˜ ì—ë„ˆì§€ë¥¼ ìµœì í™”í•˜ëŠ” ì œì•½ëœ ì„œë¸Œê·¸ë˜í”„ ì„ íƒ ë¬¸ì œë¡œ ê³µì‹í™”ë©ë‹ˆë‹¤. GPT-2ì™€ Llama-7Bì˜ í”„ë¦¬í•„ ë‹¨ê³„ í‰ê°€ ê²°ê³¼, í¬í† ë‹‰ í•˜ë“œì›¨ì–´ê°€ ì—ë„ˆì§€ë¥¼ ìµœëŒ€ 50% ì ˆê°í•˜ê³ , ë©€í‹°í”Œë ‰ì‹± ë° í• ë‹¹ ì „ëµì´ ì§€ì—° ì‹œê°„ì„ 10ë°° ì´ìƒ ê°œì„ í–ˆìœ¼ë©°, ì§€ì—° ì‹œê°„ ë˜ëŠ” ì—ë„ˆì§€ ìµœì í™”ì— ë”°ë¼ í•˜ë“œì›¨ì–´ ë§¤í•‘ì´ ë‹¬ë¼ì§ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. LightCodeëŠ” í¬í† ë‹‰ ê°€ì†ê¸°ì— LLMì„ ì»´íŒŒì¼í•˜ê¸° ìœ„í•œ ê¸°ë³¸ í”„ë ˆì„ì›Œí¬ì™€ ì‹œë®¬ë ˆì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LightCodeëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì‘ì—…ì„ í•˜ì´ë¸Œë¦¬ë“œ ê´‘ì „ì ì‹œìŠ¤í…œì— ë§¤í•‘í•˜ëŠ” ì»´íŒŒì¼ëŸ¬ í”„ë ˆì„ì›Œí¬ ë° ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤.
- 2. Stacked GraphëŠ” ê° í…ì„œ ì—°ì‚°ì˜ ì—¬ëŸ¬ í•˜ë“œì›¨ì–´ íŠ¹ì • ì‹¤í˜„ì„ ì¸ì½”ë”©í•˜ëŠ” ì¤‘ê°„ í‘œí˜„ì„ ì œê³µí•©ë‹ˆë‹¤.
- 3. LightCodeëŠ” GPT-2 ë° Llama-7Bì˜ prefill ë‹¨ê³„ì—ì„œ ê´‘ì í•˜ë“œì›¨ì–´ê°€ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ ì—ë„ˆì§€ë¥¼ ìµœëŒ€ 50%ê¹Œì§€ ì ˆê°í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. ë‹¤ì¤‘í™” ë° í• ë‹¹ ì „ëµì„ í†µí•´ ì§€ì—° ì‹œê°„ì´ 10ë°° ì´ìƒ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. ì§€ì—° ì‹œê°„ ë˜ëŠ” ì—ë„ˆì§€ ìµœì í™”ëŠ” ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ ë§¤í•‘ì„ ì´ˆë˜í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:19:53*