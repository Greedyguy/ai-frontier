---
keywords:
  - Retrieval Augmented Generation
  - Attention Mechanism
  - Context Compression
  - Large Language Model
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17486
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:29:00.997869",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval Augmented Generation",
    "Attention Mechanism",
    "Context Compression",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Retrieval Augmented Generation": 0.82,
    "Attention Mechanism": 0.8,
    "Context Compression": 0.78,
    "Large Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper's focus and links to recent trends in enhancing LLMs with external data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The attention mechanism is a key component in the proposed method, facilitating context relevance assessment.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Context Compression",
        "canonical": "Context Compression",
        "aliases": [
          "Adaptive Context Compression"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper to improve LLM efficiency by filtering irrelevant information.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are the foundational technology being enhanced by the proposed method, linking to a broad range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Context Compression",
      "resolved_canonical": "Context Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17486.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17486](https://arxiv.org/abs/2509.17486)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/CORE-RAG_ Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning_20250922|CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning]] (88.6% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility: Process-Supervised Rewrite for RAG]] (84.7% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (83.6% similar)
- [[2025-09-23/MPIC_ Position-Independent Multimodal Context Caching System for Efficient MLLM Serving_20250923|MPIC: Position-Independent Multimodal Context Caching System for Efficient MLLM Serving]] (83.4% similar)
- [[2025-09-22/ConCISE_ Confidence-guided Compression in Step-by-step Efficient Reasoning_20250922|ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Context Compression|Context Compression]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17486v1 Announce Type: new 
Abstract: Retrieval-augmented generation improves the factual accuracy of Large Language Models (LLMs) by incorporating external context, but often suffers from irrelevant retrieved content that hinders effectiveness. Context compression addresses this issue by filtering out irrelevant information from context before LLM generation. However, existing methods struggle to adaptively adjust compression rates for different context, maintain low latency and integrate information across multiple documents. To overcome these limitations, We introduce AttnComp, an adaptive, efficient and context-aware compression framework. By leveraging the attention mechanism of LLMs to identify relevant information, AttnComp employs a Top-P compression algorithm to retain the minimal set of documents whose cumulative attention weights exceeds a predefined threshold. In addition to compression, AttnComp estimates response confidence by assessing the overall relevance of the retrieved content, enabling users to gauge response reliability. Experiments demonstrate that AttnComp outperforms existing compression methods and uncompressed baselines, achieving higher accuracy with substantial compression rates and lower latency.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì™¸ë¶€ ë¬¸ë§¥ì„ í™œìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì‹¤ ì •í™•ì„±ì„ ê°œì„ í•˜ëŠ” ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ë°©ë²•ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë¬¸ë§¥ì˜ ì••ì¶• ë¹„ìœ¨ì„ ì ì ˆíˆ ì¡°ì •í•˜ê±°ë‚˜ ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ AttnCompë¼ëŠ” ì ì‘ì ì´ê³  íš¨ìœ¨ì ì¸ ë¬¸ë§¥ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” LLMì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì‹ë³„í•˜ê³ , Top-P ì••ì¶• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ëˆ„ì  ì£¼ì˜ ê°€ì¤‘ì¹˜ê°€ ì¼ì • ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ìµœì†Œ ë¬¸ì„œ ì§‘í•©ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, AttnCompëŠ” ê²€ìƒ‰ëœ ì½˜í…ì¸ ì˜ ì „ë°˜ì ì¸ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬ ì‘ë‹µì˜ ì‹ ë¢°ë„ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, AttnCompëŠ” ê¸°ì¡´ ì••ì¶• ë°©ë²•ê³¼ ë¹„ì••ì¶• ê¸°ì¤€ë³´ë‹¤ ë†’ì€ ì •í™•ì„±ê³¼ ë‚®ì€ ì§€ì—° ì‹œê°„ì„ ë‹¬ì„±í•˜ë©° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Retrieval-augmented generationì€ ì™¸ë¶€ ë¬¸ë§¥ì„ í†µí•©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì‹¤ ì •í™•ì„±ì„ ê°œì„ í•˜ì§€ë§Œ, ë¹„ê´€ë ¨ëœ ê²€ìƒ‰ ì½˜í…ì¸ ë¡œ ì¸í•´ íš¨ê³¼ê°€ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ë¬¸ë§¥ ì••ì¶•ì€ LLM ìƒì„± ì „ì— ë¹„ê´€ë ¨ ì •ë³´ë¥¼ í•„í„°ë§í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 3. AttnCompëŠ” ì ì‘ì ì´ê³  íš¨ìœ¨ì ì¸ ë¬¸ë§¥ ì¸ì‹ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¡œ, LLMì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì‹ë³„í•˜ê³  Top-P ì••ì¶• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ëˆ„ì  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ê°€ íŠ¹ì • ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ìµœì†Œí•œì˜ ë¬¸ì„œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- 4. AttnCompëŠ” ê²€ìƒ‰ëœ ì½˜í…ì¸ ì˜ ì „ë°˜ì ì¸ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬ ì‘ë‹µ ì‹ ë¢°ë„ë¥¼ ì¶”ì •í•˜ê³ , ì‚¬ìš©ìê°€ ì‘ë‹µì˜ ì‹ ë¢°ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, AttnCompëŠ” ê¸°ì¡´ ì••ì¶• ë°©ë²•ê³¼ ë¹„ì••ì¶• ê¸°ì¤€ì„ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë†’ì€ ì••ì¶•ë¥ ê³¼ ë‚®ì€ ì§€ì—° ì‹œê°„ìœ¼ë¡œ ë” ë†’ì€ ì •í™•ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:29:00*