---
keywords:
  - Large Language Model
  - Knowledge Graph
  - Commonsense Reasoning
  - CoLoTa Dataset
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.18063
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:36:58.531454",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Knowledge Graph",
    "Commonsense Reasoning",
    "CoLoTa Dataset"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Knowledge Graph": 0.82,
    "Commonsense Reasoning": 0.78,
    "CoLoTa Dataset": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Connects to a broad range of topics in AI and NLP, providing a foundation for linking related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Knowledge Graphs",
        "canonical": "Knowledge Graph",
        "aliases": [
          "KG",
          "Knowledge Graphs"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for linking research on structured knowledge integration and reasoning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Commonsense Reasoning",
        "canonical": "Commonsense Reasoning",
        "aliases": [
          "Commonsense Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Provides a unique angle on reasoning capabilities required for advanced AI tasks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "CoLoTa dataset",
        "canonical": "CoLoTa Dataset",
        "aliases": [
          "CoLoTa"
        ],
        "category": "unique_technical",
        "rationale": "Specific dataset used in the study, crucial for linking experimental results.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Knowledge Graphs",
      "resolved_canonical": "Knowledge Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Commonsense Reasoning",
      "resolved_canonical": "Commonsense Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "CoLoTa dataset",
      "resolved_canonical": "CoLoTa Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18063.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.18063](https://arxiv.org/abs/2509.18063)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (86.4% similar)
- [[2025-09-23/Large Language Models Meet Knowledge Graphs for Question Answering_ Synthesis and Opportunities_20250923|Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities]] (86.2% similar)
- [[2025-09-23/Question Answering with LLMs and Learning from Answer Sets_20250923|Question Answering with LLMs and Learning from Answer Sets]] (85.6% similar)
- [[2025-09-23/GRIL_ Knowledge Graph Retrieval-Integrated Learning with Large Language Models_20250923|GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models]] (84.4% similar)
- [[2025-09-18/KBM_ Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models_20250918|KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models]] (84.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Graph|Knowledge Graph]]
**âš¡ Unique Technical**: [[keywords/Commonsense Reasoning|Commonsense Reasoning]], [[keywords/CoLoTa Dataset|CoLoTa Dataset]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18063v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ì§€ì‹ ê·¸ë˜í”„(KG)ë¥¼ í™œìš©í•˜ëŠ” ARK-V1ì´ë¼ëŠ” ê°„ë‹¨í•œ KG ì—ì´ì „íŠ¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ARK-V1ì€ ê·¸ë˜í”„ë¥¼ ë°˜ë³µì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìì—°ì–´ ì§ˆì˜ì— ë‹µë³€í•˜ë©°, CoLoTa ë°ì´í„°ì…‹ì„ í†µí•´ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ KG ê¸°ë°˜ ë° ìƒì‹ ì¶”ë¡ ì„ ìš”êµ¬í•©ë‹ˆë‹¤. ARK-V1ì€ Chain-of-Thought ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ìœ¼ë©°, ë” í° ë°±ë³¸ ëª¨ë¸ì€ ë” ë‚˜ì€ ì»¤ë²„ë¦¬ì§€, ì •í™•ì„±, ì•ˆì •ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì´ë‚˜, íŠ¹ì • ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì‹œ ë‚´ë¶€í™”ëœ ì§€ì‹ì´ ì¢…ì¢… ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆê±°ë‚˜ ì˜ëª»ëœ ê²½ìš°ê°€ ë§ë‹¤.
- 2. ì§€ì‹ ê·¸ë˜í”„(KGs)ëŠ” êµ¬ì¡°í™”ëœ ì™¸ë¶€ ì§€ì‹ì„ ì œê³µí•˜ì§€ë§Œ, ë³µì¡ì„±ê³¼ ë‹¤ë‹¨ê³„ ì¶”ë¡  ìš”êµ¬ë¡œ ì¸í•´ í†µí•©ì´ ì–´ë µë‹¤.
- 3. ARK-V1ì€ ìì—°ì–´ ì¿¼ë¦¬ì— ë‹µí•˜ê¸° ìœ„í•´ ê·¸ë˜í”„ë¥¼ ë°˜ë³µì ìœ¼ë¡œ íƒìƒ‰í•˜ëŠ” ê°„ë‹¨í•œ KG-ì—ì´ì „íŠ¸ë¥¼ ì œì•ˆí•œë‹¤.
- 4. ARK-V1ì€ CoLoTa ë°ì´í„°ì…‹ì—ì„œ KG ê¸°ë°˜ ë° ìƒì‹ ì¶”ë¡ ì„ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ ì²´ì¸-ì˜¤ë¸Œ-ìƒê°(Chain-of-Thought) ê¸°ë°˜ë³´ë‹¤ ë†’ì€ ì¡°ê±´ë¶€ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤.
- 5. ë” í° ë°±ë³¸ ëª¨ë¸ì€ ë” ë‚˜ì€ ë²”ìœ„, ì •í™•ì„± ë° ì•ˆì •ì„±ì„ ë³´ì—¬ì£¼ëŠ” ëª…í™•í•œ ê²½í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.


---

*Generated on 2025-09-24 03:36:58*