---
keywords:
  - General User Model
  - Multimodal Learning
  - Proactive Assistants
  - Human-Computer Interaction
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.10831
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:57:49.600645",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "General User Model",
    "Multimodal Learning",
    "Proactive Assistants",
    "Human-Computer Interaction"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "General User Model": 0.8,
    "Multimodal Learning": 0.78,
    "Proactive Assistants": 0.75,
    "Human-Computer Interaction": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "General User Model",
        "canonical": "General User Model",
        "aliases": [
          "GUM"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a novel approach to user modeling.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Observations",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is a trending area that connects well with the paper's focus on diverse data inputs.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Proactive Assistants",
        "canonical": "Proactive Assistants",
        "aliases": [
          "GUMBOs"
        ],
        "category": "unique_technical",
        "rationale": "Proactive assistants represent a specific application of the GUM architecture, highlighting its practical use.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Human-Computer Interaction",
        "canonical": "Human-Computer Interaction",
        "aliases": [
          "HCI"
        ],
        "category": "broad_technical",
        "rationale": "HCI is a foundational field relevant to the paper's exploration of user models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "General User Model",
      "resolved_canonical": "General User Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Observations",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Proactive Assistants",
      "resolved_canonical": "Proactive Assistants",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Human-Computer Interaction",
      "resolved_canonical": "Human-Computer Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Creating General User Models from Computer Use

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.10831.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.10831](https://arxiv.org/abs/2505.10831)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (78.4% similar)
- [[2025-09-22/BTL-UI_ Blink-Think-Link Reasoning Model for GUI Agent_20250922|BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent]] (78.4% similar)
- [[2025-09-22/UniGist_ Towards General and Hardware-aligned Sequence-level Long Context Compression_20250922|UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression]] (78.2% similar)
- [[2025-09-22/MMAPG_ A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs_20250922|MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs]] (78.2% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (78.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Human-Computer Interaction|Human-Computer Interaction]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/General User Model|General User Model]], [[keywords/Proactive Assistants|Proactive Assistants]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.10831v3 Announce Type: replace-cross 
Abstract: Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‚¬ìš©ìì˜ ì»´í“¨í„° ìƒí˜¸ì‘ìš©ì„ ê´€ì°°í•˜ì—¬ ì‚¬ìš©ìì— ëŒ€í•œ ì¼ë°˜ ì‚¬ìš©ì ëª¨ë¸(GUM)ì„ êµ¬ì¶•í•˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. GUMì€ ë¹„êµ¬ì¡°í™”ëœ ì‚¬ìš©ì ê´€ì°° ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì‚¬ìš©ì ì§€ì‹ê³¼ ì„ í˜¸ë„ë¥¼ ìº¡ì²˜í•˜ëŠ” ëª…ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìê°€ ì¹œêµ¬ì™€ì˜ ë©”ì‹œì§€ë¥¼ í†µí•´ ê²°í˜¼ì‹ ì¤€ë¹„ ì¤‘ì„ì„ ì¶”ë¡ í•˜ê±°ë‚˜, í˜‘ì—…ìì˜ í”¼ë“œë°±ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒì„ ì¸ì‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. GUMì€ ë‹¤ì¤‘ ëª¨ë‹¬ ê´€ì°°ì„ í†µí•´ ìƒˆë¡œìš´ ëª…ì œë¥¼ ì¶”ë¡ í•˜ê³ , ê´€ë ¨ ëª…ì œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë§¥ë½ì„ ì œê³µí•˜ë©°, ê¸°ì¡´ ëª…ì œë¥¼ ì§€ì†ì ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ GUMì€ ì±„íŒ… ê¸°ë°˜ ë¹„ì„œì— ë§¥ë½ì„ ì¶”ê°€í•˜ê³ , ìš´ì˜ì²´ì œ ì•Œë¦¼ì„ ê´€ë¦¬í•˜ë©°, ì•± ì „ë°˜ì— ê±¸ì³ ì‚¬ìš©ì ì„ í˜¸ì— ë§ê²Œ ì ì‘í•˜ëŠ” ì¸í„°ë™í‹°ë¸Œ ì—ì´ì „íŠ¸ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. í‰ê°€ ê²°ê³¼, GUMì€ ì‚¬ìš©ìì˜ í–‰ë™ì„ ì •í™•íˆ ì¶”ë¡ í•˜ë©°, GUM ê¸°ë°˜ ë¹„ì„œëŠ” ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•˜ì§€ ì•Šì€ ìœ ìš©í•œ ì œì•ˆì„ ë°œê²¬í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. GUMì€ ë¹„êµ¬ì¡°í™”ëœ ë§¥ë½ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ì—¬, ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš©ì˜ ì˜¤ëœ ë¹„ì „ì„ ì‹¤í˜„í•˜ê³  ìƒˆë¡œìš´ ì¸í„°ë™í‹°ë¸Œ ì‹œìŠ¤í…œì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¼ë°˜ ì‚¬ìš©ì ëª¨ë¸(GUM)ì€ ì‚¬ìš©ìì˜ ëª¨ë“  ì»´í“¨í„° ìƒí˜¸ì‘ìš©ì„ ê´€ì°°í•˜ì—¬ ì‚¬ìš©ìì— ëŒ€í•œ ì§€ì‹ê³¼ ì„ í˜¸ë„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
- 2. GUMì€ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë‚˜ ì‘ì—… íŒ¨í„´ì„ í†µí•´ ì‚¬ìš©ìì˜ í˜„ì¬ ìƒí™©ì´ë‚˜ ê°ì •ì„ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 3. GUMì€ ë‹¤ì¤‘ ëª¨ë“œ ê´€ì°°ì„ í†µí•´ ìƒˆë¡œìš´ ì‚¬ìš©ì ì •ë³´ë¥¼ ì¶”ë¡ í•˜ê³ , ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¬¸ë§¥ì„ ì œê³µí•˜ë©°, ê¸°ì¡´ ì •ë³´ë¥¼ ì§€ì†ì ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.
- 4. GUM ê¸°ë°˜ì˜ ë¹„ì„œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•˜ì§€ ì•Šì€ ì‘ì—…ì„ ì„ ì œì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. GUMì€ ë¹„êµ¬ì¡°í™”ëœ ë¬¸ë§¥ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ëª¨ë“œ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ HCIì˜ ì˜¤ëœ ë¹„ì „ì„ ì‹¤í˜„í•˜ê³  ìƒˆë¡œìš´ ìƒí˜¸ì‘ìš© ì‹œìŠ¤í…œì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:57:49*