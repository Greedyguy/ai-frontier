---
keywords:
  - Large Language Model
  - Role-Play Dialogues
  - Human Evaluation
  - Zero-Shot Learning
  - Few-Shot Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17694
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:06:36.638960",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Role-Play Dialogues",
    "Human Evaluation",
    "Zero-Shot Learning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Role-Play Dialogues": 0.78,
    "Human Evaluation": 0.77,
    "Zero-Shot Learning": 0.8,
    "Few-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, facilitating connections across various NLP research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Role-Play Dialogues",
        "canonical": "Role-Play Dialogues",
        "aliases": [
          "Role-Playing Dialogues",
          "Simulated Dialogues"
        ],
        "category": "unique_technical",
        "rationale": "Specific to the study's context, enabling connections to dialogue systems research.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Human Evaluation",
        "canonical": "Human Evaluation",
        "aliases": [
          "Human Judgement",
          "Manual Assessment"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the study's methodology, linking to evaluation techniques in AI.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-Shot Pairwise Preference",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot Preference"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the trending concept of zero-shot learning in AI evaluations.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Stochastic 6-Shot Construct Ratings",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "6-Shot Ratings"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of few-shot learning, relevant for AI model evaluations.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "responses",
      "quality"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Role-Play Dialogues",
      "resolved_canonical": "Role-Play Dialogues",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Human Evaluation",
      "resolved_canonical": "Human Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-Shot Pairwise Preference",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Stochastic 6-Shot Construct Ratings",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17694.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17694](https://arxiv.org/abs/2509.17694)

## 🔗 유사한 논문
- [[2025-09-23/Evaluating Behavioral Alignment in Conflict Dialogue_ A Multi-Dimensional Comparison of LLM Agents and Humans_20250923|Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans]] (87.2% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (86.9% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.7% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (85.6% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (85.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Role-Play Dialogues|Role-Play Dialogues]], [[keywords/Human Evaluation|Human Evaluation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17694v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation ($N=38$) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.

## 📝 요약

이 연구는 대형 언어 모델(LLM)이 장기적이고 지식에 기반한 역할극 대화에서 어떻게 평가되는지를 조사합니다. 연구는 인간 평가와 자동화된 LLM 평가를 통해 LLM이 생성한 응답과 인간이 작성한 응답을 비교했습니다. 인간 평가에서는 LLM 응답의 자연스러움, 맥락 유지, 전반적 품질이 회차가 진행될수록 저하되는 반면, 인간 작성 응답은 점차 개선되는 것으로 나타났습니다. 참가자들은 일관되게 인간 작성 대화를 선호했으며, 이는 자동화된 평가에서도 확인되었습니다. 이 연구는 LLM의 역할극 대화에서의 품질 저하를 드러내는 벤치마크를 제공하고, LLM을 훈련 시뮬레이션에 신뢰성 있게 통합하기 위한 평가 프레임워크를 제안합니다.

## 🎯 주요 포인트

- 1. LLM이 생성한 응답은 회차가 진행될수록 자연스러움, 맥락 유지, 전반적인 품질에서 크게 저하되는 반면, 인간이 작성한 응답은 점진적으로 개선되었습니다.
- 2. 참가자들은 일관되게 인간이 작성한 대화를 선호하였으며, 이는 자동화된 LLM-as-a-judge 평가에서도 확인되었습니다.
- 3. Gemini 2.0 Flash는 인간 평가자와의 강한 일치를 보여주며, LLM과 인간 응답 간의 품질 격차가 시간이 지남에 따라 확대됨을 확인했습니다.
- 4. 본 연구는 LLM이 지식 기반 역할극 대화에서 저하되는 현상을 드러내는 다회차 벤치마크를 제공하고, 훈련 시뮬레이션에서 LLM의 신뢰할 수 있는 통합을 안내하는 검증된 하이브리드 평가 프레임워크를 제시합니다.


---

*Generated on 2025-09-24 00:06:36*