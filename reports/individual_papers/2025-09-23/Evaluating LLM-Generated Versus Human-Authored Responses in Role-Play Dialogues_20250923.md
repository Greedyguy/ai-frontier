---
keywords:
  - Large Language Model
  - Role-Play Dialogues
  - Human Evaluation
  - Zero-Shot Learning
  - Few-Shot Learning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17694
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:06:36.638960",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Role-Play Dialogues",
    "Human Evaluation",
    "Zero-Shot Learning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Role-Play Dialogues": 0.78,
    "Human Evaluation": 0.77,
    "Zero-Shot Learning": 0.8,
    "Few-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, facilitating connections across various NLP research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Role-Play Dialogues",
        "canonical": "Role-Play Dialogues",
        "aliases": [
          "Role-Playing Dialogues",
          "Simulated Dialogues"
        ],
        "category": "unique_technical",
        "rationale": "Specific to the study's context, enabling connections to dialogue systems research.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Human Evaluation",
        "canonical": "Human Evaluation",
        "aliases": [
          "Human Judgement",
          "Manual Assessment"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the study's methodology, linking to evaluation techniques in AI.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-Shot Pairwise Preference",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot Preference"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the trending concept of zero-shot learning in AI evaluations.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Stochastic 6-Shot Construct Ratings",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "6-Shot Ratings"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of few-shot learning, relevant for AI model evaluations.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "responses",
      "quality"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Role-Play Dialogues",
      "resolved_canonical": "Role-Play Dialogues",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Human Evaluation",
      "resolved_canonical": "Human Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-Shot Pairwise Preference",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Stochastic 6-Shot Construct Ratings",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17694.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17694](https://arxiv.org/abs/2509.17694)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Evaluating Behavioral Alignment in Conflict Dialogue_ A Multi-Dimensional Comparison of LLM Agents and Humans_20250923|Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans]] (87.2% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (86.9% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.7% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (85.6% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Role-Play Dialogues|Role-Play Dialogues]], [[keywords/Human Evaluation|Human Evaluation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17694v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation ($N=38$) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì¥ê¸°ì ì´ê³  ì§€ì‹ì— ê¸°ë°˜í•œ ì—­í• ê·¹ ëŒ€í™”ì—ì„œ ì–´ë–»ê²Œ í‰ê°€ë˜ëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì¸ê°„ í‰ê°€ì™€ ìë™í™”ëœ LLM í‰ê°€ë¥¼ í†µí•´ LLMì´ ìƒì„±í•œ ì‘ë‹µê³¼ ì¸ê°„ì´ ì‘ì„±í•œ ì‘ë‹µì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ì¸ê°„ í‰ê°€ì—ì„œëŠ” LLM ì‘ë‹µì˜ ìì—°ìŠ¤ëŸ¬ì›€, ë§¥ë½ ìœ ì§€, ì „ë°˜ì  í’ˆì§ˆì´ íšŒì°¨ê°€ ì§„í–‰ë ìˆ˜ë¡ ì €í•˜ë˜ëŠ” ë°˜ë©´, ì¸ê°„ ì‘ì„± ì‘ë‹µì€ ì ì°¨ ê°œì„ ë˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì°¸ê°€ìë“¤ì€ ì¼ê´€ë˜ê²Œ ì¸ê°„ ì‘ì„± ëŒ€í™”ë¥¼ ì„ í˜¸í–ˆìœ¼ë©°, ì´ëŠ” ìë™í™”ëœ í‰ê°€ì—ì„œë„ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ì—­í• ê·¹ ëŒ€í™”ì—ì„œì˜ í’ˆì§ˆ ì €í•˜ë¥¼ ë“œëŸ¬ë‚´ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œê³µí•˜ê³ , LLMì„ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ì— ì‹ ë¢°ì„± ìˆê²Œ í†µí•©í•˜ê¸° ìœ„í•œ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMì´ ìƒì„±í•œ ì‘ë‹µì€ íšŒì°¨ê°€ ì§„í–‰ë ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ì›€, ë§¥ë½ ìœ ì§€, ì „ë°˜ì ì¸ í’ˆì§ˆì—ì„œ í¬ê²Œ ì €í•˜ë˜ëŠ” ë°˜ë©´, ì¸ê°„ì´ ì‘ì„±í•œ ì‘ë‹µì€ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
- 2. ì°¸ê°€ìë“¤ì€ ì¼ê´€ë˜ê²Œ ì¸ê°„ì´ ì‘ì„±í•œ ëŒ€í™”ë¥¼ ì„ í˜¸í•˜ì˜€ìœ¼ë©°, ì´ëŠ” ìë™í™”ëœ LLM-as-a-judge í‰ê°€ì—ì„œë„ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. Gemini 2.0 FlashëŠ” ì¸ê°„ í‰ê°€ìì™€ì˜ ê°•í•œ ì¼ì¹˜ë¥¼ ë³´ì—¬ì£¼ë©°, LLMê³¼ ì¸ê°„ ì‘ë‹µ ê°„ì˜ í’ˆì§ˆ ê²©ì°¨ê°€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ í™•ëŒ€ë¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- 4. ë³¸ ì—°êµ¬ëŠ” LLMì´ ì§€ì‹ ê¸°ë°˜ ì—­í• ê·¹ ëŒ€í™”ì—ì„œ ì €í•˜ë˜ëŠ” í˜„ìƒì„ ë“œëŸ¬ë‚´ëŠ” ë‹¤íšŒì°¨ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œê³µí•˜ê³ , í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ LLMì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í†µí•©ì„ ì•ˆë‚´í•˜ëŠ” ê²€ì¦ëœ í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:06:36*