---
keywords:
  - Retrieval Augmented Generation
  - Zero-Shot Learning
  - Historical Trajectory Retrieval
  - Geographical Reranking
  - Agentic LLM Rectification
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17066
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:55:24.497409",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval Augmented Generation",
    "Zero-Shot Learning",
    "Historical Trajectory Retrieval",
    "Geographical Reranking",
    "Agentic LLM Rectification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Retrieval Augmented Generation": 0.82,
    "Zero-Shot Learning": 0.79,
    "Historical Trajectory Retrieval": 0.75,
    "Geographical Reranking": 0.78,
    "Agentic LLM Rectification": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "This technique is central to the proposed framework and connects well with existing retrieval and generation methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Zero-shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "ZSL"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a key aspect of the framework, enabling connections to broader learning paradigms.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Historical Trajectory Retriever",
        "canonical": "Historical Trajectory Retrieval",
        "aliases": [
          "HTR"
        ],
        "category": "unique_technical",
        "rationale": "This component is unique to the paper and crucial for understanding the retrieval process in the framework.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Geographical Distance Reranker",
        "canonical": "Geographical Reranking",
        "aliases": [
          "GDR"
        ],
        "category": "unique_technical",
        "rationale": "This reranking method is specific to the geographical context of the paper, enhancing spatial relevance.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.77,
        "link_intent_score": 0.78
      },
      {
        "surface": "Agentic LLM Rectifier",
        "canonical": "Agentic LLM Rectification",
        "aliases": [
          "ALR"
        ],
        "category": "unique_technical",
        "rationale": "This rectification process is a novel contribution, refining outputs through self-reflection.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "Next POI Recommendation",
      "Foursquare datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Zero-shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Historical Trajectory Retriever",
      "resolved_canonical": "Historical Trajectory Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Geographical Distance Reranker",
      "resolved_canonical": "Geographical Reranking",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.77,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Agentic LLM Rectifier",
      "resolved_canonical": "Agentic LLM Rectification",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17066.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17066](https://arxiv.org/abs/2509.17066)

## 🔗 유사한 논문
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (82.1% similar)
- [[2025-09-19/From Pixels to Urban Policy-Intelligence_ Recovering Legacy Effects of Redlining with a Multimodal LLM_20250919|From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal LLM]] (82.0% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (81.5% similar)
- [[2025-09-23/LLMs as Layout Designers_ A Spatial Reasoning Perspective_20250923|LLMs as Layout Designers: A Spatial Reasoning Perspective]] (81.2% similar)
- [[2025-09-17/Q-ROAR_ Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs_20250917|Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs]] (81.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Historical Trajectory Retrieval|Historical Trajectory Retrieval]], [[keywords/Geographical Reranking|Geographical Reranking]], [[keywords/Agentic LLM Rectification|Agentic LLM Rectification]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17066v1 Announce Type: new 
Abstract: Next point-of-interest (POI) recommendation predicts a user's next destination from historical movements. Traditional models require intensive training, while LLMs offer flexible and generalizable zero-shot solutions but often generate generic or geographically irrelevant results due to missing trajectory and spatial context. To address these issues, we propose RALLM-POI, a framework that couples LLMs with retrieval-augmented generation and self-rectification. We first propose a Historical Trajectory Retriever (HTR) that retrieves relevant past trajectories to serve as contextual references, which are then reranked by a Geographical Distance Reranker (GDR) for prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier (ALR) is designed to refine outputs through self-reflection. Without additional training, RALLM-POI achieves substantial accuracy gains across three real-world Foursquare datasets, outperforming both conventional and LLM-based baselines. Code is released at https://github.com/LKRcrocodile/RALLM-POI.

## 📝 요약

이 논문은 사용자의 다음 방문지를 예측하는 '다음 관심 장소(POI) 추천' 문제를 다룹니다. 기존 모델은 많은 훈련이 필요하지만, 대형 언어 모델(LLM)은 유연한 제로샷 솔루션을 제공하는 반면, 경로 및 공간적 맥락이 부족해 일반적이거나 지리적으로 부적절한 결과를 생성할 수 있습니다. 이를 해결하기 위해, LLM과 검색 기반 생성 및 자기 수정 기능을 결합한 RALLM-POI 프레임워크를 제안합니다. 먼저, 과거 경로를 검색해 맥락을 제공하는 '역사적 경로 검색기(HTR)'를 제안하고, 공간적으로 관련 있는 경로를 우선시하는 '지리적 거리 재정렬기(GDR)'로 재정렬합니다. 마지막으로, '에이전틱 LLM 수정기(ALR)'를 통해 결과를 자기 반성을 통해 개선합니다. 추가 훈련 없이도 RALLM-POI는 세 개의 실제 Foursquare 데이터셋에서 기존 모델과 LLM 기반 모델을 능가하는 정확도를 달성했습니다. 코드: https://github.com/LKRcrocodile/RALLM-POI.

## 🎯 주요 포인트

- 1. RALLM-POI는 LLM과 검색 강화 생성 및 자기 수정 기법을 결합하여 POI 추천의 정확성을 높이는 프레임워크입니다.
- 2. Historical Trajectory Retriever (HTR)는 과거의 관련 경로를 검색하여 문맥적 참조로 활용합니다.
- 3. Geographical Distance Reranker (GDR)는 공간적으로 관련 있는 경로를 우선시하여 재정렬합니다.
- 4. Agentic LLM Rectifier (ALR)는 자기 반성을 통해 출력 결과를 개선합니다.
- 5. RALLM-POI는 추가 훈련 없이도 세 개의 실제 Foursquare 데이터셋에서 기존 모델과 LLM 기반 모델을 능가하는 정확도를 달성했습니다.


---

*Generated on 2025-09-23 22:55:24*