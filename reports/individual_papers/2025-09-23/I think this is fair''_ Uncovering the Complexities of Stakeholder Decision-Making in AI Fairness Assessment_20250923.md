---
keywords:
  - AI Fairness
  - Stakeholder Decision-Making
  - Fairness Metrics
  - Protected Features
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17956
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:04:28.007391",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AI Fairness",
    "Stakeholder Decision-Making",
    "Fairness Metrics",
    "Protected Features"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AI Fairness": 0.79,
    "Stakeholder Decision-Making": 0.77,
    "Fairness Metrics": 0.8,
    "Protected Features": 0.81
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AI Fairness Assessment",
        "canonical": "AI Fairness",
        "aliases": [
          "Fairness in AI",
          "AI Fairness Evaluation"
        ],
        "category": "unique_technical",
        "rationale": "AI Fairness is a specialized area that connects to broader discussions on ethical AI and governance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Stakeholder Decision-Making",
        "canonical": "Stakeholder Decision-Making",
        "aliases": [
          "Stakeholder Choices",
          "Stakeholder Judgments"
        ],
        "category": "unique_technical",
        "rationale": "Understanding stakeholder decision-making is crucial for linking to governance and ethics in AI.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Fairness Metrics",
        "canonical": "Fairness Metrics",
        "aliases": [
          "Equity Metrics",
          "Bias Metrics"
        ],
        "category": "specific_connectable",
        "rationale": "Fairness metrics are a key component in evaluating AI systems, linking to technical and ethical discussions.",
        "novelty_score": 0.58,
        "connectivity_score": 0.85,
        "specificity_score": 0.76,
        "link_intent_score": 0.8
      },
      {
        "surface": "Protected Features",
        "canonical": "Protected Features",
        "aliases": [
          "Sensitive Attributes",
          "Protected Attributes"
        ],
        "category": "specific_connectable",
        "rationale": "Protected features are central to discussions on bias and fairness in AI, connecting to legal and ethical frameworks.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.81
      }
    ],
    "ban_list_suggestions": [
      "AI experts",
      "credit rating scenario"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AI Fairness Assessment",
      "resolved_canonical": "AI Fairness",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Stakeholder Decision-Making",
      "resolved_canonical": "Stakeholder Decision-Making",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Fairness Metrics",
      "resolved_canonical": "Fairness Metrics",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.85,
        "specificity": 0.76,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Protected Features",
      "resolved_canonical": "Protected Features",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.81
      }
    }
  ]
}
-->

# "I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17956.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17956](https://arxiv.org/abs/2509.17956)

## 🔗 유사한 논문
- [[2025-09-22/Algorithmic Fairness_ Not a Purely Technical but Socio-Technical Property_20250922|Algorithmic Fairness: Not a Purely Technical but Socio-Technical Property]] (86.7% similar)
- [[2025-09-22/Where Fact Ends and Fairness Begins_ Redefining AI Bias Evaluation through Cognitive Biases_20250922|Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases]] (82.9% similar)
- [[2025-09-22/Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents_20250922|Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents]] (82.2% similar)
- [[2025-09-22/The Great AI Witch Hunt_ Reviewers Perception and (Mis)Conception of Generative AI in Research Writing_20250922|The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing]] (81.7% similar)
- [[2025-09-22/Fairness-in-the-Workflow_ How Machine Learning Practitioners at Big Tech Companies Approach Fairness in Recommender Systems_20250922|Fairness-in-the-Workflow: How Machine Learning Practitioners at Big Tech Companies Approach Fairness in Recommender Systems]] (81.6% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Fairness Metrics|Fairness Metrics]], [[keywords/Protected Features|Protected Features]]
**⚡ Unique Technical**: [[keywords/AI Fairness|AI Fairness]], [[keywords/Stakeholder Decision-Making|Stakeholder Decision-Making]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17956v1 Announce Type: new 
Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI experts who select protected features, fairness metrics, and set fairness thresholds. However, little is known about how stakeholders, particularly those affected by AI outcomes but lacking AI expertise, assess fairness. To address this gap, we conducted a qualitative study with 30 stakeholders without AI expertise, representing potential decision subjects in a credit rating scenario, to examine how they assess fairness when placed in the role of deciding on features with priority, metrics, and thresholds. We reveal that stakeholders' fairness decisions are more complex than typical AI expert practices: they considered features far beyond legally protected features, tailored metrics for specific contexts, set diverse yet stricter fairness thresholds, and even preferred designing customized fairness. Our results extend the understanding of how stakeholders can meaningfully contribute to AI fairness governance and mitigation, underscoring the importance of incorporating stakeholders' nuanced fairness judgments.

## 📝 요약

이 논문은 AI 공정성 평가에서 AI 전문가가 아닌 이해관계자들이 어떻게 공정성을 평가하는지를 조사합니다. AI 전문 지식이 없는 30명의 이해관계자를 대상으로 신용 평가 시나리오에서 공정성 판단 과정을 연구한 결과, 이들은 법적으로 보호되는 특성 외에도 다양한 특성을 고려하고, 특정 맥락에 맞춘 지표를 사용하며, 더 엄격한 공정성 기준을 설정하는 등 복잡한 결정을 내렸습니다. 연구는 이해관계자의 공정성 판단이 AI 공정성 관리와 완화에 의미 있는 기여를 할 수 있음을 보여주며, 이들의 세밀한 판단을 반영하는 것이 중요하다고 강조합니다.

## 🎯 주요 포인트

- 1. AI 전문가가 아닌 이해관계자들은 법적으로 보호된 특성을 넘어 다양한 특성을 고려하여 공정성을 평가합니다.
- 2. 이해관계자들은 특정 맥락에 맞춘 맞춤형 지표를 선호하며, 더 다양하고 엄격한 공정성 기준을 설정합니다.
- 3. 이해관계자들은 맞춤형 공정성 설계를 선호하며, 이는 AI 공정성 관리와 완화에 의미 있는 기여를 할 수 있음을 보여줍니다.
- 4. 연구 결과는 이해관계자의 세심한 공정성 판단을 AI 공정성 거버넌스에 통합하는 것이 중요함을 강조합니다.


---

*Generated on 2025-09-23 23:04:28*