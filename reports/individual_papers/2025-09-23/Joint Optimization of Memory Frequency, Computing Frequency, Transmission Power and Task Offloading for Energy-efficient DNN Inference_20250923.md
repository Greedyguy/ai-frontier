---
keywords:
  - Neural Network
  - Dynamic Voltage and Frequency Scaling
  - Energy-efficient Inference
  - Task Offloading
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17970
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:14:16.974560",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Network",
    "Dynamic Voltage and Frequency Scaling",
    "Energy-efficient Inference",
    "Task Offloading"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Network": 0.85,
    "Dynamic Voltage and Frequency Scaling": 0.8,
    "Energy-efficient Inference": 0.78,
    "Task Offloading": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "DNN",
          "Deep Learning Models"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are foundational to the paper's discussion on optimizing inference, linking well with existing literature on machine learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Dynamic Voltage and Frequency Scaling",
        "canonical": "Dynamic Voltage and Frequency Scaling",
        "aliases": [
          "DVFS"
        ],
        "category": "unique_technical",
        "rationale": "DVFS is a specific technique central to the paper's optimization strategy, offering a unique angle for linking energy efficiency discussions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Energy-efficient DNN Inference",
        "canonical": "Energy-efficient Inference",
        "aliases": [
          "Efficient Inference",
          "Low-power Inference"
        ],
        "category": "unique_technical",
        "rationale": "This concept is crucial for understanding the paper's contribution to reducing energy consumption in neural network operations.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Task Offloading",
        "canonical": "Task Offloading",
        "aliases": [
          "Computation Offloading"
        ],
        "category": "specific_connectable",
        "rationale": "Task Offloading is a key strategy in distributed computing, relevant for linking discussions on resource management.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "memory frequency",
      "computing frequency",
      "transmission power"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Dynamic Voltage and Frequency Scaling",
      "resolved_canonical": "Dynamic Voltage and Frequency Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Energy-efficient DNN Inference",
      "resolved_canonical": "Energy-efficient Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Task Offloading",
      "resolved_canonical": "Task Offloading",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17970.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17970](https://arxiv.org/abs/2509.17970)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/MaRVIn_ A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration_20250919|MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration]] (83.7% similar)
- [[2025-09-23/Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers_20250923|Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers]] (83.6% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (81.6% similar)
- [[2025-09-22/Learning to Optimize Capacity Planning in Semiconductor Manufacturing_20250922|Learning to Optimize Capacity Planning in Semiconductor Manufacturing]] (81.0% similar)
- [[2025-09-17/A Neural Network for the Identical Kuramoto Equation_ Architectural Considerations and Performance Evaluation_20250917|A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation]] (80.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Task Offloading|Task Offloading]]
**âš¡ Unique Technical**: [[keywords/Dynamic Voltage and Frequency Scaling|Dynamic Voltage and Frequency Scaling]], [[keywords/Energy-efficient Inference|Energy-efficient Inference]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17970v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) have been widely applied in diverse applications, but the problems of high latency and energy overhead are inevitable on resource-constrained devices. To address this challenge, most researchers focus on the dynamic voltage and frequency scaling (DVFS) technique to balance the latency and energy consumption by changing the computing frequency of processors. However, the adjustment of memory frequency is usually ignored and not fully utilized to achieve efficient DNN inference, which also plays a significant role in the inference time and energy consumption. In this paper, we first investigate the impact of joint memory frequency and computing frequency scaling on the inference time and energy consumption with a model-based and data-driven method. Then by combining with the fitting parameters of different DNN models, we give a preliminary analysis for the proposed model to see the effects of adjusting memory frequency and computing frequency simultaneously. Finally, simulation results in local inference and cooperative inference cases further validate the effectiveness of jointly scaling the memory frequency and computing frequency to reduce the energy consumption of devices.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì› ì œì•½ì´ ìˆëŠ” ì¥ì¹˜ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì¶”ë¡  ì‹œ ë°œìƒí•˜ëŠ” ë†’ì€ ì§€ì—° ì‹œê°„ê³¼ ì—ë„ˆì§€ ì†Œëª¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë©”ëª¨ë¦¬ ì£¼íŒŒìˆ˜ì™€ ì»´í“¨íŒ… ì£¼íŒŒìˆ˜ë¥¼ ë™ì‹œì— ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ì£¼ë¡œ ì»´í“¨íŒ… ì£¼íŒŒìˆ˜ ì¡°ì •ì— ì¤‘ì ì„ ë‘ì—ˆìœ¼ë‚˜, ë©”ëª¨ë¦¬ ì£¼íŒŒìˆ˜ì˜ ì¡°ì •ë„ ì¶”ë¡  ì‹œê°„ê³¼ ì—ë„ˆì§€ ì†Œë¹„ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ëª¨ë¸ ê¸°ë°˜ ë° ë°ì´í„° ê¸°ë°˜ ë°©ë²•ì„ í†µí•´ ë©”ëª¨ë¦¬ì™€ ì»´í“¨íŒ… ì£¼íŒŒìˆ˜ì˜ ê³µë™ ì¡°ì •ì´ ì¥ì¹˜ì˜ ì—ë„ˆì§€ ì†Œë¹„ë¥¼ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìì› ì œì•½ì´ ìˆëŠ” ì¥ì¹˜ì—ì„œ ë†’ì€ ì§€ì—° ì‹œê°„ê³¼ ì—ë„ˆì§€ ì˜¤ë²„í—¤ë“œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë™ì  ì „ì•• ë° ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ë§(DVFS) ê¸°ìˆ ì´ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
- 2. ë©”ëª¨ë¦¬ ì£¼íŒŒìˆ˜ ì¡°ì •ì€ ì¢…ì¢… ê°„ê³¼ë˜ì§€ë§Œ, ì´ëŠ” ì¶”ë¡  ì‹œê°„ê³¼ ì—ë„ˆì§€ ì†Œë¹„ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë©”ëª¨ë¦¬ ì£¼íŒŒìˆ˜ì™€ ì»´í“¨íŒ… ì£¼íŒŒìˆ˜ì˜ ê³µë™ ì¡°ì •ì´ ì¶”ë¡  ì‹œê°„ê³¼ ì—ë„ˆì§€ ì†Œë¹„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ëª¨ë¸ ê¸°ë°˜ ë° ë°ì´í„° ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ DNN ëª¨ë¸ì˜ í”¼íŒ… íŒŒë¼ë¯¸í„°ë¥¼ ê²°í•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì£¼íŒŒìˆ˜ì™€ ì»´í“¨íŒ… ì£¼íŒŒìˆ˜ë¥¼ ë™ì‹œì— ì¡°ì •í•˜ëŠ” íš¨ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- 5. ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ëŠ” ë©”ëª¨ë¦¬ ì£¼íŒŒìˆ˜ì™€ ì»´í“¨íŒ… ì£¼íŒŒìˆ˜ë¥¼ ê³µë™ìœ¼ë¡œ ì¡°ì •í•¨ìœ¼ë¡œì¨ ì¥ì¹˜ì˜ ì—ë„ˆì§€ ì†Œë¹„ë¥¼ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:14:16*