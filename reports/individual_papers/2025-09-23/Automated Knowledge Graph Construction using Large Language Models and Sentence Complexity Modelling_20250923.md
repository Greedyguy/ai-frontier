---
keywords:
  - Large Language Model
  - Sentence Complexity Modelling
  - Coreference Resolution
  - Relation Extraction
  - Knowledge Graph Construction
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17289
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:22:56.714769",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Sentence Complexity Modelling",
    "Coreference Resolution",
    "Relation Extraction",
    "Knowledge Graph Construction"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Sentence Complexity Modelling": 0.78,
    "Coreference Resolution": 0.8,
    "Relation Extraction": 0.83,
    "Knowledge Graph Construction": 0.84
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and connect well with existing NLP research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sentence Complexity Modelling",
        "canonical": "Sentence Complexity Modelling",
        "aliases": [
          "Sentence Complexity"
        ],
        "category": "unique_technical",
        "rationale": "The paper introduces a novel approach to sentence complexity, which is crucial for the proposed knowledge graph construction.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Coreference Resolution",
        "canonical": "Coreference Resolution",
        "aliases": [
          "Coreference"
        ],
        "category": "specific_connectable",
        "rationale": "Coreference resolution is a key component in the pipeline, enhancing the extraction of knowledge triples.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Relation Extraction",
        "canonical": "Relation Extraction",
        "aliases": [
          "RE"
        ],
        "category": "specific_connectable",
        "rationale": "Relation extraction is a critical task in NLP and directly relates to the paper's improvements over state-of-the-art methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.83
      },
      {
        "surface": "Knowledge Graph Construction",
        "canonical": "Knowledge Graph Construction",
        "aliases": [
          "KG Construction"
        ],
        "category": "specific_connectable",
        "rationale": "The paper's primary contribution is in automated knowledge graph construction, making it a pivotal concept for linking.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.77,
        "link_intent_score": 0.84
      }
    ],
    "ban_list_suggestions": [
      "pipeline",
      "dataset",
      "code"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sentence Complexity Modelling",
      "resolved_canonical": "Sentence Complexity Modelling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Coreference Resolution",
      "resolved_canonical": "Coreference Resolution",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Relation Extraction",
      "resolved_canonical": "Relation Extraction",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Knowledge Graph Construction",
      "resolved_canonical": "Knowledge Graph Construction",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.77,
        "link_intent": 0.84
      }
    }
  ]
}
-->

# Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17289.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17289](https://arxiv.org/abs/2509.17289)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/K-DeCore_ Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling_20250923|K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling]] (84.8% similar)
- [[2025-09-19/DeKeyNLU_ Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction_20250919|DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction]] (81.4% similar)
- [[2025-09-23/MSCoRe_ A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents_20250923|MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents]] (81.1% similar)
- [[2025-09-18/KBM_ Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models_20250918|KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models]] (81.0% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Coreference Resolution|Coreference Resolution]], [[keywords/Relation Extraction|Relation Extraction]], [[keywords/Knowledge Graph Construction|Knowledge Graph Construction]]
**âš¡ Unique Technical**: [[keywords/Sentence Complexity Modelling|Sentence Complexity Modelling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17289v1 Announce Type: new 
Abstract: We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting sentence-level knowledge graphs by combining robust coreference resolution with syntactic sentence decomposition. Using our model, we contribute a dataset of over 150,000 knowledge triples, which is open source. We also contribute a training corpus of 7248 rows for sentence complexity, 190 rows of gold human annotations for co-reference resolution using open source lung-cancer abstracts from PubMed, 900 rows of gold human annotations for sentence conversion policies, and 398 triples of gold human annotations. We systematically select optimal prompt-model pairs across five complexity categories, showing that hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match accuracy on sentence simplification. On relation extraction (RE), our pipeline achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference and decomposition increases recall on rare relations by over 20%. Code and dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025

## ğŸ“ ìš”ì•½

CoDe-KGëŠ” ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ, ê°•ë ¥í•œ ëŒ€ëª…ì‚¬ í•´ì„ê³¼ êµ¬ë¬¸ ë¶„í•´ë¥¼ ê²°í•©í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ í†µí•´ 15ë§Œ ê°œ ì´ìƒì˜ ì§€ì‹ ì‚¼ì¤‘í•­ ë°ì´í„°ì…‹ì„ ì œê³µí•˜ë©°, ë¬¸ì¥ ë³µì¡ì„±, ëŒ€ëª…ì‚¬ í•´ì„, ë¬¸ì¥ ë³€í™˜ ì •ì±…ì— ëŒ€í•œ ì¸ê°„ ì£¼ì„ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤. ë¬¸ì¥ ë‹¨ìˆœí™”ì—ì„œëŠ” ìµœëŒ€ 99.8%ì˜ ì •í™•ë„ë¥¼, ê´€ê³„ ì¶”ì¶œì—ì„œëŠ” REBEL ë°ì´í„°ì…‹ì—ì„œ 65.8%ì˜ ë§¤í¬ë¡œ F1 ì ìˆ˜ë¡œ ì´ì „ ìµœê³  ì„±ëŠ¥ë³´ë‹¤ 8ì  í–¥ìƒëœ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ì½”ì–´í¼ëŸ°ìŠ¤ì™€ ë¶„í•´ í†µí•©ì´ ë“œë¬¸ ê´€ê³„ì—ì„œì˜ ì¬í˜„ìœ¨ì„ 20% ì´ìƒ ì¦ê°€ì‹œí‚´ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì½”ë“œì™€ ë°ì´í„°ì…‹ì€ GitHubì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CoDe-KGëŠ” ë¬¸ì¥ ìˆ˜ì¤€ ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œì„ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ, ê°•ë ¥í•œ ëŒ€ëª…ì‚¬ í•´ì†Œì™€ êµ¬ë¬¸ì  ë¬¸ì¥ ë¶„í•´ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
- 2. ì´ ëª¨ë¸ì„ í†µí•´ 150,000ê°œ ì´ìƒì˜ ì§€ì‹ íŠ¸ë¦¬í”Œë¡œ êµ¬ì„±ëœ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„°ì…‹ì„ ì œê³µí•©ë‹ˆë‹¤.
- 3. ë¬¸ì¥ ë‹¨ìˆœí™”ì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ê³  ì‚¬ìŠ¬ê³¼ ì†Œìˆ˜ ìƒ· í”„ë¡¬í”„íŠ¸ê°€ ìµœëŒ€ 99.8%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. ê´€ê³„ ì¶”ì¶œ(RE)ì—ì„œ REBEL ë°ì´í„°ì…‹ì—ì„œ 65.8%ì˜ ë§¤í¬ë¡œ-F1 ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©°, ì´ì „ ìµœê³  ì„±ëŠ¥ë³´ë‹¤ 8í¬ì¸íŠ¸ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. ëŒ€ëª…ì‚¬ í•´ì†Œì™€ ë¬¸ì¥ ë¶„í•´ë¥¼ í†µí•©í•˜ë©´ í¬ê·€ ê´€ê³„ì— ëŒ€í•œ ë¦¬ì½œì´ 20% ì´ìƒ ì¦ê°€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:22:56*