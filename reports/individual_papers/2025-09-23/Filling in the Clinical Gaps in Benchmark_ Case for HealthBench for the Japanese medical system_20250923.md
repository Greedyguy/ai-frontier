---
keywords:
  - HealthBench
  - Large Language Model
  - GPT-4.1
  - LLM-as-a-Judge
  - Contextual Gaps
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17444
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:27:00.893619",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "HealthBench",
    "Large Language Model",
    "GPT-4.1",
    "LLM-as-a-Judge",
    "Contextual Gaps"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "HealthBench": 0.78,
    "Large Language Model": 0.82,
    "GPT-4.1": 0.8,
    "LLM-as-a-Judge": 0.77,
    "Contextual Gaps": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "HealthBench",
        "canonical": "HealthBench",
        "aliases": [
          "Medical Benchmark"
        ],
        "category": "unique_technical",
        "rationale": "HealthBench is central to the study's evaluation framework and its adaptation is crucial for Japanese medical systems.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "The study evaluates the performance of LLMs, which is a key concept in understanding the benchmark's applicability.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "GPT-4.1",
        "canonical": "GPT-4.1",
        "aliases": [
          "Multilingual Model"
        ],
        "category": "specific_connectable",
        "rationale": "GPT-4.1 is a high-performing model used for comparison in the study, highlighting its role in multilingual contexts.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "LLM-as-a-Judge",
        "canonical": "LLM-as-a-Judge",
        "aliases": [
          "LLM Evaluation Method"
        ],
        "category": "unique_technical",
        "rationale": "This method is used to classify benchmark scenarios, making it a unique approach in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "contextual gaps",
        "canonical": "Contextual Gaps",
        "aliases": [
          "Content Misalignment"
        ],
        "category": "specific_connectable",
        "rationale": "Identifying contextual gaps is crucial for adapting benchmarks to local guidelines and systems.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "HealthBench",
      "resolved_canonical": "HealthBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "GPT-4.1",
      "resolved_canonical": "GPT-4.1",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "LLM-as-a-Judge",
      "resolved_canonical": "LLM-as-a-Judge",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "contextual gaps",
      "resolved_canonical": "Contextual Gaps",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17444.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17444](https://arxiv.org/abs/2509.17444)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (84.2% similar)
- [[2025-09-23/RephQA_ Evaluating Readability of Large Language Models in Public Health Question Answering_20250923|RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering]] (83.0% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (82.2% similar)
- [[2025-09-19/Ticket-Bench_ A Kickoff for Multilingual and Regionalized Agent Evaluation_20250919|Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation]] (81.8% similar)
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/GPT-4.1|GPT-4.1]], [[keywords/Contextual Gaps|Contextual Gaps]]
**âš¡ Unique Technical**: [[keywords/HealthBench|HealthBench]], [[keywords/LLM-as-a-Judge|LLM-as-a-Judge]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17444v1 Announce Type: new 
Abstract: This study investigates the applicability of HealthBench, a large-scale, rubric-based medical benchmark, to the Japanese context. While robust evaluation frameworks are crucial for the safe development of medical LLMs, resources in Japanese remain limited, often relying on translated multiple-choice questions. Our research addresses this gap by first establishing a performance baseline, applying a machine-translated version of HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second, we employ an LLM-as-a-Judge approach to systematically classify the benchmark's scenarios and rubric criteria, identifying "contextual gaps" where content is misaligned with Japan's clinical guidelines, healthcare systems, or cultural norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric mismatches and a significant failure in the Japanese-native model, which lacked the required clinical completeness. Furthermore, our classification indicates that while the majority of scenarios are applicable, a substantial portion of the rubric criteria requires localization. This work underscores the limitations of direct benchmark translation and highlights the urgent need for a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable and safe evaluation of medical LLMs in Japan.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€ê·œëª¨ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì¸ HealthBenchë¥¼ ì¼ë³¸ì— ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì¼ë³¸ì–´ ìì›ì€ ì œí•œì ì´ë©° ì£¼ë¡œ ë²ˆì—­ëœ ì„ íƒí˜• ì§ˆë¬¸ì— ì˜ì¡´í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë¨¼ì € HealthBenchì˜ 5,000ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê¸°ê³„ ë²ˆì—­í•˜ì—¬ ë‹¤êµ­ì–´ ëª¨ë¸(GPT-4.1)ê³¼ ì¼ë³¸ì–´ ëª¨ë¸(LLM-jp-3.1)ì˜ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, LLM-as-a-Judge ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë‚˜ë¦¬ì˜¤ì™€ ê¸°ì¤€ì„ ë¶„ë¥˜í•˜ê³ , ì¼ë³¸ì˜ ì„ìƒ ì§€ì¹¨ ë° ë¬¸í™”ì™€ ë§ì§€ ì•ŠëŠ” "ë§¥ë½ì  ì°¨ì´"ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ GPT-4.1ì˜ ì„±ëŠ¥ì´ ì•½ê°„ ì €í•˜ë˜ì—ˆê³ , ì¼ë³¸ì–´ ëª¨ë¸ì€ ì„ìƒ ì™„ì „ì„±ì´ ë¶€ì¡±í•˜ì—¬ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ë§ì€ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì ìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ìƒë‹¹ ë¶€ë¶„ì˜ ê¸°ì¤€ì€ í˜„ì§€í™”ê°€ í•„ìš”í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì§ì ‘ ë²ˆì—­ì˜ í•œê³„ë¥¼ ê°•ì¡°í•˜ë©°, ì¼ë³¸ì— ì í•©í•œ J-HealthBench ê°œë°œì˜ í•„ìš”ì„±ì„ ì œê¸°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. HealthBenchì˜ ì¼ë³¸ ì ìš© ê°€ëŠ¥ì„±ì„ ì¡°ì‚¬í•˜ì—¬ ì¼ë³¸ì–´ ì˜ë£Œ LLM í‰ê°€ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì í•©ë‹ˆë‹¤.
- 2. GPT-4.1ê³¼ ì¼ë³¸ì–´ ëª¨ë¸ LLM-jp-3.1ì„ í‰ê°€í•˜ì—¬ ì„±ëŠ¥ ê¸°ì¤€ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
- 3. LLM-as-a-Judge ì ‘ê·¼ë²•ì„ í†µí•´ ì¼ë³¸ì˜ ì„ìƒ ì§€ì¹¨ ë° ë¬¸í™”ì  ê·œë²”ê³¼ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.
- 4. GPT-4.1ì€ ë£¨ë¸Œë¦­ ë¶ˆì¼ì¹˜ë¡œ ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì˜€ê³ , ì¼ë³¸ì–´ ëª¨ë¸ì€ ì„ìƒ ì™„ì „ì„± ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.
- 5. ì§ì ‘ ë²ˆì—­ëœ ë²¤ì¹˜ë§ˆí¬ì˜ í•œê³„ë¥¼ ì§€ì í•˜ë©°, ì¼ë³¸ì— ë§ì¶˜ J-HealthBenchì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:27:00*