---
keywords:
  - HealthBench
  - Large Language Model
  - GPT-4.1
  - LLM-as-a-Judge
  - Contextual Gaps
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17444
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:27:00.893619",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "HealthBench",
    "Large Language Model",
    "GPT-4.1",
    "LLM-as-a-Judge",
    "Contextual Gaps"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "HealthBench": 0.78,
    "Large Language Model": 0.82,
    "GPT-4.1": 0.8,
    "LLM-as-a-Judge": 0.77,
    "Contextual Gaps": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "HealthBench",
        "canonical": "HealthBench",
        "aliases": [
          "Medical Benchmark"
        ],
        "category": "unique_technical",
        "rationale": "HealthBench is central to the study's evaluation framework and its adaptation is crucial for Japanese medical systems.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "The study evaluates the performance of LLMs, which is a key concept in understanding the benchmark's applicability.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "GPT-4.1",
        "canonical": "GPT-4.1",
        "aliases": [
          "Multilingual Model"
        ],
        "category": "specific_connectable",
        "rationale": "GPT-4.1 is a high-performing model used for comparison in the study, highlighting its role in multilingual contexts.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "LLM-as-a-Judge",
        "canonical": "LLM-as-a-Judge",
        "aliases": [
          "LLM Evaluation Method"
        ],
        "category": "unique_technical",
        "rationale": "This method is used to classify benchmark scenarios, making it a unique approach in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "contextual gaps",
        "canonical": "Contextual Gaps",
        "aliases": [
          "Content Misalignment"
        ],
        "category": "specific_connectable",
        "rationale": "Identifying contextual gaps is crucial for adapting benchmarks to local guidelines and systems.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "HealthBench",
      "resolved_canonical": "HealthBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "GPT-4.1",
      "resolved_canonical": "GPT-4.1",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "LLM-as-a-Judge",
      "resolved_canonical": "LLM-as-a-Judge",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "contextual gaps",
      "resolved_canonical": "Contextual Gaps",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17444.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17444](https://arxiv.org/abs/2509.17444)

## 🔗 유사한 논문
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (84.2% similar)
- [[2025-09-23/RephQA_ Evaluating Readability of Large Language Models in Public Health Question Answering_20250923|RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering]] (83.0% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (82.2% similar)
- [[2025-09-19/Ticket-Bench_ A Kickoff for Multilingual and Regionalized Agent Evaluation_20250919|Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation]] (81.8% similar)
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/GPT-4.1|GPT-4.1]], [[keywords/Contextual Gaps|Contextual Gaps]]
**⚡ Unique Technical**: [[keywords/HealthBench|HealthBench]], [[keywords/LLM-as-a-Judge|LLM-as-a-Judge]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17444v1 Announce Type: new 
Abstract: This study investigates the applicability of HealthBench, a large-scale, rubric-based medical benchmark, to the Japanese context. While robust evaluation frameworks are crucial for the safe development of medical LLMs, resources in Japanese remain limited, often relying on translated multiple-choice questions. Our research addresses this gap by first establishing a performance baseline, applying a machine-translated version of HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second, we employ an LLM-as-a-Judge approach to systematically classify the benchmark's scenarios and rubric criteria, identifying "contextual gaps" where content is misaligned with Japan's clinical guidelines, healthcare systems, or cultural norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric mismatches and a significant failure in the Japanese-native model, which lacked the required clinical completeness. Furthermore, our classification indicates that while the majority of scenarios are applicable, a substantial portion of the rubric criteria requires localization. This work underscores the limitations of direct benchmark translation and highlights the urgent need for a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable and safe evaluation of medical LLMs in Japan.

## 📝 요약

이 연구는 대규모 의료 벤치마크인 HealthBench를 일본에 적용할 수 있는지를 조사합니다. 일본어 자원은 제한적이며 주로 번역된 선택형 질문에 의존하는 문제를 해결하기 위해, 먼저 HealthBench의 5,000개 시나리오를 기계 번역하여 다국어 모델(GPT-4.1)과 일본어 모델(LLM-jp-3.1)의 성능을 평가했습니다. 또한, LLM-as-a-Judge 접근법을 사용하여 시나리오와 기준을 분류하고, 일본의 임상 지침 및 문화와 맞지 않는 "맥락적 차이"를 식별했습니다. 결과적으로 GPT-4.1의 성능이 약간 저하되었고, 일본어 모델은 임상 완전성이 부족하여 성능이 크게 떨어졌습니다. 많은 시나리오가 적용 가능하지만, 상당 부분의 기준은 현지화가 필요함을 발견했습니다. 이는 직접 번역의 한계를 강조하며, 일본에 적합한 J-HealthBench 개발의 필요성을 제기합니다.

## 🎯 주요 포인트

- 1. HealthBench의 일본 적용 가능성을 조사하여 일본어 의료 LLM 평가의 한계를 극복하고자 합니다.
- 2. GPT-4.1과 일본어 모델 LLM-jp-3.1을 평가하여 성능 기준을 설정했습니다.
- 3. LLM-as-a-Judge 접근법을 통해 일본의 임상 지침 및 문화적 규범과의 불일치를 식별했습니다.
- 4. GPT-4.1은 루브릭 불일치로 성능 저하를 보였고, 일본어 모델은 임상 완전성 부족으로 실패했습니다.
- 5. 직접 번역된 벤치마크의 한계를 지적하며, 일본에 맞춘 J-HealthBench의 필요성을 강조합니다.


---

*Generated on 2025-09-24 03:27:00*