---
keywords:
  - Large Language Model
  - Feedback Friction
  - Semantic Entropy
  - Progressive Temperature Increases
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2506.11930
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:05:00.570421",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Feedback Friction",
    "Semantic Entropy",
    "Progressive Temperature Increases"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Feedback Friction": 0.78,
    "Semantic Entropy": 0.72,
    "Progressive Temperature Increases": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's investigation and connects to broader discussions on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Feedback Friction",
        "canonical": "Feedback Friction",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specific to the paper's findings on LLMs' resistance to feedback.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Semantic Entropy",
        "canonical": "Semantic Entropy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key metric used in the paper to analyze feedback resistance, offering a unique angle for linking.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Progressive Temperature Increases",
        "canonical": "Progressive Temperature Increases",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific strategy explored in the paper to mitigate feedback friction, relevant for technical linking.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "external feedback",
      "correct solutions",
      "target performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Feedback Friction",
      "resolved_canonical": "Feedback Friction",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Semantic Entropy",
      "resolved_canonical": "Semantic Entropy",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Progressive Temperature Increases",
      "resolved_canonical": "Progressive Temperature Increases",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2506.11930.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2506.11930](https://arxiv.org/abs/2506.11930)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (86.6% similar)
- [[2025-09-23/Adaptive Distraction_ Probing LLM Contextual Robustness with Automated Tree Search_20250923|Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search]] (85.1% similar)
- [[2025-09-23/Neither Stochastic Parroting nor AGI_ LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors_20250923|Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors]] (84.8% similar)
- [[2025-09-23/Challenging the Evaluator_ LLM Sycophancy Under User Rebuttal_20250923|Challenging the Evaluator: LLM Sycophancy Under User Rebuttal]] (84.8% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (84.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Feedback Friction|Feedback Friction]], [[keywords/Semantic Entropy|Semantic Entropy]], [[keywords/Progressive Temperature Increases|Progressive Temperature Increases]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.11930v2 Announce Type: replace 
Abstract: Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and reach correct solutions. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 with extended thinking. Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term Feedback Friction. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We analyze Feedback Friction and find that models' confidence on specific questions, measured by semantic entropy, predicts feedback resistance: high-confidence predictions remain resistant to external correction. We hope that highlighting this issue in LLMs will help future research in self-improvement.

## ğŸ“ ìš”ì•½

ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì™¸ë¶€ í”¼ë“œë°±ì„ í†µí•´ ì‘ë‹µì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì¼ë¶€ ê°€ì§€ê³  ìˆì§€ë§Œ, ì´ë¥¼ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ë¶ˆëª…í™•í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” LLMsê°€ í”¼ë“œë°±ì„ í†µí•©í•˜ëŠ” ëŠ¥ë ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ í†µì œëœ ì‹¤í—˜ í™˜ê²½ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œ í•´ê²° ëª¨ë¸ì´ ì†”ë£¨ì…˜ì„ ì‹œë„í•˜ê³ , í”¼ë“œë°± ìƒì„±ê¸°ê°€ ê±°ì˜ ì™„ë²½í•œ ì •ë‹µì„ ë°”íƒ•ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•œ í›„, í•´ê²° ëª¨ë¸ì´ ë‹¤ì‹œ ì‹œë„í•˜ëŠ” ê³¼ì •ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ì´ìƒì ì¸ ì¡°ê±´ì—ì„œë„ ëª¨ë¸ë“¤ì´ í”¼ë“œë°±ì— ì €í•­í•˜ëŠ” 'í”¼ë“œë°± ë§ˆì°°' í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì ì§„ì ì¸ ì˜¨ë„ ì¦ê°€ì™€ ì´ì „ì˜ ì˜ëª»ëœ ë‹µë³€ì„ ëª…ì‹œì ìœ¼ë¡œ ê±°ë¶€í•˜ëŠ” ìƒ˜í”Œë§ ê¸°ë°˜ ì „ëµì„ ì‹œë„í–ˆìœ¼ë‚˜, ëª©í‘œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë°ëŠ” ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”¼ë“œë°± ë§ˆì°°ì€ ëª¨ë¸ì˜ íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ìì‹ ê°, ì¦‰ ì˜ë¯¸ë¡ ì  ì—”íŠ¸ë¡œí”¼ë¡œ ì¸¡ì •ëœ í”¼ë“œë°± ì €í•­ì„±ì„ ì˜ˆì¸¡í•˜ë©°, ë†’ì€ ìì‹ ê°ì˜ ì˜ˆì¸¡ì€ ì™¸ë¶€ ìˆ˜ì •ì— ì €í•­í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ ê°•ì¡°í•¨ìœ¼ë¡œì¨ LLMsì˜ ìê¸° ê°œì„ ì— ëŒ€í•œ í–¥í›„ ì—°êµ¬ì— ê¸°ì—¬í•˜ê³ ì í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMsëŠ” ì™¸ë¶€ í”¼ë“œë°±ì„ í†µí•´ ì‘ë‹µì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì–´ëŠ ì •ë„ ë³´ìœ í•˜ê³  ìˆì§€ë§Œ, ì´ë¥¼ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ë¶ˆë¶„ëª…í•˜ë‹¤.
- 2. ì—°êµ¬ì—ì„œëŠ” LLMsê°€ í”¼ë“œë°±ì„ í†µí•©í•˜ëŠ” ëŠ¥ë ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ ì œì–´ëœ ì‹¤í—˜ í™˜ê²½ì„ ì„¤ê³„í•˜ì˜€ë‹¤.
- 3. ì´ìƒì ì¸ ì¡°ê±´ì—ì„œë„ ì†”ë²„ ëª¨ë¸ì€ í”¼ë“œë°±ì— ëŒ€í•œ ì €í•­ì„±ì„ ë³´ì´ë©°, ì´ë¥¼ 'í”¼ë“œë°± ë§ˆì°°'ë¡œ ëª…ëª…í•˜ì˜€ë‹¤.
- 4. í”¼ë“œë°± ë§ˆì°°ì„ ì™„í™”í•˜ê¸° ìœ„í•´ ì ì§„ì ì¸ ì˜¨ë„ ì¦ê°€ ë° ì´ì „ì— ì‹œë„í•œ ì˜ëª»ëœ ë‹µë³€ì˜ ëª…ì‹œì  ê±°ë¶€ì™€ ê°™ì€ ìƒ˜í”Œë§ ê¸°ë°˜ ì „ëµì„ ì‹¤í—˜í•˜ì˜€ìœ¼ë‚˜ ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±ì—ëŠ” ì‹¤íŒ¨í•˜ì˜€ë‹¤.
- 5. ëª¨ë¸ì˜ íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ìì‹ ê°ì´ í”¼ë“œë°± ì €í•­ì„±ì„ ì˜ˆì¸¡í•˜ë©°, ë†’ì€ ìì‹ ê°ì„ ê°€ì§„ ì˜ˆì¸¡ì€ ì™¸ë¶€ ìˆ˜ì •ì— ì €í•­í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 04:05:00*