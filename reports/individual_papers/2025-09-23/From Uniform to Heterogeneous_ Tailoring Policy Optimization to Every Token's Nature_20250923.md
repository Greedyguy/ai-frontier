---
keywords:
  - Reinforcement Learning
  - Heterogeneous Adaptive Policy Optimization
  - Adaptive Temperature Sampling
  - Token Level Group Average
  - Asymmetric Adaptive Clipping
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16591
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:14:24.350444",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Heterogeneous Adaptive Policy Optimization",
    "Adaptive Temperature Sampling",
    "Token Level Group Average",
    "Asymmetric Adaptive Clipping"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.78,
    "Heterogeneous Adaptive Policy Optimization": 0.8,
    "Adaptive Temperature Sampling": 0.75,
    "Token Level Group Average": 0.72,
    "Asymmetric Adaptive Clipping": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational concept in optimizing reasoning processes in LLMs, providing strong connectivity to related works.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Heterogeneous Adaptive Policy Optimization",
        "canonical": "Heterogeneous Adaptive Policy Optimization",
        "aliases": [
          "HAPO"
        ],
        "category": "unique_technical",
        "rationale": "HAPO is a novel algorithm introduced in the paper, offering a unique approach to token-aware optimization.",
        "novelty_score": 0.92,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adaptive Temperature Sampling",
        "canonical": "Adaptive Temperature Sampling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is crucial for dynamically adjusting sampling strategies based on token entropy, enhancing exploration.",
        "novelty_score": 0.87,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Token Level Group Average",
        "canonical": "Token Level Group Average",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This method normalizes advantages at the token level, contributing to fine-grained control in optimization.",
        "novelty_score": 0.78,
        "connectivity_score": 0.5,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      },
      {
        "surface": "Asymmetric Adaptive Clipping",
        "canonical": "Asymmetric Adaptive Clipping",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This approach allows for targeted probability adjustments, crucial for managing token-level noise and exploration.",
        "novelty_score": 0.85,
        "connectivity_score": 0.52,
        "specificity_score": 0.83,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Heterogeneous Adaptive Policy Optimization",
      "resolved_canonical": "Heterogeneous Adaptive Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adaptive Temperature Sampling",
      "resolved_canonical": "Adaptive Temperature Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.87,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Token Level Group Average",
      "resolved_canonical": "Token Level Group Average",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.5,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Asymmetric Adaptive Clipping",
      "resolved_canonical": "Asymmetric Adaptive Clipping",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.52,
        "specificity": 0.83,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16591.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16591](https://arxiv.org/abs/2509.16591)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (85.0% similar)
- [[2025-09-23/Adaptive Overclocking_ Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals_20250923|Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals]] (84.0% similar)
- [[2025-09-23/Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization_20250923|Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization]] (83.4% similar)
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (83.2% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Heterogeneous Adaptive Policy Optimization|Heterogeneous Adaptive Policy Optimization]], [[keywords/Adaptive Temperature Sampling|Adaptive Temperature Sampling]], [[keywords/Token Level Group Average|Token Level Group Average]], [[keywords/Asymmetric Adaptive Clipping|Asymmetric Adaptive Clipping]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16591v1 Announce Type: new 
Abstract: Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO.

## ğŸ“ ìš”ì•½

ê°•í™” í•™ìŠµì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í•µì‹¬ ê¸°ìˆ ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë“  í† í°ì— ë™ì¼í•œ ìµœì í™”ë¥¼ ì ìš©í•˜ì—¬, ì¶”ë¡  ê³¼ì •ì—ì„œì˜ í† í° ì—­í•  ì°¨ì´ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Heterogeneous Adaptive Policy Optimization (HAPO)ì„ ì œì•ˆí•©ë‹ˆë‹¤. HAPOëŠ” í† í° ì—”íŠ¸ë¡œí”¼ì— ê¸°ë°˜í•˜ì—¬ ìµœì í™”ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. Adaptive Temperature Samplingì„ í†µí•´ ë¡¤ì•„ì›ƒ ìƒ˜í”Œë§ ì‹œ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒ˜í”Œë§ ì˜¨ë„ë¥¼ ì¡°ì ˆí•˜ì—¬, ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì€ í† í°ì—ì„œëŠ” íƒìƒ‰ì„ ì´‰ì§„í•˜ê³ , ë‚®ì€ í† í°ì—ì„œëŠ” ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, Token Level Group Averageë¥¼ ë„ì…í•˜ì—¬ í† í° ìˆ˜ì¤€ì—ì„œ ì´ì ì„ ì •ê·œí™”í•˜ë©°, Differential Advantage Redistributionì„ í†µí•´ ëª…í™•í•œ ì‹ í˜¸ë¥¼ ê°€ì§„ í† í°ì˜ ë³´ìƒ ì¡°ì •ì„ ìµœì í™”í•©ë‹ˆë‹¤. Asymmetric Adaptive Clippingì€ ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì€ í† í°ì— ëŒ€í•´ ê³µê²©ì ì¸ í™•ë¥  ê°ì†Œë¥¼ í—ˆìš©í•˜ê³ , ë†’ì€ í† í°ì—ì„œëŠ” íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, HAPOëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ ê·œëª¨ì—ì„œ DAPOë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. HAPOëŠ” í† í°ì˜ ì—”íŠ¸ë¡œí”¼ì— ê¸°ë°˜í•˜ì—¬ ìµœì í™”ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” í† í° ì¸ì‹ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 2. Adaptive Temperature Samplingì€ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì¤€ì— ë”°ë¼ ìƒ˜í”Œë§ ì˜¨ë„ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ íƒìƒ‰ê³¼ ì¼ê´€ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
- 3. Token Level Group AverageëŠ” í† í° ìˆ˜ì¤€ì—ì„œì˜ ì´ì ì„ ì •ê·œí™”í•˜ì—¬ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ë©´ì„œë„ í¸í–¥ ì—†ëŠ” ì²˜ë¦¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- 4. Differential Advantage Redistributionì€ ì—”íŠ¸ë¡œí”¼ì™€ ì¤‘ìš”ë„ ë¹„ìœ¨ì„ í™œìš©í•˜ì—¬ ëª…í™•í•œ ì‹ í˜¸ë¥¼ ê°€ì§„ í† í°ì˜ ë³´ìƒ ì¡°ì •ì„ ìµœì í™”í•©ë‹ˆë‹¤.
- 5. Asymmetric Adaptive Clippingì€ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì¤€ì— ë”°ë¼ í™•ë¥  ê°ì†Œì™€ íƒìƒ‰ì„ ì¡°ì ˆí•˜ì—¬ í›ˆë ¨ ë™ì ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:14:24*