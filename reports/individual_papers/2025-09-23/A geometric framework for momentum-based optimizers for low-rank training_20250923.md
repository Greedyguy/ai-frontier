---
keywords:
  - Low-Rank Training
  - Momentum-Based Optimization
  - Dynamical Low-Rank Approximation
  - Optimization Landscape
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.17475
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:46:39.996438",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Low-Rank Training",
    "Momentum-Based Optimization",
    "Dynamical Low-Rank Approximation",
    "Optimization Landscape"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Low-Rank Training": 0.78,
    "Momentum-Based Optimization": 0.8,
    "Dynamical Low-Rank Approximation": 0.75,
    "Optimization Landscape": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "low-rank pre-training",
        "canonical": "Low-Rank Training",
        "aliases": [
          "low-rank parameterization",
          "low-rank fine-tuning"
        ],
        "category": "unique_technical",
        "rationale": "Low-rank training is a specialized optimization technique that can be linked to discussions on efficient neural network training.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "momentum-based optimizers",
        "canonical": "Momentum-Based Optimization",
        "aliases": [
          "momentum methods",
          "heavy ball momentum"
        ],
        "category": "specific_connectable",
        "rationale": "Momentum-based optimization is a key concept in training neural networks and can connect to broader discussions on optimization techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "dynamical low-rank approximation",
        "canonical": "Dynamical Low-Rank Approximation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel approach that combines geometry and optimization, offering a unique perspective on training strategies.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "optimization landscape",
        "canonical": "Optimization Landscape",
        "aliases": [
          "optimization geometry"
        ],
        "category": "broad_technical",
        "rationale": "Understanding the optimization landscape is crucial for designing effective training strategies in machine learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "training methods",
      "numerical experiments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "low-rank pre-training",
      "resolved_canonical": "Low-Rank Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "momentum-based optimizers",
      "resolved_canonical": "Momentum-Based Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "dynamical low-rank approximation",
      "resolved_canonical": "Dynamical Low-Rank Approximation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "optimization landscape",
      "resolved_canonical": "Optimization Landscape",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# A geometric framework for momentum-based optimizers for low-rank training

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.17475.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.17475](https://arxiv.org/abs/2506.17475)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks_20250923|Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks]] (84.0% similar)
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (83.6% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (83.3% similar)
- [[2025-09-18/Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (82.9% similar)
- [[2025-09-23/Were Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?_20250923|Were Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?]] (82.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Optimization Landscape|Optimization Landscape]]
**ğŸ”— Specific Connectable**: [[keywords/Momentum-Based Optimization|Momentum-Based Optimization]]
**âš¡ Unique Technical**: [[keywords/Low-Rank Training|Low-Rank Training]], [[keywords/Dynamical Low-Rank Approximation|Dynamical Low-Rank Approximation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.17475v2 Announce Type: replace 
Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising techniques for reducing the computational and storage costs of large neural networks. Training low-rank parameterizations typically relies on conventional optimizers such as heavy ball momentum methods or Adam. In this work, we identify and analyze potential difficulties that these training methods encounter when used to train low-rank parameterizations of weights. In particular, we show that classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape. To address this, we introduce novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space. We validate our methods through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì‹ ê²½ë§ì˜ ê³„ì‚° ë° ì €ì¥ ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•œ ì €ë­í¬ ì‚¬ì „ í›ˆë ¨ ë° ë¯¸ì„¸ ì¡°ì • ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ëª¨ë©˜í…€ ë°©ë²•ì´ë‚˜ Adamê³¼ ê°™ì€ ìµœì í™” ê¸°ë²•ì´ ì €ë­í¬ ë§¤ê°œë³€ìˆ˜í™” í›ˆë ¨ ì‹œ ê²ªëŠ” ë¬¸ì œì ì„ ë¶„ì„í•˜ê³ , ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì´ ìµœì í™” ì§€í˜•ì˜ ê¸°í•˜í•™ì  êµ¬ì¡°ë¡œ ì¸í•´ ì§€ì—­ ìµœì ì ì— ìˆ˜ë ´í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ìƒˆë¡œìš´ í›ˆë ¨ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì „ëµì€ ë™ì  ì €ë­í¬ ê·¼ì‚¬ì™€ ëª¨ë©˜í…€ ê¸°ë°˜ ìµœì í™”ë¥¼ ê²°í•©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì˜ ë³¸ì§ˆì  ê¸°í•˜í•™ì„ ì¡´ì¤‘í•˜ëŠ” ìµœì í™” ê¸°ë²•ì„ ì„¤ê³„í•©ë‹ˆë‹¤. ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì´ ë” ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ì™€ í–¥ìƒëœ ê²€ì¦ ì§€í‘œë¥¼ ë‹¬ì„±í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì €ì°¨ì› ì‚¬ì „ í›ˆë ¨ê³¼ ë¯¸ì„¸ ì¡°ì •ì€ ëŒ€í˜• ì‹ ê²½ë§ì˜ ê³„ì‚° ë° ì €ì¥ ë¹„ìš©ì„ ì¤„ì´ëŠ” ìœ ë§í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ ëª¨ë©˜í…€ ë°©ë²•ì€ ìµœì í™” ì§€í˜•ì˜ ê¸°í•˜í•™ì  êµ¬ì¡° ë•Œë¬¸ì— ì§€ì—­ ìµœì ì ì— ìˆ˜ë ´í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê³ ë ¤í•œ ìƒˆë¡œìš´ í›ˆë ¨ ì „ëµì„ ë„ì…í•˜ì˜€ë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ë™ì  ì €ì°¨ì› ê·¼ì‚¬ì™€ ëª¨ë©˜í…€ ê¸°ë°˜ ìµœì í™”ë¥¼ ê²°í•©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì˜ ë‚´ì¬ëœ ê¸°í•˜í•™ì„ ì¡´ì¤‘í•˜ëŠ” ìµœì í™” ë„êµ¬ë¥¼ ì„¤ê³„í•œë‹¤.
- 5. ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì´ ë” ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ì™€ ê°•ë ¥í•œ ê²€ì¦ ì§€í‘œë¥¼ ë‹¬ì„±í•¨ì„ ì…ì¦í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 02:46:39*