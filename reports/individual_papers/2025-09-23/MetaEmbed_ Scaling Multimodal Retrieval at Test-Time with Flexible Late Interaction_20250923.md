---
keywords:
  - MetaEmbed
  - Multimodal Learning
  - Matryoshka Multi-Vector Retrieval
  - Vision-Language Model
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.18095
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:42:13.359491",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "MetaEmbed",
    "Multimodal Learning",
    "Matryoshka Multi-Vector Retrieval",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "MetaEmbed": 0.78,
    "Multimodal Learning": 0.82,
    "Matryoshka Multi-Vector Retrieval": 0.77,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "MetaEmbed",
        "canonical": "MetaEmbed",
        "aliases": [
          "MetaEmbed Framework"
        ],
        "category": "unique_technical",
        "rationale": "MetaEmbed is a novel framework introduced in the paper, crucial for understanding the proposed approach to multimodal retrieval.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "multimodal retrieval",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal retrieval"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal retrieval is a key application of multimodal learning, linking to broader discussions in the field.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Matryoshka Multi-Vector Retrieval",
        "canonical": "Matryoshka Multi-Vector Retrieval",
        "aliases": [
          "Matryoshka Retrieval"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific training method introduced in the paper, important for understanding the framework's operation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are relevant to the paper's context, linking to current trends in multimodal research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "universal multimodal embedding models",
      "semantic relevance",
      "fine-grained information"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "MetaEmbed",
      "resolved_canonical": "MetaEmbed",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multimodal retrieval",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Matryoshka Multi-Vector Retrieval",
      "resolved_canonical": "Matryoshka Multi-Vector Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18095.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.18095](https://arxiv.org/abs/2509.18095)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (82.9% similar)
- [[2025-09-22/Causal2Vec_ Improving Decoder-only LLMs as Versatile Embedding Models_20250922|Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models]] (82.2% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (82.1% similar)
- [[2025-09-23/LightRetriever_ A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference_20250923|LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference]] (82.1% similar)
- [[2025-09-22/MMAPG_ A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs_20250922|MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/MetaEmbed|MetaEmbed]], [[keywords/Matryoshka Multi-Vector Retrieval|Matryoshka Multi-Vector Retrieval]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18095v1 Announce Type: cross 
Abstract: Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters.

## ğŸ“ ìš”ì•½

MetaEmbedëŠ” ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ì„ ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ê¸°ì¡´ì˜ ë‹¨ì¼ ë²¡í„° ë˜ëŠ” ê³¼ë„í•œ ë‹¤ì¤‘ ë²¡í„° ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤. í•™ìŠµ ì‹œ ì…ë ¥ ì‹œí€€ìŠ¤ì— ê³ ì •ëœ ìˆ˜ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë©”íƒ€ í† í°ì„ ì¶”ê°€í•˜ê³ , í…ŒìŠ¤íŠ¸ ì‹œ ì´ë“¤ì˜ ë§ˆì§€ë§‰ ì¸µ í‘œí˜„ì„ ë‹¤ì¤‘ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •ë³´ì˜ ì„¸ë¶„í™”ëœ ì¡°ì§í™”ë¥¼ í•™ìŠµí•˜ë©°, ê²€ìƒ‰ í’ˆì§ˆê³¼ íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì„ ì‚¬ìš©ìê°€ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬(MMEB)ì™€ ë¹„ì£¼ì–¼ ë¬¸ì„œ ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬(ViDoRe)ì—ì„œì˜ í‰ê°€ ê²°ê³¼, MetaEmbedëŠ” 32B íŒŒë¼ë¯¸í„° ëª¨ë¸ì—ì„œë„ ìµœì²¨ë‹¨ ê²€ìƒ‰ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MetaEmbedëŠ” ë‹¤ì¤‘ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì˜ êµ¬ì„± ë° ìƒí˜¸ì‘ìš© ë°©ì‹ì„ ì¬ê³ í•©ë‹ˆë‹¤.
- 2. í•™ìŠµ ì‹œ, ê³ ì •ëœ ìˆ˜ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë©”íƒ€ í† í°ì„ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ë§ˆì§€ë§‰ ì¸µì˜ ë¬¸ë§¥í™”ëœ í‘œí˜„ì´ ì••ì¶•ì ì´ë©´ì„œë„ í‘œí˜„ë ¥ ìˆëŠ” ë‹¤ì¤‘ ë²¡í„° ì„ë² ë”©ì„ ì œê³µí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ Matryoshka Multi-Vector Retrieval í•™ìŠµì„ í†µí•´, MetaEmbedëŠ” ì—¬ëŸ¬ ë²¡í„°ì— ê±¸ì³ ì •ë³´ì˜ ì„¸ë¶„í™”ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
- 4. ì‚¬ìš©ìëŠ” ì¸ë±ì‹± ë° ê²€ìƒ‰ ìƒí˜¸ì‘ìš©ì— ì‚¬ìš©ë˜ëŠ” í† í° ìˆ˜ë¥¼ ì„ íƒí•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆê³¼ íš¨ìœ¨ì„± ìš”êµ¬ ì‚¬í•­ ê°„ì˜ ê· í˜•ì„ ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬(MMEB)ì™€ ì‹œê°ì  ë¬¸ì„œ ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬(ViDoRe)ì—ì„œì˜ ê´‘ë²”ìœ„í•œ í‰ê°€ ê²°ê³¼, MetaEmbedëŠ” 32B íŒŒë¼ë¯¸í„° ëª¨ë¸ì—ì„œë„ ê²¬ê³ í•˜ê²Œ í™•ì¥ë˜ë©´ì„œ ìµœì²¨ë‹¨ ê²€ìƒ‰ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:42:13*