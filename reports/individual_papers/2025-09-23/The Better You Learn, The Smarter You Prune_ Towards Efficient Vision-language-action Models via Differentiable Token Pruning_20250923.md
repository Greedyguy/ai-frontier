---
keywords:
  - Vision-Language Model
  - Differentiable Token Pruning
  - Attention Mechanism
  - Gumbel Softmax
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.12594
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:18:20.256022",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Differentiable Token Pruning",
    "Attention Mechanism",
    "Gumbel Softmax"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Differentiable Token Pruning": 0.8,
    "Attention Mechanism": 0.82,
    "Gumbel Softmax": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-language-action models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLA models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language-action models represent an evolution of multimodal learning, crucial for linking to broader vision-language research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Differentiable token pruning",
        "canonical": "Differentiable Token Pruning",
        "aliases": [
          "token pruning"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel technique specific to the paper, enhancing connectivity in token management research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Attention-based computation",
        "canonical": "Attention Mechanism",
        "aliases": [
          "attention computation"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are central to the discussed models, facilitating connections to related neural network research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Gumbel softmax",
        "canonical": "Gumbel Softmax",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Gumbel softmax is a specific technique used in differentiable selection, linking to probabilistic model research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "LightVLA",
      "LIBERO benchmark"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-language-action models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Differentiable token pruning",
      "resolved_canonical": "Differentiable Token Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Attention-based computation",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Gumbel softmax",
      "resolved_canonical": "Gumbel Softmax",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.12594.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.12594](https://arxiv.org/abs/2509.12594)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (86.7% similar)
- [[2025-09-22/Walk and Read Less_ Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning_20250922|Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning]] (85.9% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (85.7% similar)
- [[2025-09-19/ForceVLA_ Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation_20250919|ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (85.0% similar)
- [[2025-09-19/CollabVLA_ Self-Reflective Vision-Language-Action Model Dreaming Together with Human_20250919|CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Gumbel Softmax|Gumbel Softmax]]
**âš¡ Unique Technical**: [[keywords/Differentiable Token Pruning|Differentiable Token Pruning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.12594v2 Announce Type: replace-cross 
Abstract: We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.

## ğŸ“ ìš”ì•½

LightVLAëŠ” ë¹„ì „-ì–¸ì–´-í–‰ë™(VLA) ëª¨ë¸ì„ ìœ„í•œ ì°¨ë³„í™”ëœ í† í° ê°€ì§€ì¹˜ê¸° í”„ë ˆì„ì›Œí¬ë¡œ, ë¦¬ì†ŒìŠ¤ê°€ ì œí•œëœ í”Œë«í¼ì—ì„œ VLA ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì‹œê° í† í°ì˜ ì¤‘ìš”ì„±ì„ ë™ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , Gumbel softmaxë¥¼ ì‚¬ìš©í•´ ì°¨ë³„í™”ëœ í† í° ì„ íƒì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¶ˆí•„ìš”í•œ í† í°ì„ ê°€ì§€ì¹˜ê¸°í•˜ë©´ì„œë„ ì¤‘ìš”í•œ ì •ë³´ëŠ” ë³´ì¡´í•˜ì—¬ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ë™ì‹œì— ê°œì„ í•©ë‹ˆë‹¤. LightVLAëŠ” ì¶”ê°€ì ì¸ í•™ìŠµ ë§¤ê°œë³€ìˆ˜ ì—†ì´ í˜„ëŒ€ì  ì¶”ë¡  í”„ë ˆì„ì›Œí¬ì™€ í˜¸í™˜ë˜ë©°, LIBERO ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ê³¼ ë‚®ì€ ê³„ì‚° ë¹„ìš©ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. FLOPsì™€ ì§€ì—° ì‹œê°„ì„ ê°ê° 59.1%ì™€ 38.2% ì¤„ì´ë©´ì„œë„ ì‘ì—… ì„±ê³µë¥ ì„ 2.6% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” VLA ì‘ì—…ì— ì ì‘í˜• ì‹œê° í† í° ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•œ ìµœì´ˆì˜ ì‚¬ë¡€ë¡œ, ì‹¤ì‹œê°„ ë¡œë´‡ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LightVLAëŠ” VLA ëª¨ë¸ì˜ ì‹œê°ì  í† í°ì„ ì„±ëŠ¥ ê¸°ë°˜ìœ¼ë¡œ ì ì‘ì ìœ¼ë¡œ ê°€ì§€ì¹˜ê¸°í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” Gumbel softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ë³„ ê°€ëŠ¥í•œ í† í° ì„ íƒì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì¶”ê°€ì ì¸ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë„ì…í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, LightVLAëŠ” LIBERO ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ì–‘í•œ ì‘ì—…ì— ëŒ€í•´ ê¸°ì¡´ VLA ëª¨ë¸ê³¼ í† í° ê°€ì§€ì¹˜ê¸° ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©°, ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ í¬ê²Œ ì¤„ì´ë©´ì„œ ì„±ê³µë¥ ì„ ë†’ì…ë‹ˆë‹¤.
- 4. LightVLAëŠ” FLOPsì™€ ì§€ì—° ì‹œê°„ì„ ê°ê° 59.1%ì™€ 38.2% ì¤„ì´ë©´ì„œ ì‘ì—… ì„±ê³µë¥ ì„ 2.6% í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. LightVLAëŠ” VLA ì‘ì—…ì— ì ì‘í˜• ì‹œê°ì  í† í° ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•œ ìµœì´ˆì˜ ì—°êµ¬ë¡œ, ì‹¤ì‹œê°„ ë¡œë´‡ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ì§„ì „ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:18:20*