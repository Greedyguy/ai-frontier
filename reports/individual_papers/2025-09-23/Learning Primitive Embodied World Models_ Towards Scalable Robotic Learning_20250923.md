---
keywords:
  - Primitive Embodied World Models
  - Vision-Language Model
  - Start-Goal heatmap Guidance
  - Embodied Intelligence
  - Compositional Generalization
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.20840
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:23:36.531419",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Primitive Embodied World Models",
    "Vision-Language Model",
    "Start-Goal heatmap Guidance",
    "Embodied Intelligence",
    "Compositional Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Primitive Embodied World Models": 0.78,
    "Vision-Language Model": 0.85,
    "Start-Goal heatmap Guidance": 0.72,
    "Embodied Intelligence": 0.8,
    "Compositional Generalization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Primitive Embodied World Models",
        "canonical": "Primitive Embodied World Models",
        "aliases": [
          "PEWM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel paradigm for scalable robotic learning, enhancing the paper's unique contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Connects to recent advances in integrating visual and linguistic data, crucial for embodied intelligence.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Start-Goal heatmap Guidance",
        "canonical": "Start-Goal heatmap Guidance",
        "aliases": [
          "SGG"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific mechanism within the proposed framework, highlighting its innovative approach.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Embodied Intelligence",
        "canonical": "Embodied Intelligence",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Central theme of the paper, linking to the broader field of robotics and AI.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Compositional Generalization",
        "canonical": "Compositional Generalization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key concept for extending primitive-level policies to complex tasks, enhancing connectivity with learning theories.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "video generation",
      "embodied data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Primitive Embodied World Models",
      "resolved_canonical": "Primitive Embodied World Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Start-Goal heatmap Guidance",
      "resolved_canonical": "Start-Goal heatmap Guidance",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Embodied Intelligence",
      "resolved_canonical": "Embodied Intelligence",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Compositional Generalization",
      "resolved_canonical": "Compositional Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Learning Primitive Embodied World Models: Towards Scalable Robotic Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.20840.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.20840](https://arxiv.org/abs/2508.20840)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (85.0% similar)
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (84.8% similar)
- [[2025-09-23/Latent Policy Steering with Embodiment-Agnostic Pretrained World Models_20250923|Latent Policy Steering with Embodiment-Agnostic Pretrained World Models]] (83.8% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (83.6% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Embodied Intelligence|Embodied Intelligence]]
**ğŸ”— Specific Connectable**: [[keywords/Compositional Generalization|Compositional Generalization]]
**âš¡ Unique Technical**: [[keywords/Primitive Embodied World Models|Primitive Embodied World Models]], [[keywords/Start-Goal heatmap Guidance|Start-Goal heatmap Guidance]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.20840v2 Announce Type: replace-cross 
Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ë””ì˜¤ ìƒì„± ê¸°ë°˜ì˜ ì„¸ê³„ ëª¨ë¸ì´ ëŒ€ê·œëª¨ ìƒí˜¸ì‘ìš© ë°ì´í„°ì— ì˜ì¡´í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 'Primitive Embodied World Models (PEWM)'ë¼ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤. PEWMì€ ì§§ì€ ì‹œê°„ ë²”ìœ„ë¡œ ë¹„ë””ì˜¤ ìƒì„±ì„ ì œí•œí•˜ì—¬ ì–¸ì–´ì™€ ë¡œë´‡ í–‰ë™ ê°„ì˜ ì„¸ë°€í•œ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , í•™ìŠµ ë³µì¡ì„±ì„ ì¤„ì´ë©°, ë°ì´í„° ìˆ˜ì§‘ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì¶”ë¡  ì§€ì—°ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ëª¨ë“ˆì‹ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ í”Œë˜ë„ˆì™€ ì‹œì‘-ëª©í‘œ íˆíŠ¸ë§µ ê°€ì´ë“œ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ìœ ì—°í•œ íì‡„ ë£¨í”„ ì œì–´ì™€ ë³µí•©ì  ê³¼ì œì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë¹„ë””ì˜¤ ëª¨ë¸ì˜ ì‹œê³µê°„ ë¹„ì „ í”„ë¼ì´ì–´ì™€ VLMì˜ ì˜ë¯¸ ì¸ì‹ì„ í™œìš©í•˜ì—¬ ë¬¼ë¦¬ì  ìƒí˜¸ì‘ìš©ê³¼ ê³ ì°¨ì› ì¶”ë¡  ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ìƒí˜¸ì‘ìš© ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì´ ë¹„ë””ì˜¤ ìƒì„± ê¸°ë°˜ ì„¸ê³„ ëª¨ë¸ì˜ ì£¼ìš” í•œê³„ì ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. Primitive Embodied World Models (PEWM)ì€ ê³ ì •ëœ ì§§ì€ ì‹œê°„ ë²”ìœ„ ë‚´ì—ì„œ ë¹„ë””ì˜¤ ìƒì„±ì„ ì œí•œí•˜ì—¬ ì–¸ì–´ ê°œë…ê³¼ ë¡œë´‡ í–‰ë™ì˜ ì‹œê°ì  í‘œí˜„ ê°„ì˜ ì„¸ë°€í•œ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. PEWMì€ í•™ìŠµ ë³µì¡ì„±ì„ ì¤„ì´ê³ , ë°ì´í„° ìˆ˜ì§‘ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ë©°, ì¶”ë¡  ì§€ì—° ì‹œê°„ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤.
- 4. ëª¨ë“ˆì‹ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM) ê³„íšìì™€ Start-Goal íˆíŠ¸ë§µ ê°€ì´ë“œ ë©”ì»¤ë‹ˆì¦˜(SGG)ì„ í†µí•´ ìœ ì—°í•œ íì‡„ ë£¨í”„ ì œì–´ì™€ ë³µí•©ì ì¸ ì¼ë°˜í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
- 5. PEWMì€ ë¹„ë””ì˜¤ ëª¨ë¸ì˜ ì‹œê³µê°„ ë¹„ì „ ìš°ì„ ìˆœìœ„ì™€ VLMì˜ ì˜ë¯¸ ì¸ì‹ì„ í™œìš©í•˜ì—¬ ì„¸ë°€í•œ ë¬¼ë¦¬ì  ìƒí˜¸ì‘ìš©ê³¼ ê³ ì°¨ì›ì  ì¶”ë¡  ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:23:36*