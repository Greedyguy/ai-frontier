---
keywords:
  - Primitive Embodied World Models
  - Vision-Language Model
  - Start-Goal heatmap Guidance
  - Embodied Intelligence
  - Compositional Generalization
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.20840
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:23:36.531419",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Primitive Embodied World Models",
    "Vision-Language Model",
    "Start-Goal heatmap Guidance",
    "Embodied Intelligence",
    "Compositional Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Primitive Embodied World Models": 0.78,
    "Vision-Language Model": 0.85,
    "Start-Goal heatmap Guidance": 0.72,
    "Embodied Intelligence": 0.8,
    "Compositional Generalization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Primitive Embodied World Models",
        "canonical": "Primitive Embodied World Models",
        "aliases": [
          "PEWM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel paradigm for scalable robotic learning, enhancing the paper's unique contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Connects to recent advances in integrating visual and linguistic data, crucial for embodied intelligence.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Start-Goal heatmap Guidance",
        "canonical": "Start-Goal heatmap Guidance",
        "aliases": [
          "SGG"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific mechanism within the proposed framework, highlighting its innovative approach.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Embodied Intelligence",
        "canonical": "Embodied Intelligence",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Central theme of the paper, linking to the broader field of robotics and AI.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Compositional Generalization",
        "canonical": "Compositional Generalization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key concept for extending primitive-level policies to complex tasks, enhancing connectivity with learning theories.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "video generation",
      "embodied data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Primitive Embodied World Models",
      "resolved_canonical": "Primitive Embodied World Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Start-Goal heatmap Guidance",
      "resolved_canonical": "Start-Goal heatmap Guidance",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Embodied Intelligence",
      "resolved_canonical": "Embodied Intelligence",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Compositional Generalization",
      "resolved_canonical": "Compositional Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Learning Primitive Embodied World Models: Towards Scalable Robotic Learning

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.20840.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.20840](https://arxiv.org/abs/2508.20840)

## 🔗 유사한 논문
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (85.0% similar)
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (84.8% similar)
- [[2025-09-23/Latent Policy Steering with Embodiment-Agnostic Pretrained World Models_20250923|Latent Policy Steering with Embodiment-Agnostic Pretrained World Models]] (83.8% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (83.6% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (82.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Embodied Intelligence|Embodied Intelligence]]
**🔗 Specific Connectable**: [[keywords/Compositional Generalization|Compositional Generalization]]
**⚡ Unique Technical**: [[keywords/Primitive Embodied World Models|Primitive Embodied World Models]], [[keywords/Start-Goal heatmap Guidance|Start-Goal heatmap Guidance]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2508.20840v2 Announce Type: replace-cross 
Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

## 📝 요약

이 논문은 비디오 생성 기반의 세계 모델이 대규모 상호작용 데이터에 의존하는 문제를 해결하기 위해 'Primitive Embodied World Models (PEWM)'라는 새로운 패러다임을 제안합니다. PEWM은 짧은 시간 범위로 비디오 생성을 제한하여 언어와 로봇 행동 간의 세밀한 정렬을 가능하게 하고, 학습 복잡성을 줄이며, 데이터 수집 효율성을 높이고, 추론 지연을 감소시킵니다. 모듈식 비전-언어 모델 플래너와 시작-목표 히트맵 가이드 메커니즘을 통해 유연한 폐쇄 루프 제어와 복합적 과제에 대한 일반화를 지원합니다. 이 프레임워크는 비디오 모델의 시공간 비전 프라이어와 VLM의 의미 인식을 활용하여 물리적 상호작용과 고차원 추론 간의 격차를 줄입니다.

## 🎯 주요 포인트

- 1. 대규모 상호작용 데이터에 대한 의존성이 비디오 생성 기반 세계 모델의 주요 한계점으로 작용하고 있습니다.
- 2. Primitive Embodied World Models (PEWM)은 고정된 짧은 시간 범위 내에서 비디오 생성을 제한하여 언어 개념과 로봇 행동의 시각적 표현 간의 세밀한 정렬을 가능하게 합니다.
- 3. PEWM은 학습 복잡성을 줄이고, 데이터 수집의 효율성을 높이며, 추론 지연 시간을 감소시킵니다.
- 4. 모듈식 비전-언어 모델(VLM) 계획자와 Start-Goal 히트맵 가이드 메커니즘(SGG)을 통해 유연한 폐쇄 루프 제어와 복합적인 일반화를 지원합니다.
- 5. PEWM은 비디오 모델의 시공간 비전 우선순위와 VLM의 의미 인식을 활용하여 세밀한 물리적 상호작용과 고차원적 추론 간의 격차를 줄입니다.


---

*Generated on 2025-09-24 01:23:36*