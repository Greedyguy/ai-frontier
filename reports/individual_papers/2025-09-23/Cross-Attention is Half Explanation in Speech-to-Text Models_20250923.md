---
keywords:
  - Attention Mechanism
  - Speech-to-Text Models
  - Saliency Maps
  - Transformer
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.18010
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:16:57.998224",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "Speech-to-Text Models",
    "Saliency Maps",
    "Transformer"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.85,
    "Speech-to-Text Models": 0.78,
    "Saliency Maps": 0.82,
    "Transformer": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Cross-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross Attention",
          "Cross-Attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-attention is a specific type of attention mechanism, crucial for linking to broader discussions on attention in neural networks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Speech-to-Text Models",
        "canonical": "Speech-to-Text Models",
        "aliases": [
          "S2T Models",
          "Speech Recognition Models"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper and connects to specific discussions on speech processing technologies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Saliency Maps",
        "canonical": "Saliency Maps",
        "aliases": [
          "Input Saliency Maps",
          "Feature Attribution Maps"
        ],
        "category": "unique_technical",
        "rationale": "Saliency maps are key to understanding model interpretability, which is a focal point of the study.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.77,
        "link_intent_score": 0.82
      },
      {
        "surface": "Encoder-Decoder Architectures",
        "canonical": "Transformer",
        "aliases": [
          "Encoder-Decoder Models"
        ],
        "category": "broad_technical",
        "rationale": "Encoder-decoder architectures are foundational to many neural network models, including Transformers.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "downstream applications",
      "input relevance",
      "explanatory proxy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Cross-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Speech-to-Text Models",
      "resolved_canonical": "Speech-to-Text Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Saliency Maps",
      "resolved_canonical": "Saliency Maps",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.77,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Encoder-Decoder Architectures",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Cross-Attention is Half Explanation in Speech-to-Text Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18010.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.18010](https://arxiv.org/abs/2509.18010)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Shedding Light on Depth_ Explainability Assessment in Monocular Depth Estimation_20250922|Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation]] (79.7% similar)
- [[2025-09-23/Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models_20250923|Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models]] (79.4% similar)
- [[2025-09-22/Pruning the Paradox_ How CLIP's Most Informative Heads Enhance Performance While Amplifying Bias_20250922|Pruning the Paradox: How CLIP's Most Informative Heads Enhance Performance While Amplifying Bias]] (79.4% similar)
- [[2025-09-18/Stochastic Clock Attention for Aligning Continuous and Ordered Sequences_20250918|Stochastic Clock Attention for Aligning Continuous and Ordered Sequences]] (79.2% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (78.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Speech-to-Text Models|Speech-to-Text Models]], [[keywords/Saliency Maps|Saliency Maps]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18010v1 Announce Type: cross 
Abstract: Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications--such as timestamp estimation and audio-text alignment--under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder's representations--accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìŒì„±-í…ìŠ¤íŠ¸(S2T) ëª¨ë¸ì—ì„œì˜ êµì°¨ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì„¤ëª…ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. êµì°¨ ì£¼ì˜ ì ìˆ˜ê°€ ì…ë ¥ ìŒì„± í‘œí˜„ê³¼ ìƒì„±ëœ í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ì¡´ì„±ì„ ë°˜ì˜í•œë‹¤ê³  ê°€ì •í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ìš©ì— ì‚¬ìš©ë˜ì–´ ì™”ì§€ë§Œ, ìŒì„± ë¶„ì•¼ì—ì„œëŠ” ì´ ê°€ì •ì´ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì €ìë“¤ì€ êµì°¨ ì£¼ì˜ ì ìˆ˜ì™€ ì…ë ¥ ì¤‘ìš”ë„ ë§µì„ ë¹„êµí•˜ì—¬ ì´ ê°€ì •ì„ ê²€í† í•˜ì˜€ìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, ì£¼ì˜ ì ìˆ˜ëŠ” ì¤‘ìš”ë„ ê¸°ë°˜ ì„¤ëª…ê³¼ ì¤‘ê°„ì—ì„œ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ì…ë ¥ ê´€ë ¨ì„±ì„ ì•½ 50%ë§Œ í¬ì°©í•˜ë©°, ë””ì½”ë”ê°€ ì¸ì½”ë” í‘œí˜„ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ë°©ì‹ì„ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” S2T ëª¨ë¸ì˜ ì˜ˆì¸¡ ìš”ì¸ì„ ì„¤ëª…í•˜ëŠ” ë° ìˆì–´ êµì°¨ ì£¼ì˜ê°€ ë¶ˆì™„ì „í•œ ì •ë³´ë¥¼ ì œê³µí•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Cross-attention ë©”ì»¤ë‹ˆì¦˜ì€ ìŒì„±-í…ìŠ¤íŠ¸(S2T) ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•˜ë©°, ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ìš© í”„ë¡œê·¸ë¨ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. S2T ëª¨ë¸ì—ì„œì˜ cross-attention ì ìˆ˜ëŠ” ì…ë ¥ ìŒì„± í‘œí˜„ê³¼ ìƒì„±ëœ í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ì¡´ì„±ì„ ë°˜ì˜í•œë‹¤ê³  ê°€ì •ë˜ì§€ë§Œ, ì´ ê°€ì •ì€ ìŒì„± ë¶„ì•¼ì—ì„œ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼, cross-attention ì ìˆ˜ëŠ” saliency ê¸°ë°˜ ì„¤ëª…ê³¼ ì¤‘ê°„ì—ì„œ ê°•í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¼ì¹˜í•˜ì§€ë§Œ, ì…ë ¥ ê´€ë ¨ì„±ì„ ì•½ 50%ë§Œ í¬ì°©í•©ë‹ˆë‹¤.
- 4. Cross-attentionì€ ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ í‘œí˜„ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ë°©ì‹ì„ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë°˜ì˜í•˜ë©°, saliencyì˜ 52-75%ë§Œ ì„¤ëª…í•©ë‹ˆë‹¤.
- 5. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” cross-attentionì„ ì„¤ëª…ì  í”„ë¡ì‹œë¡œ í•´ì„í•˜ëŠ” ë° ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆìŒì„ ë“œëŸ¬ë‚´ë©°, S2T ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì£¼ë„í•˜ëŠ” ìš”ì¸ë“¤ì— ëŒ€í•œ ë¶ˆì™„ì „í•œ ì‹œê°ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:16:57*