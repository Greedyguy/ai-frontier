---
keywords:
  - Vision-Language Model
  - Embodied Decision-Making
  - Adversarial Attack
  - Fine-grained Adversarial Attack
  - Autonomous Driving
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16645
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:30:06.794937",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Embodied Decision-Making",
    "Adversarial Attack",
    "Fine-grained Adversarial Attack",
    "Autonomous Driving"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "Embodied Decision-Making": 0.78,
    "Adversarial Attack": 0.8,
    "Fine-grained Adversarial Attack": 0.85,
    "Autonomous Driving": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus and are a trending concept in multimodal learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.92
      },
      {
        "surface": "Embodied Decision-Making",
        "canonical": "Embodied Decision-Making",
        "aliases": [
          "EDM"
        ],
        "category": "unique_technical",
        "rationale": "Embodied Decision-Making is a specific application area for VLMs in the context of the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Adversarial Attack",
        "canonical": "Adversarial Attack",
        "aliases": [
          "Adversarial Attacks"
        ],
        "category": "broad_technical",
        "rationale": "Adversarial Attack is a key method discussed in the paper, relevant to security and robustness in AI systems.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Fine-grained Adversarial Attack",
        "canonical": "Fine-grained Adversarial Attack",
        "aliases": [
          "Fine-grained Attack"
        ],
        "category": "unique_technical",
        "rationale": "This specific type of adversarial attack is novel and central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Autonomous Driving",
        "canonical": "Autonomous Driving",
        "aliases": [
          "Self-Driving Cars"
        ],
        "category": "specific_connectable",
        "rationale": "Autonomous Driving is a key application area for embodied agents discussed in the paper.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "system prompts",
      "task context"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Embodied Decision-Making",
      "resolved_canonical": "Embodied Decision-Making",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Adversarial Attack",
      "resolved_canonical": "Adversarial Attack",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Fine-grained Adversarial Attack",
      "resolved_canonical": "Fine-grained Adversarial Attack",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Autonomous Driving",
      "resolved_canonical": "Autonomous Driving",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16645.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16645](https://arxiv.org/abs/2509.16645)

## 🔗 유사한 논문
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (86.5% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (85.8% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (85.2% similar)
- [[2025-09-19/VLM-E2E_ Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion_20250919|VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion]] (84.6% similar)
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (84.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Adversarial Attack|Adversarial Attack]]
**🔗 Specific Connectable**: [[keywords/Autonomous Driving|Autonomous Driving]]
**⚡ Unique Technical**: [[keywords/Embodied Decision-Making|Embodied Decision-Making]], [[keywords/Fine-grained Adversarial Attack|Fine-grained Adversarial Attack]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16645v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs), with their strong reasoning and planning capabilities, are widely used in embodied decision-making (EDM) tasks in embodied agents, such as autonomous driving and robotic manipulation. Recent research has increasingly explored adversarial attacks on VLMs to reveal their vulnerabilities. However, these attacks either rely on overly strong assumptions, requiring full knowledge of the victim VLM, which is impractical for attacking VLM-based agents, or exhibit limited effectiveness. The latter stems from disrupting most semantic information in the image, which leads to a misalignment between the perception and the task context defined by system prompts. This inconsistency interrupts the VLM's reasoning process, resulting in invalid outputs that fail to affect interactions in the physical world. To this end, we propose a fine-grained adversarial attack framework, ADVEDM, which modifies the VLM's perception of only a few key objects while preserving the semantics of the remaining regions. This attack effectively reduces conflicts with the task context, making VLMs output valid but incorrect decisions and affecting the actions of agents, thus posing a more substantial safety threat in the physical world. We design two variants of based on this framework, ADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific object from the image and add the semantics of a new object into the image. The experimental results in both general scenarios and EDM tasks demonstrate fine-grained control and excellent attack performance.

## 📝 요약

이 논문은 시각-언어 모델(VLM)의 취약점을 드러내기 위한 정교한 적대적 공격 프레임워크인 ADVEDM을 제안합니다. 기존의 공격은 VLM에 대한 완전한 지식을 요구하거나 효과가 제한적이었으나, ADVEDM은 몇 가지 핵심 객체의 인식만을 변경하여 나머지 영역의 의미를 보존합니다. 이를 통해 VLM이 잘못된 결정을 내리도록 유도하여 물리적 세계에서의 안전 위협을 증가시킵니다. ADVEDM-R과 ADVEDM-A라는 두 가지 변형을 통해 특정 객체의 의미를 제거하거나 새로운 객체의 의미를 추가하는 방식으로 공격을 수행하며, 실험 결과는 일반 시나리오와 체화된 의사결정 과제에서 우수한 성능을 보였습니다.

## 🎯 주요 포인트

- 1. Vision-Language Models(VLMs)는 자율 주행 및 로봇 조작과 같은 구현된 에이전트의 의사 결정 작업에서 널리 사용됩니다.
- 2. 기존의 VLM에 대한 적대적 공격은 피해 VLM에 대한 완전한 지식을 요구하거나 효과가 제한적이라는 문제점이 있습니다.
- 3. ADVEDM은 VLM의 인식을 일부 핵심 객체에 대해서만 수정하여, 나머지 영역의 의미론을 유지하는 세밀한 적대적 공격 프레임워크입니다.
- 4. ADVEDM은 VLM이 유효하지만 잘못된 결정을 내리게 하여 물리적 세계에서 에이전트의 행동에 영향을 미칩니다.
- 5. ADVEDM-R과 ADVEDM-A는 각각 이미지에서 특정 객체의 의미론을 제거하거나 새로운 객체의 의미론을 추가하는 변형입니다.


---

*Generated on 2025-09-24 04:30:06*