---
keywords:
  - Large Language Model
  - Model Pruning
  - Prefill-Decode Disaggregation
  - KV Cache Pruning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.04467
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:25:38.492154",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Model Pruning",
    "Prefill-Decode Disaggregation",
    "KV Cache Pruning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Model Pruning": 0.82,
    "Prefill-Decode Disaggregation": 0.79,
    "KV Cache Pruning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus on computational efficiency and pruning strategies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Model Pruning",
        "canonical": "Model Pruning",
        "aliases": [
          "Pruning"
        ],
        "category": "unique_technical",
        "rationale": "Model pruning is a key technique discussed for improving inference efficiency.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Prefill-Decode Disaggregation",
        "canonical": "Prefill-Decode Disaggregation",
        "aliases": [
          "PD Disaggregation"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel concept introduced in the paper, crucial for understanding the proposed pruning method.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "KV Cache Pruning",
        "canonical": "KV Cache Pruning",
        "aliases": [
          "Key-Value Cache Pruning"
        ],
        "category": "unique_technical",
        "rationale": "KV Cache Pruning is a specific technique highlighted for its role in reducing communication costs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "inference",
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Model Pruning",
      "resolved_canonical": "Model Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Prefill-Decode Disaggregation",
      "resolved_canonical": "Prefill-Decode Disaggregation",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "KV Cache Pruning",
      "resolved_canonical": "KV Cache Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.04467.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.04467](https://arxiv.org/abs/2509.04467)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (86.5% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (85.8% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (84.9% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.7% similar)
- [[2025-09-22/Backdoor Mitigation via Invertible Pruning Masks_20250922|Backdoor Mitigation via Invertible Pruning Masks]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Model Pruning|Model Pruning]], [[keywords/Prefill-Decode Disaggregation|Prefill-Decode Disaggregation]], [[keywords/KV Cache Pruning|KV Cache Pruning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.04467v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the same (default) settings, our method achieves improved performance and faster inference, along with a 4.95$\times$ reduction in data transmission bandwidth consumption.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë†’ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë¸ ê°€ì§€ì¹˜ê¸° ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ í”„ë¦¬í•„-ë””ì½”ë“œ(PD) ë¶„í•´ì˜ íŠ¹ì„±ì„ ê°„ê³¼í•˜ëŠ” ë°˜ë©´, ë³¸ ì—°êµ¬ëŠ” PD ë¶„í•´ ì¶”ë¡ ì„ ìœ„í•œ ìƒˆë¡œìš´ ê°€ì§€ì¹˜ê¸° ë°©ë²•ì„ ì œì‹œí•˜ì—¬ ë¸”ë¡ ë° KV ìºì‹œ ê°€ì§€ì¹˜ê¸°ë¥¼ ë³´ë‹¤ ì •ë°€í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. í”„ë¦¬í•„ ë° ë””ì½”ë“œ ë‹¨ê³„ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ë¸”ë¡ ì œê±°ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê°€ì§€ì¹˜ê¸° ë° ì¦ë¥˜ ì„¸íŠ¸ë¥¼ êµ¬ì„±í•˜ì—¬ ë” ë‚˜ì€ ê°€ì§€ì¹˜ê¸° ì†”ë£¨ì…˜ì„ ì–»ìŠµë‹ˆë‹¤. ë˜í•œ, í† í° ì¸ì‹ ìºì‹œ ê°€ì§€ì¹˜ê¸° ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ í”„ë¦¬í•„ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë“  KV ìºì‹œë¥¼ ìœ ì§€í•˜ë˜, ë””ì½”ë“œ ë‹¨ê³„ì—ì„œëŠ” ì„ íƒëœ ë ˆì´ì–´ì˜ ì²« ë²ˆì§¸ ë° ë§ˆì§€ë§‰ í† í° ì‹œí€€ìŠ¤ì— ëŒ€í•´ ì„ íƒì ìœ¼ë¡œ ì¬ì‚¬ìš©í•˜ì—¬ í†µì‹  ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ PD ë¶„í•´ ë° ë¹„ë¶„í•´ í™˜ê²½ ëª¨ë‘ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ ë³´ì—¬ì£¼ë©°, ë°ì´í„° ì „ì†¡ ëŒ€ì—­í­ ì†Œë¹„ë¥¼ 4.95ë°° ì¤„ì´ë©´ì„œ ì„±ëŠ¥ê³¼ ì¶”ë¡  ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ë†’ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš© ë¬¸ì œë¥¼ ëª¨ë¸ ê°€ì§€ì¹˜ê¸°ë¥¼ í†µí•´ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ê°„ê³¼í•œ prefill-decode(PD) ë¶„í•´ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ìƒˆë¡œìš´ ê°€ì§€ì¹˜ê¸° ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ prefill ë° decode ë‹¨ê³„ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ë¸”ë¡ ì œê±°ë¥¼ ìˆ˜í–‰í•˜ì—¬ ë” ë‚˜ì€ ê°€ì§€ì¹˜ê¸° ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. í† í° ì¸ì‹ ìºì‹œ ê°€ì§€ì¹˜ê¸° ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ í†µì‹  ë¹„ìš©ì„ ì¤„ì´ë©´ì„œë„ ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œë¡œ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ PD ë¶„í•´ ë° ë¹„ë¶„í•´ ì„¤ì • ëª¨ë‘ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ ë‹¬ì„±í•˜ë©° ë°ì´í„° ì „ì†¡ ëŒ€ì—­í­ ì†Œë¹„ë¥¼ 4.95ë°° ì¤„ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:25:38*