---
keywords:
  - Self-supervised Learning
  - Transformer
  - Semantic Segmentation
  - Continual Self-Supervised Pre-training
  - Global Local and Regional Enforcement
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17816
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:04:19.607240",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Transformer",
    "Semantic Segmentation",
    "Continual Self-Supervised Pre-training",
    "Global Local and Regional Enforcement"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.85,
    "Transformer": 0.8,
    "Semantic Segmentation": 0.82,
    "Continual Self-Supervised Pre-training": 0.78,
    "Global Local and Regional Enforcement": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Self-supervised Learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "SSL"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's methodology, linking to a well-established concept in machine learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "A key component in the model architecture, connecting to the broader concept of Transformers.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Semantic Segmentation",
        "canonical": "Semantic Segmentation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The primary application focus of the paper, crucial for linking to related works in computer vision.",
        "novelty_score": 0.2,
        "connectivity_score": 0.85,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "Continual Self-Supervised Pre-training",
        "canonical": "Continual Self-Supervised Pre-training",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel approach introduced in the paper, enhancing the adaptability of models across domains.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "GLARE",
        "canonical": "Global Local and Regional Enforcement",
        "aliases": [
          "GLARE"
        ],
        "category": "unique_technical",
        "rationale": "A new method proposed in the paper, essential for understanding the specific contributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Self-supervised Learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Semantic Segmentation",
      "resolved_canonical": "Semantic Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.85,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Continual Self-Supervised Pre-training",
      "resolved_canonical": "Continual Self-Supervised Pre-training",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "GLARE",
      "resolved_canonical": "Global Local and Regional Enforcement",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17816.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17816](https://arxiv.org/abs/2509.17816)

## 🔗 유사한 논문
- [[2025-09-22/Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks_20250922|Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks]] (84.1% similar)
- [[2025-09-23/Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models_20250923|Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models]] (82.9% similar)
- [[2025-09-23/Visual Instruction Pretraining for Domain-Specific Foundation Models_20250923|Visual Instruction Pretraining for Domain-Specific Foundation Models]] (82.8% similar)
- [[2025-09-22/Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization_20250922|Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization]] (82.6% similar)
- [[2025-09-22/Global Pre-fixing, Local Adjusting_ A Simple yet Effective Contrastive Strategy for Continual Learning_20250922|Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning]] (82.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Semantic Segmentation|Semantic Segmentation]]
**⚡ Unique Technical**: [[keywords/Continual Self-Supervised Pre-training|Continual Self-Supervised Pre-training]], [[keywords/Global Local and Regional Enforcement|Global Local and Regional Enforcement]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17816v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by leveraging large-scale unlabeled datasets, often producing representations with strong generalization capabilities. These models are typically pre-trained on general-purpose datasets such as ImageNet and subsequently adapted to various downstream tasks through finetuning. While recent advances have explored parameter-efficient strategies for adapting pre-trained models, extending SSL pre-training itself to new domains - particularly under limited data regimes and for dense prediction tasks - remains underexplored. In this work, we address the problem of adapting vision foundation models to new domains in an unsupervised and data-efficient manner, specifically targeting downstream semantic segmentation. We propose GLARE (Global Local and Regional Enforcement), a novel continual self-supervised pre-training task designed to enhance downstream segmentation performance. GLARE introduces patch-level augmentations to encourage local consistency and incorporates a regional consistency constraint that leverages spatial semantics in the data. For efficient continual pre-training, we initialize Vision Transformers (ViTs) with weights from existing SSL models and update only lightweight adapter modules - specifically UniAdapter - while keeping the rest of the backbone frozen. Experiments across multiple semantic segmentation benchmarks on different domains demonstrate that GLARE consistently improves downstream performance with minimal computational and parameter overhead.

## 📝 요약

이 논문은 자가 지도 학습(SSL)을 통해 대규모 비지도 데이터셋을 활용하여 강력한 일반화 능력을 가진 모델을 훈련하는 방법을 탐구합니다. 특히, 새로운 도메인에서의 비지도 및 데이터 효율적인 방식으로 비전 기반 모델을 적응시키는 문제를 다루며, 주로 하위 작업인 의미론적 분할을 목표로 합니다. 이를 위해 GLARE(Global Local and Regional Enforcement)라는 새로운 지속적 자가 지도 사전 훈련 작업을 제안합니다. GLARE는 패치 수준의 증강을 도입하여 지역적 일관성을 강화하고, 데이터의 공간적 의미론을 활용한 지역적 일관성 제약을 포함합니다. 효율적인 지속적 사전 훈련을 위해, 기존 SSL 모델의 가중치로 초기화된 Vision Transformers(ViTs)를 사용하고, 백본을 고정한 채 가벼운 어댑터 모듈인 UniAdapter만 업데이트합니다. 여러 의미론적 분할 벤치마크 실험에서 GLARE는 최소한의 계산 및 파라미터 오버헤드로 일관되게 성능을 향상시켰습니다.

## 🎯 주요 포인트

- 1. 자기 지도 학습(SSL)은 대규모 비지도 데이터셋을 활용하여 강력한 일반화 능력을 가진 표현을 생성하는 중심 패러다임으로 부상했습니다.
- 2. SSL 사전 학습을 새로운 도메인으로 확장하는 것은 제한된 데이터 환경과 밀집 예측 작업에서 여전히 탐구되지 않은 분야입니다.
- 3. GLARE는 다운스트림 의미론적 분할 성능을 향상시키기 위해 설계된 새로운 지속적 자기 지도 사전 학습 작업입니다.
- 4. GLARE는 지역 일관성 제약을 도입하여 데이터의 공간적 의미를 활용하고, 패치 수준의 증강을 통해 지역적 일관성을 촉진합니다.
- 5. 실험 결과, GLARE는 여러 도메인의 의미론적 분할 벤치마크에서 최소한의 계산 및 파라미터 오버헤드로 다운스트림 성능을 지속적으로 향상시킵니다.


---

*Generated on 2025-09-24 05:04:19*