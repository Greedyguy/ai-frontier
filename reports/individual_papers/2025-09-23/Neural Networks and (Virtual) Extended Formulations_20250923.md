---
keywords:
  - Neural Network
  - Piecewise Linear Activation
  - Extension Complexity
  - Virtual Extension Complexity
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2411.03006
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:56:55.324667",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Network",
    "Piecewise Linear Activation",
    "Extension Complexity",
    "Virtual Extension Complexity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Network": 0.85,
    "Piecewise Linear Activation": 0.78,
    "Extension Complexity": 0.82,
    "Virtual Extension Complexity": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "NN",
          "Neural Nets"
        ],
        "category": "broad_technical",
        "rationale": "Neural networks are central to the discussion and connect with a wide range of machine learning topics.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Piecewise Linear Activation Functions",
        "canonical": "Piecewise Linear Activation",
        "aliases": [
          "ReLU",
          "Maxout"
        ],
        "category": "specific_connectable",
        "rationale": "These activation functions are fundamental to the architecture of neural networks discussed in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Extension Complexity",
        "canonical": "Extension Complexity",
        "aliases": [
          "xc(P)"
        ],
        "category": "unique_technical",
        "rationale": "This concept is crucial for understanding the theoretical limits of neural network size.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "Virtual Extension Complexity",
        "canonical": "Virtual Extension Complexity",
        "aliases": [
          "vxc(P)"
        ],
        "category": "unique_technical",
        "rationale": "Introduced in the paper, this concept generalizes extension complexity and is key to the research findings.",
        "novelty_score": 0.9,
        "connectivity_score": 0.55,
        "specificity_score": 0.95,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "linear program",
      "inequalities",
      "polytope"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Piecewise Linear Activation Functions",
      "resolved_canonical": "Piecewise Linear Activation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Extension Complexity",
      "resolved_canonical": "Extension Complexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Virtual Extension Complexity",
      "resolved_canonical": "Virtual Extension Complexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.55,
        "specificity": 0.95,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Neural Networks and (Virtual) Extended Formulations

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2411.03006.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2411.03006](https://arxiv.org/abs/2411.03006)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Virtual Arc Consistency for Linear Constraints inCost Function Networks_20250923|Virtual Arc Consistency for Linear Constraints inCost Function Networks]] (80.3% similar)
- [[2025-09-23/Regularizing Extrapolation in Causal Inference_20250923|Regularizing Extrapolation in Causal Inference]] (79.7% similar)
- [[2025-09-23/Unified Framework for Pre-trained Neural Network Compression via Decomposition and Optimized Rank Selection_20250923|Unified Framework for Pre-trained Neural Network Compression via Decomposition and Optimized Rank Selection]] (79.4% similar)
- [[2025-09-23/Checking extracted rules in Neural Networks_20250923|Checking extracted rules in Neural Networks]] (79.3% similar)
- [[2025-09-23/Deep Hierarchical Learning with Nested Subspace Networks_20250923|Deep Hierarchical Learning with Nested Subspace Networks]] (79.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Piecewise Linear Activation|Piecewise Linear Activation]]
**âš¡ Unique Technical**: [[keywords/Extension Complexity|Extension Complexity]], [[keywords/Virtual Extension Complexity|Virtual Extension Complexity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2411.03006v3 Announce Type: replace-cross 
Abstract: Neural networks with piecewise linear activation functions, such as rectified linear units (ReLU) or maxout, are among the most fundamental models in modern machine learning. We make a step towards proving lower bounds on the size of such neural networks by linking their representative capabilities to the notion of the extension complexity $\mathrm{xc}(P)$ of a polytope $P$. This is a well-studied quantity in combinatorial optimization and polyhedral geometry describing the number of inequalities needed to model $P$ as a linear program. We show that $\mathrm{xc}(P)$ is a lower bound on the size of any monotone or input-convex neural network that solves the linear optimization problem over $P$. This implies exponential lower bounds on such neural networks for a variety of problems, including the polynomially solvable maximum weight matching problem.
  In an attempt to prove similar bounds also for general neural networks, we introduce the notion of virtual extension complexity $\mathrm{vxc}(P)$, which generalizes $\mathrm{xc}(P)$ and describes the number of inequalities needed to represent the linear optimization problem over $P$ as a difference of two linear programs. We prove that $\mathrm{vxc}(P)$ is a lower bound on the size of any neural network that optimizes over $P$. While it remains an open question to derive useful lower bounds on $\mathrm{vxc}(P)$, we argue that this quantity deserves to be studied independently from neural networks by proving that one can efficiently optimize over a polytope $P$ using a small virtual extended formulation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¡°ê°ë³„ ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§(ì˜ˆ: ReLU, maxout)ì˜ í¬ê¸°ì— ëŒ€í•œ í•˜í•œì„ ì¦ëª…í•˜ê¸° ìœ„í•´ ë‹¤ë©´ì²´ $P$ì˜ í™•ì¥ ë³µì¡ì„± $\mathrm{xc}(P)$ì™€ì˜ ê´€ê³„ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. $\mathrm{xc}(P)$ëŠ” ì„ í˜• í”„ë¡œê·¸ë¨ìœ¼ë¡œ $P$ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë° í•„ìš”í•œ ë¶€ë“±ì‹ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ëŠ” ì¡°í•© ìµœì í™” ë° ë‹¤ë©´ì²´ ê¸°í•˜í•™ì—ì„œ ì˜ ì—°êµ¬ëœ ê°œë…ì…ë‹ˆë‹¤. ì €ìë“¤ì€ $\mathrm{xc}(P)$ê°€ $P$ì— ëŒ€í•œ ì„ í˜• ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë‹¨ì¡° ë˜ëŠ” ì…ë ¥-ë³¼ë¡ ì‹ ê²½ë§ì˜ í¬ê¸°ì— ëŒ€í•œ í•˜í•œì„ì„ ì¦ëª…í•˜ê³ , ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë¬¸ì œì— ëŒ€í•œ ì‹ ê²½ë§ì˜ í¬ê¸°ì— ëŒ€í•œ ì§€ìˆ˜ì  í•˜í•œì„ ë„ì¶œí•©ë‹ˆë‹¤. ë˜í•œ, ì¼ë°˜ ì‹ ê²½ë§ì— ëŒ€í•œ ìœ ì‚¬í•œ í•˜í•œì„ ì¦ëª…í•˜ê¸° ìœ„í•´ ê°€ìƒ í™•ì¥ ë³µì¡ì„± $\mathrm{vxc}(P)$ë¥¼ ë„ì…í•˜ê³ , ì´ëŠ” $P$ì— ëŒ€í•œ ì„ í˜• ìµœì í™” ë¬¸ì œë¥¼ ë‘ ê°œì˜ ì„ í˜• í”„ë¡œê·¸ë¨ì˜ ì°¨ì´ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë° í•„ìš”í•œ ë¶€ë“±ì‹ì˜ ìˆ˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. $\mathrm{vxc}(P)$ê°€ $P$ì— ëŒ€í•´ ìµœì í™”í•˜ëŠ” ì‹ ê²½ë§ì˜ í¬ê¸°ì— ëŒ€í•œ í•˜í•œì„ì„ ì¦ëª…í•˜ë©°, ì´ ê°œë…ì´ ì‹ ê²½ë§ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì—°êµ¬ë  ê°€ì¹˜ê°€ ìˆìŒì„ ì£¼ì¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¡°ê°ë³„ ì„ í˜• í™œì„±í™” í•¨ìˆ˜(ReLU, maxout ë“±)ë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ì˜ í¬ê¸°ì— ëŒ€í•œ í•˜í•œì„ ì¦ëª…í•˜ê¸° ìœ„í•´ ë‹¤ë©´ì²´ $P$ì˜ í™•ì¥ ë³µì¡ì„± $\mathrm{xc}(P)$ ê°œë…ê³¼ ì—°ê²°í–ˆìŠµë‹ˆë‹¤.
- 2. $\mathrm{xc}(P)$ëŠ” $P$ë¥¼ ì„ í˜• í”„ë¡œê·¸ë¨ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë° í•„ìš”í•œ ë¶€ë“±ì‹ì˜ ìˆ˜ë¥¼ ì„¤ëª…í•˜ë©°, ì´ëŠ” ë‹¨ì¡° ë˜ëŠ” ì…ë ¥-ë³¼ë¡ ì‹ ê²½ë§ì˜ í¬ê¸°ì— ëŒ€í•œ í•˜í•œì´ ë©ë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ ë¬¸ì œì— ëŒ€í•´ ì´ëŸ¬í•œ ì‹ ê²½ë§ì— ëŒ€í•œ ì§€ìˆ˜ì  í•˜í•œì„ ì•”ì‹œí•˜ë©°, ì—¬ê¸°ì—ëŠ” ë‹¤í•­ì‹ì ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•œ ìµœëŒ€ ê°€ì¤‘ì¹˜ ë§¤ì¹­ ë¬¸ì œë„ í¬í•¨ë©ë‹ˆë‹¤.
- 4. ì¼ë°˜ ì‹ ê²½ë§ì— ëŒ€í•œ ìœ ì‚¬í•œ í•˜í•œì„ ì¦ëª…í•˜ê¸° ìœ„í•´, ì„ í˜• ìµœì í™” ë¬¸ì œë¥¼ ë‘ ê°œì˜ ì„ í˜• í”„ë¡œê·¸ë¨ì˜ ì°¨ì´ë¡œ í‘œí˜„í•˜ëŠ” ë° í•„ìš”í•œ ë¶€ë“±ì‹ì˜ ìˆ˜ë¥¼ ì„¤ëª…í•˜ëŠ” ê°€ìƒ í™•ì¥ ë³µì¡ì„± $\mathrm{vxc}(P)$ ê°œë…ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 5. $\mathrm{vxc}(P)$ì— ëŒ€í•œ ìœ ìš©í•œ í•˜í•œì„ ë„ì¶œí•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ë¯¸í•´ê²° ë¬¸ì œì´ì§€ë§Œ, ì‘ì€ ê°€ìƒ í™•ì¥ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ë©´ì²´ $P$ì— ëŒ€í•´ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:56:55*