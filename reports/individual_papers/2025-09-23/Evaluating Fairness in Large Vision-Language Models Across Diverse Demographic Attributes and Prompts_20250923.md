---
keywords:
  - Vision-Language Model
  - Visual Fairness
  - Demographic Attributes
  - Zero-Shot Learning
  - Chain-of-Thought Strategy
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2406.17974
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:43:13.782039",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Visual Fairness",
    "Demographic Attributes",
    "Zero-Shot Learning",
    "Chain-of-Thought Strategy"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Visual Fairness": 0.78,
    "Demographic Attributes": 0.7,
    "Zero-Shot Learning": 0.8,
    "Chain-of-Thought Strategy": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large vision-language models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLM",
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on fairness across demographic attributes, making them a key concept for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "visual fairness",
        "canonical": "Visual Fairness",
        "aliases": [
          "Fairness in Vision",
          "Demographic Fairness"
        ],
        "category": "unique_technical",
        "rationale": "This term is unique to the study's focus on fairness in visual models, offering a novel concept for linking.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "demographic attributes",
        "canonical": "Demographic Attributes",
        "aliases": [
          "Demographic Factors",
          "Demographic Characteristics"
        ],
        "category": "specific_connectable",
        "rationale": "Understanding demographic attributes is crucial for evaluating fairness, providing a strong link to related research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "zero-shot prompting",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot Prompting"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept that is directly applied in the study's evaluation framework.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Chain-of-thought strategy",
        "canonical": "Chain-of-Thought Strategy",
        "aliases": [
          "CoT Strategy",
          "Chain-of-Thought"
        ],
        "category": "unique_technical",
        "rationale": "This strategy is proposed as a novel method for fairness mitigation, offering a unique concept for linking.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "fairness evaluation",
      "performance disparities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large vision-language models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "visual fairness",
      "resolved_canonical": "Visual Fairness",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "demographic attributes",
      "resolved_canonical": "Demographic Attributes",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "zero-shot prompting",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Chain-of-thought strategy",
      "resolved_canonical": "Chain-of-Thought Strategy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2406.17974.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2406.17974](https://arxiv.org/abs/2406.17974)

## 🔗 유사한 논문
- [[2025-09-23/Datasets for Fairness in Language Models_ An In-Depth Survey_20250923|Datasets for Fairness in Language Models: An In-Depth Survey]] (87.9% similar)
- [[2025-09-23/Intrinsic Meets Extrinsic Fairness_ Assessing the Downstream Impact of Bias Mitigation in Large Language Models_20250923|Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models]] (85.9% similar)
- [[2025-09-23/Auto-Search and Refinement_ An Automated Framework for Gender Bias Mitigation in Large Language Models_20250923|Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models]] (84.9% similar)
- [[2025-09-23/Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization_20250923|Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization]] (83.7% similar)
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (83.5% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Demographic Attributes|Demographic Attributes]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Visual Fairness|Visual Fairness]], [[keywords/Chain-of-Thought Strategy|Chain-of-Thought Strategy]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2406.17974v3 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) have recently achieved significant progress, demonstrating strong capabilities in open-world visual understanding. However, it is not yet clear how LVLMs address demographic biases in real life, especially the disparities across attributes such as gender, skin tone, age and race. In this paper, We empirically investigate \emph{visual fairness} in several mainstream LVLMs by auditing their performance disparities across demographic attributes using public fairness benchmark datasets (e.g., FACET, UTKFace). Our fairness evaluation framework employs direct and single-choice question prompt on visual question-answering/classification tasks. Despite advancements in visual understanding, our zero-shot prompting results show that both open-source and closed-source LVLMs continue to exhibit fairness issues across different prompts and demographic groups. Furthermore, we propose a potential multi-modal Chain-of-thought (CoT) based strategy for unfairness mitigation, applicable to both open-source and closed-source LVLMs. This approach enhances transparency and offers a scalable solution for addressing fairness, providing a solid foundation for future research and practical efforts in unfairness mitigation. The dataset and code used in this study are publicly available at this GitHub Repository.

## 📝 요약

이 논문은 대규모 비전-언어 모델(LVLMs)의 시각적 공정성을 조사합니다. 연구는 성별, 피부색, 연령, 인종 등 인구통계적 속성에 따른 성능 차이를 공정성 벤치마크 데이터셋(FACET, UTKFace)을 사용해 평가합니다. 결과적으로, LVLMs는 여전히 공정성 문제를 보이며, 이를 해결하기 위해 멀티모달 Chain-of-thought(CoT) 전략을 제안합니다. 이 접근법은 투명성을 높이고 공정성 문제 해결을 위한 확장 가능한 솔루션을 제공합니다. 연구에 사용된 데이터셋과 코드는 GitHub에서 공개됩니다.

## 🎯 주요 포인트

- 1. 대규모 비전-언어 모델(LVLMs)은 시각적 이해에서 강력한 성능을 보였으나, 실제 생활에서의 인구통계학적 편향 문제는 여전히 해결되지 않았다.
- 2. 본 연구는 LVLMs의 인구통계학적 속성(성별, 피부색, 나이, 인종 등)에 따른 성능 차이를 공정성 벤치마크 데이터셋을 통해 실증적으로 조사하였다.
- 3. 실험 결과, 오픈소스 및 비공개 LVLMs 모두 다양한 프롬프트와 인구통계학적 그룹에서 여전히 공정성 문제를 보였다.
- 4. 불공정성 완화를 위해 다중 모달 체인 오브 쏘트(CoT) 기반 전략을 제안하였으며, 이는 투명성을 높이고 공정성 문제 해결에 대한 확장 가능한 솔루션을 제공한다.
- 5. 연구에 사용된 데이터셋과 코드는 GitHub 저장소에서 공개적으로 제공된다.


---

*Generated on 2025-09-24 03:43:13*