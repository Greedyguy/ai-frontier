---
keywords:
  - Transformer
  - No-Reference Image Quality Assessment
  - SigLIP2
  - Learnable Activation Selection Mechanism
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17374
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:49:53.331838",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "No-Reference Image Quality Assessment",
    "SigLIP2",
    "Learnable Activation Selection Mechanism",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "No-Reference Image Quality Assessment": 0.8,
    "SigLIP2": 0.78,
    "Learnable Activation Selection Mechanism": 0.77,
    "Vision-Language Model": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Vision Transformer",
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are foundational models in both vision and language tasks, enabling strong cross-domain linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "No-Reference Image Quality Assessment",
        "canonical": "No-Reference Image Quality Assessment",
        "aliases": [
          "NR-IQA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized task within computer vision, providing a unique link to image quality evaluation studies.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "SigLIP2",
        "canonical": "SigLIP2",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "SigLIP2 is highlighted for its strong performance, making it a key model for linking in NR-IQA research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Learnable Activation Selection Mechanism",
        "canonical": "Learnable Activation Selection Mechanism",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This mechanism introduces innovation in activation function selection, relevant for adaptive model design.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language models are an emerging area linking vision and NLP, crucial for multimodal research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "activation function",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "No-Reference Image Quality Assessment",
      "resolved_canonical": "No-Reference Image Quality Assessment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "SigLIP2",
      "resolved_canonical": "SigLIP2",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Learnable Activation Selection Mechanism",
      "resolved_canonical": "Learnable Activation Selection Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Revisiting Vision Language Foundations for No-Reference Image Quality Assessment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17374.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17374](https://arxiv.org/abs/2509.17374)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Catching the Details_ Self-Distilled RoI Predictors for Fine-Grained MLLM Perception_20250923|Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception]] (83.4% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.8% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (82.6% similar)
- [[2025-09-22/DPC-QA Net_ A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images_20250922|DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images]] (82.4% similar)
- [[2025-09-22/Modeling the Human Visual System_ Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms_20250922|Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**âš¡ Unique Technical**: [[keywords/No-Reference Image Quality Assessment|No-Reference Image Quality Assessment]], [[keywords/SigLIP2|SigLIP2]], [[keywords/Learnable Activation Selection Mechanism|Learnable Activation Selection Mechanism]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17374v1 Announce Type: new 
Abstract: Large-scale vision language pre-training has recently shown promise for no-reference image-quality assessment (NR-IQA), yet the relative merits of modern Vision Transformer foundations remain poorly understood. In this work, we present the first systematic evaluation of six prominent pretrained backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an identical lightweight MLP head. Our study uncovers two previously overlooked factors: (1) SigLIP2 consistently achieves strong performance; and (2) the choice of activation function plays a surprisingly crucial role, particularly for enhancing the generalization ability of image quality assessment models. Notably, we find that simple sigmoid activations outperform commonly used ReLU and GELU on several benchmarks. Motivated by this finding, we introduce a learnable activation selection mechanism that adaptively determines the nonlinearity for each channel, eliminating the need for manual activation design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and AGIQA3K. Extensive ablations confirm the benefits across architectures and regimes, establishing strong, resource-efficient NR-IQA baselines.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ì‚¬ì „ í•™ìŠµì´ ë¬´ì°¸ì¡° ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€(NR-IQA)ì— ìœ ë§í•˜ë‹¤ëŠ” ìµœê·¼ì˜ ë°œê²¬ì„ ë°”íƒ•ìœ¼ë¡œ, CLIP, SigLIP2, DINOv2, DINOv3, Perception, ResNet ë“± ì—¬ì„¯ ê°€ì§€ ì‚¬ì „ í•™ìŠµëœ ë°±ë³¸ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, SigLIP2ê°€ ì¼ê´€ë˜ê²Œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, í™œì„±í™” í•¨ìˆ˜ì˜ ì„ íƒì´ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¨ìˆœí•œ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ê°€ ReLUë‚˜ GELUë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì±„ë„ë³„ë¡œ ë¹„ì„ í˜•ì„±ì„ ì ì‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ í™œì„±í™” ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬, CLIVE, KADID10K, AGIQA3Kì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ì´ ì ‘ê·¼ë²•ì˜ íš¨ìœ¨ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SigLIP2ëŠ” No-Reference Image Quality Assessment(NR-IQA)ì—ì„œ ì¼ê´€ë˜ê²Œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 2. í™œì„±í™” í•¨ìˆ˜ì˜ ì„ íƒì´ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
- 3. ê°„ë‹¨í•œ sigmoid í™œì„±í™” í•¨ìˆ˜ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ReLU ë° GELUë³´ë‹¤ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 4. ì±„ë„ë³„ë¡œ ë¹„ì„ í˜•ì„±ì„ ì ì‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ í™œì„±í™” ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ìˆ˜ë™ í™œì„±í™” ì„¤ê³„ì˜ í•„ìš”ì„±ì„ ì œê±°í•©ë‹ˆë‹¤.
- 5. CLIVE, KADID10K, AGIQA3Kì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ SRCCë¥¼ ë‹¬ì„±í•˜ë©°, ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì™€ í™˜ê²½ì—ì„œ ê°•ë ¥í•˜ê³  ìì› íš¨ìœ¨ì ì¸ NR-IQA ê¸°ì¤€ì„ í™•ë¦½í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:49:53*