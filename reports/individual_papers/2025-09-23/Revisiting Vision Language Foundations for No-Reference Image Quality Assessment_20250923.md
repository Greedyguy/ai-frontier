---
keywords:
  - Transformer
  - No-Reference Image Quality Assessment
  - SigLIP2
  - Learnable Activation Selection Mechanism
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17374
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:49:53.331838",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "No-Reference Image Quality Assessment",
    "SigLIP2",
    "Learnable Activation Selection Mechanism",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "No-Reference Image Quality Assessment": 0.8,
    "SigLIP2": 0.78,
    "Learnable Activation Selection Mechanism": 0.77,
    "Vision-Language Model": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Vision Transformer",
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are foundational models in both vision and language tasks, enabling strong cross-domain linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "No-Reference Image Quality Assessment",
        "canonical": "No-Reference Image Quality Assessment",
        "aliases": [
          "NR-IQA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized task within computer vision, providing a unique link to image quality evaluation studies.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "SigLIP2",
        "canonical": "SigLIP2",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "SigLIP2 is highlighted for its strong performance, making it a key model for linking in NR-IQA research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Learnable Activation Selection Mechanism",
        "canonical": "Learnable Activation Selection Mechanism",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This mechanism introduces innovation in activation function selection, relevant for adaptive model design.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language models are an emerging area linking vision and NLP, crucial for multimodal research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "activation function",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "No-Reference Image Quality Assessment",
      "resolved_canonical": "No-Reference Image Quality Assessment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "SigLIP2",
      "resolved_canonical": "SigLIP2",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Learnable Activation Selection Mechanism",
      "resolved_canonical": "Learnable Activation Selection Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Revisiting Vision Language Foundations for No-Reference Image Quality Assessment

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17374.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17374](https://arxiv.org/abs/2509.17374)

## 🔗 유사한 논문
- [[2025-09-23/Catching the Details_ Self-Distilled RoI Predictors for Fine-Grained MLLM Perception_20250923|Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception]] (83.4% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.8% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (82.6% similar)
- [[2025-09-22/DPC-QA Net_ A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images_20250922|DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images]] (82.4% similar)
- [[2025-09-22/Modeling the Human Visual System_ Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms_20250922|Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms]] (82.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**⚡ Unique Technical**: [[keywords/No-Reference Image Quality Assessment|No-Reference Image Quality Assessment]], [[keywords/SigLIP2|SigLIP2]], [[keywords/Learnable Activation Selection Mechanism|Learnable Activation Selection Mechanism]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17374v1 Announce Type: new 
Abstract: Large-scale vision language pre-training has recently shown promise for no-reference image-quality assessment (NR-IQA), yet the relative merits of modern Vision Transformer foundations remain poorly understood. In this work, we present the first systematic evaluation of six prominent pretrained backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an identical lightweight MLP head. Our study uncovers two previously overlooked factors: (1) SigLIP2 consistently achieves strong performance; and (2) the choice of activation function plays a surprisingly crucial role, particularly for enhancing the generalization ability of image quality assessment models. Notably, we find that simple sigmoid activations outperform commonly used ReLU and GELU on several benchmarks. Motivated by this finding, we introduce a learnable activation selection mechanism that adaptively determines the nonlinearity for each channel, eliminating the need for manual activation design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and AGIQA3K. Extensive ablations confirm the benefits across architectures and regimes, establishing strong, resource-efficient NR-IQA baselines.

## 📝 요약

이 논문은 대규모 비전-언어 사전 학습이 무참조 이미지 품질 평가(NR-IQA)에 유망하다는 최근의 발견을 바탕으로, CLIP, SigLIP2, DINOv2, DINOv3, Perception, ResNet 등 여섯 가지 사전 학습된 백본을 체계적으로 평가했습니다. 연구 결과, SigLIP2가 일관되게 높은 성능을 보였으며, 활성화 함수의 선택이 모델의 일반화 능력에 중요한 역할을 한다는 점을 발견했습니다. 특히, 단순한 시그모이드 활성화 함수가 ReLU나 GELU보다 뛰어난 성능을 보였습니다. 이를 바탕으로, 채널별로 비선형성을 적응적으로 결정하는 학습 가능한 활성화 선택 메커니즘을 도입하여, CLIVE, KADID10K, AGIQA3K에서 새로운 최첨단 성능을 달성했습니다. 다양한 실험을 통해 이 접근법의 효율성을 입증했습니다.

## 🎯 주요 포인트

- 1. SigLIP2는 No-Reference Image Quality Assessment(NR-IQA)에서 일관되게 높은 성능을 보여줍니다.
- 2. 활성화 함수의 선택이 이미지 품질 평가 모델의 일반화 능력을 향상시키는 데 중요한 역할을 합니다.
- 3. 간단한 sigmoid 활성화 함수가 일반적으로 사용되는 ReLU 및 GELU보다 여러 벤치마크에서 더 우수한 성능을 보입니다.
- 4. 채널별로 비선형성을 적응적으로 결정하는 학습 가능한 활성화 선택 메커니즘을 도입하여 수동 활성화 설계의 필요성을 제거합니다.
- 5. CLIVE, KADID10K, AGIQA3K에서 새로운 최첨단 SRCC를 달성하며, 다양한 아키텍처와 환경에서 강력하고 자원 효율적인 NR-IQA 기준을 확립했습니다.


---

*Generated on 2025-09-24 04:49:53*