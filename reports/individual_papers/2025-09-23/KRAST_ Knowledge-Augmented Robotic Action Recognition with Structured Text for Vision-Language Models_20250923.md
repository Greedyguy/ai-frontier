---
keywords:
  - Vision-Language Model
  - Knowledge-Augmented Prompts
  - Prompt-Learning
  - Robotic Action Recognition
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16452
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:20:32.280610",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Knowledge-Augmented Prompts",
    "Prompt-Learning",
    "Robotic Action Recognition"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Knowledge-Augmented Prompts": 0.78,
    "Prompt-Learning": 0.8,
    "Robotic Action Recognition": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's methodology, linking vision and language processing.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Knowledge-Augmented Prompts",
        "canonical": "Knowledge-Augmented Prompts",
        "aliases": [
          "Knowledge-Enhanced Prompts"
        ],
        "category": "unique_technical",
        "rationale": "This concept is a novel approach in the paper, enhancing model performance with structured knowledge.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Prompt-Learning Framework",
        "canonical": "Prompt-Learning",
        "aliases": [
          "Prompt-Based Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Prompt-Learning is a key technique used in the paper, facilitating the integration of textual descriptions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Robotic Action Recognition",
        "canonical": "Robotic Action Recognition",
        "aliases": [
          "Robot Action Detection"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific application of the research, linking robotics and action recognition.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Knowledge-Augmented Prompts",
      "resolved_canonical": "Knowledge-Augmented Prompts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Prompt-Learning Framework",
      "resolved_canonical": "Prompt-Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Robotic Action Recognition",
      "resolved_canonical": "Robotic Action Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16452.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16452](https://arxiv.org/abs/2509.16452)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (84.6% similar)
- [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (84.5% similar)
- [[2025-09-19/CollabVLA_ Self-Reflective Vision-Language-Action Model Dreaming Together with Human_20250919|CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human]] (82.8% similar)
- [[2025-09-18/GeoAware-VLA_ Implicit Geometry Aware Vision-Language-Action Model_20250918|GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model]] (82.6% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Prompt-Learning|Prompt-Learning]]
**âš¡ Unique Technical**: [[keywords/Knowledge-Augmented Prompts|Knowledge-Augmented Prompts]], [[keywords/Robotic Action Recognition|Robotic Action Recognition]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16452v1 Announce Type: cross 
Abstract: Accurate vision-based action recognition is crucial for developing autonomous robots that can operate safely and reliably in complex, real-world environments. In this work, we advance video-based recognition of indoor daily actions for robotic perception by leveraging vision-language models (VLMs) enriched with domain-specific knowledge. We adapt a prompt-learning framework in which class-level textual descriptions of each action are embedded as learnable prompts into a frozen pre-trained VLM backbone. Several strategies for structuring and encoding these textual descriptions are designed and evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our method, using only RGB video inputs at test time, achieves over 95\% accuracy and outperforms state-of-the-art approaches. These results highlight the effectiveness of knowledge-augmented prompts in enabling robust action recognition with minimal supervision.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ììœ¨ ë¡œë´‡ì˜ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë™ì‘ì„ ìœ„í•´ ì‹¤ë‚´ ì¼ìƒ í–‰ë™ì„ ì¸ì‹í•˜ëŠ” ë¹„ë””ì˜¤ ê¸°ë°˜ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì„ ì‚¬ìš©í•˜ì—¬ ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì„ í™œìš©í•˜ê³ , í”„ë¡¬í”„íŠ¸ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ê° í–‰ë™ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ í•™ìŠµ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¡œ VLMì— ì‚½ì…í•©ë‹ˆë‹¤. ETRI-Activity3D ë°ì´í„°ì…‹ ì‹¤í—˜ì—ì„œ RGB ë¹„ë””ì˜¤ ì…ë ¥ë§Œìœ¼ë¡œ 95% ì´ìƒì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ê¸°ì¡´ ìµœì²¨ë‹¨ ë°©ë²•ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì§€ì‹ì´ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ìµœì†Œí•œì˜ ê°ë…ìœ¼ë¡œë„ ê°•ë ¥í•œ í–‰ë™ ì¸ì‹ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì„ í™œìš©í•˜ì—¬ ë¡œë´‡ì˜ ì‹¤ë‚´ ì¼ìƒ í–‰ë™ ì¸ì‹ì„ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤.
- 2. í”„ë¡¬í”„íŠ¸ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì ìš©í•˜ì—¬ ê° í–‰ë™ì˜ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ í•™ìŠµ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¡œ VLMì— ì„ë² ë”©í•˜ì˜€ìŠµë‹ˆë‹¤.
- 3. ETRI-Activity3D ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, RGB ë¹„ë””ì˜¤ ì…ë ¥ë§Œìœ¼ë¡œ 95% ì´ìƒì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
- 4. ì§€ì‹ì´ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ìµœì†Œí•œì˜ ê°ë…ìœ¼ë¡œ ê°•ë ¥í•œ í–‰ë™ ì¸ì‹ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:20:32*