---
keywords:
  - Event Cameras
  - State Space Models
  - Path-selective Event Aggregation and Scan
  - Multi-faceted Selection Guiding
  - Spatiotemporal Event Modeling
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2409.16953
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:15:56.514626",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Event Cameras",
    "State Space Models",
    "Path-selective Event Aggregation and Scan",
    "Multi-faceted Selection Guiding",
    "Spatiotemporal Event Modeling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Event Cameras": 0.78,
    "State Space Models": 0.72,
    "Path-selective Event Aggregation and Scan": 0.81,
    "Multi-faceted Selection Guiding": 0.79,
    "Spatiotemporal Event Modeling": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Event Cameras",
        "canonical": "Event Cameras",
        "aliases": [
          "Neuromorphic Cameras"
        ],
        "category": "unique_technical",
        "rationale": "Event cameras are a key component of the study, offering a unique approach to capturing high temporal resolution data.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "State Space Models",
        "canonical": "State Space Models",
        "aliases": [
          "SSMs"
        ],
        "category": "broad_technical",
        "rationale": "State Space Models are central to the proposed framework, enabling adaptive encoding of event features.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Path-selective Event Aggregation and Scan",
        "canonical": "Path-selective Event Aggregation and Scan",
        "aliases": [
          "PEAS"
        ],
        "category": "unique_technical",
        "rationale": "The PEAS module is a novel component of the framework, crucial for encoding events into fixed dimensions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.81
      },
      {
        "surface": "Multi-faceted Selection Guiding",
        "canonical": "Multi-faceted Selection Guiding",
        "aliases": [
          "MSG"
        ],
        "category": "unique_technical",
        "rationale": "MSG loss is a novel technique introduced to minimize randomness and redundancy in feature encoding.",
        "novelty_score": 0.78,
        "connectivity_score": 0.58,
        "specificity_score": 0.86,
        "link_intent_score": 0.79
      },
      {
        "surface": "Spatiotemporal Event Modeling",
        "canonical": "Spatiotemporal Event Modeling",
        "aliases": [
          "Spatiotemporal Modeling"
        ],
        "category": "specific_connectable",
        "rationale": "Spatiotemporal event modeling is essential for understanding the temporal dynamics in event-based recognition.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Event Cameras",
      "resolved_canonical": "Event Cameras",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "State Space Models",
      "resolved_canonical": "State Space Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Path-selective Event Aggregation and Scan",
      "resolved_canonical": "Path-selective Event Aggregation and Scan",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Multi-faceted Selection Guiding",
      "resolved_canonical": "Multi-faceted Selection Guiding",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.58,
        "specificity": 0.86,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Spatiotemporal Event Modeling",
      "resolved_canonical": "Spatiotemporal Event Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# PASS: Path-selective State Space Model for Event-based Recognition

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2409.16953.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2409.16953](https://arxiv.org/abs/2409.16953)

## 🔗 유사한 논문
- [[2025-09-23/Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation_20250923|Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation]] (82.6% similar)
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (82.2% similar)
- [[2025-09-19/Depth AnyEvent_ A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation_20250919|Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation]] (82.0% similar)
- [[2025-09-23/ST-GS_ Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting_20250923|ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting]] (80.2% similar)
- [[2025-09-23/Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation_20250923|Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation]] (80.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/State Space Models|State Space Models]]
**🔗 Specific Connectable**: [[keywords/Spatiotemporal Event Modeling|Spatiotemporal Event Modeling]]
**⚡ Unique Technical**: [[keywords/Event Cameras|Event Cameras]], [[keywords/Path-selective Event Aggregation and Scan|Path-selective Event Aggregation and Scan]], [[keywords/Multi-faceted Selection Guiding|Multi-faceted Selection Guiding]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2409.16953v2 Announce Type: replace 
Abstract: Event cameras are bio-inspired sensors that capture intensity changes asynchronously with distinct advantages, such as high temporal resolution. Existing methods for event-based object/action recognition predominantly sample and convert event representation at every fixed temporal interval (or frequency). However, they are constrained to processing a limited number of event lengths and show poor frequency generalization, thus not fully leveraging the event's high temporal resolution. In this paper, we present our PASS framework, exhibiting superior capacity for spatiotemporal event modeling towards a larger number of event lengths and generalization across varying inference temporal frequencies. Our key insight is to learn adaptively encoded event features via the state space models (SSMs), whose linear complexity and generalization on input frequency make them ideal for processing high temporal resolution events. Specifically, we propose a Path-selective Event Aggregation and Scan (PEAS) module to encode events into features with fixed dimensions by adaptively scanning and selecting aggregated event presentations. On top of it, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to minimize the randomness and redundancy of the encoded features during the PEAS selection process. Our method outperforms prior methods on five public datasets and shows strong generalization across varying inference frequencies with less accuracy drop (ours -8.62% vs. -20.69% for the baseline). Overall, PASS exhibits strong long spatiotemporal modeling for a broader distribution of event length (1-10^9), precise temporal perception, and generalization for real-world

## 📝 요약

이 논문에서는 이벤트 카메라의 높은 시간 해상도를 효과적으로 활용하기 위한 PASS 프레임워크를 제안합니다. 기존 방법은 고정된 시간 간격으로 이벤트를 처리하여 시간 해상도를 충분히 활용하지 못했습니다. PASS는 상태 공간 모델(SSM)을 사용하여 다양한 이벤트 길이와 주파수에 일반화할 수 있는 적응형 인코딩을 학습합니다. 특히, Path-selective Event Aggregation and Scan (PEAS) 모듈을 통해 이벤트를 고정된 차원의 특징으로 인코딩하고, Multi-faceted Selection Guiding (MSG) 손실을 도입하여 인코딩 과정의 무작위성과 중복성을 최소화합니다. 이 방법은 5개의 공개 데이터셋에서 기존 방법보다 우수한 성능을 보였으며, 다양한 추론 주파수에서도 강력한 일반화 능력을 입증했습니다. PASS는 넓은 범위의 이벤트 길이(1-10^9)에 대한 강력한 시공간 모델링과 정확한 시간 인식을 제공합니다.

## 🎯 주요 포인트

- 1. PASS 프레임워크는 다양한 이벤트 길이와 추론 주파수에 대한 일반화를 통해 뛰어난 시공간 이벤트 모델링 능력을 보여줍니다.
- 2. 상태 공간 모델(SSM)을 사용하여 적응적으로 인코딩된 이벤트 특징을 학습하여 높은 시간 해상도의 이벤트를 효과적으로 처리합니다.
- 3. PEAS 모듈은 이벤트를 고정된 차원의 특징으로 인코딩하며, MSG 손실을 도입하여 인코딩된 특징의 무작위성과 중복성을 최소화합니다.
- 4. 제안된 방법은 5개의 공개 데이터셋에서 기존 방법을 능가하며, 다양한 추론 주파수에서도 정확도 저하가 적습니다.
- 5. PASS는 넓은 범위의 이벤트 길이(1-10^9)에 대한 강력한 시공간 모델링과 정밀한 시간 인식을 제공합니다.


---

*Generated on 2025-09-24 05:15:56*