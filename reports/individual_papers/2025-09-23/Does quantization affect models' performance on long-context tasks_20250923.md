---
keywords:
  - Large Language Model
  - Quantization
  - Long-Context Tasks
  - BNB-nf4
  - Llama-3.1
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.20276
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:02:39.973253",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Quantization",
    "Long-Context Tasks",
    "BNB-nf4",
    "Llama-3.1"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Quantization": 0.88,
    "Long-Context Tasks": 0.82,
    "BNB-nf4": 0.8,
    "Llama-3.1": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect with numerous other concepts in NLP and machine learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Quantization",
        "canonical": "Quantization",
        "aliases": [
          "Quantized models"
        ],
        "category": "unique_technical",
        "rationale": "Quantization is a key technique discussed in the paper, affecting model performance and resource efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Long-context tasks",
        "canonical": "Long-Context Tasks",
        "aliases": [
          "Long-form tasks"
        ],
        "category": "unique_technical",
        "rationale": "Long-context tasks are a specific focus of the paper, highlighting challenges and performance impacts.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "BNB-nf4",
        "canonical": "BNB-nf4",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "BNB-nf4 is a specific quantization method evaluated in the study, relevant for understanding performance impacts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Llama-3.1",
        "canonical": "Llama-3.1",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Llama-3.1 is one of the models evaluated, providing insights into model-specific quantization effects.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "memory requirements",
      "inference latency",
      "performance drop"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Quantization",
      "resolved_canonical": "Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Long-context tasks",
      "resolved_canonical": "Long-Context Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "BNB-nf4",
      "resolved_canonical": "BNB-nf4",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Llama-3.1",
      "resolved_canonical": "Llama-3.1",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Does quantization affect models' performance on long-context tasks?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.20276.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.20276](https://arxiv.org/abs/2505.20276)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (88.9% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (85.4% similar)
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (84.7% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (83.1% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (82.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Quantization|Quantization]], [[keywords/Long-Context Tasks|Long-Context Tasks]], [[keywords/BNB-nf4|BNB-nf4]], [[keywords/Llama-3.1|Llama-3.1]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.20276v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long-context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and for languages other than English.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì–‘ìí™”ê°€ ê¸´ ì…ë ¥(64K í† í° ì´ìƒ)ê³¼ ê¸´ ì¶œë ¥ ì‘ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤. 9,700ê°œì˜ í…ŒìŠ¤íŠ¸ ì˜ˆì œì™€ 5ê°€ì§€ ì–‘ìí™” ë°©ë²•, 5ê°€ì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, 8ë¹„íŠ¸ ì–‘ìí™”ëŠ” ì •í™•ë„ë¥¼ ê±°ì˜ ìœ ì§€í•˜ì§€ë§Œ(~0.8% ê°ì†Œ), 4ë¹„íŠ¸ ë°©ë²•ì€ íŠ¹íˆ ê¸´ ë¬¸ë§¥ ì…ë ¥ì—ì„œ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤(ìµœëŒ€ 59% ê°ì†Œ). ë˜í•œ, ë¹„ì˜ì–´ ì…ë ¥ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ê°€ ë”ìš± ì‹¬í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì–‘ìí™”ì˜ íš¨ê³¼ëŠ” ë°©ë²•, ëª¨ë¸, ì‘ì—…ì— ë”°ë¼ í¬ê²Œ ë‹¬ë¼ì§€ë©°, ì˜ˆë¥¼ ë“¤ì–´ Qwen-2.5 72BëŠ” BNB-nf4ì—ì„œ ê°•ë ¥í•œ ë°˜ë©´, Llama-3.1 70BëŠ” ë™ì¼ ì‘ì—…ì—ì„œ 32% ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì–‘ìí™”ëœ LLMì„ ë°°í¬í•˜ê¸° ì „ì— ì‘ì—…ë³„ë¡œ ì‹ ì¤‘í•œ í‰ê°€ê°€ í•„ìš”í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì–‘ìí™”ëŠ” ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ê³¼ ì¶”ë¡  ì§€ì—°ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
- 2. 8ë¹„íŠ¸ ì–‘ìí™”ëŠ” í‰ê· ì ìœ¼ë¡œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ì§€ë§Œ, 4ë¹„íŠ¸ ì–‘ìí™”ëŠ” íŠ¹íˆ ê¸´ ë¬¸ë§¥ ì…ë ¥ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•œë‹¤.
- 3. ì–‘ìí™”ì˜ íš¨ê³¼ëŠ” ì–‘ìí™” ë°©ë²•, ëª¨ë¸, ì‘ì—…ì— í¬ê²Œ ì˜ì¡´í•˜ë©°, íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ì‹ ì¤‘í•œ í‰ê°€ê°€ í•„ìš”í•˜ë‹¤.
- 4. Qwen-2.5 72B ëª¨ë¸ì€ BNB-nf4 ì–‘ìí™”ì—ì„œë„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ì§€ë§Œ, Llama-3.1 70B ëª¨ë¸ì€ ë™ì¼í•œ ì‘ì—…ì—ì„œ 32%ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªëŠ”ë‹¤.
- 5. ì˜ì–´ ì™¸ì˜ ì–¸ì–´ë¡œ ì…ë ¥ë  ë•Œ ì„±ëŠ¥ ì €í•˜ê°€ ë”ìš± ì‹¬í™”ë  ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-24 01:02:39*