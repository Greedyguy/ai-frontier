---
keywords:
  - Audio-Reasoner
  - Multimodal Learning
  - Audio Modality
  - CoTA Dataset
  - Chain of Thought Process
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2503.02318
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:49:30.890692",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Audio-Reasoner",
    "Multimodal Learning",
    "Audio Modality",
    "CoTA Dataset",
    "Chain of Thought Process"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Audio-Reasoner": 0.78,
    "Multimodal Learning": 0.82,
    "Audio Modality": 0.75,
    "CoTA Dataset": 0.77,
    "Chain of Thought Process": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Audio-Reasoner",
        "canonical": "Audio-Reasoner",
        "aliases": [
          "Audio Reasoner"
        ],
        "category": "unique_technical",
        "rationale": "As a novel model specifically designed for audio reasoning, it represents a unique contribution to the field.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "multimodal reasoning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Links to existing work on integrating multiple data modalities, enhancing connectivity with related research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "audio modality",
        "canonical": "Audio Modality",
        "aliases": [
          "audio data",
          "audio input"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specific data type that is central to the paper's contributions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "CoTA",
        "canonical": "CoTA Dataset",
        "aliases": [
          "CoTA"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific dataset introduced in the paper, crucial for replicating and understanding the research.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "structured CoT process",
        "canonical": "Chain of Thought Process",
        "aliases": [
          "CoT process",
          "structured CoT"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to methodologies in reasoning and problem-solving, relevant to the paper's approach.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Audio-Reasoner",
      "resolved_canonical": "Audio-Reasoner",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multimodal reasoning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "audio modality",
      "resolved_canonical": "Audio Modality",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "CoTA",
      "resolved_canonical": "CoTA Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "structured CoT process",
      "resolved_canonical": "Chain of Thought Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.02318.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2503.02318](https://arxiv.org/abs/2503.02318)

## 🔗 유사한 논문
- [[2025-09-22/SightSound-R1_ Cross-Modal Reasoning Distillation from Vision to Audio Language Models_20250922|SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models]] (86.3% similar)
- [[2025-09-23/AuditoryBench++_ Can Language Models Understand Auditory Knowledge without Hearing?_20250923|AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?]] (86.1% similar)
- [[2025-09-18/Omni-CLST_ Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering_20250918|Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (86.1% similar)
- [[2025-09-18/Spatial Audio Motion Understanding and Reasoning_20250918|Spatial Audio Motion Understanding and Reasoning]] (85.1% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (83.2% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Chain of Thought Process|Chain of Thought Process]]
**⚡ Unique Technical**: [[keywords/Audio-Reasoner|Audio-Reasoner]], [[keywords/Audio Modality|Audio Modality]], [[keywords/CoTA Dataset|CoTA Dataset]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2503.02318v2 Announce Type: replace-cross 
Abstract: Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse multi-task audio dataset with simple annotations. Then, we leverage closed-source models to conduct secondary labeling, QA generation, along with structured COT process. These datasets together form a high-quality reasoning dataset with 1.2 million reasoning-rich samples, which we name CoTA. Following inference scaling principles, we train Audio-Reasoner on CoTA, enabling it to achieve great logical capabilities in audio reasoning. Experiments show state-of-the-art performance across key benchmarks, including MMAU-mini (+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our findings stress the core of structured CoT training in advancing audio reasoning.

## 📝 요약

최근 멀티모달 추론 연구에서 오디오 모달리티가 주목받지 못한 점을 개선하기 위해, 우리는 오디오 태스크에서 심층 추론을 수행할 수 있는 대규모 오디오 언어 모델인 Audio-Reasoner를 소개합니다. 간단한 주석으로 구성된 대규모 멀티태스크 오디오 데이터셋을 신중하게 구축하고, 비공개 모델을 활용해 2차 레이블링, QA 생성, 구조화된 CoT 프로세스를 수행했습니다. 이렇게 생성된 CoTA 데이터셋은 120만 개의 추론 샘플을 포함하며, Audio-Reasoner는 이를 기반으로 학습하여 오디오 추론에서 뛰어난 논리적 능력을 발휘합니다. 실험 결과, MMAU-mini, AIR-Bench, MELD 등 주요 벤치마크에서 최첨단 성능을 기록했습니다. 연구는 구조화된 CoT 훈련이 오디오 추론 발전에 핵심임을 강조합니다.

## 🎯 주요 포인트

- 1. 최근 멀티모달 추론 연구에서 오디오 모달리티가 간과되고 있는 문제를 해결하기 위해 Audio-Reasoner라는 대규모 오디오 언어 모델을 소개합니다.
- 2. 간단한 주석이 포함된 대규모 멀티태스크 오디오 데이터셋을 신중하게 구성하고, 비공개 모델을 활용하여 2차 레이블링 및 QA 생성, 구조화된 COT 프로세스를 수행했습니다.
- 3. CoTA라는 이름의 고품질 추론 데이터셋을 구축하여 Audio-Reasoner를 훈련시킴으로써 오디오 추론에서 뛰어난 논리적 능력을 발휘할 수 있게 했습니다.
- 4. 실험 결과, MMAU-mini, AIR-Bench chat/foundation, MELD 등 주요 벤치마크에서 최첨단 성능을 달성했습니다.
- 5. 구조화된 CoT 훈련이 오디오 추론 발전에 핵심적이라는 점을 강조합니다.


---

*Generated on 2025-09-24 00:49:30*