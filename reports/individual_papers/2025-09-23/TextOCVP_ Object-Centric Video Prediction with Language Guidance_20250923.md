---
keywords:
  - Object-Centric Video Prediction
  - Transformer
  - Vision-Language Model
  - Structured Latent Space
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2502.11655
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:16:52.839446",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Object-Centric Video Prediction",
    "Transformer",
    "Vision-Language Model",
    "Structured Latent Space"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Object-Centric Video Prediction": 0.78,
    "Transformer": 0.85,
    "Vision-Language Model": 0.8,
    "Structured Latent Space": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Object-Centric Video Prediction",
        "canonical": "Object-Centric Video Prediction",
        "aliases": [
          "OCVP"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach that combines object-centric models with video prediction, which is central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Text-Conditioned Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Text-Conditioned Transformer"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a fundamental component of the proposed model, linking to broader technical concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "The integration of textual descriptions with visual data aligns with the evolving concept of vision-language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Structured Latent Space",
        "canonical": "Structured Latent Space",
        "aliases": [
          "Latent Space"
        ],
        "category": "specific_connectable",
        "rationale": "Structured latent spaces are crucial for understanding the model's ability to control and predict object dynamics.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "autonomous agents",
      "complex environments",
      "video frames"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Object-Centric Video Prediction",
      "resolved_canonical": "Object-Centric Video Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Text-Conditioned Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Structured Latent Space",
      "resolved_canonical": "Structured Latent Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# TextOCVP: Object-Centric Video Prediction with Language Guidance

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2502.11655.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2502.11655](https://arxiv.org/abs/2502.11655)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (81.4% similar)
- [[2025-09-22/Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge_ 3rd Place Solution_20250922|Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution]] (80.8% similar)
- [[2025-09-23/Text-Scene_ A Scene-to-Language Parsing Framework for 3D Scene Understanding_20250923|Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding]] (80.6% similar)
- [[2025-09-23/VidCLearn_ A Continual Learning Approach for Text-to-Video Generation_20250923|VidCLearn: A Continual Learning Approach for Text-to-Video Generation]] (80.2% similar)
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Structured Latent Space|Structured Latent Space]]
**âš¡ Unique Technical**: [[keywords/Object-Centric Video Prediction|Object-Centric Video Prediction]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.11655v2 Announce Type: replace 
Abstract: Understanding and forecasting future scene states is critical for autonomous agents to plan and act effectively in complex environments. Object-centric models, with structured latent spaces, have shown promise in modeling object dynamics and predicting future scene states, but often struggle to scale beyond simple synthetic datasets and to integrate external guidance, limiting their applicability in robotics. To address these limitations, we propose TextOCVP, an object-centric model for video prediction guided by textual descriptions. TextOCVP parses an observed scene into object representations, called slots, and utilizes a text-conditioned transformer predictor to forecast future object states and video frames. Our approach jointly models object dynamics and interactions while incorporating textual guidance, enabling accurate and controllable predictions. TextOCVP's structured latent space offers a more precise control of the forecasting process, outperforming several video prediction baselines on two datasets. Additionally, we show that structured object-centric representations provide superior robustness to novel scene configurations, as well as improved controllability and interpretability, enabling more precise and understandable predictions. Videos and code are available at https://play-slot.github.io/TextOCVP.

## ğŸ“ ìš”ì•½

TextOCVPëŠ” í…ìŠ¤íŠ¸ ì„¤ëª…ì„ í™œìš©í•œ ê°ì²´ ì¤‘ì‹¬ì˜ ë¹„ë””ì˜¤ ì˜ˆì¸¡ ëª¨ë¸ë¡œ, ë³µì¡í•œ í™˜ê²½ì—ì„œì˜ ê°ì²´ ë™ì—­í•™ê³¼ ìƒí˜¸ì‘ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê´€ì°°ëœ ì¥ë©´ì„ ê°ì²´ í‘œí˜„ìœ¼ë¡œ ë¶„í•´í•˜ê³ , í…ìŠ¤íŠ¸ë¡œ ì¡°ê±´í™”ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ì˜ˆì¸¡ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ì˜ ê°ì²´ ìƒíƒœì™€ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. TextOCVPëŠ” êµ¬ì¡°í™”ëœ ì ì¬ ê³µê°„ì„ í†µí•´ ì˜ˆì¸¡ ê³¼ì •ì„ ë” ì •í™•í•˜ê²Œ ì œì–´í•˜ë©°, ë‘ ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ ë¹„ë””ì˜¤ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ ì¥ë©´ êµ¬ì„±ì— ëŒ€í•œ ê°•ì¸ì„±ê³¼ ì˜ˆì¸¡ì˜ ì œì–´ ê°€ëŠ¥ì„± ë° í•´ì„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TextOCVPëŠ” í…ìŠ¤íŠ¸ ì„¤ëª…ì„ í†µí•´ ë¹„ë””ì˜¤ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê°ì²´ ì¤‘ì‹¬ ëª¨ë¸ë¡œ, ê°ì²´ ë™ì—­í•™ê³¼ ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§í•˜ì—¬ ì •í™•í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 2. ì´ ëª¨ë¸ì€ ê´€ì°°ëœ ì¥ë©´ì„ ê°ì²´ í‘œí˜„ì¸ ìŠ¬ë¡¯ìœ¼ë¡œ íŒŒì‹±í•˜ê³ , í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ íŠ¸ëœìŠ¤í¬ë¨¸ ì˜ˆì¸¡ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ê°ì²´ ìƒíƒœì™€ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
- 3. TextOCVPëŠ” êµ¬ì¡°í™”ëœ ì ì¬ ê³µê°„ì„ í†µí•´ ì˜ˆì¸¡ ê³¼ì •ì„ ë³´ë‹¤ ì •ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆìœ¼ë©°, ë‘ ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ì—¬ëŸ¬ ë¹„ë””ì˜¤ ì˜ˆì¸¡ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 4. êµ¬ì¡°í™”ëœ ê°ì²´ ì¤‘ì‹¬ í‘œí˜„ì€ ìƒˆë¡œìš´ ì¥ë©´ êµ¬ì„±ì— ëŒ€í•œ ë›°ì–´ë‚œ ê²¬ê³ ì„±ì„ ì œê³µí•˜ë©°, ì˜ˆì¸¡ì˜ ì œì–´ ê°€ëŠ¥ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:16:52*