---
keywords:
  - Diffusion Large Language Models
  - Speculative Decoding
  - Directed Draft Graph
  - Key-Value Caching
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.18085
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:20:27.792692",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Large Language Models",
    "Speculative Decoding",
    "Directed Draft Graph",
    "Key-Value Caching"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Large Language Models": 0.78,
    "Speculative Decoding": 0.8,
    "Directed Draft Graph": 0.75,
    "Key-Value Caching": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion LLMs",
        "canonical": "Diffusion Large Language Models",
        "aliases": [
          "dLLMs"
        ],
        "category": "unique_technical",
        "rationale": "This represents a novel approach in language model architecture that is distinct from autoregressive models.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific decoding strategy that enhances the performance of language models, relevant for linking with decoding techniques.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Directed Draft Graph",
        "canonical": "Directed Draft Graph",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel graph structure proposed for optimizing language model generation, useful for linking with graph-based methods.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      },
      {
        "surface": "KV-caching",
        "canonical": "Key-Value Caching",
        "aliases": [
          "KV-caching"
        ],
        "category": "specific_connectable",
        "rationale": "A known technique for improving model efficiency, relevant for linking with memory optimization strategies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "autoregressive LLMs",
      "token generation rates",
      "output quality"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion LLMs",
      "resolved_canonical": "Diffusion Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Directed Draft Graph",
      "resolved_canonical": "Directed Draft Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "KV-caching",
      "resolved_canonical": "Key-Value Caching",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18085.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.18085](https://arxiv.org/abs/2509.18085)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (88.0% similar)
- [[2025-09-22/Discrete Diffusion in Large Language and Multimodal Models_ A Survey_20250922|Discrete Diffusion in Large Language and Multimodal Models: A Survey]] (83.8% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (83.3% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (81.8% similar)
- [[2025-09-18/Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning_20250918|Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Key-Value Caching|Key-Value Caching]]
**âš¡ Unique Technical**: [[keywords/Diffusion Large Language Models|Diffusion Large Language Models]], [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Directed Draft Graph|Directed Draft Graph]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18085v1 Announce Type: cross 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” Spiffyë¼ëŠ” ì¶”ì¸¡ ë””ì½”ë”© ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬ í™•ì‚° ê¸°ë°˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(dLLM)ì˜ ì¶”ë¡  ì†ë„ë¥¼ 2.8~3.1ë°° ê°€ì†í™”í•˜ë©´ì„œë„ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. SpiffyëŠ” dLLMì˜ ë¶„í¬ë¥¼ í™œìš©í•˜ì—¬ ìë™ ì¶”ì¸¡ ë°©ì‹ìœ¼ë¡œ ì´ˆì•ˆ ìƒíƒœë¥¼ ìƒì„±í•˜ë©°, ë…ë¦½ì ì¸ ì´ˆì•ˆ ëª¨ë¸ì˜ í›ˆë ¨ ë° ì‹¤í–‰ ë¶€ë‹´ì„ ì œê±°í•©ë‹ˆë‹¤. ë˜í•œ, ì–‘ë°©í–¥ ë¸”ë¡ ë°©ì‹ì˜ dLLM ìƒì„± íŠ¹ì„±ì„ í™œìš©í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ë°©í–¥ì„± ì´ˆì•ˆ ê·¸ë˜í”„ë¥¼ ì œì•ˆí•˜ê³ , ì´ë¥¼ ë³‘ë ¬ë¡œ ê²€ì¦í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ì˜¤í”„ë¼ì¸ ë³´ì • ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì´ˆì•ˆ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ìµœì í™”í•˜ì—¬ ìˆ˜ìš©ë¥ ì„ ë†’ì´ê³ , ì‹œìŠ¤í…œì˜ ì „ì²´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. SpiffyëŠ” KV-ìºì‹± ë° ë‹¤ì¤‘ í† í° ì–¸ë§ˆìŠ¤í‚¹ê³¼ ê°™ì€ ìµœì‹  ê¸°ë²•ê³¼ë„ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ìµœëŒ€ 7.9ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SpiffyëŠ” dLLM ì¶”ë¡  ì†ë„ë¥¼ 2.8-3.1ë°° ê°€ì†í™”í•˜ë©´ì„œ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ë³´ì¡´í•˜ëŠ” ì¶”ë¡  ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 2. SpiffyëŠ” dLLMì˜ ë¶„í¬ë¥¼ í™œìš©í•˜ì—¬ ìë™ ì¶”ë¡  ë°©ì‹ìœ¼ë¡œ ì´ˆì•ˆ ìƒíƒœë¥¼ ì œì•ˆí•˜ë©°, ë…ë¦½ì ì¸ ì´ˆì•ˆ ëª¨ë¸ì˜ í›ˆë ¨ ë° ì‹¤í–‰ ì˜¤ë²„í—¤ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©í–¥ì„± ì´ˆì•ˆ ê·¸ë˜í”„ëŠ” dLLM ìƒì„±ì˜ ì–‘ë°©í–¥, ë¸”ë¡ ë‹¨ìœ„ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë³‘ë ¬ ê²€ì¦ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ì˜¤í”„ë¼ì¸ ë³´ì • ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì´ˆì•ˆ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ìµœì í™”í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì „ì²´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. SpiffyëŠ” KV-ìºì‹± ë° ë‹¤ì¤‘ í† í° ì–¸ë§ˆìŠ¤í‚¹ê³¼ ê°™ì€ dLLM ìƒì„± ì†ë„ ê°œì„  í˜ì‹ ê³¼ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ìµœëŒ€ 7.9ë°°ì˜ ì´ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:20:27*