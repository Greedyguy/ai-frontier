---
keywords:
  - Neuro-inspired Communication
  - Sparse Neural Networks
  - Hebbian Learning
  - Dynamic Sparse Training
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.14140
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:20:30.408666",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neuro-inspired Communication",
    "Sparse Neural Networks",
    "Hebbian Learning",
    "Dynamic Sparse Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neuro-inspired Communication": 0.78,
    "Sparse Neural Networks": 0.81,
    "Hebbian Learning": 0.77,
    "Dynamic Sparse Training": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neuro-inspired Ensemble-to-Ensemble Communication",
        "canonical": "Neuro-inspired Communication",
        "aliases": [
          "Ensemble Communication",
          "Neuro-inspired Connectivity"
        ],
        "category": "unique_technical",
        "rationale": "This concept is novel and specific to the paper, offering a unique perspective on ANN design inspired by biological systems.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sparse and Efficient ANNs",
        "canonical": "Sparse Neural Networks",
        "aliases": [
          "Efficient ANNs",
          "Sparse ANNs"
        ],
        "category": "specific_connectable",
        "rationale": "Sparsity in neural networks is a key concept for improving efficiency and is highly relevant to current research trends.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.76,
        "link_intent_score": 0.81
      },
      {
        "surface": "Hebbian-inspired rewiring rule",
        "canonical": "Hebbian Learning",
        "aliases": [
          "Hebbian Rewiring",
          "Hebbian Rule"
        ],
        "category": "specific_connectable",
        "rationale": "Hebbian principles are foundational in neuroscience and provide a strong link to biologically inspired learning mechanisms.",
        "novelty_score": 0.67,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Dynamic Sparse Training",
        "canonical": "Dynamic Sparse Training",
        "aliases": [
          "DST",
          "Sparse Training"
        ],
        "category": "unique_technical",
        "rationale": "This training mechanism is a novel approach to optimizing neural networks by dynamically adjusting sparsity.",
        "novelty_score": 0.78,
        "connectivity_score": 0.71,
        "specificity_score": 0.81,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "ANN",
      "model",
      "accuracy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neuro-inspired Ensemble-to-Ensemble Communication",
      "resolved_canonical": "Neuro-inspired Communication",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sparse and Efficient ANNs",
      "resolved_canonical": "Sparse Neural Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.76,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Hebbian-inspired rewiring rule",
      "resolved_canonical": "Hebbian Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.67,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Dynamic Sparse Training",
      "resolved_canonical": "Dynamic Sparse Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.71,
        "specificity": 0.81,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.14140.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.14140](https://arxiv.org/abs/2508.14140)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Brain-HGCN_ A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis_20250919|Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis]] (85.4% similar)
- [[2025-09-22/Schreier-Coset Graph Propagation_20250922|Schreier-Coset Graph Propagation]] (82.7% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (82.5% similar)
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering]] (82.4% similar)
- [[2025-09-18/GraphTorque_ Torque-Driven Rewiring Graph Neural Network_20250918|GraphTorque: Torque-Driven Rewiring Graph Neural Network]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Sparse Neural Networks|Sparse Neural Networks]], [[keywords/Hebbian Learning|Hebbian Learning]]
**âš¡ Unique Technical**: [[keywords/Neuro-inspired Communication|Neuro-inspired Communication]], [[keywords/Dynamic Sparse Training|Dynamic Sparse Training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.14140v2 Announce Type: replace-cross 
Abstract: The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìƒë¬¼í•™ì  ì‹ ê²½ íšŒë¡œì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ì¸ê³µì§€ëŠ¥ ì‹ ê²½ë§(ANN) ì„¤ê³„ì— ì ìš©í•˜ëŠ” ì—°êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, ì¥ì˜ ì‹œê° í”¼ì§ˆì—ì„œ ê´€ì°°ëœ ê¸°ëŠ¥ì  ì—°ê²° íŒ¨í„´ì„ í™œìš©í•˜ì—¬ ANN ì„¤ê³„ì— ë°˜ì˜í•œ G2GNetì´ë¼ëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. G2GNetì€ í¬ì†Œí•˜ê³  ëª¨ë“ˆí™”ëœ ì—°ê²°ì„±ì„ ë„ì…í•˜ì—¬, ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œë„ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ë˜í•œ, ë™ì  í¬ì†Œ í›ˆë ¨(DST) ë©”ì»¤ë‹ˆì¦˜ê³¼ ìƒë¬¼í•™ì  ê°€ì†Œì„± ì›ë¦¬ì— ê¸°ë°˜í•œ Hebbian ì¬ì—°ê²° ê·œì¹™ì„ ì œì•ˆí•˜ì—¬, Fashion-MNIST, CIFAR-10, CIFAR-100 ë“±ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 75%ì˜ í¬ì†Œì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì •í™•ë„ë¥¼ ìµœëŒ€ 4.3% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ë°€ì§‘ ëª¨ë¸ë³´ë‹¤ ì ì€ ê³„ì‚°ìœ¼ë¡œë„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìƒë¬¼í•™ì  ì‹ ê²½ íšŒë¡œì˜ êµ¬ì¡°ëŠ” ì¸ê³µ ì‹ ê²½ë§ ì„¤ê³„ì— ìœ ìš©í•œ í†µì°°ì„ ì œê³µí•˜ë©°, íŠ¹íˆ ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šê³  ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ì¤‘ìš”í•˜ë‹¤.
- 2. G2GNetì€ ìƒë¬¼í•™ì ìœ¼ë¡œ ê´€ì°°ëœ ê¸°ëŠ¥ì  ì—°ê²° íŒ¨í„´ì„ êµ¬ì¡°ì  í¸í–¥ìœ¼ë¡œ ë„ì…í•œ ìµœì´ˆì˜ ì¸ê³µ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì´ë‹¤.
- 3. G2GNetì€ í”¼ë“œí¬ì›Œë“œ ê³„ì¸µ ì „ë°˜ì— ê±¸ì³ í¬ì†Œí•˜ê³  ëª¨ë“ˆì‹ ì—°ê²°ì„±ì„ ë¶€ê³¼í•˜ì—¬, ì ì€ ë§¤ê°œë³€ìˆ˜ë¡œë„ í‘œì¤€ ë¹„ì „ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤.
- 4. ë™ì  í¬ì†Œ í›ˆë ¨ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í›ˆë ¨ ì¤‘ ì—£ì§€ë¥¼ ê°€ì§€ì¹˜ê¸°í•˜ê³  ì¬ì„±ì¥ì‹œí‚¤ë©°, ìƒë¬¼í•™ì  ê°€ì†Œì„± ì›ì¹™ì— ê¸°ë°˜í•œ Hebbian ì˜ê°ì„ ë°›ì€ ì¬ë°°ì„  ê·œì¹™ì„ ì œì•ˆí•œë‹¤.
- 5. G2GNetì€ Fashion-MNIST, CIFAR-10, CIFAR-100 ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 75%ì˜ í¬ì†Œì„±ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ì •í™•ë„ë¥¼ ìµœëŒ€ 4.3% í–¥ìƒì‹œì¼œ, ì¡°ë°€í•œ ê¸°ì¤€ì„ ë³´ë‹¤ ì ì€ ê³„ì‚°ìœ¼ë¡œë„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.


---

*Generated on 2025-09-24 01:20:30*