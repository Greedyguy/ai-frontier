---
keywords:
  - Large Language Model
  - Post-training Quantization
  - Gradient Information
  - Non-uniform Scalar Quantization
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2505.07004
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:42:23.481396",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Post-training Quantization",
    "Gradient Information",
    "Non-uniform Scalar Quantization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Post-training Quantization": 0.78,
    "Gradient Information": 0.74,
    "Non-uniform Scalar Quantization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on quantization, providing a key context for linking with other works on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Post-training quantization",
        "canonical": "Post-training Quantization",
        "aliases": [
          "PTQ"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific technique crucial to the paper's methodology, offering a unique point for technical discussions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Gradient information",
        "canonical": "Gradient Information",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Integral to the proposed method, linking it to broader discussions on optimization and model training.",
        "novelty_score": 0.58,
        "connectivity_score": 0.77,
        "specificity_score": 0.68,
        "link_intent_score": 0.74
      },
      {
        "surface": "Non-uniform scalar quantization",
        "canonical": "Non-uniform Scalar Quantization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights a novel algorithm introduced in the paper, providing a specific technical advancement.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Post-training quantization",
      "resolved_canonical": "Post-training Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Gradient information",
      "resolved_canonical": "Gradient Information",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.77,
        "specificity": 0.68,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Non-uniform scalar quantization",
      "resolved_canonical": "Non-uniform Scalar Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.07004.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2505.07004](https://arxiv.org/abs/2505.07004)

## 🔗 유사한 논문
- [[2025-09-23/Does quantization affect models' performance on long-context tasks?_20250923|Does quantization affect models' performance on long-context tasks?]] (85.6% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (84.7% similar)
- [[2025-09-22/MEC-Quant_ Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training_20250922|MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training]] (84.4% similar)
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (84.2% similar)
- [[2025-09-23/How Can Quantum Deep Learning Improve Large Language Models?_20250923|How Can Quantum Deep Learning Improve Large Language Models?]] (84.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Gradient Information|Gradient Information]]
**⚡ Unique Technical**: [[keywords/Post-training Quantization|Post-training Quantization]], [[keywords/Non-uniform Scalar Quantization|Non-uniform Scalar Quantization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.07004v4 Announce Type: replace 
Abstract: Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.

## 📝 요약

이 논문에서는 대형 언어 모델의 메모리 사용량과 추론 지연을 줄이기 위한 사후 훈련 양자화 기법을 소개합니다. 기존 방법들은 숨겨진 특징의 중요성을 고려하지 않거나, 모델 가중치 간의 상호작용을 간과하는 문제점이 있습니다. 이를 해결하기 위해, 본 연구는 최종 손실의 그래디언트 정보를 양자화 목표에 통합하고 출력 채널 내 가중치 간의 의존성을 유지하는 새로운 양자화 접근법인 GuidedQuant을 제안합니다. GuidedQuant은 최첨단 양자화 방법의 성능을 일관되게 향상시키며, 새로운 비균일 스칼라 양자화 알고리즘도 도입하여 기존 방법보다 우수한 성능을 보입니다. 코드가 공개되어 있습니다.

## 🎯 주요 포인트

- 1. GuidedQuant는 최종 손실의 그래디언트 정보를 양자화 목표에 통합하여 출력 채널 내의 가중치 간 의존성을 유지합니다.
- 2. GuidedQuant는 가중치 전용 스칼라, 가중치 전용 벡터, 가중치 및 활성화 양자화에서 최첨단 양자화 방법의 성능을 일관되게 향상시킵니다.
- 3. 새로운 비균일 스칼라 양자화 알고리즘을 도입하여 양자화 목표 값을 단조롭게 감소시키며 기존 방법보다 우수한 성능을 보입니다.
- 4. GuidedQuant의 코드는 https://github.com/snu-mllab/GuidedQuant에서 공개됩니다.


---

*Generated on 2025-09-24 02:42:23*