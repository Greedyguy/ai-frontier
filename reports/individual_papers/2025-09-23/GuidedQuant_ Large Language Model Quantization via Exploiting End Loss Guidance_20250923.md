---
keywords:
  - Large Language Model
  - Post-training Quantization
  - Gradient Information
  - Non-uniform Scalar Quantization
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2505.07004
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:42:23.481396",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Post-training Quantization",
    "Gradient Information",
    "Non-uniform Scalar Quantization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Post-training Quantization": 0.78,
    "Gradient Information": 0.74,
    "Non-uniform Scalar Quantization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on quantization, providing a key context for linking with other works on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Post-training quantization",
        "canonical": "Post-training Quantization",
        "aliases": [
          "PTQ"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific technique crucial to the paper's methodology, offering a unique point for technical discussions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Gradient information",
        "canonical": "Gradient Information",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Integral to the proposed method, linking it to broader discussions on optimization and model training.",
        "novelty_score": 0.58,
        "connectivity_score": 0.77,
        "specificity_score": 0.68,
        "link_intent_score": 0.74
      },
      {
        "surface": "Non-uniform scalar quantization",
        "canonical": "Non-uniform Scalar Quantization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights a novel algorithm introduced in the paper, providing a specific technical advancement.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Post-training quantization",
      "resolved_canonical": "Post-training Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Gradient information",
      "resolved_canonical": "Gradient Information",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.77,
        "specificity": 0.68,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Non-uniform scalar quantization",
      "resolved_canonical": "Non-uniform Scalar Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.07004.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2505.07004](https://arxiv.org/abs/2505.07004)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Does quantization affect models' performance on long-context tasks?_20250923|Does quantization affect models' performance on long-context tasks?]] (85.6% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (84.7% similar)
- [[2025-09-22/MEC-Quant_ Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training_20250922|MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training]] (84.4% similar)
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (84.2% similar)
- [[2025-09-23/How Can Quantum Deep Learning Improve Large Language Models?_20250923|How Can Quantum Deep Learning Improve Large Language Models?]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Gradient Information|Gradient Information]]
**âš¡ Unique Technical**: [[keywords/Post-training Quantization|Post-training Quantization]], [[keywords/Non-uniform Scalar Quantization|Non-uniform Scalar Quantization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.07004v4 Announce Type: replace 
Abstract: Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì¶”ë¡  ì§€ì—°ì„ ì¤„ì´ê¸° ìœ„í•œ ì‚¬í›„ í›ˆë ¨ ì–‘ìí™” ê¸°ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ìˆ¨ê²¨ì§„ íŠ¹ì§•ì˜ ì¤‘ìš”ì„±ì„ ê³ ë ¤í•˜ì§€ ì•Šê±°ë‚˜, ëª¨ë¸ ê°€ì¤‘ì¹˜ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ê°„ê³¼í•˜ëŠ” ë¬¸ì œì ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ëŠ” ìµœì¢… ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ ì–‘ìí™” ëª©í‘œì— í†µí•©í•˜ê³  ì¶œë ¥ ì±„ë„ ë‚´ ê°€ì¤‘ì¹˜ ê°„ì˜ ì˜ì¡´ì„±ì„ ìœ ì§€í•˜ëŠ” ìƒˆë¡œìš´ ì–‘ìí™” ì ‘ê·¼ë²•ì¸ GuidedQuantì„ ì œì•ˆí•©ë‹ˆë‹¤. GuidedQuantì€ ìµœì²¨ë‹¨ ì–‘ìí™” ë°©ë²•ì˜ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚¤ë©°, ìƒˆë¡œìš´ ë¹„ê· ì¼ ìŠ¤ì¹¼ë¼ ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜ë„ ë„ì…í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì½”ë“œê°€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GuidedQuantëŠ” ìµœì¢… ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ ì–‘ìí™” ëª©í‘œì— í†µí•©í•˜ì—¬ ì¶œë ¥ ì±„ë„ ë‚´ì˜ ê°€ì¤‘ì¹˜ ê°„ ì˜ì¡´ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 2. GuidedQuantëŠ” ê°€ì¤‘ì¹˜ ì „ìš© ìŠ¤ì¹¼ë¼, ê°€ì¤‘ì¹˜ ì „ìš© ë²¡í„°, ê°€ì¤‘ì¹˜ ë° í™œì„±í™” ì–‘ìí™”ì—ì„œ ìµœì²¨ë‹¨ ì–‘ìí™” ë°©ë²•ì˜ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 3. ìƒˆë¡œìš´ ë¹„ê· ì¼ ìŠ¤ì¹¼ë¼ ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜ì„ ë„ì…í•˜ì—¬ ì–‘ìí™” ëª©í‘œ ê°’ì„ ë‹¨ì¡°ë¡­ê²Œ ê°ì†Œì‹œí‚¤ë©° ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 4. GuidedQuantì˜ ì½”ë“œëŠ” https://github.com/snu-mllab/GuidedQuantì—ì„œ ê³µê°œë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:42:23*