---
keywords:
  - Large Language Model
  - Linguistic Variability
  - Next-Word Prediction
  - Fine-Tuning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17794
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:32:37.440893",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Linguistic Variability",
    "Next-Word Prediction",
    "Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Linguistic Variability": 0.78,
    "Next-Word Prediction": 0.77,
    "Fine-Tuning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LMs"
        ],
        "category": "broad_technical",
        "rationale": "Language models are central to the study and are a key component in NLP tasks, making them highly connectable.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "linguistic variability",
        "canonical": "Linguistic Variability",
        "aliases": [
          "language variability",
          "variability in language"
        ],
        "category": "unique_technical",
        "rationale": "The concept of linguistic variability is unique to this study and crucial for understanding the research focus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "next-word prediction",
        "canonical": "Next-Word Prediction",
        "aliases": [
          "word prediction",
          "predicting next word"
        ],
        "category": "specific_connectable",
        "rationale": "Next-word prediction is a specific task that connects to broader NLP applications and model evaluations.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "fine-tuning techniques",
        "canonical": "Fine-Tuning",
        "aliases": [
          "model fine-tuning",
          "fine-tuning methods"
        ],
        "category": "specific_connectable",
        "rationale": "Fine-tuning is a widely applicable technique in machine learning, enhancing model performance and adaptability.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.68,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "context",
      "task",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "linguistic variability",
      "resolved_canonical": "Linguistic Variability",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "next-word prediction",
      "resolved_canonical": "Next-Word Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "fine-tuning techniques",
      "resolved_canonical": "Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.68,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17794.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17794](https://arxiv.org/abs/2509.17794)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (86.0% similar)
- [[2025-09-23/CIE_ Controlling Language Model Text Generations Using Continuous Signals_20250923|CIE: Controlling Language Model Text Generations Using Continuous Signals]] (85.8% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (85.3% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (85.3% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Next-Word Prediction|Next-Word Prediction]], [[keywords/Fine-Tuning|Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Linguistic Variability|Linguistic Variability]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17794v1 Announce Type: new 
Abstract: Natural language generation (NLG) tasks are often subject to inherent variability; \emph{e.g.} predicting the next word given a context has multiple valid responses, evident when asking multiple humans to complete the task. While having language models (LMs) that are aligned pluralistically, so that they are able to reproduce well the inherent diversity in perspectives of an entire population of interest is clearly beneficial, \citet{ilia2024predict} show that LMs do not reproduce this type of linguistic variability well. They speculate this inability might stem from the lack of consistent training of LMs with data reflecting this type of inherent variability. As such, we investigate whether training LMs on multiple plausible word continuations per context can improve their ability to reproduce human linguistic variability for next-word prediction. We employ fine-tuning techniques for pre-trained and instruction-tuned models; and demonstrate their potential when fine-tuning GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures divergence among empirically estimated human and model next-word distributions across contexts before and after fine-tuning, shows that our multi-label fine-tuning improves the LMs' ability to reproduce linguistic variability; both for contexts that admit higher and lower variability.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì—°ì–´ ìƒì„±(NLG) ì‘ì—…ì—ì„œ ì–¸ì–´ ëª¨ë¸(LM)ì´ ì¸ê°„ì˜ ì–¸ì–´ì  ë‹¤ì–‘ì„±ì„ ì˜ ì¬í˜„í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì—°êµ¬ì§„ì€ ë‹¤ì–‘í•œ ë¬¸ë§¥ì—ì„œ ì—¬ëŸ¬ ê°€ëŠ¥í•œ ë‹¨ì–´ ì—°ì†ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ GPT-2ì™€ Mistral-7B-IT ëª¨ë¸ì„ Provo Corpusë¥¼ ì‚¬ìš©í•´ ë¯¸ì„¸ ì¡°ì •í•˜ì˜€ìŠµë‹ˆë‹¤. í‰ê°€ ê²°ê³¼, ë‹¤ì¤‘ ë ˆì´ë¸” ë¯¸ì„¸ ì¡°ì • ê¸°ë²•ì´ ëª¨ë¸ì˜ ì–¸ì–´ì  ë‹¤ì–‘ì„± ì¬í˜„ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚´ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LMsê°€ ë‹¤ì–‘í•œ ì¸ê°„ì˜ ê´€ì ì„ ë” ì˜ ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ë¡ ì  ê¸°ì—¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìì—°ì–´ ìƒì„±(NLG) ì‘ì—…ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë³€ë™ì„±ì„ ê°€ì§€ë©°, ì´ëŠ” ì—¬ëŸ¬ ì‚¬ëŒì´ ë™ì¼í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ ë‹¤ì–‘í•œ ìœ íš¨í•œ ì‘ë‹µì´ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.
- 2. ì–¸ì–´ ëª¨ë¸(LM)ì´ ë‹¤ì–‘í•œ ê´€ì ì„ ì˜ ì¬í˜„í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì›ì ìœ¼ë¡œ ì •ë ¬ë˜ë©´ ìœ ìµí•˜ì§€ë§Œ, ê¸°ì¡´ ì—°êµ¬ì— ë”°ë¥´ë©´ LMsëŠ” ì´ëŸ¬í•œ ì–¸ì–´ì  ë³€ë™ì„±ì„ ì˜ ì¬í˜„í•˜ì§€ ëª»í•œë‹¤.
- 3. LMsì˜ ì–¸ì–´ì  ë³€ë™ì„± ì¬í˜„ ëŠ¥ë ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´, ì—¬ëŸ¬ ê°€ëŠ¥í•œ ë‹¨ì–´ ì—°ì†ì²´ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŒì„ ì¡°ì‚¬í•˜ì˜€ë‹¤.
- 4. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ì§€ì‹œ ì¡°ì •ëœ ëª¨ë¸ì— ëŒ€í•œ ë¯¸ì„¸ ì¡°ì • ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬, Provo Corpusë¥¼ í™œìš©í•œ GPT-2 ë° Mistral-7B-ITì˜ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥ì„±ì„ ì…ì¦í•˜ì˜€ë‹¤.
- 5. í‰ê°€ ê²°ê³¼, ë©€í‹° ë ˆì´ë¸” ë¯¸ì„¸ ì¡°ì •ì´ LMsì˜ ì–¸ì–´ì  ë³€ë™ì„± ì¬í˜„ ëŠ¥ë ¥ì„ ê°œì„ í•˜ë©°, ì´ëŠ” ë³€ë™ì„±ì´ ë†’ì€ ë¬¸ë§¥ê³¼ ë‚®ì€ ë¬¸ë§¥ ëª¨ë‘ì—ì„œ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-24 03:32:37*