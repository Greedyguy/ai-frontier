---
keywords:
  - Large Language Model
  - Hierarchical Reference Retrieval
  - Constrained Decoding
  - KILT Benchmark
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2402.17010
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:38:08.709768",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Hierarchical Reference Retrieval",
    "Constrained Decoding",
    "KILT Benchmark"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Hierarchical Reference Retrieval": 0.7,
    "Constrained Decoding": 0.8,
    "KILT Benchmark": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large-scale language models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology, linking to a foundational concept in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Hierarchical Reference Retrieval",
        "canonical": "Hierarchical Reference Retrieval",
        "aliases": [
          "reference retrieval",
          "hierarchical retrieval"
        ],
        "category": "unique_technical",
        "rationale": "Describes the novel retrieval approach proposed in the paper.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Constrained Decoding",
        "canonical": "Constrained Decoding",
        "aliases": [
          "restricted decoding",
          "controlled decoding"
        ],
        "category": "specific_connectable",
        "rationale": "A specific technique used in the retrieval process, relevant for linking to decoding strategies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "KILT knowledge-sensitive tasks",
        "canonical": "KILT Benchmark",
        "aliases": [
          "KILT tasks",
          "knowledge-intensive language tasks"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to a specific benchmark used for evaluating the method.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "parameterized knowledge",
      "document set",
      "short prefix"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Hierarchical Reference Retrieval",
      "resolved_canonical": "Hierarchical Reference Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Constrained Decoding",
      "resolved_canonical": "Constrained Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "KILT knowledge-sensitive tasks",
      "resolved_canonical": "KILT Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# MindRef: Mimicking Human Memory for Hierarchical Reference Retrieval with Fine-Grained Location Awareness

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2402.17010.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2402.17010](https://arxiv.org/abs/2402.17010)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Search and Refine During Think_ Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning_20250922|Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning]] (85.4% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (84.0% similar)
- [[2025-09-22/Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models_20250922|Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models]] (83.9% similar)
- [[2025-09-18/KBM_ Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models_20250918|KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models]] (83.8% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (83.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Constrained Decoding|Constrained Decoding]], [[keywords/KILT Benchmark|KILT Benchmark]]
**âš¡ Unique Technical**: [[keywords/Hierarchical Reference Retrieval|Hierarchical Reference Retrieval]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2402.17010v3 Announce Type: replace-cross 
Abstract: When completing knowledge-intensive tasks, humans sometimes need an answer and a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to recall reference passage from any starting position independently. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, and then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage locations in various task forms, and the obtained reference significantly assists downstream tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì €ì¥ëœ ë§¤ê°œë³€ìˆ˜í™”ëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬, ì§€ì‹ ì§‘ì•½ì  ì‘ì—…ì—ì„œ ì°¸ì¡° êµ¬ë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ íšŒìƒí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ì¶”ê°€ ê²€ìƒ‰ ëª¨ë¸ì„ í†µí•´ ë¯¸ë¦¬ ë¶„í• ëœ ê¸°ì‚¬ ì¡°ê°ì„ í•„ìš”ë¡œ í–ˆìœ¼ë‚˜, ë³¸ ì—°êµ¬ëŠ” ì¸ê°„ì´ ì‰½ê²Œ ìŠì–´ë²„ë¦¬ëŠ” ì°¸ì¡°ë¥¼ íšŒìƒí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë‘ ë‹¨ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì²« ë‹¨ê³„ì—ì„œëŠ” LLMì´ ë¬¸ì„œ ì œëª© ì‹ë³„ìë¥¼ íšŒìƒí•˜ì—¬ ëŒ€ëµì ì¸ ë¬¸ì„œ ì§‘í•©ì„ ì–»ê³ , ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¸ë¶€ êµ¬ë¬¸ì„ íšŒìƒí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì œí•œëœ ë””ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì €ì¥ëœ ë¬¸ì„œ ì™¸ì˜ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•Šë„ë¡ í•˜ë©°, ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ì§§ì€ ì ‘ë‘ì‚¬ë§Œ íšŒìƒí•˜ì—¬ ì „ì²´ êµ¬ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. KILT ì§€ì‹ ë¯¼ê° ì‘ì—…ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, LLMì´ ë‹¤ì–‘í•œ ì‘ì—… í˜•íƒœì—ì„œ ì°¸ì¡° êµ¬ë¬¸ ìœ„ì¹˜ë¥¼ ë…ë¦½ì ìœ¼ë¡œ íšŒìƒí•  ìˆ˜ ìˆìœ¼ë©°, ì–»ì–´ì§„ ì°¸ì¡°ê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— í¬ê²Œ ê¸°ì—¬í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì €ì¥ëœ ë§¤ê°œë³€ìˆ˜í™”ëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì°¸ì¡° êµ¬ë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ íšŒìƒí•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ ë‘ ë‹¨ê³„ í”„ë ˆì„ì›Œí¬ëŠ” ì¸ê°„ì´ ì‰½ê²Œ ìŠì–´ë²„ë¦¬ëŠ” ì°¸ì¡°ë¥¼ íšŒìƒí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ë©°, ì´ˆê¸°ì—ëŠ” ë¬¸ì„œ ì œëª© ì‹ë³„ìë¥¼ íšŒìƒí•˜ì—¬ ëŒ€ëµì ì¸ ë¬¸ì„œ ì§‘í•©ì„ ì–»ìŠµë‹ˆë‹¤.
- 3. ë‘ ë‹¨ê³„ íšŒìƒ ê³¼ì •ì—ì„œ ì €ì¥ëœ ë¬¸ì„œ ì™¸ì˜ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•Šë„ë¡ ì œí•œëœ ë””ì½”ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, LLMì´ ë‹¤ì–‘í•œ ì‘ì—… í˜•íƒœì—ì„œ ì°¸ì¡° êµ¬ë¬¸ ìœ„ì¹˜ë¥¼ ë…ë¦½ì ìœ¼ë¡œ íšŒìƒí•  ìˆ˜ ìˆìœ¼ë©°, ì–»ì–´ì§„ ì°¸ì¡°ê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— í¬ê²Œ ë„ì›€ì´ ë¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:38:08*