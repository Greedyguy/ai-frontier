---
keywords:
  - Natural Language Processing
  - Feature Attribution
  - Shortcut Learning
  - Annotation Quality
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.19680
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:46:56.117817",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Natural Language Processing",
    "Feature Attribution",
    "Shortcut Learning",
    "Annotation Quality"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Natural Language Processing": 0.8,
    "Feature Attribution": 0.77,
    "Shortcut Learning": 0.72,
    "Annotation Quality": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Natural Language Processing",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "This term is a key domain where the proposed method is applied, enhancing connectivity with related works.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Feature Salience",
        "canonical": "Feature Attribution",
        "aliases": [
          "Feature Importance",
          "Salience Mapping"
        ],
        "category": "specific_connectable",
        "rationale": "Feature attribution is central to the paper's methodology, linking it to a broader context of model interpretability.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Shortcut Learning",
        "canonical": "Shortcut Learning",
        "aliases": [
          "Shortcut Features"
        ],
        "category": "unique_technical",
        "rationale": "This concept is uniquely addressed in the paper, providing insights into model training challenges.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Annotation Quality",
        "canonical": "Annotation Quality",
        "aliases": [
          "Quality of Annotations"
        ],
        "category": "unique_technical",
        "rationale": "The paper highlights the importance of annotation quality, offering a unique perspective on data labeling.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "model",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Natural Language Processing",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Feature Salience",
      "resolved_canonical": "Feature Attribution",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Shortcut Learning",
      "resolved_canonical": "Shortcut Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Annotation Quality",
      "resolved_canonical": "Annotation Quality",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Model Guidance via Robust Feature Attribution

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.19680.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.19680](https://arxiv.org/abs/2506.19680)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories_20250923|Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories]] (82.4% similar)
- [[2025-09-22/Navigate Beyond Shortcuts_ Debiased Learning through the Lens of Neural Collapse_20250922|Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse]] (82.2% similar)
- [[2025-09-22/reWordBench_ Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs_20250922|reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs]] (82.2% similar)
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (82.0% similar)
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**ğŸ”— Specific Connectable**: [[keywords/Feature Attribution|Feature Attribution]]
**âš¡ Unique Technical**: [[keywords/Shortcut Learning|Shortcut Learning]], [[keywords/Annotation Quality|Annotation Quality]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.19680v2 Announce Type: replace 
Abstract: Controlling the patterns a model learns is essential to preventing reliance on irrelevant or misleading features. Such reliance on irrelevant features, often called shortcut features, has been observed across domains, including medical imaging and natural language processing, where it may lead to real-world harms. A common mitigation strategy leverages annotations (provided by humans or machines) indicating which features are relevant or irrelevant. These annotations are compared to model explanations, typically in the form of feature salience, and used to guide the loss function during training. Unfortunately, recent works have demonstrated that feature salience methods are unreliable and therefore offer a poor signal to optimize. In this work, we propose a simplified objective that simultaneously optimizes for explanation robustness and mitigation of shortcut learning. Unlike prior objectives with similar aims, we demonstrate theoretically why our approach ought to be more effective. Across a comprehensive series of experiments, we show that our approach consistently reduces test-time misclassifications by 20% compared to state-of-the-art methods. We also extend prior experimental settings to include natural language processing tasks. Additionally, we conduct novel ablations that yield practical insights, including the relative importance of annotation quality over quantity. Code for our method and experiments is available at: https://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” íŒ¨í„´ì„ ì œì–´í•˜ì—¬ ì˜ëª»ëœ íŠ¹ì§•ì— ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ íŠ¹ì§• ì¤‘ìš”ë„ë¥¼ í™œìš©í•˜ì§€ë§Œ, ì´ëŠ” ì‹ ë¢°ì„±ì´ ë‚®ì•„ ìµœì í™”ì— ë¶€ì í•©í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì„¤ëª…ì˜ ê²¬ê³ ì„±ê³¼ ë‹¨ì¶• í•™ìŠµì˜ ì™„í™”ë¥¼ ë™ì‹œì— ìµœì í™”í•˜ëŠ” ë‹¨ìˆœí™”ëœ ëª©í‘œë¥¼ ì œì•ˆí•˜ë©°, ì´ë¡ ì ìœ¼ë¡œ ê·¸ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ìµœì‹  ê¸°ë²• ëŒ€ë¹„ í…ŒìŠ¤íŠ¸ ì‹œ ì˜¤ë¶„ë¥˜ë¥¼ 20% ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì„ í¬í•¨í•œ ì‹¤í—˜ì„ í™•ì¥í•˜ê³ , ì£¼ì„ì˜ ì§ˆì´ ì–‘ë³´ë‹¤ ì¤‘ìš”í•˜ë‹¤ëŠ” ì‹¤ìš©ì  í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” íŒ¨í„´ì„ ì œì–´í•˜ëŠ” ê²ƒì€ ê´€ë ¨ ì—†ëŠ” íŠ¹ì§•ì— ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ íŠ¹ì§• ì¤‘ìš”ë„ ë°©ë²•ì€ ì‹ ë¢°ì„±ì´ ë‚®ì•„ ìµœì í™”ì— ë¶€ì ì ˆí•œ ì‹ í˜¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ì„¤ëª…ì˜ ê²¬ê³ ì„±ê³¼ ì§€ë¦„ê¸¸ í•™ìŠµì˜ ì™„í™”ë¥¼ ë™ì‹œì— ìµœì í™”í•˜ëŠ” ë‹¨ìˆœí™”ëœ ëª©í‘œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ìµœì²¨ë‹¨ ë°©ë²•ì— ë¹„í•´ í…ŒìŠ¤íŠ¸ ì‹œ ì˜¤ë¶„ë¥˜ë¥¼ 20% ì¤„ì´ëŠ” ë° ì¼ê´€ëœ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì£¼ì„ì˜ ì–‘ë³´ë‹¤ ì§ˆì´ ìƒëŒ€ì ìœ¼ë¡œ ë” ì¤‘ìš”í•˜ë‹¤ëŠ” ì‹¤ìš©ì ì¸ í†µì°°ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:46:56*