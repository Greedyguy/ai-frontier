---
keywords:
  - Video Action Recognition
  - Vision-Language Model
  - Motion Vectors
  - Zero-Shot Learning
  - Two-Stream Late Fusion
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17084
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:44:59.786758",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Video Action Recognition",
    "Vision-Language Model",
    "Motion Vectors",
    "Zero-Shot Learning",
    "Two-Stream Late Fusion"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Video Action Recognition": 0.7,
    "Vision-Language Model": 0.9,
    "Motion Vectors": 0.8,
    "Zero-Shot Learning": 0.85,
    "Two-Stream Late Fusion": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Video Action Recognition",
        "canonical": "Video Action Recognition",
        "aliases": [
          "Action Recognition"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's contribution and connects to the domain of video analysis.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Contrastive Language-Image Pre-training",
        "canonical": "Vision-Language Model",
        "aliases": [
          "CLIP"
        ],
        "category": "evolved_concepts",
        "rationale": "CLIP is a key component in the proposed method and links to the broader vision-language model trend.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.9
      },
      {
        "surface": "Motion Vectors",
        "canonical": "Motion Vectors",
        "aliases": [
          "MV"
        ],
        "category": "unique_technical",
        "rationale": "Motion vectors are crucial for the proposed method's efficiency and link to video compression techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Zero-Shot Capabilities",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot capabilities are highlighted as a strength of CLIP, connecting to the broader zero-shot learning paradigm.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Two-Stream Late Fusion",
        "canonical": "Two-Stream Late Fusion",
        "aliases": [
          "Late Fusion"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a novel aspect of the proposed framework, linking to fusion methods in video analysis.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Video Action Recognition",
      "resolved_canonical": "Video Action Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Contrastive Language-Image Pre-training",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Motion Vectors",
      "resolved_canonical": "Motion Vectors",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Zero-Shot Capabilities",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Two-Stream Late Fusion",
      "resolved_canonical": "Two-Stream Late Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17084.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17084](https://arxiv.org/abs/2509.17084)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/CardiacCLIP_ Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner_20250923|CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner]] (85.5% similar)
- [[2025-09-23/MVCL-DAF++_ Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion_20250923|MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion]] (84.5% similar)
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (84.1% similar)
- [[2025-09-22/RegionMed-CLIP_ A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding_20250922|RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding]] (82.9% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Video Action Recognition|Video Action Recognition]], [[keywords/Motion Vectors|Motion Vectors]], [[keywords/Two-Stream Late Fusion|Two-Stream Late Fusion]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17084v1 Announce Type: new 
Abstract: Video action recognition is a fundamental task in computer vision, but state-of-the-art models are often computationally expensive and rely on extensive video pre-training. In parallel, large-scale vision-language models like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot capabilities on static images, while motion vectors (MV) provide highly efficient temporal information directly from compressed video streams. To synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple yet powerful two-stream late fusion framework for efficient video recognition. Our approach combines features from a frozen CLIP image encoder with features from a lightweight, supervised network trained on raw MV. During fusion, both backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is trained, ensuring extreme efficiency. Through comprehensive experiments on the UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy, significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%) baselines. Our work provides a new, highly efficient baseline for video understanding that effectively bridges the gap between large static models and dynamic, low-cost motion cues. Our code and models are available at https://github.com/microa/MoCLIP-Lite.

## ğŸ“ ìš”ì•½

ë¹„ë””ì˜¤ í–‰ë™ ì¸ì‹ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ì¤‘ìš”í•œ ê³¼ì œì´ì§€ë§Œ, ìµœì‹  ëª¨ë¸ë“¤ì€ ê³„ì‚° ë¹„ìš©ì´ ë†’ê³  ë¹„ë””ì˜¤ ì‚¬ì „ í•™ìŠµì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ì— ë°˜í•´, ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì¸ CLIPì€ ì •ì  ì´ë¯¸ì§€ì—ì„œ ê°•ë ¥í•œ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë³´ì´ë©°, ëª¨ì…˜ ë²¡í„°(MV)ëŠ” ì••ì¶•ëœ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì—ì„œ íš¨ìœ¨ì ì¸ ì‹œê°„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì¥ì ì„ ê²°í•©í•˜ì—¬ MoCLIP-Liteë¼ëŠ” íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ ì¸ì‹ì„ ìœ„í•œ ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ ì´ì¤‘ ìŠ¤íŠ¸ë¦¼ í›„ë°˜ ìœµí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³ ì •ëœ CLIP ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ íŠ¹ì§•ê³¼ ê²½ëŸ‰ì˜ ê°ë…ëœ ë„¤íŠ¸ì›Œí¬ë¡œë¶€í„° í•™ìŠµëœ ì›ì‹œ MVì˜ íŠ¹ì§•ì„ ê²°í•©í•©ë‹ˆë‹¤. ìœµí•© ì‹œ ë‘ ë°±ë³¸ì€ ê³ ì •ë˜ê³  ì‘ì€ MLP í—¤ë“œë§Œ í•™ìŠµë˜ì–´ ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤. UCF101 ë°ì´í„°ì…‹ì—ì„œ 89.2%ì˜ Top-1 ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ì œë¡œìƒ·(65.0%) ë° MVë§Œ ì‚¬ìš©í•œ(66.5%) ê¸°ì¤€ì„ í¬ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì •ì  ëª¨ë¸ê³¼ ë™ì  ëª¨ì…˜ ë‹¨ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ìƒˆë¡œìš´ íš¨ìœ¨ì  ê¸°ì¤€ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MoCLIP-LiteëŠ” CLIP ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ ê²½ëŸ‰ì˜ ê°ë…ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ ì¸ì‹ì„ ìœ„í•œ ë‘ ìŠ¤íŠ¸ë¦¼ ëŠ¦ì€ ìœµí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì´ ë°©ë²•ì€ ê³ ì •ëœ ë°±ë³¸ì„ ì‚¬ìš©í•˜ê³  ì‘ì€ MLP í—¤ë“œë§Œì„ í›ˆë ¨í•˜ì—¬ ê·¹ë„ì˜ íš¨ìœ¨ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 3. UCF101 ë°ì´í„°ì…‹ì—ì„œ MoCLIP-LiteëŠ” 89.2%ì˜ Top-1 ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì—¬ ê°•ë ¥í•œ ì œë¡œìƒ· ë° MV ì „ìš© ê¸°ì¤€ì„ ì„ í¬ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 4. ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì •ì  ëª¨ë¸ê³¼ ì €ë¹„ìš© ë™ì  ëª¨ì…˜ í ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë©”ìš°ëŠ” ìƒˆë¡œìš´ íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ ì´í•´ ê¸°ì¤€ì„ ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:44:59*