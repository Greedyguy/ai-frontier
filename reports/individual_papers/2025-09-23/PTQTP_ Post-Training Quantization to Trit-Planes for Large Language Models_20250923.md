---
keywords:
  - Post-Training Quantization
  - Large Language Model
  - Ternary-weight Quantization
  - Quantization-aware Training
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16989
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:40:02.818293",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Post-Training Quantization",
    "Large Language Model",
    "Ternary-weight Quantization",
    "Quantization-aware Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Post-Training Quantization": 0.78,
    "Large Language Model": 0.85,
    "Ternary-weight Quantization": 0.82,
    "Quantization-aware Training": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Post-Training Quantization",
        "canonical": "Post-Training Quantization",
        "aliases": [
          "PTQ"
        ],
        "category": "unique_technical",
        "rationale": "Post-Training Quantization is a specific technique relevant to optimizing model deployment, enhancing connectivity with quantization methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus and connect broadly to the field of NLP and AI.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Ternary-weight PTQ",
        "canonical": "Ternary-weight Quantization",
        "aliases": [
          "Ternary PTQ",
          "Trit-Planes Quantization"
        ],
        "category": "unique_technical",
        "rationale": "Ternary-weight PTQ is a novel approach in the paper, offering a unique method for model optimization.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Quantization-aware Training",
        "canonical": "Quantization-aware Training",
        "aliases": [
          "QAT"
        ],
        "category": "specific_connectable",
        "rationale": "Quantization-aware Training is a related concept that enhances understanding of model optimization techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Post-Training Quantization",
      "resolved_canonical": "Post-Training Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Ternary-weight PTQ",
      "resolved_canonical": "Ternary-weight Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Quantization-aware Training",
      "resolved_canonical": "Quantization-aware Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16989.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16989](https://arxiv.org/abs/2509.16989)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (89.0% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (84.7% similar)
- [[2025-09-22/MEC-Quant_ Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training_20250922|MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training]] (83.4% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (83.4% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (81.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Quantization-aware Training|Quantization-aware Training]]
**âš¡ Unique Technical**: [[keywords/Post-Training Quantization|Post-Training Quantization]], [[keywords/Ternary-weight Quantization|Ternary-weight Quantization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16989v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation mechanisms, they suffer from either limited representational capacity or computational overhead that undermines their efficiency gains. We introduce PTQ to Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit representation. PTQTP achieves multiplication-free inference, identical to 1-bit quantization, while maintaining superior expressiveness through its novel structured decomposition. Our approach provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment across diverse modern LLMs without architectural modifications; and (3) uniform ternary operations that eliminate the need for mixed-precision or compensation schemes. Comprehensive experiments across LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP significantly outperforms existing low-bit PTQ methods, achieving 82.4% mathematical reasoning retention versus 0% for competing approaches. PTQTP approaches and sometimes surpasses 1.58-bit quantization-aware training performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods. These results establish PTQTP as a practical solution for efficient LLM deployment in resource-constrained environments.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê·¹ì €ë¹„íŠ¸ í›„ì²˜ë¦¬ ì–‘ìí™”(PTQ) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ PTQTPë¼ëŠ” ìƒˆë¡œìš´ 3ì§„ ê°€ì¤‘ì¹˜ PTQ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. PTQTPëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ êµ¬ì¡°í™”ëœ 3ì§„ {-1, 0, 1} í˜•íƒœë¡œ ë¶„í•´í•˜ì—¬ 2x1.58ë¹„íŠ¸ë¡œ í‘œí˜„í•˜ë©°, ê³±ì…ˆì´ í•„ìš” ì—†ëŠ” ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (1) ì „ì—­ ê°€ì¤‘ì¹˜ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ëŠ” ì ì§„ì  ê·¼ì‚¬ ì•Œê³ ë¦¬ì¦˜, (2) ë‹¤ì–‘í•œ LLMì— ì•„í‚¤í…ì²˜ ìˆ˜ì • ì—†ì´ ì ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¹„ì¢…ì†ì  ë°°í¬, (3) í˜¼í•© ì •ë°€ë„ë‚˜ ë³´ìƒ ì²´ê³„ê°€ í•„ìš” ì—†ëŠ” ê· ì¼í•œ 3ì§„ ì—°ì‚°. ì‹¤í—˜ ê²°ê³¼, PTQTPëŠ” ê¸°ì¡´ ì €ë¹„íŠ¸ PTQ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ìˆ˜í•™ì  ì¶”ë¡  ìœ ì§€ìœ¨ 82.4%ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ë˜í•œ, 1.58ë¹„íŠ¸ ì–‘ìí™” ì¸ì‹ í›ˆë ¨ ì„±ëŠ¥ì— ê·¼ì ‘í•˜ê±°ë‚˜ ì´ë¥¼ ì´ˆê³¼í•˜ë©´ì„œë„, ë‹¨ 1ì‹œê°„ì˜ ì–‘ìí™” ì‹œê°„ë§Œ ì†Œìš”ë©ë‹ˆë‹¤. ì´ëŠ” ìì› ì œì•½ í™˜ê²½ì—ì„œ LLMì˜ íš¨ìœ¨ì  ë°°í¬ë¥¼ ìœ„í•œ ì‹¤ìš©ì  ì†”ë£¨ì…˜ìœ¼ë¡œ ìë¦¬ì¡ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. PTQTPëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ êµ¬ì¡°í™”ëœ ì‚¼ì§„ {-1, 0, 1} íŠ¸ë¦¿-í”Œë ˆì¸ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê³±ì…ˆ ì—†ëŠ” ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 2. PTQTPëŠ” ë‹¤ì–‘í•œ í˜„ëŒ€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì•„í‚¤í…ì²˜ ìˆ˜ì • ì—†ì´ ì ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸-ë¹„ì¢…ì†ì  ë°°í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 3. PTQTPëŠ” í˜¼í•© ì •ë°€ë„ë‚˜ ë³´ìƒ ì²´ê³„ê°€ í•„ìš” ì—†ëŠ” ê· ì¼í•œ ì‚¼ì§„ ì—°ì‚°ì„ í†µí•´ ê¸°ì¡´ ì €ë¹„íŠ¸ PTQ ë°©ë²•ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. PTQTPëŠ” ìˆ˜í•™ì  ì¶”ë¡  ìœ ì§€ìœ¨ì—ì„œ ê¸°ì¡´ ë°©ë²•ì˜ 0%ì— ë¹„í•´ 82.4%ë¥¼ ë‹¬ì„±í•˜ë©°, 1.58ë¹„íŠ¸ ì–‘ìí™” ì¸ì‹ í›ˆë ¨ ì„±ëŠ¥ì— ê·¼ì ‘í•˜ê±°ë‚˜ ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.
- 5. PTQTPëŠ” ë‹¨ì‹œê°„(ëª‡ ì‹œê°„) ì–‘ìí™”ë¡œë„ ê¸°ì¡´ í›ˆë ¨ ê¸°ë°˜ ë°©ë²•ì˜ 10-14 GPUì¼ ì†Œìš”ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:40:02*