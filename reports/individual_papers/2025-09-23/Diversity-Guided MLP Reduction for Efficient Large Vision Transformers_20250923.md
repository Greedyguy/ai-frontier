---
keywords:
  - Transformer
  - Vision Transformers
  - Multilayer Perceptron
  - Weight Pruning
  - Knowledge Distillation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.08591
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:05:00.017932",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Vision Transformers",
    "Multilayer Perceptron",
    "Weight Pruning",
    "Knowledge Distillation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Vision Transformers": 0.88,
    "Multilayer Perceptron": 0.82,
    "Weight Pruning": 0.79,
    "Knowledge Distillation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [
          "Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the paper's focus on model reduction, linking to a broad range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.95,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision Transformers",
        "canonical": "Vision Transformers",
        "aliases": [
          "ViT"
        ],
        "category": "specific_connectable",
        "rationale": "Vision Transformers are a specific application of Transformers, relevant for connecting to computer vision research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.8,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Multilayer Perceptron",
        "canonical": "Multilayer Perceptron",
        "aliases": [
          "MLP"
        ],
        "category": "specific_connectable",
        "rationale": "MLPs are a key component in the model architecture discussed, facilitating connections to neural network research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Weight Pruning",
        "canonical": "Weight Pruning",
        "aliases": [
          "Pruning"
        ],
        "category": "unique_technical",
        "rationale": "Weight pruning is a unique technique highlighted in the paper for reducing model parameters.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "Distillation",
        "canonical": "Knowledge Distillation",
        "aliases": [
          "Model Distillation"
        ],
        "category": "specific_connectable",
        "rationale": "Knowledge distillation is crucial for model recovery, linking to efficient training methods.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.95,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision Transformers",
      "resolved_canonical": "Vision Transformers",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.8,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Multilayer Perceptron",
      "resolved_canonical": "Multilayer Perceptron",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Weight Pruning",
      "resolved_canonical": "Weight Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Distillation",
      "resolved_canonical": "Knowledge Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Diversity-Guided MLP Reduction for Efficient Large Vision Transformers

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.08591.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.08591](https://arxiv.org/abs/2506.08591)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (86.4% similar)
- [[2025-09-23/On the Simplification of Neural Network Architectures for Predictive Process Monitoring_20250923|On the Simplification of Neural Network Architectures for Predictive Process Monitoring]] (85.3% similar)
- [[2025-09-23/Scaling Efficient LLMs_20250923|Scaling Efficient LLMs]] (85.1% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (84.5% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Vision Transformers|Vision Transformers]], [[keywords/Multilayer Perceptron|Multilayer Perceptron]], [[keywords/Knowledge Distillation|Knowledge Distillation]]
**âš¡ Unique Technical**: [[keywords/Weight Pruning|Weight Pruning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.08591v2 Announce Type: replace-cross 
Abstract: Transformer models achieve excellent scaling property, where the performance is improved with the increment of model capacity. However, large-scale model parameters lead to an unaffordable cost of computing and memory. We analyze popular transformer architectures and find that multilayer perceptron (MLP) modules take up the majority of model parameters. To this end, we focus on the recoverability of the compressed models and propose a Diversity-Guided MLP Reduction (DGMR) method to significantly reduce the parameters of large vision transformers with only negligible performance degradation. Specifically, we conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons of MLP hidden layer, while preserving weight diversity for better performance recover during distillation. Compared to the model trained from scratch, our pruned model only requires 0.06\% data of LAION-2B (for the training of large vision transformers) without labels (ImageNet-1K) to recover the original performance. Experimental results on several state-of-the-art large vision transformers demonstrate that our method achieves a more than 57.0\% parameter and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B), our method accomplishes a 71.5\% parameter and FLOPs reduction without performance degradation. The source code and trained weights are available at https://github.com/visresearch/DGMR.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê¸° ìœ„í•œ Diversity-Guided MLP Reduction (DGMR) ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” MLP ëª¨ë“ˆì˜ ì¤‘ë³µ ë‰´ëŸ°ì„ ì œê±°í•˜ì—¬ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ 57.0% ì´ìƒ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. íŠ¹íˆ, EVA-CLIP-E ëª¨ë¸ì—ì„œëŠ” 71.5%ì˜ ë§¤ê°œë³€ìˆ˜ ê°ì†Œë¥¼ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œë„ ì›ë˜ ì„±ëŠ¥ì„ íšŒë³µí•  ìˆ˜ ìˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ëŠ” ì—¬ëŸ¬ ìµœì‹  ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ê·¸ íš¨ìœ¨ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ëª¨ë¸ ìš©ëŸ‰ ì¦ê°€ì™€ í•¨ê»˜ í–¥ìƒë˜ì§€ë§Œ, ëŒ€ê·œëª¨ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ë†’ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ì„ ì´ˆë˜í•©ë‹ˆë‹¤.
- 2. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP) ëª¨ë“ˆì´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•¨ì„ ë°œê²¬í•˜ê³ , ì´ë¥¼ ì••ì¶•í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ ì—†ì´ íŒŒë¼ë¯¸í„°ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ Diversity-Guided MLP Reduction (DGMR) ë°©ë²•ì€ MLP ìˆ¨ê²¨ì§„ ì¸µì˜ ì¤‘ë³µ ë‰´ëŸ°ì„ ì œê±°í•˜ë©´ì„œ ê°€ì¤‘ì¹˜ ë‹¤ì–‘ì„±ì„ ìœ ì§€í•˜ì—¬ ì„±ëŠ¥ íšŒë³µì„ ë•ìŠµë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, DGMR ë°©ë²•ì€ ì—¬ëŸ¬ ìµœì‹  ëŒ€í˜• ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ 57.0% ì´ìƒì˜ íŒŒë¼ë¯¸í„° ë° FLOPs ê°ì†Œë¥¼ ê±°ì˜ ì†ì‹¤ ì—†ì´ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 5. íŠ¹íˆ EVA-CLIP-E (4.4B) ëª¨ë¸ì—ì„œ 71.5%ì˜ íŒŒë¼ë¯¸í„° ë° FLOPs ê°ì†Œë¥¼ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:05:00*