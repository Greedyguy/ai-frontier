---
keywords:
  - Large Language Model
  - Intelligent Virtual Agent
  - Risk Ontology
  - Conversational AI
  - Therapeutic Interaction Risks
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.15108
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:00:07.559382",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Intelligent Virtual Agent",
    "Risk Ontology",
    "Conversational AI",
    "Therapeutic Interaction Risks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Intelligent Virtual Agent": 0.82,
    "Risk Ontology": 0.9,
    "Conversational AI": 0.78,
    "Therapeutic Interaction Risks": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the development of AI-powered psychotherapy agents, providing a strong technical foundation for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Intelligent Virtual Agents",
        "canonical": "Intelligent Virtual Agent",
        "aliases": [
          "Virtual Agents",
          "IVAs"
        ],
        "category": "unique_technical",
        "rationale": "These agents are the primary focus of the paper, representing a novel application in mental health support.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.79,
        "link_intent_score": 0.82
      },
      {
        "surface": "Risk Ontology",
        "canonical": "Risk Ontology",
        "aliases": [
          "Ontology of Risk"
        ],
        "category": "unique_technical",
        "rationale": "The risk ontology is a novel framework introduced in the paper, crucial for evaluating AI psychotherapy agents.",
        "novelty_score": 0.85,
        "connectivity_score": 0.72,
        "specificity_score": 0.88,
        "link_intent_score": 0.9
      },
      {
        "surface": "Conversational AI",
        "canonical": "Conversational AI",
        "aliases": [
          "Chatbot AI",
          "Dialogue Systems"
        ],
        "category": "specific_connectable",
        "rationale": "Conversational AI is a key technology enabling the functionality of AI psychotherapists, facilitating strong connections to related research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.84,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Therapeutic Interaction Risks",
        "canonical": "Therapeutic Interaction Risks",
        "aliases": [
          "Risks in Therapy",
          "Therapy Risks"
        ],
        "category": "unique_technical",
        "rationale": "Understanding and evaluating these risks are central to the paper's objectives, offering a unique perspective on AI therapy.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "methodologies",
      "evaluation techniques",
      "use cases",
      "assessment tools"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Intelligent Virtual Agents",
      "resolved_canonical": "Intelligent Virtual Agent",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.79,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Risk Ontology",
      "resolved_canonical": "Risk Ontology",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.72,
        "specificity": 0.88,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Conversational AI",
      "resolved_canonical": "Conversational AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.84,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Therapeutic Interaction Risks",
      "resolved_canonical": "Therapeutic Interaction Risks",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.15108.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.15108](https://arxiv.org/abs/2505.15108)

## 🔗 유사한 논문
- [[2025-09-23/SouLLMate_ An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques_20250923|SouLLMate: An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques]] (84.3% similar)
- [[2025-09-23/Domain-Specific Constitutional AI_ Enhancing Safety in LLM-Powered Mental Health Chatbots_20250923|Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots]] (84.1% similar)
- [[2025-09-18/Designing AI-Agents with Personalities_ A Psychometric Approach_20250918|Designing AI-Agents with Personalities: A Psychometric Approach]] (83.1% similar)
- [[2025-09-18/When Avatars Have Personality_ Effects on Engagement and Communication in Immersive Medical Training_20250918|When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training]] (82.1% similar)
- [[2025-09-19/Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (81.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Conversational AI|Conversational AI]]
**⚡ Unique Technical**: [[keywords/Intelligent Virtual Agent|Intelligent Virtual Agent]], [[keywords/Risk Ontology|Risk Ontology]], [[keywords/Therapeutic Interaction Risks|Therapeutic Interaction Risks]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.15108v2 Announce Type: replace-cross 
Abstract: The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk ontology specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the ontology aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this ontology, detailing its grounding, and discuss potential use cases. We discuss four use cases in detail: monitoring real user interactions, evaluation with simulated patients, benchmarking and comparative analysis, and identifying unexpected outcomes. The proposed ontology offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.

## 📝 요약

대형 언어 모델(LLM)과 지능형 가상 에이전트가 심리치료사로서의 역할을 수행하는 것은 정신 건강 관리 접근성을 확대할 수 있는 기회를 제공합니다. 그러나 표준화된 평가 방법론의 부재로 인해 사용자 피해 및 자살과 같은 심각한 부작용이 발생할 수 있습니다. 기존 평가 기술은 치료 세션 중 환자의 인지 및 행동의 미묘한 변화를 감지하는 데 한계가 있습니다. 본 연구는 대화형 AI 심리치료사를 체계적으로 평가하기 위한 새로운 위험 온톨로지를 제안합니다. 이는 심리치료 위험 문헌 검토, 임상 및 법률 전문가와의 질적 인터뷰, DSM-5 및 기존 평가 도구와의 정렬을 통해 개발되었습니다. 이 온톨로지는 사용자 및 환자 피해를 식별하고 평가하는 구조적 접근 방식을 제공합니다. 실제 사용자 상호작용 모니터링, 시뮬레이션 환자 평가, 벤치마킹 및 비교 분석, 예기치 않은 결과 식별 등 네 가지 사용 사례를 논의하며, AI 기반 정신 건강 지원의 안전하고 책임 있는 혁신을 위한 기초를 제공합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델과 지능형 가상 에이전트의 심리치료사 역할은 정신 건강 관리 접근성을 확대할 수 있는 기회를 제공하지만, 표준화된 평가 방법론 부족으로 사용자 피해 및 자살 등 심각한 부작용이 발생할 수 있다.
- 2. 현재의 평가 기법은 치료 세션 중 환자의 인지와 행동에서 미묘한 변화를 감지하는 데 민감하지 않아, 이후의 악화를 초래할 수 있는 위험을 포착하기 어렵다.
- 3. 본 연구는 대화형 AI 심리치료사의 체계적인 평가를 위한 새로운 위험 온톨로지를 소개하며, 이는 심리치료 위험 문헌 검토, 임상 및 법률 전문가와의 질적 인터뷰, DSM-5와 같은 임상 기준 및 기존 평가 도구와의 정렬을 통해 개발되었다.
- 4. 제안된 온톨로지는 사용자/환자 피해를 식별하고 평가하는 구조화된 접근 방식을 제공하며, 실제 사용자 상호작용 모니터링, 시뮬레이션 환자 평가, 벤치마킹 및 비교 분석, 예상치 못한 결과 식별 등의 사용 사례를 포함한다.
- 5. 이 온톨로지는 AI 기반 정신 건강 지원 분야에서 더 안전하고 책임 있는 혁신을 위한 기초적인 단계를 제공한다.


---

*Generated on 2025-09-24 01:00:07*