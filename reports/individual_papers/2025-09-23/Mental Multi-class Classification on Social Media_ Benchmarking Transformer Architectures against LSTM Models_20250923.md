---
keywords:
  - Transformer
  - LSTM
  - Mental Health Detection
  - Attention Mechanism
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16542
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:10:18.699291",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "LSTM",
    "Mental Health Detection",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.88,
    "LSTM": 0.82,
    "Mental Health Detection": 0.78,
    "Attention Mechanism": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer architectures",
        "canonical": "Transformer",
        "aliases": [
          "Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a fundamental model architecture in NLP, crucial for linking with other deep learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.95,
        "specificity_score": 0.65,
        "link_intent_score": 0.88
      },
      {
        "surface": "Long Short-Term Memory",
        "canonical": "LSTM",
        "aliases": [
          "Long Short-Term Memory",
          "LSTMs"
        ],
        "category": "specific_connectable",
        "rationale": "LSTMs are a key architecture for time series and sequence data, offering strong connectivity with historical NLP models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "mental health detection",
        "canonical": "Mental Health Detection",
        "aliases": [
          "mental health classification",
          "mental health identification"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized application area within NLP, linking mental health and AI research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "attention-augmented LSTMs",
        "canonical": "Attention Mechanism",
        "aliases": [
          "attention-based LSTMs"
        ],
        "category": "specific_connectable",
        "rationale": "Combining attention mechanisms with LSTMs enhances model performance, linking to advanced neural network techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "social media",
      "dataset",
      "experimental results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer architectures",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.95,
        "specificity": 0.65,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Long Short-Term Memory",
      "resolved_canonical": "LSTM",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "mental health detection",
      "resolved_canonical": "Mental Health Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "attention-augmented LSTMs",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16542.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16542](https://arxiv.org/abs/2509.16542)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection_20250923|Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection]] (83.0% similar)
- [[2025-09-23/ReDepress_ A Cognitive Framework for Detecting Depression Relapse from Social Media_20250923|ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media]] (82.4% similar)
- [[2025-09-18/A Comparative Analysis of Transformer Models in Social Bot Detection_20250918|A Comparative Analysis of Transformer Models in Social Bot Detection]] (80.8% similar)
- [[2025-09-22/A Layered Multi-Expert Framework for Long-Context Mental Health Assessments_20250922|A Layered Multi-Expert Framework for Long-Context Mental Health Assessments]] (80.8% similar)
- [[2025-09-22/A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media_20250922|A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media]] (80.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/LSTM|LSTM]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Mental Health Detection|Mental Health Detection]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16542v1 Announce Type: cross 
Abstract: Millions of people openly share mental health struggles on social media, providing rich data for early detection of conditions such as depression, bipolar disorder, etc. However, most prior Natural Language Processing (NLP) research has focused on single-disorder identification, leaving a gap in understanding the efficacy of advanced NLP techniques for distinguishing among multiple mental health conditions. In this work, we present a large-scale comparative study of state-of-the-art transformer versus Long Short-Term Memory (LSTM)-based models to classify mental health posts into exclusive categories of mental health conditions. We first curate a large dataset of Reddit posts spanning six mental health conditions and a control group, using rigorous filtering and statistical exploratory analysis to ensure annotation quality. We then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against several LSTM variants (with or without attention, using contextual or static embeddings) under identical conditions. Experimental results show that transformer models consistently outperform the alternatives, with RoBERTa achieving 91-99% F1-scores and accuracies across all classes. Notably, attention-augmented LSTMs with BERT embeddings approach transformer performance (up to 97% F1-score) while training 2-3.5 times faster, whereas LSTMs using static embeddings fail to learn useful signals. These findings represent the first comprehensive benchmark for multi-class mental health detection, offering practical guidance on model selection and highlighting an accuracy-efficiency trade-off for real-world deployment of mental health NLP systems.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì†Œì…œ ë¯¸ë””ì–´ì—ì„œì˜ ì •ì‹  ê±´ê°• ê´€ë ¨ ê²Œì‹œë¬¼ì„ ë¶„ì„í•˜ì—¬ ì—¬ëŸ¬ ì •ì‹  ê±´ê°• ìƒíƒœë¥¼ êµ¬ë¶„í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP) ê¸°ë²•ì˜ íš¨ê³¼ë¥¼ ì—°êµ¬í•œ ê²ƒì´ë‹¤. ì—°êµ¬ì§„ì€ Redditì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ê³¼ LSTM ê¸°ë°˜ ëª¨ë¸ì„ ë¹„êµí•˜ì—¬ ì •ì‹  ê±´ê°• ìƒíƒœë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì „ë°˜ì ìœ¼ë¡œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ RoBERTa ëª¨ë¸ì´ 91-99%ì˜ F1-scoreì™€ ì •í™•ë„ë¥¼ ê¸°ë¡í–ˆë‹¤. ì£¼ëª©í•  ë§Œí•œ ì ì€ BERT ì„ë² ë”©ì„ ì‚¬ìš©í•œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì¶”ê°€í•œ LSTMì´ íŠ¸ëœìŠ¤í¬ë¨¸ì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œë„ í•™ìŠµ ì†ë„ê°€ 2-3.5ë°° ë¹ ë¥´ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ ì—°êµ¬ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ì •ì‹  ê±´ê°• íƒì§€ì— ëŒ€í•œ ì²« ì¢…í•© ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œê³µí•˜ë©°, ëª¨ë¸ ì„ íƒì— ëŒ€í•œ ì‹¤ìš©ì  ì§€ì¹¨ê³¼ ì •í™•ë„-íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì„ ê°•ì¡°í•œë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì •ì‹  ê±´ê°• ìƒíƒœë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì²¨ë‹¨ NLP ê¸°ë²•ì˜ íš¨ëŠ¥ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ì—°êµ¬ì—ì„œëŠ” ì—¬ì„¯ ê°€ì§€ ì •ì‹  ê±´ê°• ìƒíƒœì™€ ëŒ€ì¡°êµ°ì„ í¬í•¨í•œ ëŒ€ê·œëª¨ Reddit ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, RoBERTaë¥¼ í¬í•¨í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ëª¨ë“  í´ë˜ìŠ¤ì—ì„œ 91-99%ì˜ F1-ìŠ¤ì½”ì–´ì™€ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©° LSTM ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì¶”ê°€ëœ LSTM ëª¨ë¸ì€ BERT ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ì •ì  ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” LSTMì€ ìœ ìš©í•œ ì‹ í˜¸ë¥¼ í•™ìŠµí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ì •ì‹  ê±´ê°• íƒì§€ë¥¼ ìœ„í•œ ìµœì´ˆì˜ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œê³µí•˜ë©°, ëª¨ë¸ ì„ íƒì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ ì§€ì¹¨ê³¼ ì •í™•ë„-íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:10:18*