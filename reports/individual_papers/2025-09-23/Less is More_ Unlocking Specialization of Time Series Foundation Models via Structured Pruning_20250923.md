---
keywords:
  - Time Series Foundation Models
  - structured pruning
  - Zero-Shot Learning
  - fine-tuning
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.23195
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:03:39.520660",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Time Series Foundation Models",
    "structured pruning",
    "Zero-Shot Learning",
    "fine-tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Time Series Foundation Models": 0.78,
    "structured pruning": 0.77,
    "Zero-Shot Learning": 0.8,
    "fine-tuning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Time Series Foundation Models",
        "canonical": "Time Series Foundation Models",
        "aliases": [
          "TSFMs"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a specialized model type, enhancing specificity in time series forecasting discussions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "structured pruning",
        "canonical": "structured pruning",
        "aliases": [
          "prune-then-finetune"
        ],
        "category": "unique_technical",
        "rationale": "Structured pruning is a novel approach in the paper, crucial for optimizing model performance, thus offering a unique link opportunity.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "zero-shot forecasting",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot forecasting is a specific application of zero-shot learning, linking to broader discussions on model generalization.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "fine-tuning",
        "canonical": "fine-tuning",
        "aliases": [
          "model adaptation"
        ],
        "category": "broad_technical",
        "rationale": "Fine-tuning is a widely applicable concept in machine learning, crucial for adapting pre-trained models to specific tasks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "forecasting performance",
      "empirical studies",
      "source code"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Time Series Foundation Models",
      "resolved_canonical": "Time Series Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "structured pruning",
      "resolved_canonical": "structured pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "zero-shot forecasting",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "fine-tuning",
      "resolved_canonical": "fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.23195.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.23195](https://arxiv.org/abs/2505.23195)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/TFMAdapter_ Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates_20250917|TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates]] (85.5% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (81.8% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (81.8% similar)
- [[2025-09-22/Modeling Temporal Dependencies within the Target for Long-Term Time Series Forecasting_20250922|Modeling Temporal Dependencies within the Target for Long-Term Time Series Forecasting]] (81.7% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/fine-tuning|fine-tuning]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Time Series Foundation Models|Time Series Foundation Models]], [[keywords/structured pruning|structured pruning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.23195v2 Announce Type: replace-cross 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This prune-then-finetune paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at https://github.com/SJTU-DMTai/Prune-then-Finetune.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ì „ í•™ìŠµí•˜ì—¬ ë›°ì–´ë‚œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì‹œê³„ì—´ ê¸°ì´ˆ ëª¨ë¸(TSFM)ì˜ íš¨ê³¼ì ì¸ ì ì‘ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, TSFMì€ ë‚´ì¬ëœ ê³„ì‚°ì˜ í¬ì†Œì„±ê³¼ ì¤‘ë³µì„±ì„ ë³´ì—¬ì£¼ë©°, ë‹¤ì–‘í•œ ì˜ˆì¸¡ ì‘ì—…ì— ì í•©í•œ ë„¤íŠ¸ì›Œí¬ í•˜ìœ„ êµ¬ì¡°ë¥¼ í™œì„±í™”í•˜ëŠ” ëŠ¥ë ¥ì„ í•™ìŠµí•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ìœ ìš©í•œ ì‚¬ì „ ì§€ì‹ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸° ë°©ë²•ì„ ì œì•ˆí•˜ì—¬, í›„ì† ë¯¸ì„¸ ì¡°ì • ê³¼ì •ì„ ë” ê´€ë ¨ì„± ìˆê³  ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ê³µê°„ì— ì§‘ì¤‘ì‹œí‚µë‹ˆë‹¤. 7ê°œì˜ TSFMê³¼ 6ê°œì˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ, ê°€ì§€ì¹˜ê¸° í›„ ë¯¸ì„¸ ì¡°ì •ëœ TSFMì´ ì›ë˜ ëª¨ë¸ë³´ë‹¤ ì˜ˆì¸¡ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ TSFMì´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ê°•ë ¥í•œ íŠ¹í™” ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TSFMsëŠ” ëŒ€ê·œëª¨ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ì „ í•™ìŠµí•˜ì—¬ ë›°ì–´ë‚œ ì œë¡œìƒ· ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì§€ë§Œ, ì„¸ë°€í•œ ì¡°ì • í›„ì—ë„ ì „ë¬¸í™”ëœ ì†Œí˜• ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ì§€ ëª»í•¨.
- 2. TSFMsì˜ íš¨ê³¼ì ì¸ ì ì‘ì„ ìœ„í•´, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë“¤ì´ ë‹¤ì–‘í•œ ì˜ˆì¸¡ ì‘ì—…ì— ì í•©í•œ ë„¤íŠ¸ì›Œí¬ í•˜ìœ„ êµ¬ì¡°ë¥¼ í™œì„±í™”í•˜ëŠ” ëŠ¥ë ¥ì„ í•™ìŠµí–ˆìŒì„ ë°œê²¬.
- 3. ê¸°ì¡´ì˜ ì‚¬ì „ ì§€ì‹ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´, êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸° ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë³´ë‹¤ ê´€ë ¨ì„± ìˆê³  ì••ì¶•ëœ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì— ì´ˆì ì„ ë§ì¶° ì„¸ë°€í•œ ì¡°ì • ê³¼ì •ì„ ê·œì œ.
- 4. 7ê°œì˜ TSFMsì™€ 6ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ê°€ì§€ì¹˜ê¸° í›„ ì„¸ë°€í•œ ì¡°ì •ëœ TSFMì´ ì›ë˜ ëª¨ë¸ì„ ì¡°ì •í•œ ê²ƒë³´ë‹¤ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´.
- 5. ì œì•ˆëœ ê°€ì§€ì¹˜ê¸° í›„ ì„¸ë°€ ì¡°ì • íŒ¨ëŸ¬ë‹¤ì„ì€ TSFMsê°€ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ê°•ë ¥í•œ ì „ë¬¸í™”ëœ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•  ìˆ˜ ìˆë„ë¡ í•¨.


---

*Generated on 2025-09-24 01:03:39*