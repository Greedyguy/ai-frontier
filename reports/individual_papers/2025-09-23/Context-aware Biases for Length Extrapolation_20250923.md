---
keywords:
  - Transformer
  - Relative Positional Encoding
  - Context-Aware Biases for Length Extrapolation
  - Attention Mechanism
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2503.08067
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:52:27.968250",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Relative Positional Encoding",
    "Context-Aware Biases for Length Extrapolation",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Relative Positional Encoding": 0.78,
    "Context-Aware Biases for Length Extrapolation": 0.8,
    "Attention Mechanism": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformers",
        "canonical": "Transformer",
        "aliases": [
          "Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the paper's discussion on length extrapolation and positional encoding.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Relative Positional Encoding",
        "canonical": "Relative Positional Encoding",
        "aliases": [
          "RPE"
        ],
        "category": "specific_connectable",
        "rationale": "RPE is a key concept in addressing the paper's main problem of length extrapolation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Context-Aware Biases for Length Extrapolation",
        "canonical": "Context-Aware Biases for Length Extrapolation",
        "aliases": [
          "CABLE"
        ],
        "category": "unique_technical",
        "rationale": "CABLE is the novel method proposed in the paper, offering a unique approach to positional encoding.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Attention Head",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Heads"
        ],
        "category": "specific_connectable",
        "rationale": "Attention heads are crucial for understanding how CABLE modifies transformer behavior.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Relative Positional Encoding",
      "resolved_canonical": "Relative Positional Encoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Context-Aware Biases for Length Extrapolation",
      "resolved_canonical": "Context-Aware Biases for Length Extrapolation",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Attention Head",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Context-aware Biases for Length Extrapolation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.08067.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2503.08067](https://arxiv.org/abs/2503.08067)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features_20250923|Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features]] (82.2% similar)
- [[2025-09-23/Language Modeling with Learned Meta-Tokens_20250923|Language Modeling with Learned Meta-Tokens]] (80.5% similar)
- [[2025-09-22/BEFT_ Bias-Efficient Fine-Tuning of Language Models_20250922|BEFT: Bias-Efficient Fine-Tuning of Language Models]] (80.0% similar)
- [[2025-09-23/Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry_20250923|Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry]] (79.4% similar)
- [[2025-09-23/Efficient Beam Search for Large Language Models Using Trie-Based Decoding_20250923|Efficient Beam Search for Large Language Models Using Trie-Based Decoding]] (79.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Relative Positional Encoding|Relative Positional Encoding]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Context-Aware Biases for Length Extrapolation|Context-Aware Biases for Length Extrapolation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.08067v3 Announce Type: replace 
Abstract: Transformers often struggle to generalize to longer sequences than those seen during training, a limitation known as length extrapolation. Most existing Relative Positional Encoding (RPE) methods attempt to address this by introducing either fixed linear biases or globally learned biases, which lack the capacity to adapt to different input contexts. In this work, we propose an additive RPE, Context-Aware Biases for Length Extrapolation (CABLE), a method that learns token-specific, context-aware biases for each attention head in transformers. By dynamically adjusting positional biases based on the input sequence, CABLE overcomes the rigidity of fixed RPEs. When evaluated on sequences longer than originally trained with, GPT-2 Medium (334M parameters) with CABLE achieves lower perplexity than counterparts using other widely adopted positional encoding methods. Additionally, by applying CABLE to the BERT base model we improved performance in long-context retrieval tasks. Our method significantly enhances the extrapolation performance of existing RPE methods tested on the FineWeb-Edu-10B and WikiText-103 datasets. Our code is available at: https://github.com/AlgonetLabs/Cable.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ê¸¸ì´ ì™¸ì‚½ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ìƒëŒ€ì  ìœ„ì¹˜ ì¸ì½”ë”©(RPE) ë°©ë²•ì¸ CABLEì„ ì œì•ˆí•©ë‹ˆë‹¤. CABLEì€ ê° ì£¼ì˜ í—¤ë“œì— ëŒ€í•´ í† í°ë³„ë¡œ ë¬¸ë§¥ì— ë§ì¶˜ ë°”ì´ì–´ìŠ¤ë¥¼ í•™ìŠµí•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ì— ë”°ë¼ ìœ„ì¹˜ ë°”ì´ì–´ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì¡´ì˜ ê³ ì •ëœ RPEì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , GPT-2 Medium ëª¨ë¸ì—ì„œ ë‹¤ë¥¸ ìœ„ì¹˜ ì¸ì½”ë”© ë°©ë²•ë³´ë‹¤ ë‚®ì€ í¼í”Œë ‰ì‹œí‹°ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, BERT ëª¨ë¸ì— CABLEì„ ì ìš©í•˜ì—¬ ê¸´ ë¬¸ë§¥ ê²€ìƒ‰ ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, FineWeb-Edu-10B ë° WikiText-103 ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ RPE ë°©ë²•ì˜ ì™¸ì‚½ ì„±ëŠ¥ì„ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transformer ëª¨ë¸ì˜ ê¸¸ì´ ì™¸ì‚½ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¬¸ë§¥ ì¸ì‹ ë°”ì´ì–´ìŠ¤ë¥¼ í•™ìŠµí•˜ëŠ” CABLE ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. CABLEì€ ì…ë ¥ ì‹œí€€ìŠ¤ì— ë”°ë¼ ìœ„ì¹˜ ë°”ì´ì–´ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ê³ ì •ëœ ìƒëŒ€ì  ìœ„ì¹˜ ì¸ì½”ë”©(RPE)ì˜ ê²½ì§ì„±ì„ ê·¹ë³µí•©ë‹ˆë‹¤.
- 3. CABLEì„ ì ìš©í•œ GPT-2 Medium ëª¨ë¸ì€ ê¸°ì¡´ì˜ ìœ„ì¹˜ ì¸ì½”ë”© ë°©ë²•ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë‚®ì€ í¼í”Œë ‰ì‹œí‹°ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 4. BERT base ëª¨ë¸ì— CABLEì„ ì ìš©í•˜ì—¬ ê¸´ ë¬¸ë§¥ ê²€ìƒ‰ ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 5. FineWeb-Edu-10B ë° WikiText-103 ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ RPE ë°©ë²•ì˜ ì™¸ì‚½ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:52:27*