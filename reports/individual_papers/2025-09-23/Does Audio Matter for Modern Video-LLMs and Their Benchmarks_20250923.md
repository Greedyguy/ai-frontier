---
keywords:
  - Vision-Language Model
  - Audio-Sensitive Evaluation
  - Audio Encoder
  - Audio-Visual Question Answering
  - Token Compression
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17901
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:05:26.515976",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Audio-Sensitive Evaluation",
    "Audio Encoder",
    "Audio-Visual Question Answering",
    "Token Compression"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Audio-Sensitive Evaluation": 0.78,
    "Audio Encoder": 0.8,
    "Audio-Visual Question Answering": 0.77,
    "Token Compression": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Video-LLMs",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Video Large Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "This term represents a key concept in the paper, linking vision and language processing in multimodal contexts.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "audio-sensitive subsets",
        "canonical": "Audio-Sensitive Evaluation",
        "aliases": [
          "audio-sensitive benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific evaluation focus that bridges audio and video analysis, crucial for understanding the paper's contributions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.67,
        "specificity_score": 0.79,
        "link_intent_score": 0.78
      },
      {
        "surface": "speech/audio encoder",
        "canonical": "Audio Encoder",
        "aliases": [
          "speech encoder",
          "audio processing module"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's methodology, enabling integration of audio data into video models.",
        "novelty_score": 0.48,
        "connectivity_score": 0.82,
        "specificity_score": 0.76,
        "link_intent_score": 0.8
      },
      {
        "surface": "AVQA-Hard",
        "canonical": "Audio-Visual Question Answering",
        "aliases": [
          "AVQA",
          "AVQA-Hard dataset"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific dataset introduced in the paper, critical for evaluating audio-visual models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mamba-based state-space token compressor",
        "canonical": "Token Compression",
        "aliases": [
          "Mamba compressor",
          "state-space token compression"
        ],
        "category": "unique_technical",
        "rationale": "A novel technical contribution addressing token explosion, relevant for model efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.83,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "muted videos",
      "single frame",
      "recent video benchmarks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Video-LLMs",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "audio-sensitive subsets",
      "resolved_canonical": "Audio-Sensitive Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.67,
        "specificity": 0.79,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "speech/audio encoder",
      "resolved_canonical": "Audio Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.82,
        "specificity": 0.76,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "AVQA-Hard",
      "resolved_canonical": "Audio-Visual Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mamba-based state-space token compressor",
      "resolved_canonical": "Token Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.83,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Does Audio Matter for Modern Video-LLMs and Their Benchmarks?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17901.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17901](https://arxiv.org/abs/2509.17901)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/SightSound-R1_ Cross-Modal Reasoning Distillation from Vision to Audio Language Models_20250922|SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models]] (85.4% similar)
- [[2025-09-18/Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing_20250918|Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing]] (84.1% similar)
- [[2025-09-18/Spatial Audio Motion Understanding and Reasoning_20250918|Spatial Audio Motion Understanding and Reasoning]] (84.1% similar)
- [[2025-09-23/AuditoryBench++_ Can Language Models Understand Auditory Knowledge without Hearing?_20250923|AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?]] (83.6% similar)
- [[2025-09-23/SoundMind_ RL-Incentivized Logic Reasoning for Audio-Language Models_20250923|SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Audio Encoder|Audio Encoder]]
**âš¡ Unique Technical**: [[keywords/Audio-Sensitive Evaluation|Audio-Sensitive Evaluation]], [[keywords/Audio-Visual Question Answering|Audio-Visual Question Answering]], [[keywords/Token Compression|Token Compression]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17901v1 Announce Type: new 
Abstract: Modern multimodal large language models often claim "video understanding," yet most evaluations use muted videos or simply discard audio. We ask a direct question: how much does audio actually matter for contemporary Video-LLMs and the benchmarks that certify them? We audit widely used suites and observe that many items are even solvable from a single frame, rendering audio largely redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio encoder (e.g., Whisper) and analyze when audio helps, while addressing audio token explosion with a lightweight Mamba-based state-space token compressor. We find that audio yields minimal gains on recent video benchmarks but is decisive on curated, audio-sensitive subsets. To enable faithful evaluation, we release AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a growing gap between current academic practice and real-world expectations, and provide practical tools for scalable audio-visual Video-LLMs. We will fully open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.

## ğŸ“ ìš”ì•½

í˜„ëŒ€ì˜ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì¢…ì¢… "ë¹„ë””ì˜¤ ì´í•´"ë¥¼ ì£¼ì¥í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ í‰ê°€ì—ì„œëŠ” ìŒì†Œê±°ëœ ë¹„ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì˜¤ë””ì˜¤ë¥¼ ë‹¨ìˆœíˆ ë¬´ì‹œí•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì˜¤ë””ì˜¤ê°€ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¡°ì‚¬í•˜ë©°, LLaVA-OneVision ì•„í‚¤í…ì²˜ì— ìŒì„±/ì˜¤ë””ì˜¤ ì¸ì½”ë”ë¥¼ ì¶”ê°€í•˜ì—¬ ì˜¤ë””ì˜¤ì˜ ì¤‘ìš”ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ìµœê·¼ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ì˜¤ë””ì˜¤ì˜ ê¸°ì—¬ê°€ ë¯¸ë¯¸í•˜ì§€ë§Œ, ì˜¤ë””ì˜¤ì— ë¯¼ê°í•œ íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œëŠ” ê²°ì •ì ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ AVQA-Hard ë° Music-AVQA-Hard ë°ì´í„°ì…‹ê³¼ ëª¨ë¸, ì½”ë“œë¥¼ ê³µê°œí•˜ë©°, í•™ê³„ì™€ ì‹¤ì œ ê¸°ëŒ€ì¹˜ ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í˜„ëŒ€ì˜ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ë“¤ì€ ì¢…ì¢… "ë¹„ë””ì˜¤ ì´í•´"ë¥¼ ì£¼ì¥í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ í‰ê°€ì—ì„œëŠ” ìŒì†Œê±°ëœ ë¹„ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì˜¤ë””ì˜¤ë¥¼ ë‹¨ìˆœíˆ ë¬´ì‹œí•©ë‹ˆë‹¤.
- 2. ë§ì€ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ í•­ëª©ë“¤ì´ ë‹¨ì¼ í”„ë ˆì„ìœ¼ë¡œë„ í•´ê²° ê°€ëŠ¥í•˜ì—¬ ì˜¤ë””ì˜¤ì˜ ì¤‘ìš”ì„±ì´ í¬ê²Œ ê°ì†Œí•©ë‹ˆë‹¤.
- 3. LLaVA-OneVision ì•„í‚¤í…ì²˜ì— ìŒì„±/ì˜¤ë””ì˜¤ ì¸ì½”ë”ë¥¼ ì¶”ê°€í•˜ì—¬ ì˜¤ë””ì˜¤ê°€ ì–¸ì œ ë„ì›€ì´ ë˜ëŠ”ì§€ ë¶„ì„í•˜ê³ , ê²½ëŸ‰ Mamba ê¸°ë°˜ í† í° ì••ì¶•ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ í† í° í­ë°œ ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
- 4. ìµœê·¼ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ì˜¤ë””ì˜¤ì˜ ì´ì ì´ ë¯¸ë¯¸í•˜ì§€ë§Œ, ì˜¤ë””ì˜¤ì— ë¯¼ê°í•œ íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œëŠ” ê²°ì •ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” í˜„ì¬ í•™ê³„ì˜ ì‹¤ì²œê³¼ ì‹¤ì œ ê¸°ëŒ€ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ë“œëŸ¬ë‚´ë©°, í™•ì¥ ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤-ë¹„ì£¼ì–¼ ë¹„ë””ì˜¤-LLMì„ ìœ„í•œ ì‹¤ìš©ì ì¸ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:05:26*