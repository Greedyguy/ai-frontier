---
keywords:
  - Referring Expression Segmentation
  - Adversarial Examples
  - Multimodal Learning
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.16157
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:25:03.288843",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Referring Expression Segmentation",
    "Adversarial Examples",
    "Multimodal Learning",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Referring Expression Segmentation": 0.78,
    "Adversarial Examples": 0.8,
    "Multimodal Learning": 0.77,
    "Vision-Language Model": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Referring Expression Segmentation",
        "canonical": "Referring Expression Segmentation",
        "aliases": [
          "RES"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's focus and represents a specific task in vision-language systems.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Adversarial Examples",
        "canonical": "Adversarial Examples",
        "aliases": [
          "Adversarial Attacks"
        ],
        "category": "specific_connectable",
        "rationale": "Adversarial examples are crucial for understanding model vulnerabilities, linking to security in AI systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Structure",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Systems"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is essential for integrating vision and language, a key aspect of the discussed models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language Systems",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Integration"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language models are central to the paper's theme, focusing on integrating visual and textual data.",
        "novelty_score": 0.48,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "object segmentation",
      "natural language descriptions",
      "privacy protection"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Referring Expression Segmentation",
      "resolved_canonical": "Referring Expression Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Adversarial Examples",
      "resolved_canonical": "Adversarial Examples",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Structure",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language Systems",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.16157.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.16157](https://arxiv.org/abs/2506.16157)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (82.0% similar)
- [[2025-09-19/LLM Agents at the Roundtable_ A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring_20250919|LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring]] (82.0% similar)
- [[2025-09-23/ADVEDM_Fine-grained Adversarial Attack against VLM-based Embodied Agents_20250923|ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents]] (81.0% similar)
- [[2025-09-22/Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models_20250922|Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models]] (80.7% similar)
- [[2025-09-23/Beyond Prompting_ An Efficient Embedding Framework for Open-Domain Question Answering_20250923|Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Adversarial Examples|Adversarial Examples]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Referring Expression Segmentation|Referring Expression Segmentation]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.16157v2 Announce Type: replace 
Abstract: Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tasks. Despite its impressive performance, the robustness of RES models against adversarial examples remains largely unexplored. While prior adversarial attack methods have explored adversarial robustness on conventional segmentation models, they perform poorly when directly applied to RES models, failing to expose vulnerabilities in its multimodal structure. In practical open-world scenarios, users typically issue multiple, diverse referring expressions to interact with the same image, highlighting the need for adversarial examples that generalize across varied textual inputs. Furthermore, from the perspective of privacy protection, ensuring that RES models do not segment sensitive content without explicit authorization is a crucial aspect of enhancing the robustness and security of multimodal vision-language systems. To address these challenges, we present PEAT, an Embedding-Guided Bidirectional Attack for RES models. Extensive experiments across multiple RES architectures and standard benchmarks show that PEAT consistently outperforms competitive baselines.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì—°ì–´ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ë‚´ ê°ì²´ë¥¼ ì •í™•í•˜ê²Œ ë¶„í• í•˜ëŠ” ê¸°ìˆ ì¸ ì§€ì‹œ í‘œí˜„ ë¶„í• (RES)ì˜ ì·¨ì•½ì ì„ íƒêµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì ëŒ€ì  ê³µê²© ë°©ë²•ì€ RES ëª¨ë¸ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ êµ¬ì¡°ì˜ ì·¨ì•½ì ì„ ì¶©ë¶„íˆ ë“œëŸ¬ë‚´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•´ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ì ëŒ€ì  ì˜ˆì œë¥¼ ì œì‹œí•  í•„ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ PEATë¼ëŠ” ì„ë² ë”© ê¸°ë°˜ ì–‘ë°©í–¥ ê³µê²© ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—¬ëŸ¬ RES ì•„í‚¤í…ì²˜ì™€ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, PEATëŠ” ê²½ìŸ ë°©ë²•ë“¤ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìì—°ì–´ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ë¥¼ ì •í™•í•˜ê²Œ ë¶„í• í•˜ëŠ” ì°¸ì¡° í‘œí˜„ ë¶„í• (RES)ì€ ì‹¤ì œ ë¹„ì „ ì‘ì—…ì—ì„œ ë†’ì€ ìœ ì—°ì„±ê³¼ ë„“ì€ ì ìš©ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì ëŒ€ì  ê³µê²© ë°©ë²•ì€ RES ëª¨ë¸ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ êµ¬ì¡°ì˜ ì·¨ì•½ì„±ì„ ë“œëŸ¬ë‚´ì§€ ëª»í•˜ì—¬, RES ëª¨ë¸ì— ì§ì ‘ ì ìš©í•  ê²½ìš° ì„±ëŠ¥ì´ ì €ì¡°í•©ë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•´ ì¼ë°˜í™”ëœ ì ëŒ€ì  ì˜ˆì œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©°, ì´ëŠ” ì‹¤ì œ ì˜¤í”ˆì›”ë“œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‚¬ìš©ìê°€ ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•´ ì—¬ëŸ¬ ì°¸ì¡° í‘œí˜„ì„ ë°œí–‰í•˜ëŠ” ê²½ìš°ì— í•„ìš”í•©ë‹ˆë‹¤.
- 4. í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ ê´€ì ì—ì„œ, RES ëª¨ë¸ì´ ëª…ì‹œì  ìŠ¹ì¸ ì—†ì´ ë¯¼ê°í•œ ì½˜í…ì¸ ë¥¼ ë¶„í• í•˜ì§€ ì•Šë„ë¡ ë³´ì¥í•˜ëŠ” ê²ƒì´ ë‹¤ì¤‘ ëª¨ë‹¬ ë¹„ì „-ì–¸ì–´ ì‹œìŠ¤í…œì˜ ê²¬ê³ ì„±ê³¼ ë³´ì•ˆì„ ê°•í™”í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.
- 5. PEATëŠ” RES ëª¨ë¸ì„ ìœ„í•œ ì„ë² ë”© ê¸°ë°˜ ì–‘ë°©í–¥ ê³µê²© ë°©ë²•ìœ¼ë¡œ, ì—¬ëŸ¬ RES ì•„í‚¤í…ì²˜ì™€ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê¸°ì¤€ì„ ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:25:03*