---
keywords:
  - VC-Inspector
  - Reference-Free Evaluation
  - Factual Grounding
  - Multimodal Learning
  - Large Language Model
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16538
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:38:30.426934",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "VC-Inspector",
    "Reference-Free Evaluation",
    "Factual Grounding",
    "Multimodal Learning",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "VC-Inspector": 0.8,
    "Reference-Free Evaluation": 0.78,
    "Factual Grounding": 0.77,
    "Multimodal Learning": 0.79,
    "Large Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "VC-Inspector",
        "canonical": "VC-Inspector",
        "aliases": [
          "Video Caption Inspector"
        ],
        "category": "unique_technical",
        "rationale": "VC-Inspector is a novel tool introduced in the paper, providing a unique method for evaluating video captions without references.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "reference-free evaluation",
        "canonical": "Reference-Free Evaluation",
        "aliases": [
          "non-reference evaluation"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper's contribution, offering a new paradigm in video caption evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "factual grounding",
        "canonical": "Factual Grounding",
        "aliases": [
          "fact-based grounding"
        ],
        "category": "specific_connectable",
        "rationale": "Factual grounding is crucial for ensuring the accuracy of video captions, linking to broader concepts in AI evaluation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.8,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "multimodal model",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal approach"
        ],
        "category": "specific_connectable",
        "rationale": "The use of multimodal models is a key aspect of the evaluation framework, connecting to ongoing trends in AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are foundational to the proposed evaluation framework, linking to a broad technical category.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "ground truth captions",
      "human annotations",
      "video domains"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "VC-Inspector",
      "resolved_canonical": "VC-Inspector",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "reference-free evaluation",
      "resolved_canonical": "Reference-Free Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "factual grounding",
      "resolved_canonical": "Factual Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.8,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "multimodal model",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Advancing Reference-free Evaluation of Video Captions with Factual Analysis

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16538.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16538](https://arxiv.org/abs/2509.16538)

## 🔗 유사한 논문
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.4% similar)
- [[2025-09-18/EdiVal-Agent_ An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing_20250918|EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing]] (82.1% similar)
- [[2025-09-18/Aligning Audio Captions with Human Preferences_20250918|Aligning Audio Captions with Human Preferences]] (81.6% similar)
- [[2025-09-23/ProtoVQA_ An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering_20250923|ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering]] (81.3% similar)
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (80.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Factual Grounding|Factual Grounding]], [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/VC-Inspector|VC-Inspector]]
**🚀 Evolved Concepts**: [[keywords/Reference-Free Evaluation|Reference-Free Evaluation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16538v1 Announce Type: cross 
Abstract: Video captions offer concise snapshots of actors, objects, and actions within a video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains.

## 📝 요약

이 논문은 비디오 캡션의 평가를 위한 참조 기반 평가의 한계를 극복하기 위해 참조가 필요 없는 평가 프레임워크를 제안합니다. 기존 모델은 다양한 비디오 도메인에서 성능 평가가 어려웠으나, 이 연구에서는 사실 기반의 평가를 통해 캡션 품질을 정확히 평가할 수 있는 VC-Inspector를 도입했습니다. 대형 언어 모델을 활용해 다양한 품질의 가상 캡션을 생성하고, 이를 통해 멀티모달 모델(Qwen2.5-VL)을 평가자로 훈련시켰습니다. 이 방법은 VATEX-Eval 데이터셋에서 인간의 판단과 높은 일치를 보였으며, 이미지 캡션 데이터셋에서도 우수한 성능을 보였습니다. VC-Inspector는 다양한 비디오 도메인에서 비디오 캡션의 사실적 정확성을 평가하는 데 있어 확장 가능하고 일반화 가능한 솔루션을 제공합니다.

## 🎯 주요 포인트

- 1. 비디오 캡션에 대한 인간 주석 획득은 비용이 많이 들거나 다양한 비디오 도메인에서는 실현 불가능할 수 있습니다.
- 2. 기존 모델은 참조 기반 평가 프로토콜에 의존하여 다양한 도메인에서 성능 평가에 어려움을 겪습니다.
- 3. 우리는 참조 캡션 없이도 캡션 품질을 평가할 수 있는 VC-Inspector라는 새로운 평가 프레임워크를 제안합니다.
- 4. VC-Inspector는 대규모 언어 모델을 활용하여 다양한 품질의 의사 캡션을 생성하고, 이를 통해 멀티모달 모델을 훈련합니다.
- 5. 우리의 접근 방식은 VATEX-Eval 데이터셋에서 인간 판단과의 우수한 정렬을 보여주며, 이미지 캡션 데이터셋에서도 일반화된 성능을 보입니다.


---

*Generated on 2025-09-24 03:38:30*