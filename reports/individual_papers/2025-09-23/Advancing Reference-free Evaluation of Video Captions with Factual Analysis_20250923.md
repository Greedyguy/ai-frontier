---
keywords:
  - VC-Inspector
  - Reference-Free Evaluation
  - Factual Grounding
  - Multimodal Learning
  - Large Language Model
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16538
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:38:30.426934",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "VC-Inspector",
    "Reference-Free Evaluation",
    "Factual Grounding",
    "Multimodal Learning",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "VC-Inspector": 0.8,
    "Reference-Free Evaluation": 0.78,
    "Factual Grounding": 0.77,
    "Multimodal Learning": 0.79,
    "Large Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "VC-Inspector",
        "canonical": "VC-Inspector",
        "aliases": [
          "Video Caption Inspector"
        ],
        "category": "unique_technical",
        "rationale": "VC-Inspector is a novel tool introduced in the paper, providing a unique method for evaluating video captions without references.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "reference-free evaluation",
        "canonical": "Reference-Free Evaluation",
        "aliases": [
          "non-reference evaluation"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper's contribution, offering a new paradigm in video caption evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "factual grounding",
        "canonical": "Factual Grounding",
        "aliases": [
          "fact-based grounding"
        ],
        "category": "specific_connectable",
        "rationale": "Factual grounding is crucial for ensuring the accuracy of video captions, linking to broader concepts in AI evaluation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.8,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "multimodal model",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal approach"
        ],
        "category": "specific_connectable",
        "rationale": "The use of multimodal models is a key aspect of the evaluation framework, connecting to ongoing trends in AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are foundational to the proposed evaluation framework, linking to a broad technical category.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "ground truth captions",
      "human annotations",
      "video domains"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "VC-Inspector",
      "resolved_canonical": "VC-Inspector",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "reference-free evaluation",
      "resolved_canonical": "Reference-Free Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "factual grounding",
      "resolved_canonical": "Factual Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.8,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "multimodal model",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Advancing Reference-free Evaluation of Video Captions with Factual Analysis

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16538.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16538](https://arxiv.org/abs/2509.16538)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.4% similar)
- [[2025-09-18/EdiVal-Agent_ An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing_20250918|EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing]] (82.1% similar)
- [[2025-09-18/Aligning Audio Captions with Human Preferences_20250918|Aligning Audio Captions with Human Preferences]] (81.6% similar)
- [[2025-09-23/ProtoVQA_ An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering_20250923|ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering]] (81.3% similar)
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (80.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Factual Grounding|Factual Grounding]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/VC-Inspector|VC-Inspector]]
**ğŸš€ Evolved Concepts**: [[keywords/Reference-Free Evaluation|Reference-Free Evaluation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16538v1 Announce Type: cross 
Abstract: Video captions offer concise snapshots of actors, objects, and actions within a video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ë””ì˜¤ ìº¡ì…˜ì˜ í‰ê°€ë¥¼ ìœ„í•œ ì°¸ì¡° ê¸°ë°˜ í‰ê°€ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì°¸ì¡°ê°€ í•„ìš” ì—†ëŠ” í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ í‰ê°€ê°€ ì–´ë ¤ì› ìœ¼ë‚˜, ì´ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì‹¤ ê¸°ë°˜ì˜ í‰ê°€ë¥¼ í†µí•´ ìº¡ì…˜ í’ˆì§ˆì„ ì •í™•íˆ í‰ê°€í•  ìˆ˜ ìˆëŠ” VC-Inspectorë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•´ ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ê°€ìƒ ìº¡ì…˜ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(Qwen2.5-VL)ì„ í‰ê°€ìë¡œ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ VATEX-Eval ë°ì´í„°ì…‹ì—ì„œ ì¸ê°„ì˜ íŒë‹¨ê³¼ ë†’ì€ ì¼ì¹˜ë¥¼ ë³´ì˜€ìœ¼ë©°, ì´ë¯¸ì§€ ìº¡ì…˜ ë°ì´í„°ì…‹ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. VC-InspectorëŠ” ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ë„ë©”ì¸ì—ì„œ ë¹„ë””ì˜¤ ìº¡ì…˜ì˜ ì‚¬ì‹¤ì  ì •í™•ì„±ì„ í‰ê°€í•˜ëŠ” ë° ìˆì–´ í™•ì¥ ê°€ëŠ¥í•˜ê³  ì¼ë°˜í™” ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ë””ì˜¤ ìº¡ì…˜ì— ëŒ€í•œ ì¸ê°„ ì£¼ì„ íšë“ì€ ë¹„ìš©ì´ ë§ì´ ë“¤ê±°ë‚˜ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ë„ë©”ì¸ì—ì„œëŠ” ì‹¤í˜„ ë¶ˆê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ ëª¨ë¸ì€ ì°¸ì¡° ê¸°ë°˜ í‰ê°€ í”„ë¡œí† ì½œì— ì˜ì¡´í•˜ì—¬ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ í‰ê°€ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” ì°¸ì¡° ìº¡ì…˜ ì—†ì´ë„ ìº¡ì…˜ í’ˆì§ˆì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” VC-Inspectorë¼ëŠ” ìƒˆë¡œìš´ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. VC-InspectorëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ì˜ì‚¬ ìº¡ì…˜ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
- 5. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ VATEX-Eval ë°ì´í„°ì…‹ì—ì„œ ì¸ê°„ íŒë‹¨ê³¼ì˜ ìš°ìˆ˜í•œ ì •ë ¬ì„ ë³´ì—¬ì£¼ë©°, ì´ë¯¸ì§€ ìº¡ì…˜ ë°ì´í„°ì…‹ì—ì„œë„ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:38:30*