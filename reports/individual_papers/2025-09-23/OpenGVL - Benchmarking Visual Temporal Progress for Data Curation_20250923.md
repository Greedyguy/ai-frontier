---
keywords:
  - Generative Value Learning
  - Vision-Language Model
  - OpenGVL
  - Temporal Progress Prediction
  - Robotics Data Curation
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17321
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:41:10.280411",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Generative Value Learning",
    "Vision-Language Model",
    "OpenGVL",
    "Temporal Progress Prediction",
    "Robotics Data Curation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Generative Value Learning": 0.78,
    "Vision-Language Model": 0.85,
    "OpenGVL": 0.8,
    "Temporal Progress Prediction": 0.77,
    "Robotics Data Curation": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Generative Value Learning",
        "canonical": "Generative Value Learning",
        "aliases": [
          "GVL"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach specific to the paper, enhancing connectivity with topics on task progress prediction.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Connects with the trending topic of integrating vision and language for task prediction.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "OpenGVL",
        "canonical": "OpenGVL",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific benchmark introduced in the paper, crucial for linking to data curation topics.",
        "novelty_score": 0.9,
        "connectivity_score": 0.65,
        "specificity_score": 0.92,
        "link_intent_score": 0.8
      },
      {
        "surface": "Temporal Progress Prediction",
        "canonical": "Temporal Progress Prediction",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A key concept for linking discussions on task completion and data annotation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Robotics Data Curation",
        "canonical": "Robotics Data Curation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for connecting with themes on data management and utilization in robotics.",
        "novelty_score": 0.65,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "task",
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Generative Value Learning",
      "resolved_canonical": "Generative Value Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "OpenGVL",
      "resolved_canonical": "OpenGVL",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.65,
        "specificity": 0.92,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Temporal Progress Prediction",
      "resolved_canonical": "Temporal Progress Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Robotics Data Curation",
      "resolved_canonical": "Robotics Data Curation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# OpenGVL - Benchmarking Visual Temporal Progress for Data Curation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17321.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17321](https://arxiv.org/abs/2509.17321)

## 🔗 유사한 논문
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (85.0% similar)
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (84.1% similar)
- [[2025-09-19/ForceVLA_ Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation_20250919|ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (83.6% similar)
- [[2025-09-19/ScaleCUA_ Scaling Open-Source Computer Use Agents with Cross-Platform Data_20250919|ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data]] (82.6% similar)
- [[2025-09-18/TrajBooster_ Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning_20250918|TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning]] (82.5% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Temporal Progress Prediction|Temporal Progress Prediction]], [[keywords/Robotics Data Curation|Robotics Data Curation]]
**⚡ Unique Technical**: [[keywords/Generative Value Learning|Generative Value Learning]], [[keywords/OpenGVL|OpenGVL]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17321v1 Announce Type: cross 
Abstract: Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \href{github.com/budzianowski/opengvl}{OpenGVL}.

## 📝 요약

이 논문은 로봇 공학에서 데이터 부족 문제를 해결하기 위해 제안된 Generative Value Learning(GVL) 접근법을 기반으로 한 OpenGVL 벤치마크를 소개합니다. OpenGVL은 다양한 로봇 및 인간 조작 작업에서 과제 진행 상황을 예측하는 데 중점을 두고 있습니다. 연구 결과, 공개된 오픈소스 모델이 비공개 모델에 비해 성능이 약 70%에 그친다는 것을 보여주었습니다. OpenGVL은 대규모 로봇 데이터셋의 자동화된 데이터 큐레이션 및 필터링 도구로 활용될 수 있으며, 이를 통해 데이터 품질 평가가 효율적으로 이루어질 수 있습니다. 연구팀은 OpenGVL 벤치마크와 전체 코드베이스를 공개했습니다.

## 🎯 주요 포인트

- 1. 데이터 부족은 로봇 공학 발전의 주요 제한 요인 중 하나로 남아 있지만, 야생에서 사용 가능한 로봇 데이터의 양이 기하급수적으로 증가하고 있다.
- 2. Generative Value Learning (GVL) 접근법은 시각-언어 모델(VLMs)에 내재된 지식을 활용하여 시각적 관찰로부터 작업 진행을 예측한다.
- 3. OpenGVL은 로봇 및 인간 구현을 포함한 다양한 조작 작업에서 작업 진행을 추정하기 위한 종합적인 벤치마크를 제안한다.
- 4. 공개된 오픈소스 모델들이 비공개 모델들에 비해 성능이 약 70%에 불과한 것으로 나타났다.
- 5. OpenGVL은 대규모 로봇 데이터셋의 효율적인 품질 평가를 가능하게 하는 자동 데이터 큐레이션 및 필터링의 실용적인 도구로 활용될 수 있다.


---

*Generated on 2025-09-24 03:41:10*