---
keywords:
  - Counterfactual Explanations
  - Deep Learning
  - Feature Space
  - Decision Boundaries
  - Mirror-CFE
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16822
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:35:35.402311",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Counterfactual Explanations",
    "Deep Learning",
    "Feature Space",
    "Decision Boundaries",
    "Mirror-CFE"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Counterfactual Explanations": 0.78,
    "Deep Learning": 0.72,
    "Feature Space": 0.79,
    "Decision Boundaries": 0.81,
    "Mirror-CFE": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Counterfactual explanations",
        "canonical": "Counterfactual Explanations",
        "aliases": [
          "CFE"
        ],
        "category": "unique_technical",
        "rationale": "Counterfactual explanations are crucial for understanding model decisions, offering a unique perspective in model interpretability.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Deep image classifiers",
        "canonical": "Deep Learning",
        "aliases": [
          "Deep Image Classification"
        ],
        "category": "broad_technical",
        "rationale": "Deep learning is a fundamental concept in the paper, linking it to broader discussions on neural networks and image processing.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Feature space",
        "canonical": "Feature Space",
        "aliases": [
          "Feature Representation"
        ],
        "category": "specific_connectable",
        "rationale": "Feature space is a critical element in understanding how classifiers make decisions, connecting to discussions on model architecture.",
        "novelty_score": 0.58,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "Decision boundaries",
        "canonical": "Decision Boundaries",
        "aliases": [
          "Classification Boundaries"
        ],
        "category": "specific_connectable",
        "rationale": "Decision boundaries are essential for understanding classifier behavior, providing strong links to model evaluation discussions.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.81
      },
      {
        "surface": "Mirror-CFE",
        "canonical": "Mirror-CFE",
        "aliases": [
          "Mirror Counterfactual Explanations"
        ],
        "category": "unique_technical",
        "rationale": "Mirror-CFE is a novel method introduced in the paper, offering a unique contribution to the field of model interpretability.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Counterfactual explanations",
      "resolved_canonical": "Counterfactual Explanations",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Deep image classifiers",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Feature space",
      "resolved_canonical": "Feature Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Decision boundaries",
      "resolved_canonical": "Decision Boundaries",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Mirror-CFE",
      "resolved_canonical": "Mirror-CFE",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16822.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16822](https://arxiv.org/abs/2509.16822)

## 🔗 유사한 논문
- [[2025-09-23/V-CECE_ Visual Counterfactual Explanations via Conceptual Edits_20250923|V-CECE: Visual Counterfactual Explanations via Conceptual Edits]] (82.7% similar)
- [[2025-09-19/MARIC_ Multi-Agent Reasoning for Image Classification_20250919|MARIC: Multi-Agent Reasoning for Image Classification]] (82.4% similar)
- [[2025-09-22/Shedding Light on Depth_ Explainability Assessment in Monocular Depth Estimation_20250922|Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation]] (82.3% similar)
- [[2025-09-18/Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients_20250918|Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients]] (81.4% similar)
- [[2025-09-23/From Easy to Hard_ The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning_20250923|From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning]] (81.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**🔗 Specific Connectable**: [[keywords/Feature Space|Feature Space]], [[keywords/Decision Boundaries|Decision Boundaries]]
**⚡ Unique Technical**: [[keywords/Counterfactual Explanations|Counterfactual Explanations]], [[keywords/Mirror-CFE|Mirror-CFE]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16822v1 Announce Type: new 
Abstract: Counterfactual explanations (CFE) for deep image classifiers aim to reveal how minimal input changes lead to different model decisions, providing critical insights for model interpretation and improvement. However, existing CFE methods often rely on additional image encoders and generative models to create plausible images, neglecting the classifier's own feature space and decision boundaries. As such, they do not explain the intrinsic feature space and decision boundaries learned by the classifier. To address this limitation, we propose Mirror-CFE, a novel method that generates faithful counterfactual explanations by operating directly in the classifier's feature space, treating decision boundaries as mirrors that ``reflect'' feature representations in the mirror. Mirror-CFE learns a mapping function from feature space to image space while preserving distance relationships, enabling smooth transitions between source images and their counterfactuals. Through extensive experiments on four image datasets, we demonstrate that Mirror-CFE achieves superior performance in validity while maintaining input resemblance compared to state-of-the-art explanation methods. Finally, mirror-CFE provides interpretable visualization of the classifier's decision process by generating step-wise transitions that reveal how features evolve as classification confidence changes.

## 📝 요약

이 논문은 심층 이미지 분류기의 반사실적 설명(CFE)을 개선하기 위해 Mirror-CFE라는 새로운 방법을 제안합니다. 기존 CFE 방법은 추가적인 이미지 인코더와 생성 모델에 의존하여 분류기의 고유한 특징 공간과 결정 경계를 설명하지 못하는 한계가 있습니다. Mirror-CFE는 분류기의 특징 공간에서 직접 작동하여 결정 경계를 '거울'로 활용, 특징 표현을 반사시킵니다. 이를 통해 특징 공간에서 이미지 공간으로의 매핑을 학습하며, 입력 이미지와 반사실적 이미지 간의 매끄러운 전환을 가능하게 합니다. 네 가지 이미지 데이터셋에 대한 실험 결과, Mirror-CFE는 기존 방법보다 높은 유효성을 유지하면서 입력 유사성을 유지하는 데 뛰어난 성능을 보였습니다. 또한, 분류기의 결정 과정을 단계별로 시각화하여 해석 가능한 설명을 제공합니다.

## 🎯 주요 포인트

- 1. Mirror-CFE는 분류기의 특징 공간에서 직접 작동하여 신뢰할 수 있는 반사적 반사 설명을 생성하는 새로운 방법입니다.
- 2. 이 방법은 특징 공간에서 이미지 공간으로의 매핑 함수를 학습하여 원본 이미지와 반사적 반사 이미지 간의 부드러운 전환을 가능하게 합니다.
- 3. Mirror-CFE는 네 가지 이미지 데이터셋에 대한 실험을 통해 최신 설명 방법과 비교하여 유효성에서 우수한 성능을 보여줍니다.
- 4. 이 방법은 분류기의 결정 과정을 해석 가능한 시각화로 제공하여 분류 신뢰도가 변화함에 따라 특징이 어떻게 진화하는지를 단계별로 드러냅니다.


---

*Generated on 2025-09-24 04:35:35*