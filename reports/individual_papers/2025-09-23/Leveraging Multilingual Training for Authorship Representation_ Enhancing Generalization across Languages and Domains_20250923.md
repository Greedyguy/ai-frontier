---
keywords:
  - Authorship Representation
  - Multilingual Training
  - Probabilistic Content Masking
  - Language-Aware Batching
  - Cross-Lingual Generalization
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16531
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:13:03.198923",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Authorship Representation",
    "Multilingual Training",
    "Probabilistic Content Masking",
    "Language-Aware Batching",
    "Cross-Lingual Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Authorship Representation": 0.78,
    "Multilingual Training": 0.8,
    "Probabilistic Content Masking": 0.77,
    "Language-Aware Batching": 0.75,
    "Cross-Lingual Generalization": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Authorship Representation",
        "canonical": "Authorship Representation",
        "aliases": [
          "AR"
        ],
        "category": "unique_technical",
        "rationale": "Authorship Representation is a specialized concept central to the paper, offering potential for unique connections in authorship analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multilingual Training",
        "canonical": "Multilingual Training",
        "aliases": [
          "Multilingual Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Multilingual Training is crucial for cross-lingual generalization, linking to broader discussions in multilingual NLP.",
        "novelty_score": 0.68,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Probabilistic Content Masking",
        "canonical": "Probabilistic Content Masking",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is a novel contribution of the paper, enhancing model focus on stylistic features.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Language-Aware Batching",
        "canonical": "Language-Aware Batching",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Language-Aware Batching is a unique method introduced to reduce cross-lingual interference, relevant for multilingual model training.",
        "novelty_score": 0.78,
        "connectivity_score": 0.63,
        "specificity_score": 0.86,
        "link_intent_score": 0.75
      },
      {
        "surface": "Cross-Lingual Generalization",
        "canonical": "Cross-Lingual Generalization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Cross-Lingual Generalization is a key outcome of the study, linking to broader themes in multilingual NLP.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Authorship Representation",
      "resolved_canonical": "Authorship Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multilingual Training",
      "resolved_canonical": "Multilingual Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Probabilistic Content Masking",
      "resolved_canonical": "Probabilistic Content Masking",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Language-Aware Batching",
      "resolved_canonical": "Language-Aware Batching",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.63,
        "specificity": 0.86,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Cross-Lingual Generalization",
      "resolved_canonical": "Cross-Lingual Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16531.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16531](https://arxiv.org/abs/2509.16531)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Catch Me If You Can? Not Yet_ LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors_20250918|Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors]] (82.9% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (82.1% similar)
- [[2025-09-22/Language Mixing in Reasoning Language Models_ Patterns, Impact, and Internal Causes_20250922|Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes]] (82.0% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (81.8% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multilingual Training|Multilingual Training]], [[keywords/Cross-Lingual Generalization|Cross-Lingual Generalization]]
**âš¡ Unique Technical**: [[keywords/Authorship Representation|Authorship Representation]], [[keywords/Probabilistic Content Masking|Probabilistic Content Masking]], [[keywords/Language-Aware Batching|Language-Aware Batching]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16531v1 Announce Type: new 
Abstract: Authorship representation (AR) learning, which models an author's unique writing style, has demonstrated strong performance in authorship attribution tasks. However, prior research has primarily focused on monolingual settings-mostly in English-leaving the potential benefits of multilingual AR models underexplored. We introduce a novel method for multilingual AR learning that incorporates two key innovations: probabilistic content masking, which encourages the model to focus on stylistically indicative words rather than content-specific words, and language-aware batching, which improves contrastive learning by reducing cross-lingual interference. Our model is trained on over 4.5 million authors across 36 languages and 13 domains. It consistently outperforms monolingual baselines in 21 out of 22 non-English languages, achieving an average Recall@8 improvement of 4.85%, with a maximum gain of 15.91% in a single language. Furthermore, it exhibits stronger cross-lingual and cross-domain generalization compared to a monolingual model trained solely on English. Our analysis confirms the effectiveness of both proposed techniques, highlighting their critical roles in the model's improved performance.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì €ì í‘œí˜„ í•™ìŠµ(AR)ì˜ ë‹¤êµ­ì–´ ëª¨ë¸ì„ ì œì•ˆí•˜ë©°, ì €ìì˜ ë…íŠ¹í•œ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ì˜ì–´ì— ì§‘ì¤‘ëœ ë‹¨ì¼ ì–¸ì–´ í™˜ê²½ì— ì´ˆì ì„ ë§ì·„ìœ¼ë‚˜, ë³¸ ì—°êµ¬ëŠ” ë‹¤êµ­ì–´ AR ëª¨ë¸ì˜ ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ í™•ë¥ ì  ì½˜í…ì¸  ë§ˆìŠ¤í‚¹ê³¼ ì–¸ì–´ ì¸ì‹ ë°°ì¹­ì„ í†µí•´ ìŠ¤íƒ€ì¼ì— ì˜í–¥ì„ ì£¼ëŠ” ë‹¨ì–´ì— ì§‘ì¤‘í•˜ê³ , ì–¸ì–´ ê°„ ê°„ì„­ì„ ì¤„ì—¬ ëŒ€ì¡° í•™ìŠµì„ ê°œì„ í•©ë‹ˆë‹¤. 36ê°œ ì–¸ì–´ì™€ 13ê°œ ë„ë©”ì¸ì—ì„œ 450ë§Œ ëª… ì´ìƒì˜ ì €ìë¥¼ ëŒ€ìƒìœ¼ë¡œ í›ˆë ¨ëœ ì´ ëª¨ë¸ì€ 22ê°œì˜ ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ ì¤‘ 21ê°œì—ì„œ ë‹¨ì¼ ì–¸ì–´ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë©°, í‰ê·  4.85%ì˜ Recall@8 í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ì˜ì–´ë¡œë§Œ í›ˆë ¨ëœ ë‹¨ì¼ ì–¸ì–´ ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì–¸ì–´ ê°„ ë° ë„ë©”ì¸ ê°„ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì œì•ˆëœ ë‘ ê°€ì§€ ê¸°ìˆ ì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì €ì í‘œí˜„ í•™ìŠµì€ ì €ìì˜ ë…íŠ¹í•œ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ì„ ëª¨ë¸ë§í•˜ì—¬ ì €ì ì‹ë³„ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ë‹¤êµ­ì–´ ì €ì í‘œí˜„ í•™ìŠµì„ ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ë©°, í™•ë¥ ì  ì½˜í…ì¸  ë§ˆìŠ¤í‚¹ê³¼ ì–¸ì–´ ì¸ì‹ ë°°ì¹­ì„ ë„ì…í•˜ì˜€ë‹¤.
- 3. ì œì•ˆëœ ëª¨ë¸ì€ 36ê°œ ì–¸ì–´ì™€ 13ê°œ ë„ë©”ì¸ì—ì„œ 450ë§Œ ëª… ì´ìƒì˜ ì €ìë¥¼ ëŒ€ìƒìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, 22ê°œì˜ ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ ì¤‘ 21ê°œ ì–¸ì–´ì—ì„œ ë‹¨ì¼ ì–¸ì–´ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ì˜€ë‹¤.
- 4. í‰ê·  Recall@8ì´ 4.85% ê°œì„ ë˜ì—ˆê³ , íŠ¹ì • ì–¸ì–´ì—ì„œëŠ” ìµœëŒ€ 15.91%ì˜ í–¥ìƒì„ ë³´ì˜€ë‹¤.
- 5. ì œì•ˆëœ ê¸°ë²•ë“¤ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, êµì°¨ ì–¸ì–´ ë° êµì°¨ ë„ë©”ì¸ ì¼ë°˜í™”ì—ì„œë„ ê°•ì ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 03:13:03*