---
keywords:
  - Authorship Representation
  - Multilingual Training
  - Probabilistic Content Masking
  - Language-Aware Batching
  - Cross-Lingual Generalization
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16531
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:13:03.198923",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Authorship Representation",
    "Multilingual Training",
    "Probabilistic Content Masking",
    "Language-Aware Batching",
    "Cross-Lingual Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Authorship Representation": 0.78,
    "Multilingual Training": 0.8,
    "Probabilistic Content Masking": 0.77,
    "Language-Aware Batching": 0.75,
    "Cross-Lingual Generalization": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Authorship Representation",
        "canonical": "Authorship Representation",
        "aliases": [
          "AR"
        ],
        "category": "unique_technical",
        "rationale": "Authorship Representation is a specialized concept central to the paper, offering potential for unique connections in authorship analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multilingual Training",
        "canonical": "Multilingual Training",
        "aliases": [
          "Multilingual Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Multilingual Training is crucial for cross-lingual generalization, linking to broader discussions in multilingual NLP.",
        "novelty_score": 0.68,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Probabilistic Content Masking",
        "canonical": "Probabilistic Content Masking",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is a novel contribution of the paper, enhancing model focus on stylistic features.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Language-Aware Batching",
        "canonical": "Language-Aware Batching",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Language-Aware Batching is a unique method introduced to reduce cross-lingual interference, relevant for multilingual model training.",
        "novelty_score": 0.78,
        "connectivity_score": 0.63,
        "specificity_score": 0.86,
        "link_intent_score": 0.75
      },
      {
        "surface": "Cross-Lingual Generalization",
        "canonical": "Cross-Lingual Generalization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Cross-Lingual Generalization is a key outcome of the study, linking to broader themes in multilingual NLP.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Authorship Representation",
      "resolved_canonical": "Authorship Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multilingual Training",
      "resolved_canonical": "Multilingual Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Probabilistic Content Masking",
      "resolved_canonical": "Probabilistic Content Masking",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Language-Aware Batching",
      "resolved_canonical": "Language-Aware Batching",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.63,
        "specificity": 0.86,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Cross-Lingual Generalization",
      "resolved_canonical": "Cross-Lingual Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16531.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16531](https://arxiv.org/abs/2509.16531)

## 🔗 유사한 논문
- [[2025-09-18/Catch Me If You Can? Not Yet_ LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors_20250918|Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors]] (82.9% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (82.1% similar)
- [[2025-09-22/Language Mixing in Reasoning Language Models_ Patterns, Impact, and Internal Causes_20250922|Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes]] (82.0% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (81.8% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multilingual Training|Multilingual Training]], [[keywords/Cross-Lingual Generalization|Cross-Lingual Generalization]]
**⚡ Unique Technical**: [[keywords/Authorship Representation|Authorship Representation]], [[keywords/Probabilistic Content Masking|Probabilistic Content Masking]], [[keywords/Language-Aware Batching|Language-Aware Batching]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16531v1 Announce Type: new 
Abstract: Authorship representation (AR) learning, which models an author's unique writing style, has demonstrated strong performance in authorship attribution tasks. However, prior research has primarily focused on monolingual settings-mostly in English-leaving the potential benefits of multilingual AR models underexplored. We introduce a novel method for multilingual AR learning that incorporates two key innovations: probabilistic content masking, which encourages the model to focus on stylistically indicative words rather than content-specific words, and language-aware batching, which improves contrastive learning by reducing cross-lingual interference. Our model is trained on over 4.5 million authors across 36 languages and 13 domains. It consistently outperforms monolingual baselines in 21 out of 22 non-English languages, achieving an average Recall@8 improvement of 4.85%, with a maximum gain of 15.91% in a single language. Furthermore, it exhibits stronger cross-lingual and cross-domain generalization compared to a monolingual model trained solely on English. Our analysis confirms the effectiveness of both proposed techniques, highlighting their critical roles in the model's improved performance.

## 📝 요약

이 논문은 저자 표현 학습(AR)의 다국어 모델을 제안하며, 저자의 독특한 글쓰기 스타일을 효과적으로 모델링하는 방법을 소개합니다. 기존 연구는 주로 영어에 집중된 단일 언어 환경에 초점을 맞췄으나, 본 연구는 다국어 AR 모델의 가능성을 탐구합니다. 제안된 방법은 확률적 콘텐츠 마스킹과 언어 인식 배칭을 통해 스타일에 영향을 주는 단어에 집중하고, 언어 간 간섭을 줄여 대조 학습을 개선합니다. 36개 언어와 13개 도메인에서 450만 명 이상의 저자를 대상으로 훈련된 이 모델은 22개의 비영어권 언어 중 21개에서 단일 언어 기준 모델을 능가하며, 평균 4.85%의 Recall@8 향상을 보였습니다. 또한, 영어로만 훈련된 단일 언어 모델보다 뛰어난 언어 간 및 도메인 간 일반화 성능을 보였습니다. 제안된 두 가지 기술의 효과를 분석하여 모델 성능 향상에 중요한 역할을 한다는 것을 확인했습니다.

## 🎯 주요 포인트

- 1. 저자 표현 학습은 저자의 독특한 글쓰기 스타일을 모델링하여 저자 식별 작업에서 강력한 성능을 보여주었다.
- 2. 본 연구는 다국어 저자 표현 학습을 위한 새로운 방법을 제안하며, 확률적 콘텐츠 마스킹과 언어 인식 배칭을 도입하였다.
- 3. 제안된 모델은 36개 언어와 13개 도메인에서 450만 명 이상의 저자를 대상으로 훈련되었으며, 22개의 비영어권 언어 중 21개 언어에서 단일 언어 기준 모델을 능가하였다.
- 4. 평균 Recall@8이 4.85% 개선되었고, 특정 언어에서는 최대 15.91%의 향상을 보였다.
- 5. 제안된 기법들은 모델의 성능 향상에 중요한 역할을 하며, 교차 언어 및 교차 도메인 일반화에서도 강점을 보였다.


---

*Generated on 2025-09-24 03:13:03*