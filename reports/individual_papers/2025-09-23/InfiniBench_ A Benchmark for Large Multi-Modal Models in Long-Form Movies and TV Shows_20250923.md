---
keywords:
  - Multimodal Learning
  - Long Video Understanding
  - Vision-Language Model
  - Grounding-Based Skills
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2406.19875
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:14:26.216558",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Long Video Understanding",
    "Vision-Language Model",
    "Grounding-Based Skills"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Long Video Understanding": 0.78,
    "Vision-Language Model": 0.82,
    "Grounding-Based Skills": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "multi-modal models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multi-modal learning",
          "multimodal models"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is crucial for understanding complex video content, linking vision and language processing.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "long video understanding",
        "canonical": "Long Video Understanding",
        "aliases": [
          "long-form video comprehension",
          "extended video analysis"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique challenge in video processing that requires specialized techniques, offering new research opportunities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "vision-language models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "vision-language systems",
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language models are at the forefront of integrating visual and textual data, crucial for multimodal tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "grounding-based skills",
        "canonical": "Grounding-Based Skills",
        "aliases": [
          "grounding tasks",
          "scene grounding"
        ],
        "category": "unique_technical",
        "rationale": "These skills are essential for understanding spatial and temporal contexts in videos, linking to cognitive processing.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "metadata"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "multi-modal models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "long video understanding",
      "resolved_canonical": "Long Video Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "vision-language models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "grounding-based skills",
      "resolved_canonical": "Grounding-Based Skills",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2406.19875.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2406.19875](https://arxiv.org/abs/2406.19875)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/All-in-one_ Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark_20250923|All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark]] (83.1% similar)
- [[2025-09-17/Dense Video Understanding with Gated Residual Tokenization_20250917|Dense Video Understanding with Gated Residual Tokenization]] (83.0% similar)
- [[2025-09-23/RealBench_ A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios_20250923|RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios]] (82.1% similar)
- [[2025-09-19/MovieCORE_ COgnitive REasoning in Movies_20250919|MovieCORE: COgnitive REasoning in Movies]] (82.0% similar)
- [[2025-09-23/NUMINA_ A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities_20250923|NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Long Video Understanding|Long Video Understanding]], [[keywords/Grounding-Based Skills|Grounding-Based Skills]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2406.19875v4 Announce Type: replace 
Abstract: Understanding long-form videos, such as movies and TV episodes ranging from tens of minutes to two hours, remains a significant challenge for multi-modal models. Existing benchmarks often fail to test the full range of cognitive skills needed to process these temporally rich and narratively complex inputs. Therefore, we introduce InfiniBench, a comprehensive benchmark designed to evaluate the capabilities of models in long video understanding rigorously. InfiniBench offers:(1) Over 1,000 hours of video content, with an average video length of 53 minutes. (2) The largest set of question-answer pairs for long video comprehension, totaling around 87.7 K. (3) Eight diverse skills that span both grounding-based (e.g., scene transitions, character actions) and reasoning-based (e.g., deep context understanding, multi-event linking). (4) Rich annotation formats, including both multiple-choice and open-ended questions. We conducted an in-depth evaluation across both commercial (GPT-4o, Gemini 2.0 Flash) and most recent open-source vision-language models such as Qwen2.5-VL, InternVL3.0). Results reveal that:(1) Models struggle across the board: Even the best model, GPT-4o, achieves only 47.1 % on grounding-based skills, with most models performing near or just above random chance. (2) Strong reliance on world knowledge: Models achieve surprisingly high scores using only metadata (e.g., video titles), highlighting a tendency to rely on pre-trained knowledge rather than actual visual or temporal understanding. (3) Multi-Modal Importance: When provided with full video and subtitle context, however, models show substantial improvements, confirming the critical role of multimodal input in video understanding. InfiniBench is publicly available at https://vision-cair.github.io/Infinibench

## ğŸ“ ìš”ì•½

ì¥í¸ ë¹„ë””ì˜¤ ì´í•´ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ì—ê²Œ ì—¬ì „íˆ í° ë„ì „ ê³¼ì œì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ InfiniBenchë¼ëŠ” í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. InfiniBenchëŠ” í‰ê·  53ë¶„ ê¸¸ì´ì˜ 1,000ì‹œê°„ ì´ìƒì˜ ë¹„ë””ì˜¤ ì½˜í…ì¸ ì™€ ì•½ 87,700ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì¥ë©´ ì „í™˜, ìºë¦­í„° í–‰ë™ ë“±ì˜ ê¸°ì´ˆì  ì´í•´ì™€ ê¹Šì€ ë¬¸ë§¥ ì´í•´, ë‹¤ì¤‘ ì´ë²¤íŠ¸ ì—°ê²° ë“±ì˜ ì¶”ë¡  ê¸°ë°˜ ê¸°ìˆ ì„ í‰ê°€í•©ë‹ˆë‹¤. í‰ê°€ ê²°ê³¼, ëª¨ë¸ë“¤ì€ ì „ë°˜ì ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, íŠ¹íˆ GPT-4o ëª¨ë¸ë„ ê¸°ì´ˆì  ì´í•´ì—ì„œ 47.1%ì˜ ì„±ê³¼ë§Œì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ëª¨ë¸ë“¤ì€ ì‚¬ì „ í•™ìŠµëœ ì§€ì‹ì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ì „ì²´ ë¹„ë””ì˜¤ì™€ ìë§‰ ë§¥ë½ì´ ì œê³µë  ë•Œ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤. InfiniBenchëŠ” ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. InfiniBenchëŠ” ì¥ì‹œê°„ ë¹„ë””ì˜¤ ì´í•´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¡œ, í‰ê·  53ë¶„ ê¸¸ì´ì˜ ë¹„ë””ì˜¤ ì½˜í…ì¸  1,000ì‹œê°„ ì´ìƒì„ í¬í•¨í•©ë‹ˆë‹¤.
- 2. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì¥ì‹œê°„ ë¹„ë””ì˜¤ ì´í•´ë¥¼ ìœ„í•œ ì•½ 87,700ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì¸ì§€ ê¸°ìˆ ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 3. InfiniBenchëŠ” ì¥ë©´ ì „í™˜, ìºë¦­í„° í–‰ë™ê³¼ ê°™ì€ ê¸°ë°˜ ê¸°ìˆ ê³¼ ê¹Šì€ ë§¥ë½ ì´í•´, ë‹¤ì¤‘ ì´ë²¤íŠ¸ ì—°ê²°ê³¼ ê°™ì€ ì¶”ë¡  ê¸°ìˆ ì„ í¬í•¨í•œ 8ê°œì˜ ë‹¤ì–‘í•œ ê¸°ìˆ ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 4. ìƒì—…ì  ëª¨ë¸ê³¼ ìµœì‹  ì˜¤í”ˆ ì†ŒìŠ¤ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ëª¨ë¸ë“¤ì€ ì „ë°˜ì ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, íŠ¹íˆ ì‚¬ì „ ì§€ì‹ì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. ì „ì²´ ë¹„ë””ì˜¤ ë° ìë§‰ ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë  ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ë©°, ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì˜ ì¤‘ìš”ì„±ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:14:26*