---
keywords:
  - Large Language Model
  - Trauma-Informed Dialogue for Empathy
  - Zero-Shot Learning
  - Empathetic Response Generation
  - Knowledge Transfer Limitation
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.15065
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:59:45.849042",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Trauma-Informed Dialogue for Empathy",
    "Zero-Shot Learning",
    "Empathetic Response Generation",
    "Knowledge Transfer Limitation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Trauma-Informed Dialogue for Empathy": 0.78,
    "Zero-Shot Learning": 0.8,
    "Empathetic Response Generation": 0.77,
    "Knowledge Transfer Limitation": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "small language models",
        "canonical": "Large Language Model",
        "aliases": [
          "small LLMs",
          "language models"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader concept of language models, crucial for understanding the context of empathy generation.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Trauma-Informed Dialogue for Empathy",
        "canonical": "Trauma-Informed Dialogue for Empathy",
        "aliases": [
          "TIDE"
        ],
        "category": "unique_technical",
        "rationale": "Represents a unique dataset that is central to the study, offering a specific point of connection for PTSD dialogue support.",
        "novelty_score": 0.75,
        "connectivity_score": 0.67,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-shot settings",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the concept of learning without prior examples, relevant for evaluating model performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "empathetic responses",
        "canonical": "Empathetic Response Generation",
        "aliases": [
          "empathy generation"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the core goal of the models, providing a unique link to emotional intelligence in AI.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "knowledge transfer ceiling",
        "canonical": "Knowledge Transfer Limitation",
        "aliases": [
          "transfer ceiling"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific challenge in model training, offering insight into model limitations.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "Claude Sonnet 3.5",
      "p = .004",
      "p > 0.15"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "small language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Trauma-Informed Dialogue for Empathy",
      "resolved_canonical": "Trauma-Informed Dialogue for Empathy",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.67,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-shot settings",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "empathetic responses",
      "resolved_canonical": "Empathetic Response Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "knowledge transfer ceiling",
      "resolved_canonical": "Knowledge Transfer Limitation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.15065.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.15065](https://arxiv.org/abs/2505.15065)

## 🔗 유사한 논문
- [[2025-09-22/Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter_20250922|Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter]] (83.9% similar)
- [[2025-09-23/How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues_20250923|How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues]] (83.4% similar)
- [[2025-09-23/Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression_ A Feasibility Study_20250923|Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression: A Feasibility Study]] (83.2% similar)
- [[2025-09-22/Empathy-R1_ A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support_20250922|Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support]] (83.2% similar)
- [[2025-09-23/SENSE-7_ Taxonomy and Dataset for Measuring User Perceptions of Empathy in Sustained Human-AI Conversations_20250923|SENSE-7: Taxonomy and Dataset for Measuring User Perceptions of Empathy in Sustained Human-AI Conversations]] (82.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Trauma-Informed Dialogue for Empathy|Trauma-Informed Dialogue for Empathy]], [[keywords/Empathetic Response Generation|Empathetic Response Generation]], [[keywords/Knowledge Transfer Limitation|Knowledge Transfer Limitation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.15065v2 Announce Type: replace-cross 
Abstract: This paper investigates the capacity of small language models (0.5B-5B parameters) to generate empathetic responses for individuals with PTSD. We introduce Trauma-Informed Dialogue for Empathy (TIDE), a novel dataset comprising 10,000 two-turn conversations across 500 diverse, clinically-grounded PTSD personas (https://huggingface.co/datasets/yenopoya/TIDE). Using frontier model outputs as ground truth, we evaluate eight small LLMs in zero-shot settings and after fine-tuning. Fine-tuning enhances empathetic capabilities, improving cosine similarity and perceived empathy, although gains vary across emotional scenarios and smaller models exhibit a "knowledge transfer ceiling." As expected, Claude Sonnet 3.5 consistently outperforms all models, but surprisingly, the smaller models often approach human-rated empathy levels. Demographic analyses showed that older adults favored responses that validated distress before offering support (p = .004), while graduate-educated users preferred emotionally layered replies in specific scenarios. Gender-based differences were minimal (p > 0.15), suggesting the feasibility of broadly empathetic model designs. This work offers insights into building resource-efficient, emotionally intelligent systems for mental health support.

## 📝 요약

이 논문은 작은 언어 모델(0.5B-5B 매개변수)이 PTSD 환자를 위한 공감적 응답을 생성하는 능력을 조사합니다. 저자들은 500개의 다양한 PTSD 페르소나를 기반으로 한 10,000개의 대화를 포함하는 새로운 데이터셋인 TIDE를 소개합니다. 제로샷 설정과 미세 조정 후 8개의 작은 LLM을 평가한 결과, 미세 조정은 공감 능력을 향상시키지만, 감정적 시나리오에 따라 성과가 다르게 나타났습니다. Claude Sonnet 3.5 모델이 가장 우수했지만, 작은 모델들도 인간 수준의 공감에 근접했습니다. 인구통계학적 분석에서는 나이 든 성인이 고통을 인정하는 응답을 선호하고, 고학력 사용자는 감정적으로 복잡한 응답을 선호하는 경향을 보였습니다. 성별 차이는 미미했으며, 이는 광범위한 공감적 모델 설계의 가능성을 시사합니다. 이 연구는 정신 건강 지원을 위한 자원 효율적이고 감정적으로 지능적인 시스템 구축에 대한 통찰을 제공합니다.

## 🎯 주요 포인트

- 1. 본 연구는 소형 언어 모델(0.5B-5B 매개변수)이 PTSD 환자를 위한 공감적 응답을 생성할 수 있는 능력을 조사합니다.
- 2. Trauma-Informed Dialogue for Empathy (TIDE)라는 새로운 데이터셋을 소개하며, 이는 500개의 다양한 PTSD 페르소나를 기반으로 한 10,000개의 대화를 포함합니다.
- 3. 모델 미세 조정은 공감 능력을 향상시키며, 특히 Claude Sonnet 3.5 모델이 가장 우수한 성능을 보입니다.
- 4. 인구통계학적 분석 결과, 나이 든 성인은 고통을 먼저 인정하는 응답을 선호하고, 대학원 교육을 받은 사용자는 감정적으로 복잡한 응답을 선호하는 경향이 있습니다.
- 5. 성별에 따른 차이는 미미하여, 폭넓은 공감적 모델 설계가 가능함을 시사합니다.


---

*Generated on 2025-09-24 00:59:45*