---
keywords:
  - Large Language Model
  - Remote Sensing
  - Visual Prompting
  - Cross-domain Fusion
  - Spatial Reasoning
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2504.12795
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:20:53.250832",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Remote Sensing",
    "Visual Prompting",
    "Cross-domain Fusion",
    "Spatial Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Remote Sensing": 0.78,
    "Visual Prompting": 0.8,
    "Cross-domain Fusion": 0.72,
    "Spatial Reasoning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-modal Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "MLLM",
          "Multimodal LLM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to existing knowledge on large language models and their application across modalities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Remote Sensing Imagery",
        "canonical": "Remote Sensing",
        "aliases": [
          "RS Imagery",
          "Remote Sensing Data"
        ],
        "category": "specific_connectable",
        "rationale": "Facilitates connections to studies and technologies focused on remote sensing data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Visual Prompting",
        "canonical": "Visual Prompting",
        "aliases": [
          "Visual Prompts",
          "Prompting"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach in the context of remote sensing and language models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cross-domain Fusion Training",
        "canonical": "Cross-domain Fusion",
        "aliases": [
          "Fusion Training",
          "Cross-domain Training"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific training strategy relevant to multi-modal and multi-task learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Hierarchical Spatial Reasoning",
        "canonical": "Spatial Reasoning",
        "aliases": [
          "Hierarchical Reasoning",
          "Spatial Analysis"
        ],
        "category": "specific_connectable",
        "rationale": "Links to concepts in spatial analysis and reasoning, crucial for interpreting remote sensing data.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.77,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-modal Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Remote Sensing Imagery",
      "resolved_canonical": "Remote Sensing",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Visual Prompting",
      "resolved_canonical": "Visual Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cross-domain Fusion Training",
      "resolved_canonical": "Cross-domain Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Hierarchical Spatial Reasoning",
      "resolved_canonical": "Spatial Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.77,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# EarthGPT-X: A Spatial MLLM for Multi-level Multi-Source Remote Sensing Imagery Understanding with Visual Prompting

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2504.12795.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2504.12795](https://arxiv.org/abs/2504.12795)

## 🔗 유사한 논문
- [[2025-09-22/See&Trek_ Training-Free Spatial Prompting for Multimodal Large Language Model_20250922|See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model]] (84.9% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (83.7% similar)
- [[2025-09-23/GeoPQA_ Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning_20250923|GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning]] (83.7% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (83.2% similar)
- [[2025-09-23/ProReason_ Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom_20250923|ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom]] (83.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Remote Sensing|Remote Sensing]], [[keywords/Spatial Reasoning|Spatial Reasoning]]
**⚡ Unique Technical**: [[keywords/Visual Prompting|Visual Prompting]], [[keywords/Cross-domain Fusion|Cross-domain Fusion]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2504.12795v2 Announce Type: replace 
Abstract: Recent advances in natural-domain multi-modal large language models (MLLMs) have demonstrated effective spatial reasoning through visual and textual prompting. However, their direct transfer to remote sensing (RS) is hindered by heterogeneous sensing physics, diverse modalities, and unique spatial scales. Existing RS MLLMs are mainly limited to optical imagery and plain language interaction, preventing flexible and scalable real-world applications. In this article, EarthGPT-X is proposed, the first flexible spatial MLLM that unifies multi-source RS imagery comprehension and accomplishes both coarse-grained and fine-grained visual tasks under diverse visual prompts in a single framework. Distinct from prior models, EarthGPT-X introduces: 1) a dual-prompt mechanism combining text instructions with various visual prompts (i.e., point, box, and free-form) to mimic the versatility of referring in human life; 2) a comprehensive multi-source multi-level prompting dataset, the model advances beyond holistic image understanding to support hierarchical spatial reasoning, including scene-level understanding and fine-grained object attributes and relational analysis; 3) a cross-domain one-stage fusion training strategy, enabling efficient and consistent alignment across modalities and tasks. Extensive experiments demonstrate that EarthGPT-X substantially outperforms prior nature and RS MLLMs, establishing the first framework capable of multi-source, multi-task, and multi-level interpretation using visual prompting in RS scenarios.

## 📝 요약

최근 자연 분야의 다중 모달 대형 언어 모델(MLLMs)은 시각적 및 텍스트 프롬프트를 통해 효과적인 공간 추론을 보여주었습니다. 그러나 원격 탐사(RS) 분야로의 직접적인 전환은 다양한 감지 물리학, 모달리티, 공간 규모로 인해 어려움을 겪고 있습니다. 기존 RS MLLMs는 주로 광학 이미지와 단순한 언어 상호작용에 제한되어 있어 유연하고 확장 가능한 실제 응용에 한계가 있습니다. 이를 해결하기 위해 제안된 EarthGPT-X는 다중 소스 RS 이미지 이해를 통합하고 다양한 시각적 프롬프트 하에서 대규모 및 세부 시각 작업을 수행할 수 있는 최초의 유연한 공간 MLLM입니다. EarthGPT-X는 텍스트 지시와 다양한 시각적 프롬프트를 결합한 이중 프롬프트 메커니즘, 포괄적인 다중 소스 다중 레벨 프롬프트 데이터셋, 크로스 도메인 원스테이지 융합 훈련 전략을 도입하여 다양한 모달리티와 작업 간의 효율적이고 일관된 정렬을 가능하게 합니다. 실험 결과, EarthGPT-X는 기존의 자연 및 RS MLLMs를 능가하며, RS 시나리오에서 시각적 프롬프트를 사용한 다중 소스, 다중 작업, 다중 레벨 해석이 가능한 최초의 프레임워크를 확립했습니다.

## 🎯 주요 포인트

- 1. EarthGPT-X는 다양한 시각적 프롬프트를 활용하여 다중 소스 원격 감지 이미지를 이해하고, 다양한 시각적 작업을 수행할 수 있는 유연한 공간 MLLM입니다.
- 2. 이 모델은 텍스트 지시와 다양한 시각적 프롬프트(점, 박스, 자유형)를 결합한 이중 프롬프트 메커니즘을 도입하여 인간의 참조 다변성을 모방합니다.
- 3. 포괄적인 다중 소스 다중 레벨 프롬프트 데이터셋을 통해 장면 수준의 이해와 세부 객체 속성 및 관계 분석을 지원하는 계층적 공간 추론을 가능하게 합니다.
- 4. 크로스 도메인 원스테이지 융합 훈련 전략을 통해 모달리티와 작업 간의 효율적이고 일관된 정렬을 실현합니다.
- 5. 실험 결과, EarthGPT-X는 기존의 자연 및 원격 감지 MLLM을 능가하며, 원격 감지 시나리오에서 시각적 프롬프트를 사용하는 다중 소스, 다중 작업, 다중 레벨 해석이 가능한 첫 번째 프레임워크를 확립했습니다.


---

*Generated on 2025-09-24 05:20:53*