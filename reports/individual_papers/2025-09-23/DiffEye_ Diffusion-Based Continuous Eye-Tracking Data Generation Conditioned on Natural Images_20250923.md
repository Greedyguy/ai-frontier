---
keywords:
  - Diffusion Model
  - Eye-Tracking
  - Saliency Map
  - Natural Images
  - Corresponding Positional Embedding
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16767
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:33:52.057050",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Model",
    "Eye-Tracking",
    "Saliency Map",
    "Natural Images",
    "Corresponding Positional Embedding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Model": 0.78,
    "Eye-Tracking": 0.82,
    "Saliency Map": 0.8,
    "Natural Images": 0.7,
    "Corresponding Positional Embedding": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Model",
        "canonical": "Diffusion Model",
        "aliases": [
          "Diffusion-Based Model"
        ],
        "category": "unique_technical",
        "rationale": "Diffusion models are a novel approach in generating continuous trajectories, distinguishing this work from traditional methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Eye-Tracking",
        "canonical": "Eye-Tracking",
        "aliases": [
          "Gaze Tracking"
        ],
        "category": "specific_connectable",
        "rationale": "Eye-tracking is a central theme of the paper, linking it to broader research in visual attention.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Saliency Map",
        "canonical": "Saliency Map",
        "aliases": [
          "Visual Saliency"
        ],
        "category": "specific_connectable",
        "rationale": "Saliency maps are crucial for understanding visual attention, connecting this work to existing models in computer vision.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Natural Images",
        "canonical": "Natural Images",
        "aliases": [
          "Real-World Images"
        ],
        "category": "broad_technical",
        "rationale": "The use of natural images is a key aspect of the study, linking it to real-world applications in computer vision.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "Corresponding Positional Embedding",
        "canonical": "Corresponding Positional Embedding",
        "aliases": [
          "CPE"
        ],
        "category": "unique_technical",
        "rationale": "This novel component aligns spatial gaze information with semantic features, offering a unique contribution to the field.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Model",
      "resolved_canonical": "Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Eye-Tracking",
      "resolved_canonical": "Eye-Tracking",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Saliency Map",
      "resolved_canonical": "Saliency Map",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Natural Images",
      "resolved_canonical": "Natural Images",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Corresponding Positional Embedding",
      "resolved_canonical": "Corresponding Positional Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16767.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16767](https://arxiv.org/abs/2509.16767)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/FlightDiffusion_ Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video_20250918|FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video]] (82.3% similar)
- [[2025-09-22/LowDiff_ Efficient Diffusion Sampling with Low-Resolution Condition_20250922|LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition]] (81.5% similar)
- [[2025-09-19/Roll Your Eyes_ Gaze Redirection via Explicit 3D Eyeball Rotation_20250919|Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation]] (81.5% similar)
- [[2025-09-18/Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model_20250918|Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model]] (80.7% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Natural Images|Natural Images]]
**ğŸ”— Specific Connectable**: [[keywords/Eye-Tracking|Eye-Tracking]], [[keywords/Saliency Map|Saliency Map]]
**âš¡ Unique Technical**: [[keywords/Diffusion Model|Diffusion Model]], [[keywords/Corresponding Positional Embedding|Corresponding Positional Embedding]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16767v1 Announce Type: new 
Abstract: Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: https://diff-eye.github.io/

## ğŸ“ ìš”ì•½

DiffEyeëŠ” ìì—° ì´ë¯¸ì§€ ììœ  ì‹œì²­ ì‹œ ì—°ì†ì ì´ê³  ë‹¤ì–‘í•œ ì‹œì„  ì´ë™ ê²½ë¡œë¥¼ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ ê°œë°œëœ í™•ì‚° ê¸°ë°˜ í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ë“¤ì´ ê³ ì •ëœ ê¸¸ì´ì˜ ë‹¨ì¼ ì‹œì„  ê²½ë¡œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°˜ë©´, DiffEyeëŠ” ì‹œê° ìê·¹ì— ì¡°ê±´í™”ëœ í™•ì‚° ëª¨ë¸ê³¼ ìƒˆë¡œìš´ ëŒ€ì‘ ìœ„ì¹˜ ì„ë² ë”©(CPE)ì„ ë„ì…í•˜ì—¬ ì‹œì„  ì •ë³´ë¥¼ ì‹œê° ì…ë ¥ì˜ íŒ¨ì¹˜ ê¸°ë°˜ ì˜ë¯¸ì  íŠ¹ì§•ê³¼ ì •ë ¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì›ì‹œ ì‹œì„  ì¶”ì  ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¸ê°„ì˜ ì‹œì„  í–‰ë™ì˜ ë³€ë™ì„±ì„ í¬ì°©í•˜ê³ , í˜„ì‹¤ì ì¸ ì‹œì„  ì´ë™ íŒ¨í„´ì„ ìƒì„±í•©ë‹ˆë‹¤. DiffEyeëŠ” ì†Œê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œë„ ê³ í’ˆì§ˆì˜ ì‹œì„  ê²½ë¡œì™€ ì£¼ì˜ ë§µì„ ìƒì„±í•˜ë©°, ì´ëŠ” ì¸ê°„ ì‹œê° ì£¼ì˜ì˜ ë¶„í¬ë¥¼ ë” ì •í™•íˆ ë°˜ì˜í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ìì—° ì´ë¯¸ì§€ì— ëŒ€í•´ í™•ì‚° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—°ì†ì ì¸ ì‹œì„  ì´ë™ ê²½ë¡œë¥¼ ìƒì„±í•œ ìµœì´ˆì˜ ì‚¬ë¡€ë¡œ, ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DiffEyeëŠ” ìì—° ì´ë¯¸ì§€ ììœ  ì‹œì²­ ì‹œ ì—°ì†ì ì´ê³  ë‹¤ì–‘í•œ ì•ˆêµ¬ ìš´ë™ ê¶¤ì ì„ ëª¨ë¸ë§í•˜ëŠ” í™•ì‚° ê¸°ë°˜ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì´ ë°©ë²•ì€ ì‹œê°ì  ìê·¹ì— ì¡°ê±´í™”ëœ í™•ì‚° ëª¨ë¸ê³¼ ì‹œê° ì…ë ¥ì˜ íŒ¨ì¹˜ ê¸°ë°˜ ì˜ë¯¸ì  íŠ¹ì§•ê³¼ ê³µê°„ì  ì‹œì„  ì •ë³´ë¥¼ ì •ë ¬í•˜ëŠ” ìƒˆë¡œìš´ êµ¬ì„± ìš”ì†Œì¸ Corresponding Positional Embedding (CPE)ì„ ë„ì…í•©ë‹ˆë‹¤.
- 3. DiffEyeëŠ” ìŠ¤ìº”íŒ¨ìŠ¤ ëŒ€ì‹  ì›ì‹œ ì•ˆêµ¬ ì¶”ì  ê¶¤ì ì„ í™œìš©í•˜ì—¬ ì¸ê°„ì˜ ì‹œì„  í–‰ë™ì˜ ê³ ìœ í•œ ë³€ë™ì„±ì„ í¬ì°©í•˜ê³ , ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ê³ í’ˆì§ˆì˜ í˜„ì‹¤ì ì¸ ì•ˆêµ¬ ìš´ë™ íŒ¨í„´ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 4. ìƒì„±ëœ ê¶¤ì ì€ ìŠ¤ìº”íŒ¨ìŠ¤ì™€ ì£¼ì˜ ì§‘ì¤‘ ì§€ë„(saliency map)ë¡œ ë³€í™˜ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì¸ê°„ ì‹œê° ì£¼ì˜ì˜ ë¶„í¬ë¥¼ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.
- 5. DiffEyeëŠ” í™•ì‚° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìì—° ì´ë¯¸ì§€ì—ì„œ ì´ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ìµœì´ˆì˜ ë°©ë²•ì´ë©°, ì›ì‹œ ì•ˆêµ¬ ì¶”ì  ë°ì´í„°ì˜ í’ë¶€í•¨ì„ ì™„ì „íˆ í™œìš©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:33:52*