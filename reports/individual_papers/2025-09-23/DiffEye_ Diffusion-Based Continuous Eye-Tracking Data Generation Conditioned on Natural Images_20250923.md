---
keywords:
  - Diffusion Model
  - Eye-Tracking
  - Saliency Map
  - Natural Images
  - Corresponding Positional Embedding
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16767
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:33:52.057050",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Model",
    "Eye-Tracking",
    "Saliency Map",
    "Natural Images",
    "Corresponding Positional Embedding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Model": 0.78,
    "Eye-Tracking": 0.82,
    "Saliency Map": 0.8,
    "Natural Images": 0.7,
    "Corresponding Positional Embedding": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Model",
        "canonical": "Diffusion Model",
        "aliases": [
          "Diffusion-Based Model"
        ],
        "category": "unique_technical",
        "rationale": "Diffusion models are a novel approach in generating continuous trajectories, distinguishing this work from traditional methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Eye-Tracking",
        "canonical": "Eye-Tracking",
        "aliases": [
          "Gaze Tracking"
        ],
        "category": "specific_connectable",
        "rationale": "Eye-tracking is a central theme of the paper, linking it to broader research in visual attention.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Saliency Map",
        "canonical": "Saliency Map",
        "aliases": [
          "Visual Saliency"
        ],
        "category": "specific_connectable",
        "rationale": "Saliency maps are crucial for understanding visual attention, connecting this work to existing models in computer vision.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Natural Images",
        "canonical": "Natural Images",
        "aliases": [
          "Real-World Images"
        ],
        "category": "broad_technical",
        "rationale": "The use of natural images is a key aspect of the study, linking it to real-world applications in computer vision.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "Corresponding Positional Embedding",
        "canonical": "Corresponding Positional Embedding",
        "aliases": [
          "CPE"
        ],
        "category": "unique_technical",
        "rationale": "This novel component aligns spatial gaze information with semantic features, offering a unique contribution to the field.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Model",
      "resolved_canonical": "Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Eye-Tracking",
      "resolved_canonical": "Eye-Tracking",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Saliency Map",
      "resolved_canonical": "Saliency Map",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Natural Images",
      "resolved_canonical": "Natural Images",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Corresponding Positional Embedding",
      "resolved_canonical": "Corresponding Positional Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16767.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16767](https://arxiv.org/abs/2509.16767)

## 🔗 유사한 논문
- [[2025-09-18/FlightDiffusion_ Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video_20250918|FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video]] (82.3% similar)
- [[2025-09-22/LowDiff_ Efficient Diffusion Sampling with Low-Resolution Condition_20250922|LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition]] (81.5% similar)
- [[2025-09-19/Roll Your Eyes_ Gaze Redirection via Explicit 3D Eyeball Rotation_20250919|Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation]] (81.5% similar)
- [[2025-09-18/Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model_20250918|Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model]] (80.7% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (80.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Natural Images|Natural Images]]
**🔗 Specific Connectable**: [[keywords/Eye-Tracking|Eye-Tracking]], [[keywords/Saliency Map|Saliency Map]]
**⚡ Unique Technical**: [[keywords/Diffusion Model|Diffusion Model]], [[keywords/Corresponding Positional Embedding|Corresponding Positional Embedding]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16767v1 Announce Type: new 
Abstract: Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: https://diff-eye.github.io/

## 📝 요약

DiffEye는 자연 이미지 자유 시청 시 연속적이고 다양한 시선 이동 경로를 모델링하기 위해 개발된 확산 기반 학습 프레임워크입니다. 기존 모델들이 고정된 길이의 단일 시선 경로를 예측하는 반면, DiffEye는 시각 자극에 조건화된 확산 모델과 새로운 대응 위치 임베딩(CPE)을 도입하여 시선 정보를 시각 입력의 패치 기반 의미적 특징과 정렬합니다. 이를 통해 원시 시선 추적 데이터를 활용하여 인간의 시선 행동의 변동성을 포착하고, 현실적인 시선 이동 패턴을 생성합니다. DiffEye는 소규모 데이터셋으로도 고품질의 시선 경로와 주의 맵을 생성하며, 이는 인간 시각 주의의 분포를 더 정확히 반영합니다. 이 연구는 자연 이미지에 대해 확산 모델을 사용하여 연속적인 시선 이동 경로를 생성한 최초의 사례로, 최첨단 성능을 입증했습니다.

## 🎯 주요 포인트

- 1. DiffEye는 자연 이미지 자유 시청 시 연속적이고 다양한 안구 운동 궤적을 모델링하는 확산 기반 훈련 프레임워크입니다.
- 2. 이 방법은 시각적 자극에 조건화된 확산 모델과 시각 입력의 패치 기반 의미적 특징과 공간적 시선 정보를 정렬하는 새로운 구성 요소인 Corresponding Positional Embedding (CPE)을 도입합니다.
- 3. DiffEye는 스캔패스 대신 원시 안구 추적 궤적을 활용하여 인간의 시선 행동의 고유한 변동성을 포착하고, 작은 데이터셋으로 훈련되었음에도 불구하고 고품질의 현실적인 안구 운동 패턴을 생성합니다.
- 4. 생성된 궤적은 스캔패스와 주의 집중 지도(saliency map)로 변환될 수 있으며, 이는 인간 시각 주의의 분포를 더 정확하게 반영합니다.
- 5. DiffEye는 확산 모델을 사용하여 자연 이미지에서 이 작업을 처리하는 최초의 방법이며, 원시 안구 추적 데이터의 풍부함을 완전히 활용합니다.


---

*Generated on 2025-09-24 04:33:52*