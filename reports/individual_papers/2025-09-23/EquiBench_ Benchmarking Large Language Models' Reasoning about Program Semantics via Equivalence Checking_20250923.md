---
keywords:
  - Large Language Model
  - Equivalence Checking
  - Program Semantics
  - Program Analysis
  - Superoptimization
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2502.12466
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:47:11.950770",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Equivalence Checking",
    "Program Semantics",
    "Program Analysis",
    "Superoptimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Equivalence Checking": 0.8,
    "Program Semantics": 0.78,
    "Program Analysis": 0.77,
    "Superoptimization": 0.76
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating reasoning capabilities in code-related tasks.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Equivalence Checking",
        "canonical": "Equivalence Checking",
        "aliases": [
          "Program Equivalence",
          "Semantic Equivalence"
        ],
        "category": "unique_technical",
        "rationale": "Core concept for assessing program semantics understanding in LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Program Semantics",
        "canonical": "Program Semantics",
        "aliases": [
          "Code Semantics"
        ],
        "category": "unique_technical",
        "rationale": "Essential for understanding the depth of LLMs' reasoning capabilities.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Program Analysis",
        "canonical": "Program Analysis",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Important for generating program pairs and ensuring high-confidence labels.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "Superoptimization",
        "canonical": "Superoptimization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Used in generating challenging program pairs, highlighting advanced optimization techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.75,
        "link_intent_score": 0.76
      }
    ],
    "ban_list_suggestions": [
      "task",
      "model",
      "benchmark",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Equivalence Checking",
      "resolved_canonical": "Equivalence Checking",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Program Semantics",
      "resolved_canonical": "Program Semantics",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Program Analysis",
      "resolved_canonical": "Program Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Superoptimization",
      "resolved_canonical": "Superoptimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.75,
        "link_intent": 0.76
      }
    }
  ]
}
-->

# EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.12466.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2502.12466](https://arxiv.org/abs/2502.12466)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/seqBench_ A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs_20250923|seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs]] (88.8% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (88.5% similar)
- [[2025-09-23/SATBench_ Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas_20250923|SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas]] (87.6% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (87.1% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Program Analysis|Program Analysis]], [[keywords/Superoptimization|Superoptimization]]
**âš¡ Unique Technical**: [[keywords/Equivalence Checking|Equivalence Checking]], [[keywords/Program Semantics|Program Semantics]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.12466v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become integral to code-related tasks, a central question emerges: Do LLMs truly understand program semantics? We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs. Unlike prior code generation benchmarks, this task directly tests a model's ability to reason about program semantics. EquiBench consists of 2400 program pairs across four languages and six categories. These pairs are generated through program analysis, compiler scheduling, and superoptimization, ensuring high-confidence labels, nontrivial difficulty, and full automation. We evaluate 19 state-of-the-art LLMs and find that in the most challenging categories, the best accuracies are 63.8% and 76.2%, only modestly above the 50% random baseline. Further analysis reveals that models often rely on syntactic similarity rather than exhibiting robust reasoning about program semantics, highlighting current limitations. Our code and dataset are publicly available at https://github.com/Anjiang-Wei/equibench

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ì½”ë“œ ê´€ë ¨ ì‘ì—…ì— í•„ìˆ˜ì ì´ ë˜ë©´ì„œ, ì´ë“¤ì´ í”„ë¡œê·¸ë¨ ì˜ë¯¸ë¥¼ ì§„ì •ìœ¼ë¡œ ì´í•´í•˜ëŠ”ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì œê¸°ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” EquiBenchë¼ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•˜ì—¬ LLMsë¥¼ ë™ë“±ì„± ê²€ì‚¬ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ì´ëŠ” ë‘ í”„ë¡œê·¸ë¨ì´ ëª¨ë“  ê°€ëŠ¥í•œ ì…ë ¥ì— ëŒ€í•´ ë™ì¼í•œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. EquiBenchëŠ” ë„¤ ê°œì˜ ì–¸ì–´ì™€ ì—¬ì„¯ ê°œì˜ ë²”ì£¼ì— ê±¸ì³ 2400ê°œì˜ í”„ë¡œê·¸ë¨ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í”„ë¡œê·¸ë¨ ë¶„ì„, ì»´íŒŒì¼ëŸ¬ ìŠ¤ì¼€ì¤„ë§, ì´ˆìµœì í™”ë¥¼ í†µí•´ ìƒì„±ë©ë‹ˆë‹¤. 19ê°œì˜ ìµœì‹  LLMsë¥¼ í‰ê°€í•œ ê²°ê³¼, ê°€ì¥ ì–´ë ¤ìš´ ë²”ì£¼ì—ì„œ ìµœê³  ì •í™•ë„ëŠ” 63.8%ì™€ 76.2%ë¡œ, 50%ì˜ ë¬´ì‘ìœ„ ê¸°ì¤€ì„ ì•½ê°„ ìƒíšŒí•˜ëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, ëª¨ë¸ë“¤ì´ í”„ë¡œê·¸ë¨ ì˜ë¯¸ì— ëŒ€í•œ ê°•ë ¥í•œ ì¶”ë¡ ë³´ë‹¤ëŠ” êµ¬ë¬¸ ìœ ì‚¬ì„±ì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë³´ì—¬ì£¼ì–´ í˜„ì¬ì˜ í•œê³„ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. ì½”ë“œì™€ ë°ì´í„°ì…‹ì€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. EquiBenchëŠ” í”„ë¡œê·¸ë¨ì˜ ë™ë“±ì„± ê²€ì‚¬ë¥¼ í†µí•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í”„ë¡œê·¸ë¨ ì˜ë¯¸ ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- 2. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ë„¤ ê°€ì§€ ì–¸ì–´ì™€ ì—¬ì„¯ ê°€ì§€ ì¹´í…Œê³ ë¦¬ì—ì„œ 2400ê°œì˜ í”„ë¡œê·¸ë¨ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í”„ë¡œê·¸ë¨ ë¶„ì„, ì»´íŒŒì¼ëŸ¬ ìŠ¤ì¼€ì¤„ë§, ìŠˆí¼ìµœì í™”ë¥¼ í†µí•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. 19ê°œì˜ ìµœì²¨ë‹¨ LLMì„ í‰ê°€í•œ ê²°ê³¼, ê°€ì¥ ì–´ë ¤ìš´ ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœê³  ì •í™•ë„ëŠ” 63.8%ì™€ 76.2%ë¡œ, 50%ì˜ ëœë¤ ê¸°ì¤€ì„ ë³´ë‹¤ ì•½ê°„ ë†’ì€ ìˆ˜ì¤€ì…ë‹ˆë‹¤.
- 4. ë¶„ì„ ê²°ê³¼, ëª¨ë¸ë“¤ì´ í”„ë¡œê·¸ë¨ ì˜ë¯¸ì— ëŒ€í•œ ê²¬ê³ í•œ ì¶”ë¡ ë³´ë‹¤ëŠ” êµ¬ë¬¸ì  ìœ ì‚¬ì„±ì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- 5. EquiBenchì˜ ì½”ë“œì™€ ë°ì´í„°ì…‹ì€ https://github.com/Anjiang-Wei/equibenchì—ì„œ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:47:11*