---
keywords:
  - Table Question Answering
  - Large Language Model
  - Cross-lingual Scenarios
  - Semantic Accuracy
  - SEAT Evaluation Framework
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2506.03949
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:03:04.438103",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Table Question Answering",
    "Large Language Model",
    "Cross-lingual Scenarios",
    "Semantic Accuracy",
    "SEAT Evaluation Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Table Question Answering": 0.78,
    "Large Language Model": 0.85,
    "Cross-lingual Scenarios": 0.74,
    "Semantic Accuracy": 0.79,
    "SEAT Evaluation Framework": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Table Question Answering",
        "canonical": "Table Question Answering",
        "aliases": [
          "TableQA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task within NLP that involves interpreting and answering questions based on tabular data, crucial for linking related research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus and connect to a wide range of research in NLP and AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.89,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-lingual Scenarios",
        "canonical": "Cross-lingual Scenarios",
        "aliases": [
          "Multilingual Contexts"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is important for linking studies on multilingual data processing and language models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.77,
        "link_intent_score": 0.74
      },
      {
        "surface": "Semantic Accuracy",
        "canonical": "Semantic Accuracy",
        "aliases": [
          "Semantic Precision"
        ],
        "category": "unique_technical",
        "rationale": "Semantic accuracy is a critical metric for evaluating the alignment of model outputs with human understanding, relevant to NLP research.",
        "novelty_score": 0.71,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "SEAT Evaluation Framework",
        "canonical": "SEAT Evaluation Framework",
        "aliases": [
          "SEAT"
        ],
        "category": "unique_technical",
        "rationale": "This framework offers a novel approach to evaluating TableQA, providing a unique link to evaluation methodologies.",
        "novelty_score": 0.82,
        "connectivity_score": 0.58,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "data leakage",
      "real-world documents"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Table Question Answering",
      "resolved_canonical": "Table Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.89,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-lingual Scenarios",
      "resolved_canonical": "Cross-lingual Scenarios",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.77,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Semantic Accuracy",
      "resolved_canonical": "Semantic Accuracy",
      "decision": "linked",
      "scores": {
        "novelty": 0.71,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "SEAT Evaluation Framework",
      "resolved_canonical": "SEAT Evaluation Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.58,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2506.03949.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2506.03949](https://arxiv.org/abs/2506.03949)

## 🔗 유사한 논문
- [[2025-09-23/When TableQA Meets Noise_ A Dual Denoising Framework for Complex Questions and Large-scale Tables_20250923|When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables]] (84.9% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (83.9% similar)
- [[2025-09-19/TableDART_ Dynamic Adaptive Multi-Modal Routing for Table Understanding_20250919|TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding]] (82.9% similar)
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (82.8% similar)
- [[2025-09-23/Quality Assessment of Tabular Data using Large Language Models and Code Generation_20250923|Quality Assessment of Tabular Data using Large Language Models and Code Generation]] (82.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Cross-lingual Scenarios|Cross-lingual Scenarios]]
**⚡ Unique Technical**: [[keywords/Table Question Answering|Table Question Answering]], [[keywords/Semantic Accuracy|Semantic Accuracy]], [[keywords/SEAT Evaluation Framework|SEAT Evaluation Framework]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.03949v3 Announce Type: replace 
Abstract: LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: https://github.com/wenge-research/TableEval.

## 📝 요약

LLMs는 자연어 처리에서 큰 발전을 이루었지만, 다양한 테이블 구조, 다국어 데이터, 도메인별 추론이 중요한 TableQA에서는 여전히 어려움을 겪고 있습니다. 기존의 TableQA 벤치마크는 단순한 테이블에 집중하고 데이터 누출 문제가 있으며, 대부분 단일 언어로 되어 있어 실제 응용에서의 다양성을 포착하지 못합니다. 이를 해결하기 위해, 우리는 현실적인 TableQA 작업을 평가하기 위한 새로운 벤치마크인 TableEval을 소개합니다. TableEval은 다양한 구조의 테이블과 다국어 시나리오를 포함하며, 데이터 누출 위험을 줄이기 위해 최신 문서에서 데이터를 수집했습니다. 또한, 기존의 평가 지표가 의미적 정확성을 포착하지 못하는 문제를 해결하기 위해 SEAT라는 새로운 평가 프레임워크를 제안했습니다. 실험 결과, SEAT는 인간 판단과 높은 일치를 보였으며, TableEval을 통한 실험은 최신 LLMs가 복잡한 현실 세계의 TableQA 작업을 처리하는 데 있어 중요한 격차가 있음을 드러냈습니다.

## 🎯 주요 포인트

- 1. LLMs는 자연어 처리에서 큰 발전을 보였지만, 다양한 테이블 구조, 다국어 데이터, 도메인 특화 추론이 중요한 TableQA에서는 여전히 어려움을 겪고 있습니다.
- 2. 기존의 TableQA 벤치마크는 단순한 평면 테이블에 집중하고 데이터 누출 문제가 있으며, 대부분 단일 언어로 되어 있어 실질적인 응용에서의 다국어 및 도메인 간 변동성을 포착하지 못합니다.
- 3. 이러한 한계를 극복하기 위해, 다양한 구조의 테이블과 다국어 시나리오를 포함한 새로운 벤치마크 TableEval을 소개합니다.
- 4. TableEval은 정부, 금융, 학계, 산업 보고서 등 네 가지 도메인에서 수집된 테이블을 포함하며, 간체 중국어, 번체 중국어, 영어로 된 테이블을 제공합니다.
- 5. SEAT라는 새로운 평가 프레임워크를 제안하여 모델 응답과 참조 답변 간의 정렬을 하위 질문 수준에서 평가하며, 이는 인간 판단과 높은 일치를 보입니다.


---

*Generated on 2025-09-24 04:03:04*