---
keywords:
  - Table Question Answering
  - Large Language Model
  - Cross-lingual Scenarios
  - Semantic Accuracy
  - SEAT Evaluation Framework
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2506.03949
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:03:04.438103",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Table Question Answering",
    "Large Language Model",
    "Cross-lingual Scenarios",
    "Semantic Accuracy",
    "SEAT Evaluation Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Table Question Answering": 0.78,
    "Large Language Model": 0.85,
    "Cross-lingual Scenarios": 0.74,
    "Semantic Accuracy": 0.79,
    "SEAT Evaluation Framework": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Table Question Answering",
        "canonical": "Table Question Answering",
        "aliases": [
          "TableQA"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task within NLP that involves interpreting and answering questions based on tabular data, crucial for linking related research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus and connect to a wide range of research in NLP and AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.89,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-lingual Scenarios",
        "canonical": "Cross-lingual Scenarios",
        "aliases": [
          "Multilingual Contexts"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is important for linking studies on multilingual data processing and language models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.77,
        "link_intent_score": 0.74
      },
      {
        "surface": "Semantic Accuracy",
        "canonical": "Semantic Accuracy",
        "aliases": [
          "Semantic Precision"
        ],
        "category": "unique_technical",
        "rationale": "Semantic accuracy is a critical metric for evaluating the alignment of model outputs with human understanding, relevant to NLP research.",
        "novelty_score": 0.71,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "SEAT Evaluation Framework",
        "canonical": "SEAT Evaluation Framework",
        "aliases": [
          "SEAT"
        ],
        "category": "unique_technical",
        "rationale": "This framework offers a novel approach to evaluating TableQA, providing a unique link to evaluation methodologies.",
        "novelty_score": 0.82,
        "connectivity_score": 0.58,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "data leakage",
      "real-world documents"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Table Question Answering",
      "resolved_canonical": "Table Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.89,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-lingual Scenarios",
      "resolved_canonical": "Cross-lingual Scenarios",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.77,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Semantic Accuracy",
      "resolved_canonical": "Semantic Accuracy",
      "decision": "linked",
      "scores": {
        "novelty": 0.71,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "SEAT Evaluation Framework",
      "resolved_canonical": "SEAT Evaluation Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.58,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2506.03949.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2506.03949](https://arxiv.org/abs/2506.03949)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/When TableQA Meets Noise_ A Dual Denoising Framework for Complex Questions and Large-scale Tables_20250923|When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables]] (84.9% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (83.9% similar)
- [[2025-09-19/TableDART_ Dynamic Adaptive Multi-Modal Routing for Table Understanding_20250919|TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding]] (82.9% similar)
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (82.8% similar)
- [[2025-09-23/Quality Assessment of Tabular Data using Large Language Models and Code Generation_20250923|Quality Assessment of Tabular Data using Large Language Models and Code Generation]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Cross-lingual Scenarios|Cross-lingual Scenarios]]
**âš¡ Unique Technical**: [[keywords/Table Question Answering|Table Question Answering]], [[keywords/Semantic Accuracy|Semantic Accuracy]], [[keywords/SEAT Evaluation Framework|SEAT Evaluation Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.03949v3 Announce Type: replace 
Abstract: LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: https://github.com/wenge-research/TableEval.

## ğŸ“ ìš”ì•½

LLMsëŠ” ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í° ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ë‹¤ì–‘í•œ í…Œì´ë¸” êµ¬ì¡°, ë‹¤êµ­ì–´ ë°ì´í„°, ë„ë©”ì¸ë³„ ì¶”ë¡ ì´ ì¤‘ìš”í•œ TableQAì—ì„œëŠ” ì—¬ì „íˆ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ TableQA ë²¤ì¹˜ë§ˆí¬ëŠ” ë‹¨ìˆœí•œ í…Œì´ë¸”ì— ì§‘ì¤‘í•˜ê³  ë°ì´í„° ëˆ„ì¶œ ë¬¸ì œê°€ ìˆìœ¼ë©°, ëŒ€ë¶€ë¶„ ë‹¨ì¼ ì–¸ì–´ë¡œ ë˜ì–´ ìˆì–´ ì‹¤ì œ ì‘ìš©ì—ì„œì˜ ë‹¤ì–‘ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í˜„ì‹¤ì ì¸ TableQA ì‘ì—…ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ TableEvalì„ ì†Œê°œí•©ë‹ˆë‹¤. TableEvalì€ ë‹¤ì–‘í•œ êµ¬ì¡°ì˜ í…Œì´ë¸”ê³¼ ë‹¤êµ­ì–´ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬í•¨í•˜ë©°, ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´ ìµœì‹  ë¬¸ì„œì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ê¸°ì¡´ì˜ í‰ê°€ ì§€í‘œê°€ ì˜ë¯¸ì  ì •í™•ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ SEATë¼ëŠ” ìƒˆë¡œìš´ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SEATëŠ” ì¸ê°„ íŒë‹¨ê³¼ ë†’ì€ ì¼ì¹˜ë¥¼ ë³´ì˜€ìœ¼ë©°, TableEvalì„ í†µí•œ ì‹¤í—˜ì€ ìµœì‹  LLMsê°€ ë³µì¡í•œ í˜„ì‹¤ ì„¸ê³„ì˜ TableQA ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ ê²©ì°¨ê°€ ìˆìŒì„ ë“œëŸ¬ëƒˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLMsëŠ” ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í° ë°œì „ì„ ë³´ì˜€ì§€ë§Œ, ë‹¤ì–‘í•œ í…Œì´ë¸” êµ¬ì¡°, ë‹¤êµ­ì–´ ë°ì´í„°, ë„ë©”ì¸ íŠ¹í™” ì¶”ë¡ ì´ ì¤‘ìš”í•œ TableQAì—ì„œëŠ” ì—¬ì „íˆ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ TableQA ë²¤ì¹˜ë§ˆí¬ëŠ” ë‹¨ìˆœí•œ í‰ë©´ í…Œì´ë¸”ì— ì§‘ì¤‘í•˜ê³  ë°ì´í„° ëˆ„ì¶œ ë¬¸ì œê°€ ìˆìœ¼ë©°, ëŒ€ë¶€ë¶„ ë‹¨ì¼ ì–¸ì–´ë¡œ ë˜ì–´ ìˆì–´ ì‹¤ì§ˆì ì¸ ì‘ìš©ì—ì„œì˜ ë‹¤êµ­ì–´ ë° ë„ë©”ì¸ ê°„ ë³€ë™ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
- 3. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ êµ¬ì¡°ì˜ í…Œì´ë¸”ê³¼ ë‹¤êµ­ì–´ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬í•¨í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ TableEvalì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 4. TableEvalì€ ì •ë¶€, ê¸ˆìœµ, í•™ê³„, ì‚°ì—… ë³´ê³ ì„œ ë“± ë„¤ ê°€ì§€ ë„ë©”ì¸ì—ì„œ ìˆ˜ì§‘ëœ í…Œì´ë¸”ì„ í¬í•¨í•˜ë©°, ê°„ì²´ ì¤‘êµ­ì–´, ë²ˆì²´ ì¤‘êµ­ì–´, ì˜ì–´ë¡œ ëœ í…Œì´ë¸”ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. SEATë¼ëŠ” ìƒˆë¡œìš´ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ ëª¨ë¸ ì‘ë‹µê³¼ ì°¸ì¡° ë‹µë³€ ê°„ì˜ ì •ë ¬ì„ í•˜ìœ„ ì§ˆë¬¸ ìˆ˜ì¤€ì—ì„œ í‰ê°€í•˜ë©°, ì´ëŠ” ì¸ê°„ íŒë‹¨ê³¼ ë†’ì€ ì¼ì¹˜ë¥¼ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:03:04*