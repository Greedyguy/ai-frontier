---
keywords:
  - Large Language Model
  - Medical Question-Answering
  - Demographic Bias
  - Global South
  - Biomedical Large Language Models
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2411.15640
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:44:48.680871",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Medical Question-Answering",
    "Demographic Bias",
    "Global South",
    "Biomedical Large Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Medical Question-Answering": 0.78,
    "Demographic Bias": 0.8,
    "Global South": 0.7,
    "Biomedical Large Language Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the dataset's evaluation, linking to advancements in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Medical Question-Answering",
        "canonical": "Medical Question-Answering",
        "aliases": [
          "Medical QA"
        ],
        "category": "unique_technical",
        "rationale": "Focus of the dataset, providing a unique link to medical AI applications.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Demographic Bias",
        "canonical": "Demographic Bias",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Relevant for understanding biases in AI models, crucial for ethical AI discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Global South",
        "canonical": "Global South",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Important for contextualizing the geographical focus of the dataset.",
        "novelty_score": 0.6,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Biomedical LLMs",
        "canonical": "Biomedical Large Language Models",
        "aliases": [
          "Biomedical LLMs"
        ],
        "category": "unique_technical",
        "rationale": "Specifically evaluated in the study, highlighting a niche application of LLMs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "performance variation",
      "consumer preference"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Medical Question-Answering",
      "resolved_canonical": "Medical Question-Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Demographic Bias",
      "resolved_canonical": "Demographic Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Global South",
      "resolved_canonical": "Global South",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Biomedical LLMs",
      "resolved_canonical": "Biomedical Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2411.15640.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2411.15640](https://arxiv.org/abs/2411.15640)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/RephQA_ Evaluating Readability of Large Language Models in Public Health Question Answering_20250923|RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering]] (85.8% similar)
- [[2025-09-23/MedFact_ A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses_20250923|MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses]] (84.6% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (84.5% similar)
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (84.4% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Demographic Bias|Demographic Bias]]
**âš¡ Unique Technical**: [[keywords/Medical Question-Answering|Medical Question-Answering]], [[keywords/Biomedical Large Language Models|Biomedical Large Language Models]]
**ğŸš€ Evolved Concepts**: [[keywords/Global South|Global South]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2411.15640v4 Announce Type: replace 
Abstract: Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì˜ë£Œ ë¶„ì•¼ì˜ ê°ê´€ì‹ ë¬¸ì œ(MCQ) ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì´ë©°, íŠ¹íˆ ì˜ì‚¬ ë¶€ì¡± ë¬¸ì œë¥¼ ê²ªê³  ìˆëŠ” ì €ì†Œë“ ë° ì¤‘ê°„ ì†Œë“ êµ­ê°€ì—ì„œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì•„í”„ë¦¬ì¹´ ëŒ€ë¥™ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ìµœì´ˆì˜ ëŒ€ê·œëª¨ ë‹¤í•™ì œ ì˜ë£Œ ì§ˆë¬¸-ë‹µë³€(QA) ë°ì´í„°ì…‹ì¸ AfriMed-QAë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 16ê°œêµ­ 60ê°œ ì´ìƒì˜ ì˜ê³¼ëŒ€í•™ì—ì„œ ìˆ˜ì§‘í•œ 15,000ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, 32ê°œ ì˜ë£Œ ì „ë¬¸ ë¶„ì•¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. 30ê°œì˜ LLMì„ ì •í™•ì„±ê³¼ ì¸êµ¬í†µê³„í•™ì  í¸í–¥ì„ í¬í•¨í•œ ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ í‰ê°€í•œ ê²°ê³¼, ì „ë¬¸ ë¶„ì•¼ì™€ ì§€ì—­ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¤ë¥´ë©°, MCQ ì„±ëŠ¥ì€ ë¯¸êµ­ ì˜ì‚¬ ë©´í—ˆ ì‹œí—˜(USMLE)ì— ë¹„í•´ ë–¨ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ìƒì˜í•™ LLMì€ ì¼ë°˜ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ì•˜ê³ , ì†Œí˜• LLMì€ í•©ê²© ì ìˆ˜ë¥¼ ì–»ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¸ê°„ í‰ê°€ì—ì„œëŠ” LLMì˜ ë‹µë³€ê³¼ ì„¤ëª…ì´ ì„ìƒì˜ì˜ ë‹µë³€ë³´ë‹¤ ì„ í˜¸ë˜ëŠ” ê²½í–¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì €ì†Œë“ ë° ì¤‘ê°„ ì†Œë“ êµ­ê°€ì—ì„œ ì˜ë£Œ ì ‘ê·¼ì„±ì„ í–¥ìƒì‹œí‚¤ê³  ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆëŠ” ì ì¬ì ì¸ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 2. AfriMed-QAëŠ” 16ê°œêµ­ì˜ 60ê°œ ì´ìƒì˜ ì˜ê³¼ëŒ€í•™ì—ì„œ ìˆ˜ì§‘ëœ 15,000ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ëœ ìµœì´ˆì˜ ë²”ì•„í”„ë¦¬ì¹´ ì˜ì–´ ë‹¤ì „ë¬¸ ì˜ë£Œ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼, LLMì˜ ì„±ëŠ¥ì€ ì „ë¬¸ ë¶„ì•¼ì™€ ì§€ì—­ì— ë”°ë¼ í¬ê²Œ ë‹¤ë¥´ë©°, MCQ ì„±ëŠ¥ì€ USMLE (MedQA)ì— ë¹„í•´ ë’¤ì²˜ì§‘ë‹ˆë‹¤.
- 4. ìƒì˜í•™ LLMì€ ì¼ë°˜ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ê³ , ì†Œí˜• ì—£ì§€ ì¹œí™”ì  LLMì€ í•©ê²© ì ìˆ˜ë¥¼ ì–»ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
- 5. ì¸ê°„ í‰ê°€ì—ì„œëŠ” LLMì˜ ë‹µë³€ê³¼ ì„¤ëª…ì´ ì„ìƒì˜ì˜ ë‹µë³€ì— ë¹„í•´ ì¼ê´€ë˜ê²Œ ì†Œë¹„ì ì„ í˜¸ë„ë¥¼ ì–»ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:44:48*