---
keywords:
  - Large Language Model
  - Medical Question-Answering
  - Demographic Bias
  - Global South
  - Biomedical Large Language Models
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2411.15640
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:44:48.680871",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Medical Question-Answering",
    "Demographic Bias",
    "Global South",
    "Biomedical Large Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Medical Question-Answering": 0.78,
    "Demographic Bias": 0.8,
    "Global South": 0.7,
    "Biomedical Large Language Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the dataset's evaluation, linking to advancements in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Medical Question-Answering",
        "canonical": "Medical Question-Answering",
        "aliases": [
          "Medical QA"
        ],
        "category": "unique_technical",
        "rationale": "Focus of the dataset, providing a unique link to medical AI applications.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Demographic Bias",
        "canonical": "Demographic Bias",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Relevant for understanding biases in AI models, crucial for ethical AI discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Global South",
        "canonical": "Global South",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Important for contextualizing the geographical focus of the dataset.",
        "novelty_score": 0.6,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Biomedical LLMs",
        "canonical": "Biomedical Large Language Models",
        "aliases": [
          "Biomedical LLMs"
        ],
        "category": "unique_technical",
        "rationale": "Specifically evaluated in the study, highlighting a niche application of LLMs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "performance variation",
      "consumer preference"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Medical Question-Answering",
      "resolved_canonical": "Medical Question-Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Demographic Bias",
      "resolved_canonical": "Demographic Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Global South",
      "resolved_canonical": "Global South",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Biomedical LLMs",
      "resolved_canonical": "Biomedical Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2411.15640.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2411.15640](https://arxiv.org/abs/2411.15640)

## 🔗 유사한 논문
- [[2025-09-23/RephQA_ Evaluating Readability of Large Language Models in Public Health Question Answering_20250923|RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering]] (85.8% similar)
- [[2025-09-23/MedFact_ A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses_20250923|MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses]] (84.6% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (84.5% similar)
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (84.4% similar)
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Demographic Bias|Demographic Bias]]
**⚡ Unique Technical**: [[keywords/Medical Question-Answering|Medical Question-Answering]], [[keywords/Biomedical Large Language Models|Biomedical Large Language Models]]
**🚀 Evolved Concepts**: [[keywords/Global South|Global South]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2411.15640v4 Announce Type: replace 
Abstract: Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.

## 📝 요약

최근 대형 언어 모델(LLM)이 의료 분야의 객관식 문제(MCQ) 벤치마크에서 뛰어난 성과를 보이며, 특히 의사 부족 문제를 겪고 있는 저소득 및 중간 소득 국가에서 관심이 높아지고 있습니다. 본 연구에서는 아프리카 대륙을 중심으로 한 최초의 대규모 다학제 의료 질문-답변(QA) 데이터셋인 AfriMed-QA를 소개합니다. 이 데이터셋은 16개국 60개 이상의 의과대학에서 수집한 15,000개의 질문으로 구성되어 있으며, 32개 의료 전문 분야를 포함합니다. 30개의 LLM을 정확성과 인구통계학적 편향을 포함한 여러 측면에서 평가한 결과, 전문 분야와 지역에 따라 성능이 크게 다르며, MCQ 성능은 미국 의사 면허 시험(USMLE)에 비해 떨어지는 것으로 나타났습니다. 생의학 LLM은 일반 모델보다 성능이 낮았고, 소형 LLM은 합격 점수를 얻기 어려웠습니다. 그러나 인간 평가에서는 LLM의 답변과 설명이 임상의의 답변보다 선호되는 경향이 있었습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 저소득 및 중간 소득 국가에서 의료 접근성을 향상시키고 비용을 절감할 수 있는 잠재적인 경로를 제공합니다.
- 2. AfriMed-QA는 16개국의 60개 이상의 의과대학에서 수집된 15,000개의 질문으로 구성된 최초의 범아프리카 영어 다전문 의료 질문-답변 데이터셋입니다.
- 3. 연구 결과, LLM의 성능은 전문 분야와 지역에 따라 크게 다르며, MCQ 성능은 USMLE (MedQA)에 비해 뒤처집니다.
- 4. 생의학 LLM은 일반 모델보다 성능이 낮고, 소형 엣지 친화적 LLM은 합격 점수를 얻는 데 어려움을 겪습니다.
- 5. 인간 평가에서는 LLM의 답변과 설명이 임상의의 답변에 비해 일관되게 소비자 선호도를 얻는 것으로 나타났습니다.


---

*Generated on 2025-09-24 03:44:48*