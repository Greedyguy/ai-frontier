---
keywords:
  - Learned Sparse Retrieval
  - SPLADE
  - Transformer
  - Vocabulary Granularity
  - ESPLADE
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16621
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:39:06.295133",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Learned Sparse Retrieval",
    "SPLADE",
    "Transformer",
    "Vocabulary Granularity",
    "ESPLADE"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Learned Sparse Retrieval": 0.85,
    "SPLADE": 0.8,
    "Transformer": 0.75,
    "Vocabulary Granularity": 0.7,
    "ESPLADE": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Learned Sparse Retrieval",
        "canonical": "Learned Sparse Retrieval",
        "aliases": [
          "LSR"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's focus on retrieval efficiency and effectiveness, offering unique insights into sparse representation learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "SPLADE",
        "canonical": "SPLADE",
        "aliases": [
          "Sparse Lattice"
        ],
        "category": "unique_technical",
        "rationale": "SPLADE is a specific model discussed extensively in the paper, critical for understanding the proposed improvements.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "BERT",
        "canonical": "Transformer",
        "aliases": [
          "Bidirectional Encoder Representations from Transformers"
        ],
        "category": "broad_technical",
        "rationale": "BERT is a well-known Transformer model used in the study, linking to broader discussions on Transformer-based architectures.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "vocabularic granularity",
        "canonical": "Vocabulary Granularity",
        "aliases": [
          "vocabularic detail"
        ],
        "category": "unique_technical",
        "rationale": "This concept is crucial for understanding the paper's exploration of vocabulary roles in retrieval models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "ESPLADE",
        "canonical": "ESPLADE",
        "aliases": [
          "Expanded SPLADE"
        ],
        "category": "unique_technical",
        "rationale": "ESPLADE represents a key advancement in the paper, focusing on expanded vocabularies for improved retrieval.",
        "novelty_score": 0.68,
        "connectivity_score": 0.58,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Learned Sparse Retrieval",
      "resolved_canonical": "Learned Sparse Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "SPLADE",
      "resolved_canonical": "SPLADE",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "BERT",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "vocabularic granularity",
      "resolved_canonical": "Vocabulary Granularity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "ESPLADE",
      "resolved_canonical": "ESPLADE",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.58,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# The Role of Vocabularies in Learning Sparse Representations for Ranking

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16621.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16621](https://arxiv.org/abs/2509.16621)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval_20250923|Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval]] (80.8% similar)
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (78.9% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (78.9% similar)
- [[2025-09-23/Federated Learning with Ad-hoc Adapter Insertions_ The Case of Soft-Embeddings for Training Classifier-as-Retriever_20250923|Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever]] (78.8% similar)
- [[2025-09-23/LCES_ Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models_20250923|LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models]] (78.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**âš¡ Unique Technical**: [[keywords/Learned Sparse Retrieval|Learned Sparse Retrieval]], [[keywords/SPLADE|SPLADE]], [[keywords/Vocabulary Granularity|Vocabulary Granularity]], [[keywords/ESPLADE|ESPLADE]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16621v1 Announce Type: cross 
Abstract: Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for effective semantic 1st stage matching while enjoying the efficiency of inverted indices. A recent work on learning SPLADE models with expanded vocabularies (ESPLADE) was proposed to represent queries and documents into a sparse space of custom vocabulary which have different levels of vocabularic granularity. Within this effort, however, there have not been many studies on the role of vocabulary in SPLADE models and their relationship to retrieval efficiency and effectiveness.
  To study this, we construct BERT models with 100K-sized output vocabularies, one initialized with the ESPLADE pretraining method and one initialized randomly. After finetune on real-world search click logs, we applied logit score-based queries and documents pruning to max size for further balancing efficiency. The experimental result in our evaluation set shows that, when pruning is applied, the two models are effective compared to the 32K-sized normal SPLADE model in the computational budget under the BM25. And the ESPLADE models are more effective than the random vocab model, while having a similar retrieval cost.
  The result indicates that the size and pretrained weight of output vocabularies play the role of configuring the representational specification for queries, documents, and their interactions in the retrieval engine, beyond their original meaning and purposes in NLP. These findings can provide a new room for improvement for LSR by identifying the importance of representational specification from vocabulary configuration for efficient and effective retrieval.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ SPLADE ëª¨ë¸ì˜ ì–´íœ˜ í™•ì¥ì„ í†µí•œ í•™ìŠµ í¬ì†Œ ê²€ìƒ‰(LSR)ì˜ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ESPLADEë¼ëŠ” í™•ì¥ëœ ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¥¼ í¬ì†Œ ê³µê°„ì— í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 10ë§Œ ê°œì˜ ì¶œë ¥ ì–´íœ˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ESPLADE ì‚¬ì „ í•™ìŠµ ë°©ë²•ê³¼ ë¬´ì‘ìœ„ ì´ˆê¸°í™”ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ì‹¤ì œ ê²€ìƒ‰ í´ë¦­ ë¡œê·¸ë¡œ ë¯¸ì„¸ ì¡°ì • í›„, íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë‘ ëª¨ë¸ ëª¨ë‘ 3.2ë§Œ ê°œ ì–´íœ˜ì˜ ì¼ë°˜ SPLADE ëª¨ë¸ë³´ë‹¤ íš¨ìœ¨ì ì´ë©°, ESPLADE ëª¨ë¸ì´ ë¬´ì‘ìœ„ ì–´íœ˜ ëª¨ë¸ë³´ë‹¤ ë” íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì–´íœ˜ì˜ í¬ê¸°ì™€ ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ê°€ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ í‘œí˜„ ì‚¬ì–‘ì„ êµ¬ì„±í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ LSRì˜ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ESPLADEëŠ” í™•ì¥ëœ ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¥¼ í¬ì†Œ ê³µê°„ì— í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ì–´íœ˜ì˜ ì—­í• ê³¼ ê²€ìƒ‰ íš¨ìœ¨ì„± ë° íš¨ê³¼ì„±ì˜ ê´€ê³„ì— ëŒ€í•œ ì—°êµ¬ëŠ” ë¶€ì¡±í–ˆë‹¤.
- 2. 100K í¬ê¸°ì˜ ì¶œë ¥ ì–´íœ˜ë¥¼ ê°€ì§„ BERT ëª¨ë¸ì„ êµ¬ì¶•í•˜ì—¬, ESPLADE ì‚¬ì „ í•™ìŠµ ë°©ë²•ê³¼ ëœë¤ ì´ˆê¸°í™”ë¥¼ ë¹„êµí•˜ê³  ì‹¤ì œ ê²€ìƒ‰ í´ë¦­ ë¡œê·¸ë¡œ ë¯¸ì„¸ ì¡°ì •í–ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, ë¡œê·¸ ì ìˆ˜ ê¸°ë°˜ì˜ ì¿¼ë¦¬ ë° ë¬¸ì„œ ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•˜ë©´ ë‘ ëª¨ë¸ ëª¨ë‘ 32K í¬ê¸°ì˜ ì¼ë°˜ SPLADE ëª¨ë¸ë³´ë‹¤ íš¨ìœ¨ì ì´ë©°, ESPLADE ëª¨ë¸ì´ ëœë¤ ì–´íœ˜ ëª¨ë¸ë³´ë‹¤ ë” íš¨ê³¼ì ì´ì—ˆë‹¤.
- 4. ì¶œë ¥ ì–´íœ˜ì˜ í¬ê¸°ì™€ ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ëŠ” ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ í‘œí˜„ ì‚¬ì–‘ì„ êµ¬ì„±í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- 5. ì–´íœ˜ êµ¬ì„±ì—ì„œì˜ í‘œí˜„ ì‚¬ì–‘ì˜ ì¤‘ìš”ì„±ì„ ì¸ì‹í•¨ìœ¼ë¡œì¨ LSRì˜ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-24 03:39:06*