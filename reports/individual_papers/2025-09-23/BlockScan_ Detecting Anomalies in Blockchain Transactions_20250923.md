---
keywords:
  - BlockScan
  - Transformer
  - Multimodal Learning
  - RoPE Embedding
  - FlashAttention
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2410.04039
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:39:53.938691",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "BlockScan",
    "Transformer",
    "Multimodal Learning",
    "RoPE Embedding",
    "FlashAttention"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "BlockScan": 0.8,
    "Transformer": 0.85,
    "Multimodal Learning": 0.78,
    "RoPE Embedding": 0.75,
    "FlashAttention": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "BlockScan",
        "canonical": "BlockScan",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "BlockScan is a novel system specifically designed for anomaly detection in blockchain transactions, offering unique features not covered by existing terms.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model architecture in deep learning, relevant to the paper's methodology.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "The paper discusses handling multi-modal inputs, which is central to multimodal learning approaches.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "RoPE embedding",
        "canonical": "RoPE Embedding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "RoPE embedding is a specific technique used in the paper to enhance the Transformer model's capability.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "FlashAttention",
        "canonical": "FlashAttention",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "FlashAttention is a specialized mechanism used to handle longer sequences, relevant to the paper's focus.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "anomaly detection",
      "blockchain transactions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "BlockScan",
      "resolved_canonical": "BlockScan",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "RoPE embedding",
      "resolved_canonical": "RoPE Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "FlashAttention",
      "resolved_canonical": "FlashAttention",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# BlockScan: Detecting Anomalies in Blockchain Transactions

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2410.04039.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2410.04039](https://arxiv.org/abs/2410.04039)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Evaluating Supervised Learning Models for Fraud Detection_ A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data_20250919|Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data]] (78.1% similar)
- [[2025-09-18/Credit Card Fraud Detection_20250918|Credit Card Fraud Detection]] (76.6% similar)
- [[2025-09-23/R-Net_ A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration_20250923|R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration]] (76.5% similar)
- [[2025-09-23/Transformer-Gather, Fuzzy-Reconsider_ A Scalable Hybrid Framework for Entity Resolution_20250923|Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for Entity Resolution]] (76.4% similar)
- [[2025-09-22/TSCAN_ Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis_20250922|TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis]] (76.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/BlockScan|BlockScan]], [[keywords/RoPE Embedding|RoPE Embedding]], [[keywords/FlashAttention|FlashAttention]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.04039v4 Announce Type: replace-cross 
Abstract: We propose BlockScan, a customized Transformer for anomaly detection in blockchain transactions. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models (LLMs), BlockScan introduces a series of customized designs to effectively model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a novel modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized masked language modeling mechanism for pretraining the Transformer architecture, incorporating RoPE embedding and FlashAttention for handling longer sequences. Finally, we design a novel anomaly detection method based on the model outputs. We further provide theoretical analysis for the detection method of our system. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockScan's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockScan is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work sets a new benchmark for applying Transformer-based approaches in blockchain data analysis.

## ğŸ“ ìš”ì•½

BlockScanì€ ë¸”ë¡ì²´ì¸ ê±°ë˜ì˜ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ ë§ì¶¤í˜• íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œì´ë‚˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ê³¼ ë‹¬ë¦¬, BlockScanì€ ë¸”ë¡ì²´ì¸ ê±°ë˜ì˜ ë…íŠ¹í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ ë§ì¶¤í˜• ì„¤ê³„ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ë¸”ë¡ì²´ì¸ ê±°ë˜ëŠ” ë¸”ë¡ì²´ì¸ íŠ¹ìœ ì˜ í† í°, í…ìŠ¤íŠ¸, ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ë¥¼ ê°€ì§€ë©°, ì´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ëª¨ë“ˆí™”ëœ í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, RoPE ì„ë² ë”©ê³¼ FlashAttentionì„ í™œìš©í•˜ì—¬ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë§ì¶¤í˜• ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§ ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. BlockScanì€ ì´ ëª¨ë¸ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ ì´ìƒ íƒì§€ ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì´ ë°©ë²•ì— ëŒ€í•œ ì´ë¡ ì  ë¶„ì„ë„ ì œê³µí•©ë‹ˆë‹¤. ì´ë”ë¦¬ì›€ê³¼ ì†”ë¼ë‚˜ ê±°ë˜ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ í‰ê°€ ê²°ê³¼, BlockScanì€ ë‚®ì€ ì˜¤íƒë¥ ì„ ìœ ì§€í•˜ë©´ì„œ ë›°ì–´ë‚œ ì´ìƒ íƒì§€ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, íŠ¹íˆ ì†”ë¼ë‚˜ì—ì„œ ë†’ì€ ì •í™•ë„ë¡œ ì´ìƒ ê±°ë˜ë¥¼ íƒì§€í•œ ìœ ì¼í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë¸”ë¡ì²´ì¸ ë°ì´í„° ë¶„ì„ì— íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. BlockScanì€ ë¸”ë¡ì²´ì¸ ê±°ë˜ì˜ ì´ìƒ íƒì§€ë¥¼ ìœ„í•´ ë§ì¶¤í˜• Transformerë¥¼ ì œì•ˆí•˜ë©°, ê¸°ì¡´ì˜ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œì´ë‚˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì§ì ‘ ì ìš©í•˜ëŠ” ë°©ë²•ê³¼ ì°¨ë³„í™”ë©ë‹ˆë‹¤.
- 2. ë¸”ë¡ì²´ì¸ ê±°ë˜ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ íŠ¹ì„±ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ëª¨ë“ˆí™”ëœ í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ê³„í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ì •ë³´ë¥¼ ê· í˜• ìˆê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- 3. RoPE ì„ë² ë”©ê³¼ FlashAttentionì„ í†µí•©í•˜ì—¬ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë§ì¶¤í˜• ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§ ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤.
- 4. ì´ë¡ ì  ë¶„ì„ì„ í†µí•´ BlockScanì˜ ì´ìƒ íƒì§€ ë°©ë²•ì˜ ìœ íš¨ì„±ì„ ì…ì¦í•˜ì˜€ìœ¼ë©°, Ethereumê³¼ Solana ê±°ë˜ì— ëŒ€í•œ í‰ê°€ì—ì„œ ë†’ì€ íƒì§€ ì„±ëŠ¥ê³¼ ë‚®ì€ ì˜¤íƒë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. íŠ¹íˆ Solanaì—ì„œì˜ ì´ìƒ ê±°ë˜ íƒì§€ì—ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©°, ë‹¤ë¥¸ ë°©ë²•ë“¤ì´ ë‚®ê±°ë‚˜ 0ì— ê°€ê¹Œìš´ íƒì§€ ì„±ëŠ¥ì„ ë³´ì¸ ê²ƒê³¼ ëŒ€ë¹„ë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:39:53*