---
keywords:
  - BlockScan
  - Transformer
  - Multimodal Learning
  - RoPE Embedding
  - FlashAttention
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2410.04039
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:39:53.938691",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "BlockScan",
    "Transformer",
    "Multimodal Learning",
    "RoPE Embedding",
    "FlashAttention"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "BlockScan": 0.8,
    "Transformer": 0.85,
    "Multimodal Learning": 0.78,
    "RoPE Embedding": 0.75,
    "FlashAttention": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "BlockScan",
        "canonical": "BlockScan",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "BlockScan is a novel system specifically designed for anomaly detection in blockchain transactions, offering unique features not covered by existing terms.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model architecture in deep learning, relevant to the paper's methodology.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "The paper discusses handling multi-modal inputs, which is central to multimodal learning approaches.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "RoPE embedding",
        "canonical": "RoPE Embedding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "RoPE embedding is a specific technique used in the paper to enhance the Transformer model's capability.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "FlashAttention",
        "canonical": "FlashAttention",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "FlashAttention is a specialized mechanism used to handle longer sequences, relevant to the paper's focus.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "anomaly detection",
      "blockchain transactions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "BlockScan",
      "resolved_canonical": "BlockScan",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "RoPE embedding",
      "resolved_canonical": "RoPE Embedding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "FlashAttention",
      "resolved_canonical": "FlashAttention",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# BlockScan: Detecting Anomalies in Blockchain Transactions

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2410.04039.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2410.04039](https://arxiv.org/abs/2410.04039)

## 🔗 유사한 논문
- [[2025-09-19/Evaluating Supervised Learning Models for Fraud Detection_ A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data_20250919|Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data]] (78.1% similar)
- [[2025-09-18/Credit Card Fraud Detection_20250918|Credit Card Fraud Detection]] (76.6% similar)
- [[2025-09-23/R-Net_ A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration_20250923|R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration]] (76.5% similar)
- [[2025-09-23/Transformer-Gather, Fuzzy-Reconsider_ A Scalable Hybrid Framework for Entity Resolution_20250923|Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for Entity Resolution]] (76.4% similar)
- [[2025-09-22/TSCAN_ Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis_20250922|TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis]] (76.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/BlockScan|BlockScan]], [[keywords/RoPE Embedding|RoPE Embedding]], [[keywords/FlashAttention|FlashAttention]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2410.04039v4 Announce Type: replace-cross 
Abstract: We propose BlockScan, a customized Transformer for anomaly detection in blockchain transactions. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models (LLMs), BlockScan introduces a series of customized designs to effectively model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a novel modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized masked language modeling mechanism for pretraining the Transformer architecture, incorporating RoPE embedding and FlashAttention for handling longer sequences. Finally, we design a novel anomaly detection method based on the model outputs. We further provide theoretical analysis for the detection method of our system. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockScan's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockScan is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work sets a new benchmark for applying Transformer-based approaches in blockchain data analysis.

## 📝 요약

BlockScan은 블록체인 거래의 이상 탐지를 위한 맞춤형 트랜스포머 모델입니다. 기존의 규칙 기반 시스템이나 대형 언어 모델을 직접 사용하는 방법과 달리, BlockScan은 블록체인 거래의 독특한 데이터 구조를 효과적으로 모델링하기 위해 맞춤형 설계를 도입했습니다. 블록체인 거래는 블록체인 특유의 토큰, 텍스트, 숫자를 포함하는 다중 모달 데이터를 가지며, 이를 처리하기 위해 모듈화된 토크나이저를 설계했습니다. 또한, RoPE 임베딩과 FlashAttention을 활용하여 긴 시퀀스를 처리할 수 있는 맞춤형 마스크드 언어 모델링 메커니즘을 설계했습니다. BlockScan은 이 모델 출력을 기반으로 한 새로운 이상 탐지 방법을 제안하며, 이 방법에 대한 이론적 분석도 제공합니다. 이더리움과 솔라나 거래에 대한 광범위한 평가 결과, BlockScan은 낮은 오탐률을 유지하면서 뛰어난 이상 탐지 능력을 보여주었으며, 특히 솔라나에서 높은 정확도로 이상 거래를 탐지한 유일한 방법입니다. 이 연구는 블록체인 데이터 분석에 트랜스포머 기반 접근 방식을 적용하는 새로운 기준을 제시합니다.

## 🎯 주요 포인트

- 1. BlockScan은 블록체인 거래의 이상 탐지를 위해 맞춤형 Transformer를 제안하며, 기존의 규칙 기반 시스템이나 대형 언어 모델을 직접 적용하는 방법과 차별화됩니다.
- 2. 블록체인 거래의 다중 모달 특성을 처리하기 위해 모듈화된 토크나이저를 설계하여 다양한 모달리티 간의 정보를 균형 있게 처리합니다.
- 3. RoPE 임베딩과 FlashAttention을 통합하여 긴 시퀀스를 처리할 수 있는 맞춤형 마스크드 언어 모델링 메커니즘을 설계하였습니다.
- 4. 이론적 분석을 통해 BlockScan의 이상 탐지 방법의 유효성을 입증하였으며, Ethereum과 Solana 거래에 대한 평가에서 높은 탐지 성능과 낮은 오탐률을 보였습니다.
- 5. 특히 Solana에서의 이상 거래 탐지에서 높은 정확도를 기록하며, 다른 방법들이 낮거나 0에 가까운 탐지 성능을 보인 것과 대비됩니다.


---

*Generated on 2025-09-24 00:39:53*