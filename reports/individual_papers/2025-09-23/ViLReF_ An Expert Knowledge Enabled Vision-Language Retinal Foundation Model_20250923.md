---
keywords:
  - Vision-Language Model
  - Weighted Similarity Coupling Loss
  - Zero-Shot Learning
  - Batch Expansion Module
  - Dynamic Memory Queues
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2408.10894
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:15:02.726502",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Weighted Similarity Coupling Loss",
    "Zero-Shot Learning",
    "Batch Expansion Module",
    "Dynamic Memory Queues"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.88,
    "Weighted Similarity Coupling Loss": 0.7,
    "Zero-Shot Learning": 0.85,
    "Batch Expansion Module": 0.72,
    "Dynamic Memory Queues": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Retinal Foundation Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "ViLReF"
        ],
        "category": "evolved_concepts",
        "rationale": "This connects to the broader concept of vision-language models, which is crucial for understanding the integration of visual and textual data.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Weighted Similarity Coupling Loss",
        "canonical": "Weighted Similarity Coupling Loss",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique loss function introduced in the paper, which is essential for understanding the model's training process.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a key capability of the model, linking it to broader trends in machine learning.",
        "novelty_score": 0.6,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Batch Expansion Module",
        "canonical": "Batch Expansion Module",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This module is a novel component of the model architecture, important for understanding its data handling strategy.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Dynamic Memory Queues",
        "canonical": "Dynamic Memory Queues",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Dynamic memory queues are crucial for the model's ability to manage data efficiently during training.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "pre-training",
      "model's learning ability",
      "extensive experiments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Retinal Foundation Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Weighted Similarity Coupling Loss",
      "resolved_canonical": "Weighted Similarity Coupling Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Batch Expansion Module",
      "resolved_canonical": "Batch Expansion Module",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Dynamic Memory Queues",
      "resolved_canonical": "Dynamic Memory Queues",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# ViLReF: An Expert Knowledge Enabled Vision-Language Retinal Foundation Model

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2408.10894.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2408.10894](https://arxiv.org/abs/2408.10894)

## 🔗 유사한 논문
- [[2025-09-23/Vision Language Models Are Not (Yet) Spelling Correctors_20250923|Vision Language Models Are Not (Yet) Spelling Correctors]] (85.0% similar)
- [[2025-09-23/Visual Instruction Pretraining for Domain-Specific Foundation Models_20250923|Visual Instruction Pretraining for Domain-Specific Foundation Models]] (85.0% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (84.1% similar)
- [[2025-09-23/Catching the Details_ Self-Distilled RoI Predictors for Fine-Grained MLLM Perception_20250923|Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception]] (83.8% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (83.6% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Weighted Similarity Coupling Loss|Weighted Similarity Coupling Loss]], [[keywords/Batch Expansion Module|Batch Expansion Module]], [[keywords/Dynamic Memory Queues|Dynamic Memory Queues]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2408.10894v4 Announce Type: replace 
Abstract: Subtle semantic differences in retinal image and text data present great challenges for pre-training visual-language models. Moreover, false negative samples, i.e., image-text pairs having the same semantics but incorrectly regarded as negatives, disrupt the visual-language pre-training process and affect the model's learning ability. This work aims to develop a retinal foundation model, called ViLReF, by pre-training on a paired dataset comprising 451,956 retinal images and corresponding diagnostic text reports. In our vision-language pre-training strategy, we leverage expert knowledge to facilitate the extraction of labels and propose a novel constraint, the Weighted Similarity Coupling Loss, to adjust the speed of pushing sample pairs further apart dynamically within the feature space. Furthermore, we employ a batch expansion module with dynamic memory queues, maintained by momentum encoders, to supply extra samples and compensate for the vacancies caused by eliminating false negatives. Extensive experiments are conducted on multiple datasets for downstream classification and segmentation tasks. The experimental results demonstrate the powerful zero-shot and transfer learning capabilities of ViLReF, verifying the effectiveness of our pre-training strategy. Our ViLReF model is available at: https://github.com/T6Yang/ViLReF.

## 📝 요약

이 연구는 망막 이미지와 진단 텍스트 보고서로 구성된 데이터셋을 사용하여 시각-언어 모델인 ViLReF를 사전 훈련하는 방법을 제안합니다. 주요 기여는 전문가 지식을 활용한 레이블 추출과 '가중 유사성 결합 손실'이라는 새로운 제약 조건을 도입하여 샘플 쌍 간의 거리를 동적으로 조절하는 것입니다. 또한, 모멘텀 인코더로 유지되는 동적 메모리 큐를 사용하여 배치 확장 모듈을 구현, 잘못된 부정 샘플을 제거한 후의 공백을 보완합니다. 다양한 데이터셋에 대한 실험 결과, ViLReF의 강력한 제로샷 및 전이 학습 능력이 입증되었습니다.

## 🎯 주요 포인트

- 1. ViLReF는 451,956개의 망막 이미지와 진단 텍스트 보고서로 구성된 데이터셋을 사용하여 사전 학습된 망막 기초 모델입니다.
- 2. 시각-언어 사전 학습 과정에서 전문가 지식을 활용하여 레이블 추출을 용이하게 하고, Weighted Similarity Coupling Loss라는 새로운 제약을 제안합니다.
- 3. 배치 확장 모듈과 동적 메모리 큐를 사용하여 잘못된 음성 샘플을 제거함으로써 발생하는 공백을 보완합니다.
- 4. 다양한 데이터셋에서의 실험 결과, ViLReF는 강력한 제로샷 및 전이 학습 능력을 보여주며, 사전 학습 전략의 효과성을 입증합니다.


---

*Generated on 2025-09-24 05:15:02*