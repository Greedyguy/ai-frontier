---
keywords:
  - Long-tailed Semi-Supervised Learning
  - Open-World Scenarios
  - Parameter-Efficient Fine-Tuning
  - Large Language Model
  - Out-of-Distribution Samples
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.09926
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:52:27.559295",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Long-tailed Semi-Supervised Learning",
    "Open-World Scenarios",
    "Parameter-Efficient Fine-Tuning",
    "Large Language Model",
    "Out-of-Distribution Samples"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Long-tailed Semi-Supervised Learning": 0.78,
    "Open-World Scenarios": 0.77,
    "Parameter-Efficient Fine-Tuning": 0.79,
    "Large Language Model": 0.75,
    "Out-of-Distribution Samples": 0.76
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Long-tailed Semi-Supervised Learning",
        "canonical": "Long-tailed Semi-Supervised Learning",
        "aliases": [
          "LTSSL"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and connects to niche research in handling imbalanced datasets.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Open-World Scenarios",
        "canonical": "Open-World Scenarios",
        "aliases": [
          "Open World"
        ],
        "category": "unique_technical",
        "rationale": "The concept of open-world scenarios is crucial for understanding the challenges addressed in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Parameter-Efficient Fine-Tuning",
        "canonical": "Parameter-Efficient Fine-Tuning",
        "aliases": [
          "Efficient Fine-Tuning"
        ],
        "category": "unique_technical",
        "rationale": "This approach is a key innovation in the paper, linking to efficiency in model training.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Foundation Model",
        "canonical": "Large Language Model",
        "aliases": [
          "Foundation Models"
        ],
        "category": "broad_technical",
        "rationale": "Foundation models are a broad category that connects to the use of large pre-trained models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "Out-of-Distribution Samples",
        "canonical": "Out-of-Distribution Samples",
        "aliases": [
          "OOD Samples"
        ],
        "category": "specific_connectable",
        "rationale": "Handling OOD samples is critical for the proposed method's effectiveness in open-world scenarios.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.76
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Long-tailed Semi-Supervised Learning",
      "resolved_canonical": "Long-tailed Semi-Supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Open-World Scenarios",
      "resolved_canonical": "Open-World Scenarios",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Parameter-Efficient Fine-Tuning",
      "resolved_canonical": "Parameter-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Foundation Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Out-of-Distribution Samples",
      "resolved_canonical": "Out-of-Distribution Samples",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.76
      }
    }
  ]
}
-->

# LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.09926.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.09926](https://arxiv.org/abs/2509.09926)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (85.1% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (83.2% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.8% similar)
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (82.1% similar)
- [[2025-09-23/LLM-Guided Co-Training for Text Classification_20250923|LLM-Guided Co-Training for Text Classification]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Out-of-Distribution Samples|Out-of-Distribution Samples]]
**âš¡ Unique Technical**: [[keywords/Long-tailed Semi-Supervised Learning|Long-tailed Semi-Supervised Learning]], [[keywords/Open-World Scenarios|Open-World Scenarios]], [[keywords/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.09926v2 Announce Type: replace 
Abstract: Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\% of the unlabeled data compared with previous works.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸´ ê¼¬ë¦¬ í•™ìŠµ(Long-tailed learning)ì—ì„œì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ LoFTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê¸´ ê¼¬ë¦¬ ë°˜ì§€ë„ í•™ìŠµ(LTSSL) ë°©ë²•ì€ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ, ê³¼ì‹ ê³¼ ë‚®ì€ í’ˆì§ˆì˜ ê°€ì§œ ë ˆì´ë¸” ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ ì—°êµ¬ëŠ” íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê°€ì§œ ë ˆì´ë¸”ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, ì—´ë¦° ì„¸ê³„ ì¡°ê±´ì—ì„œì˜ ë°˜ì§€ë„ í•™ìŠµì„ íƒêµ¬í•˜ì—¬ ë¶„ë¥˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” LoFT-OWë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì´ì „ ë°©ë²•ë“¤ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì ì€ ì–‘ì˜ ë¯¸ë¼ë²¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œë„ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Long-Tailed Semi-Supervised Learning (LTSSL)ì€ ë¶ˆê· í˜•í•œ ë¼ë²¨ ë°ì´í„°ì…‹ì— ë§ì€ ì–‘ì˜ ë¹„ë¼ë²¨ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ íš¨ê³¼ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ LTSSL ë°©ë²•ì€ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ, ê³¼ì‹  ë° ë‚®ì€ í’ˆì§ˆì˜ ê°€ì§œ ë¼ë²¨ ë¬¸ì œë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” LTSSLì„ ê¸°ì´ˆ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ í™•ì¥í•˜ì—¬, LoFTë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ë¯¸ì„¸ ì¡°ì •ëœ ê¸°ì´ˆ ëª¨ë¸ì€ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê°€ì§œ ë¼ë²¨ì„ ìƒì„±í•˜ì—¬ ë¶ˆê· í˜• í•™ìŠµì— ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. LoFT-OWëŠ” ê°œë°©í˜• ì„¸ê³„ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë¹„ë¼ë²¨ ë°ì´í„°ì— í¬í•¨ë  ìˆ˜ ìˆëŠ” ë¶„í¬ ì™¸ ìƒ˜í”Œì„ ì²˜ë¦¬í•˜ì—¬ íŒë³„ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:52:27*