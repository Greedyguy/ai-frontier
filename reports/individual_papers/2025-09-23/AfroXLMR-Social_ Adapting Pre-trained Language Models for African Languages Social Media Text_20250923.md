---
keywords:
  - Domain Adaptive Pre-training
  - Task Adaptive Pre-training
  - African Languages Social Media
  - Sentiment Analysis
  - Hate Speech Classification
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2503.18247
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:53:35.186333",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Domain Adaptive Pre-training",
    "Task Adaptive Pre-training",
    "African Languages Social Media",
    "Sentiment Analysis",
    "Hate Speech Classification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Domain Adaptive Pre-training": 0.78,
    "Task Adaptive Pre-training": 0.77,
    "African Languages Social Media": 0.79,
    "Sentiment Analysis": 0.8,
    "Hate Speech Classification": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Domain Adaptive Pre-training",
        "canonical": "Domain Adaptive Pre-training",
        "aliases": [
          "DAPT"
        ],
        "category": "unique_technical",
        "rationale": "This technique is crucial for improving language model performance in specific domains, enhancing connectivity with domain-specific NLP research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Task Adaptive Pre-training",
        "canonical": "Task Adaptive Pre-training",
        "aliases": [
          "TAPT"
        ],
        "category": "unique_technical",
        "rationale": "TAPT is a specialized pre-training approach that links well with task-specific NLP advancements.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "African Languages Social Media",
        "canonical": "African Languages Social Media",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Focusing on African languages in social media provides unique insights into multilingual NLP challenges.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "Sentiment Analysis",
        "canonical": "Sentiment Analysis",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A fundamental NLP task that connects to various emotion and opinion mining studies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Hate Speech Classification",
        "canonical": "Hate Speech Classification",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This task is critical for social media analysis and connects to broader discussions on online safety.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Domain Adaptive Pre-training",
      "resolved_canonical": "Domain Adaptive Pre-training",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Task Adaptive Pre-training",
      "resolved_canonical": "Task Adaptive Pre-training",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "African Languages Social Media",
      "resolved_canonical": "African Languages Social Media",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Sentiment Analysis",
      "resolved_canonical": "Sentiment Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Hate Speech Classification",
      "resolved_canonical": "Hate Speech Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.18247.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2503.18247](https://arxiv.org/abs/2503.18247)

## 🔗 유사한 논문
- [[2025-09-19/Advancing Conversational AI with Shona Slang_ A Dataset and Hybrid Model for Digital Inclusion_20250919|Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion]] (83.9% similar)
- [[2025-09-23/Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis_ A Comparative Study of Domain Adaptation and Fine-Tuning Strategies_20250923|Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies]] (83.7% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (81.0% similar)
- [[2025-09-17/Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications_20250917|Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications]] (80.3% similar)
- [[2025-09-23/AutoArabic_ A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks_20250923|AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks]] (80.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Sentiment Analysis|Sentiment Analysis]]
**🔗 Specific Connectable**: [[keywords/Hate Speech Classification|Hate Speech Classification]]
**⚡ Unique Technical**: [[keywords/Domain Adaptive Pre-training|Domain Adaptive Pre-training]], [[keywords/Task Adaptive Pre-training|Task Adaptive Pre-training]], [[keywords/African Languages Social Media|African Languages Social Media]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2503.18247v3 Announce Type: replace 
Abstract: Language models built from various sources are the foundation of today's NLP progress. However, for many low-resource languages, the diversity of domains is often limited, more biased to a religious domain, which impacts their performance when evaluated on distant and rapidly evolving domains such as social media. Domain adaptive pre-training (DAPT) and task-adaptive pre-training (TAPT) are popular techniques to reduce this bias through continual pre-training for BERT-based models, but they have not been explored for African multilingual encoders. In this paper, we explore DAPT and TAPT continual pre-training approaches for African languages social media domain. We introduce AfriSocial, a large-scale social media and news domain corpus for continual pre-training on several African languages. Leveraging AfriSocial, we show that DAPT consistently improves performance (from 1% to 30% F1 score) on three subjective tasks: sentiment analysis, multi-label emotion, and hate speech classification, covering 19 languages. Similarly, leveraging TAPT on the data from one task enhances performance on other related tasks. For example, training with unlabeled sentiment data (source) for a fine-grained emotion classification task (target) improves the baseline results by an F1 score ranging from 0.55% to 15.11%. Combining these two methods (i.e. DAPT + TAPT) further improves the overall performance. The data and model resources are available at HuggingFace.

## 📝 요약

이 논문은 아프리카 다국어 인코더에 대한 도메인 적응 사전 훈련(DAPT)과 작업 적응 사전 훈련(TAPT) 접근 방식을 탐구합니다. 저자들은 AfriSocial이라는 대규모 소셜 미디어 및 뉴스 도메인 코퍼스를 소개하여 여러 아프리카 언어에 대한 지속적인 사전 훈련을 수행했습니다. 이를 통해 DAPT는 감정 분석, 멀티라벨 감정, 혐오 발언 분류와 같은 주관적 작업에서 성능을 1%에서 30%까지 향상시켰습니다. 또한, TAPT는 한 작업의 데이터를 활용하여 관련 작업의 성능을 향상시켰으며, 두 방법을 결합하면 전체 성능이 더욱 개선되었습니다. 데이터와 모델 자원은 HuggingFace에서 제공됩니다.

## 🎯 주요 포인트

- 1. 아프리카 다국어 인코더에 대한 도메인 적응 사전 훈련(DAPT)과 작업 적응 사전 훈련(TAPT) 접근법을 탐구했습니다.
- 2. 대규모 소셜 미디어 및 뉴스 도메인 코퍼스인 AfriSocial을 소개하여 여러 아프리카 언어에 대한 지속적인 사전 훈련을 수행했습니다.
- 3. DAPT는 감정 분석, 다중 레이블 감정, 혐오 발언 분류와 같은 주관적 작업에서 성능을 1%에서 30%까지 향상시켰습니다.
- 4. TAPT는 한 작업의 데이터를 사용하여 관련 작업의 성능을 향상시키며, 예를 들어, 감정 분류 작업에서 F1 점수를 0.55%에서 15.11%까지 개선했습니다.
- 5. DAPT와 TAPT를 결합하면 전체 성능이 더욱 향상됩니다.


---

*Generated on 2025-09-24 03:53:35*