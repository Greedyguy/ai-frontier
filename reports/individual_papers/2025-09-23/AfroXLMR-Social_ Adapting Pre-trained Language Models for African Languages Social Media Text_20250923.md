---
keywords:
  - Domain Adaptive Pre-training
  - Task Adaptive Pre-training
  - African Languages Social Media
  - Sentiment Analysis
  - Hate Speech Classification
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2503.18247
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:53:35.186333",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Domain Adaptive Pre-training",
    "Task Adaptive Pre-training",
    "African Languages Social Media",
    "Sentiment Analysis",
    "Hate Speech Classification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Domain Adaptive Pre-training": 0.78,
    "Task Adaptive Pre-training": 0.77,
    "African Languages Social Media": 0.79,
    "Sentiment Analysis": 0.8,
    "Hate Speech Classification": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Domain Adaptive Pre-training",
        "canonical": "Domain Adaptive Pre-training",
        "aliases": [
          "DAPT"
        ],
        "category": "unique_technical",
        "rationale": "This technique is crucial for improving language model performance in specific domains, enhancing connectivity with domain-specific NLP research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Task Adaptive Pre-training",
        "canonical": "Task Adaptive Pre-training",
        "aliases": [
          "TAPT"
        ],
        "category": "unique_technical",
        "rationale": "TAPT is a specialized pre-training approach that links well with task-specific NLP advancements.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "African Languages Social Media",
        "canonical": "African Languages Social Media",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Focusing on African languages in social media provides unique insights into multilingual NLP challenges.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "Sentiment Analysis",
        "canonical": "Sentiment Analysis",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A fundamental NLP task that connects to various emotion and opinion mining studies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Hate Speech Classification",
        "canonical": "Hate Speech Classification",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This task is critical for social media analysis and connects to broader discussions on online safety.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Domain Adaptive Pre-training",
      "resolved_canonical": "Domain Adaptive Pre-training",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Task Adaptive Pre-training",
      "resolved_canonical": "Task Adaptive Pre-training",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "African Languages Social Media",
      "resolved_canonical": "African Languages Social Media",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Sentiment Analysis",
      "resolved_canonical": "Sentiment Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Hate Speech Classification",
      "resolved_canonical": "Hate Speech Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.18247.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2503.18247](https://arxiv.org/abs/2503.18247)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Advancing Conversational AI with Shona Slang_ A Dataset and Hybrid Model for Digital Inclusion_20250919|Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion]] (83.9% similar)
- [[2025-09-23/Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis_ A Comparative Study of Domain Adaptation and Fine-Tuning Strategies_20250923|Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies]] (83.7% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (81.0% similar)
- [[2025-09-17/Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications_20250917|Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications]] (80.3% similar)
- [[2025-09-23/AutoArabic_ A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks_20250923|AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks]] (80.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Sentiment Analysis|Sentiment Analysis]]
**ğŸ”— Specific Connectable**: [[keywords/Hate Speech Classification|Hate Speech Classification]]
**âš¡ Unique Technical**: [[keywords/Domain Adaptive Pre-training|Domain Adaptive Pre-training]], [[keywords/Task Adaptive Pre-training|Task Adaptive Pre-training]], [[keywords/African Languages Social Media|African Languages Social Media]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.18247v3 Announce Type: replace 
Abstract: Language models built from various sources are the foundation of today's NLP progress. However, for many low-resource languages, the diversity of domains is often limited, more biased to a religious domain, which impacts their performance when evaluated on distant and rapidly evolving domains such as social media. Domain adaptive pre-training (DAPT) and task-adaptive pre-training (TAPT) are popular techniques to reduce this bias through continual pre-training for BERT-based models, but they have not been explored for African multilingual encoders. In this paper, we explore DAPT and TAPT continual pre-training approaches for African languages social media domain. We introduce AfriSocial, a large-scale social media and news domain corpus for continual pre-training on several African languages. Leveraging AfriSocial, we show that DAPT consistently improves performance (from 1% to 30% F1 score) on three subjective tasks: sentiment analysis, multi-label emotion, and hate speech classification, covering 19 languages. Similarly, leveraging TAPT on the data from one task enhances performance on other related tasks. For example, training with unlabeled sentiment data (source) for a fine-grained emotion classification task (target) improves the baseline results by an F1 score ranging from 0.55% to 15.11%. Combining these two methods (i.e. DAPT + TAPT) further improves the overall performance. The data and model resources are available at HuggingFace.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì•„í”„ë¦¬ì¹´ ë‹¤êµ­ì–´ ì¸ì½”ë”ì— ëŒ€í•œ ë„ë©”ì¸ ì ì‘ ì‚¬ì „ í›ˆë ¨(DAPT)ê³¼ ì‘ì—… ì ì‘ ì‚¬ì „ í›ˆë ¨(TAPT) ì ‘ê·¼ ë°©ì‹ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì €ìë“¤ì€ AfriSocialì´ë¼ëŠ” ëŒ€ê·œëª¨ ì†Œì…œ ë¯¸ë””ì–´ ë° ë‰´ìŠ¤ ë„ë©”ì¸ ì½”í¼ìŠ¤ë¥¼ ì†Œê°œí•˜ì—¬ ì—¬ëŸ¬ ì•„í”„ë¦¬ì¹´ ì–¸ì–´ì— ëŒ€í•œ ì§€ì†ì ì¸ ì‚¬ì „ í›ˆë ¨ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ DAPTëŠ” ê°ì • ë¶„ì„, ë©€í‹°ë¼ë²¨ ê°ì •, í˜ì˜¤ ë°œì–¸ ë¶„ë¥˜ì™€ ê°™ì€ ì£¼ê´€ì  ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ 1%ì—ì„œ 30%ê¹Œì§€ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ, TAPTëŠ” í•œ ì‘ì—…ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ê´€ë ¨ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, ë‘ ë°©ë²•ì„ ê²°í•©í•˜ë©´ ì „ì²´ ì„±ëŠ¥ì´ ë”ìš± ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì™€ ëª¨ë¸ ìì›ì€ HuggingFaceì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì•„í”„ë¦¬ì¹´ ë‹¤êµ­ì–´ ì¸ì½”ë”ì— ëŒ€í•œ ë„ë©”ì¸ ì ì‘ ì‚¬ì „ í›ˆë ¨(DAPT)ê³¼ ì‘ì—… ì ì‘ ì‚¬ì „ í›ˆë ¨(TAPT) ì ‘ê·¼ë²•ì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤.
- 2. ëŒ€ê·œëª¨ ì†Œì…œ ë¯¸ë””ì–´ ë° ë‰´ìŠ¤ ë„ë©”ì¸ ì½”í¼ìŠ¤ì¸ AfriSocialì„ ì†Œê°œí•˜ì—¬ ì—¬ëŸ¬ ì•„í”„ë¦¬ì¹´ ì–¸ì–´ì— ëŒ€í•œ ì§€ì†ì ì¸ ì‚¬ì „ í›ˆë ¨ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
- 3. DAPTëŠ” ê°ì • ë¶„ì„, ë‹¤ì¤‘ ë ˆì´ë¸” ê°ì •, í˜ì˜¤ ë°œì–¸ ë¶„ë¥˜ì™€ ê°™ì€ ì£¼ê´€ì  ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ 1%ì—ì„œ 30%ê¹Œì§€ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 4. TAPTëŠ” í•œ ì‘ì—…ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ì˜ˆë¥¼ ë“¤ì–´, ê°ì • ë¶„ë¥˜ ì‘ì—…ì—ì„œ F1 ì ìˆ˜ë¥¼ 0.55%ì—ì„œ 15.11%ê¹Œì§€ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
- 5. DAPTì™€ TAPTë¥¼ ê²°í•©í•˜ë©´ ì „ì²´ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:53:35*