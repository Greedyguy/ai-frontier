---
keywords:
  - Multimodal Learning
  - Radiology Report Generation
  - Large Language Model
  - Segmentation Mask
  - Transformer
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2506.21535
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:05:52.557685",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Radiology Report Generation",
    "Large Language Model",
    "Segmentation Mask",
    "Transformer"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Radiology Report Generation": 0.78,
    "Large Language Model": 0.85,
    "Segmentation Mask": 0.75,
    "Transformer": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is crucial for integrating visual and textual data, enhancing connectivity with related multimodal research.",
        "novelty_score": 0.58,
        "connectivity_score": 0.87,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Radiology Report Generation",
        "canonical": "Radiology Report Generation",
        "aliases": [
          "RRG"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific application area that connects to medical imaging and report automation research.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are a foundational technology, linking to a wide range of NLP and AI research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Segmentation Mask",
        "canonical": "Segmentation Mask",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Segmentation masks are critical in medical imaging, linking to image processing and analysis techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Vision Transformers are a key architecture in computer vision, connecting to transformer-based models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "3D CT",
      "GREEN score",
      "AMOS-MM challenge"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.87,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Radiology Report Generation",
      "resolved_canonical": "Radiology Report Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Segmentation Mask",
      "resolved_canonical": "Segmentation Mask",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Exploring the Design Space of 3D MLLMs for CT Report Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.21535.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2506.21535](https://arxiv.org/abs/2506.21535)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model_20250919|Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model]] (85.7% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (84.3% similar)
- [[2025-09-23/Surgical-MambaLLM_ Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery_20250923|Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery]] (83.6% similar)
- [[2025-09-23/Medical AI Consensus_ A Multi-Agent Framework for Radiology Report Generation and Evaluation_20250923|Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation]] (83.5% similar)
- [[2025-09-23/SD-VLM_ Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models_20250923|SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Radiology Report Generation|Radiology Report Generation]], [[keywords/Segmentation Mask|Segmentation Mask]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.21535v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë°©ì‚¬ì„  ë³´ê³ ì„œ ìƒì„± ìë™í™”ë¥¼ ìœ„í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ì„¤ê³„ ê³µê°„ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤. 3D CT ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì‹œê°ì  ì…ë ¥ í‘œí˜„, í”„ë¡œì í„°, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs), ë¯¸ì„¸ ì¡°ì • ê¸°ë²•ì„ íƒêµ¬í•˜ë©°, ë‘ ê°€ì§€ ì§€ì‹ ê¸°ë°˜ ë³´ê³ ì„œ ì¦ê°• ë°©ë²•ì„ ë„ì…í•˜ì—¬ GREEN ì ìˆ˜ë¥¼ ìµœëŒ€ 10% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë™ì¼í•œ í›ˆë ¨ í”„ë¡œí† ì½œ í•˜ì—ì„œ LLMì˜ í¬ê¸°ì™€ RRG ì„±ëŠ¥ì€ í¬ê²Œ ë…ë¦½ì ì´ë©°, ì›ë˜ ViTê°€ ì‘ì€ ë³¼ë¥¨ í¬ê¸°ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ê²½ìš° ë” í° ë³¼ë¥¨ í¬ê¸°ê°€ í•­ìƒ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, CT ë³¼ë¥¨ê³¼ í•¨ê»˜ ì„¸ë¶„í™” ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” MICCAI 2024 AMOS-MM ì±Œë¦°ì§€ì—ì„œ 2ìœ„ë¥¼ ì°¨ì§€í–ˆìœ¼ë©°, ê´€ë ¨ ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. 3D ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ë°©ì‚¬ì„  ë³´ê³ ì„œ ìƒì„± ìë™í™”ì— ìœ ë§í•œ ë°©ë²•ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.
- 2. 3D CT ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ì‹œê°ì  ì…ë ¥ í‘œí˜„, í”„ë¡œì í„°, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs), ë¯¸ì„¸ ì¡°ì • ê¸°ìˆ ì˜ ì„¤ê³„ ê³µê°„ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ì˜€ë‹¤.
- 3. ë‘ ê°€ì§€ ì§€ì‹ ê¸°ë°˜ ë³´ê³ ì„œ ì¦ê°• ë°©ë²•ì„ ë„ì…í•˜ì—¬ GREEN ì ìˆ˜ë¥¼ ìµœëŒ€ 10% í–¥ìƒì‹œì¼°ìœ¼ë©°, MICCAI 2024 AMOS-MM ì±Œë¦°ì§€ì—ì„œ 2ìœ„ë¥¼ ì°¨ì§€í–ˆë‹¤.
- 4. ë™ì¼í•œ í›ˆë ¨ í”„ë¡œí† ì½œ í•˜ì—ì„œ LLMì˜ í¬ê¸°ê°€ RRG ì„±ëŠ¥ì— í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.
- 5. CT ë³¼ë¥¨ê³¼ í•¨ê»˜ ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.


---

*Generated on 2025-09-24 03:05:52*