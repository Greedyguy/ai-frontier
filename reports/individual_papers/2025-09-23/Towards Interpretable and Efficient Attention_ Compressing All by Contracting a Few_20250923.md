---
keywords:
  - Attention Mechanism
  - Self-Attention
  - Contract-and-Broadcast Self-Attention
  - Transformer
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16875
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:44:02.677060",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "Self-Attention",
    "Contract-and-Broadcast Self-Attention",
    "Transformer"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.88,
    "Self-Attention": 0.85,
    "Contract-and-Broadcast Self-Attention": 0.78,
    "Transformer": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Attention Mechanisms",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention",
          "Attention Layer"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are central to the paper's proposed method and are a key concept in linking related works.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Self-Attention",
        "canonical": "Self-Attention",
        "aliases": [
          "Self Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Self-attention is a specific type of attention mechanism that the paper aims to optimize, making it highly relevant for linking.",
        "novelty_score": 0.5,
        "connectivity_score": 0.87,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Contract-and-Broadcast Self-Attention",
        "canonical": "Contract-and-Broadcast Self-Attention",
        "aliases": [
          "CBSA"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel mechanism introduced by the paper, crucial for understanding its unique contribution.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformers",
        "canonical": "Transformer",
        "aliases": [
          "Transformer Model"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are the foundational architecture for the attention mechanisms discussed, providing broad connectivity.",
        "novelty_score": 0.3,
        "connectivity_score": 0.92,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "optimization objective",
      "quadratic complexity",
      "visual tasks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Attention Mechanisms",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Self-Attention",
      "resolved_canonical": "Self-Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.87,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Contract-and-Broadcast Self-Attention",
      "resolved_canonical": "Contract-and-Broadcast Self-Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.92,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16875.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16875](https://arxiv.org/abs/2509.16875)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (86.3% similar)
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers]] (83.1% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop: A Novel Regularization Method for Transformer Models]] (82.9% similar)
- [[2025-09-18/Stochastic Clock Attention for Aligning Continuous and Ordered Sequences_20250918|Stochastic Clock Attention for Aligning Continuous and Ordered Sequences]] (81.4% similar)
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering]] (80.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Self-Attention|Self-Attention]]
**âš¡ Unique Technical**: [[keywords/Contract-and-Broadcast Self-Attention|Contract-and-Broadcast Self-Attention]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16875v1 Announce Type: new 
Abstract: Attention mechanisms in Transformers have gained significant empirical success. Nonetheless, the optimization objectives underlying their forward pass are still unclear. Additionally, the quadratic complexity of self-attention is increasingly prohibitive. Unlike the prior work on addressing the interpretability or efficiency issue separately, we propose a unified optimization objective to alleviate both issues simultaneously. By unrolling the optimization over the objective, we derive an inherently interpretable and efficient attention mechanism, which compresses all tokens into low-dimensional structures by contracting a few representative tokens and then broadcasting the contractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism can not only scale linearly but also generalize existing attention mechanisms as its special cases. Experiments further demonstrate comparable performance and even superior advantages of CBSA on several visual tasks. Code is available at this https URL.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ëª© ë©”ì»¤ë‹ˆì¦˜ì˜ ìµœì í™” ëª©í‘œë¥¼ ëª…í™•íˆ í•˜ê³ , ìê°€ ì£¼ëª©ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ì™€ ë‹¬ë¦¬, í•´ì„ ê°€ëŠ¥ì„±ê³¼ íš¨ìœ¨ì„± ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ëŠ” í†µí•© ìµœì í™” ëª©í‘œë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë³¸ì§ˆì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•˜ê³  íš¨ìœ¨ì ì¸ ì£¼ëª© ë©”ì»¤ë‹ˆì¦˜ì¸ 'Contract-and-Broadcast Self-Attention (CBSA)'ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. CBSAëŠ” ëª‡ ê°œì˜ ëŒ€í‘œ í† í°ì„ ì••ì¶•í•˜ì—¬ ì €ì°¨ì› êµ¬ì¡°ë¡œ ë§Œë“¤ê³ , ì´ë¥¼ ë‹¤ì‹œ í™•ì¥í•˜ì—¬ ì „ë‹¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•˜ë©°, ì„ í˜• í™•ì¥ì„±ì„ ê°€ì§‘ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CBSAëŠ” ì—¬ëŸ¬ ì‹œê°ì  ê³¼ì œì—ì„œ ê¸°ì¡´ ì£¼ëª© ë©”ì»¤ë‹ˆì¦˜ê³¼ ë¹„êµí•´ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ê²½í—˜ì ìœ¼ë¡œ ì„±ê³µì„ ê±°ë‘ì—ˆìœ¼ë‚˜, ê·¸ ìµœì í™” ëª©í‘œëŠ” ëª…í™•í•˜ì§€ ì•Šë‹¤.
- 2. ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì´ì°¨ ë³µì¡ì„±ì€ ì ì  ë” í° ë¶€ë‹´ì´ ë˜ê³  ìˆë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” í•´ì„ ê°€ëŠ¥ì„±ê³¼ íš¨ìœ¨ì„± ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ê¸° ìœ„í•œ í†µí•© ìµœì í™” ëª©í‘œë¥¼ ì œì•ˆí•œë‹¤.
- 4. ì œì•ˆëœ Contract-and-Broadcast Self-Attention (CBSA) ë©”ì»¤ë‹ˆì¦˜ì€ ì„ í˜• í™•ì¥ì´ ê°€ëŠ¥í•˜ë©° ê¸°ì¡´ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, CBSAëŠ” ì—¬ëŸ¬ ì‹œê°ì  ê³¼ì œì—ì„œ ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 01:44:02*