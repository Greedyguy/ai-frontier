---
keywords:
  - Large Language Model
  - Engineering Problem Solving
  - Mathematical Reasoning
  - Benchmarking
  - Open-ended Modeling
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17677
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:02:34.402322",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Engineering Problem Solving",
    "Mathematical Reasoning",
    "Benchmarking",
    "Open-ended Modeling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Engineering Problem Solving": 0.78,
    "Mathematical Reasoning": 0.72,
    "Benchmarking": 0.7,
    "Open-ended Modeling": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader field of AI and connects with other works on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Engineering Problem Solving",
        "canonical": "Engineering Problem Solving",
        "aliases": [
          "Engineering Tasks"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the application of LLMs in engineering, which is a unique aspect of this paper.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mathematical Reasoning",
        "canonical": "Mathematical Reasoning",
        "aliases": [
          "Math Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding the capabilities and limitations of LLMs in problem-solving.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Benchmarking",
        "canonical": "Benchmarking",
        "aliases": [
          "Evaluation",
          "Testing"
        ],
        "category": "specific_connectable",
        "rationale": "Benchmarking is crucial for assessing LLM performance and comparing models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.8,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Open-ended Modeling",
        "canonical": "Open-ended Modeling",
        "aliases": [
          "Open-ended Scenarios"
        ],
        "category": "unique_technical",
        "rationale": "Represents a complex challenge for LLMs, highlighting the need for advanced reasoning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "experiment",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Engineering Problem Solving",
      "resolved_canonical": "Engineering Problem Solving",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mathematical Reasoning",
      "resolved_canonical": "Mathematical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Benchmarking",
      "resolved_canonical": "Benchmarking",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.8,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Open-ended Modeling",
      "resolved_canonical": "Open-ended Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17677.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17677](https://arxiv.org/abs/2509.17677)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/On LLM-Based Scientific Inductive Reasoning Beyond Equations_20250923|On LLM-Based Scientific Inductive Reasoning Beyond Equations]] (88.5% similar)
- [[2025-09-23/seqBench_ A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs_20250923|seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs]] (88.0% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (86.6% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.5% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (86.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Mathematical Reasoning|Mathematical Reasoning]], [[keywords/Benchmarking|Benchmarking]]
**âš¡ Unique Technical**: [[keywords/Engineering Problem Solving|Engineering Problem Solving]], [[keywords/Open-ended Modeling|Open-ended Modeling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17677v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation -- they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model's robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at https://github.com/EngiBench/EngiBench.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì‹¤ì œ ê³µí•™ ë¬¸ì œëŠ” ë¶ˆí™•ì‹¤ì„±, ë§¥ë½, ê°œë°©í˜• ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤. ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì´ëŸ¬í•œ ë³µì¡ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì˜ ê³µí•™ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ EngiBenchë¼ëŠ” ê³„ì¸µì  ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ì´ˆ ì§€ì‹ ê²€ìƒ‰, ë‹¤ë‹¨ê³„ ë§¥ë½ ì¶”ë¡ , ê°œë°©í˜• ëª¨ë¸ë§ì˜ ì„¸ ê°€ì§€ ë‚œì´ë„ë¡œ êµ¬ì„±ë˜ë©° ë‹¤ì–‘í•œ ê³µí•™ ë¶„ì•¼ë¥¼ í¬ê´„í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ ì´í•´í•˜ê¸° ìœ„í•´ ê° ë¬¸ì œë¥¼ ë³€í˜•í•˜ì—¬ ëª¨ë¸ì˜ ê²¬ê³ ì„±, ë„ë©”ì¸ ì§€ì‹, ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ëª¨ë¸ì€ ë‚œì´ë„ê°€ ë†’ì•„ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì €í•˜ë˜ê³ , ë¬¸ì œì˜ ì•½ê°„ì˜ ë³€í™”ì—ë„ ì·¨ì•½í•˜ë©°, ê³ ë‚œë„ ê³µí•™ ë¬¸ì œì—ì„œëŠ” ì¸ê°„ ì „ë¬¸ê°€ì— ë¹„í•´ í¬ê²Œ ë’¤ì²˜ì§‘ë‹ˆë‹¤. ì´ëŠ” í˜„ì¬ LLMì´ ì‹¤ì œ ê³µí•™ì— í•„ìš”í•œ ê³ ì°¨ì› ì¶”ë¡  ëŠ¥ë ¥ì´ ë¶€ì¡±í•¨ì„ ë³´ì—¬ì£¼ë©°, í–¥í›„ ë” ê¹Šê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ê°–ì¶˜ ëª¨ë¸ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. EngiBenchì˜ ì†ŒìŠ¤ ì½”ë“œì™€ ë°ì´í„°ëŠ” [GitHub ë§í¬](https://github.com/EngiBench/EngiBench)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì‹¤ì œ ì—”ì§€ë‹ˆì–´ë§ ë¬¸ì œ í•´ê²°ì—ëŠ” ë¶ˆí™•ì‹¤ì„±, ë§¥ë½, ê°œë°©í˜• ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ ëŠ¥ë ¥ì´ í•„ìš”í•˜ë‹¤.
- 2. EngiBenchëŠ” LLMsê°€ ì—”ì§€ë‹ˆì–´ë§ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ê³„ì¸µì  ë²¤ì¹˜ë§ˆí¬ë¡œ, ë‹¤ì–‘í•œ ì—”ì§€ë‹ˆì–´ë§ ë¶„ì•¼ë¥¼ í¬í•¨í•˜ê³  ì„¸ ê°€ì§€ ë‚œì´ë„ ìˆ˜ì¤€ì„ ì œê³µí•œë‹¤.
- 3. ê° ë¬¸ì œë¥¼ ë³€í˜•í•˜ì—¬ ëª¨ë¸ì˜ ê°•ê±´ì„±, ë„ë©”ì¸ ì§€ì‹, ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ë³„ë„ë¡œ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ëª¨ë¸ì€ ë‚œì´ë„ê°€ ë†’ì•„ì§ˆìˆ˜ë¡ ë” ë§ì€ ì–´ë ¤ì›€ì„ ê²ªê³ , ë¬¸ì œì˜ ì•½ê°„ì˜ ë³€í™”ì—ë„ ì„±ëŠ¥ì´ ì €í•˜ë˜ë©°, ê³ ë‚œë„ì˜ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…ì—ì„œ ì¸ê°„ ì „ë¬¸ê°€ì— ë¹„í•´ í¬ê²Œ ë’¤ì²˜ì§„ë‹¤.
- 5. í˜„ì¬ LLMsëŠ” ì‹¤ì œ ì—”ì§€ë‹ˆì–´ë§ì— í•„ìš”í•œ ê³ ì°¨ì› ì¶”ë¡  ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë©°, í–¥í›„ ë” ê¹Šê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ê°–ì¶˜ ëª¨ë¸ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-23 23:02:34*