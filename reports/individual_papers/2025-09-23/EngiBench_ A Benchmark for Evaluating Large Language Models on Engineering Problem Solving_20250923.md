---
keywords:
  - Large Language Model
  - Engineering Problem Solving
  - Mathematical Reasoning
  - Benchmarking
  - Open-ended Modeling
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17677
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:02:34.402322",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Engineering Problem Solving",
    "Mathematical Reasoning",
    "Benchmarking",
    "Open-ended Modeling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Engineering Problem Solving": 0.78,
    "Mathematical Reasoning": 0.72,
    "Benchmarking": 0.7,
    "Open-ended Modeling": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader field of AI and connects with other works on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Engineering Problem Solving",
        "canonical": "Engineering Problem Solving",
        "aliases": [
          "Engineering Tasks"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the application of LLMs in engineering, which is a unique aspect of this paper.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mathematical Reasoning",
        "canonical": "Mathematical Reasoning",
        "aliases": [
          "Math Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding the capabilities and limitations of LLMs in problem-solving.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Benchmarking",
        "canonical": "Benchmarking",
        "aliases": [
          "Evaluation",
          "Testing"
        ],
        "category": "specific_connectable",
        "rationale": "Benchmarking is crucial for assessing LLM performance and comparing models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.8,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Open-ended Modeling",
        "canonical": "Open-ended Modeling",
        "aliases": [
          "Open-ended Scenarios"
        ],
        "category": "unique_technical",
        "rationale": "Represents a complex challenge for LLMs, highlighting the need for advanced reasoning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "experiment",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Engineering Problem Solving",
      "resolved_canonical": "Engineering Problem Solving",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mathematical Reasoning",
      "resolved_canonical": "Mathematical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Benchmarking",
      "resolved_canonical": "Benchmarking",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.8,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Open-ended Modeling",
      "resolved_canonical": "Open-ended Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17677.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17677](https://arxiv.org/abs/2509.17677)

## 🔗 유사한 논문
- [[2025-09-23/On LLM-Based Scientific Inductive Reasoning Beyond Equations_20250923|On LLM-Based Scientific Inductive Reasoning Beyond Equations]] (88.5% similar)
- [[2025-09-23/seqBench_ A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs_20250923|seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs]] (88.0% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (86.6% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.5% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (86.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Mathematical Reasoning|Mathematical Reasoning]], [[keywords/Benchmarking|Benchmarking]]
**⚡ Unique Technical**: [[keywords/Engineering Problem Solving|Engineering Problem Solving]], [[keywords/Open-ended Modeling|Open-ended Modeling]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17677v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation -- they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model's robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at https://github.com/EngiBench/EngiBench.

## 📝 요약

대형 언어 모델(LLM)은 수학적 추론에서 강력한 성능을 보이지만, 실제 공학 문제는 불확실성, 맥락, 개방형 시나리오를 다루어야 합니다. 기존 벤치마크는 이러한 복잡성을 포착하지 못합니다. 우리는 LLM의 공학 문제 해결 능력을 평가하기 위해 EngiBench라는 계층적 벤치마크를 소개합니다. 이는 기초 지식 검색, 다단계 맥락 추론, 개방형 모델링의 세 가지 난이도로 구성되며 다양한 공학 분야를 포괄합니다. 모델의 성능을 심층적으로 이해하기 위해 각 문제를 변형하여 모델의 견고성, 도메인 지식, 수학적 추론 능력을 평가합니다. 실험 결과, 모델은 난이도가 높아질수록 성능이 저하되고, 문제의 약간의 변화에도 취약하며, 고난도 공학 문제에서는 인간 전문가에 비해 크게 뒤처집니다. 이는 현재 LLM이 실제 공학에 필요한 고차원 추론 능력이 부족함을 보여주며, 향후 더 깊고 신뢰할 수 있는 문제 해결 능력을 갖춘 모델의 필요성을 강조합니다. EngiBench의 소스 코드와 데이터는 [GitHub 링크](https://github.com/EngiBench/EngiBench)에서 확인할 수 있습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 수학적 추론에서 강력한 성능을 보이지만, 실제 엔지니어링 문제 해결에는 불확실성, 맥락, 개방형 시나리오 처리 능력이 필요하다.
- 2. EngiBench는 LLMs가 엔지니어링 문제를 해결하는 능력을 평가하기 위해 설계된 계층적 벤치마크로, 다양한 엔지니어링 분야를 포함하고 세 가지 난이도 수준을 제공한다.
- 3. 각 문제를 변형하여 모델의 강건성, 도메인 지식, 수학적 추론 능력을 별도로 평가할 수 있도록 하였다.
- 4. 실험 결과, 모델은 난이도가 높아질수록 더 많은 어려움을 겪고, 문제의 약간의 변화에도 성능이 저하되며, 고난도의 엔지니어링 작업에서 인간 전문가에 비해 크게 뒤처진다.
- 5. 현재 LLMs는 실제 엔지니어링에 필요한 고차원 추론 능력이 부족하며, 향후 더 깊고 신뢰할 수 있는 문제 해결 능력을 갖춘 모델의 필요성을 강조한다.


---

*Generated on 2025-09-23 23:02:34*