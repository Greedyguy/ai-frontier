---
keywords:
  - Large Language Model
  - Scalar Construct Measurement
  - Pairwise Comparisons
  - Token Probability
  - Finetuning
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.03116
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:10:47.061067",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Scalar Construct Measurement",
    "Pairwise Comparisons",
    "Token Probability",
    "Finetuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Scalar Construct Measurement": 0.7,
    "Pairwise Comparisons": 0.75,
    "Token Probability": 0.68,
    "Finetuning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology, connecting to broader discussions on language model applications.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Scalar Construct Measurement",
        "canonical": "Scalar Construct Measurement",
        "aliases": [
          "Scalar Constructs",
          "Measurement of Scalar Constructs"
        ],
        "category": "unique_technical",
        "rationale": "A unique focus of the paper, offering potential for new research connections in social science measurement.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Pairwise Comparisons",
        "canonical": "Pairwise Comparisons",
        "aliases": [
          "Pairwise Comparison"
        ],
        "category": "specific_connectable",
        "rationale": "A key method evaluated in the paper, relevant to discussions on comparative analysis techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      },
      {
        "surface": "Token Probability",
        "canonical": "Token Probability",
        "aliases": [
          "Token-Probability"
        ],
        "category": "unique_technical",
        "rationale": "An innovative approach in the paper, relevant for discussions on probabilistic scoring in LLMs.",
        "novelty_score": 0.65,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.68
      },
      {
        "surface": "Finetuning",
        "canonical": "Finetuning",
        "aliases": [
          "Fine-tuning",
          "Model Finetuning"
        ],
        "category": "specific_connectable",
        "rationale": "A significant technique discussed, connecting to broader themes in model optimization.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "public speech",
      "numerical outputs",
      "training pairs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Scalar Construct Measurement",
      "resolved_canonical": "Scalar Construct Measurement",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Pairwise Comparisons",
      "resolved_canonical": "Pairwise Comparisons",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Token Probability",
      "resolved_canonical": "Token Probability",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.68
      }
    },
    {
      "candidate_surface": "Finetuning",
      "resolved_canonical": "Finetuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Measuring Scalar Constructs in Social Science with LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.03116.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.03116](https://arxiv.org/abs/2509.03116)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (89.6% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (87.7% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (87.2% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (87.1% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (87.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Pairwise Comparisons|Pairwise Comparisons]], [[keywords/Finetuning|Finetuning]]
**âš¡ Unique Technical**: [[keywords/Scalar Construct Measurement|Scalar Construct Measurement]], [[keywords/Token Probability|Token Probability]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.03116v2 Announce Type: replace 
Abstract: Many constructs that characterize language, like its complexity or emotionality, have a naturally continuous semantic structure; a public speech is not just "simple" or "complex," but exists on a continuum between extremes. Although large language models (LLMs) are an attractive tool for measuring scalar constructs, their idiosyncratic treatment of numerical outputs raises questions of how to best apply them. We address these questions with a comprehensive evaluation of LLM-based approaches to scalar construct measurement in social science. Using multiple datasets sourced from the political science literature, we evaluate four approaches: unweighted direct pointwise scoring, aggregation of pairwise comparisons, token-probability-weighted pointwise scoring, and finetuning. Our study finds that pairwise comparisons made by LLMs produce better measurements than simply prompting the LLM to directly output the scores, which suffers from bunching around arbitrary numbers. However, taking the weighted mean over the token probability of scores further improves the measurements over the two previous approaches. Finally, finetuning smaller models with as few as 1,000 training pairs can match or exceed the performance of prompted LLMs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ì˜ ë³µì¡ì„±ì´ë‚˜ ê°ì • í‘œí˜„ê³¼ ê°™ì€ ì—°ì†ì ì¸ ì˜ë¯¸ êµ¬ì¡°ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ìˆì–´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í™œìš©ì„ í‰ê°€í•©ë‹ˆë‹¤. ì •ì¹˜í•™ ë¬¸í—Œì—ì„œ ì–»ì€ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë„¤ ê°€ì§€ ì ‘ê·¼ë²•ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤: ì§ì ‘ ì ìˆ˜ ë¶€ì—¬, ìŒ ë¹„êµ ì§‘ê³„, í† í° í™•ë¥  ê°€ì¤‘ ì ìˆ˜ ë¶€ì—¬, ê·¸ë¦¬ê³  ë¯¸ì„¸ ì¡°ì •ì…ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, LLMì„ ì‚¬ìš©í•œ ìŒ ë¹„êµê°€ ì§ì ‘ ì ìˆ˜ ë¶€ì—¬ë³´ë‹¤ ë” ë‚˜ì€ ì¸¡ì •ê°’ì„ ì œê³µí•˜ë©°, í† í° í™•ë¥  ê°€ì¤‘ í‰ê· ì„ ì‚¬ìš©í•˜ë©´ ì¸¡ì • ì •í™•ë„ê°€ ë”ìš± í–¥ìƒë©ë‹ˆë‹¤. ë˜í•œ, 1,000ê°œì˜ í›ˆë ¨ ìŒë§Œìœ¼ë¡œë„ ì‘ì€ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ LLMì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ì˜ ë³µì¡ì„±ì´ë‚˜ ê°ì • í‘œí˜„ê³¼ ê°™ì€ ì–¸ì–´ì  íŠ¹ì„±ì€ ì—°ì†ì ì¸ ì˜ë¯¸ êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ì´ëŠ” ë‹¨ìˆœíˆ "ë‹¨ìˆœ"í•˜ê±°ë‚˜ "ë³µì¡"í•œ ê²ƒì´ ì•„ë‹ˆë¼ ê·¹ë‹¨ ì‚¬ì´ì˜ ì—°ì†ì²´ì— ì¡´ì¬í•œë‹¤.
- 2. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì´ëŸ¬í•œ ìŠ¤ì¹¼ë¼ êµ¬ì¡°ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ìœ ìš©í•˜ì§€ë§Œ, ìˆ˜ì¹˜ ì¶œë ¥ì— ëŒ€í•œ íŠ¹ì´í•œ ì²˜ë¦¬ ë°©ì‹ ë•Œë¬¸ì— ìµœì ì˜ ì ìš© ë°©ë²•ì— ëŒ€í•œ ì˜ë¬¸ì´ ì œê¸°ëœë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼, LLMì„ í™œìš©í•œ ìŒë³„ ë¹„êµ ë°©ì‹ì´ ì§ì ‘ ì ìˆ˜ë¥¼ ì¶œë ¥í•˜ëŠ” ë°©ì‹ë³´ë‹¤ ë” ë‚˜ì€ ì¸¡ì • ê²°ê³¼ë¥¼ ì œê³µí•œë‹¤.
- 4. ì ìˆ˜ì˜ í† í° í™•ë¥ ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œ í‰ê· ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ì´ì „ì˜ ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ë³´ë‹¤ ì¸¡ì • ê²°ê³¼ë¥¼ ë”ìš± ê°œì„ í•œë‹¤.
- 5. ì†Œê·œëª¨ ëª¨ë¸ì„ 1,000ê°œì˜ í›ˆë ¨ ìŒìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ LLMì˜ ì„±ëŠ¥ì„ ë§ì¶”ê±°ë‚˜ ì´ˆê³¼í•  ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-24 04:10:47*