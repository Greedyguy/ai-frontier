---
keywords:
  - Large Language Model
  - Sparse Autoencoder
  - Group-SAE
  - AMAD
  - Pythia Model Family
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2410.21508
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:41:09.209350",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Sparse Autoencoder",
    "Group-SAE",
    "AMAD",
    "Pythia Model Family"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Sparse Autoencoder": 0.88,
    "Group-SAE": 0.85,
    "AMAD": 0.8,
    "Pythia Model Family": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on improving training efficiency for these models.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Sparse Autoencoders",
        "canonical": "Sparse Autoencoder",
        "aliases": [
          "SAE",
          "Sparse Autoencoders"
        ],
        "category": "unique_technical",
        "rationale": "Key method discussed for improving layer representation understanding.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Group-SAE",
        "canonical": "Group-SAE",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Novel strategy introduced in the paper for efficient training.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Average Maximum Angular Distance",
        "canonical": "AMAD",
        "aliases": [
          "Average Maximum Angular Distance"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new metric for optimizing layer grouping.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.8
      },
      {
        "surface": "Pythia family",
        "canonical": "Pythia Model Family",
        "aliases": [
          "Pythia family"
        ],
        "category": "specific_connectable",
        "rationale": "Specific model family used in experiments, relevant for contextual linking.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Sparse Autoencoders",
      "resolved_canonical": "Sparse Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Group-SAE",
      "resolved_canonical": "Group-SAE",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Average Maximum Angular Distance",
      "resolved_canonical": "AMAD",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Pythia family",
      "resolved_canonical": "Pythia Model Family",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2410.21508.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2410.21508](https://arxiv.org/abs/2410.21508)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (82.0% similar)
- [[2025-09-23/LLMs as Layout Designers_ A Spatial Reasoning Perspective_20250923|LLMs as Layout Designers: A Spatial Reasoning Perspective]] (80.9% similar)
- [[2025-09-23/GALLa_ Graph Aligned Large Language Models for Improved Source Code Understanding_20250923|GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding]] (80.6% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (80.4% similar)
- [[2025-09-23/Probabilistic Token Alignment for Large Language Model Fusion_20250923|Probabilistic Token Alignment for Large Language Model Fusion]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Pythia Model Family|Pythia Model Family]]
**âš¡ Unique Technical**: [[keywords/Sparse Autoencoder|Sparse Autoencoder]], [[keywords/Group-SAE|Group-SAE]], [[keywords/AMAD|AMAD]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.21508v2 Announce Type: replace-cross 
Abstract: SAEs have recently been employed as a promising unsupervised approach for understanding the representations of layers of Large Language Models (LLMs). However, with the growth in model size and complexity, training SAEs is computationally intensive, as typically one SAE is trained for each model layer. To address such limitation, we propose \textit{Group-SAE}, a novel strategy to train SAEs. Our method considers the similarity of the residual stream representations between contiguous layers to group similar layers and train a single SAE per group. To balance the trade-off between efficiency and performance, we further introduce \textit{AMAD} (Average Maximum Angular Distance), an empirical metric that guides the selection of an optimal number of groups based on representational similarity across layers. Experiments on models from the Pythia family show that our approach significantly accelerates training with minimal impact on reconstruction quality and comparable downstream task performance and interpretability over baseline SAEs trained layer by layer. This method provides an efficient and scalable strategy for training SAEs in modern LLMs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê° ì¸µì˜ í‘œí˜„ì„ ì´í•´í•˜ê¸° ìœ„í•œ ë¹„ì§€ë„ í•™ìŠµ ì ‘ê·¼ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” SAEì˜ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ \textit{Group-SAE}ë¼ëŠ” ìƒˆë¡œìš´ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ê° ì¸µë§ˆë‹¤ SAEë¥¼ í›ˆë ¨í•´ì•¼ í–ˆì§€ë§Œ, ì œì•ˆëœ ë°©ë²•ì€ ì¸ì ‘í•œ ì¸µ ê°„ì˜ ì”ì—¬ ìŠ¤íŠ¸ë¦¼ í‘œí˜„ì˜ ìœ ì‚¬ì„±ì„ ê³ ë ¤í•˜ì—¬ ìœ ì‚¬í•œ ì¸µì„ ê·¸ë£¹í™”í•˜ê³  ê·¸ë£¹ë‹¹ í•˜ë‚˜ì˜ SAEë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. ë˜í•œ, \textit{AMAD}ë¼ëŠ” ê²½í—˜ì  ì§€í‘œë¥¼ ë„ì…í•˜ì—¬ ì¸µ ê°„ í‘œí˜„ ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ê·¸ë£¹ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. Pythia ëª¨ë¸ ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì€ í›ˆë ¨ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ì¬êµ¬ì„± í’ˆì§ˆê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥ ë° í•´ì„ ê°€ëŠ¥ì„±ì—ì„œ ê¸°ì¡´ SAEì™€ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ í˜„ëŒ€ LLMì—ì„œ SAE í›ˆë ¨ì˜ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SAEsëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì¸µ í‘œí˜„ì„ ì´í•´í•˜ê¸° ìœ„í•œ ìœ ë§í•œ ë¹„ì§€ë„ í•™ìŠµ ì ‘ê·¼ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ëª¨ë¸ í¬ê¸°ì™€ ë³µì¡ì„±ì´ ì¦ê°€í•¨ì— ë”°ë¼ ê° ì¸µë§ˆë‹¤ SAEë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ê³„ì‚°ì ìœ¼ë¡œ ë¶€ë‹´ì´ í½ë‹ˆë‹¤.
- 3. Group-SAEëŠ” ìœ ì‚¬í•œ ì¸µì„ ê·¸ë£¹í™”í•˜ì—¬ ê·¸ë£¹ë‹¹ í•˜ë‚˜ì˜ SAEë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. AMAD(í‰ê·  ìµœëŒ€ ê°ë„ ê±°ë¦¬)ëŠ” ì¸µ ê°„ í‘œí˜„ ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ê·¸ë£¹ ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê²½í—˜ì  ì§€í‘œì…ë‹ˆë‹¤.
- 5. Pythia ëª¨ë¸êµ°ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ í›ˆë ¨ ì†ë„ë¥¼ í¬ê²Œ ë†’ì´ë©´ì„œë„ ì¬êµ¬ì„± í’ˆì§ˆê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥ì— ìµœì†Œí•œì˜ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:41:09*