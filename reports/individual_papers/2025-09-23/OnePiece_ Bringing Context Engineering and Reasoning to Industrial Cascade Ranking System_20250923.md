---
keywords:
  - Transformer
  - Large Language Model
  - Context Engineering
  - Multi-step Reasoning
  - Progressive Multi-task Training
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.18091
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:20:48.476585",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Large Language Model",
    "Context Engineering",
    "Multi-step Reasoning",
    "Progressive Multi-task Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Large Language Model": 0.8,
    "Context Engineering": 0.78,
    "Multi-step Reasoning": 0.77,
    "Progressive Multi-task Training": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are foundational to the proposed framework and connect with existing knowledge on neural architectures.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's theme and link to broader discussions on model architectures.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Context Engineering",
        "canonical": "Context Engineering",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel mechanism proposed in the paper, crucial for understanding the framework's innovation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multi-step Reasoning",
        "canonical": "Multi-step Reasoning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is pivotal to the paper's methodology, offering a new approach to model refinement.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Progressive Multi-task Training",
        "canonical": "Progressive Multi-task Training",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduced as a key innovation, it enhances model training and supervision, linking to advanced training techniques.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "system"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Context Engineering",
      "resolved_canonical": "Context Engineering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multi-step Reasoning",
      "resolved_canonical": "Multi-step Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Progressive Multi-task Training",
      "resolved_canonical": "Progressive Multi-task Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18091.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.18091](https://arxiv.org/abs/2509.18091)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/WebCoT_ Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback_20250919|WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback]] (81.3% similar)
- [[2025-09-23/MSCoRe_ A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents_20250923|MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents]] (80.5% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (80.2% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (80.2% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (80.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Context Engineering|Context Engineering]], [[keywords/Multi-step Reasoning|Multi-step Reasoning]], [[keywords/Progressive Multi-task Training|Progressive Multi-task Training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18091v1 Announce Type: cross 
Abstract: Despite the growing interest in replicating the scaled success of large language models (LLMs) in industrial search and recommender systems, most existing industrial efforts remain limited to transplanting Transformer architectures, which bring only incremental improvements over strong Deep Learning Recommendation Models (DLRMs). From a first principle perspective, the breakthroughs of LLMs stem not only from their architectures but also from two complementary mechanisms: context engineering, which enriches raw input queries with contextual cues to better elicit model capabilities, and multi-step reasoning, which iteratively refines model outputs through intermediate reasoning paths. However, these two mechanisms and their potential to unlock substantial improvements remain largely underexplored in industrial ranking systems.
  In this paper, we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines. OnePiece is built on a pure Transformer backbone and further introduces three key innovations: (1) structured context engineering, which augments interaction history with preference and scenario signals and unifies them into a structured tokenized input sequence for both retrieval and ranking; (2) block-wise latent reasoning, which equips the model with multi-step refinement of representations and scales reasoning bandwidth via block size; (3) progressive multi-task training, which leverages user feedback chains to effectively supervise reasoning steps during training. OnePiece has been deployed in the main personalized search scenario of Shopee and achieves consistent online gains across different key business metrics, including over $+2\%$ GMV/UU and a $+2.90\%$ increase in advertising revenue.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ê³µì„ ì‚°ì—… ê²€ìƒ‰ ë° ì¶”ì²œ ì‹œìŠ¤í…œì— ì ìš©í•˜ë ¤ëŠ” ì‹œë„ì—ì„œ ê¸°ì¡´ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ OnePieceë¼ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” LLM ìŠ¤íƒ€ì¼ì˜ ë¬¸ë§¥ ì—”ì§€ë‹ˆì–´ë§ê³¼ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì„ ì‚°ì—…ìš© ë­í‚¹ ì‹œìŠ¤í…œì— í†µí•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. OnePieceëŠ” Transformer ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©°, ì„¸ ê°€ì§€ í˜ì‹ ì„ ë„ì…í•©ë‹ˆë‹¤: (1) êµ¬ì¡°í™”ëœ ë¬¸ë§¥ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ìƒí˜¸ì‘ìš© ì´ë ¥ì„ ì„ í˜¸ë„ ë° ì‹œë‚˜ë¦¬ì˜¤ ì‹ í˜¸ì™€ í•¨ê»˜ êµ¬ì¡°í™”ëœ ì…ë ¥ ì‹œí€€ìŠ¤ë¡œ í†µí•©, (2) ë¸”ë¡ ë‹¨ìœ„ì˜ ì ì¬ ì¶”ë¡ ìœ¼ë¡œ í‘œí˜„ì˜ ë‹¤ë‹¨ê³„ ì •ì œë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  ì¶”ë¡  ëŒ€ì—­í­ì„ í™•ì¥, (3) ì ì§„ì  ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµìœ¼ë¡œ ì‚¬ìš©ì í”¼ë“œë°±ì„ í™œìš©í•´ í›ˆë ¨ ì¤‘ ì¶”ë¡  ë‹¨ê³„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê°ë…í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” Shopeeì˜ ê°œì¸í™”ëœ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ì— ì ìš©ë˜ì–´ ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œì—ì„œ ì¼ê´€ëœ ì„±ê³¼ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ê³µì„ ì‚°ì—… ê²€ìƒ‰ ë° ì¶”ì²œ ì‹œìŠ¤í…œì— ì ìš©í•˜ë ¤ëŠ” ì‹œë„ê°€ ì¦ê°€í•˜ê³  ìˆìœ¼ë‚˜, ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ì‚°ì—…ì  ë…¸ë ¥ì€ Transformer ì•„í‚¤í…ì²˜ì˜ ì´ì‹ì— êµ­í•œë˜ì–´ ìˆë‹¤.
- 2. LLMì˜ í˜ì‹ ì€ ì•„í‚¤í…ì²˜ë¿ë§Œ ì•„ë‹ˆë¼ ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§ê³¼ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì´ë¼ëŠ” ë‘ ê°€ì§€ ë³´ì™„ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ë¹„ë¡¯ëœë‹¤.
- 3. OnePieceëŠ” LLM ìŠ¤íƒ€ì¼ì˜ ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§ê³¼ ì¶”ë¡ ì„ ì‚°ì—…ì  ìˆœìœ„ ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ë° ìˆœìœ„ ëª¨ë¸ì— í†µí•©í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. OnePieceëŠ” êµ¬ì¡°í™”ëœ ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§, ë¸”ë¡ ë‹¨ìœ„ ì ì¬ ì¶”ë¡ , ì ì§„ì  ë‹¤ì¤‘ ì‘ì—… í›ˆë ¨ì´ë¼ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” í˜ì‹ ì„ ë„ì…í•œë‹¤.
- 5. OnePieceëŠ” Shopeeì˜ ê°œì¸í™”ëœ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ì— ë°°ì¹˜ë˜ì–´ ë‹¤ì–‘í•œ ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œì—ì„œ ì¼ê´€ëœ ì˜¨ë¼ì¸ ì„±ê³¼ í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤.


---

*Generated on 2025-09-24 00:20:48*