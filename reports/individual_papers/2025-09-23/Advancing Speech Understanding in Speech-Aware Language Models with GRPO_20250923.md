---
keywords:
  - Speech-Aware Large Language Models
  - Group Relative Policy Optimization
  - Spoken Question Answering
  - Automatic Speech Translation
  - BLEU
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16990
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:40:20.931624",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Speech-Aware Large Language Models",
    "Group Relative Policy Optimization",
    "Spoken Question Answering",
    "Automatic Speech Translation",
    "BLEU"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Speech-Aware Large Language Models": 0.78,
    "Group Relative Policy Optimization": 0.8,
    "Spoken Question Answering": 0.75,
    "Automatic Speech Translation": 0.77,
    "BLEU": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Speech-Aware Large Language Models",
        "canonical": "Speech-Aware Large Language Models",
        "aliases": [
          "SALLMs"
        ],
        "category": "unique_technical",
        "rationale": "This represents a specialized application of language models in speech understanding, offering unique insights into model training and optimization.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "GRPO is a novel optimization technique that enhances model training efficiency, relevant for linking to optimization strategies.",
        "novelty_score": 0.88,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spoken Question Answering",
        "canonical": "Spoken Question Answering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This task is a specific application of language models in speech, relevant for connecting to similar tasks in NLP.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      },
      {
        "surface": "Automatic Speech Translation",
        "canonical": "Automatic Speech Translation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This task bridges speech and language processing, facilitating connections to translation and speech technologies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "BLEU",
        "canonical": "BLEU",
        "aliases": [
          "Bilingual Evaluation Understudy"
        ],
        "category": "specific_connectable",
        "rationale": "BLEU is a widely used metric for evaluating translation tasks, relevant for linking to evaluation methodologies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "task",
      "model",
      "approach",
      "research"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Speech-Aware Large Language Models",
      "resolved_canonical": "Speech-Aware Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.88,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spoken Question Answering",
      "resolved_canonical": "Spoken Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Automatic Speech Translation",
      "resolved_canonical": "Automatic Speech Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "BLEU",
      "resolved_canonical": "BLEU",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Advancing Speech Understanding in Speech-Aware Language Models with GRPO

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16990.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16990](https://arxiv.org/abs/2509.16990)

## 🔗 유사한 논문
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (82.9% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (81.7% similar)
- [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (80.9% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (80.6% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (80.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Spoken Question Answering|Spoken Question Answering]], [[keywords/Automatic Speech Translation|Automatic Speech Translation]], [[keywords/BLEU|BLEU]]
**⚡ Unique Technical**: [[keywords/Speech-Aware Large Language Models|Speech-Aware Large Language Models]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16990v1 Announce Type: cross 
Abstract: In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based method for training Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks, such as Spoken Question Answering and Automatic Speech Translation. SALLMs have proven highly effective for speech understanding tasks. GRPO has recently gained traction for its efficiency in training LLMs, and prior work has explored its application to SALLMs, primarily in multiple-choice tasks. Building on this, we focus on open-format tasks that better reflect the generative abilities of the models. Our approach leverages GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate empirically that it surpasses standard SFT across several key metrics. Finally, we explore the potential of incorporating off-policy samples within GRPO for these tasks, highlighting avenues for further improvement and further research.

## 📝 요약

이 논문에서는 음성 인식 작업을 위한 대규모 언어 모델(SALLMs)을 훈련하기 위해 GRPO(Group Relative Policy Optimization) 기반 방법을 제안합니다. GRPO는 LLMs 훈련의 효율성으로 주목받고 있으며, 기존 연구에서는 주로 선택형 과제에 적용되었습니다. 본 연구는 생성 능력을 잘 반영하는 개방형 과제에 초점을 맞추어 GRPO와 BLEU를 보상 신호로 활용하여 SALLMs를 최적화합니다. 실험 결과, 제안된 방법이 여러 주요 지표에서 기존 SFT를 능가함을 입증하였으며, GRPO 내 오프 정책 샘플의 잠재력을 탐구하여 추가 연구 가능성을 제시합니다.

## 🎯 주요 포인트

- 1. 본 논문에서는 GRPO 기반 방법을 사용하여 SALLMs를 개방형 음성 이해 작업에 훈련시키는 방법을 소개합니다.
- 2. SALLMs는 음성 이해 작업에서 높은 효과를 발휘하며, GRPO는 LLMs 훈련의 효율성으로 주목받고 있습니다.
- 3. 본 연구는 BLEU를 보상 신호로 활용하여 SALLMs를 최적화하며, 표준 SFT를 여러 주요 지표에서 능가함을 실증적으로 보여줍니다.
- 4. GRPO 내에서 오프-정책 샘플을 통합하는 가능성을 탐구하여 추가적인 개선 및 연구의 가능성을 제시합니다.


---

*Generated on 2025-09-23 23:40:20*