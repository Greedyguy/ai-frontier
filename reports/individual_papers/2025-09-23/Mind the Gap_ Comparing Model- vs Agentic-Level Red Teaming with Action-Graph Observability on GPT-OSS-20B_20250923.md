---
keywords:
  - Agentic AI Systems
  - Red Teaming
  - AgentSeer
  - Tool-Calling Contexts
  - Iterative Attacks
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17259
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:58:00.797592",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Agentic AI Systems",
    "Red Teaming",
    "AgentSeer",
    "Tool-Calling Contexts",
    "Iterative Attacks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Agentic AI Systems": 0.78,
    "Red Teaming": 0.8,
    "AgentSeer": 0.77,
    "Tool-Calling Contexts": 0.75,
    "Iterative Attacks": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Agentic AI Systems",
        "canonical": "Agentic AI Systems",
        "aliases": [
          "Agentic Systems",
          "Agentic AI"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific type of AI system with unique vulnerabilities, crucial for understanding agentic-level risks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Red Teaming",
        "canonical": "Red Teaming",
        "aliases": [
          "Adversarial Testing",
          "Security Testing"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to security and vulnerability assessment practices, relevant for understanding AI system robustness.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "AgentSeer",
        "canonical": "AgentSeer",
        "aliases": [
          "Agent Observability Framework"
        ],
        "category": "unique_technical",
        "rationale": "A specific framework used for deconstructing agentic systems, essential for detailed analysis of AI vulnerabilities.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.77
      },
      {
        "surface": "Tool-Calling Contexts",
        "canonical": "Tool-Calling Contexts",
        "aliases": [
          "Tool Interaction Contexts"
        ],
        "category": "unique_technical",
        "rationale": "Identifies a specific context where agentic vulnerabilities are more pronounced, aiding in targeted security analysis.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Iterative Attacks",
        "canonical": "Iterative Attacks",
        "aliases": [
          "Repeated Attacks",
          "Cyclic Attacks"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a method of attack that is crucial for understanding the dynamics of agentic vulnerabilities.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "model level",
      "vulnerability profiles",
      "standalone model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Agentic AI Systems",
      "resolved_canonical": "Agentic AI Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Red Teaming",
      "resolved_canonical": "Red Teaming",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "AgentSeer",
      "resolved_canonical": "AgentSeer",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Tool-Calling Contexts",
      "resolved_canonical": "Tool-Calling Contexts",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Iterative Attacks",
      "resolved_canonical": "Iterative Attacks",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17259.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17259](https://arxiv.org/abs/2509.17259)

## 🔗 유사한 논문
- [[2025-09-18/The Cybersecurity of a Humanoid Robot_20250918|The Cybersecurity of a Humanoid Robot]] (83.3% similar)
- [[2025-09-22/Activation Space Interventions Can Be Transferred Between Large Language Models_20250922|Activation Space Interventions Can Be Transferred Between Large Language Models]] (82.4% similar)
- [[2025-09-19/Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems_20250919|Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems]] (82.1% similar)
- [[2025-09-22/Understanding AI Evaluation Patterns_ How Different GPT Models Assess Vision-Language Descriptions_20250922|Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions]] (82.0% similar)
- [[2025-09-18/Position_ AI Safety Must Embrace an Antifragile Perspective_20250918|Position: AI Safety Must Embrace an Antifragile Perspective]] (81.9% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Red Teaming|Red Teaming]], [[keywords/Iterative Attacks|Iterative Attacks]]
**⚡ Unique Technical**: [[keywords/Agentic AI Systems|Agentic AI Systems]], [[keywords/AgentSeer|AgentSeer]], [[keywords/Tool-Calling Contexts|Tool-Calling Contexts]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17259v1 Announce Type: new 
Abstract: As the industry increasingly adopts agentic AI systems, understanding their unique vulnerabilities becomes critical. Prior research suggests that security flaws at the model level do not fully capture the risks present in agentic deployments, where models interact with tools and external environments. This paper investigates this gap by conducting a comparative red teaming analysis of GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability framework AgentSeer to deconstruct agentic systems into granular actions and components, we apply iterative red teaming attacks with harmful objectives from HarmBench at two distinct levels: the standalone model and the model operating within an agentic loop. Our evaluation reveals fundamental differences between model level and agentic level vulnerability profiles. Critically, we discover the existence of agentic-only vulnerabilities, attack vectors that emerge exclusively within agentic execution contexts while remaining inert against standalone models. Agentic level iterative attacks successfully compromise objectives that completely failed at the model level, with tool-calling contexts showing 24\% higher vulnerability than non-tool contexts. Conversely, certain model-specific exploits work exclusively at the model level and fail when transferred to agentic contexts, demonstrating that standalone model vulnerabilities do not always generalize to deployed systems.

## 📝 요약

이 논문은 에이전트 기반 AI 시스템의 고유한 취약성을 이해하는 것이 중요하다는 점을 강조합니다. 기존 연구에서는 모델 수준의 보안 결함이 에이전트 배포에서의 위험을 완전히 설명하지 못한다고 지적합니다. 이를 해결하기 위해, 저자들은 200억 개의 매개변수를 가진 오픈 소스 모델인 GPT-OSS-20B를 대상으로 비교적 레드 팀 분석을 수행했습니다. AgentSeer라는 관측 프레임워크를 사용하여 에이전트 시스템을 세분화된 행동과 구성 요소로 분해하고, HarmBench의 유해한 목표를 기반으로 반복적인 레드 팀 공격을 모델 단독 실행과 에이전트 루프 내 실행 두 가지 수준에서 적용했습니다. 평가 결과, 모델 수준과 에이전트 수준의 취약성 프로파일 간에 근본적인 차이가 있음을 발견했습니다. 특히, 에이전트 실행 환경에서만 나타나는 '에이전트 전용 취약성'이 존재하며, 이는 독립 실행 모델에서는 드러나지 않는 공격 벡터입니다. 도구 호출 환경에서는 비도구 환경보다 24% 더 높은 취약성을 보였으며, 일부 모델 전용 익스플로잇은 모델 수준에서만 작동하고 에이전트 환경에서는 실패했습니다. 이는 독립 실행 모델의 취약성이 배포 시스템에 항상 일반화되지 않음을 보여줍니다.

## 🎯 주요 포인트

- 1. 에이전틱 AI 시스템의 고유한 취약성을 이해하는 것이 중요해지고 있습니다.
- 2. 모델 수준의 보안 결함은 에이전틱 배포에서의 위험을 완전히 포착하지 못합니다.
- 3. 에이전틱 시스템의 취약성은 모델 수준과 에이전틱 수준에서 근본적으로 다릅니다.
- 4. 에이전틱 실행 환경에서만 나타나는 고유한 취약성이 존재합니다.
- 5. 도구 호출 문맥에서는 비도구 문맥보다 24% 더 높은 취약성을 보입니다.


---

*Generated on 2025-09-23 22:58:00*