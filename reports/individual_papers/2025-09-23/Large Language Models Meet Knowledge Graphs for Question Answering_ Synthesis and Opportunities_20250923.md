---
keywords:
  - Large Language Model
  - Knowledge Graph
  - Question Answering
  - Reasoning Capacity
  - Hallucination in AI
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.20099
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:02:06.765528",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Knowledge Graph",
    "Question Answering",
    "Reasoning Capacity",
    "Hallucination in AI"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Knowledge Graph": 0.88,
    "Question Answering": 0.82,
    "Reasoning Capacity": 0.78,
    "Hallucination in AI": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion on QA and are a widely recognized concept in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Knowledge graphs",
        "canonical": "Knowledge Graph",
        "aliases": [
          "KG",
          "Knowledge Graphs"
        ],
        "category": "specific_connectable",
        "rationale": "Knowledge Graphs are pivotal in addressing the challenges of QA tasks as discussed in the paper.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Question Answering",
        "canonical": "Question Answering",
        "aliases": [
          "QA"
        ],
        "category": "specific_connectable",
        "rationale": "Question Answering is the primary application area discussed, linking LLMs and KGs.",
        "novelty_score": 0.2,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Reasoning capacity",
        "canonical": "Reasoning Capacity",
        "aliases": [
          "Reasoning Abilities"
        ],
        "category": "unique_technical",
        "rationale": "Reasoning capacity is a unique challenge in QA tasks that the paper aims to address.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Hallucinations",
        "canonical": "Hallucination in AI",
        "aliases": [
          "AI Hallucinations"
        ],
        "category": "unique_technical",
        "rationale": "Hallucinations are a critical issue in LLM-based QA that the paper discusses.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "methodology",
      "approaches",
      "evaluation metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Knowledge graphs",
      "resolved_canonical": "Knowledge Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Question Answering",
      "resolved_canonical": "Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Reasoning capacity",
      "resolved_canonical": "Reasoning Capacity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Hallucinations",
      "resolved_canonical": "Hallucination in AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.20099.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.20099](https://arxiv.org/abs/2505.20099)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Question Answering with LLMs and Learning from Answer Sets_20250923|Question Answering with LLMs and Learning from Answer Sets]] (86.8% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (86.6% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (86.1% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (85.3% similar)
- [[2025-09-18/From Automation to Autonomy_ A Survey on Large Language Models in Scientific Discovery_20250918|From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Graph|Knowledge Graph]], [[keywords/Question Answering|Question Answering]]
**âš¡ Unique Technical**: [[keywords/Reasoning Capacity|Reasoning Capacity]], [[keywords/Hallucination in AI|Hallucination in AI]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.20099v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art methods in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì´í•´ì™€ ìƒì„± ëŠ¥ë ¥ ë•ë¶„ì— ì§ˆë¬¸-ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ì§€ë§Œ, ë³µì¡í•œ QA ì‘ì—…ì—ì„œëŠ” ì¶”ë¡  ëŠ¥ë ¥ ë¶€ì¡±, ìµœì‹  ì§€ì‹ ë¶€ì¡±, í™˜ê° ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LLMê³¼ ì§€ì‹ ê·¸ë˜í”„(KG)ë¥¼ ê²°í•©í•œ ë°©ë²•ë“¤ì´ ìµœê·¼ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ LLMê³¼ KGë¥¼ QAì— í†µí•©í•˜ëŠ” ë°©ë²•ë¡ ì„ ìƒˆë¡œìš´ êµ¬ì¡°ì  ë¶„ë¥˜ ì²´ê³„ë¡œ ì œì•ˆí•˜ê³ , ìµœì‹  ë°©ë²•ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ì—¬ ê°•ì , í•œê³„, KG ìš”êµ¬ì‚¬í•­ ì¸¡ë©´ì—ì„œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤. ë˜í•œ, ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì´ ë³µì¡í•œ QAì˜ ì£¼ìš” ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ ë…¼ì˜í•˜ê³ , ë°œì „ ì‚¬í•­, í‰ê°€ ì§€í‘œ, ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ìš”ì•½í•˜ë©°, ë‚¨ì•„ìˆëŠ” ë„ì „ê³¼ ê¸°íšŒë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì´í•´ì™€ ìƒì„± ëŠ¥ë ¥ ë•ë¶„ì— ì§ˆë¬¸-ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 2. LLM ê¸°ë°˜ QAëŠ” ë³µì¡í•œ QA ì‘ì—…ì—ì„œ ì¶”ë¡  ëŠ¥ë ¥ ë¶€ì¡±, ì˜¤ë˜ëœ ì§€ì‹, í™˜ê° ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤.
- 3. ìµœê·¼ ì—°êµ¬ë“¤ì€ LLMê³¼ ì§€ì‹ ê·¸ë˜í”„(KG)ë¥¼ ê²°í•©í•˜ì—¬ QAì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ê³  ì‹œë„í•œë‹¤.
- 4. ë³¸ ë…¼ë¬¸ì€ LLMê³¼ KGë¥¼ ê²°í•©í•œ QA ë°©ë²•ë¡ ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ , ìµœì‹  ë°©ë²•ë“¤ì„ ê°•ì , í•œê³„, KG ìš”êµ¬ ì‚¬í•­ ì¸¡ë©´ì—ì„œ ë¹„êµ ë¶„ì„í•œë‹¤.
- 5. ë³µì¡í•œ QAì˜ ì£¼ìš” ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì„ ë…¼ì˜í•˜ê³ , ë°œì „, í‰ê°€ ì§€í‘œ, ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ìš”ì•½í•˜ë©°, ë‚¨ì€ ê³¼ì œì™€ ê¸°íšŒë¥¼ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-24 01:02:06*