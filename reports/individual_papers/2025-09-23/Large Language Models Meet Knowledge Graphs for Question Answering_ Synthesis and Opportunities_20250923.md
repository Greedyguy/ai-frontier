---
keywords:
  - Large Language Model
  - Knowledge Graph
  - Question Answering
  - Reasoning Capacity
  - Hallucination in AI
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2505.20099
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:02:06.765528",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Knowledge Graph",
    "Question Answering",
    "Reasoning Capacity",
    "Hallucination in AI"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Knowledge Graph": 0.88,
    "Question Answering": 0.82,
    "Reasoning Capacity": 0.78,
    "Hallucination in AI": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion on QA and are a widely recognized concept in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Knowledge graphs",
        "canonical": "Knowledge Graph",
        "aliases": [
          "KG",
          "Knowledge Graphs"
        ],
        "category": "specific_connectable",
        "rationale": "Knowledge Graphs are pivotal in addressing the challenges of QA tasks as discussed in the paper.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Question Answering",
        "canonical": "Question Answering",
        "aliases": [
          "QA"
        ],
        "category": "specific_connectable",
        "rationale": "Question Answering is the primary application area discussed, linking LLMs and KGs.",
        "novelty_score": 0.2,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Reasoning capacity",
        "canonical": "Reasoning Capacity",
        "aliases": [
          "Reasoning Abilities"
        ],
        "category": "unique_technical",
        "rationale": "Reasoning capacity is a unique challenge in QA tasks that the paper aims to address.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Hallucinations",
        "canonical": "Hallucination in AI",
        "aliases": [
          "AI Hallucinations"
        ],
        "category": "unique_technical",
        "rationale": "Hallucinations are a critical issue in LLM-based QA that the paper discusses.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "methodology",
      "approaches",
      "evaluation metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Knowledge graphs",
      "resolved_canonical": "Knowledge Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Question Answering",
      "resolved_canonical": "Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Reasoning capacity",
      "resolved_canonical": "Reasoning Capacity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Hallucinations",
      "resolved_canonical": "Hallucination in AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.20099.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2505.20099](https://arxiv.org/abs/2505.20099)

## 🔗 유사한 논문
- [[2025-09-23/Question Answering with LLMs and Learning from Answer Sets_20250923|Question Answering with LLMs and Learning from Answer Sets]] (86.8% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (86.6% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (86.1% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (85.3% similar)
- [[2025-09-18/From Automation to Autonomy_ A Survey on Large Language Models in Scientific Discovery_20250918|From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery]] (85.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Knowledge Graph|Knowledge Graph]], [[keywords/Question Answering|Question Answering]]
**⚡ Unique Technical**: [[keywords/Reasoning Capacity|Reasoning Capacity]], [[keywords/Hallucination in AI|Hallucination in AI]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.20099v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art methods in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.

## 📝 요약

대형 언어 모델(LLM)은 자연어 이해와 생성 능력 덕분에 질문-응답(QA) 작업에서 뛰어난 성과를 보였지만, 복잡한 QA 작업에서는 추론 능력 부족, 최신 지식 부족, 환각 문제로 어려움을 겪습니다. 이를 해결하기 위해 LLM과 지식 그래프(KG)를 결합한 방법들이 최근 연구되고 있습니다. 본 논문은 LLM과 KG를 QA에 통합하는 방법론을 새로운 구조적 분류 체계로 제안하고, 최신 방법들을 체계적으로 조사하여 강점, 한계, KG 요구사항 측면에서 비교 분석합니다. 또한, 이러한 접근법이 복잡한 QA의 주요 문제를 어떻게 해결하는지 논의하고, 발전 사항, 평가 지표, 벤치마크 데이터셋을 요약하며, 남아있는 도전과 기회를 강조합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 자연어 이해와 생성 능력 덕분에 질문-응답(QA) 작업에서 뛰어난 성능을 보인다.
- 2. LLM 기반 QA는 복잡한 QA 작업에서 추론 능력 부족, 오래된 지식, 환각 문제로 어려움을 겪는다.
- 3. 최근 연구들은 LLM과 지식 그래프(KG)를 결합하여 QA의 문제를 해결하려고 시도한다.
- 4. 본 논문은 LLM과 KG를 결합한 QA 방법론을 체계적으로 분류하고, 최신 방법들을 강점, 한계, KG 요구 사항 측면에서 비교 분석한다.
- 5. 복잡한 QA의 주요 과제를 해결하는 접근 방식을 논의하고, 발전, 평가 지표, 벤치마크 데이터셋을 요약하며, 남은 과제와 기회를 강조한다.


---

*Generated on 2025-09-24 01:02:06*