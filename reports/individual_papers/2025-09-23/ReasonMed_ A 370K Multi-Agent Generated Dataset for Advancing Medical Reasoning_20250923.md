---
keywords:
  - Large Language Model
  - Medical Reasoning
  - Chain of Thought
  - Error Refiner
  - PubMedQA
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2506.09513
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:06:27.680441",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Medical Reasoning",
    "Chain of Thought",
    "Error Refiner",
    "PubMedQA"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Medical Reasoning": 0.78,
    "Chain of Thought": 0.82,
    "Error Refiner": 0.7,
    "PubMedQA": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "This is a foundational technology for the dataset and connects to various models and techniques.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Medical Reasoning",
        "canonical": "Medical Reasoning",
        "aliases": [
          "Clinical Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus, linking medical reasoning with AI advancements.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain of Thought",
        "canonical": "Chain of Thought",
        "aliases": [
          "CoT"
        ],
        "category": "specific_connectable",
        "rationale": "A key technique for enhancing reasoning capabilities in models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.77,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Error Refiner",
        "canonical": "Error Refiner",
        "aliases": [
          "Error Correction"
        ],
        "category": "unique_technical",
        "rationale": "A novel component in the dataset's generation process, enhancing reasoning quality.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      },
      {
        "surface": "PubMedQA",
        "canonical": "PubMedQA",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A benchmark dataset that highlights the performance of ReasonMed.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "example",
      "process"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Medical Reasoning",
      "resolved_canonical": "Medical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain of Thought",
      "resolved_canonical": "Chain of Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.77,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Error Refiner",
      "resolved_canonical": "Error Refiner",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "PubMedQA",
      "resolved_canonical": "PubMedQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.09513.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2506.09513](https://arxiv.org/abs/2506.09513)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Fleming-R1_ Toward Expert-Level Medical Reasoning via Reinforcement Learning_20250922|Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning]] (87.0% similar)
- [[2025-09-19/MedFact-R1_ Towards Factual Medical Reasoning via Pseudo-Label Augmentation_20250919|MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation]] (85.3% similar)
- [[2025-09-23/MSCoRe_ A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents_20250923|MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents]] (84.8% similar)
- [[2025-09-23/Audio-Reasoner_ Improving Reasoning Capability in Large Audio Language Models_20250923|Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models]] (83.6% similar)
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Chain of Thought|Chain of Thought]], [[keywords/PubMedQA|PubMedQA]]
**âš¡ Unique Technical**: [[keywords/Medical Reasoning|Medical Reasoning]], [[keywords/Error Refiner|Error Refiner]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.09513v2 Announce Type: replace-cross 
Abstract: Reasoning-based large language models have excelled in mathematics and programming, yet their potential in knowledge-intensive medical question answering remains underexplored and insufficiently validated in clinical contexts. To bridge this gap, we introduce ReasonMed, the largest medical reasoning dataset to date, comprising 370k high-quality examples distilled from 1.75 million initial reasoning paths generated by complementary LLMs and curated through a cost-efficient easy-medium-difficult (EMD) pipeline. ReasonMed is built through a multi-agent generation, verification, and refinement process, in which an Error Refiner improves reasoning paths by correcting error-prone steps identified by a verifier. Using ReasonMed, we investigate effective strategies for training medical reasoning models and find that integrating detailed CoT reasoning with concise answer summaries yields the most robust fine-tuning results. Models trained on ReasonMed set a new benchmark: ReasonMed-7B surpasses the prior best sub-10B models by 4.17% and even exceeds LLaMA3.1-70B on PubMedQA by 4.60%. When scaled to ReasonMed-14B, it remains highly competitive, underscoring consistent scaling potential. The codes and datasets are available at https://github.com/YuSun-Work/ReasonMed.

## ğŸ“ ìš”ì•½

ReasonMedëŠ” ì§€ì‹ ì§‘ì•½ì ì¸ ì˜ë£Œ ì§ˆë¬¸ ì‘ë‹µ ë¶„ì•¼ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì ì¬ë ¥ì„ íƒêµ¬í•˜ê¸° ìœ„í•´ ê°œë°œëœ ê°€ì¥ í° ì˜ë£Œ ì¶”ë¡  ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. 370,000ê°œì˜ ê³ í’ˆì§ˆ ì˜ˆì‹œë¡œ êµ¬ì„±ëœ ì´ ë°ì´í„°ì…‹ì€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ìƒì„±, ê²€ì¦, ê°œì„  ê³¼ì •ì„ í†µí•´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. ReasonMedë¥¼ í™œìš©í•œ ì—°êµ¬ëŠ” ì„¸ë¶€ì ì¸ ì—°ì‡„ì  ì‚¬ê³ (CoT)ì™€ ê°„ê²°í•œ ë‹µë³€ ìš”ì•½ì„ ê²°í•©í•œ í›ˆë ¨ ì „ëµì´ ê°€ì¥ ê°•ë ¥í•œ ë¯¸ì„¸ ì¡°ì • ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ReasonMed-7B ëª¨ë¸ì€ ì´ì „ì˜ ìµœê³  ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë©°, ReasonMed-14Bë¡œ í™•ì¥ ì‹œì—ë„ ë†’ì€ ê²½ìŸë ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ê³¼ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ReasonMedëŠ” 370kì˜ ê³ í’ˆì§ˆ ì˜ˆì œë¥¼ í¬í•¨í•œ ê°€ì¥ í° ì˜ë£Œ ì¶”ë¡  ë°ì´í„°ì…‹ìœ¼ë¡œ, 1.75ë°±ë§Œ ê°œì˜ ì´ˆê¸° ì¶”ë¡  ê²½ë¡œì—ì„œ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.
- 2. ReasonMedëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ìƒì„±, ê²€ì¦, ê°œì„  ê³¼ì •ì„ í†µí•´ êµ¬ì¶•ë˜ì—ˆìœ¼ë©°, ì˜¤ë¥˜ ì •ì œìê°€ ê²€ì¦ìê°€ ì‹ë³„í•œ ì˜¤ë¥˜ê°€ ìˆëŠ” ë‹¨ê³„ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¶”ë¡  ê²½ë¡œë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 3. ReasonMedë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë£Œ ì¶”ë¡  ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” íš¨ê³¼ì ì¸ ì „ëµì„ ì¡°ì‚¬í•œ ê²°ê³¼, ìƒì„¸í•œ CoT ì¶”ë¡ ê³¼ ê°„ê²°í•œ ë‹µë³€ ìš”ì•½ì„ í†µí•©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ê°•ë ¥í•œ ë¯¸ì„¸ ì¡°ì • ê²°ê³¼ë¥¼ ë‚³ì•˜ìŠµë‹ˆë‹¤.
- 4. ReasonMed-7B ëª¨ë¸ì€ ì´ì „ì˜ 10B ì´í•˜ ëª¨ë¸ì„ 4.17% ì´ˆê³¼í•˜ë©°, PubMedQAì—ì„œ LLaMA3.1-70Bë¥¼ 4.60% ì´ˆê³¼í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
- 5. ReasonMed-14Bë¡œ í™•ì¥í–ˆì„ ë•Œë„ ì—¬ì „íˆ ë†’ì€ ê²½ìŸë ¥ì„ ìœ ì§€í•˜ë©°, ì¼ê´€ëœ í™•ì¥ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:06:27*