---
keywords:
  - Large Language Model
  - Medical Reasoning
  - Chain of Thought
  - Error Refiner
  - PubMedQA
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2506.09513
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:06:27.680441",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Medical Reasoning",
    "Chain of Thought",
    "Error Refiner",
    "PubMedQA"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Medical Reasoning": 0.78,
    "Chain of Thought": 0.82,
    "Error Refiner": 0.7,
    "PubMedQA": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "This is a foundational technology for the dataset and connects to various models and techniques.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Medical Reasoning",
        "canonical": "Medical Reasoning",
        "aliases": [
          "Clinical Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus, linking medical reasoning with AI advancements.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain of Thought",
        "canonical": "Chain of Thought",
        "aliases": [
          "CoT"
        ],
        "category": "specific_connectable",
        "rationale": "A key technique for enhancing reasoning capabilities in models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.77,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Error Refiner",
        "canonical": "Error Refiner",
        "aliases": [
          "Error Correction"
        ],
        "category": "unique_technical",
        "rationale": "A novel component in the dataset's generation process, enhancing reasoning quality.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      },
      {
        "surface": "PubMedQA",
        "canonical": "PubMedQA",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A benchmark dataset that highlights the performance of ReasonMed.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "example",
      "process"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Medical Reasoning",
      "resolved_canonical": "Medical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain of Thought",
      "resolved_canonical": "Chain of Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.77,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Error Refiner",
      "resolved_canonical": "Error Refiner",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "PubMedQA",
      "resolved_canonical": "PubMedQA",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.09513.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2506.09513](https://arxiv.org/abs/2506.09513)

## 🔗 유사한 논문
- [[2025-09-22/Fleming-R1_ Toward Expert-Level Medical Reasoning via Reinforcement Learning_20250922|Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning]] (87.0% similar)
- [[2025-09-19/MedFact-R1_ Towards Factual Medical Reasoning via Pseudo-Label Augmentation_20250919|MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation]] (85.3% similar)
- [[2025-09-23/MSCoRe_ A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents_20250923|MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents]] (84.8% similar)
- [[2025-09-23/Audio-Reasoner_ Improving Reasoning Capability in Large Audio Language Models_20250923|Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models]] (83.6% similar)
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (83.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Chain of Thought|Chain of Thought]], [[keywords/PubMedQA|PubMedQA]]
**⚡ Unique Technical**: [[keywords/Medical Reasoning|Medical Reasoning]], [[keywords/Error Refiner|Error Refiner]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.09513v2 Announce Type: replace-cross 
Abstract: Reasoning-based large language models have excelled in mathematics and programming, yet their potential in knowledge-intensive medical question answering remains underexplored and insufficiently validated in clinical contexts. To bridge this gap, we introduce ReasonMed, the largest medical reasoning dataset to date, comprising 370k high-quality examples distilled from 1.75 million initial reasoning paths generated by complementary LLMs and curated through a cost-efficient easy-medium-difficult (EMD) pipeline. ReasonMed is built through a multi-agent generation, verification, and refinement process, in which an Error Refiner improves reasoning paths by correcting error-prone steps identified by a verifier. Using ReasonMed, we investigate effective strategies for training medical reasoning models and find that integrating detailed CoT reasoning with concise answer summaries yields the most robust fine-tuning results. Models trained on ReasonMed set a new benchmark: ReasonMed-7B surpasses the prior best sub-10B models by 4.17% and even exceeds LLaMA3.1-70B on PubMedQA by 4.60%. When scaled to ReasonMed-14B, it remains highly competitive, underscoring consistent scaling potential. The codes and datasets are available at https://github.com/YuSun-Work/ReasonMed.

## 📝 요약

ReasonMed는 지식 집약적인 의료 질문 응답 분야에서 대규모 언어 모델의 잠재력을 탐구하기 위해 개발된 가장 큰 의료 추론 데이터셋입니다. 370,000개의 고품질 예시로 구성된 이 데이터셋은 다중 에이전트 생성, 검증, 개선 과정을 통해 구축되었습니다. ReasonMed를 활용한 연구는 세부적인 연쇄적 사고(CoT)와 간결한 답변 요약을 결합한 훈련 전략이 가장 강력한 미세 조정 결과를 가져온다는 것을 발견했습니다. ReasonMed-7B 모델은 이전의 최고 모델을 능가하며, ReasonMed-14B로 확장 시에도 높은 경쟁력을 유지합니다. 데이터셋과 코드는 GitHub에서 제공됩니다.

## 🎯 주요 포인트

- 1. ReasonMed는 370k의 고품질 예제를 포함한 가장 큰 의료 추론 데이터셋으로, 1.75백만 개의 초기 추론 경로에서 추출되었습니다.
- 2. ReasonMed는 다중 에이전트 생성, 검증, 개선 과정을 통해 구축되었으며, 오류 정제자가 검증자가 식별한 오류가 있는 단계를 수정하여 추론 경로를 개선합니다.
- 3. ReasonMed를 사용하여 의료 추론 모델을 훈련하는 효과적인 전략을 조사한 결과, 상세한 CoT 추론과 간결한 답변 요약을 통합하는 것이 가장 강력한 미세 조정 결과를 낳았습니다.
- 4. ReasonMed-7B 모델은 이전의 10B 이하 모델을 4.17% 초과하며, PubMedQA에서 LLaMA3.1-70B를 4.60% 초과하는 새로운 벤치마크를 설정했습니다.
- 5. ReasonMed-14B로 확장했을 때도 여전히 높은 경쟁력을 유지하며, 일관된 확장 가능성을 강조합니다.


---

*Generated on 2025-09-24 01:06:27*