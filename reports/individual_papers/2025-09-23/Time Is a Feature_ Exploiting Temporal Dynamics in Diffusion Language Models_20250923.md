---
keywords:
  - Large Language Model
  - Temporal Self-Consistency Voting
  - Temporal Consistency Reinforcement
  - Temporal Semantic Entropy
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.09138
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:19:44.147191",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Temporal Self-Consistency Voting",
    "Temporal Consistency Reinforcement",
    "Temporal Semantic Entropy"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Temporal Self-Consistency Voting": 0.8,
    "Temporal Consistency Reinforcement": 0.82,
    "Temporal Semantic Entropy": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "dLLM",
          "Diffusion LLM"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader concept of large language models, which is central to the paper's focus.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.9
      },
      {
        "surface": "Temporal Self-Consistency Voting",
        "canonical": "Temporal Self-Consistency Voting",
        "aliases": [
          "TSC Voting"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method specific to the paper, enhancing understanding of temporal dynamics.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Temporal Consistency Reinforcement",
        "canonical": "Temporal Consistency Reinforcement",
        "aliases": [
          "TCR"
        ],
        "category": "unique_technical",
        "rationale": "Represents a unique post-training method that is central to the paper's contributions.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "Temporal Semantic Entropy",
        "canonical": "Temporal Semantic Entropy",
        "aliases": [
          "TSE"
        ],
        "category": "unique_technical",
        "rationale": "A novel metric introduced in the paper, crucial for understanding the proposed methods.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "temporal dynamics",
      "denoising steps"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Temporal Self-Consistency Voting",
      "resolved_canonical": "Temporal Self-Consistency Voting",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Temporal Consistency Reinforcement",
      "resolved_canonical": "Temporal Consistency Reinforcement",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Temporal Semantic Entropy",
      "resolved_canonical": "Temporal Semantic Entropy",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.09138.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.09138](https://arxiv.org/abs/2508.09138)

## 🔗 유사한 논문
- [[2025-09-22/Discrete Diffusion in Large Language and Multimodal Models_ A Survey_20250922|Discrete Diffusion in Large Language and Multimodal Models: A Survey]] (87.3% similar)
- [[2025-09-18/Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning_20250918|Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning]] (85.6% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (84.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.0% similar)
- [[2025-09-23/Spiffy_ Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding_20250923|Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding]] (83.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Temporal Self-Consistency Voting|Temporal Self-Consistency Voting]], [[keywords/Temporal Consistency Reinforcement|Temporal Consistency Reinforcement]], [[keywords/Temporal Semantic Entropy|Temporal Semantic Entropy]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2508.09138v2 Announce Type: replace-cross 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.

## 📝 요약

이 논문은 확산 대형 언어 모델(dLLMs)의 텍스트 생성 과정에서 발생하는 '시간적 진동' 현상을 다룹니다. 이는 중간 단계에서 올바른 답이 나타나지만 이후 단계에서 덮어씌워지는 문제입니다. 이를 해결하기 위해 두 가지 방법을 제안합니다. 첫째, '시간적 자기 일관성 투표'는 중간 예측을 모아 가장 일관된 출력을 선택하는 훈련 불필요한 디코딩 전략입니다. 둘째, '시간적 일관성 강화'는 중간 예측의 의미적 안정성을 측정하는 '시간적 의미 엔트로피(TSE)'를 보상 신호로 사용하여 안정적인 생성을 유도합니다. 실험 결과, TSE 보상만으로도 Countdown 데이터셋에서 평균 24.7%의 성능 향상을 보였으며, 다른 데이터셋에서도 성능이 향상되었습니다. 이 연구는 dLLMs의 시간적 역학의 잠재력을 강조하며, 이를 활용할 수 있는 간단하지만 효과적인 도구를 제공합니다.

## 🎯 주요 포인트

- 1. 확산 대형 언어 모델(dLLMs)은 반복적인 노이즈 제거 과정을 통해 텍스트를 생성하지만, 현재의 디코딩 전략은 중간 예측을 버리고 최종 출력에 집중합니다.
- 2. 중간 과정에서 정답이 나타나지만 이후 단계에서 덮어쓰이는 '시간적 진동' 현상이 발견되었습니다.
- 3. 시간적 일관성을 활용하기 위해 두 가지 방법을 제안합니다: 1) 시간적 자기 일관성 투표, 2) 시간적 일관성 강화.
- 4. 제안된 방법은 다양한 벤치마크에서 효과적임을 입증했으며, 특히 Countdown 데이터셋에서 24.7%의 평균 개선을 보였습니다.
- 5. 연구 결과는 dLLMs의 시간적 역학의 잠재력을 강조하며, 이를 활용할 수 있는 간단하면서도 효과적인 도구를 제공합니다.


---

*Generated on 2025-09-24 01:19:44*