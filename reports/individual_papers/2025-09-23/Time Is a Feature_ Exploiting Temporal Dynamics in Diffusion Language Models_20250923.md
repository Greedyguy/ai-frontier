---
keywords:
  - Large Language Model
  - Temporal Self-Consistency Voting
  - Temporal Consistency Reinforcement
  - Temporal Semantic Entropy
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2508.09138
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:19:44.147191",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Temporal Self-Consistency Voting",
    "Temporal Consistency Reinforcement",
    "Temporal Semantic Entropy"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Temporal Self-Consistency Voting": 0.8,
    "Temporal Consistency Reinforcement": 0.82,
    "Temporal Semantic Entropy": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "dLLM",
          "Diffusion LLM"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader concept of large language models, which is central to the paper's focus.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.9
      },
      {
        "surface": "Temporal Self-Consistency Voting",
        "canonical": "Temporal Self-Consistency Voting",
        "aliases": [
          "TSC Voting"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method specific to the paper, enhancing understanding of temporal dynamics.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Temporal Consistency Reinforcement",
        "canonical": "Temporal Consistency Reinforcement",
        "aliases": [
          "TCR"
        ],
        "category": "unique_technical",
        "rationale": "Represents a unique post-training method that is central to the paper's contributions.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "Temporal Semantic Entropy",
        "canonical": "Temporal Semantic Entropy",
        "aliases": [
          "TSE"
        ],
        "category": "unique_technical",
        "rationale": "A novel metric introduced in the paper, crucial for understanding the proposed methods.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "temporal dynamics",
      "denoising steps"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Temporal Self-Consistency Voting",
      "resolved_canonical": "Temporal Self-Consistency Voting",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Temporal Consistency Reinforcement",
      "resolved_canonical": "Temporal Consistency Reinforcement",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Temporal Semantic Entropy",
      "resolved_canonical": "Temporal Semantic Entropy",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.09138.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2508.09138](https://arxiv.org/abs/2508.09138)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Discrete Diffusion in Large Language and Multimodal Models_ A Survey_20250922|Discrete Diffusion in Large Language and Multimodal Models: A Survey]] (87.3% similar)
- [[2025-09-18/Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning_20250918|Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning]] (85.6% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (84.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.0% similar)
- [[2025-09-23/Spiffy_ Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding_20250923|Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Temporal Self-Consistency Voting|Temporal Self-Consistency Voting]], [[keywords/Temporal Consistency Reinforcement|Temporal Consistency Reinforcement]], [[keywords/Temporal Semantic Entropy|Temporal Semantic Entropy]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.09138v2 Announce Type: replace-cross 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í™•ì‚° ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(dLLMs)ì˜ í…ìŠ¤íŠ¸ ìƒì„± ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” 'ì‹œê°„ì  ì§„ë™' í˜„ìƒì„ ë‹¤ë£¹ë‹ˆë‹¤. ì´ëŠ” ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì´ ë‚˜íƒ€ë‚˜ì§€ë§Œ ì´í›„ ë‹¨ê³„ì—ì„œ ë®ì–´ì”Œì›Œì§€ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì²«ì§¸, 'ì‹œê°„ì  ìê¸° ì¼ê´€ì„± íˆ¬í‘œ'ëŠ” ì¤‘ê°„ ì˜ˆì¸¡ì„ ëª¨ì•„ ê°€ì¥ ì¼ê´€ëœ ì¶œë ¥ì„ ì„ íƒí•˜ëŠ” í›ˆë ¨ ë¶ˆí•„ìš”í•œ ë””ì½”ë”© ì „ëµì…ë‹ˆë‹¤. ë‘˜ì§¸, 'ì‹œê°„ì  ì¼ê´€ì„± ê°•í™”'ëŠ” ì¤‘ê°„ ì˜ˆì¸¡ì˜ ì˜ë¯¸ì  ì•ˆì •ì„±ì„ ì¸¡ì •í•˜ëŠ” 'ì‹œê°„ì  ì˜ë¯¸ ì—”íŠ¸ë¡œí”¼(TSE)'ë¥¼ ë³´ìƒ ì‹ í˜¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ ìƒì„±ì„ ìœ ë„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TSE ë³´ìƒë§Œìœ¼ë¡œë„ Countdown ë°ì´í„°ì…‹ì—ì„œ í‰ê·  24.7%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” dLLMsì˜ ì‹œê°„ì  ì—­í•™ì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•˜ë©°, ì´ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í™•ì‚° ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(dLLMs)ì€ ë°˜ë³µì ì¸ ë…¸ì´ì¦ˆ ì œê±° ê³¼ì •ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ë§Œ, í˜„ì¬ì˜ ë””ì½”ë”© ì „ëµì€ ì¤‘ê°„ ì˜ˆì¸¡ì„ ë²„ë¦¬ê³  ìµœì¢… ì¶œë ¥ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.
- 2. ì¤‘ê°„ ê³¼ì •ì—ì„œ ì •ë‹µì´ ë‚˜íƒ€ë‚˜ì§€ë§Œ ì´í›„ ë‹¨ê³„ì—ì„œ ë®ì–´ì“°ì´ëŠ” 'ì‹œê°„ì  ì§„ë™' í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. ì‹œê°„ì  ì¼ê´€ì„±ì„ í™œìš©í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤: 1) ì‹œê°„ì  ìê¸° ì¼ê´€ì„± íˆ¬í‘œ, 2) ì‹œê°„ì  ì¼ê´€ì„± ê°•í™”.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìœ¼ë©°, íŠ¹íˆ Countdown ë°ì´í„°ì…‹ì—ì„œ 24.7%ì˜ í‰ê·  ê°œì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” dLLMsì˜ ì‹œê°„ì  ì—­í•™ì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•˜ë©°, ì´ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:19:44*