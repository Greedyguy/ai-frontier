---
keywords:
  - Attention Mechanism
  - Large Language Model
  - KV-caching
  - query-key normalization
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2502.00919
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:44:09.305461",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "Large Language Model",
    "KV-caching",
    "query-key normalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.8,
    "Large Language Model": 0.85,
    "KV-caching": 0.7,
    "query-key normalization": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "attention sinks",
        "canonical": "Attention Mechanism",
        "aliases": [
          "attention sink"
        ],
        "category": "specific_connectable",
        "rationale": "Attention sinks are a specific aspect of the attention mechanism, crucial for understanding model performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large language model"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect to various advanced topics in NLP.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "KV-caching",
        "canonical": "KV-caching",
        "aliases": [
          "key-value caching"
        ],
        "category": "unique_technical",
        "rationale": "KV-caching is a unique technical concept essential for understanding memory efficiency in models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "query-key normalization",
        "canonical": "query-key normalization",
        "aliases": [
          "QK normalization"
        ],
        "category": "unique_technical",
        "rationale": "Query-key normalization is a novel concept that affects attention distribution in models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "tokens",
      "sequence",
      "model performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "attention sinks",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "KV-caching",
      "resolved_canonical": "KV-caching",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "query-key normalization",
      "resolved_canonical": "query-key normalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.00919.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2502.00919](https://arxiv.org/abs/2502.00919)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models_20250923|Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models]] (82.5% similar)
- [[2025-09-22/Tag&Tab_ Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack_20250922|Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack]] (81.3% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (81.1% similar)
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (80.1% similar)
- [[2025-09-22/Causal2Vec_ Improving Decoder-only LLMs as Versatile Embedding Models_20250922|Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models]] (80.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/KV-caching|KV-caching]], [[keywords/query-key normalization|query-key normalization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.00919v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as attention sinks. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks -- especially those beyond the first token -- remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: catch a sequence of tokens, tag them using a common direction in embedding space, and release them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the 'catch, tag, release' mechanism, and where it emerges through training.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì—ì„œ ì£¼ì˜ê°€ ì§‘ì¤‘ë˜ëŠ” íŠ¹ì • í† í°ì¸ 'ì£¼ì˜ ì‹±í¬'ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì£¼ì˜ ì‹±í¬ëŠ” ëª¨ë¸ ì„±ëŠ¥, íŠ¹íˆ ëª¨ë¸ ì••ì¶•ê³¼ KV-ìºì‹±ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ì§€ë§Œ, ê·¸ ê¸°ëŠ¥ê³¼ ê¸°ì›ì€ ì˜ ì•Œë ¤ì ¸ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” ì£¼ì˜ ì‹±í¬ê°€ í† í°ì„ ì¡ì•„ ê³µí†µ ë°©í–¥ìœ¼ë¡œ íƒœê·¸ë¥¼ ë¶™ì´ê³ , ë‚˜ì¤‘ì— íƒœê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í°ì„ íšŒìˆ˜í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì „ë‹¬í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì€ ì¶”ë¡  ëª¨ë¸ì—ì„œë„ í™•ì¥ë˜ë©°, ìµœê·¼ì˜ ì¿¼ë¦¬-í‚¤ ì •ê·œí™” ëª¨ë¸ì—ì„œë„ ì—¬ì „íˆ ì¤‘ìš”í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” 'ì¡ê¸°, íƒœê·¸, ë°©ì¶œ' ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë¬¸ì œë¥¼ ì œì‹œí•˜ì—¬ ì´ë¡ ì  ë¶„ì„ì„ ì¥ë ¤í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì£¼ë¡œ íŠ¹ì • í† í°ì— ì§‘ì¤‘í•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ì´ë¥¼ ì£¼ì˜ ì‹±í¬ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
- 2. ì£¼ì˜ ì‹±í¬ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, íŠ¹íˆ ëª¨ë¸ ì••ì¶•ê³¼ KV-ìºì‹±ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤.
- 3. ì£¼ì˜ ì‹±í¬ëŠ” í† í°ì„ ì¡ì•„ë‚´ê³ , ê³µí†µì˜ ë°©í–¥ìœ¼ë¡œ íƒœê·¸ë¥¼ ë¶€ì—¬í•œ í›„, ì”ì—¬ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë‹¤ì‹œ ë°©ì¶œí•©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ì´ëŸ¬í•œ íƒœê·¸ëŠ” ëª…ì œì˜ ì§„ì‹¤ì„±ê³¼ ê°™ì€ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŒì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.
- 5. ìš°ë¦¬ëŠ” 'ì¡ê¸°, íƒœê·¸í•˜ê¸°, ë°©ì¶œí•˜ê¸°' ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë¬¸ì œë¥¼ ì œì•ˆí•˜ì—¬ ì´ë¡ ì  ë¶„ì„ì„ ì¥ë ¤í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:44:09*