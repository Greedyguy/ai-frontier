---
keywords:
  - Vision-Language Model
  - Seeing Culture Benchmark
  - Cultural Reasoning
  - Visual Question Answering
  - Cultural Artifacts
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16517
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:23:00.735282",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Seeing Culture Benchmark",
    "Cultural Reasoning",
    "Visual Question Answering",
    "Cultural Artifacts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.88,
    "Seeing Culture Benchmark": 0.82,
    "Cultural Reasoning": 0.79,
    "Visual Question Answering": 0.77,
    "Cultural Artifacts": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal vision-language models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Multimodal VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on cultural reasoning, linking visual and textual content.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.88
      },
      {
        "surface": "Seeing Culture Benchmark",
        "canonical": "Seeing Culture Benchmark",
        "aliases": [
          "SCB"
        ],
        "category": "unique_technical",
        "rationale": "This benchmark is a novel contribution for evaluating cultural reasoning in VLMs.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "cultural reasoning",
        "canonical": "Cultural Reasoning",
        "aliases": [
          "cultural understanding"
        ],
        "category": "specific_connectable",
        "rationale": "Cultural reasoning is a key aspect of the paper, linking to the broader theme of understanding diverse cultures.",
        "novelty_score": 0.58,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "visual question answering",
        "canonical": "Visual Question Answering",
        "aliases": [
          "VQA"
        ],
        "category": "specific_connectable",
        "rationale": "VQA is a critical component of the benchmark's methodology, connecting visual reasoning with textual queries.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "cultural artifacts",
        "canonical": "Cultural Artifacts",
        "aliases": [
          "cultural items"
        ],
        "category": "specific_connectable",
        "rationale": "Cultural artifacts are central to the benchmark's focus, providing tangible links to cultural reasoning.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "datasets",
      "images",
      "questions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal vision-language models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Seeing Culture Benchmark",
      "resolved_canonical": "Seeing Culture Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "cultural reasoning",
      "resolved_canonical": "Cultural Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "visual question answering",
      "resolved_canonical": "Visual Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "cultural artifacts",
      "resolved_canonical": "Cultural Artifacts",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Seeing Culture: A Benchmark for Visual Reasoning and Grounding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16517.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16517](https://arxiv.org/abs/2509.16517)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (86.2% similar)
- [[2025-09-22/ORIC_ Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models_20250922|ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models]] (81.6% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (81.1% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (80.5% similar)
- [[2025-09-22/Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study_20250922|Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Cultural Reasoning|Cultural Reasoning]], [[keywords/Visual Question Answering|Visual Question Answering]], [[keywords/Cultural Artifacts|Cultural Artifacts]]
**âš¡ Unique Technical**: [[keywords/Seeing Culture Benchmark|Seeing Culture Benchmark]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16517v1 Announce Type: cross 
Abstract: Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ì´ ë¬¸í™”ì  ì´í•´ë¥¼ ìš”êµ¬í•˜ëŠ” ì‘ì—…ì—ì„œì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ Seeing Culture Benchmark(SCB)ì„ ì†Œê°œí•©ë‹ˆë‹¤. SCBëŠ” ë¬¸í™”ì ìœ¼ë¡œ í’ë¶€í•œ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ VLMsê°€ ë‘ ë‹¨ê³„ì— ê±¸ì³ ë¬¸í™”ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ë‹¤ì§€ì„ ë‹¤í˜• ì‹œê° ì§ˆë¬¸ ì‘ë‹µ(VQA)ì„ í†µí•´ ì˜¬ë°”ë¥¸ ì‹œê° ì˜µì…˜ì„ ì„ íƒí•˜ê³ , ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ê´€ë ¨ ë¬¸í™” ìœ ë¬¼ì„ ì¦ê±°ë¡œ ë¶„í• í•©ë‹ˆë‹¤. SCBëŠ” ë™ë‚¨ì•„ì‹œì•„ 7ê°œêµ­ì˜ ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ í¬ê´„í•˜ëŠ” 1,065ê°œì˜ ì´ë¯¸ì§€ì™€ 3,178ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ë¬¸í™”ì  ì¶”ë¡ ì˜ ë³µì¡ì„±ì„ ë“œëŸ¬ë‚´ê³  VLMsì˜ ì‹œê°ì  ì¶”ë¡ ê³¼ ê³µê°„ì  ê¸°ë°˜ì˜ ì°¨ì´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. SCBëŠ” ë¬¸í™”ì  ì¶”ë¡  ë¶„ì•¼ì˜ ë°œì „ì„ ìœ„í•œ ì¤‘ìš”í•œ ê¸°ì¤€ì ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë©€í‹°ëª¨ë‹¬ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ì‹œê° ë° í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì˜ ê²°í•©ëœ ì´í•´ë¥¼ ìš”êµ¬í•˜ëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ìƒë‹¹í•œ ë°œì „ì„ ì´ë£¨ì—ˆìœ¼ë‚˜, ë§ì€ ë¬¸í™”ê°€ ê³¼ì†Œ ëŒ€í‘œë˜ëŠ” ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¸í™”ì  ì¶”ë¡ ì— ì¤‘ì ì„ ë‘” Seeing Culture Benchmark(SCB)ë¥¼ ì†Œê°œí•˜ë©°, VLMsê°€ ë¬¸í™”ì ìœ¼ë¡œ í’ë¶€í•œ ì´ë¯¸ì§€ë¥¼ ë‘ ë‹¨ê³„ë¡œ ì¶”ë¡ í•˜ë„ë¡ ìš”êµ¬í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. SCB ë²¤ì¹˜ë§ˆí¬ëŠ” ë™ë‚¨ì•„ì‹œì•„ 7ê°œêµ­ì˜ ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ í¬ì°©í•œ 1,065ê°œì˜ ì´ë¯¸ì§€ì™€ 3,178ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ë¬¸í™”ì  ì¶”ë¡ ì˜ ë³µì¡ì„±ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ VLMsì˜ í‰ê°€ ê²°ê³¼, ë¬¸í™”ì ìœ¼ë¡œ ë¯¸ë¬˜í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‹œê°ì  ì¶”ë¡ ê³¼ ê³µê°„ì  ê·¼ê±° ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ê°€ ë“œëŸ¬ë‚¬ìœ¼ë©°, ì´ëŠ” ë¬¸í™”ì  ì¶”ë¡  ë¶„ì•¼ì˜ í–¥í›„ ë°œì „ì„ ìœ„í•œ ì¤‘ìš”í•œ ê¸°ì¤€ì ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:23:00*