---
keywords:
  - Large Language Model
  - Brain Alignment
  - Formal Linguistic Competence
  - Functional Linguistic Competence
  - Neural Language Benchmarks
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2503.01830
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:51:33.092466",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Brain Alignment",
    "Formal Linguistic Competence",
    "Functional Linguistic Competence",
    "Neural Language Benchmarks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Brain Alignment": 0.8,
    "Formal Linguistic Competence": 0.78,
    "Functional Linguistic Competence": 0.77,
    "Neural Language Benchmarks": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to a broad range of AI research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "brain alignment",
        "canonical": "Brain Alignment",
        "aliases": [
          "neural alignment"
        ],
        "category": "unique_technical",
        "rationale": "Key concept in understanding the relationship between models and human cognition.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "formal linguistic competence",
        "canonical": "Formal Linguistic Competence",
        "aliases": [
          "linguistic rules knowledge"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the focus on structural language understanding in models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "functional linguistic competence",
        "canonical": "Functional Linguistic Competence",
        "aliases": [
          "world knowledge and reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Contrasts with formal competence, emphasizing broader cognitive functions.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "neural language benchmarks",
        "canonical": "Neural Language Benchmarks",
        "aliases": [
          "language benchmarks"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for evaluating model performance against human-like understanding.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "training checkpoints",
      "model sizes",
      "feature size"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "brain alignment",
      "resolved_canonical": "Brain Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "formal linguistic competence",
      "resolved_canonical": "Formal Linguistic Competence",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "functional linguistic competence",
      "resolved_canonical": "Functional Linguistic Competence",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "neural language benchmarks",
      "resolved_canonical": "Neural Language Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# From Language to Cognition: How LLMs Outgrow the Human Language Network

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.01830.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2503.01830](https://arxiv.org/abs/2503.01830)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs_20250918|Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs]] (85.7% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (85.5% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (85.5% similar)
- [[2025-09-22/Modeling Transformers as complex networks to analyze learning dynamics_20250922|Modeling Transformers as complex networks to analyze learning dynamics]] (85.5% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Neural Language Benchmarks|Neural Language Benchmarks]]
**âš¡ Unique Technical**: [[keywords/Brain Alignment|Brain Alignment]], [[keywords/Formal Linguistic Competence|Formal Linguistic Competence]], [[keywords/Functional Linguistic Competence|Functional Linguistic Competence]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.01830v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì¸ê°„ì˜ ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ì™€ ìœ ì‚¬í•œ ì‹ ê²½ í™œë™ì„ ë³´ì´ì§€ë§Œ, ì–¸ì–´ê°€ ë‡Œì™€ ìœ ì‚¬í•œ í‘œí˜„ì„ í˜•ì„±í•˜ëŠ” ì£¼ìš” íŠ¹ì„±ê³¼ í›ˆë ¨ ì¤‘ ë‹¤ì–‘í•œ ê³¼ì œì— ë”°ë¥¸ ì§„í™”ëŠ” ëª…í™•í•˜ì§€ ì•Šë‹¤ê³  ì§€ì í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” 8ê°€ì§€ ëª¨ë¸ í¬ê¸°ì—ì„œ 3000ì–µ ê°œì˜ í† í°ì„ í¬í•¨í•œ 34ê°œì˜ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‡Œ ì •ë ¬ì´ ì–¸ì–´ì  ëŠ¥ë ¥ê³¼ ì–´ë–»ê²Œ ê´€ë ¨ë˜ëŠ”ì§€ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ë‡Œ ì •ë ¬ì€ ì–¸ì–´ ê·œì¹™ì— ëŒ€í•œ ì§€ì‹ì¸ í˜•ì‹ì  ì–¸ì–´ ëŠ¥ë ¥ì˜ ë°œë‹¬ê³¼ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì§€ë§Œ, ì„¸ê³„ ì§€ì‹ê³¼ ì¶”ë¡ ì„ í¬í•¨í•˜ëŠ” ê¸°ëŠ¥ì  ì–¸ì–´ ëŠ¥ë ¥ê³¼ì˜ ê´€ê³„ëŠ” ì•½í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ í¬ê¸°ëŠ” ë‡Œ ì •ë ¬ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ë³€ìˆ˜ê°€ ì•„ë‹ˆë©°, ëª¨ë¸ì´ ì¸ê°„ì˜ ì–¸ì–´ ëŠ¥ë ¥ì„ ì´ˆê³¼í•˜ë©´ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡, í–‰ë™ ì •ë ¬ ë° ë‡Œ ì •ë ¬ ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ê°ì†Œí•œë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–¸ì–´ ë‡Œ ì •ë ¬ ë²¤ì¹˜ë§ˆí¬ê°€ ì•„ì§ í¬í™”ë˜ì§€ ì•Šì•˜ìŒì„ ë³´ì—¬ì£¼ë©°, ë¯¸ë˜ ëª¨ë¸ ê°œì„ ì˜ ê¸°íšŒë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ, ì¸ê°„ì˜ ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ëŠ” ê¸°ëŠ¥ì  ì¸¡ë©´ë³´ë‹¤ëŠ” í˜•ì‹ì  ì¸¡ë©´ì— ì˜í•´ ê°€ì¥ ì˜ ëª¨ë¸ë§ëœë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì¸ê°„ ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ì˜ ì‹ ê²½ í™œë™ê³¼ ìœ ì‚¬í•œ íŠ¹ì„±ì„ ë³´ì¸ë‹¤.
- 2. ë‡Œ ì •ë ¬ì€ í˜•ì‹ì  ì–¸ì–´ ëŠ¥ë ¥ì˜ ë°œë‹¬ê³¼ ë” ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìœ¼ë©°, ê¸°ëŠ¥ì  ì–¸ì–´ ëŠ¥ë ¥ê³¼ì˜ ê´€ê³„ëŠ” ì•½í•˜ë‹¤.
- 3. ëª¨ë¸ í¬ê¸°ëŠ” ë‡Œ ì •ë ¬ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ë³€ìˆ˜ê°€ ì•„ë‹ˆë©°, ì¸ê°„ ì–¸ì–´ ëŠ¥ë ¥ì„ ì´ˆê³¼í•˜ëŠ” ëª¨ë¸ì—ì„œëŠ” ì˜ˆì¸¡ ì •í™•ë„ì™€ ë‡Œ ì •ë ¬ì˜ ìƒê´€ê´€ê³„ê°€ ê°ì†Œí•œë‹¤.
- 4. ì–¸ì–´ ë‡Œ ì •ë ¬ ë²¤ì¹˜ë§ˆí¬ëŠ” ì•„ì§ í¬í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ì´ëŠ” í–¥í›„ ëª¨ë¸ ê°œì„ ì˜ ê¸°íšŒë¥¼ ì œê³µí•œë‹¤.
- 5. ì¸ê°„ ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ëŠ” ê¸°ëŠ¥ì  ì¸¡ë©´ë³´ë‹¤ëŠ” í˜•ì‹ì  ì¸¡ë©´ìœ¼ë¡œ ê°€ì¥ ì˜ ëª¨ë¸ë§ëœë‹¤.


---

*Generated on 2025-09-24 03:51:33*