---
keywords:
  - Large Language Model
  - GPU Infrastructure Management
  - Fault Tolerance
  - ByteRobust
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16293
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:12:50.682531",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "GPU Infrastructure Management",
    "Fault Tolerance",
    "ByteRobust"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "GPU Infrastructure Management": 0.78,
    "Fault Tolerance": 0.82,
    "ByteRobust": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large-Scale Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on training infrastructure, providing a strong link to existing LLM research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.92,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "GPU Infrastructure Management",
        "canonical": "GPU Infrastructure Management",
        "aliases": [
          "GPU Management",
          "GPU Infrastructure"
        ],
        "category": "unique_technical",
        "rationale": "Describes a unique system developed for managing large-scale GPU resources, crucial for LLM training.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Fault Tolerance",
        "canonical": "Fault Tolerance",
        "aliases": [
          "Failure Tolerance",
          "Error Tolerance"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for linking to broader discussions on system reliability and robustness in AI infrastructure.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "ByteRobust",
        "canonical": "ByteRobust",
        "aliases": [
          "ByteRobust System"
        ],
        "category": "unique_technical",
        "rationale": "A specific system introduced in the paper, offering a unique contribution to LLM training solutions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "training scale",
      "resource scale",
      "training stability"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.92,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "GPU Infrastructure Management",
      "resolved_canonical": "GPU Infrastructure Management",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Fault Tolerance",
      "resolved_canonical": "Fault Tolerance",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "ByteRobust",
      "resolved_canonical": "ByteRobust",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Robust LLM Training Infrastructure at ByteDance

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16293.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16293](https://arxiv.org/abs/2509.16293)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (85.6% similar)
- [[2025-09-19/Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization_20250919|Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization]] (84.3% similar)
- [[2025-09-18/LNE-Blocking_ An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models_20250918|LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models]] (83.4% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (82.3% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Fault Tolerance|Fault Tolerance]]
**âš¡ Unique Technical**: [[keywords/GPU Infrastructure Management|GPU Infrastructure Management]], [[keywords/ByteRobust|ByteRobust]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16293v1 Announce Type: cross 
Abstract: The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ í›ˆë ¨ì„ ì§€ì›í•˜ê¸° ìœ„í•œ GPU ì¸í”„ë¼ ê´€ë¦¬ ì‹œìŠ¤í…œì¸ ByteRobustë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. LLM í›ˆë ¨ ê³¼ì •ì˜ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬, ì‹¤íŒ¨ ê°ì§€ ë° ë³µêµ¬ë¥¼ ìš°ì„ ì‹œí•˜ë©°, ë³‘ë ¬ ì²˜ë¦¬ì™€ ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜¤ë¥˜ ë‚´ì„± ë° ì‹ ì†í•œ ì˜¤ë¥˜ ì§„ë‹¨ì„ ì œê³µí•©ë‹ˆë‹¤. ByteRobustëŠ” 20ë§Œ ê°œ ì´ìƒì˜ GPUê°€ ìˆëŠ” í”Œë«í¼ì— ë°°ì¹˜ë˜ì–´, 9,600ê°œì˜ GPUë¥¼ ì‚¬ìš©í•œ 3ê°œì›”ê°„ì˜ í›ˆë ¨ ì‘ì—…ì—ì„œ 97%ì˜ ETTRì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ëŒ€ê·œëª¨ LLM í›ˆë ¨ì˜ ì—°ì†ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë³´ì¥í•˜ëŠ” ë° ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í›ˆë ¨ ê·œëª¨ê°€ ìˆ˜ë§Œ ê°œì˜ GPUì— ì´ë¥´ë©°, ì´ëŠ” í›ˆë ¨ ì•ˆì •ì„±ì— í° ë„ì „ì„ ì œê¸°í•©ë‹ˆë‹¤.
- 2. ByteRobustëŠ” LLMì˜ ê²¬ê³ í•˜ê³  ì•ˆì •ì ì¸ í›ˆë ¨ì„ ìœ„í•´ ì„¤ê³„ëœ ëŒ€ê·œëª¨ GPU ì¸í”„ë¼ ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- 3. ByteRobustëŠ” LLM í›ˆë ¨ì˜ ë³‘ë ¬ì„±ê³¼ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë†’ì€ ìˆ˜ì¤€ì˜ ì¥ì•  í—ˆìš©ê³¼ ì‹ ì†í•œ ì¥ì•  ì§„ë‹¨ ë° ìœ„ì¹˜ íŒŒì•…ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. ì´ ì‹œìŠ¤í…œì€ 200,000ê°œ ì´ìƒì˜ GPUê°€ ìˆëŠ” í”Œë«í¼ì— ë°°ì¹˜ë˜ì–´ 9,600ê°œì˜ GPUë¡œ ìˆ˜í–‰ëœ 3ê°œì›” í›ˆë ¨ ì‘ì—…ì—ì„œ 97%ì˜ ETTRì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:12:50*