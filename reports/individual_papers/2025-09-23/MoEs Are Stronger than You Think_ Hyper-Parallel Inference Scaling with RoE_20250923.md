---
keywords:
  - Mixture-of-Experts
  - Roster of Experts
  - Hyper-parallel scaling
  - Large Language Model
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17238
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:57:21.891931",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Roster of Experts",
    "Hyper-parallel scaling",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.88,
    "Roster of Experts": 0.92,
    "Hyper-parallel scaling": 0.89,
    "Large Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "Mixture-of-Experts is a key model architecture discussed in the paper, relevant for linking to other works on model ensembles.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.88
      },
      {
        "surface": "Roster of Experts",
        "canonical": "Roster of Experts",
        "aliases": [
          "RoE"
        ],
        "category": "unique_technical",
        "rationale": "Roster of Experts is a novel inference algorithm introduced in the paper, offering a unique approach to model scaling.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.92
      },
      {
        "surface": "Hyper-parallel scaling",
        "canonical": "Hyper-parallel scaling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Hyper-parallel scaling is a new framework proposed in the paper, enhancing token-level prediction quality.",
        "novelty_score": 0.88,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.89
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion, providing context for the proposed scaling methods.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "inference",
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Roster of Experts",
      "resolved_canonical": "Roster of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Hyper-parallel scaling",
      "resolved_canonical": "Hyper-parallel scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.88,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17238.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17238](https://arxiv.org/abs/2509.17238)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (86.8% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (86.7% similar)
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (86.4% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (84.3% similar)
- [[2025-09-22/LongCat-Flash Technical Report_20250922|LongCat-Flash Technical Report]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]]
**âš¡ Unique Technical**: [[keywords/Roster of Experts|Roster of Experts]], [[keywords/Hyper-parallel scaling|Hyper-parallel scaling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17238v1 Announce Type: new 
Abstract: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì˜ˆì¸¡ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì¸ í•˜ì´í¼ ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” í† í° ìˆ˜ì¤€ì—ì„œ ì—¬ëŸ¬ ì¶œë ¥ ì œì•ˆì„ ê³„ì‚°í•˜ê³  ì§‘ê³„í•˜ì—¬ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì „ë¬¸ê°€ í˜¼í•©(MoE) ëª¨ë¸ì— ì ìš©ë˜ì–´ 'ì „ë¬¸ê°€ ëª…ë‹¨(RoE)'ì´ë¼ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤. RoEëŠ” í›ˆë ¨ ì—†ì´ ë‹¨ì¼ MoEë¥¼ ë™ì  MoE ì•™ìƒë¸”ë¡œ ë³€í™˜í•˜ë©°, ì „ë¬¸ê°€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì— í†µì œëœ í™•ë¥ ì„±ì„ ì£¼ì…í•´ ê° í† í°ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì „ë¬¸ê°€ë¥¼ ìƒ˜í”Œë§í•˜ê³  ê·¸ ì¶œë ¥ì„ ì§‘ê³„í•©ë‹ˆë‹¤. ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ íš¨ìœ¨ì ì¸ ë°°ì¹­ ì „ëµê³¼ íŠ¹ìˆ˜í•œ KV-ìºì‹± ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ RoEëŠ” 7B MoE ëª¨ë¸ì´ 10.5B MoE ëª¨ë¸ì˜ ì„±ëŠ¥ì„ 30% ì ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„±ê³¼ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Hyper-parallel scalingì€ í† í° ìˆ˜ì¤€ì—ì„œ ì˜ˆì¸¡ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë³´ì™„ì  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. Roster of Experts (RoE)ëŠ” Mixture-of-Experts (MoE) ëª¨ë¸ì— ì ìš©ëœ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” ì¶”ë¡  ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 3. RoEëŠ” ì „ë¬¸ê°€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì— ì œì–´ëœ í™•ë¥ ì„±ì„ ì£¼ì…í•˜ì—¬ ê° í† í°ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì „ë¬¸ê°€ë¥¼ ìƒ˜í”Œë§í•˜ê³  ê·¸ ì¶œë ¥ì„ ì§‘ê³„í•©ë‹ˆë‹¤.
- 4. RoEëŠ” íš¨ìœ¨ì ì¸ ë°°ì¹­ ì „ëµê³¼ KV-caching ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.
- 5. RoEëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ 7B MoE ëª¨ë¸ì´ 10.5B MoE ëª¨ë¸ì˜ ì„±ëŠ¥ì„ 30% ì ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 22:57:21*