---
keywords:
  - Neural Ordinary Differential Equations
  - Control Disturbance
  - Flat Minima
  - Projected Gradient Descent
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.18034
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:01:26.602605",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Ordinary Differential Equations",
    "Control Disturbance",
    "Flat Minima",
    "Projected Gradient Descent"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Ordinary Differential Equations": 0.78,
    "Control Disturbance": 0.77,
    "Flat Minima": 0.75,
    "Projected Gradient Descent": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural ODEs",
        "canonical": "Neural Ordinary Differential Equations",
        "aliases": [
          "Neural ODE",
          "NODE"
        ],
        "category": "unique_technical",
        "rationale": "Neural ODEs are a specific type of neural network model that extends the concept of ordinary differential equations to machine learning, providing a unique approach to modeling dynamic systems.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "control disturbance",
        "canonical": "Control Disturbance",
        "aliases": [
          "parameter disturbance",
          "control perturbation"
        ],
        "category": "unique_technical",
        "rationale": "Control disturbance is a key concept in the paper, focusing on the model's resilience to parameter changes, which is crucial for robustness in dynamic systems.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "flat minima",
        "canonical": "Flat Minima",
        "aliases": [
          "flat minimum",
          "flat minima concept"
        ],
        "category": "specific_connectable",
        "rationale": "The concept of flat minima is important for understanding the optimization landscape in neural networks, contributing to the robustness and generalization of models.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "projected gradient descent",
        "canonical": "Projected Gradient Descent",
        "aliases": [
          "PGD",
          "projected GD"
        ],
        "category": "specific_connectable",
        "rationale": "Projected gradient descent is a specific optimization technique used in the paper, relevant for its role in handling constraints in parameter space.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.76,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "iterative training algorithm",
      "parameter space",
      "simulations"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural ODEs",
      "resolved_canonical": "Neural Ordinary Differential Equations",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "control disturbance",
      "resolved_canonical": "Control Disturbance",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "flat minima",
      "resolved_canonical": "Flat Minima",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "projected gradient descent",
      "resolved_canonical": "Projected Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.76,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Control Disturbance Rejection in Neural ODEs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18034.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.18034](https://arxiv.org/abs/2509.18034)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.9% similar)
- [[2025-09-22/Geometric Integration for Neural Control Variates_20250922|Geometric Integration for Neural Control Variates]] (81.4% similar)
- [[2025-09-23/Were Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?_20250923|Were Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?]] (81.1% similar)
- [[2025-09-23/Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm_20250923|Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm]] (80.8% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (80.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Flat Minima|Flat Minima]], [[keywords/Projected Gradient Descent|Projected Gradient Descent]]
**âš¡ Unique Technical**: [[keywords/Neural Ordinary Differential Equations|Neural Ordinary Differential Equations]], [[keywords/Control Disturbance|Control Disturbance]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18034v1 Announce Type: new 
Abstract: In this paper, we propose an iterative training algorithm for Neural ODEs that provides models resilient to control (parameter) disturbances. The method builds on our earlier work Tuning without Forgetting-and similarly introduces training points sequentially, and updates the parameters on new data within the space of parameters that do not decrease performance on the previously learned training points-with the key difference that, inspired by the concept of flat minima, we solve a minimax problem for a non-convex non-concave functional over an infinite-dimensional control space. We develop a projected gradient descent algorithm on the space of parameters that admits the structure of an infinite-dimensional Banach subspace. We show through simulations that this formulation enables the model to effectively learn new data points and gain robustness against control disturbance.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì œì–´(ë§¤ê°œë³€ìˆ˜) êµë€ì— ê°•ì¸í•œ Neural ODE ëª¨ë¸ì„ ìœ„í•œ ë°˜ë³µì  í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ì „ ì—°êµ¬ì¸ 'Tuning without Forgetting'ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ì´ì „ í•™ìŠµ ë°ì´í„°ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì£¼ìš” ì°¨ë³„ì ì€ í‰íƒ„í•œ ìµœì†Œê°’ ê°œë…ì„ í™œìš©í•˜ì—¬, ë¬´í•œ ì°¨ì›ì˜ ì œì–´ ê³µê°„ì—ì„œ ë¹„ë³¼ë¡ ë¹„ì˜¤ëª© í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë‹ˆë§¥ìŠ¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¬´í•œ ì°¨ì›ì˜ ë°”ë‚˜í ë¶€ë¶„ê³µê°„ êµ¬ì¡°ë¥¼ ê°–ëŠ” ë§¤ê°œë³€ìˆ˜ ê³µê°„ì—ì„œ íˆ¬ì˜ ê²½ì‚¬ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ ë°©ë²•ì´ ìƒˆë¡œìš´ ë°ì´í„° í•™ìŠµê³¼ ì œì–´ êµë€ì— ëŒ€í•œ ê°•ì¸ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í–¥ìƒì‹œí‚´ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì œì–´(íŒŒë¼ë¯¸í„°) êµë€ì— ê°•ì¸í•œ Neural ODE ëª¨ë¸ì„ ìœ„í•œ ë°˜ë³µì  í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì´ì „ ì—°êµ¬ì¸ Tuning without Forgettingì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜ ì—†ì´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 3. í‰íƒ„í•œ ìµœì†Œê°’ ê°œë…ì— ì˜ê°ì„ ë°›ì•„, ë¬´í•œ ì°¨ì›ì˜ ì œì–´ ê³µê°„ì—ì„œ ë¹„ë³¼ë¡ ë¹„ì˜¤ëª© í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë‹ˆë§¥ìŠ¤ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 4. ë¬´í•œ ì°¨ì›ì˜ ë°”ë‚˜í ë¶€ë¶„ê³µê°„ êµ¬ì¡°ë¥¼ ê°–ëŠ” íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ íˆ¬ì˜ëœ ê²½ì‚¬ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤.
- 5. ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì´ ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì œì–´ êµë€ì— ëŒ€í•œ ê°•ì¸ì„±ì„ íšë“í•¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:01:26*