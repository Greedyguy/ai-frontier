---
keywords:
  - Reinforcement Learning with Verifiable Reward
  - Large Language Model
  - Chain-of-Thought Reasoning
  - Curriculum Learning Framework
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2508.07809
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:49:42.668228",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning with Verifiable Reward",
    "Large Language Model",
    "Chain-of-Thought Reasoning",
    "Curriculum Learning Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning with Verifiable Reward": 0.8,
    "Large Language Model": 0.85,
    "Chain-of-Thought Reasoning": 0.78,
    "Curriculum Learning Framework": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning with Verifiable Reward",
        "canonical": "Reinforcement Learning with Verifiable Reward",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's approach and represents a novel paradigm in reinforcement learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are crucial to the paper's methodology and are a well-established concept in machine learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Chain-of-Thought Reasoning",
        "canonical": "Chain-of-Thought Reasoning",
        "aliases": [
          "CoT Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "This reasoning method is a key component of the proposed framework and enhances understanding of LLM capabilities.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Curriculum Learning Framework",
        "canonical": "Curriculum Learning Framework",
        "aliases": [
          "Curriculum Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Curriculum learning is a strategic approach in machine learning that aligns with the paper's focus on improving LLM performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "exploration bottleneck",
      "rollout accuracy",
      "sparse rewards"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning with Verifiable Reward",
      "resolved_canonical": "Reinforcement Learning with Verifiable Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Chain-of-Thought Reasoning",
      "resolved_canonical": "Chain-of-Thought Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Curriculum Learning Framework",
      "resolved_canonical": "Curriculum Learning Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2508.07809.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2508.07809](https://arxiv.org/abs/2508.07809)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (86.7% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (86.6% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (86.5% similar)
- [[2025-09-23/Large Language Models as End-to-end Combinatorial Optimization Solvers_20250923|Large Language Models as End-to-end Combinatorial Optimization Solvers]] (86.2% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Chain-of-Thought Reasoning|Chain-of-Thought Reasoning]], [[keywords/Curriculum Learning Framework|Curriculum Learning Framework]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning with Verifiable Reward|Reinforcement Learning with Verifiable Reward]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.07809v3 Announce Type: replace 
Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models (LLMs) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes sparse, limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on teacher models for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens CoT steps to expand the space in a controlled way. The framework enables LLMs to stably learn from initially unsolved hard problems under sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì—ì„œ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì„ í™œìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ EvoCoTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ì–´ë ¤ìš´ ë¬¸ì œì—ì„œ ë³´ìƒì´ í¬ì†Œí•´ í•™ìŠµ íš¨ìœ¨ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. EvoCoTëŠ” ë‘ ë‹¨ê³„ì˜ ì‚¬ê³  ì‚¬ìŠ¬(CoT) ìµœì í™”ë¥¼ í†µí•´ íƒìƒ‰ ê³µê°„ì„ ì œí•œí•˜ê³ , ìê°€ ìƒì„± ë° ê²€ì¦ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ CoT ë‹¨ê³„ë¥¼ ì¤„ì—¬ê°€ë©° íƒìƒ‰ ê³µê°„ì„ í™•ì¥í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì´ˆê¸°ì—ëŠ” í•´ê²°ë˜ì§€ ì•Šì•˜ë˜ ì–´ë ¤ìš´ ë¬¸ì œì—ì„œë„ LLMì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. EvoCoTëŠ” Qwen, DeepSeek, Llama ë“± ë‹¤ì–‘í•œ LLMì— ì ìš©ë˜ì–´ ì´ì „ì— í•´ê²°í•˜ì§€ ëª»í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì™¸ë¶€ CoT ê°ë… ì—†ì´ë„ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, EvoCoTëŠ” ë‹¤ì–‘í•œ ê°•í™” í•™ìŠµ ë¯¸ì„¸ ì¡°ì • ë°©ë²•ê³¼ í˜¸í™˜ë˜ë©°, ì†ŒìŠ¤ ì½”ë“œë¥¼ ê³µê°œí•˜ì—¬ í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµì—ì„œ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ(RLVR)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìœ ë§í•œ íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤.
- 2. EvoCoTëŠ” ë‘ ë‹¨ê³„ì˜ ì—°ì‡„ì  ì‚¬ê³ (CoT) ìµœì í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìê¸° ì§„í™”í˜• ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 3. EvoCoTëŠ” CoT ê²½ë¡œë¥¼ ìì²´ ìƒì„± ë° ê²€ì¦í•˜ì—¬ íƒìƒ‰ ê³µê°„ì„ ì œí•œí•˜ê³ , CoT ë‹¨ê³„ë¥¼ ì ì§„ì ìœ¼ë¡œ ë‹¨ì¶•í•˜ì—¬ íƒìƒ‰ ê³µê°„ì„ í†µì œëœ ë°©ì‹ìœ¼ë¡œ í™•ì¥í•œë‹¤.
- 4. EvoCoTëŠ” Qwen, DeepSeek, Llamaë¥¼ í¬í•¨í•œ ì—¬ëŸ¬ LLM ê³„ì—´ì— ì ìš©ë˜ì–´, ì™¸ë¶€ CoT ê°ë… ì—†ì´ë„ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ë‹¤ì–‘í•œ ê°•í™” í•™ìŠµ ë¯¸ì„¸ ì¡°ì • ë°©ë²•ê³¼ í˜¸í™˜ëœë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, EvoCoTëŠ” ì´ì „ì— í•´ê²°ë˜ì§€ ì•Šì•˜ë˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆê²Œ í•˜ê³ , ì†ŒìŠ¤ ì½”ë“œë¥¼ ê³µê°œí•˜ì—¬ í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•œë‹¤.


---

*Generated on 2025-09-24 02:49:42*