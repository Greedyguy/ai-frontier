---
keywords:
  - Text-to-Image Synthesis
  - Attribute-Specific Image Prompting
  - Diffusion Model
  - Compositional Control
  - Cross-Reference Training Strategy
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.18092
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:08:47.604882",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text-to-Image Synthesis",
    "Attribute-Specific Image Prompting",
    "Diffusion Model",
    "Compositional Control",
    "Cross-Reference Training Strategy"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text-to-Image Synthesis": 0.82,
    "Attribute-Specific Image Prompting": 0.79,
    "Diffusion Model": 0.8,
    "Compositional Control": 0.77,
    "Cross-Reference Training Strategy": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "text-to-image synthesis",
        "canonical": "Text-to-Image Synthesis",
        "aliases": [
          "text-to-image generation"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper's focus on generating images from textual descriptions, making it a key linkable topic.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "attribute-specific image prompting",
        "canonical": "Attribute-Specific Image Prompting",
        "aliases": [
          "attribute-based image generation"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced by the paper, emphasizing control over specific image attributes.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "diffusion model",
        "canonical": "Diffusion Model",
        "aliases": [
          "diffusion-based model"
        ],
        "category": "broad_technical",
        "rationale": "Diffusion models are a fundamental technology used in the paper's methodology, linking to broader machine learning concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "compositional control",
        "canonical": "Compositional Control",
        "aliases": [
          "compositional generation"
        ],
        "category": "unique_technical",
        "rationale": "This term describes the paper's ability to manage multiple visual factors simultaneously, which is a unique feature of the proposed method.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.77
      },
      {
        "surface": "cross-reference training strategy",
        "canonical": "Cross-Reference Training Strategy",
        "aliases": [
          "cross-reference training"
        ],
        "category": "unique_technical",
        "rationale": "This strategy is a novel contribution of the paper, enhancing the model's ability to handle misaligned inputs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "identity preservation",
      "visual prompts"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "text-to-image synthesis",
      "resolved_canonical": "Text-to-Image Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "attribute-specific image prompting",
      "resolved_canonical": "Attribute-Specific Image Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "diffusion model",
      "resolved_canonical": "Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "compositional control",
      "resolved_canonical": "Compositional Control",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "cross-reference training strategy",
      "resolved_canonical": "Cross-Reference Training Strategy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18092.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.18092](https://arxiv.org/abs/2509.18092)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/RespoDiff_ Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation_20250922|RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation]] (83.9% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (83.7% similar)
- [[2025-09-22/AcT2I_ Evaluating and Improving Action Depiction in Text-to-Image Models_20250922|AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models]] (83.2% similar)
- [[2025-09-23/Stencil_ Subject-Driven Generation with Context Guidance_20250923|Stencil: Subject-Driven Generation with Context Guidance]] (82.7% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (82.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Diffusion Model|Diffusion Model]]
**ğŸ”— Specific Connectable**: [[keywords/Text-to-Image Synthesis|Text-to-Image Synthesis]]
**âš¡ Unique Technical**: [[keywords/Attribute-Specific Image Prompting|Attribute-Specific Image Prompting]], [[keywords/Compositional Control|Compositional Control]], [[keywords/Cross-Reference Training Strategy|Cross-Reference Training Strategy]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18092v1 Announce Type: new 
Abstract: Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: https://snap-research.github.io/composeme/.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°œì¸í™”ëœ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í•©ì„±ì—ì„œ ì¸ê°„ì˜ ì„¸ë¶€ ì†ì„±ì— ëŒ€í•œ ì •êµí•œ ì œì–´ë¥¼ í†µí•´ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì°¸ì¡° ì´ë¯¸ì§€ë¡œë¶€í„°ì˜ ì •ì²´ì„± ë³´ì¡´ì— ì¤‘ì ì„ ë‘ì—ˆì§€ë§Œ, ëª¨ë“ˆì„±ì´ ë¶€ì¡±í•˜ê³  íŠ¹ì • ì‹œê°ì  ì†ì„±ì— ëŒ€í•œ ë¶„ë¦¬ëœ ì œì–´ë¥¼ ì œê³µí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ë¨¸ë¦¬ ìŠ¤íƒ€ì¼, ì˜ìƒ, ì •ì²´ì„±ê³¼ ê°™ì€ ì¸ê°„ ì™¸ëª¨ì˜ ê°œë³„ ì¸¡ë©´ì„ ì•ˆë‚´í•˜ê¸° ìœ„í•´ ì†ì„±ë³„ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë°©ì‹ì„ ë„ì…í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì†ì„±ë³„ í† í°ì„ ì‚¬ì „ í›ˆë ¨ëœ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í™•ì‚° ëª¨ë¸ì— ì£¼ì…í•˜ì—¬ ì—¬ëŸ¬ ì‹œê°ì  ìš”ì†Œì— ëŒ€í•œ ì¡°í•©ì ì´ê³  ë¶„ë¦¬ëœ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ìì„¸ì™€ í‘œì •ì˜ ì£¼ì œë¥¼ í¬í•¨í•œ êµì°¨ ì°¸ì¡° í›ˆë ¨ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê³ , ì˜ëª» ì •ë ¬ëœ ì†ì„± ì…ë ¥ìœ¼ë¡œë¶€í„°ë„ ì¶©ì‹¤í•œ ì¶œë ¥ì„ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” ë‹¤ì¤‘ ì†ì„± êµì°¨ ì°¸ì¡° í›ˆë ¨ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë³¸ ë°©ë²•ì´ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ë”°ë¥´ëŠ” ë° ìˆì–´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì‹œê°ì  í”„ë¡¬í”„íŠ¸ì™€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ì„ ê²°í•©í•˜ì—¬ ë³´ë‹¤ êµ¬ì„± ê°€ëŠ¥í•œ ì¸ê°„ ì´ë¯¸ì§€ í•©ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì„¸ë¶„í™”ëœ ì¸ê°„ ì†ì„± ì œì–´ê°€ ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±ì„ ëª©í‘œë¡œ í•˜ë©°, íŠ¹íˆ í—¤ì–´ìŠ¤íƒ€ì¼ê³¼ ì˜ìƒê³¼ ê°™ì€ ì†ì„±ì— ëŒ€í•œ ì„¸ë°€í•œ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ì˜ ëª¨ë“ˆì„± ë¶€ì¡±ê³¼ íŠ¹ì • ì‹œê°ì  ì†ì„±ì— ëŒ€í•œ ë¶„ë¦¬ëœ ì œì–´ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì†ì„±ë³„ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë°©ì‹ì„ ë„ì…í•˜ì—¬ ê°œë³„ ì†ì„±ì— ëŒ€í•œ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ì†ì„±ë³„ í† í°ì„ ì‚¬ì „ í•™ìŠµëœ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í™•ì‚° ëª¨ë¸ì— ì£¼ì…í•˜ì—¬, ì—¬ëŸ¬ ì‹œê°ì  ìš”ì†Œì— ëŒ€í•œ ì¡°í•©ì ì´ê³  ë¶„ë¦¬ëœ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ í¬ì¦ˆì™€ í‘œì •ì„ ê°€ì§„ ì£¼ì œë¥¼ í¬í•¨í•œ êµì°¨ ì°¸ì¡° í•™ìŠµ ë°ì´í„°ì…‹ì„ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì„±ê³¼ ê°•ë ¥í•œ ë¶„ë¦¬ë¥¼ ì´‰ì§„í•˜ë©°, ë‹¤ì¤‘ ì†ì„± êµì°¨ ì°¸ì¡° í•™ìŠµ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ì  í”„ë¡¬í”„íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ë”°ë¥´ëŠ” ë° ìˆì–´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì‹œê°ì  í”„ë¡¬í”„íŠ¸ì™€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ì„ ê²°í•©í•˜ì—¬ ì¸ê°„ ì´ë¯¸ì§€ í•©ì„±ì„ ë³´ë‹¤ êµ¬ì„± ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:08:47*