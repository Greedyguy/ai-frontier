---
keywords:
  - Process Reward Models
  - Self-Denoising Monte Carlo Annotation
  - Monte Carlo Estimation
  - Large Language Model
  - Noise-Tolerant Learning Framework
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16548
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:39:40.318959",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Process Reward Models",
    "Self-Denoising Monte Carlo Annotation",
    "Monte Carlo Estimation",
    "Large Language Model",
    "Noise-Tolerant Learning Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Process Reward Models": 0.85,
    "Self-Denoising Monte Carlo Annotation": 0.88,
    "Monte Carlo Estimation": 0.8,
    "Large Language Model": 0.82,
    "Noise-Tolerant Learning Framework": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Process Reward Models",
        "canonical": "Process Reward Models",
        "aliases": [
          "PRMs"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a specific type of model that enhances reasoning in LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-Denoising Monte Carlo Annotation",
        "canonical": "Self-Denoising Monte Carlo Annotation",
        "aliases": [
          "SCAN"
        ],
        "category": "unique_technical",
        "rationale": "SCAN is a novel framework introduced in the paper, crucial for understanding the proposed methodology.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Monte Carlo Estimation",
        "canonical": "Monte Carlo Estimation",
        "aliases": [
          "MC Estimation"
        ],
        "category": "broad_technical",
        "rationale": "Monte Carlo Estimation is a foundational technique used in the paper for synthetic data generation.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are integral to the paper's context, as they are the primary application area for the proposed models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.82
      },
      {
        "surface": "Noise-Tolerant Learning Framework",
        "canonical": "Noise-Tolerant Learning Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This framework is a key innovation in the paper, addressing noise issues in synthetic data.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "synthetic data",
      "human-annotated data",
      "inference cost"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Process Reward Models",
      "resolved_canonical": "Process Reward Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-Denoising Monte Carlo Annotation",
      "resolved_canonical": "Self-Denoising Monte Carlo Annotation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Monte Carlo Estimation",
      "resolved_canonical": "Monte Carlo Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Noise-Tolerant Learning Framework",
      "resolved_canonical": "Noise-Tolerant Learning Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16548.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16548](https://arxiv.org/abs/2509.16548)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (85.9% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (84.9% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (84.5% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (81.8% similar)
- [[2025-09-22/BaseReward_ A Strong Baseline for Multimodal Reward Model_20250922|BaseReward: A Strong Baseline for Multimodal Reward Model]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Monte Carlo Estimation|Monte Carlo Estimation]], [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Process Reward Models|Process Reward Models]], [[keywords/Self-Denoising Monte Carlo Annotation|Self-Denoising Monte Carlo Annotation]], [[keywords/Noise-Tolerant Learning Framework|Noise-Tolerant Learning Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16548v1 Announce Type: new 
Abstract: Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰ì„ ë•ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM)ì˜ ê°œë°œì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì¸ê°„ ì£¼ì„ ë°ì´í„°ëŠ” ë¹„ìš©ì´ ë†’ê³  í™•ì¥ì„±ì´ ì œí•œì ì´ë¯€ë¡œ, ì €ìëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ(MC) ì¶”ì •ì—ì„œ ìƒì„±ëœ í•©ì„± ë°ì´í„°ë¥¼ ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë°ì´í„°ëŠ” ë†’ì€ ì¡ìŒ ë¹„ìœ¨ë¡œ ì¸í•´ ê³¼ì í•© ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ìëŠ” ìê¸°-ì¡ìŒ ì œê±° ëª¬í…Œì¹´ë¥¼ë¡œ ì£¼ì„(SCAN)ì´ë¼ëŠ” íš¨ìœ¨ì ì¸ ë°ì´í„° í•©ì„± ë° ì¡ìŒ ë‚´ì„± í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ ì‚¬í•­ìœ¼ë¡œëŠ” ê²½ëŸ‰ ëª¨ë¸ë„ ìê¸°-ì¡ìŒ ì œê±° ì „ëµì„ í†µí•´ ê³ í’ˆì§ˆ ì£¼ì„ì„ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, PRMì´ ì ì€ ì¶”ë¡  ë¹„ìš©ìœ¼ë¡œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë˜í•œ, SCANì„ í†µí•´ í•™ìŠµëœ PRMì€ ì•½í•œ ê°ë…ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ëŒ€ê·œëª¨ ì¸ê°„ ì£¼ì„ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ëª¨ë¸ë³´ë‹¤ë„ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. SCANì€ í™•ì¥ì„±ê³¼ ë¹„ìš© íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ PRM í›ˆë ¨ì— í° ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ìˆ˜í•™ì  ì¶”ë¡ ê³¼ ê°™ì€ ë³µì¡í•œ ì‘ì—…ì— íš¨ê³¼ì ì´ì§€ë§Œ, ì¸ê°„ ì£¼ì„ ë°ì´í„°ì˜ ë†’ì€ ë¹„ìš©ê³¼ ì œí•œëœ í™•ì¥ì„± ë•Œë¬¸ì— ê°œë°œì´ ì–´ë µë‹¤.
- 2. ëª¬í…Œì¹´ë¥¼ë¡œ(MC) ì¶”ì •ì—ì„œ ìƒì„±ëœ í•©ì„± ë°ì´í„°ëŠ” ìœ ë§í•œ ëŒ€ì•ˆì´ì§€ë§Œ, ë†’ì€ ë…¸ì´ì¦ˆ ë¹„ìœ¨ë¡œ ì¸í•´ ê³¼ì í•©ì„ ì´ˆë˜í•˜ê³  ëŒ€ê·œëª¨ í•™ìŠµì„ ë°©í•´í•  ìˆ˜ ìˆë‹¤.
- 3. Self-Denoising Monte Carlo Annotation(SCAN)ì€ íš¨ìœ¨ì ì¸ ë°ì´í„° í•©ì„±ê³¼ ë…¸ì´ì¦ˆ ë‚´ì„± í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, ê²½ëŸ‰ ëª¨ë¸ë„ ê³ í’ˆì§ˆ ì£¼ì„ì„ ìƒì„±í•˜ì—¬ PRMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 4. SCANì„ í†µí•´ PRMì€ ì•½í•œ ê°ë…ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë©°, ProcessBenchì—ì„œ F1 ì ìˆ˜ë¥¼ 19.9ì—ì„œ 59.1ë¡œ 39.2ì  í–¥ìƒì‹œì¼°ë‹¤.
- 5. SCANì€ ëŒ€ê·œëª¨ ì¸ê°„ ì£¼ì„ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ê°•ë ¥í•œ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ë©°, í•©ì„± ë°ì´í„°ì˜ í™•ì¥ì— ë”°ë¼ ì„±ëŠ¥ì´ ê³„ì† í–¥ìƒë˜ì–´ ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ PRM í•™ìŠµì˜ ì ì¬ë ¥ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-24 01:39:40*