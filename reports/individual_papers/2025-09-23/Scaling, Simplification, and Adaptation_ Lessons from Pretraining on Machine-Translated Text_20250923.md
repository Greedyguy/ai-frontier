---
keywords:
  - Multilingual Pretraining
  - Machine Translation
  - Source-side Simplification
  - Pretraining on MT-derived Data
  - Cultural Nuance Tasks
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17317
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:51:53.979505",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multilingual Pretraining",
    "Machine Translation",
    "Source-side Simplification",
    "Pretraining on MT-derived Data",
    "Cultural Nuance Tasks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multilingual Pretraining": 0.78,
    "Machine Translation": 0.8,
    "Source-side Simplification": 0.77,
    "Pretraining on MT-derived Data": 0.79,
    "Cultural Nuance Tasks": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multilingual Pretraining",
        "canonical": "Multilingual Pretraining",
        "aliases": [
          "Multilingual Training"
        ],
        "category": "unique_technical",
        "rationale": "This concept addresses the challenge of language imbalance in pretraining, which is central to the paper's investigation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Machine Translation",
        "canonical": "Machine Translation",
        "aliases": [
          "MT"
        ],
        "category": "broad_technical",
        "rationale": "Machine Translation is a core method used in the study to generate data for pretraining, linking it to broader NLP research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Source-side Simplification",
        "canonical": "Source-side Simplification",
        "aliases": [
          "Simplifying Source Text"
        ],
        "category": "unique_technical",
        "rationale": "This technique is explored as a potential method to improve model generalization, making it a unique focus of the paper.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Pretraining on MT-derived Data",
        "canonical": "Pretraining on MT-derived Data",
        "aliases": [
          "MT Data Pretraining"
        ],
        "category": "unique_technical",
        "rationale": "The paper investigates the effectiveness of this pretraining strategy, which is crucial for understanding its findings.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "Cultural Nuance Tasks",
        "canonical": "Cultural Nuance Tasks",
        "aliases": [
          "Culturally Sensitive Tasks"
        ],
        "category": "unique_technical",
        "rationale": "These tasks highlight the limitations of MT-pretrained models, emphasizing the need for native data exposure.",
        "novelty_score": 0.66,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "data wall",
      "model capacity",
      "native text"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multilingual Pretraining",
      "resolved_canonical": "Multilingual Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Machine Translation",
      "resolved_canonical": "Machine Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Source-side Simplification",
      "resolved_canonical": "Source-side Simplification",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Pretraining on MT-derived Data",
      "resolved_canonical": "Pretraining on MT-derived Data",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Cultural Nuance Tasks",
      "resolved_canonical": "Cultural Nuance Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.66,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17317.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17317](https://arxiv.org/abs/2509.17317)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (85.6% similar)
- [[2025-09-22/Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation_20250922|Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation]] (84.3% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (83.8% similar)
- [[2025-09-23/Rethinking the Role of Text Complexity in Language Model Pretraining_20250923|Rethinking the Role of Text Complexity in Language Model Pretraining]] (83.2% similar)
- [[2025-09-19/Translate, then Detect_ Leveraging Machine Translation for Cross-Lingual Toxicity Classification_20250919|Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Machine Translation|Machine Translation]]
**âš¡ Unique Technical**: [[keywords/Multilingual Pretraining|Multilingual Pretraining]], [[keywords/Source-side Simplification|Source-side Simplification]], [[keywords/Pretraining on MT-derived Data|Pretraining on MT-derived Data]], [[keywords/Cultural Nuance Tasks|Cultural Nuance Tasks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17317v1 Announce Type: cross 
Abstract: Most languages lack sufficient data for large-scale monolingual pretraining, creating a "data wall." Multilingual pretraining helps but is limited by language imbalance and the "curse of multilinguality." An alternative is to translate high-resource text with machine translation (MT), which raises three questions: (1) How does MT-derived data scale with model capacity? (2) Can source-side transformations (e.g., simplifying English with an LLM) improve generalization to native text? (3) How well do models pretrained on MT-derived data adapt when continually trained on limited native text? We investigate these questions by translating English into Indonesian and Tamil--two typologically distant, lower-resource languages--and pretraining GPT-2 models (124M-774M) on native or MT-derived corpora from raw and LLM-simplified English. We evaluate cross-entropy loss on native text, along with accuracy on syntactic probes and downstream tasks. Our results show that (1) MT-pretrained models benefit from scaling; (2) source-side simplification harms generalization to native text; and (3) adapting MT-pretrained models on native text often yields better performance than native-only models, even with less native data. However, tasks requiring cultural nuance (e.g., toxicity detection) demand more exposure to native data.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ë‹¨ì¼ì–¸ì–´ ì‚¬ì „í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì–¸ì–´ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ, ê¸°ê³„ ë²ˆì—­(MT)ì„ í™œìš©í•œ ëŒ€ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì˜ì–´ë¥¼ ì¸ë„ë„¤ì‹œì•„ì–´ì™€ íƒ€ë°€ì–´ë¡œ ë²ˆì—­í•˜ì—¬ GPT-2 ëª¨ë¸ì„ ì‚¬ì „í•™ìŠµí•˜ê³ , ì›ì–´ë¯¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¼ë°˜í™”ì™€ ì ì‘ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (1) MTë¡œ ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸ì€ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤. (2) ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœí™”í•˜ëŠ” ê²ƒì€ ì›ì–´ë¯¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¼ë°˜í™”ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. (3) MT ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸ì„ ì›ì–´ë¯¼ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµí•˜ë©´, ì œí•œëœ ì›ì–´ë¯¼ ë°ì´í„°ë§Œ ì‚¬ìš©í•œ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¬¸í™”ì  ë‰˜ì•™ìŠ¤ë¥¼ ìš”êµ¬í•˜ëŠ” ì‘ì—…ì—ëŠ” ë” ë§ì€ ì›ì–´ë¯¼ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤êµ­ì–´ ì‚¬ì „ í•™ìŠµì€ ì–¸ì–´ ë¶ˆê· í˜•ê³¼ ë‹¤êµ­ì–´ì˜ ì €ì£¼ë¡œ ì¸í•´ í•œê³„ê°€ ìˆë‹¤.
- 2. ê¸°ê³„ ë²ˆì—­ì„ í†µí•œ ë°ì´í„°ëŠ” ëª¨ë¸ ìš©ëŸ‰ì´ ì¦ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.
- 3. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœí™”í•˜ëŠ” ê²ƒì€ ì›ì–´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¼ë°˜í™”ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
- 4. ê¸°ê³„ ë²ˆì—­ ë°ì´í„°ë¥¼ ì‚¬ì „ í•™ìŠµí•œ ëª¨ë¸ì€ ì œí•œëœ ì›ì–´ í…ìŠ¤íŠ¸ë¡œ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•  ë•Œ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.
- 5. ë¬¸í™”ì  ë‰˜ì•™ìŠ¤ê°€ í•„ìš”í•œ ì‘ì—…ì—ì„œëŠ” ë” ë§ì€ ì›ì–´ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤.


---

*Generated on 2025-09-23 23:51:53*