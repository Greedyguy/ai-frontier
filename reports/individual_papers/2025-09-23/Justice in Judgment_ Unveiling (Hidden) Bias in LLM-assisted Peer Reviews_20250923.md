---
keywords:
  - Large Language Model
  - Peer Review Bias
  - Affiliation Bias
  - Gender Bias
  - Token-based Soft Ratings
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.13400
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:30:05.132441",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Peer Review Bias",
    "Affiliation Bias",
    "Gender Bias",
    "Token-based Soft Ratings"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Peer Review Bias": 0.7,
    "Affiliation Bias": 0.65,
    "Gender Bias": 0.68,
    "Token-based Soft Ratings": 0.6
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's investigation, linking to existing discussions on LLMs.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Peer Review Bias",
        "canonical": "Peer Review Bias",
        "aliases": [
          "Bias in Peer Review"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the specific issue of bias in the context of peer reviews, a novel investigation area.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Affiliation Bias",
        "canonical": "Affiliation Bias",
        "aliases": [
          "Institutional Bias"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific type of bias that could connect to broader discussions on institutional influence.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.65
      },
      {
        "surface": "Gender Preferences",
        "canonical": "Gender Bias",
        "aliases": [
          "Gender Preferences"
        ],
        "category": "unique_technical",
        "rationale": "Addresses gender-related biases, linking to discussions on gender equality in academia.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.7,
        "link_intent_score": 0.68
      },
      {
        "surface": "Token-based Soft Ratings",
        "canonical": "Token-based Soft Ratings",
        "aliases": [
          "Soft Ratings"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept in evaluating biases, potentially linking to technical discussions on rating systems.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.6
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Peer Review Bias",
      "resolved_canonical": "Peer Review Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Affiliation Bias",
      "resolved_canonical": "Affiliation Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Gender Preferences",
      "resolved_canonical": "Gender Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.7,
        "link_intent": 0.68
      }
    },
    {
      "candidate_surface": "Token-based Soft Ratings",
      "resolved_canonical": "Token-based Soft Ratings",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.6
      }
    }
  ]
}
-->

# Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.13400.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.13400](https://arxiv.org/abs/2509.13400)

## 🔗 유사한 논문
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (87.5% similar)
- [[2025-09-22/Bias Beware_ The Impact of Cognitive Biases on LLM-Driven Product Recommendations_20250922|Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations]] (86.5% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (86.4% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (86.4% similar)
- [[2025-09-19/Judging with Many Minds_ Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge_20250919|Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge]] (86.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Peer Review Bias|Peer Review Bias]], [[keywords/Affiliation Bias|Affiliation Bias]], [[keywords/Gender Bias|Gender Bias]], [[keywords/Token-based Soft Ratings|Token-based Soft Ratings]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13400v2 Announce Type: replace-cross 
Abstract: The adoption of large language models (LLMs) is transforming the peer review process, from assisting reviewers in writing more detailed evaluations to generating entire reviews automatically. While these capabilities offer exciting opportunities, they also raise critical concerns about fairness and reliability. In this paper, we investigate bias in LLM-generated peer reviews by conducting controlled experiments on sensitive metadata, including author affiliation and gender. Our analysis consistently shows affiliation bias favoring institutions highly ranked on common academic rankings. Additionally, we find some gender preferences, which, even though subtle in magnitude, have the potential to compound over time. Notably, we uncover implicit biases that become more evident with token-based soft ratings.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 도입이 동료 평가 과정에 미치는 영향을 조사합니다. 특히, LLM이 생성한 리뷰에서 발생할 수 있는 편향 문제를 중점적으로 다룹니다. 저자 소속 및 성별과 같은 민감한 메타데이터를 활용한 실험을 통해, LLM이 높은 순위의 학술 기관에 유리한 소속 편향을 보인다는 것을 발견했습니다. 또한, 성별에 대한 미묘한 선호가 시간이 지남에 따라 누적될 수 있음을 확인했습니다. 특히, 토큰 기반의 소프트 평가에서 이러한 암묵적 편향이 더 명확하게 드러났습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)의 도입이 동료 평가 과정에 변화를 가져오고 있으며, 이는 공정성과 신뢰성에 대한 우려를 제기한다.
- 2. LLM이 생성한 동료 평가에서 저자 소속과 성별과 같은 민감한 메타데이터에 대한 편향을 조사하였다.
- 3. 분석 결과, 일반적인 학술 순위에서 높은 순위를 차지한 기관에 유리한 소속 편향이 일관되게 나타났다.
- 4. 성별 선호도도 발견되었으며, 이는 미묘하지만 시간이 지남에 따라 누적될 가능성이 있다.
- 5. 토큰 기반의 소프트 평가에서 암묵적인 편향이 더욱 명확하게 드러났다.


---

*Generated on 2025-09-24 01:30:05*