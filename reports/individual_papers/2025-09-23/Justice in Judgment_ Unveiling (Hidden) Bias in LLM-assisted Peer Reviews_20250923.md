---
keywords:
  - Large Language Model
  - Peer Review Bias
  - Affiliation Bias
  - Gender Bias
  - Token-based Soft Ratings
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.13400
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:30:05.132441",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Peer Review Bias",
    "Affiliation Bias",
    "Gender Bias",
    "Token-based Soft Ratings"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Peer Review Bias": 0.7,
    "Affiliation Bias": 0.65,
    "Gender Bias": 0.68,
    "Token-based Soft Ratings": 0.6
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's investigation, linking to existing discussions on LLMs.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Peer Review Bias",
        "canonical": "Peer Review Bias",
        "aliases": [
          "Bias in Peer Review"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the specific issue of bias in the context of peer reviews, a novel investigation area.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Affiliation Bias",
        "canonical": "Affiliation Bias",
        "aliases": [
          "Institutional Bias"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific type of bias that could connect to broader discussions on institutional influence.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.65
      },
      {
        "surface": "Gender Preferences",
        "canonical": "Gender Bias",
        "aliases": [
          "Gender Preferences"
        ],
        "category": "unique_technical",
        "rationale": "Addresses gender-related biases, linking to discussions on gender equality in academia.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.7,
        "link_intent_score": 0.68
      },
      {
        "surface": "Token-based Soft Ratings",
        "canonical": "Token-based Soft Ratings",
        "aliases": [
          "Soft Ratings"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept in evaluating biases, potentially linking to technical discussions on rating systems.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.6
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Peer Review Bias",
      "resolved_canonical": "Peer Review Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Affiliation Bias",
      "resolved_canonical": "Affiliation Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Gender Preferences",
      "resolved_canonical": "Gender Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.7,
        "link_intent": 0.68
      }
    },
    {
      "candidate_surface": "Token-based Soft Ratings",
      "resolved_canonical": "Token-based Soft Ratings",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.6
      }
    }
  ]
}
-->

# Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.13400.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.13400](https://arxiv.org/abs/2509.13400)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (87.5% similar)
- [[2025-09-22/Bias Beware_ The Impact of Cognitive Biases on LLM-Driven Product Recommendations_20250922|Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations]] (86.5% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (86.4% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (86.4% similar)
- [[2025-09-19/Judging with Many Minds_ Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge_20250919|Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge]] (86.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Peer Review Bias|Peer Review Bias]], [[keywords/Affiliation Bias|Affiliation Bias]], [[keywords/Gender Bias|Gender Bias]], [[keywords/Token-based Soft Ratings|Token-based Soft Ratings]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13400v2 Announce Type: replace-cross 
Abstract: The adoption of large language models (LLMs) is transforming the peer review process, from assisting reviewers in writing more detailed evaluations to generating entire reviews automatically. While these capabilities offer exciting opportunities, they also raise critical concerns about fairness and reliability. In this paper, we investigate bias in LLM-generated peer reviews by conducting controlled experiments on sensitive metadata, including author affiliation and gender. Our analysis consistently shows affiliation bias favoring institutions highly ranked on common academic rankings. Additionally, we find some gender preferences, which, even though subtle in magnitude, have the potential to compound over time. Notably, we uncover implicit biases that become more evident with token-based soft ratings.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë„ì…ì´ ë™ë£Œ í‰ê°€ ê³¼ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. íŠ¹íˆ, LLMì´ ìƒì„±í•œ ë¦¬ë·°ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” í¸í–¥ ë¬¸ì œë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì €ì ì†Œì† ë° ì„±ë³„ê³¼ ê°™ì€ ë¯¼ê°í•œ ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ ì‹¤í—˜ì„ í†µí•´, LLMì´ ë†’ì€ ìˆœìœ„ì˜ í•™ìˆ  ê¸°ê´€ì— ìœ ë¦¬í•œ ì†Œì† í¸í–¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„±ë³„ì— ëŒ€í•œ ë¯¸ë¬˜í•œ ì„ í˜¸ê°€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ëˆ„ì ë  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, í† í° ê¸°ë°˜ì˜ ì†Œí”„íŠ¸ í‰ê°€ì—ì„œ ì´ëŸ¬í•œ ì•”ë¬µì  í¸í–¥ì´ ë” ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë„ì…ì´ ë™ë£Œ í‰ê°€ ê³¼ì •ì— ë³€í™”ë¥¼ ê°€ì ¸ì˜¤ê³  ìˆìœ¼ë©°, ì´ëŠ” ê³µì •ì„±ê³¼ ì‹ ë¢°ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ì œê¸°í•œë‹¤.
- 2. LLMì´ ìƒì„±í•œ ë™ë£Œ í‰ê°€ì—ì„œ ì €ì ì†Œì†ê³¼ ì„±ë³„ê³¼ ê°™ì€ ë¯¼ê°í•œ ë©”íƒ€ë°ì´í„°ì— ëŒ€í•œ í¸í–¥ì„ ì¡°ì‚¬í•˜ì˜€ë‹¤.
- 3. ë¶„ì„ ê²°ê³¼, ì¼ë°˜ì ì¸ í•™ìˆ  ìˆœìœ„ì—ì„œ ë†’ì€ ìˆœìœ„ë¥¼ ì°¨ì§€í•œ ê¸°ê´€ì— ìœ ë¦¬í•œ ì†Œì† í¸í–¥ì´ ì¼ê´€ë˜ê²Œ ë‚˜íƒ€ë‚¬ë‹¤.
- 4. ì„±ë³„ ì„ í˜¸ë„ë„ ë°œê²¬ë˜ì—ˆìœ¼ë©°, ì´ëŠ” ë¯¸ë¬˜í•˜ì§€ë§Œ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ëˆ„ì ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.
- 5. í† í° ê¸°ë°˜ì˜ ì†Œí”„íŠ¸ í‰ê°€ì—ì„œ ì•”ë¬µì ì¸ í¸í–¥ì´ ë”ìš± ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚¬ë‹¤.


---

*Generated on 2025-09-24 01:30:05*