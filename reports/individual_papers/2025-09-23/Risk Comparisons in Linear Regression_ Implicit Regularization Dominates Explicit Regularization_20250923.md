---
keywords:
  - Gradient Descent
  - Ridge Regression
  - Stochastic Gradient Descent
  - Benign Overfitting
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17251
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:21:13.547838",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Gradient Descent",
    "Ridge Regression",
    "Stochastic Gradient Descent",
    "Benign Overfitting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Gradient Descent": 0.78,
    "Ridge Regression": 0.72,
    "Stochastic Gradient Descent": 0.75,
    "Benign Overfitting": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Gradient Descent",
        "canonical": "Gradient Descent",
        "aliases": [
          "GD"
        ],
        "category": "broad_technical",
        "rationale": "Gradient Descent is a fundamental optimization algorithm widely used in machine learning, making it a strong link across various studies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Ridge Regression",
        "canonical": "Ridge Regression",
        "aliases": [
          "Ridge"
        ],
        "category": "specific_connectable",
        "rationale": "Ridge Regression is a specific regularization technique that is crucial for understanding the paper's comparative analysis.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      },
      {
        "surface": "Stochastic Gradient Descent",
        "canonical": "Stochastic Gradient Descent",
        "aliases": [
          "SGD"
        ],
        "category": "specific_connectable",
        "rationale": "Stochastic Gradient Descent is a key algorithm in the paper's analysis, providing a basis for comparison with Gradient Descent.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Benign Overfitting",
        "canonical": "Benign Overfitting",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Benign Overfitting is a unique concept explored in the paper, highlighting specific conditions where GD can be suboptimal.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "minimax optimal",
      "well-specified linear regression problem"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Gradient Descent",
      "resolved_canonical": "Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Ridge Regression",
      "resolved_canonical": "Ridge Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Stochastic Gradient Descent",
      "resolved_canonical": "Stochastic Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Benign Overfitting",
      "resolved_canonical": "Benign Overfitting",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17251.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17251](https://arxiv.org/abs/2509.17251)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (82.0% similar)
- [[2025-09-17/Online Bayesian Risk-Averse Reinforcement Learning_20250917|Online Bayesian Risk-Averse Reinforcement Learning]] (79.7% similar)
- [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (79.3% similar)
- [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (79.0% similar)
- [[2025-09-19/Evaluating Supervised Learning Models for Fraud Detection_ A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data_20250919|Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data]] (78.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Gradient Descent|Gradient Descent]]
**ğŸ”— Specific Connectable**: [[keywords/Ridge Regression|Ridge Regression]], [[keywords/Stochastic Gradient Descent|Stochastic Gradient Descent]]
**âš¡ Unique Technical**: [[keywords/Benign Overfitting|Benign Overfitting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17251v1 Announce Type: cross 
Abstract: Existing theory suggests that for linear regression problems categorized by capacity and source conditions, gradient descent (GD) is always minimax optimal, while both ridge regression and online stochastic gradient descent (SGD) are polynomially suboptimal for certain categories of such problems. Moving beyond minimax theory, this work provides instance-wise comparisons of the finite-sample risks for these algorithms on any well-specified linear regression problem.
  Our analysis yields three key findings. First, GD dominates ridge regression: with comparable regularization, the excess risk of GD is always within a constant factor of ridge, but ridge can be polynomially worse even when tuned optimally. Second, GD is incomparable with SGD. While it is known that for certain problems GD can be polynomially better than SGD, the reverse is also true: we construct problems, inspired by benign overfitting theory, where optimally stopped GD is polynomially worse. Finally, GD dominates SGD for a significant subclass of problems -- those with fast and continuously decaying covariance spectra -- which includes all problems satisfying the standard capacity condition.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì„ í˜• íšŒê·€ ë¬¸ì œì—ì„œ ê²½ì‚¬ í•˜ê°•ë²•(GD), ë¦¿ì§€ íšŒê·€, ì˜¨ë¼ì¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì˜ ìœ í•œ ìƒ˜í”Œ ìœ„í—˜ì„ ë¹„êµí•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì²«ì§¸, GDëŠ” ë¦¿ì§€ íšŒê·€ë³´ë‹¤ í•­ìƒ ìš°ìˆ˜í•˜ë©°, ë¦¿ì§€ê°€ ìµœì  ì¡°ì •ë˜ë”ë¼ë„ GDì˜ ì´ˆê³¼ ìœ„í—˜ì´ ì¼ì •í•œ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, GDì™€ SGDëŠ” ë¹„êµí•  ìˆ˜ ì—†ìœ¼ë©°, íŠ¹ì • ë¬¸ì œì—ì„œëŠ” GDê°€ SGDë³´ë‹¤ ìš°ìˆ˜í•˜ê±°ë‚˜ ê·¸ ë°˜ëŒ€ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, GDëŠ” ë¹ ë¥´ê²Œ ê°ì†Œí•˜ëŠ” ê³µë¶„ì‚° ìŠ¤í™íŠ¸ëŸ¼ì„ ê°€ì§„ ë¬¸ì œì—ì„œ SGDë³´ë‹¤ ìš°ìˆ˜í•˜ë©°, ì´ëŠ” í‘œì¤€ ìš©ëŸ‰ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ë¬¸ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì„ í˜• íšŒê·€ ë¬¸ì œì—ì„œ ê²½ì‚¬ í•˜ê°•ë²•(GD)ì€ ë¦¬ì§€ íšŒê·€ë³´ë‹¤ í•­ìƒ ìš°ìˆ˜í•˜ë©°, ë¦¬ì§€ íšŒê·€ëŠ” ìµœì í™”ë˜ë”ë¼ë„ ë‹¤í•­ì‹ì ìœ¼ë¡œ ë” ë‚˜ì  ìˆ˜ ìˆë‹¤.
- 2. ê²½ì‚¬ í•˜ê°•ë²•(GD)ê³¼ ì˜¨ë¼ì¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì€ ë¹„êµí•  ìˆ˜ ì—†ìœ¼ë©°, íŠ¹ì • ë¬¸ì œì—ì„œëŠ” GDê°€ SGDë³´ë‹¤ ë‹¤í•­ì‹ì ìœ¼ë¡œ ìš°ìˆ˜í•  ìˆ˜ ìˆì§€ë§Œ ê·¸ ë°˜ëŒ€ë„ ê°€ëŠ¥í•˜ë‹¤.
- 3. ê²½ì‚¬ í•˜ê°•ë²•(GD)ì€ ë¹ ë¥´ê³  ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ê³µë¶„ì‚° ìŠ¤í™íŠ¸ëŸ¼ì„ ê°€ì§„ ë¬¸ì œë“¤, ì¦‰ í‘œì¤€ ìš©ëŸ‰ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ë¬¸ì œì— ëŒ€í•´ ì˜¨ë¼ì¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì„ ì§€ë°°í•œë‹¤.


---

*Generated on 2025-09-24 02:21:13*