---
keywords:
  - Nested Subspace Networks
  - Large Language Model
  - Compute-Performance Frontier
  - Uncertainty-Aware Objective
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17874
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:59:00.778653",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Nested Subspace Networks",
    "Large Language Model",
    "Compute-Performance Frontier",
    "Uncertainty-Aware Objective"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Nested Subspace Networks": 0.88,
    "Large Language Model": 0.85,
    "Compute-Performance Frontier": 0.8,
    "Uncertainty-Aware Objective": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Nested Subspace Networks",
        "canonical": "Nested Subspace Networks",
        "aliases": [
          "NSNs"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel architectural paradigm that enhances model adaptability across compute budgets.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Key component in the application of NSNs, relevant for linking with foundational model discussions.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "compute-performance frontier",
        "canonical": "Compute-Performance Frontier",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Describes the balance between computational efficiency and model performance, crucial for adaptive models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "uncertainty-aware objective",
        "canonical": "Uncertainty-Aware Objective",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights a method for optimizing models by balancing contributions based on difficulty, relevant for model training.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "efficiency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Nested Subspace Networks",
      "resolved_canonical": "Nested Subspace Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "compute-performance frontier",
      "resolved_canonical": "Compute-Performance Frontier",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "uncertainty-aware objective",
      "resolved_canonical": "Uncertainty-Aware Objective",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Deep Hierarchical Learning with Nested Subspace Networks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17874.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17874](https://arxiv.org/abs/2509.17874)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (83.1% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (83.1% similar)
- [[2025-09-17/NIRVANA_ Structured pruning reimagined for large language models compression_20250917|NIRVANA: Structured pruning reimagined for large language models compression]] (82.4% similar)
- [[2025-09-19/Hierarchical Federated Learning for Social Network with Mobility_20250919|Hierarchical Federated Learning for Social Network with Mobility]] (82.2% similar)
- [[2025-09-22/Latent Zoning Network_ A Unified Principle for Generative Modeling, Representation Learning, and Classification_20250922|Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Uncertainty-Aware Objective|Uncertainty-Aware Objective]]
**âš¡ Unique Technical**: [[keywords/Nested Subspace Networks|Nested Subspace Networks]]
**ğŸš€ Evolved Concepts**: [[keywords/Compute-Performance Frontier|Compute-Performance Frontier]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17874v1 Announce Type: new 
Abstract: Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ìì› ì œì•½ì´ë‚˜ ë™ì  í™˜ê²½ì—ì„œì˜ íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ì‹ ê²½ë§ í™œìš©ì„ ìœ„í•œ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ íŒ¨ëŸ¬ë‹¤ì„ì¸ Nested Subspace Networks(NSNs)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. NSNsëŠ” ë‹¨ì¼ ëª¨ë¸ì´ ì¶”ë¡  ì‹œ ë‹¤ì–‘í•œ ê³„ì‚° ì˜ˆì‚°ì— ë§ì¶° ë™ì ìœ¼ë¡œ ì¡°ì •ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì„ í˜• ê³„ì¸µì„ ì¬êµ¬ì„±í•˜ì—¬ íŠ¹ì • ë­í¬ì—ì„œ ê³„ì‚°ëœ í•¨ìˆ˜ê°€ ë” ë†’ì€ ë­í¬ì˜ í•¨ìˆ˜ì˜ ì—„ê²©í•œ ë¶€ë¶„ ê³µê°„ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ ëª©í‘œë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë­í¬ì˜ ê¸°ì—¬ë„ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, NSNsëŠ” ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì— ì ìš©ë˜ì–´ ê³„ì‚°ëŸ‰ì„ 50% ì¤„ì´ë©´ì„œë„ ì •í™•ë„ ì†ì‹¤ì„ 5%ë¡œ ì œí•œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. NSNsëŠ” ì°¨ì„¸ëŒ€ ì ì‘í˜• ëª¨ë¸ ê°œë°œì— ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì‹ ê²½ë§ì˜ ê³ ì •ëœ ê³„ì‚° ì˜ˆì‚° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Nested Subspace Networks(NSNs)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. NSNsëŠ” ë‹¨ì¼ ëª¨ë¸ì„ í†µí•´ ì¶”ë¡  ì‹œ ë‹¤ì–‘í•œ ê³„ì‚° ì˜ˆì‚°ì— ë§ì¶° ë™ì ì´ê³  ì„¸ë°€í•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤.
- 3. NSNsëŠ” ì„ í˜• ë ˆì´ì–´ë¥¼ ì¬ë§¤ê°œë³€ìˆ˜í™”í•˜ì—¬ íŠ¹ì • ë­í¬ì—ì„œ ê³„ì‚°ëœ í•¨ìˆ˜ê°€ ë” ë†’ì€ ë­í¬ì˜ í•¨ìˆ˜ì˜ ì—„ê²©í•œ ë¶€ë¶„ ê³µê°„ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
- 4. NSNsëŠ” ì‚¬ì „ í•™ìŠµëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì ìš©ë˜ì–´ ë§¤ë„ëŸ½ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê³„ì‚°-ì„±ëŠ¥ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 5. NSNsë¥¼ ì ìš©í•œ ëª¨ë¸ì€ ì¶”ë¡  ì‹œ FLOPsë¥¼ 50% ì¤„ì´ë©´ì„œë„ ì •í™•ë„ ì†ì‹¤ì„ 5% í¬ì¸íŠ¸ë¡œ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:59:00*