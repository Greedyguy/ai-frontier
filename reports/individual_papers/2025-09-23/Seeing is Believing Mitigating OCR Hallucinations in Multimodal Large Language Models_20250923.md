---
keywords:
  - Multimodal Learning
  - OCR Hallucination
  - Vision-Faithful Reasoning
  - Reinforcement Learning
  - KIE-HVQA Benchmark
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2506.20168
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:25:35.164357",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "OCR Hallucination",
    "Vision-Faithful Reasoning",
    "Reinforcement Learning",
    "KIE-HVQA Benchmark"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "OCR Hallucination": 0.78,
    "Vision-Faithful Reasoning": 0.77,
    "Reinforcement Learning": 0.7,
    "KIE-HVQA Benchmark": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "multimodal large language models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal LLMs"
        ],
        "category": "specific_connectable",
        "rationale": "This term connects advancements in integrating textual and visual information, linking to broader multimodal research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "OCR hallucination",
        "canonical": "OCR Hallucination",
        "aliases": [
          "optical character recognition hallucination"
        ],
        "category": "unique_technical",
        "rationale": "This unique phenomenon is central to the paper's focus on visual degradation and model accuracy.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "vision-faithful reasoning",
        "canonical": "Vision-Faithful Reasoning",
        "aliases": [
          "accurate visual reasoning"
        ],
        "category": "unique_technical",
        "rationale": "This concept is key to understanding how models can avoid hallucinations by accurately interpreting visual data.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "reinforcement learning framework",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL framework"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement learning is a foundational technique applied to improve model performance in the study.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "KIE-HVQA",
        "canonical": "KIE-HVQA Benchmark",
        "aliases": [
          "KIE-HVQA dataset"
        ],
        "category": "unique_technical",
        "rationale": "This benchmark is a novel contribution of the paper, essential for evaluating OCR hallucination.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "document understanding",
      "visual degradation",
      "linguistic priors"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "multimodal large language models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "OCR hallucination",
      "resolved_canonical": "OCR Hallucination",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "vision-faithful reasoning",
      "resolved_canonical": "Vision-Faithful Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "reinforcement learning framework",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "KIE-HVQA",
      "resolved_canonical": "KIE-HVQA Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.20168.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2506.20168](https://arxiv.org/abs/2506.20168)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Uncertainty-Aware Attention Heads_ Efficient Unsupervised Uncertainty Quantification for LLMs_20250923|Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs]] (85.8% similar)
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (84.8% similar)
- [[2025-09-23/SafeEraser_ Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning_20250923|SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning]] (84.1% similar)
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (84.0% similar)
- [[2025-09-22/EyePCR_ A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery_20250922|EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/OCR Hallucination|OCR Hallucination]], [[keywords/Vision-Faithful Reasoning|Vision-Faithful Reasoning]], [[keywords/KIE-HVQA Benchmark|KIE-HVQA Benchmark]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.20168v2 Announce Type: replace 
Abstract: Recent advancements in multimodal large language models have enhanced document understanding by integrating textual and visual information. However, existing models exhibit incompleteness within their paradigm in real-world scenarios, particularly under visual degradation. In such conditions, the current response paradigm often fails to adequately perceive visual degradation and ambiguity, leading to overreliance on linguistic priors or misaligned visual-textual reasoning. This difficulty in recognizing uncertainty frequently results in the generation of hallucinatory content, especially when a precise answer is not feasible. To better demonstrate and analyze this phenomenon and problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR hallucination in degraded document understanding. This dataset includes test samples spanning identity cards and invoices, with simulated real-world degradations for OCR reliability. This setup allows for evaluating models' capacity, under degraded input, to distinguish reliable visual information and answer accordingly, thereby highlighting the challenge of avoiding hallucination on uncertain data. To achieve vision-faithful reasoning and thereby avoid the aforementioned issues, we further introduce a GRPO-based framework featuring a novel reward mechanism. By incorporating a self-awareness of visual uncertainty and an analysis method that initiates refusal to answer to increase task difficulty within our supervised fine-tuning and reinforcement learning framework, we successfully mitigated hallucinations in ambiguous regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA and there is no significant performance drop in standard tasks, highlighting both effectiveness and robustness.

## ğŸ“ ìš”ì•½

ìµœê·¼ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ í…ìŠ¤íŠ¸ì™€ ì‹œê° ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë¬¸ì„œ ì´í•´ê°€ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹œê°ì  ì—´í™”ê°€ ìˆëŠ” ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ê¸°ì¡´ ëª¨ë¸ì´ ë¶ˆì™„ì „ì„±ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ëª¨ë¸ì€ ì‹œê°ì  ë¶ˆí™•ì‹¤ì„±ì„ ì¸ì‹í•˜ì§€ ëª»í•˜ê³  ì–¸ì–´ì  í¸í–¥ì— ì˜ì¡´í•˜ê±°ë‚˜ ì‹œê°-í…ìŠ¤íŠ¸ ì¶”ë¡ ì´ ì˜ëª»ë˜ì–´ í™˜ê°ì  ë‚´ìš©ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì—´í™”ëœ ë¬¸ì„œ ì´í•´ì—ì„œ OCR í™˜ê°ì„ í‰ê°€í•˜ëŠ” ìµœì´ˆì˜ ë²¤ì¹˜ë§ˆí¬ì¸ KIE-HVQAë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì‹ ë¶„ì¦ê³¼ ì²­êµ¬ì„œ ë“±ì˜ ìƒ˜í”Œì„ í¬í•¨í•˜ë©°, OCR ì‹ ë¢°ì„±ì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ëœ ì—´í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ì‹œê°ì  ë¶ˆí™•ì‹¤ì„±ì„ ì¸ì‹í•˜ê³  ë‹µë³€ì„ ê±°ë¶€í•˜ëŠ” ìƒˆë¡œìš´ ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œ GRPO ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ í™˜ê°ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, Qwen2.5-VL ëª¨ë¸ì€ KIE-HVQAì—ì„œ GPT-4o ëŒ€ë¹„ 22%ì˜ í™˜ê° ì—†ëŠ” ì •í™•ë„ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, í‘œì¤€ ì‘ì—…ì—ì„œ ì„±ëŠ¥ ì €í•˜ ì—†ì´ íš¨ê³¼ì„±ê³¼ ê²¬ê³ ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ì™€ ì‹œê° ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë¬¸ì„œ ì´í•´ë¥¼ í–¥ìƒì‹œì¼°ì§€ë§Œ, ì‹œê°ì  ì—´í™” ìƒí™©ì—ì„œëŠ” ë¶ˆì™„ì „í•¨ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.
- 2. ì‹œê°ì  ì—´í™”ì™€ ëª¨í˜¸ì„±ì„ ì¶©ë¶„íˆ ì¸ì‹í•˜ì§€ ëª»í•´ ì–¸ì–´ì  ì„ ì…ê²¬ì— ì˜ì¡´í•˜ê±°ë‚˜ ì‹œê°-í…ìŠ¤íŠ¸ ì¶”ë¡ ì´ ì˜ëª» ì •ë ¬ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.
- 3. KIE-HVQAëŠ” ì—´í™”ëœ ë¬¸ì„œ ì´í•´ì—ì„œ OCR í™˜ê°ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìµœì´ˆì˜ ë²¤ì¹˜ë§ˆí¬ë¡œ, ì‹ ë¶„ì¦ê³¼ ì†¡ì¥ì„ í¬í•¨í•œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. GRPO ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ì™€ ìƒˆë¡œìš´ ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ì‹œê°ì  ë¶ˆí™•ì‹¤ì„±ì„ ì¸ì‹í•˜ê³  ë‹µë³€ ê±°ë¶€ë¥¼ í†µí•´ í™˜ê°ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤.
- 5. Qwen2.5-VL ì‹¤í—˜ ê²°ê³¼, KIE-HVQAì—ì„œ GPT-4o ëŒ€ë¹„ í™˜ê° ì—†ëŠ” ì •í™•ë„ê°€ 22% í–¥ìƒë˜ì—ˆìœ¼ë©°, í‘œì¤€ ì‘ì—…ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ì—†ìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:25:35*