---
keywords:
  - Masked Diffusion Models
  - Reward-Weighted Sampling
  - Non-Autoregressive Generation
  - Large Language Model
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.00707
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:24:23.282387",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Masked Diffusion Models",
    "Reward-Weighted Sampling",
    "Non-Autoregressive Generation",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Masked Diffusion Models": 0.78,
    "Reward-Weighted Sampling": 0.82,
    "Non-Autoregressive Generation": 0.8,
    "Large Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Masked Diffusion Models",
        "canonical": "Masked Diffusion Models",
        "aliases": [
          "MDMs"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel approach in language modeling, distinct from traditional autoregressive models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reward-Weighted Sampling",
        "canonical": "Reward-Weighted Sampling",
        "aliases": [
          "RWS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new decoding strategy that enhances non-autoregressive characteristics, pivotal for linking novel methodologies.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Non-Autoregressive Generation",
        "canonical": "Non-Autoregressive Generation",
        "aliases": [
          "NAG"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in the paper that contrasts with autoregressive methods, facilitating connections to related non-sequential models.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Modeling",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Broad technical term that provides context and links to other works in language modeling.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "decoding methods",
      "evaluation metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Masked Diffusion Models",
      "resolved_canonical": "Masked Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reward-Weighted Sampling",
      "resolved_canonical": "Reward-Weighted Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Non-Autoregressive Generation",
      "resolved_canonical": "Non-Autoregressive Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Modeling",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.00707.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.00707](https://arxiv.org/abs/2509.00707)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Masked Diffusion Models as Energy Minimization_20250917|Masked Diffusion Models as Energy Minimization]] (83.4% similar)
- [[2025-09-18/Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning_20250918|Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning]] (82.7% similar)
- [[2025-09-23/Time Is a Feature_ Exploiting Temporal Dynamics in Diffusion Language Models_20250923|Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models]] (82.6% similar)
- [[2025-09-22/Discrete Diffusion in Large Language and Multimodal Models_ A Survey_20250922|Discrete Diffusion in Large Language and Multimodal Models: A Survey]] (82.2% similar)
- [[2025-09-23/Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing_20250923|Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Non-Autoregressive Generation|Non-Autoregressive Generation]]
**âš¡ Unique Technical**: [[keywords/Masked Diffusion Models|Masked Diffusion Models]], [[keywords/Reward-Weighted Sampling|Reward-Weighted Sampling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.00707v2 Announce Type: replace-cross 
Abstract: Masked diffusion models (MDMs) offer a promising non-autoregressive alternative for large language modeling. Standard decoding methods for MDMs, such as confidence-based sampling, select tokens independently based on individual token confidences at each diffusion step. However, we observe that this independent token selection often results in generation orders resembling sequential autoregressive processes, limiting the advantages of non-autoregressive modeling. To mitigate this pheonomenon, we propose Reward-Weighted Sampling (RWS), a novel decoding strategy that leverages an external reward model to provide a principled global signal during the iterative diffusion process. Specifically, at each diffusion step, RWS evaluates the quality of the entire intermediate sequence and scales token logits accordingly, guiding token selection by integrating global sequence-level coherence. This method selectively increases the confidence of tokens that initially have lower scores, thereby promoting a more non-autoregressive generation order. Furthermore, we provide theoretical justification showing that reward-weighted logit scaling induces beneficial rank reversals in token selection and consistently improves expected reward. Experiments demonstrate that RWS significantly promotes non-autoregressive generation orders, leading to improvements across multiple evaluation metrics. These results highlight the effectiveness of integrating global signals in enhancing both the non-autoregressive properties and overall performance of MDMs.

## ğŸ“ ìš”ì•½

ë§ˆìŠ¤í¬ë“œ ë””í“¨ì „ ëª¨ë¸(MDM)ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë§ì— ë¹„ìë™íšŒê·€ì  ëŒ€ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ MDM ë””ì½”ë”© ë°©ë²•ì€ ê° ë””í“¨ì „ ë‹¨ê³„ì—ì„œ ê°œë³„ í† í°ì˜ ì‹ ë¢°ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë…ë¦½ì ìœ¼ë¡œ í† í°ì„ ì„ íƒí•˜ì§€ë§Œ, ì´ëŠ” ì¢…ì¢… ìˆœì°¨ì  ìë™íšŒê·€ ê³¼ì •ê³¼ ìœ ì‚¬í•œ ìƒì„± ìˆœì„œë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì™¸ë¶€ ë³´ìƒ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì „ì²´ ì¤‘ê°„ ì‹œí€€ìŠ¤ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  í† í° ë¡œì§“ì„ ì¡°ì •í•˜ëŠ” ë³´ìƒ ê°€ì¤‘ ìƒ˜í”Œë§(RWS)ì´ë¼ëŠ” ìƒˆë¡œìš´ ë””ì½”ë”© ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. RWSëŠ” ì´ˆê¸° ì ìˆ˜ê°€ ë‚®ì€ í† í°ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ ë¹„ìë™íšŒê·€ì  ìƒì„± ìˆœì„œë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í† í° ì„ íƒì—ì„œ ìœ ìµí•œ ìˆœìœ„ ë³€í™”ë¥¼ ìœ ë„í•˜ê³  ê¸°ëŒ€ ë³´ìƒì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, RWSëŠ” ë¹„ìë™íšŒê·€ì  ìƒì„± ìˆœì„œë¥¼ í¬ê²Œ ì´‰ì§„í•˜ì—¬ ì—¬ëŸ¬ í‰ê°€ ì§€í‘œì—ì„œ ì„±ëŠ¥ì„ ê°œì„ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Masked diffusion models(MDMs)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ ë¹„ìê¸°íšŒê·€ì  ëŒ€ì•ˆìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ MDMs ë””ì½”ë”© ë°©ë²•ì€ ë…ë¦½ì ì¸ í† í° ì„ íƒìœ¼ë¡œ ì¸í•´ ìê¸°íšŒê·€ì  ìƒì„± ìˆœì„œë¥¼ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚´ëŠ” ê²½í–¥ì´ ìˆë‹¤.
- 3. Reward-Weighted Sampling(RWS)ëŠ” ì™¸ë¶€ ë³´ìƒ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì „ì—­ì ì¸ ì‹ í˜¸ë¥¼ ì œê³µí•˜ëŠ” ìƒˆë¡œìš´ ë””ì½”ë”© ì „ëµì´ë‹¤.
- 4. RWSëŠ” ì „ì—­ ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì¼ê´€ì„±ì„ í†µí•©í•˜ì—¬ í† í° ì„ íƒì„ ì•ˆë‚´í•˜ë©° ë¹„ìê¸°íšŒê·€ì  ìƒì„± ìˆœì„œë¥¼ ì´‰ì§„í•œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, RWSëŠ” ë¹„ìê¸°íšŒê·€ì  ìƒì„± ìˆœì„œë¥¼ í¬ê²Œ ì´‰ì§„í•˜ê³  ì—¬ëŸ¬ í‰ê°€ ì§€í‘œì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ë‚¸ë‹¤.


---

*Generated on 2025-09-24 01:24:23*