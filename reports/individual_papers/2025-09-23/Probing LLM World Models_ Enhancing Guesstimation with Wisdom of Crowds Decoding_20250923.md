---
keywords:
  - Large Language Model
  - Wisdom of Crowds
  - World Model
  - Guesstimation
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2501.17310
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:24:43.497644",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Wisdom of Crowds",
    "World Model",
    "Guesstimation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Wisdom of Crowds": 0.78,
    "World Model": 0.8,
    "Guesstimation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader field of language models and their applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Wisdom of Crowds",
        "canonical": "Wisdom of Crowds",
        "aliases": [
          "WOC"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a unique approach to improve LLM performance through crowd-sourced estimation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "World Model",
        "canonical": "World Model",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the conceptual framework within which LLMs operate for reasoning tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Guesstimation",
        "canonical": "Guesstimation",
        "aliases": [
          "Estimation"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on using LLMs for approximate reasoning tasks.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Wisdom of Crowds",
      "resolved_canonical": "Wisdom of Crowds",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "World Model",
      "resolved_canonical": "World Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Guesstimation",
      "resolved_canonical": "Guesstimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2501.17310.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2501.17310](https://arxiv.org/abs/2501.17310)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (84.6% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (84.3% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (84.0% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (83.0% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/World Model|World Model]]
**âš¡ Unique Technical**: [[keywords/Wisdom of Crowds|Wisdom of Crowds]], [[keywords/Guesstimation|Guesstimation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.17310v3 Announce Type: replace 
Abstract: Guesstimation--the task of making approximate quantitative estimates about objects or events-is a common real--world skill, yet remains underexplored in large language model (LLM) research. We introduce three guesstimation datasets: MARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many marbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential election). Inspired by the social science concept of Wisdom of Crowds (WOC)- where the median of multiple estimates improves accuracy-we propose WOC decoding for LLMs. We replicate WOC effects in human participants and find that LLMs exhibit similar benefits: median aggregation across sampled responses consistently improves accuracy over greedy decoding, self-consistency decoding, and mean decoding. This suggests that LLMs encode a world model that supports approximate reasoning. Our results position guesstimation as a useful probe of LLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM guesstimation performance on real-world tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ì—°êµ¬ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ëœ íƒêµ¬ëœ 'ì¶”ì •' ê³¼ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì €ìë“¤ì€ ë¬¼ë¦¬ì  ì¶”ì •ë¶€í„° ì¶”ìƒì  ì˜ˆì¸¡ê¹Œì§€ ì•„ìš°ë¥´ëŠ” ì„¸ ê°€ì§€ ë°ì´í„°ì…‹(MARBLES, FUTURE, ELECPRED)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì‚¬íšŒê³¼í•™ì˜ 'êµ°ì¤‘ì˜ ì§€í˜œ(WOC)' ê°œë…ì— ì˜ê°ì„ ë°›ì•„, LLMì—ì„œ WOC ë””ì½”ë”©ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, LLMì—ì„œë„ WOC íš¨ê³¼ê°€ ë‚˜íƒ€ë‚˜ë©°, ìƒ˜í”Œë§ëœ ì‘ë‹µì˜ ì¤‘ìœ„ìˆ˜ ì§‘ê³„ê°€ ì •í™•ì„±ì„ í–¥ìƒì‹œí‚´ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” LLMì´ ì„¸ê³„ ëª¨ë¸ì„ í†µí•´ ê·¼ì‚¬ ì¶”ë¡ ì„ ì§€ì›í•¨ì„ ì‹œì‚¬í•˜ë©°, WOC ë””ì½”ë”©ì´ ì‹¤ì œ ê³¼ì œì—ì„œ LLMì˜ ì¶”ì • ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì „ëµì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Guesstimationì€ ì‹¤ì œ ì„¸ê³„ì—ì„œ í”íˆ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì´ì§€ë§Œ, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ì—°êµ¬ì—ì„œëŠ” ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¬¼ë¦¬ì  ì¶”ì •ë¶€í„° ì¶”ìƒì  ì˜ˆì¸¡ê¹Œì§€ ì•„ìš°ë¥´ëŠ” ì„¸ ê°€ì§€ guesstimation ë°ì´í„°ì…‹(MARBLES, FUTURE, ELECPRED)ì„ ì†Œê°œí•œë‹¤.
- 3. êµ°ì¤‘ì˜ ì§€í˜œ(Wisdom of Crowds, WOC) ê°œë…ì— ì˜ê°ì„ ë°›ì•„, LLMì˜ ì •í™•ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ WOC ë””ì½”ë”© ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
- 4. WOC íš¨ê³¼ë¥¼ ì¸ê°„ ì°¸ì—¬ìì™€ LLMì—ì„œ ì¬í˜„í•œ ê²°ê³¼, ìƒ˜í”Œë§ëœ ì‘ë‹µì˜ ì¤‘ìœ„ìˆ˜ ì§‘ê³„ê°€ ì •í™•ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” guesstimationì´ LLMì˜ ì„¸ê³„ ì§€ì‹ì„ íƒêµ¬í•˜ëŠ” ìœ ìš©í•œ ë„êµ¬ì„ì„ ë³´ì—¬ì£¼ë©°, WOC ë””ì½”ë”©ì´ ì‹¤ì œ ê³¼ì œì—ì„œ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì „ëµì„ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-24 00:24:43*