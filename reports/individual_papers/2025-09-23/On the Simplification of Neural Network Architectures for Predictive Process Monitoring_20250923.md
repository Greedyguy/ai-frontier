---
keywords:
  - Predictive Process Monitoring
  - Transformer
  - Long Short-Term Memory
  - Model Simplification
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17145
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:47:22.114787",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Predictive Process Monitoring",
    "Transformer",
    "Long Short-Term Memory",
    "Model Simplification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Predictive Process Monitoring": 0.78,
    "Transformer": 0.85,
    "Long Short-Term Memory": 0.82,
    "Model Simplification": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Predictive Process Monitoring",
        "canonical": "Predictive Process Monitoring",
        "aliases": [
          "PPM"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized application area that can connect to various process mining and monitoring studies.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model in deep learning, relevant to the simplification study discussed.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "LSTM",
        "canonical": "Long Short-Term Memory",
        "aliases": [
          "LSTM"
        ],
        "category": "specific_connectable",
        "rationale": "LSTMs are a key model type in predictive tasks and their simplification is central to the paper.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "model simplification",
        "canonical": "Model Simplification",
        "aliases": [
          "simplifying models"
        ],
        "category": "unique_technical",
        "rationale": "The paper focuses on simplifying neural network architectures, making this a unique technical focus.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "deep learning models",
      "event logs",
      "predictive performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Predictive Process Monitoring",
      "resolved_canonical": "Predictive Process Monitoring",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "LSTM",
      "resolved_canonical": "Long Short-Term Memory",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "model simplification",
      "resolved_canonical": "Model Simplification",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# On the Simplification of Neural Network Architectures for Predictive Process Monitoring

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17145.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17145](https://arxiv.org/abs/2509.17145)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/SCAN_ Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning_20250923|SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning]] (82.0% similar)
- [[2025-09-23/Less is More_ Unlocking Specialization of Time Series Foundation Models via Structured Pruning_20250923|Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning]] (81.9% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (81.9% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (81.8% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Long Short-Term Memory|Long Short-Term Memory]]
**âš¡ Unique Technical**: [[keywords/Predictive Process Monitoring|Predictive Process Monitoring]], [[keywords/Model Simplification|Model Simplification]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17145v1 Announce Type: new 
Abstract: Predictive Process Monitoring (PPM) aims to forecast the future behavior of ongoing process instances using historical event data, enabling proactive decision-making. While recent advances rely heavily on deep learning models such as LSTMs and Transformers, their high computational cost hinders practical adoption. Prior work has explored data reduction techniques and alternative feature encodings, but the effect of simplifying model architectures themselves remains underexplored. In this paper, we analyze how reducing model complexity, both in terms of parameter count and architectural depth, impacts predictive performance, using two established PPM approaches. Across five diverse event logs, we show that shrinking the Transformer model by 85% results in only a 2-3% drop in performance across various PPM tasks, while the LSTM proves slightly more sensitive, particularly for waiting time prediction. Overall, our findings suggest that substantial model simplification can preserve predictive accuracy, paving the way for more efficient and scalable PPM solutions.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§(PPM)ì—ì„œ ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ëŠ” ê²ƒì´ ì˜ˆì¸¡ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ LSTMê³¼ Transformer ëª¨ë¸ì˜ ë†’ì€ ê³„ì‚° ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì™€ êµ¬ì¡°ì  ê¹Šì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤. ë‹¤ì„¯ ê°œì˜ ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ í†µí•´ ì‹¤í—˜í•œ ê²°ê³¼, Transformer ëª¨ë¸ì˜ í¬ê¸°ë¥¼ 85% ì¤„ì—¬ë„ ì„±ëŠ¥ ì €í•˜ê°€ 2-3%ì— ë¶ˆê³¼í–ˆìœ¼ë©°, LSTMì€ ëŒ€ê¸° ì‹œê°„ ì˜ˆì¸¡ì—ì„œ ë” ë¯¼ê°í•œ ë°˜ì‘ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ëª¨ë¸ ë‹¨ìˆœí™”ê°€ ì˜ˆì¸¡ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ PPM ì†”ë£¨ì…˜ì„ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§(PPM)ì€ ê³¼ê±° ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ë¯¸ë˜ í–‰ë™ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
- 2. ìµœê·¼ì˜ PPM ì—°êµ¬ëŠ” LSTMê³¼ Transformer ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì˜ì¡´í•˜ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ì´ ì‹¤ìš©ì ì¸ ì±„íƒì„ ë°©í•´í•œë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì€ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì™€ êµ¬ì¡°ì  ê¹Šì´ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ì˜ˆì¸¡ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì˜€ë‹¤.
- 4. Transformer ëª¨ë¸ì˜ í¬ê¸°ë¥¼ 85% ì¤„ì—¬ë„ ë‹¤ì–‘í•œ PPM ì‘ì—…ì—ì„œ ì„±ëŠ¥ì´ 2-3%ë§Œ ê°ì†Œí•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.
- 5. ëª¨ë¸ ë‹¨ìˆœí™”ê°€ ì˜ˆì¸¡ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ë” íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ PPM ì†”ë£¨ì…˜ì„ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.


---

*Generated on 2025-09-24 01:47:22*