---
keywords:
  - Diffusion Model
  - Facial Landmarks
  - Expression-Aware Landmarks
  - Fine-Grained Facial Loss
  - Progressive Generation
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.16630
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:28:40.238319",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Model",
    "Facial Landmarks",
    "Expression-Aware Landmarks",
    "Fine-Grained Facial Loss",
    "Progressive Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Model": 0.78,
    "Facial Landmarks": 0.81,
    "Expression-Aware Landmarks": 0.79,
    "Fine-Grained Facial Loss": 0.77,
    "Progressive Generation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "diffusion-based framework",
        "canonical": "Diffusion Model",
        "aliases": [
          "diffusion framework",
          "diffusion-based model"
        ],
        "category": "broad_technical",
        "rationale": "Diffusion models are a key component in generative modeling, linking to broader machine learning discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "facial landmarks",
        "canonical": "Facial Landmarks",
        "aliases": [
          "face landmarks",
          "landmark points"
        ],
        "category": "specific_connectable",
        "rationale": "Facial landmarks are essential for animation and recognition tasks, providing a bridge to computer vision topics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.81
      },
      {
        "surface": "expression-aware landmarks",
        "canonical": "Expression-Aware Landmarks",
        "aliases": [
          "emotion landmarks",
          "expression landmarks"
        ],
        "category": "unique_technical",
        "rationale": "This concept is unique to the paper, enhancing the understanding of emotion representation in animations.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "fine-grained facial loss",
        "canonical": "Fine-Grained Facial Loss",
        "aliases": [
          "detailed facial loss",
          "facial loss function"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel loss function for better expression capture, relevant to deep learning optimization.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "progressive generation strategy",
        "canonical": "Progressive Generation",
        "aliases": [
          "progressive animation",
          "stepwise generation"
        ],
        "category": "specific_connectable",
        "rationale": "This strategy is crucial for achieving stable animations, linking to iterative improvement methods.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "efficiency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "diffusion-based framework",
      "resolved_canonical": "Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "facial landmarks",
      "resolved_canonical": "Facial Landmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "expression-aware landmarks",
      "resolved_canonical": "Expression-Aware Landmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "fine-grained facial loss",
      "resolved_canonical": "Fine-Grained Facial Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "progressive generation strategy",
      "resolved_canonical": "Progressive Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16630.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.16630](https://arxiv.org/abs/2509.16630)

## 🔗 유사한 논문
- [[2025-09-19/Controllable Localized Face Anonymization Via Diffusion Inpainting_20250919|Controllable Localized Face Anonymization Via Diffusion Inpainting]] (83.2% similar)
- [[2025-09-22/FLOAT_ Generative Motion Latent Flow Matching for Audio-driven Talking Portrait_20250922|FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait]] (83.2% similar)
- [[2025-09-22/Lynx_ Towards High-Fidelity Personalized Video Generation_20250922|Lynx: Towards High-Fidelity Personalized Video Generation]] (80.8% similar)
- [[2025-09-22/Tiny is not small enough_ High-quality, low-resource facial animation models through hybrid knowledge distillation_20250922|Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation]] (80.7% similar)
- [[2025-09-22/RespoDiff_ Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation_20250922|RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation]] (80.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Diffusion Model|Diffusion Model]]
**🔗 Specific Connectable**: [[keywords/Facial Landmarks|Facial Landmarks]], [[keywords/Progressive Generation|Progressive Generation]]
**⚡ Unique Technical**: [[keywords/Expression-Aware Landmarks|Expression-Aware Landmarks]], [[keywords/Fine-Grained Facial Loss|Fine-Grained Facial Loss]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16630v1 Announce Type: new 
Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.

## 📝 요약

Follow-Your-Emoji-Faster는 얼굴 랜드마크를 기반으로 한 초상화 애니메이션 프레임워크로, 정체성 유지, 표정 전이, 장기적 일관성을 해결합니다. 이 모델은 표현 인식 랜드마크와 세밀한 얼굴 손실을 통해 모션 정렬과 정체성 유출 감소를 개선하며, 다양한 초상화 유형에 대해 표현력 있는 애니메이션을 지원합니다. 또한, 장기적 안정성을 위한 점진적 생성 전략과 테일러 보간 캐시를 도입하여 2.6배의 가속을 달성했습니다. EmojiBench++ 벤치마크 평가에서 높은 품질과 제어성을 입증했습니다.

## 🎯 주요 포인트

- 1. Follow-Your-Emoji-Faster는 얼굴 랜드마크를 기반으로 한 자유로운 초상화 애니메이션을 위한 효율적인 확산 기반 프레임워크를 제시합니다.
- 2. 이 모델은 표현 인식 랜드마크와 세밀한 얼굴 손실을 통해 참조 초상의 정체성을 보존하고 목표 표현을 정확하게 전달합니다.
- 3. 장기적인 안정적인 애니메이션 생성을 위해 점진적 생성 전략과 테일러 보간 캐시를 도입하여 2.6배의 무손실 가속을 달성합니다.
- 4. EmojiBench++라는 포괄적인 벤치마크를 통해 다양한 초상화와 구동 비디오, 랜드마크 시퀀스를 평가하며, 뛰어난 애니메이션 품질과 제어 가능성을 입증합니다.
- 5. 이 연구의 코드, 학습 데이터셋 및 벤치마크는 https://follow-your-emoji.github.io/에서 제공됩니다.


---

*Generated on 2025-09-24 04:28:40*