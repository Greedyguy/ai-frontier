---
keywords:
  - Transformer
  - AutoGraph
  - Graph Generation
  - Substructure-conditioned Generation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2502.02216
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:37:50.783010",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "AutoGraph",
    "Graph Generation",
    "Substructure-conditioned Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "AutoGraph": 0.8,
    "Graph Generation": 0.78,
    "Substructure-conditioned Generation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformers",
        "canonical": "Transformer",
        "aliases": [
          "Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the paper's methodology, linking it to a broad technical category.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "AutoGraph",
        "canonical": "AutoGraph",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "AutoGraph is the novel model introduced in the paper, representing a unique technical concept.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Graph Generation",
        "canonical": "Graph Generation",
        "aliases": [
          "Graph Generators"
        ],
        "category": "specific_connectable",
        "rationale": "Graph Generation is a key application area for the model, linking it to specific connectable concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Substructure-conditioned Generation",
        "canonical": "Substructure-conditioned Generation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique feature of the model that allows for nuanced graph generation, enhancing specificity.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "sequence",
      "sampling complexity",
      "state-of-the-art"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "AutoGraph",
      "resolved_canonical": "AutoGraph",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Graph Generation",
      "resolved_canonical": "Graph Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Substructure-conditioned Generation",
      "resolved_canonical": "Substructure-conditioned Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Flatten Graphs as Sequences: Transformers are Scalable Graph Generators

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.02216.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2502.02216](https://arxiv.org/abs/2502.02216)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/GraphWeave_ Interpretable and Robust Graph Generation via Random Walk Trajectories_20250923|GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories]] (84.0% similar)
- [[2025-09-22/Schreier-Coset Graph Propagation_20250922|Schreier-Coset Graph Propagation]] (81.6% similar)
- [[2025-09-23/GraphMend_ Code Transformations for Fixing Graph Breaks in PyTorch 2_20250923|GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2]] (81.1% similar)
- [[2025-09-23/Optimizing Inference in Transformer-Based Models_ A Multi-Method Benchmark_20250923|Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark]] (81.0% similar)
- [[2025-09-22/BBScoreV2_ Learning Time-Evolution and Latent Alignment from Stochastic Representation_20250922|BBScoreV2: Learning Time-Evolution and Latent Alignment from Stochastic Representation]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Graph Generation|Graph Generation]]
**âš¡ Unique Technical**: [[keywords/AutoGraph|AutoGraph]], [[keywords/Substructure-conditioned Generation|Substructure-conditioned Generation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.02216v2 Announce Type: replace 
Abstract: We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.

## ğŸ“ ìš”ì•½

AutoGraphëŠ” ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì„± ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ìê¸°íšŒê·€ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ í† í°ì˜ ì„ì˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€ì ì¸ ë…¸ë“œ íŠ¹ì§• ì—†ì´ë„ ê·¸ë˜í”„ë¥¼ ì‹œí€€ìŠ¤ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì—£ì§€ ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ìƒ˜í”Œë§ ë³µì¡ë„ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì„ í˜•ì ìœ¼ë¡œ í™•ì¥ë˜ì–´ ëŒ€ê·œëª¨ í¬ì†Œ ê·¸ë˜í”„ì— íš¨ìœ¨ì ì…ë‹ˆë‹¤. AutoGraphëŠ” ì‹œí€€ìŠ¤ ì ‘ë‘ì‚¬ê°€ ìœ ë„ëœ ë¶€ë¶„ ê·¸ë˜í”„ë¥¼ ë‚˜íƒ€ë‚´ì–´ ì–¸ì–´ ëª¨ë¸ë§ê³¼ ì§ì ‘ ì—°ê²°ë©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, í•©ì„± ë° ë¶„ì ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, ê¸°ì¡´ í™•ì‚° ëª¨ë¸ë³´ë‹¤ ìµœëŒ€ 100ë°° ë¹ ë¥¸ ìƒì„±ê³¼ 3ë°° ë¹ ë¥¸ í•™ìŠµì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„¸ë¶€ êµ¬ì¡° ì¡°ê±´ ìƒì„±ê³¼ ì „ì´ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ ê·¸ë˜í”„ ê¸°ì´ˆ ëª¨ë¸ì˜ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AutoGraphëŠ” ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì„± ê·¸ë˜í”„ ìƒì„±ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ìê¸° íšŒê·€ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ê·¸ë˜í”„ë¥¼ í† í°ì˜ ëœë¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€ì ì¸ ë…¸ë“œ íŠ¹ì§• ì—†ì´ë„ ê·¸ë˜í”„ë¥¼ ì‹œí€€ìŠ¤ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 3. AutoGraphëŠ” ì—£ì§€ ìˆ˜ì— ë”°ë¼ ìƒ˜í”Œë§ ë³µì¡ì„±ê³¼ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì„ í˜•ì ìœ¼ë¡œ í™•ì¥ë˜ì–´ ëŒ€ê·œëª¨ í¬ì†Œ ê·¸ë˜í”„ì— íš¨ìœ¨ì ì…ë‹ˆë‹¤.
- 4. AutoGraphëŠ” í•©ì„± ë° ë¶„ì ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ê¸°ì¡´ í™•ì‚° ëª¨ë¸ë³´ë‹¤ ìµœëŒ€ 100ë°° ë¹ ë¥¸ ìƒì„±ê³¼ 3ë°° ë¹ ë¥¸ í•™ìŠµì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. í•˜ìœ„ êµ¬ì¡° ì¡°ê±´ë¶€ ìƒì„±ì„ ì§€ì›í•˜ë©°, ì–¸ì–´ ëª¨ë¸ë§ê³¼ ê·¸ë˜í”„ ìƒì„±ì„ ì—°ê²°í•˜ì—¬ ê·¸ë˜í”„ ê¸°ì´ˆ ëª¨ë¸ì˜ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 02:37:50*