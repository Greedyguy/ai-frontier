---
keywords:
  - CardiacCLIP
  - Left Ventricular Ejection Fraction Prediction
  - Attention Mechanism
  - Few-Shot Learning
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17065
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:43:54.706185",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "CardiacCLIP",
    "Left Ventricular Ejection Fraction Prediction",
    "Attention Mechanism",
    "Few-Shot Learning",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "CardiacCLIP": 0.8,
    "Left Ventricular Ejection Fraction Prediction": 0.85,
    "Attention Mechanism": 0.83,
    "Few-Shot Learning": 0.88,
    "Vision-Language Model": 0.86
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "CardiacCLIP",
        "canonical": "CardiacCLIP",
        "aliases": [
          "Cardiac CLIP"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel adaptation of CLIP models specifically for echocardiogram video analysis.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "LVEF Prediction",
        "canonical": "Left Ventricular Ejection Fraction Prediction",
        "aliases": [
          "LVEF Estimation"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's focus on improving diagnostic accuracy in cardiac assessments.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Attention-based Frame Aggregation",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Frame Aggregation"
        ],
        "category": "specific_connectable",
        "rationale": "Utilizes attention mechanisms to enhance video-based predictions, linking to broader attention-based methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.83
      },
      {
        "surface": "Few-shot Echocardiogram Video Analysis",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-shot Echocardiography"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the application of few-shot learning in a specialized domain, facilitating connections with similar methodologies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.88
      },
      {
        "surface": "Vision-Language Models for Echocardiography",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Echocardiography"
        ],
        "category": "evolved_concepts",
        "rationale": "Connects to the evolving field of vision-language models applied to medical imaging.",
        "novelty_score": 0.65,
        "connectivity_score": 0.87,
        "specificity_score": 0.72,
        "link_intent_score": 0.86
      }
    ],
    "ban_list_suggestions": [
      "Echocardiography",
      "Video Dataset"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "CardiacCLIP",
      "resolved_canonical": "CardiacCLIP",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "LVEF Prediction",
      "resolved_canonical": "Left Ventricular Ejection Fraction Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Attention-based Frame Aggregation",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Few-shot Echocardiogram Video Analysis",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Vision-Language Models for Echocardiography",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.87,
        "specificity": 0.72,
        "link_intent": 0.86
      }
    }
  ]
}
-->

# CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17065.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17065](https://arxiv.org/abs/2509.17065)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (85.7% similar)
- [[2025-09-22/RegionMed-CLIP_ A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding_20250922|RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding]] (83.6% similar)
- [[2025-09-23/Echo-Path_ Pathology-Conditioned Echo Video Generation_20250923|Echo-Path: Pathology-Conditioned Echo Video Generation]] (83.5% similar)
- [[2025-09-22/GLip_ A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition_20250922|GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition]] (83.0% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Left Ventricular Ejection Fraction Prediction|Left Ventricular Ejection Fraction Prediction]], [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/CardiacCLIP|CardiacCLIP]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17065v1 Announce Type: new 
Abstract: Echocardiography is a vital non-invasive modality for cardiac assessment, with left ventricular ejection fraction (LVEF) serving as a key indicator of heart function. Existing LVEF estimation methods depend on large-scale annotated video datasets, which are costly and limit adaptability across various clinical settings. Recent vision-language models for echocardiography, such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial temporal dynamics and localized cardiac structures essential for accurate diagnosis. To address these challenges, we propose CardiacCLIP, a video-based framework that enhances LVEF prediction through attention-based frame aggregation and multi-resolution input scaling. Specifically, we introduce MFL (Multi Frame Learning), a novel attention-based mechanism for selectively fusing informative frames, and EchoZoom, a multi-scale feature extraction strategy that refines spatial representations of cardiac structures. As a novel adaptation of CLIP models for few-shot echocardiogram video analysis, our approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on the EchoNet-Dynamic dataset under 1-shot setting. The code is available at https://github.com/xmed-lab/CardiacCLIP.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì´ˆìŒíŒŒë¥¼ í†µí•œ ì¢Œì‹¬ì‹¤ ë°•ì¶œê³„ìˆ˜(LVEF) ì˜ˆì¸¡ì„ ê°œì„ í•˜ê¸° ìœ„í•´ CardiacCLIPì´ë¼ëŠ” ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ëŒ€ê·œëª¨ ì£¼ì„ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì—¬ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ë‹¤ì–‘í•œ ì„ìƒ í™˜ê²½ì— ì ì‘í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. CardiacCLIPì€ ì£¼ì˜ ê¸°ë°˜ í”„ë ˆì„ ì§‘ê³„ì™€ ë‹¤ì¤‘ í•´ìƒë„ ì…ë ¥ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. íŠ¹íˆ, ì •ë³´ê°€ ë§ì€ í”„ë ˆì„ì„ ì„ íƒì ìœ¼ë¡œ ìœµí•©í•˜ëŠ” MFL(Multi Frame Learning) ë©”ì»¤ë‹ˆì¦˜ê³¼ ì‹¬ì¥ êµ¬ì¡°ì˜ ê³µê°„ì  í‘œí˜„ì„ ì •ì œí•˜ëŠ” EchoZoom ì „ëµì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ EchoNet-Dynamic ë°ì´í„°ì…‹ì˜ 1-shot ì„¤ì •ì—ì„œ MAEë¥¼ 2.07 ê°ì†Œì‹œì¼œ ì§„ë‹¨ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¬ì´ˆìŒíŒŒì—ì„œ ì¢Œì‹¬ì‹¤ ë°•ì¶œë¥ (LVEF)ì€ ì‹¬ì¥ ê¸°ëŠ¥ì˜ ì£¼ìš” ì§€í‘œë¡œ ì‚¬ìš©ë˜ë©°, ê¸°ì¡´ì˜ LVEF ì¶”ì • ë°©ë²•ì€ ëŒ€ê·œëª¨ ì£¼ì„ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì—¬ ì„ìƒ í™˜ê²½ì—ì„œì˜ ì ì‘ì„±ì„ ì œí•œí•©ë‹ˆë‹¤.
- 2. EchoCLIPê³¼ ê°™ì€ ê¸°ì¡´ì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì‚¬ì „ í•™ìŠµì„ ì ìš©í•˜ì§€ë§Œ, ì •í™•í•œ ì§„ë‹¨ì— í•„ìˆ˜ì ì¸ ì‹œê°„ì  ì—­í•™ê³¼ êµ­ì†Œì ì¸ ì‹¬ì¥ êµ¬ì¡°ë¥¼ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
- 3. CardiacCLIPì€ ì£¼ì˜ ê¸°ë°˜ í”„ë ˆì„ ì§‘ê³„ì™€ ë‹¤ì¤‘ í•´ìƒë„ ì…ë ¥ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ LVEF ì˜ˆì¸¡ì„ í–¥ìƒì‹œí‚¤ëŠ” ë¹„ë””ì˜¤ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ë¡œ, ì •ë³´ì„± í”„ë ˆì„ì„ ì„ íƒì ìœ¼ë¡œ ìœµí•©í•˜ëŠ” MFL(Multi Frame Learning) ë©”ì»¤ë‹ˆì¦˜ê³¼ ì‹¬ì¥ êµ¬ì¡°ì˜ ê³µê°„ì  í‘œí˜„ì„ ì •ì œí•˜ëŠ” EchoZoom ì „ëµì„ ë„ì…í•©ë‹ˆë‹¤.
- 4. CardiacCLIPì€ EchoNet-Dynamic ë°ì´í„°ì…‹ì˜ 1-shot ì„¤ì •ì—ì„œ MAEë¥¼ 2.07 ê°ì†Œì‹œì¼œ ì§„ë‹¨ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. CardiacCLIPì˜ ì½”ë“œëŠ” https://github.com/xmed-lab/CardiacCLIPì—ì„œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:43:54*