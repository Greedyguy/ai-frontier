---
keywords:
  - Transformer
  - Post-Training Quantization
  - Twin-Log Quantization
  - Adaptive Rotation Scheme
  - Computer Vision
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2508.03485
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:28:12.336150",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Post-Training Quantization",
    "Twin-Log Quantization",
    "Adaptive Rotation Scheme",
    "Computer Vision"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Post-Training Quantization": 0.9,
    "Twin-Log Quantization": 0.88,
    "Adaptive Rotation Scheme": 0.87,
    "Computer Vision": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformers",
        "canonical": "Transformer",
        "aliases": [
          "DiT"
        ],
        "category": "broad_technical",
        "rationale": "As a variant of Transformers, it connects well with existing Transformer-based models and methods.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Post-Training Quantization",
        "canonical": "Post-Training Quantization",
        "aliases": [
          "PTQ"
        ],
        "category": "unique_technical",
        "rationale": "This technique is central to the paper's contribution and connects to model compression discussions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Twin-Log Quantization",
        "canonical": "Twin-Log Quantization",
        "aliases": [
          "TLQ"
        ],
        "category": "unique_technical",
        "rationale": "A novel quantization method introduced in the paper, relevant for discussions on quantization techniques.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Adaptive Rotation Scheme",
        "canonical": "Adaptive Rotation Scheme",
        "aliases": [
          "ARS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new approach to handle activation outliers, linking to optimization techniques.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.87
      },
      {
        "surface": "Image and Video Generation",
        "canonical": "Computer Vision",
        "aliases": [
          "Image Generation",
          "Video Generation"
        ],
        "category": "broad_technical",
        "rationale": "This connects to the broader field of computer vision, relevant for linking across visual generation topics.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Gaussian-like distribution",
      "activation quantization",
      "resource-constrained scenarios"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Post-Training Quantization",
      "resolved_canonical": "Post-Training Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Twin-Log Quantization",
      "resolved_canonical": "Twin-Log Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Adaptive Rotation Scheme",
      "resolved_canonical": "Adaptive Rotation Scheme",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.87
      }
    },
    {
      "candidate_surface": "Image and Video Generation",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2508.03485.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2508.03485](https://arxiv.org/abs/2508.03485)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (85.7% similar)
- [[2025-09-17/BWCache_ Accelerating Video Diffusion Transformers through Block-Wise Caching_20250917|BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching]] (84.5% similar)
- [[2025-09-23/DiCo_ Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling_20250923|DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling]] (84.1% similar)
- [[2025-09-23/QVGen_ Pushing the Limit of Quantized Video Generative Models_20250923|QVGen: Pushing the Limit of Quantized Video Generative Models]] (83.9% similar)
- [[2025-09-23/AlignedGen_ Aligning Style Across Generated Images_20250923|AlignedGen: Aligning Style Across Generated Images]] (83.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Computer Vision|Computer Vision]]
**âš¡ Unique Technical**: [[keywords/Post-Training Quantization|Post-Training Quantization]], [[keywords/Twin-Log Quantization|Twin-Log Quantization]], [[keywords/Adaptive Rotation Scheme|Adaptive Rotation Scheme]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.03485v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) have achieved impressive performance in text-to-image and text-to-video generation. However, their high computational cost and large parameter sizes pose significant challenges for usage in resource-constrained scenarios. Effective compression of models has become a crucial issue that urgently needs to be addressed. Post-training quantization (PTQ) is a promising solution to reduce memory usage and accelerate inference, but existing PTQ methods suffer from severe performance degradation under extreme low-bit settings. After experiments and analysis, we identify two key obstacles to low-bit PTQ for DiTs: (1) the weights of DiT models follow a Gaussian-like distribution with long tails, causing uniform quantization to poorly allocate intervals and leading to significant quantization errors. This issue has been observed in the linear layer weights of different DiT models, which deeply limits the performance. (2) two types of activation outliers in DiT models: (i) Mild Outliers with slightly elevated values, and (ii) Salient Outliers with large magnitudes concentrated in specific channels, which disrupt activation quantization. To address these issues, we propose LRQ-DiT, an efficient and accurate post-training quantization framework for image and video generation. First, we introduce Twin-Log Quantization (TLQ), a log-based method that allocates more quantization intervals to the intermediate dense regions, effectively achieving alignment with the weight distribution and reducing quantization errors. Second, we propose an Adaptive Rotation Scheme (ARS) that dynamically applies Hadamard or outlier-aware rotations based on activation fluctuation, effectively mitigating the impact of both types of outliers.Extensive experiments on various text-to-image and text-to-video DiT models demonstrate that LRQ-DiT preserves high generation quality.

## ğŸ“ ìš”ì•½

Diffusion Transformers (DiTs)ëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ë†’ì€ ê³„ì‚° ë¹„ìš©ê³¼ í° íŒŒë¼ë¯¸í„° í¬ê¸°ë¡œ ì¸í•´ ìì› ì œì•½ í™˜ê²½ì—ì„œ ì‚¬ìš©ì´ ì–´ë ¤ìš´ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë¸ ì••ì¶•ì´ ì¤‘ìš”í•´ì¡Œìœ¼ë©°, ì‚¬í›„ ì–‘ìí™”(PTQ)ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ìœ ë§í•œ ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ PTQ ë°©ë²•ì€ ê·¹ë‹¨ì ì¸ ì €ë¹„íŠ¸ ì„¤ì •ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìŠµë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” DiT ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ê¸´ ê¼¬ë¦¬ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ê³ , í™œì„±í™” ê°’ì—ì„œ ë‘ ê°€ì§€ ìœ í˜•ì˜ ì´ìƒì¹˜ê°€ ë°œìƒí•˜ì—¬ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì´ˆë˜í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LRQ-DiTë¼ëŠ” íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ì‚¬í›„ ì–‘ìí™” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. Twin-Log Quantization(TLQ) ë°©ë²•ì„ ë„ì…í•˜ì—¬ ê°€ì¤‘ì¹˜ ë¶„í¬ì— ë§ì¶° ì–‘ìí™” ê°„ê²©ì„ í• ë‹¹í•˜ê³ , Adaptive Rotation Scheme(ARS)ì„ í†µí•´ í™œì„±í™” ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ì™„í™”í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ LRQ-DiTê°€ ë†’ì€ ìƒì„± í’ˆì§ˆì„ ìœ ì§€í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Diffusion Transformers (DiTs)ëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ë†’ì€ ê³„ì‚° ë¹„ìš©ê³¼ í° íŒŒë¼ë¯¸í„° í¬ê¸°ë¡œ ì¸í•´ ìì› ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©ì´ ì–´ë ¤ì›€.
- 2. ê¸°ì¡´ì˜ ì‚¬í›„ í›ˆë ¨ ì–‘ìí™”(PTQ) ë°©ë²•ì€ ê·¹ë‹¨ì ì¸ ì €ë¹„íŠ¸ ì„¤ì •ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìŒ.
- 3. DiT ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ê¸´ ê¼¬ë¦¬ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¼ ê· ì¼ ì–‘ìí™”ê°€ ê°„ê²©ì„ ì˜ëª» í• ë‹¹í•˜ì—¬ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ìœ ë°œí•¨.
- 4. DiT ëª¨ë¸ì˜ í™œì„±í™”ì—ì„œ ë‘ ê°€ì§€ ìœ í˜•ì˜ ì´ìƒì¹˜ê°€ ë°œê²¬ë¨: ì•½ê°„ ë†’ì€ ê°’ì„ ê°€ì§„ Mild Outliersì™€ íŠ¹ì • ì±„ë„ì— ì§‘ì¤‘ëœ í° í¬ê¸°ì˜ Salient Outliers.
- 5. ì œì•ˆëœ LRQ-DiT í”„ë ˆì„ì›Œí¬ëŠ” Twin-Log Quantizationê³¼ Adaptive Rotation Schemeì„ í†µí•´ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì¤„ì´ê³ , í™œì„±í™” ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ì™„í™”í•˜ì—¬ ë†’ì€ ìƒì„± í’ˆì§ˆì„ ìœ ì§€í•¨.


---

*Generated on 2025-09-24 05:28:12*