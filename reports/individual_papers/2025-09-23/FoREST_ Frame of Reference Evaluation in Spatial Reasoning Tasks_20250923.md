---
keywords:
  - Frame of Reference
  - Spatial Reasoning
  - Large Language Model
  - Text-to-Image Models
  - Spatial-Guided Prompting
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2502.17775
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:50:54.429556",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Frame of Reference",
    "Spatial Reasoning",
    "Large Language Model",
    "Text-to-Image Models",
    "Spatial-Guided Prompting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Frame of Reference": 0.78,
    "Spatial Reasoning": 0.72,
    "Large Language Model": 0.7,
    "Text-to-Image Models": 0.8,
    "Spatial-Guided Prompting": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Frame of Reference",
        "canonical": "Frame of Reference",
        "aliases": [
          "FoR"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on spatial reasoning, providing a unique perspective for linking spatial cognition concepts.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Spatial Reasoning",
        "canonical": "Spatial Reasoning",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A core concept in AI and cognitive science, facilitating connections to related spatial intelligence research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Key technology evaluated in the paper, relevant to discussions on AI model capabilities.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Text-to-Image Models",
        "canonical": "Text-to-Image Models",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the application of spatial reasoning in multimodal AI tasks, linking to vision-language research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spatial-Guided Prompting",
        "canonical": "Spatial-Guided Prompting",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for improving spatial reasoning in LLMs, offering a unique link to prompting techniques.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "performance gap"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Frame of Reference",
      "resolved_canonical": "Frame of Reference",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Spatial Reasoning",
      "resolved_canonical": "Spatial Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Text-to-Image Models",
      "resolved_canonical": "Text-to-Image Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spatial-Guided Prompting",
      "resolved_canonical": "Spatial-Guided Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.17775.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2502.17775](https://arxiv.org/abs/2502.17775)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/LLMs as Layout Designers_ A Spatial Reasoning Perspective_20250923|LLMs as Layout Designers: A Spatial Reasoning Perspective]] (83.1% similar)
- [[2025-09-22/How Good are Foundation Models in Step-by-Step Embodied Reasoning?_20250922|How Good are Foundation Models in Step-by-Step Embodied Reasoning?]] (82.8% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (82.6% similar)
- [[2025-09-23/GeoPQA_ Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning_20250923|GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning]] (81.9% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Spatial Reasoning|Spatial Reasoning]], [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Text-to-Image Models|Text-to-Image Models]]
**âš¡ Unique Technical**: [[keywords/Frame of Reference|Frame of Reference]], [[keywords/Spatial-Guided Prompting|Spatial-Guided Prompting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.17775v3 Announce Type: replace 
Abstract: Spatial reasoning is a fundamental aspect of human intelligence. One key concept in spatial cognition is the Frame of Reference, which identifies the perspective of spatial expressions. Despite its significance, FoR has received limited attention in AI models that need spatial intelligence. There is a lack of dedicated benchmarks and in-depth evaluation of large language models (LLMs) in this area. To address this issue, we introduce the Frame of Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess FoR comprehension in LLMs. We evaluate LLMs on answering questions that require FoR comprehension and layout generation in text-to-image models using FoREST. Our results reveal a notable performance gap across different FoR classes in various LLMs, affecting their ability to generate accurate layouts for text-to-image generation. This highlights critical shortcomings in FoR comprehension. To improve FoR understanding, we propose Spatial-Guided prompting, which improves LLMs ability to extract essential spatial concepts. Our proposed method improves overall performance across spatial reasoning tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ ì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ì¸¡ë©´ì¸ ê³µê°„ ì¶”ë¡ ì—ì„œ 'ì°¸ì¡° í”„ë ˆì„(Frame of Reference, FoR)'ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, AI ëª¨ë¸ì—ì„œì˜ ë¶€ì¡±í•œ FoR ì´í•´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ì—°êµ¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ FoR ì´í•´ë„ë¥¼ í‰ê°€í•˜ëŠ” FoREST ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆí•˜ê³ , ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ë“¤ì´ FoR ì´í•´ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ê³  í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ëª¨ë¸ì—ì„œ ë ˆì´ì•„ì›ƒì„ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë‹¤ì–‘í•œ LLMì—ì„œ FoR í´ë˜ìŠ¤ ê°„ ì„±ëŠ¥ ê²©ì°¨ê°€ ë“œëŸ¬ë‚¬ìœ¼ë©°, ì´ëŠ” ì •í™•í•œ ë ˆì´ì•„ì›ƒ ìƒì„±ì— ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 'Spatial-Guided prompting' ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ LLMì˜ ê³µê°„ ê°œë… ì¶”ì¶œ ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼°ê³ , ì „ë°˜ì ì¸ ê³µê°„ ì¶”ë¡  ì‘ì—… ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³µê°„ ì¶”ë¡ ì—ì„œ ì¤‘ìš”í•œ ê°œë…ì¸ ì°¸ì¡° í”„ë ˆì„(FoR)ì´ AI ëª¨ë¸ì—ì„œ ì¶©ë¶„íˆ ë‹¤ë£¨ì–´ì§€ì§€ ì•Šì•˜ë‹¤.
- 2. FoR ì´í•´ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ FoREST ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í‰ê°€í•˜ì˜€ë‹¤.
- 3. ë‹¤ì–‘í•œ LLMsì—ì„œ FoR í´ë˜ìŠ¤ ê°„ ì„±ëŠ¥ ê²©ì°¨ê°€ ì¡´ì¬í•˜ë©°, ì´ëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì˜ ì •í™•í•œ ë ˆì´ì•„ì›ƒ ìƒì„±ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
- 4. FoR ì´í•´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì œì•ˆëœ Spatial-Guided í”„ë¡¬í”„íŠ¸ëŠ” LLMsì˜ ê³µê°„ ê°œë… ì¶”ì¶œ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ì „ë°˜ì ì¸ ê³µê°„ ì¶”ë¡  ì‘ì—… ì„±ëŠ¥ì„ ê°œì„ í•œë‹¤.


---

*Generated on 2025-09-24 03:50:54*