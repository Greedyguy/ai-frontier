---
keywords:
  - Frame of Reference
  - Spatial Reasoning
  - Large Language Model
  - Text-to-Image Models
  - Spatial-Guided Prompting
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2502.17775
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:50:54.429556",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Frame of Reference",
    "Spatial Reasoning",
    "Large Language Model",
    "Text-to-Image Models",
    "Spatial-Guided Prompting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Frame of Reference": 0.78,
    "Spatial Reasoning": 0.72,
    "Large Language Model": 0.7,
    "Text-to-Image Models": 0.8,
    "Spatial-Guided Prompting": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Frame of Reference",
        "canonical": "Frame of Reference",
        "aliases": [
          "FoR"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus on spatial reasoning, providing a unique perspective for linking spatial cognition concepts.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Spatial Reasoning",
        "canonical": "Spatial Reasoning",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A core concept in AI and cognitive science, facilitating connections to related spatial intelligence research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Key technology evaluated in the paper, relevant to discussions on AI model capabilities.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Text-to-Image Models",
        "canonical": "Text-to-Image Models",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the application of spatial reasoning in multimodal AI tasks, linking to vision-language research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spatial-Guided Prompting",
        "canonical": "Spatial-Guided Prompting",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for improving spatial reasoning in LLMs, offering a unique link to prompting techniques.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "performance gap"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Frame of Reference",
      "resolved_canonical": "Frame of Reference",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Spatial Reasoning",
      "resolved_canonical": "Spatial Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Text-to-Image Models",
      "resolved_canonical": "Text-to-Image Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spatial-Guided Prompting",
      "resolved_canonical": "Spatial-Guided Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.17775.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2502.17775](https://arxiv.org/abs/2502.17775)

## 🔗 유사한 논문
- [[2025-09-23/LLMs as Layout Designers_ A Spatial Reasoning Perspective_20250923|LLMs as Layout Designers: A Spatial Reasoning Perspective]] (83.1% similar)
- [[2025-09-22/How Good are Foundation Models in Step-by-Step Embodied Reasoning?_20250922|How Good are Foundation Models in Step-by-Step Embodied Reasoning?]] (82.8% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (82.6% similar)
- [[2025-09-23/GeoPQA_ Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning_20250923|GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning]] (81.9% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (81.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Spatial Reasoning|Spatial Reasoning]], [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Text-to-Image Models|Text-to-Image Models]]
**⚡ Unique Technical**: [[keywords/Frame of Reference|Frame of Reference]], [[keywords/Spatial-Guided Prompting|Spatial-Guided Prompting]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.17775v3 Announce Type: replace 
Abstract: Spatial reasoning is a fundamental aspect of human intelligence. One key concept in spatial cognition is the Frame of Reference, which identifies the perspective of spatial expressions. Despite its significance, FoR has received limited attention in AI models that need spatial intelligence. There is a lack of dedicated benchmarks and in-depth evaluation of large language models (LLMs) in this area. To address this issue, we introduce the Frame of Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess FoR comprehension in LLMs. We evaluate LLMs on answering questions that require FoR comprehension and layout generation in text-to-image models using FoREST. Our results reveal a notable performance gap across different FoR classes in various LLMs, affecting their ability to generate accurate layouts for text-to-image generation. This highlights critical shortcomings in FoR comprehension. To improve FoR understanding, we propose Spatial-Guided prompting, which improves LLMs ability to extract essential spatial concepts. Our proposed method improves overall performance across spatial reasoning tasks.

## 📝 요약

이 논문은 인간 지능의 중요한 측면인 공간 추론에서 '참조 프레임(Frame of Reference, FoR)'의 중요성을 강조하며, AI 모델에서의 부족한 FoR 이해를 개선하기 위한 연구를 소개합니다. 이를 위해 FoR 이해도를 평가하는 FoREST 벤치마크를 제안하고, 대형 언어 모델(LLM)들이 FoR 이해를 필요로 하는 질문에 답하고 텍스트-이미지 모델에서 레이아웃을 생성하는 능력을 평가했습니다. 연구 결과, 다양한 LLM에서 FoR 클래스 간 성능 격차가 드러났으며, 이는 정확한 레이아웃 생성에 영향을 미쳤습니다. 이를 해결하기 위해 'Spatial-Guided prompting' 방법을 제안하여 LLM의 공간 개념 추출 능력을 향상시켰고, 전반적인 공간 추론 작업 성능을 개선했습니다.

## 🎯 주요 포인트

- 1. 공간 추론에서 중요한 개념인 참조 프레임(FoR)이 AI 모델에서 충분히 다루어지지 않았다.
- 2. FoR 이해도를 평가하기 위해 FoREST 벤치마크를 도입하여 대형 언어 모델(LLMs)을 평가하였다.
- 3. 다양한 LLMs에서 FoR 클래스 간 성능 격차가 존재하며, 이는 텍스트-이미지 생성의 정확한 레이아웃 생성에 영향을 미친다.
- 4. FoR 이해를 개선하기 위해 제안된 Spatial-Guided 프롬프트는 LLMs의 공간 개념 추출 능력을 향상시킨다.
- 5. 제안된 방법은 전반적인 공간 추론 작업 성능을 개선한다.


---

*Generated on 2025-09-24 03:50:54*