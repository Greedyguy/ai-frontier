---
keywords:
  - Qwen3-Omni
  - Thinker-Talker MoE Architecture
  - Multimodal Learning
  - Audio Captioning Model
  - Streaming Synthesis
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17765
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:08:22.456095",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Qwen3-Omni",
    "Thinker-Talker MoE Architecture",
    "Multimodal Learning",
    "Audio Captioning Model",
    "Streaming Synthesis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Qwen3-Omni": 0.8,
    "Thinker-Talker MoE Architecture": 0.75,
    "Multimodal Learning": 0.78,
    "Audio Captioning Model": 0.72,
    "Streaming Synthesis": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Qwen3-Omni",
        "canonical": "Qwen3-Omni",
        "aliases": [
          "Qwen3",
          "Omni"
        ],
        "category": "unique_technical",
        "rationale": "Qwen3-Omni is a unique multimodal model that integrates multiple modalities, making it a key concept for linking in multimodal research.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Thinker-Talker MoE architecture",
        "canonical": "Thinker-Talker MoE Architecture",
        "aliases": [
          "Thinker-Talker",
          "MoE Architecture"
        ],
        "category": "unique_technical",
        "rationale": "This architecture is central to the model's ability to unify perception and generation, providing a unique linking point for multimodal architectures.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that connects various modalities, crucial for understanding the scope of Qwen3-Omni.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "audio captioning model",
        "canonical": "Audio Captioning Model",
        "aliases": [
          "audio captioning"
        ],
        "category": "unique_technical",
        "rationale": "This model addresses a gap in the research community, providing a unique link for audio processing advancements.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "streaming synthesis",
        "canonical": "Streaming Synthesis",
        "aliases": [
          "streaming synthesis"
        ],
        "category": "unique_technical",
        "rationale": "Streaming Synthesis is a critical feature of Qwen3-Omni, enhancing real-time processing capabilities.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "state-of-the-art",
      "performance",
      "benchmarks",
      "languages"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Qwen3-Omni",
      "resolved_canonical": "Qwen3-Omni",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Thinker-Talker MoE architecture",
      "resolved_canonical": "Thinker-Talker MoE Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "audio captioning model",
      "resolved_canonical": "Audio Captioning Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "streaming synthesis",
      "resolved_canonical": "Streaming Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Qwen3-Omni Technical Report

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17765.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17765](https://arxiv.org/abs/2509.17765)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Omni-CLST_ Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering_20250918|Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (81.8% similar)
- [[2025-09-22/Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding_20250922|Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding]] (80.9% similar)
- [[2025-09-22/VoXtream_ Full-Stream Text-to-Speech with Extremely Low Latency_20250922|VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency]] (79.9% similar)
- [[2025-09-22/Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices_20250922|Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices]] (79.5% similar)
- [[2025-09-19/AToken_ A Unified Tokenizer for Vision_20250919|AToken: A Unified Tokenizer for Vision]] (78.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Qwen3-Omni|Qwen3-Omni]], [[keywords/Thinker-Talker MoE Architecture|Thinker-Talker MoE Architecture]], [[keywords/Audio Captioning Model|Audio Captioning Model]], [[keywords/Streaming Synthesis|Streaming Synthesis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17765v1 Announce Type: cross 
Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.

## ğŸ“ ìš”ì•½

Qwen3-OmniëŠ” í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ì—ì„œ ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©°, íŠ¹íˆ ì˜¤ë””ì˜¤ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì´ëŠ” ìµœì´ˆì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 36ê°œì˜ ì˜¤ë””ì˜¤ ë° ì˜¤ë””ì˜¤-ë¹„ì£¼ì–¼ ë²¤ì¹˜ë§ˆí¬ ì¤‘ 32ê°œì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ SOTAë¥¼ ë‹¬ì„±í•˜ê³ , 22ê°œì—ì„œëŠ” ì „ì²´ SOTAë¥¼ ê¸°ë¡í•˜ë©°, Gemini-2.5-Pro, Seed-ASR, GPT-4o-Transcribe ê°™ì€ ê°•ë ¥í•œ ë¹„ê³µê°œ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. Qwen3-OmniëŠ” Thinker-Talker MoE ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•˜ì—¬ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ì „ë°˜ì— ê±¸ì³ ì¸ì‹ê³¼ ìƒì„±ì„ í†µí•©í•˜ê³ , 119ê°œ ì–¸ì–´ì˜ í…ìŠ¤íŠ¸ ìƒí˜¸ì‘ìš©, 19ê°œ ì–¸ì–´ì˜ ìŒì„± ì´í•´, 10ê°œ ì–¸ì–´ì˜ ìŒì„± ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° í•©ì„±ì˜ ì²« íŒ¨í‚· ì§€ì—°ì„ ì¤„ì´ê¸° ìœ„í•´, TalkerëŠ” ë‹¤ì¤‘ ì½”ë“œë¶ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì½”ë±ì„ ìë™ íšŒê·€ì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ë˜í•œ, Qwen3-OmniëŠ” ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ê°•í™”ë¥¼ ìœ„í•´ ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ì˜ ì…ë ¥ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” Thinking ëª¨ë¸ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— ì¼ë°˜ì ì¸ ì˜¤ë””ì˜¤ ìº¡ì…”ë‹ ëª¨ë¸ì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œ, Qwen3-Omni-30B-A3Bë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ Qwen3-Omni-30B-A3B-Captionerë¥¼ ê°œë°œ, ìƒì„¸í•˜ê³  í™˜ê°ì´ ì ì€ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì€ Apache 2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Qwen3-OmniëŠ” í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ì—ì„œ ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ê³¼ ë¹„êµí•´ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ìµœì´ˆì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì…ë‹ˆë‹¤.
- 2. Qwen3-OmniëŠ” 36ê°œì˜ ì˜¤ë””ì˜¤ ë° ì˜¤ë””ì˜¤-ë¹„ì£¼ì–¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œ 32ê°œì˜ ì˜¤í”ˆì†ŒìŠ¤ SOTAì™€ 22ê°œì˜ ì „ì²´ SOTAë¥¼ ë‹¬ì„±í•˜ë©°, Gemini-2.5-Pro, Seed-ASR, GPT-4o-Transcribeì™€ ê°™ì€ ê°•ë ¥í•œ ë¹„ê³µê°œ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 3. Qwen3-OmniëŠ” Thinker-Talker MoE ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•˜ì—¬ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ì „ë°˜ì— ê±¸ì³ ì¸ì‹ê³¼ ìƒì„±ì„ í†µí•©í•˜ë©°, 119ê°œ ì–¸ì–´ì˜ í…ìŠ¤íŠ¸ ìƒí˜¸ì‘ìš©, 19ê°œ ì–¸ì–´ì˜ ìŒì„± ì´í•´, 10ê°œ ì–¸ì–´ì˜ ìŒì„± ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.
- 4. Qwen3-OmniëŠ” ìŠ¤íŠ¸ë¦¬ë° í•©ì„±ì—ì„œ ì²« íŒ¨í‚· ì§€ì—°ì„ ì¤„ì´ê¸° ìœ„í•´ ë©€í‹° ì½”ë“œë¶ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì½”ë±ì„ ì˜ˆì¸¡í•˜ê³ , ê²½ëŸ‰ì˜ ì¸ê³¼ì  ConvNetì„ ì‚¬ìš©í•˜ì—¬ ì²« ì½”ë± í”„ë ˆì„ë¶€í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 5. Qwen3-Omni-30B-A3B-CaptionerëŠ” ì¼ë°˜ì ì¸ ì˜¤ë””ì˜¤ ìº¡ì…”ë‹ ëª¨ë¸ì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œ ì„¸ë¶€ì ì´ê³  í™˜ê°ì´ ì ì€ ìº¡ì…˜ì„ ìƒì„±í•˜ê¸° ìœ„í•´ Qwen3-Omni-30B-A3Bë¥¼ ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 00:08:22*