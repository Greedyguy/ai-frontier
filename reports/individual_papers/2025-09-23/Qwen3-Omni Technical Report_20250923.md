---
keywords:
  - Qwen3-Omni
  - Thinker-Talker MoE Architecture
  - Multimodal Learning
  - Audio Captioning Model
  - Streaming Synthesis
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17765
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:08:22.456095",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Qwen3-Omni",
    "Thinker-Talker MoE Architecture",
    "Multimodal Learning",
    "Audio Captioning Model",
    "Streaming Synthesis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Qwen3-Omni": 0.8,
    "Thinker-Talker MoE Architecture": 0.75,
    "Multimodal Learning": 0.78,
    "Audio Captioning Model": 0.72,
    "Streaming Synthesis": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Qwen3-Omni",
        "canonical": "Qwen3-Omni",
        "aliases": [
          "Qwen3",
          "Omni"
        ],
        "category": "unique_technical",
        "rationale": "Qwen3-Omni is a unique multimodal model that integrates multiple modalities, making it a key concept for linking in multimodal research.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Thinker-Talker MoE architecture",
        "canonical": "Thinker-Talker MoE Architecture",
        "aliases": [
          "Thinker-Talker",
          "MoE Architecture"
        ],
        "category": "unique_technical",
        "rationale": "This architecture is central to the model's ability to unify perception and generation, providing a unique linking point for multimodal architectures.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that connects various modalities, crucial for understanding the scope of Qwen3-Omni.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "audio captioning model",
        "canonical": "Audio Captioning Model",
        "aliases": [
          "audio captioning"
        ],
        "category": "unique_technical",
        "rationale": "This model addresses a gap in the research community, providing a unique link for audio processing advancements.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "streaming synthesis",
        "canonical": "Streaming Synthesis",
        "aliases": [
          "streaming synthesis"
        ],
        "category": "unique_technical",
        "rationale": "Streaming Synthesis is a critical feature of Qwen3-Omni, enhancing real-time processing capabilities.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "state-of-the-art",
      "performance",
      "benchmarks",
      "languages"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Qwen3-Omni",
      "resolved_canonical": "Qwen3-Omni",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Thinker-Talker MoE architecture",
      "resolved_canonical": "Thinker-Talker MoE Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "audio captioning model",
      "resolved_canonical": "Audio Captioning Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "streaming synthesis",
      "resolved_canonical": "Streaming Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Qwen3-Omni Technical Report

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17765.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17765](https://arxiv.org/abs/2509.17765)

## 🔗 유사한 논문
- [[2025-09-18/Omni-CLST_ Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering_20250918|Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (81.8% similar)
- [[2025-09-22/Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding_20250922|Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding]] (80.9% similar)
- [[2025-09-22/VoXtream_ Full-Stream Text-to-Speech with Extremely Low Latency_20250922|VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency]] (79.9% similar)
- [[2025-09-22/Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices_20250922|Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices]] (79.5% similar)
- [[2025-09-19/AToken_ A Unified Tokenizer for Vision_20250919|AToken: A Unified Tokenizer for Vision]] (78.9% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Qwen3-Omni|Qwen3-Omni]], [[keywords/Thinker-Talker MoE Architecture|Thinker-Talker MoE Architecture]], [[keywords/Audio Captioning Model|Audio Captioning Model]], [[keywords/Streaming Synthesis|Streaming Synthesis]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17765v1 Announce Type: cross 
Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.

## 📝 요약

Qwen3-Omni는 텍스트, 이미지, 오디오, 비디오에서 단일 모달 모델과 동등한 성능을 유지하며, 특히 오디오 작업에서 뛰어난 성과를 보이는 최초의 멀티모달 모델입니다. 이 모델은 36개의 오디오 및 오디오-비주얼 벤치마크 중 32개에서 오픈 소스 SOTA를 달성하고, 22개에서는 전체 SOTA를 기록하며, Gemini-2.5-Pro, Seed-ASR, GPT-4o-Transcribe 같은 강력한 비공개 모델을 능가합니다. Qwen3-Omni는 Thinker-Talker MoE 아키텍처를 채택하여 텍스트, 이미지, 오디오, 비디오 전반에 걸쳐 인식과 생성을 통합하고, 119개 언어의 텍스트 상호작용, 19개 언어의 음성 이해, 10개 언어의 음성 생성을 지원합니다. 스트리밍 합성의 첫 패킷 지연을 줄이기 위해, Talker는 다중 코드북 방식을 사용하여 음성 코덱을 자동 회귀적으로 예측합니다. 또한, Qwen3-Omni는 멀티모달 추론 강화를 위해 모든 모달리티의 입력을 명시적으로 추론하는 Thinking 모델을 도입했습니다. 연구 커뮤니티에 일반적인 오디오 캡셔닝 모델이 부족한 상황에서, Qwen3-Omni-30B-A3B를 미세 조정하여 Qwen3-Omni-30B-A3B-Captioner를 개발, 상세하고 환각이 적은 캡션을 생성합니다. 이 모델들은 Apache 2.0 라이선스 하에 공개되었습니다.

## 🎯 주요 포인트

- 1. Qwen3-Omni는 텍스트, 이미지, 오디오, 비디오에서 단일 모달 모델과 비교해 성능 저하 없이 최첨단 성능을 유지하는 최초의 멀티모달 모델입니다.
- 2. Qwen3-Omni는 36개의 오디오 및 오디오-비주얼 벤치마크에서 32개의 오픈소스 SOTA와 22개의 전체 SOTA를 달성하며, Gemini-2.5-Pro, Seed-ASR, GPT-4o-Transcribe와 같은 강력한 비공개 모델을 능가합니다.
- 3. Qwen3-Omni는 Thinker-Talker MoE 아키텍처를 채택하여 텍스트, 이미지, 오디오, 비디오 전반에 걸쳐 인식과 생성을 통합하며, 119개 언어의 텍스트 상호작용, 19개 언어의 음성 이해, 10개 언어의 음성 생성을 지원합니다.
- 4. Qwen3-Omni는 스트리밍 합성에서 첫 패킷 지연을 줄이기 위해 멀티 코드북 방식을 사용하여 음성 코덱을 예측하고, 경량의 인과적 ConvNet을 사용하여 첫 코덱 프레임부터 스트리밍을 가능하게 합니다.
- 5. Qwen3-Omni-30B-A3B-Captioner는 일반적인 오디오 캡셔닝 모델이 부족한 상황에서 세부적이고 환각이 적은 캡션을 생성하기 위해 Qwen3-Omni-30B-A3B를 미세 조정한 모델입니다.


---

*Generated on 2025-09-24 00:08:22*