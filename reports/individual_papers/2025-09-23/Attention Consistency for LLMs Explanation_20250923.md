---
keywords:
  - Large Language Model
  - Multi-Layer Attention Consistency Score
  - Attention Mechanism
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.17178
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:21:51.201211",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multi-Layer Attention Consistency Score",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multi-Layer Attention Consistency Score": 0.8,
    "Attention Mechanism": 0.88
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus, linking to a broad technical concept widely used in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multi-Layer Attention Consistency Score",
        "canonical": "Multi-Layer Attention Consistency Score",
        "aliases": [
          "MACS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method specific to the paper, enhancing unique technical linkage.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key concept in the paper, facilitating connections with related works on attention in neural networks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      }
    ],
    "ban_list_suggestions": [
      "interpretability methods",
      "computational efficiency",
      "input tokens"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multi-Layer Attention Consistency Score",
      "resolved_canonical": "Multi-Layer Attention Consistency Score",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    }
  ]
}
-->

# Attention Consistency for LLMs Explanation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17178.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.17178](https://arxiv.org/abs/2509.17178)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment]] (87.3% similar)
- [[2025-09-22/Calibrating LLM Confidence by Probing Perturbed Representation Stability_20250922|Calibrating LLM Confidence by Probing Perturbed Representation Stability]] (85.8% similar)
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (85.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (85.3% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Multi-Layer Attention Consistency Score|Multi-Layer Attention Consistency Score]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17178v1 Announce Type: new 
Abstract: Understanding the decision-making processes of large language models (LLMs) is essential for their trustworthy development and deployment. However, current interpretability methods often face challenges such as low resolution and high computational cost. To address these limitations, we propose the \textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight, and easily deployable heuristic for estimating the importance of input tokens in decoder-based models. MACS measures contributions of input tokens based on the consistency of maximal attention. Empirical evaluations demonstrate that MACS achieves a favorable trade-off between interpretability quality and computational efficiency, showing faithfulness comparable to complex techniques with a 22\% decrease in VRAM usage and 30\% reduction in latency.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ì´í•´í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ ë‹¤ì¸µ ì£¼ì˜ ì¼ê´€ì„± ì ìˆ˜(MACS)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MACSëŠ” ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì—ì„œ ì…ë ¥ í† í°ì˜ ì¤‘ìš”ì„±ì„ í‰ê°€í•˜ëŠ” ê²½ëŸ‰ì˜ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•ìœ¼ë¡œ, ìµœëŒ€ ì£¼ì˜ì˜ ì¼ê´€ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MACSëŠ” í•´ì„ ê°€ëŠ¥ì„±ê³¼ ê³„ì‚° íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì„ ì˜ ë§ì¶”ë©°, ë³µì¡í•œ ê¸°ìˆ ê³¼ ìœ ì‚¬í•œ ì‹ ë¢°ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ VRAM ì‚¬ìš©ëŸ‰ì„ 22% ì¤„ì´ê³  ì§€ì—° ì‹œê°„ì„ 30% ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ì´í•´í•˜ëŠ” ê²ƒì€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê°œë°œê³¼ ë°°í¬ì— í•„ìˆ˜ì ì´ë‹¤.
- 2. ê¸°ì¡´ í•´ì„ ë°©ë²•ì€ ë‚®ì€ í•´ìƒë„ì™€ ë†’ì€ ê³„ì‚° ë¹„ìš©ê³¼ ê°™ì€ ë¬¸ì œì— ì§ë©´í•˜ê³  ìˆë‹¤.
- 3. Multi-Layer Attention Consistency Score (MACS)ëŠ” ì…ë ¥ í† í°ì˜ ì¤‘ìš”ì„±ì„ ì¶”ì •í•˜ê¸° ìœ„í•œ ê²½ëŸ‰ì˜ ìƒˆë¡œìš´ ë°©ë²•ìœ¼ë¡œ ì œì•ˆë˜ì—ˆë‹¤.
- 4. MACSëŠ” ìµœëŒ€ ì£¼ì˜ë ¥ì˜ ì¼ê´€ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì…ë ¥ í† í°ì˜ ê¸°ì—¬ë„ë¥¼ ì¸¡ì •í•œë‹¤.
- 5. MACSëŠ” ë³µì¡í•œ ê¸°ìˆ ê³¼ ìœ ì‚¬í•œ ì¶©ì‹¤ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ VRAM ì‚¬ìš©ëŸ‰ì„ 22% ì¤„ì´ê³  ì§€ì—° ì‹œê°„ì„ 30% ê°ì†Œì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-24 03:21:51*