---
keywords:
  - Human-Object Interaction Recognition
  - 3D Hand Pose Analysis
  - Augmented Reality
  - Inter-Hand Spatial Envelope
  - Egocentric Video Analysis
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16557
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T02:10:35.858527",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Human-Object Interaction Recognition",
    "3D Hand Pose Analysis",
    "Augmented Reality",
    "Inter-Hand Spatial Envelope",
    "Egocentric Video Analysis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Human-Object Interaction Recognition": 0.82,
    "3D Hand Pose Analysis": 0.8,
    "Augmented Reality": 0.7,
    "Inter-Hand Spatial Envelope": 0.78,
    "Egocentric Video Analysis": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Human-Object Interaction Recognition",
        "canonical": "Human-Object Interaction Recognition",
        "aliases": [
          "HOIR"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for linking user identification with AR systems.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "3D Hand Pose",
        "canonical": "3D Hand Pose Analysis",
        "aliases": [
          "3D Hand Pose"
        ],
        "category": "unique_technical",
        "rationale": "Central to the method for user identification in egocentric videos.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.88,
        "link_intent_score": 0.8
      },
      {
        "surface": "Augmented Reality",
        "canonical": "Augmented Reality",
        "aliases": [
          "AR"
        ],
        "category": "broad_technical",
        "rationale": "Provides context for the application of the proposed framework.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Inter-Hand Spatial Envelope",
        "canonical": "Inter-Hand Spatial Envelope",
        "aliases": [
          "IHSE"
        ],
        "category": "unique_technical",
        "rationale": "A novel descriptor introduced in this work, enhancing feature extraction.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Egocentric Videos",
        "canonical": "Egocentric Video Analysis",
        "aliases": [
          "Egocentric Videos"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant for linking with other works on user perspective analysis.",
        "novelty_score": 0.68,
        "connectivity_score": 0.82,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "user identification",
      "feature extraction"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Human-Object Interaction Recognition",
      "resolved_canonical": "Human-Object Interaction Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "3D Hand Pose",
      "resolved_canonical": "3D Hand Pose Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.88,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Augmented Reality",
      "resolved_canonical": "Augmented Reality",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Inter-Hand Spatial Envelope",
      "resolved_canonical": "Inter-Hand Spatial Envelope",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Egocentric Videos",
      "resolved_canonical": "Egocentric Video Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.82,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16557.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16557](https://arxiv.org/abs/2509.16557)

## 🔗 유사한 논문
- [[2025-09-22/Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration_20250922|Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration]] (81.4% similar)
- [[2025-09-19/Human Interaction for Collaborative Semantic SLAM using Extended Reality_20250919|Human Interaction for Collaborative Semantic SLAM using Extended Reality]] (81.3% similar)
- [[2025-09-18/RoboEye_ Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching_20250918|RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching]] (80.1% similar)
- [[2025-09-22/DAOcc_ 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction_20250922|DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction]] (78.6% similar)
- [[2025-09-23/DETACH_ Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts_20250923|DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts]] (78.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Augmented Reality|Augmented Reality]]
**🔗 Specific Connectable**: [[keywords/Egocentric Video Analysis|Egocentric Video Analysis]]
**⚡ Unique Technical**: [[keywords/Human-Object Interaction Recognition|Human-Object Interaction Recognition]], [[keywords/3D Hand Pose Analysis|3D Hand Pose Analysis]], [[keywords/Inter-Hand Spatial Envelope|Inter-Hand Spatial Envelope]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16557v1 Announce Type: cross 
Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a crucial role in advancing augmented reality (AR)-based personalized assistive technologies. These systems are increasingly being deployed in high-stakes, human-centric environments such as aircraft cockpits, aerospace maintenance, and surgical procedures. This research introduces I2S (Interact2Sign), a multi stage framework designed for unobtrusive user identification through human object interaction recognition, leveraging 3D hand pose analysis in egocentric videos. I2S utilizes handcrafted features extracted from 3D hand poses and per forms sequential feature augmentation: first identifying the object class, followed by HOI recognition, and ultimately, user identification. A comprehensive feature extraction and description process was carried out for 3D hand poses, organizing the extracted features into semantically meaningful categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive ablation studies were conducted to determine the most effective combination of features. The optimal configuration achieved an impressive average F1-score of 97.52% for user identification, evaluated on a bimanual object manipulation dataset derived from the ARCTIC and H2O datasets. I2S demonstrates state-of-the-art performance while maintaining a lightweight model size of under 4 MB and a fast inference time of 0.1 seconds. These characteristics make the proposed framework highly suitable for real-time, on-device authentication in security-critical, AR-based systems.

## 📝 요약

이 연구는 증강 현실(AR) 기반 개인 맞춤형 보조 기술을 위한 인간-객체 상호작용 인식(HOIR)과 사용자 식별의 중요성을 강조합니다. 연구에서는 I2S(Interact2Sign)라는 다단계 프레임워크를 제안하여, 3D 손 자세 분석을 통해 사용자 식별을 수행합니다. I2S는 3D 손 자세에서 추출한 특징을 활용하여 객체 분류, HOI 인식, 사용자 식별 순으로 특징을 증강합니다. 연구는 공간, 주파수, 운동학, 방향성, 그리고 새롭게 제안된 Inter-Hand Spatial Envelope(IHSE) 등으로 특징을 분류했습니다. 최적의 특징 조합은 ARCTIC 및 H2O 데이터셋에서 평균 F1-score 97.52%를 기록했습니다. I2S는 4MB 이하의 경량 모델 크기와 0.1초의 빠른 추론 시간을 유지하며, 보안이 중요한 AR 기반 시스템에서 실시간 기기 내 인증에 적합합니다.

## 🎯 주요 포인트

- 1. I2S(Interact2Sign)는 인간-객체 상호작용 인식을 통해 사용자 식별을 수행하는 다단계 프레임워크로, 3D 손 자세 분석을 활용합니다.
- 2. 이 연구는 3D 손 자세에서 추출된 수작업 특징을 활용하여 객체 클래스 식별, HOI 인식, 사용자 식별의 순차적 특징 증강을 수행합니다.
- 3. I2S는 공간, 주파수, 운동학, 방향성, 그리고 새로운 기술인 Inter-Hand Spatial Envelope(IHSE)로 특징을 분류하여 포괄적인 특징 추출 및 설명 과정을 거칩니다.
- 4. 최적의 구성은 ARCTIC 및 H2O 데이터셋에서 파생된 양손 객체 조작 데이터셋을 평가하여 사용자 식별에 대해 평균 F1-점수 97.52%를 달성했습니다.
- 5. I2S는 4MB 이하의 경량 모델 크기와 0.1초의 빠른 추론 시간을 유지하며, 실시간 기기 내 인증에 적합한 성능을 보여줍니다.


---

*Generated on 2025-09-24 02:10:35*