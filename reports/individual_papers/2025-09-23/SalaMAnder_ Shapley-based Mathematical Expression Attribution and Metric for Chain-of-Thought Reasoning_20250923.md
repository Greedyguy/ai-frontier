---
keywords:
  - Chain-of-Thought Reasoning
  - Large Language Model
  - Shapley Value
  - Few-Shot Learning
  - Mathematical Expression Attribution
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16561
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T22:50:12.518911",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Chain-of-Thought Reasoning",
    "Large Language Model",
    "Shapley Value",
    "Few-Shot Learning",
    "Mathematical Expression Attribution"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Chain-of-Thought Reasoning": 0.8,
    "Large Language Model": 0.7,
    "Shapley Value": 0.78,
    "Few-Shot Learning": 0.79,
    "Mathematical Expression Attribution": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought Reasoning",
        "aliases": [
          "CoT"
        ],
        "category": "specific_connectable",
        "rationale": "Chain-of-Thought reasoning is a key concept in enhancing the reasoning capabilities of LLMs, making it a strong link for related studies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus, providing a broad technical foundation for linking.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Shapley Value",
        "canonical": "Shapley Value",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The Shapley Value is a unique technical concept used for mathematical expression attribution in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Few-Shot CoT Reasoning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot CoT"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is crucial for understanding the paper's methodology in CoT reasoning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Mathematical Expression Attribution",
        "canonical": "Mathematical Expression Attribution",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique technical concept introduced in the paper, essential for understanding its methodology.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "methodology",
      "evaluation metric",
      "model performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Shapley Value",
      "resolved_canonical": "Shapley Value",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Few-Shot CoT Reasoning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Mathematical Expression Attribution",
      "resolved_canonical": "Mathematical Expression Attribution",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16561.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16561](https://arxiv.org/abs/2509.16561)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (83.9% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (83.8% similar)
- [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (82.6% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (82.4% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Chain-of-Thought Reasoning|Chain-of-Thought Reasoning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Shapley Value|Shapley Value]], [[keywords/Mathematical Expression Attribution|Mathematical Expression Attribution]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16561v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of large language models (LLMs) to a large margin. However, the mechanism underlying such improvements remains unexplored. In this paper, we present \textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed \textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd} M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a mathematically rigorous evaluation metric for quantifying component-level contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley value for mathematical expression attribution and develop an efficient stratified sampling algorithm that significantly reduces the computational complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality \textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance analysis. Comprehensive validation across popular LLM models and diverse mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder framework exhibits a robust monotonic correlation with model performance, not only providing theoretical explanations for the empirical success of existing few-shot CoT but also establishing mathematically rigorous principles for prompt construction optimization. Furthermore, we verify the reliability of the explanation, based on which we unify the insights of previous work.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” Chain-of-Thought(CoT) í”„ë¡¬í”„íŠ¸ì˜ ê°œì„  ë©”ì»¤ë‹ˆì¦˜ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì €ìë“¤ì€ \textbf{SalaMAnder}ë¼ëŠ” ì´ë¡ ì  ë°©ë²•ë¡ ê³¼ ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€í•œ í‰ê°€ ì§€í‘œë¥¼ ì œì‹œí•˜ì—¬, ì†Œìˆ˜ì˜ ì˜ˆì‹œë¥¼ í†µí•œ CoT ì¶”ë¡ ì—ì„œ êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ë¥¼ ì •ëŸ‰í™”í•©ë‹ˆë‹¤. Shapley ê°’ì„ í™œìš©í•˜ì—¬ ìˆ˜í•™ì  í‘œí˜„ì˜ ê¸°ì—¬ë„ë¥¼ í‰ê°€í•˜ê³ , íš¨ìœ¨ì ì¸ ì¸µí™” ìƒ˜í”Œë§ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•´ ê³„ì‚° ë³µì¡ì„±ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, \textbf{CoSP}ë¼ëŠ” ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ê³¼ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ LLM ëª¨ë¸ê³¼ ìˆ˜í•™ì  ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•´ ê²€ì¦í•œ ê²°ê³¼, CoSP ì§€í‘œëŠ” ëª¨ë¸ ì„±ëŠ¥ê³¼ ê°•í•œ ë‹¨ì¡° ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ì£¼ë©°, ê¸°ì¡´ CoTì˜ ì„±ê³µì— ëŒ€í•œ ì´ë¡ ì  ì„¤ëª…ì„ ì œê³µí•˜ê³  í”„ë¡¬í”„íŠ¸ ìµœì í™”ì˜ ì›ì¹™ì„ í™•ë¦½í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì´ì „ ì—°êµ¬ì˜ í†µì°°ì„ í†µí•©í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Chain-of-Thought (CoT) í”„ë¡¬í”„íŠ¸ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. SalaMAnderëŠ” ìˆ˜í•™ì  í‘œí˜„ ê·€ì†ì„ ìœ„í•œ Shapley ê°’ì„ í™œìš©í•˜ì—¬ ì»´í¬ë„ŒíŠ¸ ìˆ˜ì¤€ì˜ ê¸°ì—¬ë„ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
- 3. CoSP(CSP) ë©”íŠ¸ë¦­ì€ ê³µë¶„ì‚° ë¶„ì„ì„ í†µí•´ ê°œë°œë˜ì—ˆìœ¼ë©°, ëª¨ë¸ ì„±ëŠ¥ê³¼ ê°•ë ¥í•œ ë‹¨ì¡° ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤.
- 4. SalaMAnder í”„ë ˆì„ì›Œí¬ëŠ” ê¸°ì¡´ì˜ few-shot CoTì˜ ì„±ê³µì— ëŒ€í•œ ì´ë¡ ì  ì„¤ëª…ì„ ì œê³µí•˜ê³  í”„ë¡¬í”„íŠ¸ ìµœì í™”ì˜ ìˆ˜í•™ì  ì›ì¹™ì„ í™•ë¦½í•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ìˆ˜í•™ì  ë²¤ì¹˜ë§ˆí¬ì™€ LLM ëª¨ë¸ì„ í†µí•´ SalaMAnderì˜ ì‹ ë¢°ì„±ì„ ê²€ì¦í•˜ê³  ì´ì „ ì—°êµ¬ì˜ í†µì°°ì„ í†µí•©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 22:50:12*