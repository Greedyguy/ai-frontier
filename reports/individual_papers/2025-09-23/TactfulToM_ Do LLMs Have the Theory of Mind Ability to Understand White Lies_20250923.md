---
keywords:
  - Large Language Model
  - Theory of Mind
  - White Lies
  - Human-in-the-Loop
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.17054
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:42:42.230860",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Theory of Mind",
    "White Lies",
    "Human-in-the-Loop"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Theory of Mind": 0.8,
    "White Lies": 0.78,
    "Human-in-the-Loop": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking it to broader discussions on language models and AI capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Theory of Mind",
        "canonical": "Theory of Mind",
        "aliases": [
          "ToM"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for understanding cognitive abilities in AI, relevant to psychological and AI research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "white lies",
        "canonical": "White Lies",
        "aliases": [
          "prosocial lies"
        ],
        "category": "unique_technical",
        "rationale": "Specific focus of the paper, relevant for discussions on social reasoning and ethics in AI.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "human-in-the-loop",
        "canonical": "Human-in-the-Loop",
        "aliases": [
          "HITL"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a methodology crucial for training and evaluating AI models with human guidance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance",
      "conversation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Theory of Mind",
      "resolved_canonical": "Theory of Mind",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "white lies",
      "resolved_canonical": "White Lies",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "human-in-the-loop",
      "resolved_canonical": "Human-in-the-Loop",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17054.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.17054](https://arxiv.org/abs/2509.17054)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (84.1% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (84.0% similar)
- [[2025-09-23/Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs_ A Case Study with In-the-Wild Data_20250923|Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data]] (83.6% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (83.2% similar)
- [[2025-09-19/OnlineMate_ An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning_20250919|OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Human-in-the-Loop|Human-in-the-Loop]]
**âš¡ Unique Technical**: [[keywords/Theory of Mind|Theory of Mind]], [[keywords/White Lies|White Lies]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17054v1 Announce Type: cross 
Abstract: While recent studies explore Large Language Models' (LLMs) performance on Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require more nuanced social context is limited, such as white lies. We introduce TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to understand white lies within real-life conversations and reason about prosocial motivations behind them, particularly when they are used to spare others' feelings and maintain social harmony. Our benchmark is generated through a multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed stories into conversations to maintain the information asymmetry between participants necessary for authentic white lies. We show that TactfulToM is challenging for state-of-the-art models, which perform substantially below humans, revealing shortcomings in their ability to fully comprehend the ToM reasoning that enables true understanding of white lies.

## ğŸ“ ìš”ì•½

ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë§ˆìŒ ì´ë¡ (ToM) ì¶”ë¡  ê³¼ì œ ìˆ˜í–‰ ëŠ¥ë ¥ì„ íƒêµ¬í•˜ê³  ìˆì§€ë§Œ, ë³´ë‹¤ ë¯¸ë¬˜í•œ ì‚¬íšŒì  ë§¥ë½ì´ í•„ìš”í•œ ToM ëŠ¥ë ¥, ì˜ˆë¥¼ ë“¤ì–´ ì„ ì˜ì˜ ê±°ì§“ë§ì— ëŒ€í•œ ì—°êµ¬ëŠ” ì œí•œì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” TactfulToMì´ë¼ëŠ” ìƒˆë¡œìš´ ì˜ì–´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í•˜ì—¬ LLMì´ ì‹¤ì œ ëŒ€í™”ì—ì„œ ì„ ì˜ì˜ ê±°ì§“ë§ì„ ì´í•´í•˜ê³ , ì´ë¥¼ í†µí•´ íƒ€ì¸ì˜ ê°ì •ì„ ë°°ë ¤í•˜ê³  ì‚¬íšŒì  ì¡°í™”ë¥¼ ìœ ì§€í•˜ë ¤ëŠ” ì¹œì‚¬íšŒì  ë™ê¸°ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì¸ê°„ ì°¸ì—¬ìê°€ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì„¤ê³„í•œ ì´ˆê¸° ì´ì•¼ê¸°ë¥¼ LLMì´ í™•ì¥í•˜ì—¬ ëŒ€í™”ë¡œ ë°œì „ì‹œí‚¤ëŠ” ë‹¤ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ìƒì„±ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì°¸ê°€ì ê°„ ì •ë³´ ë¹„ëŒ€ì¹­ì„ ìœ ì§€í•˜ì—¬ ì§„ì •í•œ ì„ ì˜ì˜ ê±°ì§“ë§ì„ í‰ê°€í•©ë‹ˆë‹¤. TactfulToMì€ ìµœì‹  ëª¨ë¸ì—ê²Œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•˜ë©°, ì´ ëª¨ë¸ë“¤ì´ ì¸ê°„ë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ì„±ê³¼ë¥¼ ë³´ì„ìœ¼ë¡œì¨ ì„ ì˜ì˜ ê±°ì§“ë§ì„ ì™„ì „íˆ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ ToM ì¶”ë¡  ëŠ¥ë ¥ì˜ í•œê³„ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TactfulToMì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ì‹¤ì œ ëŒ€í™”ì—ì„œ ë°±ìƒ‰ ê±°ì§“ë§ì„ ì´í•´í•˜ê³  ê·¸ ë’¤ì— ìˆëŠ” ì¹œì‚¬íšŒì  ë™ê¸°ë¥¼ ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ìƒˆë¡œìš´ ì˜ì–´ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- 2. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì¸ê°„ ì°¸ì—¬í˜• ë‹¤ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ìƒì„±ë˜ë©°, LLMì´ ìˆ˜ë™ìœ¼ë¡œ ì„¤ê³„ëœ ì‹œë“œ ìŠ¤í† ë¦¬ë¥¼ ëŒ€í™”ë¡œ í™•ì¥í•˜ì—¬ ì°¸ê°€ì ê°„ ì •ë³´ ë¹„ëŒ€ì¹­ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 3. TactfulToMì€ ìµœì‹  ëª¨ë¸ë“¤ì—ê²Œ ë„ì „ì ì¸ ê³¼ì œë¡œ, ì¸ê°„ë³´ë‹¤ í˜„ì €íˆ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ë©° ë°±ìƒ‰ ê±°ì§“ë§ì˜ ì§„ì •í•œ ì´í•´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë§ˆìŒ ì´ë¡ (ToM) ì¶”ë¡  ëŠ¥ë ¥ì˜ í•œê³„ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:42:42*