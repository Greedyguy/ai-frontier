---
keywords:
  - Large Language Model
  - Weighted Co-Training
  - Self-supervised Learning
  - Encoder-Only Networks
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16516
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:39:03.248739",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Weighted Co-Training",
    "Self-supervised Learning",
    "Encoder-Only Networks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Weighted Co-Training": 0.78,
    "Self-supervised Learning": 0.82,
    "Encoder-Only Networks": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the proposed method and connect broadly with existing NLP and ML research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Weighted Co-Training",
        "canonical": "Weighted Co-Training",
        "aliases": [
          "Co-Training",
          "Weighted Co-Training"
        ],
        "category": "unique_technical",
        "rationale": "This novel approach is the core contribution of the paper, offering a new method for semi-supervised learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Semi-Supervised Learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "SSL",
          "Semi-Supervised Learning"
        ],
        "category": "specific_connectable",
        "rationale": "The paper advances SSL techniques, which are pivotal in modern ML frameworks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Encoder-Only Networks",
        "canonical": "Encoder-Only Networks",
        "aliases": [
          "Encoder Networks"
        ],
        "category": "unique_technical",
        "rationale": "These networks are integral to the paper's methodology, distinguishing it from other architectures.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Weighted Co-Training",
      "resolved_canonical": "Weighted Co-Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Semi-Supervised Learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Encoder-Only Networks",
      "resolved_canonical": "Encoder-Only Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# LLM-Guided Co-Training for Text Classification

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16516.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16516](https://arxiv.org/abs/2509.16516)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Federated Learning with Ad-hoc Adapter Insertions_ The Case of Soft-Embeddings for Training Classifier-as-Retriever_20250923|Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever]] (85.2% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.8% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (84.5% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (84.3% similar)
- [[2025-09-22/SENTRA_ Selected-Next-Token Transformer for LLM Text Detection_20250922|SENTRA: Selected-Next-Token Transformer for LLM Text Detection]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Weighted Co-Training|Weighted Co-Training]], [[keywords/Encoder-Only Networks|Encoder-Only Networks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16516v1 Announce Type: new 
Abstract: In this paper, we introduce a novel weighted co-training approach that is guided by Large Language Models (LLMs). Namely, in our co-training approach, we use LLM labels on unlabeled data as target labels and co-train two encoder-only based networks that train each other over multiple iterations: first, all samples are forwarded through each network and historical estimates of each network's confidence in the LLM label are recorded; second, a dynamic importance weight is derived for each sample according to each network's belief in the quality of the LLM label for that sample; finally, the two networks exchange importance weights with each other -- each network back-propagates all samples weighted with the importance weights coming from its peer network and updates its own parameters. By strategically utilizing LLM-generated guidance, our approach significantly outperforms conventional SSL methods, particularly in settings with abundant unlabeled data. Empirical results show that it achieves state-of-the-art performance on 4 out of 5 benchmark datasets and ranks first among 14 compared methods according to the Friedman test. Our results highlight a new direction in semi-supervised learning -- where LLMs serve as knowledge amplifiers, enabling backbone co-training models to achieve state-of-the-art performance efficiently.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì•ˆë‚´í•˜ëŠ” ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ê³µë™ í•™ìŠµ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì—ì„œëŠ” LLMì´ ìƒì„±í•œ ë¼ë²¨ì„ ëª©í‘œ ë¼ë²¨ë¡œ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ ì¸ì½”ë” ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ê°€ ì„œë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ê° ë„¤íŠ¸ì›Œí¬ëŠ” LLM ë¼ë²¨ì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ ê¸°ë¡í•˜ê³ , ìƒ˜í”Œì˜ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ì„œë¡œ êµí™˜í•©ë‹ˆë‹¤. ì´ ë°©ì‹ì€ íŠ¹íˆ ë§ì€ ë¯¸ë¼ë²¨ ë°ì´í„°ê°€ ìˆëŠ” í™˜ê²½ì—ì„œ ê¸°ì¡´ì˜ ë°˜ì§€ë„ í•™ìŠµ(SSL) ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, 5ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì¤‘ 4ê°œì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, 14ê°œì˜ ë¹„êµ ë°©ë²• ì¤‘ í”„ë¦¬ë“œë¨¼ í…ŒìŠ¤íŠ¸ì—ì„œ 1ìœ„ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì„ ì§€ì‹ ì¦í­ê¸°ë¡œ í™œìš©í•˜ì—¬ ê³µë™ í•™ìŠµ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìƒˆë¡œìš´ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì•ˆë‚´í•˜ëŠ” ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ê³µë™ í•™ìŠµ ì ‘ê·¼ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 2. LLM ë ˆì´ë¸”ì„ ëª©í‘œ ë ˆì´ë¸”ë¡œ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ ì¸ì½”ë” ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ê°€ ìƒí˜¸ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ë„¤íŠ¸ì›Œí¬ ê°„ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ë¥¼ êµí™˜í•˜ì—¬ ê° ë„¤íŠ¸ì›Œí¬ê°€ ìƒëŒ€ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë“  ìƒ˜í”Œì„ ì—­ì „íŒŒí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ íŠ¹íˆ ë§ì€ ë¹„ë¼ë²¨ ë°ì´í„° í™˜ê²½ì—ì„œ ê¸°ì¡´ì˜ ë°˜ì§€ë„ í•™ìŠµ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 5. 5ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì¤‘ 4ê°œì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³ , 14ê°œì˜ ë¹„êµ ë°©ë²• ì¤‘ í”„ë¦¬ë“œë¨¼ í…ŒìŠ¤íŠ¸ì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:39:03*