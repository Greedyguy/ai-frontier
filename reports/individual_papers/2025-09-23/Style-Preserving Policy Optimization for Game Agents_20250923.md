---
keywords:
  - Reinforcement Learning
  - Mixed Proximal Policy Optimization
  - Diverse Play Styles
  - Evolutionary Algorithms
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2506.16995
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:28:00.080980",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Mixed Proximal Policy Optimization",
    "Diverse Play Styles",
    "Evolutionary Algorithms"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.8,
    "Mixed Proximal Policy Optimization": 0.85,
    "Diverse Play Styles": 0.78,
    "Evolutionary Algorithms": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "reinforcement learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational technique in game AI, providing strong connectivity with existing literature.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Mixed Proximal Policy Optimization",
        "canonical": "Mixed Proximal Policy Optimization",
        "aliases": [
          "MPPO"
        ],
        "category": "unique_technical",
        "rationale": "MPPO is a novel approach introduced in this paper, crucial for connecting discussions on style-preserving optimization.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.85
      },
      {
        "surface": "diverse play styles",
        "canonical": "Diverse Play Styles",
        "aliases": [
          "play style diversity"
        ],
        "category": "specific_connectable",
        "rationale": "Diverse Play Styles are central to the paper's contribution, linking to broader discussions on agent behavior.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "evolution algorithms",
        "canonical": "Evolutionary Algorithms",
        "aliases": [
          "evolutionary methods"
        ],
        "category": "specific_connectable",
        "rationale": "Evolutionary Algorithms provide a contrasting approach to RL, enhancing connectivity with optimization techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.68,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "proficiency",
      "gaming experience",
      "empirical results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "reinforcement learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Mixed Proximal Policy Optimization",
      "resolved_canonical": "Mixed Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "diverse play styles",
      "resolved_canonical": "Diverse Play Styles",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "evolution algorithms",
      "resolved_canonical": "Evolutionary Algorithms",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.68,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Style-Preserving Policy Optimization for Game Agents

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.16995.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2506.16995](https://arxiv.org/abs/2506.16995)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Reinforcement Learning Agent for a 2D Shooter Game_20250919|Reinforcement Learning Agent for a 2D Shooter Game]] (85.3% similar)
- [[2025-09-18/$Agent^2$_ An Agent-Generates-Agent Framework for Reinforcement Learning Automation_20250918|$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation]] (83.1% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (82.7% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (81.7% similar)
- [[2025-09-19/Beyond the high score_ Prosocial ability profiles of multi-agent populations_20250919|Beyond the high score: Prosocial ability profiles of multi-agent populations]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Diverse Play Styles|Diverse Play Styles]], [[keywords/Evolutionary Algorithms|Evolutionary Algorithms]]
**âš¡ Unique Technical**: [[keywords/Mixed Proximal Policy Optimization|Mixed Proximal Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.16995v3 Announce Type: replace 
Abstract: Proficient game agents with diverse play styles enrich the gaming experience and enhance the replay value of games. However, recent advancements in game AI based on reinforcement learning have predominantly focused on improving proficiency, whereas methods based on evolution algorithms generate agents with diverse play styles but exhibit subpar performance compared to RL methods. To address this gap, this paper proposes Mixed Proximal Policy Optimization (MPPO), a method designed to improve the proficiency of existing suboptimal agents while retaining their distinct styles. MPPO unifies loss objectives for both online and offline samples and introduces an implicit constraint to approximate demonstrator policies by adjusting the empirical distribution of samples. Empirical results across environments of varying scales demonstrate that MPPO achieves proficiency levels comparable to, or even superior to, pure online algorithms while preserving demonstrators' play styles. This work presents an effective approach for generating highly proficient and diverse game agents, ultimately contributing to more engaging gameplay experiences.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê²Œì„ AIì—ì„œ ë‹¤ì–‘í•œ í”Œë ˆì´ ìŠ¤íƒ€ì¼ì„ ê°€ì§„ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµì€ ì£¼ë¡œ ëŠ¥ë ¥ í–¥ìƒì— ì¤‘ì ì„ ë‘ì—ˆê³ , ì§„í™” ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì„ ìƒì„±í•˜ì§€ë§Œ ì„±ëŠ¥ì´ ë‚®ì•˜ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ ë…¼ë¬¸ì€ Mixed Proximal Policy Optimization (MPPO) ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ê¸°ì¡´ì˜ ìµœì ì´ ì•„ë‹Œ ì—ì´ì „íŠ¸ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê·¸ë“¤ì˜ ê³ ìœ í•œ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•©ë‹ˆë‹¤. MPPOëŠ” ì˜¨ë¼ì¸ ë° ì˜¤í”„ë¼ì¸ ìƒ˜í”Œì˜ ì†ì‹¤ ëª©í‘œë¥¼ í†µí•©í•˜ê³ , ìƒ˜í”Œì˜ ê²½í—˜ì  ë¶„í¬ë¥¼ ì¡°ì •í•˜ì—¬ ì‹œì—°ì ì •ì±…ì„ ê·¼ì‚¬í•˜ëŠ” ì•”ë¬µì  ì œì•½ì„ ë„ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MPPOëŠ” ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ìˆœìˆ˜ ì˜¨ë¼ì¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë†’ì€ ìˆ˜ì¤€ì˜ ëŠ¥ë ¥ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ì‹œì—°ìì˜ í”Œë ˆì´ ìŠ¤íƒ€ì¼ì„ ë³´ì¡´í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ëŠ¥ë ¥ ìˆê³  ë‹¤ì–‘í•œ ê²Œì„ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” íš¨ê³¼ì ì¸ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ì—¬ ê²Œì„ í”Œë ˆì´ ê²½í—˜ì„ ë”ìš± í’ë¶€í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ ê²Œì„ AIëŠ” ì£¼ë¡œ ê°•í™” í•™ìŠµì„ í†µí•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆìœ¼ë‚˜, ë‹¤ì–‘í•œ í”Œë ˆì´ ìŠ¤íƒ€ì¼ì„ ìƒì„±í•˜ëŠ” ì§„í™” ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ë°©ë²•ì€ ì„±ëŠ¥ì´ ë‚®ì•˜ë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì€ Mixed Proximal Policy Optimization (MPPO)ì„ ì œì•ˆí•˜ì—¬ ê¸°ì¡´ì˜ ìµœì í™”ë˜ì§€ ì•Šì€ ì—ì´ì „íŠ¸ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê·¸ë“¤ì˜ ë…íŠ¹í•œ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•œë‹¤.
- 3. MPPOëŠ” ì˜¨ë¼ì¸ ë° ì˜¤í”„ë¼ì¸ ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ ëª©í‘œë¥¼ í†µí•©í•˜ê³ , ìƒ˜í”Œì˜ ê²½í—˜ì  ë¶„í¬ë¥¼ ì¡°ì •í•˜ì—¬ ì‹œì—°ì ì •ì±…ì„ ê·¼ì‚¬í•˜ëŠ” ì•”ì‹œì  ì œì•½ì„ ë„ì…í•œë‹¤.
- 4. ë‹¤ì–‘í•œ ê·œëª¨ì˜ í™˜ê²½ì—ì„œ ì‹¤í—˜ ê²°ê³¼, MPPOëŠ” ìˆœìˆ˜ ì˜¨ë¼ì¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí•˜ì—¬ ë™ë“±í•˜ê±°ë‚˜ ë” ìš°ìˆ˜í•œ ëŠ¥ë ¥ ìˆ˜ì¤€ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ì‹œì—°ìì˜ í”Œë ˆì´ ìŠ¤íƒ€ì¼ì„ ë³´ì¡´í•œë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ë†’ì€ ëŠ¥ë ¥ê³¼ ë‹¤ì–‘í•œ ê²Œì„ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” íš¨ê³¼ì ì¸ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ì—¬, ë³´ë‹¤ ëª°ì…ê° ìˆëŠ” ê²Œì„ í”Œë ˆì´ ê²½í—˜ì— ê¸°ì—¬í•œë‹¤.


---

*Generated on 2025-09-24 00:28:00*