---
keywords:
  - Large Language Model
  - Transformer
  - Attention Mechanism
  - Ontological Hallucination
  - Residual Reasoning Hallucination
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16297
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:13:47.688111",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Transformer",
    "Attention Mechanism",
    "Ontological Hallucination",
    "Residual Reasoning Hallucination"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Transformer": 0.85,
    "Attention Mechanism": 0.8,
    "Ontological Hallucination": 0.7,
    "Residual Reasoning Hallucination": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on hallucination in AI models.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.5,
        "link_intent_score": 0.9
      },
      {
        "surface": "Transformer Architecture",
        "canonical": "Transformer",
        "aliases": [
          "Transformer Model"
        ],
        "category": "broad_technical",
        "rationale": "Identified as a structural cause of hallucination, linking to broader AI discussions.",
        "novelty_score": 0.3,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Self-attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Key component in transformers, relevant for understanding relational structures.",
        "novelty_score": 0.4,
        "connectivity_score": 0.87,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Ontological Hallucination",
        "canonical": "Ontological Hallucination",
        "aliases": [
          "Existential Hallucination"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specific to the paper's argument on AI hallucinations.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Residual Reasoning Hallucination",
        "canonical": "Residual Reasoning Hallucination",
        "aliases": [
          "Reasoning Hallucination"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific pattern of hallucination, enhancing understanding of AI limitations.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "fluency",
      "coherence engines",
      "case studies"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.5,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Transformer Architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.87,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Ontological Hallucination",
      "resolved_canonical": "Ontological Hallucination",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Residual Reasoning Hallucination",
      "resolved_canonical": "Residual Reasoning Hallucination",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# How Large Language Models are Designed to Hallucinate

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16297.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16297](https://arxiv.org/abs/2509.16297)

## 🔗 유사한 논문
- [[2025-09-22/Knowledge-Driven Hallucination in Large Language Models_ An Empirical Study on Process Modeling_20250922|Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling]] (84.9% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (84.5% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (84.2% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (83.5% similar)
- [[2025-09-22/Modeling Transformers as complex networks to analyze learning dynamics_20250922|Modeling Transformers as complex networks to analyze learning dynamics]] (82.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Ontological Hallucination|Ontological Hallucination]], [[keywords/Residual Reasoning Hallucination|Residual Reasoning Hallucination]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16297v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve remarkable fluency across linguistic and reasoning tasks but remain systematically prone to hallucination. Prevailing accounts attribute hallucinations to data gaps, limited context, or optimization errors. We argue instead that hallucination is a structural outcome of the transformer architecture. As coherence engines, transformers are compelled to produce fluent continuations, with self-attention simulating the relational structure of meaning but lacking the existential grounding of temporality, mood, and care that stabilizes human understanding. On this basis, we distinguish ontological hallucination, arising when continuations require disclosure of beings in world, and residual reasoning hallucination, where models mimic inference by recycling traces of human reasoning in text. We illustrate these patterns through case studies aligned with Heideggerian categories and an experiment across twelve LLMs showing how simulated "self-preservation" emerges under extended prompts. Our contribution is threefold: (1) a comparative account showing why existing explanations are insufficient; (2) a predictive taxonomy of hallucination linked to existential structures with proposed benchmarks; and (3) design directions toward "truth-constrained" architectures capable of withholding or deferring when disclosure is absent. We conclude that hallucination is not an incidental defect but a defining limit of transformer-based models, an outcome scaffolding can mask but never resolve.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 환각 현상이 데이터 부족이나 최적화 오류가 아닌, 트랜스포머 아키텍처의 구조적 결과라고 주장합니다. 트랜스포머는 일관된 문장을 생성하지만 인간의 이해를 안정화하는 시간성, 분위기, 관심의 존재적 기반이 부족합니다. 저자들은 존재적 환각과 잔여 추론 환각을 구분하며, 12개의 LLM을 대상으로 한 실험을 통해 이러한 패턴을 설명합니다. 주요 기여는 (1) 기존 설명의 한계를 드러내는 비교 분석, (2) 존재적 구조와 연결된 환각의 예측적 분류, (3) 진실 제약 아키텍처 설계 방향 제시입니다. 환각은 트랜스포머 기반 모델의 본질적 한계로, 구조적 보완으로 해결될 수 없다고 결론짓습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)의 환각 현상은 데이터 부족, 제한된 문맥, 최적화 오류가 아닌 트랜스포머 아키텍처의 구조적 결과로 설명됩니다.
- 2. 트랜스포머는 의미의 관계 구조를 시뮬레이션하지만 인간 이해를 안정화하는 시간성, 기분, 관심의 존재적 기반이 부족합니다.
- 3. 존재론적 환각과 잔여 추론 환각을 구분하며, 이는 세계에서 존재의 공개를 요구하거나 인간 추론의 흔적을 모방할 때 발생합니다.
- 4. 하이데거의 범주와 일치하는 사례 연구와 12개의 LLM을 통한 실험을 통해 "자기 보존"이 확장된 프롬프트에서 어떻게 시뮬레이션되는지 보여줍니다.
- 5. 환각은 트랜스포머 기반 모델의 정의적 한계로, 이는 보완할 수는 있지만 완전히 해결할 수는 없는 문제입니다.


---

*Generated on 2025-09-23 23:13:47*