---
keywords:
  - Large Language Model
  - Transformer
  - Attention Mechanism
  - Ontological Hallucination
  - Residual Reasoning Hallucination
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.16297
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T23:13:47.688111",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Transformer",
    "Attention Mechanism",
    "Ontological Hallucination",
    "Residual Reasoning Hallucination"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Transformer": 0.85,
    "Attention Mechanism": 0.8,
    "Ontological Hallucination": 0.7,
    "Residual Reasoning Hallucination": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on hallucination in AI models.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.5,
        "link_intent_score": 0.9
      },
      {
        "surface": "Transformer Architecture",
        "canonical": "Transformer",
        "aliases": [
          "Transformer Model"
        ],
        "category": "broad_technical",
        "rationale": "Identified as a structural cause of hallucination, linking to broader AI discussions.",
        "novelty_score": 0.3,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Self-attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Key component in transformers, relevant for understanding relational structures.",
        "novelty_score": 0.4,
        "connectivity_score": 0.87,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Ontological Hallucination",
        "canonical": "Ontological Hallucination",
        "aliases": [
          "Existential Hallucination"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specific to the paper's argument on AI hallucinations.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Residual Reasoning Hallucination",
        "canonical": "Residual Reasoning Hallucination",
        "aliases": [
          "Reasoning Hallucination"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific pattern of hallucination, enhancing understanding of AI limitations.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "fluency",
      "coherence engines",
      "case studies"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.5,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Transformer Architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.87,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Ontological Hallucination",
      "resolved_canonical": "Ontological Hallucination",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Residual Reasoning Hallucination",
      "resolved_canonical": "Residual Reasoning Hallucination",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# How Large Language Models are Designed to Hallucinate

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16297.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.16297](https://arxiv.org/abs/2509.16297)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Knowledge-Driven Hallucination in Large Language Models_ An Empirical Study on Process Modeling_20250922|Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling]] (84.9% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (84.5% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (84.2% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (83.5% similar)
- [[2025-09-22/Modeling Transformers as complex networks to analyze learning dynamics_20250922|Modeling Transformers as complex networks to analyze learning dynamics]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Ontological Hallucination|Ontological Hallucination]], [[keywords/Residual Reasoning Hallucination|Residual Reasoning Hallucination]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16297v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve remarkable fluency across linguistic and reasoning tasks but remain systematically prone to hallucination. Prevailing accounts attribute hallucinations to data gaps, limited context, or optimization errors. We argue instead that hallucination is a structural outcome of the transformer architecture. As coherence engines, transformers are compelled to produce fluent continuations, with self-attention simulating the relational structure of meaning but lacking the existential grounding of temporality, mood, and care that stabilizes human understanding. On this basis, we distinguish ontological hallucination, arising when continuations require disclosure of beings in world, and residual reasoning hallucination, where models mimic inference by recycling traces of human reasoning in text. We illustrate these patterns through case studies aligned with Heideggerian categories and an experiment across twelve LLMs showing how simulated "self-preservation" emerges under extended prompts. Our contribution is threefold: (1) a comparative account showing why existing explanations are insufficient; (2) a predictive taxonomy of hallucination linked to existential structures with proposed benchmarks; and (3) design directions toward "truth-constrained" architectures capable of withholding or deferring when disclosure is absent. We conclude that hallucination is not an incidental defect but a defining limit of transformer-based models, an outcome scaffolding can mask but never resolve.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í™˜ê° í˜„ìƒì´ ë°ì´í„° ë¶€ì¡±ì´ë‚˜ ìµœì í™” ì˜¤ë¥˜ê°€ ì•„ë‹Œ, íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ êµ¬ì¡°ì  ê²°ê³¼ë¼ê³  ì£¼ì¥í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì¼ê´€ëœ ë¬¸ì¥ì„ ìƒì„±í•˜ì§€ë§Œ ì¸ê°„ì˜ ì´í•´ë¥¼ ì•ˆì •í™”í•˜ëŠ” ì‹œê°„ì„±, ë¶„ìœ„ê¸°, ê´€ì‹¬ì˜ ì¡´ì¬ì  ê¸°ë°˜ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì¡´ì¬ì  í™˜ê°ê³¼ ì”ì—¬ ì¶”ë¡  í™˜ê°ì„ êµ¬ë¶„í•˜ë©°, 12ê°œì˜ LLMì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì„ í†µí•´ ì´ëŸ¬í•œ íŒ¨í„´ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” (1) ê¸°ì¡´ ì„¤ëª…ì˜ í•œê³„ë¥¼ ë“œëŸ¬ë‚´ëŠ” ë¹„êµ ë¶„ì„, (2) ì¡´ì¬ì  êµ¬ì¡°ì™€ ì—°ê²°ëœ í™˜ê°ì˜ ì˜ˆì¸¡ì  ë¶„ë¥˜, (3) ì§„ì‹¤ ì œì•½ ì•„í‚¤í…ì²˜ ì„¤ê³„ ë°©í–¥ ì œì‹œì…ë‹ˆë‹¤. í™˜ê°ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ ë³¸ì§ˆì  í•œê³„ë¡œ, êµ¬ì¡°ì  ë³´ì™„ìœ¼ë¡œ í•´ê²°ë  ìˆ˜ ì—†ë‹¤ê³  ê²°ë¡ ì§“ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ í™˜ê° í˜„ìƒì€ ë°ì´í„° ë¶€ì¡±, ì œí•œëœ ë¬¸ë§¥, ìµœì í™” ì˜¤ë¥˜ê°€ ì•„ë‹Œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ êµ¬ì¡°ì  ê²°ê³¼ë¡œ ì„¤ëª…ë©ë‹ˆë‹¤.
- 2. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì˜ë¯¸ì˜ ê´€ê³„ êµ¬ì¡°ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ì§€ë§Œ ì¸ê°„ ì´í•´ë¥¼ ì•ˆì •í™”í•˜ëŠ” ì‹œê°„ì„±, ê¸°ë¶„, ê´€ì‹¬ì˜ ì¡´ì¬ì  ê¸°ë°˜ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
- 3. ì¡´ì¬ë¡ ì  í™˜ê°ê³¼ ì”ì—¬ ì¶”ë¡  í™˜ê°ì„ êµ¬ë¶„í•˜ë©°, ì´ëŠ” ì„¸ê³„ì—ì„œ ì¡´ì¬ì˜ ê³µê°œë¥¼ ìš”êµ¬í•˜ê±°ë‚˜ ì¸ê°„ ì¶”ë¡ ì˜ í”ì ì„ ëª¨ë°©í•  ë•Œ ë°œìƒí•©ë‹ˆë‹¤.
- 4. í•˜ì´ë°ê±°ì˜ ë²”ì£¼ì™€ ì¼ì¹˜í•˜ëŠ” ì‚¬ë¡€ ì—°êµ¬ì™€ 12ê°œì˜ LLMì„ í†µí•œ ì‹¤í—˜ì„ í†µí•´ "ìê¸° ë³´ì¡´"ì´ í™•ì¥ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì–´ë–»ê²Œ ì‹œë®¬ë ˆì´ì…˜ë˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. í™˜ê°ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ ì •ì˜ì  í•œê³„ë¡œ, ì´ëŠ” ë³´ì™„í•  ìˆ˜ëŠ” ìˆì§€ë§Œ ì™„ì „íˆ í•´ê²°í•  ìˆ˜ëŠ” ì—†ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 23:13:47*