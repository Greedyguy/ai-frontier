---
keywords:
  - FP4 Training
  - Hadamard Transformation
  - Stochastic Rounding
  - Tensor Scaling
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17791
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:56:44.490275",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "FP4 Training",
    "Hadamard Transformation",
    "Stochastic Rounding",
    "Tensor Scaling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "FP4 Training": 0.78,
    "Hadamard Transformation": 0.75,
    "Stochastic Rounding": 0.8,
    "Tensor Scaling": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "FP4 training",
        "canonical": "FP4 Training",
        "aliases": [
          "4-bit Floating Point Training"
        ],
        "category": "unique_technical",
        "rationale": "FP4 Training is a distinct topic within low-precision training, offering unique insights into computational efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Hadamard transformations",
        "canonical": "Hadamard Transformation",
        "aliases": [
          "Hadamard Transform"
        ],
        "category": "specific_connectable",
        "rationale": "Hadamard transformations are crucial for optimizing computational efficiency in FP4 training.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "stochastic rounding",
        "canonical": "Stochastic Rounding",
        "aliases": [
          "Randomized Rounding"
        ],
        "category": "specific_connectable",
        "rationale": "Stochastic rounding is a key technique for stabilizing low-precision training, enhancing model performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "tensor scaling",
        "canonical": "Tensor Scaling",
        "aliases": [
          "Scaling of Tensors"
        ],
        "category": "specific_connectable",
        "rationale": "Tensor scaling is integral to managing precision and computational overhead in FP4 training.",
        "novelty_score": 0.58,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "foundation models",
      "computational overheads"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "FP4 training",
      "resolved_canonical": "FP4 Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Hadamard transformations",
      "resolved_canonical": "Hadamard Transformation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "stochastic rounding",
      "resolved_canonical": "Stochastic Rounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "tensor scaling",
      "resolved_canonical": "Tensor Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Elucidating the Design Space of FP4 training

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17791.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17791](https://arxiv.org/abs/2509.17791)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Search-Optimized Quantization in Biomedical Ontology Alignment_20250923|Search-Optimized Quantization in Biomedical Ontology Alignment]] (82.8% similar)
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (82.5% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (82.3% similar)
- [[2025-09-23/PTQTP_ Post-Training Quantization to Trit-Planes for Large Language Models_20250923|PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models]] (82.3% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Hadamard Transformation|Hadamard Transformation]], [[keywords/Stochastic Rounding|Stochastic Rounding]], [[keywords/Tensor Scaling|Tensor Scaling]]
**âš¡ Unique Technical**: [[keywords/FP4 Training|FP4 Training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17791v1 Announce Type: new 
Abstract: The increasing computational demands of foundation models have spurred research into low-precision training, with 4-bit floating-point (\texttt{FP4}) formats emerging as a frontier for maximizing hardware throughput. While numerous techniques have been proposed to stabilize \texttt{FP4} training, they often present isolated solutions with varying, and not always clear, computational overheads. This paper aims to provide a unified view of the design space of \texttt{FP4} training. We introduce a comprehensive, quantisation gradient-based framework for microscaling quantization that allows for a theoretical analysis of the computational costs associated with different stabilization methods on both the forward and backward passes. Using a simulator built on this framework, we conduct an extensive empirical study across a wide range of machine learning tasks, including regression, image classification, diffusion models, and language models. By systematically evaluating thousands of combinations of techniques, such as novel gradient approximations, rounding strategies, and scaling methods, we identify which configurations offer the most favourable performance-to-overhead trade-off. We find that the techniques enabling the best trade-off involve carefully combining Hadamard transformations, tensor scaling and stochastic rounding. We further find that using \texttt{UE5M3} as a scaling factor potentially offers a good compromise between range and precision with manageable computational overhead.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ì´ˆ ëª¨ë¸ì˜ ê³„ì‚° ìš”êµ¬ê°€ ì¦ê°€í•¨ì— ë”°ë¼ í•˜ë“œì›¨ì–´ ì²˜ë¦¬ëŸ‰ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ 4ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì (\texttt{FP4}) í˜•ì‹ì˜ ì €ì •ë°€ë„ í•™ìŠµ ì—°êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. \texttt{FP4} í•™ìŠµì„ ì•ˆì •í™”í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ìˆ ì´ ì œì•ˆë˜ì—ˆìœ¼ë‚˜, ì´ë“¤ì€ ì¢…ì¢… ê°œë³„ì ì¸ í•´ê²°ì±…ìœ¼ë¡œ ëª…í™•í•˜ì§€ ì•Šì€ ê³„ì‚° ë¹„ìš©ì„ ìˆ˜ë°˜í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” \texttt{FP4} í•™ìŠµì˜ ì„¤ê³„ ê³µê°„ì„ í†µí•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ë¯¸ì„¸ ìŠ¤ì¼€ì¼ ì–‘ìí™”ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì•ˆì •í™” ë°©ë²•ì˜ ê³„ì‚° ë¹„ìš©ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ì‹œë®¬ë ˆì´í„°ë¥¼ í™œìš©í•´ íšŒê·€, ì´ë¯¸ì§€ ë¶„ë¥˜, í™•ì‚° ëª¨ë¸, ì–¸ì–´ ëª¨ë¸ ë“± ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì‘ì—…ì„ ëŒ€ìƒìœ¼ë¡œ ê´‘ë²”ìœ„í•œ ì‹¤ì¦ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ìˆ˜ì²œ ê°€ì§€ ê¸°ìˆ  ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼, Hadamard ë³€í™˜, í…ì„œ ìŠ¤ì¼€ì¼ë§, í™•ë¥ ì  ë°˜ì˜¬ë¦¼ì„ ê²°í•©í•œ ë°©ë²•ì´ ê°€ì¥ íš¨ìœ¨ì ì¸ ì„±ëŠ¥-ì˜¤ë²„í—¤ë“œ ê· í˜•ì„ ì œê³µí•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, \texttt{UE5M3} ìŠ¤ì¼€ì¼ë§ íŒ©í„°ê°€ ë²”ìœ„ì™€ ì •ë°€ë„ ì‚¬ì´ì—ì„œ ì ì ˆí•œ íƒ€í˜‘ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì´ˆ ëª¨ë¸ì˜ ê³„ì‚° ìˆ˜ìš” ì¦ê°€ë¡œ ì¸í•´ 4ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì (\texttt{FP4}) í˜•ì‹ì„ í™œìš©í•œ ì €ì •ë°€ë„ í›ˆë ¨ ì—°êµ¬ê°€ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. \texttt{FP4} í›ˆë ¨ì˜ ì„¤ê³„ ê³µê°„ì„ í†µí•©ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ìœ„í•´ ë¯¸ì„¸ ìŠ¤ì¼€ì¼ ì–‘ìí™”ì— ê¸°ë°˜í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ ê¸°ë²•ì˜ ì„±ëŠ¥ê³¼ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ê°€ì¥ ìœ ë¦¬í•œ ì„±ëŠ¥-ì˜¤ë²„í—¤ë“œ ê· í˜•ì„ ì œê³µí•˜ëŠ” êµ¬ì„± ìš”ì†Œë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.
- 4. Hadamard ë³€í™˜, í…ì„œ ìŠ¤ì¼€ì¼ë§, í™•ë¥ ì  ë°˜ì˜¬ë¦¼ì„ ì¡°í•©í•˜ëŠ” ê²ƒì´ ìµœì ì˜ ì„±ëŠ¥-ì˜¤ë²„í—¤ë“œ ê· í˜•ì„ ì œê³µí•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 5. \texttt{UE5M3} ìŠ¤ì¼€ì¼ë§ íŒ©í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ë²”ìœ„ì™€ ì •ë°€ë„ ì‚¬ì´ì—ì„œ ì ì ˆí•œ íƒ€í˜‘ì„ ì´ë£¨ë©´ì„œ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:56:44*