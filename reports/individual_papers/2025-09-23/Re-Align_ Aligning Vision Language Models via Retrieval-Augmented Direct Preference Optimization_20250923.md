---
keywords:
  - Vision-Language Model
  - Direct Preference Optimization
  - Reinforcement Learning from Human Feedback
  - Visual Question-Answering
  - Image Retrieval
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2502.13146
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:01:32.711917",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Direct Preference Optimization",
    "Reinforcement Learning from Human Feedback",
    "Visual Question-Answering",
    "Image Retrieval"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "Direct Preference Optimization": 0.8,
    "Reinforcement Learning from Human Feedback": 0.85,
    "Visual Question-Answering": 0.78,
    "Image Retrieval": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on cross-modal applications and are a trending concept.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "Direct Preference Optimization",
        "canonical": "Direct Preference Optimization",
        "aliases": [
          "DPO"
        ],
        "category": "unique_technical",
        "rationale": "Direct Preference Optimization is a unique technique introduced in the paper for aligning models, offering new insights.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning from Human Feedback",
        "canonical": "Reinforcement Learning from Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "specific_connectable",
        "rationale": "RLHF is a well-known technique that connects with existing literature on model alignment strategies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Visual Question-Answering",
        "canonical": "Visual Question-Answering",
        "aliases": [
          "VQA"
        ],
        "category": "specific_connectable",
        "rationale": "Visual Question-Answering is a specific application area that benefits from the discussed alignment techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Image Retrieval",
        "canonical": "Image Retrieval",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Image Retrieval is a fundamental component of the proposed alignment framework, linking visual data with textual data.",
        "novelty_score": 0.3,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "hallucinations",
      "performance gains",
      "real-world scenarios"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Direct Preference Optimization",
      "resolved_canonical": "Direct Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Human Feedback",
      "resolved_canonical": "Reinforcement Learning from Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Visual Question-Answering",
      "resolved_canonical": "Visual Question-Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Image Retrieval",
      "resolved_canonical": "Image Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.13146.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2502.13146](https://arxiv.org/abs/2502.13146)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization_20250923|Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization]] (87.2% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.5% similar)
- [[2025-09-23/LifeAlign_ Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization_20250923|LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization]] (86.1% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (85.9% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Image Retrieval|Image Retrieval]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning from Human Feedback|Reinforcement Learning from Human Feedback]], [[keywords/Visual Question-Answering|Visual Question-Answering]]
**âš¡ Unique Technical**: [[keywords/Direct Preference Optimization|Direct Preference Optimization]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.13146v3 Announce Type: replace-cross 
Abstract: The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in https://github.com/taco-group/Re-Align.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì˜ í™˜ê° ë¬¸ì œ, íŠ¹íˆ êµì°¨ ëª¨ë‹¬ ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì •ë ¬ í”„ë ˆì„ì›Œí¬ì¸ Re-Alignì„ ì œì•ˆí•©ë‹ˆë‹¤. Re-Alignì€ ì´ë¯¸ì§€ ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì‹œê°ì  ì„ í˜¸ ì‹ í˜¸ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì´ì¤‘ ì„ í˜¸ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ë˜í•œ, ì‹œê°ì  ì„ í˜¸ ëª©í‘œë¥¼ ì¶”ê°€í•œ rDPOë¥¼ ë„ì…í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í™˜ê°ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³ , ì¼ë°˜ì ì¸ ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ(VQA) ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. Re-Alignì€ ë‹¤ì–‘í•œ VLM í¬ê¸°ì™€ ì•„í‚¤í…ì²˜ì—ì„œ ê²¬ê³ ì„±ê³¼ í™•ì¥ì„±ì„ ìœ ì§€í•˜ë©°, ë©€í‹°ëª¨ë‹¬ LLM ì •ë ¬ì— ì¤‘ìš”í•œ ì§„ì „ì„ ì´ë£¹ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì€ ì‹œê°ì  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë²”ìœ„ì™€ ê¸°ëŠ¥ì„ í™•ì¥í•©ë‹ˆë‹¤.
- 2. VLMì€ íŠ¹íˆ êµì°¨ ëª¨ë‹¬ ë¶ˆì¼ì¹˜ í˜•íƒœì˜ í™˜ê°ì— ì·¨ì•½í•˜ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ(RLHF)ê³¼ ì§ì ‘ ì„ í˜¸ ìµœì í™”(DPO)ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
- 3. Re-Alignì€ ì´ë¯¸ì§€ ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì‹œê°ì  ì„ í˜¸ ì‹ í˜¸ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì´ì¤‘ ì„ í˜¸ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ëŠ” ìƒˆë¡œìš´ ì •ë ¬ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 4. rDPOëŠ” í‘œì¤€ ì§ì ‘ ì„ í˜¸ ìµœì í™”ì˜ í™•ì¥ìœ¼ë¡œ, ì‹œê°ì  ì„ í˜¸ ëª©í‘œë¥¼ ì¶”ê°€í•˜ì—¬ ë¯¸ì„¸ ì¡°ì • ì¤‘ì— ì ìš©ë©ë‹ˆë‹¤.
- 5. Re-Alignì€ í™˜ê°ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  ì¼ë°˜ì ì¸ ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ(VQA) ì‘ì—…ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, ë‹¤ì–‘í•œ VLM í¬ê¸°ì™€ ì•„í‚¤í…ì²˜ì— ê±¸ì³ ê²¬ê³ ì„±ê³¼ í™•ì¥ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 03:01:32*