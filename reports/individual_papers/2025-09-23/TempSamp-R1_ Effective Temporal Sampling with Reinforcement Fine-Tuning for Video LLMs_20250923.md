---
keywords:
  - Multimodal Learning
  - Temporal Grounding
  - Reinforcement Learning
  - Chain-of-Thought Reasoning
  - Few-Shot Learning
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.18056
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:07:56.259218",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Temporal Grounding",
    "Reinforcement Learning",
    "Chain-of-Thought Reasoning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Temporal Grounding": 0.78,
    "Reinforcement Learning": 0.83,
    "Chain-of-Thought Reasoning": 0.81,
    "Few-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the growing field of integrating multiple data types, enhancing cross-disciplinary research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Temporal Grounding Tasks",
        "canonical": "Temporal Grounding",
        "aliases": [
          "Video Temporal Grounding"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specific application area within video analysis, offering unique insights.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Fine-Tuning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "Reinforcement Tuning"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader field of reinforcement learning, a key area in adaptive systems.",
        "novelty_score": 0.48,
        "connectivity_score": 0.92,
        "specificity_score": 0.7,
        "link_intent_score": 0.83
      },
      {
        "surface": "Chain-of-Thought Training",
        "canonical": "Chain-of-Thought Reasoning",
        "aliases": [
          "CoT Training"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents an advanced reasoning technique that enhances model interpretability and performance.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.79,
        "link_intent_score": 0.81
      },
      {
        "surface": "Few-Shot Generalization",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Capability"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the model's ability to generalize from limited data, crucial for real-world applications.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Temporal Grounding Tasks",
      "resolved_canonical": "Temporal Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Fine-Tuning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.92,
        "specificity": 0.7,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Chain-of-Thought Training",
      "resolved_canonical": "Chain-of-Thought Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.79,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Few-Shot Generalization",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18056.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.18056](https://arxiv.org/abs/2509.18056)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (86.4% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (86.1% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (84.3% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (83.8% similar)
- [[2025-09-23/AHA -- Predicting What Matters Next_ Online Highlight Detection Without Looking Ahead_20250923|AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead]] (83.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Temporal Grounding|Temporal Grounding]]
**ğŸš€ Evolved Concepts**: [[keywords/Chain-of-Thought Reasoning|Chain-of-Thought Reasoning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18056v1 Announce Type: new 
Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1

## ğŸ“ ìš”ì•½

TempSamp-R1ì€ ë¹„ë””ì˜¤ ì‹œê°„ì  ì—°ê²° ê³¼ì œì— ì í•©í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ì ì‘ íš¨ê³¼ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ ë°©ë²•ì¸ GRPOëŠ” ì •ì±… ì—…ë°ì´íŠ¸ì— ì˜¨-ì •ì±… ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ëŠ” í° ì‹œê°„ì  íƒìƒ‰ ê³µê°„ì„ ê°€ì§„ ê³¼ì œì—ì„œ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. TempSamp-R1ì€ ì˜¤í”„-ì •ì±… ê°ë…ìœ¼ë¡œ ì‹¤ì œ ì£¼ì„ì„ í™œìš©í•˜ì—¬ ì‹œê°„ì ìœ¼ë¡œ ì •í™•í•œ ì§€ì¹¨ì„ ì œê³µí•˜ê³ , ë¹„ì„ í˜• ì†Œí”„íŠ¸ ì–´ë“œë°´í‹°ì§€ ê³„ì‚° ë°©ë²•ì„ í†µí•´ ë³´ìƒ í”¼ë“œë°±ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. ë˜í•œ, CoT ë° ë¹„-CoT ì¶”ë¡  ëª¨ë“œë¥¼ ì§€ì›í•˜ëŠ” ë‹¨ì¼ ëª¨ë¸ì„ ìµœì í™”í•˜ì—¬ ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ ì¿¼ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TempSamp-R1ì€ GRPO ê¸°ë°˜ì˜ ê¸°ì¤€ì„ ëŠ¥ê°€í•˜ë©°, Charades-STA, ActivityNet Captions, QVHighlights ë°ì´í„°ì…‹ì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. TempSamp-R1ì€ ì œí•œëœ ë°ì´í„° í™˜ê²½ì—ì„œë„ ê°•ë ¥í•œ ì†Œìˆ˜ ìƒ· ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TempSamp-R1ì€ ë¹„ë””ì˜¤ ì‹œê°„ì  ê·¸ë¼ìš´ë”© ì‘ì—…ì— ì í•©í•˜ë„ë¡ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ê°•í™” í•™ìŠµìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ê°•í™” í•™ìŠµ ë°©ë²•ì€ ì •ì±… ì—…ë°ì´íŠ¸ì— ì˜¨-ì •ì±… ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì§€ë§Œ, TempSamp-R1ì€ ì˜¤í”„-ì •ì±… ê°ë…ì„ í™œìš©í•˜ì—¬ ì‹œê°„ì  ì •í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 3. TempSamp-R1ì€ ë¹„ì„ í˜• ì†Œí”„íŠ¸ ì–´ë“œë°´í‹°ì§€ ê³„ì‚° ë°©ë²•ì„ í†µí•´ ë³´ìƒ í”¼ë“œë°±ì„ ë¹„ëŒ€ì¹­ ë³€í™˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ í›ˆë ¨ì„ ì•ˆì •í™”í•˜ê³  ë³´ìƒ ê¸°ë°˜ ì—…ë°ì´íŠ¸ì˜ ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤.
- 4. í•˜ì´ë¸Œë¦¬ë“œ Chain-of-Thought(CoT) í›ˆë ¨ íŒ¨ëŸ¬ë‹¤ì„ì„ ì‚¬ìš©í•˜ì—¬ TempSamp-R1ì€ CoT ë° ë¹„-CoT ì¶”ë¡  ëª¨ë“œë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ë‹¨ì¼ í†µí•© ëª¨ë¸ì„ ìµœì í™”í•©ë‹ˆë‹¤.
- 5. TempSamp-R1ì€ Charades-STA, ActivityNet Captions, QVHighlightsì™€ ê°™ì€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì œí•œëœ ë°ì´í„° í™˜ê²½ì—ì„œë„ ê°•ë ¥í•œ ì†Œìˆ˜ ìƒ· ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:07:56*