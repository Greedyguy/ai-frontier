---
keywords:
  - Multimodal Learning
  - Temporal Grounding
  - Reinforcement Learning
  - Chain-of-Thought Reasoning
  - Few-Shot Learning
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.18056
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:07:56.259218",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Temporal Grounding",
    "Reinforcement Learning",
    "Chain-of-Thought Reasoning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Temporal Grounding": 0.78,
    "Reinforcement Learning": 0.83,
    "Chain-of-Thought Reasoning": 0.81,
    "Few-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the growing field of integrating multiple data types, enhancing cross-disciplinary research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Temporal Grounding Tasks",
        "canonical": "Temporal Grounding",
        "aliases": [
          "Video Temporal Grounding"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specific application area within video analysis, offering unique insights.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Fine-Tuning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "Reinforcement Tuning"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader field of reinforcement learning, a key area in adaptive systems.",
        "novelty_score": 0.48,
        "connectivity_score": 0.92,
        "specificity_score": 0.7,
        "link_intent_score": 0.83
      },
      {
        "surface": "Chain-of-Thought Training",
        "canonical": "Chain-of-Thought Reasoning",
        "aliases": [
          "CoT Training"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents an advanced reasoning technique that enhances model interpretability and performance.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.79,
        "link_intent_score": 0.81
      },
      {
        "surface": "Few-Shot Generalization",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Capability"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the model's ability to generalize from limited data, crucial for real-world applications.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Temporal Grounding Tasks",
      "resolved_canonical": "Temporal Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Fine-Tuning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.92,
        "specificity": 0.7,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Chain-of-Thought Training",
      "resolved_canonical": "Chain-of-Thought Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.79,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Few-Shot Generalization",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18056.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.18056](https://arxiv.org/abs/2509.18056)

## 🔗 유사한 논문
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (86.4% similar)
- [[2025-09-22/ChronoForge-RL_ Chronological Forging through Reinforcement Learning for Enhanced Video Understanding_20250922|ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding]] (86.1% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (84.3% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (83.8% similar)
- [[2025-09-23/AHA -- Predicting What Matters Next_ Online Highlight Detection Without Looking Ahead_20250923|AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead]] (83.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Temporal Grounding|Temporal Grounding]]
**🚀 Evolved Concepts**: [[keywords/Chain-of-Thought Reasoning|Chain-of-Thought Reasoning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18056v1 Announce Type: new 
Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1

## 📝 요약

TempSamp-R1은 비디오 시간적 연결 과제에 적합한 다중 모달 대형 언어 모델(MLLMs)의 적응 효과를 향상시키기 위한 새로운 강화 학습 프레임워크입니다. 기존의 강화 학습 방법인 GRPO는 정책 업데이트에 온-정책 샘플링을 사용하지만, 이는 큰 시간적 탐색 공간을 가진 과제에서 비효율적입니다. TempSamp-R1은 오프-정책 감독으로 실제 주석을 활용하여 시간적으로 정확한 지침을 제공하고, 비선형 소프트 어드밴티지 계산 방법을 통해 보상 피드백을 동적으로 조정합니다. 또한, CoT 및 비-CoT 추론 모드를 지원하는 단일 모델을 최적화하여 다양한 복잡도의 쿼리를 효율적으로 처리합니다. 실험 결과, TempSamp-R1은 GRPO 기반의 기준을 능가하며, Charades-STA, ActivityNet Captions, QVHighlights 데이터셋에서 새로운 최고 성능을 기록했습니다. TempSamp-R1은 제한된 데이터 환경에서도 강력한 소수 샷 일반화 능력을 보여줍니다.

## 🎯 주요 포인트

- 1. TempSamp-R1은 비디오 시간적 그라운딩 작업에 적합하도록 멀티모달 대형 언어 모델을 강화 학습으로 미세 조정하는 새로운 프레임워크입니다.
- 2. 기존 강화 학습 방법은 정책 업데이트에 온-정책 샘플링을 사용하지만, TempSamp-R1은 오프-정책 감독을 활용하여 시간적 정확성을 높입니다.
- 3. TempSamp-R1은 비선형 소프트 어드밴티지 계산 방법을 통해 보상 피드백을 비대칭 변환으로 동적으로 조정하여 훈련을 안정화하고 보상 기반 업데이트의 분산을 줄입니다.
- 4. 하이브리드 Chain-of-Thought(CoT) 훈련 패러다임을 사용하여 TempSamp-R1은 CoT 및 비-CoT 추론 모드를 모두 지원하는 단일 통합 모델을 최적화합니다.
- 5. TempSamp-R1은 Charades-STA, ActivityNet Captions, QVHighlights와 같은 벤치마크 데이터셋에서 새로운 최첨단 성능을 달성하며, 제한된 데이터 환경에서도 강력한 소수 샷 일반화 능력을 보여줍니다.


---

*Generated on 2025-09-24 05:07:56*