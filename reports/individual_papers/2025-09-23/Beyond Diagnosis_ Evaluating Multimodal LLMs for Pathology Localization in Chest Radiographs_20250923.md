---
keywords:
  - Multimodal Learning
  - Pathology Localization
  - Vision-Language Model
  - CheXlocalize Dataset
category: cs.AI
publish_date: 2025-09-23
arxiv_id: 2509.18015
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T00:17:31.233896",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Pathology Localization",
    "Vision-Language Model",
    "CheXlocalize Dataset"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.79,
    "Pathology Localization": 0.72,
    "Vision-Language Model": 0.81,
    "CheXlocalize Dataset": 0.69
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal LLMs",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Large Language Models"
        ],
        "category": "specific_connectable",
        "rationale": "Connects advancements in language models with multimodal capabilities, relevant for linking to recent trends.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Pathology Localization",
        "canonical": "Pathology Localization",
        "aliases": [
          "Disease Localization",
          "Lesion Localization"
        ],
        "category": "unique_technical",
        "rationale": "A specific task in medical imaging that bridges clinical applications and AI models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.72
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents the integration of visual and textual data processing, crucial for understanding multimodal systems.",
        "novelty_score": 0.57,
        "connectivity_score": 0.84,
        "specificity_score": 0.78,
        "link_intent_score": 0.81
      },
      {
        "surface": "CheXlocalize dataset",
        "canonical": "CheXlocalize Dataset",
        "aliases": [
          "CheXlocalize"
        ],
        "category": "unique_technical",
        "rationale": "A specific dataset used for evaluating pathology localization, linking data resources with research outcomes.",
        "novelty_score": 0.73,
        "connectivity_score": 0.62,
        "specificity_score": 0.85,
        "link_intent_score": 0.69
      }
    ],
    "ban_list_suggestions": [
      "diagnostic tasks",
      "clinical utility",
      "spatial understanding"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal LLMs",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Pathology Localization",
      "resolved_canonical": "Pathology Localization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.57,
        "connectivity": 0.84,
        "specificity": 0.78,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "CheXlocalize dataset",
      "resolved_canonical": "CheXlocalize Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.73,
        "connectivity": 0.62,
        "specificity": 0.85,
        "link_intent": 0.69
      }
    }
  ]
}
-->

# Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18015.pdf)
**Category**: cs.AI
**Published**: 2025-09-23
**ArXiv ID**: [2509.18015](https://arxiv.org/abs/2509.18015)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (84.9% similar)
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (83.7% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.8% similar)
- [[2025-09-23/Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation_20250923|Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation]] (82.6% similar)
- [[2025-09-18/Limitations of Public Chest Radiography Datasets for Artificial Intelligence_ Label Quality, Domain Shift, Bias and Evaluation Challenges_20250918|Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Pathology Localization|Pathology Localization]], [[keywords/CheXlocalize Dataset|CheXlocalize Dataset]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18015v1 Announce Type: cross 
Abstract: Recent work has shown promising performance of frontier large language models (LLMs) and their multimodal counterparts in medical quizzes and diagnostic tasks, highlighting their potential for broad clinical utility given their accessible, general-purpose nature. However, beyond diagnosis, a fundamental aspect of medical image interpretation is the ability to localize pathological findings. Evaluating localization not only has clinical and educational relevance but also provides insight into a model's spatial understanding of anatomy and disease. Here, we systematically assess two general-purpose MLLMs (GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to localize pathologies on chest radiographs, using a prompting pipeline that overlays a spatial grid and elicits coordinate-based predictions. Averaged across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%), all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark (80.1%). Despite modest performance, error analysis revealed that GPT-5's predictions were largely in anatomically plausible regions, just not always precisely localized. GPT-4 performed well on pathologies with fixed anatomical locations, but struggled with spatially variable findings and exhibited anatomically implausible predictions more frequently. MedGemma demonstrated the lowest performance on all pathologies, showing limited capacity to generalize to this novel task. Our findings highlight both the promise and limitations of current MLLMs in medical imaging and underscore the importance of integrating them with task-specific tools for reliable use.

## ğŸ“ ìš”ì•½

ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ê·¸ ë©€í‹°ëª¨ë‹¬ ë²„ì „ì´ ì˜ë£Œ í€´ì¦ˆì™€ ì§„ë‹¨ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ë³‘ë¦¬í•™ì  ì†Œê²¬ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ì€ ì—¬ì „íˆ ë„ì „ ê³¼ì œì…ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì¼ë°˜ ëª©ì ì˜ MLLM(GPT-4, GPT-5)ê³¼ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸(MedGemma)ì´ í‰ë¶€ Xì„ ì—ì„œ ë³‘ë¦¬í•™ì  ì†Œê²¬ì„ ì–¼ë§ˆë‚˜ ì˜ ìœ„ì¹˜í™”í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í–ˆìŠµë‹ˆë‹¤. CheXlocalize ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²°ê³¼, GPT-5ëŠ” 49.7%ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ê³ , GPT-4ëŠ” 39.1%, MedGemmaëŠ” 17.7%ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹ì • ì‘ì—…ì— ìµœì í™”ëœ CNN(59.9%)ê³¼ ë°©ì‚¬ì„  ì „ë¬¸ì˜(80.1%)ì˜ ì •í™•ë„ë³´ë‹¤ ë‚®ì•˜ìŠµë‹ˆë‹¤. GPT-5ëŠ” í•´ë¶€í•™ì ìœ¼ë¡œ ê·¸ëŸ´ë“¯í•œ ì˜ì—­ì—ì„œ ì˜ˆì¸¡í–ˆìœ¼ë‚˜ ì •í™•í•œ ìœ„ì¹˜í™”ëŠ” ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤. GPT-4ëŠ” ê³ ì •ëœ í•´ë¶€í•™ì  ìœ„ì¹˜ì˜ ë³‘ë¦¬ì—ì„œ ì˜ ì‘ë™í–ˆìœ¼ë‚˜, ê°€ë³€ì ì¸ ìœ„ì¹˜ì—ì„œëŠ” ì–´ë ¤ì›€ì„ ê²ªì—ˆìŠµë‹ˆë‹¤. MedGemmaëŠ” ëª¨ë“  ë³‘ë¦¬ì—ì„œ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” í˜„ì¬ MLLMì˜ ê°€ëŠ¥ì„±ê³¼ í•œê³„ë¥¼ ë³´ì—¬ì£¼ë©°, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ìš©ì„ ìœ„í•´ íŠ¹ì • ì‘ì—…ì— ë§ì¶˜ ë„êµ¬ì™€ì˜ í†µí•©ì´ ì¤‘ìš”í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ê³¼ ê·¸ ë©€í‹°ëª¨ë‹¬ ë²„ì „ì€ ì˜ë£Œ í€´ì¦ˆ ë° ì§„ë‹¨ ì‘ì—…ì—ì„œ ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ë³‘ë¦¬í•™ì  ì†Œê²¬ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ì€ ì—¬ì „íˆ ê³¼ì œë¡œ ë‚¨ì•„ìˆë‹¤.
- 2. GPT-5ëŠ” í‰ë¶€ Xì„  ì‚¬ì§„ì—ì„œ ë³‘ë¦¬í•™ì  ì†Œê²¬ì„ ìœ„ì¹˜í™”í•˜ëŠ” ë° ìˆì–´ 49.7%ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ìœ¼ë©°, ì´ëŠ” GPT-4(39.1%)ì™€ MedGemma(17.7%)ë³´ë‹¤ ë†’ì•˜ìœ¼ë‚˜, íŠ¹ì • ê³¼ì œì— íŠ¹í™”ëœ CNN(59.9%)ê³¼ ë°©ì‚¬ì„  ì „ë¬¸ì˜ ê¸°ì¤€(80.1%)ë³´ë‹¤ëŠ” ë‚®ì•˜ë‹¤.
- 3. GPT-5ëŠ” í•´ë¶€í•™ì ìœ¼ë¡œ ê·¸ëŸ´ë“¯í•œ ì˜ì—­ì—ì„œ ì˜ˆì¸¡ì„ í–ˆìœ¼ë‚˜, í•­ìƒ ì •í™•íˆ ìœ„ì¹˜í™”í•˜ì§€ëŠ” ëª»í–ˆë‹¤.
- 4. GPT-4ëŠ” ê³ ì •ëœ í•´ë¶€í•™ì  ìœ„ì¹˜ì— ìˆëŠ” ë³‘ë¦¬í•™ì  ì†Œê²¬ì—ì„œëŠ” ì˜ ìˆ˜í–‰í–ˆìœ¼ë‚˜, ê³µê°„ì ìœ¼ë¡œ ë³€ë™ì´ ìˆëŠ” ì†Œê²¬ì—ì„œëŠ” ì–´ë ¤ì›€ì„ ê²ªì—ˆë‹¤.
- 5. MedGemmaëŠ” ëª¨ë“  ë³‘ë¦¬í•™ì  ì†Œê²¬ì—ì„œ ê°€ì¥ ë‚®ì€ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©°, ìƒˆë¡œìš´ ê³¼ì œì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì œí•œì ì´ì—ˆë‹¤.


---

*Generated on 2025-09-24 00:17:31*