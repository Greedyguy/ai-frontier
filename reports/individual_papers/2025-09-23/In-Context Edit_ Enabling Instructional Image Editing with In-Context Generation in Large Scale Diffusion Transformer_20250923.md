---
keywords:
  - Diffusion Transformer
  - Vision-Language Model
  - In-Context Editing
  - Instruction-based Image Editing
  - Early Filter Inference-Time Scaling
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2504.20690
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T05:21:28.996087",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Transformer",
    "Vision-Language Model",
    "In-Context Editing",
    "Instruction-based Image Editing",
    "Early Filter Inference-Time Scaling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Transformer": 0.78,
    "Vision-Language Model": 0.81,
    "In-Context Editing": 0.77,
    "Instruction-based Image Editing": 0.75,
    "Early Filter Inference-Time Scaling": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformers",
        "canonical": "Diffusion Transformer",
        "aliases": [
          "DiTs"
        ],
        "category": "unique_technical",
        "rationale": "Diffusion Transformers represent a novel approach within the Transformer architecture, enabling new capabilities in image editing.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for understanding and generating image content based on textual instructions, linking visual and language modalities.",
        "novelty_score": 0.58,
        "connectivity_score": 0.85,
        "specificity_score": 0.79,
        "link_intent_score": 0.81
      },
      {
        "surface": "In-Context Editing",
        "canonical": "In-Context Editing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "In-Context Editing is a specific technique that enhances the precision of image modifications without altering the underlying architecture.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Instruction-based Image Editing",
        "canonical": "Instruction-based Image Editing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's approach, focusing on modifying images through natural language instructions.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Early Filter Inference-Time Scaling",
        "canonical": "Early Filter Inference-Time Scaling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique optimizes the efficiency of the editing process by selecting high-quality noise samples, enhancing performance.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.83,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "fine-tuning",
      "training data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformers",
      "resolved_canonical": "Diffusion Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.85,
        "specificity": 0.79,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "In-Context Editing",
      "resolved_canonical": "In-Context Editing",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Instruction-based Image Editing",
      "resolved_canonical": "Instruction-based Image Editing",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Early Filter Inference-Time Scaling",
      "resolved_canonical": "Early Filter Inference-Time Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.83,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2504.20690.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2504.20690](https://arxiv.org/abs/2504.20690)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/ContextFlow_ Training-Free Video Object Editing via Adaptive Context Enrichment_20250923|ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment]] (84.9% similar)
- [[2025-09-18/EdiVal-Agent_ An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing_20250918|EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing]] (84.5% similar)
- [[2025-09-19/TIDE_ Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement_20250919|TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement]] (82.6% similar)
- [[2025-09-22/RespoDiff_ Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation_20250922|RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation]] (82.0% similar)
- [[2025-09-23/Optimizing Inference in Transformer-Based Models_ A Multi-Method Benchmark_20250923|Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/Diffusion Transformer|Diffusion Transformer]], [[keywords/In-Context Editing|In-Context Editing]], [[keywords/Instruction-based Image Editing|Instruction-based Image Editing]], [[keywords/Early Filter Inference-Time Scaling|Early Filter Inference-Time Scaling]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.20690v2 Announce Type: replace 
Abstract: Instruction-based image editing enables precise modifications via natural language prompts, but existing methods face a precision-efficiency tradeoff: fine-tuning demands massive datasets (>10M) and computational resources, while training-free approaches suffer from weak instruction comprehension. We address this by proposing ICEdit, which leverages the inherent comprehension and generation abilities of large-scale Diffusion Transformers (DiTs) through three key innovations: (1) An in-context editing paradigm without architectural modifications; (2) Minimal parameter-efficient fine-tuning for quality improvement; (3) Early Filter Inference-Time Scaling, which uses VLMs to select high-quality noise samples for efficiency. Experiments show that ICEdit achieves state-of-the-art editing performance with only 0.1\% of the training data and 1\% trainable parameters compared to previous methods. Our approach establishes a new paradigm for balancing precision and efficiency in instructional image editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.

## ğŸ“ ìš”ì•½

ICEditëŠ” ìì—°ì–´ ì§€ì‹œë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì •ë°€í•˜ê²Œ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ê¸°ì¡´ ë°©ë²•ë“¤ì´ ì§ë©´í•œ ì •ë°€ë„ì™€ íš¨ìœ¨ì„± ê°„ì˜ ê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ICEditëŠ” ëŒ€ê·œëª¨ Diffusion Transformersì˜ ì´í•´ ë° ìƒì„± ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬, (1) ì•„í‚¤í…ì²˜ ìˆ˜ì • ì—†ì´ ë§¥ë½ ë‚´ í¸ì§‘, (2) ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •, (3) VLMì„ í†µí•œ ê³ í’ˆì§ˆ ë…¸ì´ì¦ˆ ìƒ˜í”Œ ì„ íƒìœ¼ë¡œ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” Early Filter Inference-Time Scalingì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ICEditëŠ” ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ 0.1%ì˜ í›ˆë ¨ ë°ì´í„°ì™€ 1%ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œìœ¼ë¡œ ìµœì²¨ë‹¨ í¸ì§‘ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì§€ì‹œ ê¸°ë°˜ ì´ë¯¸ì§€ í¸ì§‘ì—ì„œ ì •ë°€ë„ì™€ íš¨ìœ¨ì„±ì˜ ê· í˜•ì„ ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ICEditëŠ” ëŒ€ê·œëª¨ Diffusion Transformers(DiTs)ì˜ ì´í•´ ë° ìƒì„± ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ í¸ì§‘ì˜ ì •ë°€ë„ì™€ íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 2. ICEditëŠ” ì•„í‚¤í…ì²˜ ìˆ˜ì • ì—†ì´ ì»¨í…ìŠ¤íŠ¸ ë‚´ í¸ì§‘ íŒ¨ëŸ¬ë‹¤ì„ì„ ë„ì…í•˜ê³ , ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤.
- 3. VLMsë¥¼ í™œìš©í•œ Early Filter Inference-Time Scalingì„ í†µí•´ ê³ í’ˆì§ˆì˜ ë…¸ì´ì¦ˆ ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 4. ICEditëŠ” ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ 0.1%ì˜ í›ˆë ¨ ë°ì´í„°ì™€ 1%ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œìœ¼ë¡œ ìµœì²¨ë‹¨ í¸ì§‘ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 5. ICEditì˜ ì½”ë“œì™€ ë°ëª¨ëŠ” https://river-zhang.github.io/ICEdit-gh-pages/ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 05:21:28*