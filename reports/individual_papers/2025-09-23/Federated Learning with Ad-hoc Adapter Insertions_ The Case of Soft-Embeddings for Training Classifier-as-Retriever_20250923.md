---
keywords:
  - Federated Learning
  - Differential Privacy
  - Soft Embeddings
  - Classifier-as-Retriever
  - Retrieval Augmented Generation
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.16508
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:38:43.791060",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Federated Learning",
    "Differential Privacy",
    "Soft Embeddings",
    "Classifier-as-Retriever",
    "Retrieval Augmented Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Federated Learning": 0.85,
    "Differential Privacy": 0.8,
    "Soft Embeddings": 0.78,
    "Classifier-as-Retriever": 0.77,
    "Retrieval Augmented Generation": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Federated Learning",
        "canonical": "Federated Learning",
        "aliases": [
          "FL"
        ],
        "category": "specific_connectable",
        "rationale": "Federated Learning is crucial for enabling privacy-preserving and efficient training across distributed devices, making it a key concept for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Differential Privacy",
        "canonical": "Differential Privacy",
        "aliases": [
          "DP"
        ],
        "category": "specific_connectable",
        "rationale": "Differential Privacy is essential for ensuring privacy in federated learning, providing strong connectivity to privacy-preserving techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Soft Embeddings",
        "canonical": "Soft Embeddings",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Soft Embeddings represent a novel approach to enhance encoder capabilities, offering a unique technical link.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Classifier-as-Retriever",
        "canonical": "Classifier-as-Retriever",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This novel retrieval mechanism enhances the retrieval process, providing a unique technical concept for linking.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Retrieval Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "RAG is a trending concept that integrates retrieval with generation, offering strong connectivity to current research trends.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "solution",
      "analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Federated Learning",
      "resolved_canonical": "Federated Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Differential Privacy",
      "resolved_canonical": "Differential Privacy",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Soft Embeddings",
      "resolved_canonical": "Soft Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Classifier-as-Retriever",
      "resolved_canonical": "Classifier-as-Retriever",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Retrieval Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16508.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.16508](https://arxiv.org/abs/2509.16508)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Large Language Models as End-to-end Combinatorial Optimization Solvers_20250923|Large Language Models as End-to-end Combinatorial Optimization Solvers]] (83.7% similar)
- [[2025-09-23/LightRetriever_ A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference_20250923|LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference]] (83.5% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (83.5% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (83.4% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Federated Learning|Federated Learning]], [[keywords/Differential Privacy|Differential Privacy]], [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**âš¡ Unique Technical**: [[keywords/Soft Embeddings|Soft Embeddings]], [[keywords/Classifier-as-Retriever|Classifier-as-Retriever]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16508v1 Announce Type: new 
Abstract: When existing retrieval-augmented generation (RAG) solutions are intended to be used for new knowledge domains, it is necessary to update their encoders, which are taken to be pretrained large language models (LLMs). However, fully finetuning these large models is compute- and memory-intensive, and even infeasible when deployed on resource-constrained edge devices. We propose a novel encoder architecture in this work that addresses this limitation by using a frozen small language model (SLM), which satisfies the memory constraints of edge devices, and inserting a small adapter network before the transformer blocks of the SLM. The trainable adapter takes the token embeddings of the new corpus and learns to produce enhanced soft embeddings for it, while requiring significantly less compute power to update than full fine-tuning. We further propose a novel retrieval mechanism by attaching a classifier head to the SLM encoder, which is trained to learn a similarity mapping of the input embeddings to their corresponding documents. Finally, to enable the online fine-tuning of both (i) the encoder soft embeddings and (ii) the classifier-as-retriever on edge devices, we adopt federated learning (FL) and differential privacy (DP) to achieve an efficient, privacy-preserving, and product-grade training solution. We conduct a theoretical analysis of our methodology, establishing convergence guarantees under mild assumptions on gradient variance when deployed for general smooth nonconvex loss functions. Through extensive numerical experiments, we demonstrate (i) the efficacy of obtaining soft embeddings to enhance the encoder, (ii) training a classifier to improve the retriever, and (iii) the role of FL in achieving speedup.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ì§€ì‹ ë„ë©”ì¸ì— ì ìš©í•  ë•Œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì´ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì†Œí˜• ì–¸ì–´ ëª¨ë¸(SLM)ê³¼ ì–´ëŒ‘í„° ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•œ ìƒˆë¡œìš´ ì¸ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì—£ì§€ ë””ë°”ì´ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì œì•½ì„ ë§Œì¡±ì‹œí‚¤ë©°, ìƒˆë¡œìš´ ì½”í¼ìŠ¤ì˜ í† í° ì„ë² ë”©ì„ ê°œì„ ëœ ì†Œí”„íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë˜í•œ, SLM ì¸ì½”ë”ì— ë¶„ë¥˜ê¸° í—¤ë“œë¥¼ ë¶€ì°©í•˜ì—¬ ì…ë ¥ ì„ë² ë”©ê³¼ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ì„±ì„ í•™ìŠµí•˜ëŠ” ìƒˆë¡œìš´ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œì˜ ì˜¨ë¼ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ ì—°í•© í•™ìŠµ(FL)ê³¼ ì°¨ë“± í”„ë¼ì´ë²„ì‹œ(DP)ë¥¼ ë„ì…í•˜ì—¬ íš¨ìœ¨ì ì´ê³  í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ëŠ” í•™ìŠµ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ë°©ë²•ë¡ ì˜ ìˆ˜ë ´ì„±ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì†Œí”„íŠ¸ ì„ë² ë”©ì˜ íš¨ê³¼, ê²€ìƒ‰ê¸° ì„±ëŠ¥ í–¥ìƒ, FLì˜ ì†ë„ í–¥ìƒ ì—­í• ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìƒˆë¡œìš´ ì§€ì‹ ë„ë©”ì¸ì— ì ìš©í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì˜ RAG ì†”ë£¨ì…˜ì˜ ì¸ì½”ë” ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•˜ì§€ë§Œ, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì€ ìì› ì œì•½ì´ ìˆëŠ” ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œëŠ” ë¹„í˜„ì‹¤ì ì…ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‘ì€ ì–¸ì–´ ëª¨ë¸(SLM)ì„ ê³ ì •í•˜ê³ , SLMì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ ì•ì— ì‘ì€ ì–´ëŒ‘í„° ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚½ì…í•˜ëŠ” ìƒˆë¡œìš´ ì¸ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ì–´ëŒ‘í„°ëŠ” ìƒˆë¡œìš´ ì½”í¼ìŠ¤ì˜ í† í° ì„ë² ë”©ì„ ë°›ì•„ ê°•í™”ëœ ì†Œí”„íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•˜ë©°, ì „ì²´ ë¯¸ì„¸ ì¡°ì •ë³´ë‹¤ ì ì€ ê³„ì‚° ìì›ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.
- 4. SLM ì¸ì½”ë”ì— ë¶„ë¥˜ê¸° í—¤ë“œë¥¼ ë¶€ì°©í•˜ì—¬ ì…ë ¥ ì„ë² ë”©ê³¼ í•´ë‹¹ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ì„± ë§¤í•‘ì„ í•™ìŠµí•˜ëŠ” ìƒˆë¡œìš´ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œ ì¸ì½”ë” ì†Œí”„íŠ¸ ì„ë² ë”©ê³¼ ë¶„ë¥˜ê¸°ë¥¼ ì˜¨ë¼ì¸ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´ ì—°í•© í•™ìŠµê³¼ ì°¨ë“± í”„ë¼ì´ë²„ì‹œë¥¼ ì±„íƒí•˜ì—¬ íš¨ìœ¨ì ì´ê³  í”„ë¼ì´ë²„ì‹œë¥¼ ë³´ì¥í•˜ëŠ” í•™ìŠµ ì†”ë£¨ì…˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:38:43*