---
keywords:
  - Continual Structured Knowledge Reasoning
  - Knowledge Decoupling
  - Large Language Model
  - Memory Consolidation Mechanism
  - Structure-guided Pseudo-data Synthesis
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16929
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:20:45.554084",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Continual Structured Knowledge Reasoning",
    "Knowledge Decoupling",
    "Large Language Model",
    "Memory Consolidation Mechanism",
    "Structure-guided Pseudo-data Synthesis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Continual Structured Knowledge Reasoning": 0.8,
    "Knowledge Decoupling": 0.75,
    "Large Language Model": 0.7,
    "Memory Consolidation Mechanism": 0.72,
    "Structure-guided Pseudo-data Synthesis": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Continual Structured Knowledge Reasoning",
        "canonical": "Continual Structured Knowledge Reasoning",
        "aliases": [
          "CSKR"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific framework that addresses unique challenges in sequential task handling, making it a novel concept in the field.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Knowledge Decoupling",
        "canonical": "Knowledge Decoupling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This mechanism is central to the paper's proposed framework and offers a unique approach to task-specific and task-agnostic reasoning.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are integral to the framework's implementation, providing a strong link to existing research in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Memory Consolidation Mechanism",
        "canonical": "Memory Consolidation Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This mechanism is a key component of the proposed framework, enhancing the model's ability to generalize across tasks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Structure-guided Pseudo-data Synthesis",
        "canonical": "Structure-guided Pseudo-data Synthesis",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This strategy is innovative in enhancing model generalization, making it a unique technical contribution.",
        "novelty_score": 0.82,
        "connectivity_score": 0.68,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Continual Structured Knowledge Reasoning",
      "resolved_canonical": "Continual Structured Knowledge Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Knowledge Decoupling",
      "resolved_canonical": "Knowledge Decoupling",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Memory Consolidation Mechanism",
      "resolved_canonical": "Memory Consolidation Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Structure-guided Pseudo-data Synthesis",
      "resolved_canonical": "Structure-guided Pseudo-data Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.68,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16929.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16929](https://arxiv.org/abs/2509.16929)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Reasoning Core_ A Scalable RL Environment for LLM Symbolic Reasoning_20250923|Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning]] (84.2% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (82.6% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (82.4% similar)
- [[2025-09-22/Search and Refine During Think_ Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning_20250922|Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning]] (82.1% similar)
- [[2025-09-23/Retrieval Enhanced Feedback via In-context Neural Error-book_20250923|Retrieval Enhanced Feedback via In-context Neural Error-book]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Memory Consolidation Mechanism|Memory Consolidation Mechanism]]
**âš¡ Unique Technical**: [[keywords/Continual Structured Knowledge Reasoning|Continual Structured Knowledge Reasoning]], [[keywords/Knowledge Decoupling|Knowledge Decoupling]], [[keywords/Structure-guided Pseudo-data Synthesis|Structure-guided Pseudo-data Synthesis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16929v1 Announce Type: new 
Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to handle sequential tasks, where each task involves translating natural language questions into structured queries grounded in structured knowledge. Existing general continual learning approaches face significant challenges when applied to this task, including poor generalization to heterogeneous structured knowledge and inefficient reasoning due to parameter growth as tasks increase. To address these limitations, we propose a novel CSKR framework, \textsc{K-DeCore}, which operates with a fixed number of tunable parameters. Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling mechanism that disentangles the reasoning process into task-specific and task-agnostic stages, effectively bridging the gaps across diverse tasks. Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective memory consolidation mechanism for distinct stages and introduces a structure-guided pseudo-data synthesis strategy to further enhance the model's generalization capabilities. Extensive experiments on four benchmark datasets demonstrate the superiority of \textsc{K-DeCore} over existing continual learning methods across multiple metrics, leveraging various backbone large language models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì—°ì†ì ì¸ êµ¬ì¡°í™” ì§€ì‹ ì¶”ë¡ (CSKR) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ \textsc{K-DeCore}ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CSKRì€ ìì—°ì–´ ì§ˆë¬¸ì„ êµ¬ì¡°í™”ëœ ì§€ì‹ì— ê¸°ë°˜í•œ ì§ˆì˜ë¡œ ë³€í™˜í•˜ëŠ” ì—°ì†ì  ì‘ì—…ì„ ë‹¤ë£¨ë©°, ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì´ì§ˆì ì¸ ì§€ì‹ì— ëŒ€í•œ ì¼ë°˜í™”ì™€ ë¹„íš¨ìœ¨ì ì¸ ì¶”ë¡  ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. \textsc{K-DeCore}ëŠ” ê³ ì •ëœ ìˆ˜ì˜ ì¡°ì • ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©°, ì§€ì‹ ë¶„ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì‘ì—…ë³„ ë° ì‘ì—… ë¹„íŠ¹ì´ì  ë‹¨ê³„ë¡œ ì¶”ë¡  ê³¼ì •ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤. ë˜í•œ, ì´ì¤‘ ê´€ì  ë©”ëª¨ë¦¬ í†µí•© ë©”ì»¤ë‹ˆì¦˜ê³¼ êµ¬ì¡°í™”ëœ ê°€ì´ë“œì˜ ê°€ìƒ ë°ì´í„° ìƒì„± ì „ëµì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë„¤ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, \textsc{K-DeCore}ëŠ” ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Continual Structured Knowledge Reasoning(CSKR)ì€ ìì—°ì–´ ì§ˆë¬¸ì„ êµ¬ì¡°í™”ëœ ì§€ì‹ ê¸°ë°˜ì˜ ì§ˆì˜ë¡œ ë³€í™˜í•˜ëŠ” ì—°ì†ì ì¸ ì‘ì—…ì„ ë‹¤ë£¨ëŠ” ëª¨ë¸ í›ˆë ¨ì„ ëª©í‘œë¡œ í•œë‹¤.
- 2. ê¸°ì¡´ì˜ ì¼ë°˜ì ì¸ ì—°ì† í•™ìŠµ ì ‘ê·¼ë²•ì€ ì´ì§ˆì ì¸ êµ¬ì¡°ì  ì§€ì‹ì— ëŒ€í•œ ì¼ë°˜í™” ë¶€ì¡±ê³¼ ì‘ì—… ì¦ê°€ì— ë”°ë¥¸ ë¹„íš¨ìœ¨ì ì¸ ì¶”ë¡  ë¬¸ì œë¥¼ ê²ªëŠ”ë‹¤.
- 3. \textsc{K-DeCore}ëŠ” ê³ ì •ëœ ìˆ˜ì˜ ì¡°ì • ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¡œ ì‘ë™í•˜ë©°, ì§€ì‹ ë¶„ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ì‘ì—…ë³„ ë° ì‘ì—… ë¹„íŠ¹ì´ì  ë‹¨ê³„ë¡œ ì¶”ë¡  ê³¼ì •ì„ ë¶„ë¦¬í•œë‹¤.
- 4. \textsc{K-DeCore}ëŠ” ì´ì¤‘ ê´€ì ì˜ ë©”ëª¨ë¦¬ í†µí•© ë©”ì»¤ë‹ˆì¦˜ê³¼ êµ¬ì¡° ì•ˆë‚´ ê°€ìƒ ë°ì´í„° í•©ì„± ì „ëµì„ í†µí•©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 5. ë„¤ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, \textsc{K-DeCore}ëŠ” ë‹¤ì–‘í•œ ë°±ë³¸ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì—¬ëŸ¬ ì§€í‘œì—ì„œ ê¸°ì¡´ ì—°ì† í•™ìŠµ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.


---

*Generated on 2025-09-24 03:20:45*