---
keywords:
  - Continual Structured Knowledge Reasoning
  - Knowledge Decoupling
  - Large Language Model
  - Memory Consolidation Mechanism
  - Structure-guided Pseudo-data Synthesis
category: cs.CL
publish_date: 2025-09-23
arxiv_id: 2509.16929
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T03:20:45.554084",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Continual Structured Knowledge Reasoning",
    "Knowledge Decoupling",
    "Large Language Model",
    "Memory Consolidation Mechanism",
    "Structure-guided Pseudo-data Synthesis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Continual Structured Knowledge Reasoning": 0.8,
    "Knowledge Decoupling": 0.75,
    "Large Language Model": 0.7,
    "Memory Consolidation Mechanism": 0.72,
    "Structure-guided Pseudo-data Synthesis": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Continual Structured Knowledge Reasoning",
        "canonical": "Continual Structured Knowledge Reasoning",
        "aliases": [
          "CSKR"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific framework that addresses unique challenges in sequential task handling, making it a novel concept in the field.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Knowledge Decoupling",
        "canonical": "Knowledge Decoupling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This mechanism is central to the paper's proposed framework and offers a unique approach to task-specific and task-agnostic reasoning.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are integral to the framework's implementation, providing a strong link to existing research in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Memory Consolidation Mechanism",
        "canonical": "Memory Consolidation Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This mechanism is a key component of the proposed framework, enhancing the model's ability to generalize across tasks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Structure-guided Pseudo-data Synthesis",
        "canonical": "Structure-guided Pseudo-data Synthesis",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This strategy is innovative in enhancing model generalization, making it a unique technical contribution.",
        "novelty_score": 0.82,
        "connectivity_score": 0.68,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Continual Structured Knowledge Reasoning",
      "resolved_canonical": "Continual Structured Knowledge Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Knowledge Decoupling",
      "resolved_canonical": "Knowledge Decoupling",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Memory Consolidation Mechanism",
      "resolved_canonical": "Memory Consolidation Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Structure-guided Pseudo-data Synthesis",
      "resolved_canonical": "Structure-guided Pseudo-data Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.68,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16929.pdf)
**Category**: cs.CL
**Published**: 2025-09-23
**ArXiv ID**: [2509.16929](https://arxiv.org/abs/2509.16929)

## 🔗 유사한 논문
- [[2025-09-23/Reasoning Core_ A Scalable RL Environment for LLM Symbolic Reasoning_20250923|Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning]] (84.2% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (82.6% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (82.4% similar)
- [[2025-09-22/Search and Refine During Think_ Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning_20250922|Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning]] (82.1% similar)
- [[2025-09-23/Retrieval Enhanced Feedback via In-context Neural Error-book_20250923|Retrieval Enhanced Feedback via In-context Neural Error-book]] (81.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Memory Consolidation Mechanism|Memory Consolidation Mechanism]]
**⚡ Unique Technical**: [[keywords/Continual Structured Knowledge Reasoning|Continual Structured Knowledge Reasoning]], [[keywords/Knowledge Decoupling|Knowledge Decoupling]], [[keywords/Structure-guided Pseudo-data Synthesis|Structure-guided Pseudo-data Synthesis]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16929v1 Announce Type: new 
Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to handle sequential tasks, where each task involves translating natural language questions into structured queries grounded in structured knowledge. Existing general continual learning approaches face significant challenges when applied to this task, including poor generalization to heterogeneous structured knowledge and inefficient reasoning due to parameter growth as tasks increase. To address these limitations, we propose a novel CSKR framework, \textsc{K-DeCore}, which operates with a fixed number of tunable parameters. Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling mechanism that disentangles the reasoning process into task-specific and task-agnostic stages, effectively bridging the gaps across diverse tasks. Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective memory consolidation mechanism for distinct stages and introduces a structure-guided pseudo-data synthesis strategy to further enhance the model's generalization capabilities. Extensive experiments on four benchmark datasets demonstrate the superiority of \textsc{K-DeCore} over existing continual learning methods across multiple metrics, leveraging various backbone large language models.

## 📝 요약

이 논문은 연속적인 구조화 지식 추론(CSKR) 문제를 해결하기 위한 새로운 프레임워크 \textsc{K-DeCore}를 제안합니다. CSKR은 자연어 질문을 구조화된 지식에 기반한 질의로 변환하는 연속적 작업을 다루며, 기존 방법들은 이질적인 지식에 대한 일반화와 비효율적인 추론 문제를 겪습니다. \textsc{K-DeCore}는 고정된 수의 조정 가능한 매개변수를 사용하며, 지식 분리 메커니즘을 통해 작업별 및 작업 비특이적 단계로 추론 과정을 분리합니다. 또한, 이중 관점 메모리 통합 메커니즘과 구조화된 가이드의 가상 데이터 생성 전략을 도입하여 모델의 일반화 능력을 향상시킵니다. 네 가지 벤치마크 데이터셋에서의 실험 결과, \textsc{K-DeCore}는 다양한 대형 언어 모델을 활용하여 기존 방법들보다 우수한 성능을 보였습니다.

## 🎯 주요 포인트

- 1. Continual Structured Knowledge Reasoning(CSKR)은 자연어 질문을 구조화된 지식 기반의 질의로 변환하는 연속적인 작업을 다루는 모델 훈련을 목표로 한다.
- 2. 기존의 일반적인 연속 학습 접근법은 이질적인 구조적 지식에 대한 일반화 부족과 작업 증가에 따른 비효율적인 추론 문제를 겪는다.
- 3. \textsc{K-DeCore}는 고정된 수의 조정 가능한 매개변수로 작동하며, 지식 분리 메커니즘을 도입하여 작업별 및 작업 비특이적 단계로 추론 과정을 분리한다.
- 4. \textsc{K-DeCore}는 이중 관점의 메모리 통합 메커니즘과 구조 안내 가상 데이터 합성 전략을 통합하여 모델의 일반화 능력을 향상시킨다.
- 5. 네 가지 벤치마크 데이터셋에 대한 실험 결과, \textsc{K-DeCore}는 다양한 백본 대형 언어 모델을 활용하여 여러 지표에서 기존 연속 학습 방법보다 우수한 성능을 보인다.


---

*Generated on 2025-09-24 03:20:45*