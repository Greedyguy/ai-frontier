---
keywords:
  - Performative Prediction
  - Stochastic Gradient Descent
  - Variance Reduction
  - Non-convex Models
category: cs.LG
publish_date: 2025-09-23
arxiv_id: 2509.17304
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T01:52:03.716061",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Performative Prediction",
    "Stochastic Gradient Descent",
    "Variance Reduction",
    "Non-convex Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Performative Prediction": 0.8,
    "Stochastic Gradient Descent": 0.85,
    "Variance Reduction": 0.78,
    "Non-convex Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Performative Prediction",
        "canonical": "Performative Prediction",
        "aliases": [
          "PP"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a unique framework in machine learning where model deployment affects data distribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Stochastic Gradient Descent",
        "canonical": "Stochastic Gradient Descent",
        "aliases": [
          "SGD"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental optimization technique in machine learning, relevant for linking with other optimization methods discussed.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Variance Reduction",
        "canonical": "Variance Reduction",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A key technique in the proposed algorithm SPRINT, enhancing its performance over traditional methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Non-convex Models",
        "canonical": "Non-convex Models",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The paper focuses on improving convergence in non-convex settings, which is crucial for understanding the algorithm's applicability.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Performative Prediction",
      "resolved_canonical": "Performative Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Stochastic Gradient Descent",
      "resolved_canonical": "Stochastic Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Variance Reduction",
      "resolved_canonical": "Variance Reduction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Non-convex Models",
      "resolved_canonical": "Non-convex Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# SPRINT: Stochastic Performative Prediction With Variance Reduction

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17304.pdf)
**Category**: cs.LG
**Published**: 2025-09-23
**ArXiv ID**: [2509.17304](https://arxiv.org/abs/2509.17304)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/On the Simplification of Neural Network Architectures for Predictive Process Monitoring_20250923|On the Simplification of Neural Network Architectures for Predictive Process Monitoring]] (81.4% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (81.2% similar)
- [[2025-09-22/SAMPO_Scale-wise Autoregression with Motion PrOmpt for generative world models_20250922|SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models]] (80.7% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (80.7% similar)
- [[2025-09-19/ActivePusher_ Active Learning and Planning with Residual Physics for Nonprehensile Manipulation_20250919|ActivePusher: Active Learning and Planning with Residual Physics for Nonprehensile Manipulation]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Stochastic Gradient Descent|Stochastic Gradient Descent]]
**ğŸ”— Specific Connectable**: [[keywords/Variance Reduction|Variance Reduction]], [[keywords/Non-convex Models|Non-convex Models]]
**âš¡ Unique Technical**: [[keywords/Performative Prediction|Performative Prediction]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17304v1 Announce Type: new 
Abstract: Performative prediction (PP) is an algorithmic framework for optimizing machine learning (ML) models where the model's deployment affects the distribution of the data it is trained on. Compared to traditional ML with fixed data, designing algorithms in PP converging to a stable point -- known as a stationary performative stable (SPS) solution -- is more challenging than the counterpart in conventional ML tasks due to the model-induced distribution shifts. While considerable efforts have been made to find SPS solutions using methods such as repeated gradient descent (RGD) and greedy stochastic gradient descent (SGD-GD), most prior studies assumed a strongly convex loss until a recent work established $\mathcal{O}(1/\sqrt{T})$ convergence of SGD-GD to SPS solutions under smooth, non-convex losses. However, this latest progress is still based on the restricted bounded variance assumption in stochastic gradient estimates and yields convergence bounds with a non-vanishing error neighborhood that scales with the variance. This limitation motivates us to improve convergence rates and reduce error in stochastic optimization for PP, particularly in non-convex settings. Thus, we propose a new algorithm called stochastic performative prediction with variance reduction (SPRINT) and establish its convergence to an SPS solution at a rate of $\mathcal{O}(1/T)$. Notably, the resulting error neighborhood is **independent** of the variance of the stochastic gradients. Experiments on multiple real datasets with non-convex models demonstrate that SPRINT outperforms SGD-GD in both convergence rate and stability.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìˆ˜í–‰ì  ì˜ˆì¸¡(Performative Prediction, PP)ì—ì„œ ëª¨ë¸ ë°°í¬ê°€ ë°ì´í„° ë¶„í¬ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìƒí™©ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê¸°ë²•ë“¤ì€ ê°•í•œ ë³¼ë¡ ì†ì‹¤ì„ ê°€ì •í–ˆìœ¼ë‚˜, ìµœê·¼ ì—°êµ¬ëŠ” ë§¤ë„ëŸ½ê³  ë¹„ë³¼ë¡í•œ ì†ì‹¤ì—ì„œë„ ìˆ˜ë ´ì„±ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” í™•ë¥ ì  ê²½ì‚¬ ì¶”ì •ì˜ ì œí•œëœ ë¶„ì‚° ê°€ì •ì— ê¸°ë°˜í•˜ì—¬, ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¬ê³  ì˜¤ë¥˜ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ëŠ” ë¶„ì‚° ê°ì†Œë¥¼ í†µí•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ SPRINTë¥¼ ì œì•ˆí•˜ë©°, ì´ëŠ” $\mathcal{O}(1/T)$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë³´ì¥í•˜ê³ , ì˜¤ë¥˜ ë²”ìœ„ê°€ ë¶„ì‚°ì— ë…ë¦½ì ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SPRINTëŠ” ë¹„ë³¼ë¡ ëª¨ë¸ì—ì„œ SGD-GDë³´ë‹¤ ë” ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìˆ˜í–‰ ì˜ˆì¸¡(PP)ì€ ëª¨ë¸ ë°°í¬ê°€ ë°ì´í„° ë¶„í¬ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìƒí™©ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. PPì—ì„œì˜ ì•ˆì •ì  í•´(SPS) ì†”ë£¨ì…˜ìœ¼ë¡œì˜ ìˆ˜ë ´ì€ ì „í†µì ì¸ ML ê³¼ì œë³´ë‹¤ ë” ë³µì¡í•˜ë©°, ì´ëŠ” ëª¨ë¸ë¡œ ì¸í•œ ë¶„í¬ ë³€í™” ë•Œë¬¸ì…ë‹ˆë‹¤.
- 3. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ë§¤ë„ëŸ½ê³  ë¹„ë³¼ë¡ ì†ì‹¤ í•˜ì—ì„œ SGD-GDì˜ SPS ì†”ë£¨ì…˜ ìˆ˜ë ´ì„ $\mathcal{O}(1/\sqrt{T})$ë¡œ í™•ë¦½í–ˆìœ¼ë‚˜, ì´ëŠ” í™•ë¥ ì  ê²½ì‚¬ ì¶”ì •ì˜ ì œí•œëœ ë¶„ì‚° ê°€ì •ì— ê¸°ë°˜í•©ë‹ˆë‹¤.
- 4. ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì¸ SPRINTëŠ” í™•ë¥ ì  ê²½ì‚¬ë„ì˜ ë¶„ì‚°ì— ë…ë¦½ì ì¸ ì˜¤ë¥˜ ì´ì›ƒì„ ê°€ì§€ë©°, $\mathcal{O}(1/T)$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 5. ì—¬ëŸ¬ ì‹¤ì œ ë°ì´í„°ì…‹ ì‹¤í—˜ì—ì„œ SPRINTëŠ” ë¹„ë³¼ë¡ ëª¨ë¸ì—ì„œ SGD-GDë³´ë‹¤ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„± ë©´ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 01:52:03*