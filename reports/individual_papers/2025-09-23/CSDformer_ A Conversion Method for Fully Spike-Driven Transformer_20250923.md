---
keywords:
  - Spike-based Transformer
  - CSDformer
  - Neural Network
  - Temporal Decomposition
  - Integrate-and-Fire Neurons
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17461
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:52:16.767176",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Spike-based Transformer",
    "CSDformer",
    "Neural Network",
    "Temporal Decomposition",
    "Integrate-and-Fire Neurons"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Spike-based Transformer": 0.78,
    "CSDformer": 0.8,
    "Neural Network": 0.75,
    "Temporal Decomposition": 0.72,
    "Integrate-and-Fire Neurons": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Spike-based transformer",
        "canonical": "Spike-based Transformer",
        "aliases": [
          "Spike-driven Transformer"
        ],
        "category": "unique_technical",
        "rationale": "This represents a novel architecture that combines spiking neural networks with transformer models, offering a unique approach in the field.",
        "novelty_score": 0.85,
        "connectivity_score": 0.67,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "CSDformer",
        "canonical": "CSDformer",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "CSDformer is a new conversion method specifically designed for fully spike-driven transformers, marking a significant innovation.",
        "novelty_score": 0.9,
        "connectivity_score": 0.7,
        "specificity_score": 0.92,
        "link_intent_score": 0.8
      },
      {
        "surface": "spiking neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "SNN"
        ],
        "category": "broad_technical",
        "rationale": "Spiking neural networks are a key component of the discussed architecture, linking it to broader neural network research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "temporal decomposition technique",
        "canonical": "Temporal Decomposition",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is crucial for converting models into spike-driven formats, representing a specialized method in the field.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Integrate-and-Fire neurons",
        "canonical": "Integrate-and-Fire Neurons",
        "aliases": [
          "I&F Neurons"
        ],
        "category": "specific_connectable",
        "rationale": "These neurons are fundamental to reducing conversion errors and improving performance in spike-driven models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "training costs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Spike-based transformer",
      "resolved_canonical": "Spike-based Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.67,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "CSDformer",
      "resolved_canonical": "CSDformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.7,
        "specificity": 0.92,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "spiking neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "temporal decomposition technique",
      "resolved_canonical": "Temporal Decomposition",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Integrate-and-Fire neurons",
      "resolved_canonical": "Integrate-and-Fire Neurons",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# CSDformer: A Conversion Method for Fully Spike-Driven Transformer

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17461.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17461](https://arxiv.org/abs/2509.17461)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model_20250923|Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model]] (83.6% similar)
- [[2025-09-23/Scaling Efficient LLMs_20250923|Scaling Efficient LLMs]] (83.3% similar)
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (82.2% similar)
- [[2025-09-23/DBConformer_ Dual-Branch Convolutional Transformer for EEG Decoding_20250923|DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding]] (82.0% similar)
- [[2025-09-22/SPACE_ SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks_20250922|SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Integrate-and-Fire Neurons|Integrate-and-Fire Neurons]]
**âš¡ Unique Technical**: [[keywords/Spike-based Transformer|Spike-based Transformer]], [[keywords/CSDformer|CSDformer]], [[keywords/Temporal Decomposition|Temporal Decomposition]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.17461v1 Announce Type: new 
Abstract: Spike-based transformer is a novel architecture aiming to enhance the performance of spiking neural networks while mitigating the energy overhead inherent to transformers. However, methods for generating these models suffer from critical limitations: excessive training costs introduced by direct training methods, or unavoidably hardware-unfriendly operations in existing conversion methods. In this paper, we propose CSDformer, a novel conversion method for fully spike-driven transformers. We tailor a conversion-oriented transformer-based architecture and propose a new function NReLU to replace softmax in self-attention. Subsequently, this model is quantized and trained, and converted into a fully spike-driven model with temporal decomposition technique. Also, we propose delayed Integrate-andFire neurons to reduce conversion errors and improve the performance of spiking models. We evaluate CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1 accuracy under 7 time-steps on ImageNet, demonstrating superiority over state-of-the-art models. Furthermore, CSDformer eliminates the need for training SNNs, thereby reducing training costs (reducing computational resource by 75% and accelerating training speed by 2-3$\times$). To the best of our knowledge, this is the first fully spike-driven transformer-based model developed via conversion method, achieving high performance under ultra-low latency, while dramatically reducing both computational complexity and training overhead.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ìŠ¤íŒŒì´í¬ ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©´ì„œ ì—ë„ˆì§€ ì†Œëª¨ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì¸ CSDformerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ê°€ì§„ ë†’ì€ í›ˆë ¨ ë¹„ìš©ê³¼ í•˜ë“œì›¨ì–´ ë¹„í˜¸í™˜ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³€í™˜ ì§€í–¥ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì™€ ìƒˆë¡œìš´ NReLU í•¨ìˆ˜ë¥¼ ë„ì…í•˜ì—¬ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì–‘ìí™” ë° í›ˆë ¨ í›„ ì‹œê°„ì  ë¶„í•´ ê¸°ë²•ì„ í†µí•´ ì™„ì „í•œ ìŠ¤íŒŒì´í¬ ê¸°ë°˜ ëª¨ë¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ë˜í•œ, ì§€ì—°ëœ ì ë¶„-ë°œí™” ë‰´ëŸ°ì„ ì œì•ˆí•˜ì—¬ ë³€í™˜ ì˜¤ë¥˜ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ImageNet, CIFAR-10, CIFAR-100 ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, ImageNetì—ì„œ 76.36%ì˜ ì •í™•ë„ë¥¼ 7 íƒ€ì„ìŠ¤í… ë‚´ì— ë‹¬ì„±í•˜ë©° ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. CSDformerëŠ” SNN í›ˆë ¨ì´ í•„ìš” ì—†ìœ¼ë©°, í›ˆë ¨ ë¹„ìš©ì„ 75% ì¤„ì´ê³  í›ˆë ¨ ì†ë„ë¥¼ 2-3ë°° ê°€ì†í™”í•©ë‹ˆë‹¤. ì´ëŠ” ì´ˆì €ì§€ì—°ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ê³„ì‚° ë³µì¡ì„±ê³¼ í›ˆë ¨ ë¶€ë‹´ì„ í¬ê²Œ ì¤„ì¸ ìµœì´ˆì˜ ì™„ì „ ìŠ¤íŒŒì´í¬ ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CSDformerëŠ” ìŠ¤íŒŒì´í¬ ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë³€í™˜ ë°©ë²•ìœ¼ë¡œ, ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ëŒ€ì²´í•˜ëŠ” NReLU í•¨ìˆ˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì´ ëª¨ë¸ì€ ì •ëŸ‰í™” ë° í›ˆë ¨ í›„ ì‹œê°„ ë¶„í•´ ê¸°ìˆ ì„ í†µí•´ ì™„ì „í•œ ìŠ¤íŒŒì´í¬ ê¸°ë°˜ ëª¨ë¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
- 3. ì§€ì—°ëœ Integrate-and-Fire ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ ì˜¤ë¥˜ë¥¼ ì¤„ì´ê³  ìŠ¤íŒŒì´í‚¹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. ImageNet, CIFAR-10, CIFAR-100 ë°ì´í„°ì…‹ì—ì„œ CSDformerëŠ” 7 íƒ€ì„ìŠ¤í… ë‚´ì— 76.36%ì˜ top-1 ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ìµœì²¨ë‹¨ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. CSDformerëŠ” SNN í›ˆë ¨ì´ í•„ìš” ì—†ìœ¼ë©°, í›ˆë ¨ ë¹„ìš©ì„ 75% ì ˆê°í•˜ê³  í›ˆë ¨ ì†ë„ë¥¼ 2-3ë°° ê°€ì†í™”í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 04:52:16*