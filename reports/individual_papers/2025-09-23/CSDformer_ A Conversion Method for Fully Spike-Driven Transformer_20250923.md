---
keywords:
  - Spike-based Transformer
  - CSDformer
  - Neural Network
  - Temporal Decomposition
  - Integrate-and-Fire Neurons
category: cs.CV
publish_date: 2025-09-23
arxiv_id: 2509.17461
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T04:52:16.767176",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Spike-based Transformer",
    "CSDformer",
    "Neural Network",
    "Temporal Decomposition",
    "Integrate-and-Fire Neurons"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Spike-based Transformer": 0.78,
    "CSDformer": 0.8,
    "Neural Network": 0.75,
    "Temporal Decomposition": 0.72,
    "Integrate-and-Fire Neurons": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Spike-based transformer",
        "canonical": "Spike-based Transformer",
        "aliases": [
          "Spike-driven Transformer"
        ],
        "category": "unique_technical",
        "rationale": "This represents a novel architecture that combines spiking neural networks with transformer models, offering a unique approach in the field.",
        "novelty_score": 0.85,
        "connectivity_score": 0.67,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "CSDformer",
        "canonical": "CSDformer",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "CSDformer is a new conversion method specifically designed for fully spike-driven transformers, marking a significant innovation.",
        "novelty_score": 0.9,
        "connectivity_score": 0.7,
        "specificity_score": 0.92,
        "link_intent_score": 0.8
      },
      {
        "surface": "spiking neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "SNN"
        ],
        "category": "broad_technical",
        "rationale": "Spiking neural networks are a key component of the discussed architecture, linking it to broader neural network research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "temporal decomposition technique",
        "canonical": "Temporal Decomposition",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is crucial for converting models into spike-driven formats, representing a specialized method in the field.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Integrate-and-Fire neurons",
        "canonical": "Integrate-and-Fire Neurons",
        "aliases": [
          "I&F Neurons"
        ],
        "category": "specific_connectable",
        "rationale": "These neurons are fundamental to reducing conversion errors and improving performance in spike-driven models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "training costs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Spike-based transformer",
      "resolved_canonical": "Spike-based Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.67,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "CSDformer",
      "resolved_canonical": "CSDformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.7,
        "specificity": 0.92,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "spiking neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "temporal decomposition technique",
      "resolved_canonical": "Temporal Decomposition",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Integrate-and-Fire neurons",
      "resolved_canonical": "Integrate-and-Fire Neurons",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# CSDformer: A Conversion Method for Fully Spike-Driven Transformer

## 📋 메타데이터

**Links**: [[daily_digest_20250923|20250923]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.17461.pdf)
**Category**: cs.CV
**Published**: 2025-09-23
**ArXiv ID**: [2509.17461](https://arxiv.org/abs/2509.17461)

## 🔗 유사한 논문
- [[2025-09-23/Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model_20250923|Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model]] (83.6% similar)
- [[2025-09-23/Scaling Efficient LLMs_20250923|Scaling Efficient LLMs]] (83.3% similar)
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (82.2% similar)
- [[2025-09-23/DBConformer_ Dual-Branch Convolutional Transformer for EEG Decoding_20250923|DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding]] (82.0% similar)
- [[2025-09-22/SPACE_ SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks_20250922|SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks]] (81.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Neural Network|Neural Network]]
**🔗 Specific Connectable**: [[keywords/Integrate-and-Fire Neurons|Integrate-and-Fire Neurons]]
**⚡ Unique Technical**: [[keywords/Spike-based Transformer|Spike-based Transformer]], [[keywords/CSDformer|CSDformer]], [[keywords/Temporal Decomposition|Temporal Decomposition]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.17461v1 Announce Type: new 
Abstract: Spike-based transformer is a novel architecture aiming to enhance the performance of spiking neural networks while mitigating the energy overhead inherent to transformers. However, methods for generating these models suffer from critical limitations: excessive training costs introduced by direct training methods, or unavoidably hardware-unfriendly operations in existing conversion methods. In this paper, we propose CSDformer, a novel conversion method for fully spike-driven transformers. We tailor a conversion-oriented transformer-based architecture and propose a new function NReLU to replace softmax in self-attention. Subsequently, this model is quantized and trained, and converted into a fully spike-driven model with temporal decomposition technique. Also, we propose delayed Integrate-andFire neurons to reduce conversion errors and improve the performance of spiking models. We evaluate CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1 accuracy under 7 time-steps on ImageNet, demonstrating superiority over state-of-the-art models. Furthermore, CSDformer eliminates the need for training SNNs, thereby reducing training costs (reducing computational resource by 75% and accelerating training speed by 2-3$\times$). To the best of our knowledge, this is the first fully spike-driven transformer-based model developed via conversion method, achieving high performance under ultra-low latency, while dramatically reducing both computational complexity and training overhead.

## 📝 요약

이 논문에서는 스파이크 기반 트랜스포머의 성능을 향상시키면서 에너지 소모를 줄이기 위한 새로운 방법인 CSDformer를 제안합니다. 기존 방법들이 가진 높은 훈련 비용과 하드웨어 비호환성 문제를 해결하기 위해, 변환 지향 트랜스포머 아키텍처와 새로운 NReLU 함수를 도입하여 소프트맥스를 대체했습니다. 이 모델은 양자화 및 훈련 후 시간적 분해 기법을 통해 완전한 스파이크 기반 모델로 변환됩니다. 또한, 지연된 적분-발화 뉴런을 제안하여 변환 오류를 줄이고 성능을 개선했습니다. ImageNet, CIFAR-10, CIFAR-100 데이터셋에서 테스트한 결과, ImageNet에서 76.36%의 정확도를 7 타임스텝 내에 달성하며 기존 모델보다 우수한 성능을 보였습니다. CSDformer는 SNN 훈련이 필요 없으며, 훈련 비용을 75% 줄이고 훈련 속도를 2-3배 가속화합니다. 이는 초저지연에서 높은 성능을 유지하면서 계산 복잡성과 훈련 부담을 크게 줄인 최초의 완전 스파이크 기반 트랜스포머 모델입니다.

## 🎯 주요 포인트

- 1. CSDformer는 스파이크 기반 트랜스포머를 위한 새로운 변환 방법으로, 소프트맥스를 대체하는 NReLU 함수를 제안합니다.
- 2. 이 모델은 정량화 및 훈련 후 시간 분해 기술을 통해 완전한 스파이크 기반 모델로 변환됩니다.
- 3. 지연된 Integrate-and-Fire 뉴런을 사용하여 변환 오류를 줄이고 스파이킹 모델의 성능을 향상시킵니다.
- 4. ImageNet, CIFAR-10, CIFAR-100 데이터셋에서 CSDformer는 7 타임스텝 내에 76.36%의 top-1 정확도를 달성하며, 최첨단 모델보다 우수한 성능을 보입니다.
- 5. CSDformer는 SNN 훈련이 필요 없으며, 훈련 비용을 75% 절감하고 훈련 속도를 2-3배 가속화합니다.


---

*Generated on 2025-09-24 04:52:16*