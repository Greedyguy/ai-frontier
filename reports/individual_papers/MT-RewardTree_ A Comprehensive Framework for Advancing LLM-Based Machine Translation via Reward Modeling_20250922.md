# MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling

**Korean Title:** MT-RewardTree: ë³´ìƒ ëª¨ë¸ë§ì„ í†µí•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ê¸°ê³„ ë²ˆì—­ ë°œì „ì„ ìœ„í•œ ì¢…í•© í”„ë ˆì„ì›Œí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Token-level Supervision

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (86.6% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (83.0% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (82.8% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (82.3% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (81.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.12123v2 Announce Type: replace-cross 
Abstract: Process reward models (PRMs) have shown success in complex reasoning tasks for large language models (LLMs). However, their application to machine translation (MT) remains underexplored due to the lack of systematic methodologies and evaluation benchmarks. To address this gap, we introduce \textbf{MT-RewardTree}, a comprehensive framework for constructing, evaluating, and deploying process reward models in MT. Unlike traditional vanilla preference pair construction, we propose a novel method for automatically generating token-level preference pairs using approximate Monte Carlo Tree Search (MCTS), which mitigates the prohibitive cost of human annotation for fine-grained steps. Then, we establish the first MT-specific reward model benchmark and provide a systematic comparison of different reward modeling architectures, revealing that token-level supervision effectively captures fine-grained preferences. Experimental results demonstrate that our MT-PRM-Qwen-2.5-3B achieves state-of-the-art performance in both token-level and sequence-level evaluation given the same input prefix. Furthermore, we showcase practical applications where PRMs enable test-time alignment for LLMs without additional alignment training and significantly improve performance in hypothesis ensembling. Our work provides valuable insights into the role of reward models in MT research. Our code and data are released in \href{https://sabijun.github.io/MT_RewardTreePage/}{https://sabijun.github.io/MT\_RewardTreePage}.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2503.12123v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRMs)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì—ì„œ ì„±ê³µì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì²´ê³„ì ì¸ ë°©ë²•ë¡ ê³¼ í‰ê°€ ê¸°ì¤€ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ê¸°ê³„ ë²ˆì—­(MT) ë¶„ì•¼ì—ì„œì˜ ì ìš©ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” MTì—ì„œ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ êµ¬ì¶•, í‰ê°€ ë° ë°°í¬í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ì¸ \textbf{MT-RewardTree}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì „í†µì ì¸ ê¸°ë³¸ ì„ í˜¸ ìŒ êµ¬ì„±ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ëŠ” ê·¼ì‚¬ ëª¬í…Œì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰(MCTS)ì„ ì‚¬ìš©í•˜ì—¬ í† í° ìˆ˜ì¤€ì˜ ì„ í˜¸ ìŒì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ì„¸ë¶€ ë‹¨ê³„ì— ëŒ€í•œ ì¸ê°„ ì£¼ì„ì˜ ê³¼ë„í•œ ë¹„ìš©ì„ ì™„í™”í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ìš°ë¦¬ëŠ” ìµœì´ˆì˜ MT ì „ìš© ë³´ìƒ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì„¤ì •í•˜ê³  ë‹¤ì–‘í•œ ë³´ìƒ ëª¨ë¸ë§ ì•„í‚¤í…ì²˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•˜ì—¬ í† í° ìˆ˜ì¤€ì˜ ê°ë…ì´ ì„¸ë¶€ì ì¸ ì„ í˜¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•¨ì„ ë°í˜€ëƒ…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ìš°ë¦¬ì˜ MT-PRM-Qwen-2.5-3BëŠ” ë™ì¼í•œ ì…ë ¥ ì ‘ë‘ì–´ë¥¼ ì£¼ì—ˆì„ ë•Œ í† í° ìˆ˜ì¤€ ë° ì‹œí€€ìŠ¤ ìˆ˜ì¤€ í‰ê°€ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, PRMsê°€ ì¶”ê°€ì ì¸ ì •ë ¬ í›ˆë ¨ ì—†ì´ LLMsì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì‹œê°„ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  ê°€ì„¤ ì•™ìƒë¸”ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤ìš©ì ì¸ ì‘ìš© ì‚¬ë¡€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” MT ì—°êµ¬ì—ì„œ ë³´ìƒ ëª¨ë¸ì˜ ì—­í• ì— ëŒ€í•œ ê·€ì¤‘í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œì™€ ë°ì´í„°ëŠ” \href{https://sabijun.github.io/MT_RewardTreePage/}{https://sabijun.github.io/MT\_RewardTreePage}ì—ì„œ ê³µê°œë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ê³„ ë²ˆì—­(MT)ì—ì„œ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM)ì˜ í™œìš©ì„ íƒêµ¬í•˜ë©°, MT-RewardTreeë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” í† í° ìˆ˜ì¤€ì˜ ì„ í˜¸ ìŒì„ ìë™ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ë„ì…í•˜ì—¬ ì¸ê°„ ì£¼ì„ì˜ ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤. ë˜í•œ, MTì— íŠ¹í™”ëœ ë³´ìƒ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í•˜ê³  ë‹¤ì–‘í•œ ë³´ìƒ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ì—¬ í† í° ìˆ˜ì¤€ì˜ ê°ë…ì´ ì„¸ë°€í•œ ì„ í˜¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ MT-PRM-Qwen-2.5-3B ëª¨ë¸ì´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, PRMì´ ì¶”ê°€ì ì¸ ì •ë ¬ í›ˆë ¨ ì—†ì´ í…ŒìŠ¤íŠ¸ ì‹œ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  ê°€ì„¤ ì•™ìƒë¸”ë§ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´ì„ ì…ì¦í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” MT ì—°êµ¬ì—ì„œ ë³´ìƒ ëª¨ë¸ì˜ ì—­í• ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MT-RewardTreeëŠ” ê¸°ê³„ ë²ˆì—­ì—ì„œ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM)ì„ êµ¬ì¶•, í‰ê°€ ë° ë°°í¬í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

- 2. ìƒˆë¡œìš´ í† í° ìˆ˜ì¤€ì˜ ì„ í˜¸ ìŒ ìƒì„± ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ì¸ê°„ ì£¼ì„ì˜ ë†’ì€ ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

- 3. MT-RewardTreeëŠ” ìµœì´ˆì˜ ê¸°ê³„ ë²ˆì—­ ì „ìš© ë³´ìƒ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜ë¦½í•˜ê³  ë‹¤ì–‘í•œ ë³´ìƒ ëª¨ë¸ë§ ì•„í‚¤í…ì²˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, MT-PRM-Qwen-2.5-3B ëª¨ë¸ì´ í† í° ë° ì‹œí€€ìŠ¤ ìˆ˜ì¤€ í‰ê°€ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- 5. PRMì€ ì¶”ê°€ì ì¸ ì •ë ¬ í›ˆë ¨ ì—†ì´ í…ŒìŠ¤íŠ¸ ì‹œ LLMì˜ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  ê°€ì„¤ ì•™ìƒë¸”ë§ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:44:32*