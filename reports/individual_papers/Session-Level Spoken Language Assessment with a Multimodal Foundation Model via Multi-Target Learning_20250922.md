# Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning

**Korean Title:** ë‹¤ì¤‘ ëª©í‘œ í•™ìŠµì„ í†µí•œ ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ì´ˆ ëª¨ë¸ì„ í™œìš©í•œ ì„¸ì…˜ ìˆ˜ì¤€ êµ¬ì–´ ì–¸ì–´ í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Session-Level Evaluation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (84.2% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (83.9% similar)
- [[2025-09-19/An LLM-based multi-agent framework for agile effort estimation_20250919|An LLM-based multi-agent framework for agile effort estimation]] (82.4% similar)
- [[2025-09-22/mucAI at BAREC Shared Task 2025_ Towards Uncertainty Aware Arabic Readability Assessment_20250922|mucAI at BAREC Shared Task 2025 Towards Uncertainty Aware Arabic Readability Assessment]] (82.4% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (82.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16025v1 Announce Type: cross 
Abstract: Spoken Language Assessment (SLA) estimates a learner's oral proficiency from spontaneous speech. The growing population of L2 English speakers has intensified the demand for reliable SLA, a critical component of Computer Assisted Language Learning (CALL). Existing efforts often rely on cascaded pipelines, which are prone to error propagation, or end-to-end models that often operate on a short audio window, which might miss discourse-level evidence. This paper introduces a novel multimodal foundation model approach that performs session-level evaluation in a single pass. Our approach couples multi-target learning with a frozen, Whisper ASR model-based speech prior for acoustic-aware calibration, allowing for jointly learning holistic and trait-level objectives of SLA without resorting to handcrafted features. By coherently processing the entire response session of an L2 speaker, the model excels at predicting holistic oral proficiency. Experiments conducted on the Speak & Improve benchmark demonstrate that our proposed approach outperforms the previous state-of-the-art cascaded system and exhibits robust cross-part generalization, producing a compact deployable grader that is tailored for CALL applications.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16025v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: êµ¬ì–´ í‰ê°€(SLA)ëŠ” í•™ìŠµìì˜ ìë°œì  ë°œí™”ë¥¼ í†µí•´ êµ¬ìˆ  ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. L2 ì˜ì–´ ì‚¬ìš©ìì˜ ì¦ê°€ë¡œ ì¸í•´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” SLAì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í–ˆìœ¼ë©°, ì´ëŠ” ì»´í“¨í„° ë³´ì¡° ì–¸ì–´ í•™ìŠµ(CALL)ì˜ ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë…¸ë ¥ì€ ì¢…ì¢… ì˜¤ë¥˜ ì „íŒŒì— ì·¨ì•½í•œ ë‹¨ê³„ì  íŒŒì´í”„ë¼ì¸ì´ë‚˜ ë‹´í™” ìˆ˜ì¤€ì˜ ì¦ê±°ë¥¼ ë†“ì¹  ìˆ˜ ìˆëŠ” ì§§ì€ ì˜¤ë””ì˜¤ ì°½ì—ì„œ ì‘ë™í•˜ëŠ” ì¢…ë‹¨ ê°„ ëª¨ë¸ì— ì˜ì¡´í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì„¸ì…˜ ìˆ˜ì¤€ í‰ê°€ë¥¼ ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ ìˆ˜í–‰í•˜ëŠ” ìƒˆë¡œìš´ ë‹¤ì¤‘ ëª¨ë“œ ê¸°ì´ˆ ëª¨ë¸ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ì¤‘ ëª©í‘œ í•™ìŠµì„ ê³ ì •ëœ Whisper ASR ëª¨ë¸ ê¸°ë°˜ì˜ ìŒì„± ì‚¬ì „ê³¼ ê²°í•©í•˜ì—¬ ìŒí–¥ ì¸ì‹ ë³´ì •ì„ ìˆ˜í–‰í•˜ë©°, ìˆ˜ì‘ì—…ìœ¼ë¡œ ë§Œë“  íŠ¹ì§•ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ SLAì˜ ì „ì²´ì  ë° íŠ¹ì„± ìˆ˜ì¤€ ëª©í‘œë¥¼ ê³µë™ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. L2 í™”ìì˜ ì „ì²´ ì‘ë‹µ ì„¸ì…˜ì„ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì€ ì „ì²´ì ì¸ êµ¬ìˆ  ëŠ¥ë ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë›°ì–´ë‚©ë‹ˆë‹¤. Speak & Improve ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìˆ˜í–‰ëœ ì‹¤í—˜ì€ ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì´ ì´ì „ ìµœì²¨ë‹¨ ë‹¨ê³„ì  ì‹œìŠ¤í…œì„ ëŠ¥ê°€í•˜ë©°, CALL ì‘ìš© í”„ë¡œê·¸ë¨ì— ë§ì¶°ì§„ ì»´íŒ©íŠ¸í•œ ë°°í¬ ê°€ëŠ¥í•œ ì±„ì ê¸°ë¥¼ ìƒì„±í•˜ë©´ì„œ ê°•ë ¥í•œ êµì°¨ ë¶€ë¶„ ì¼ë°˜í™”ë¥¼ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìë°œì  ë°œí™”ë¥¼ í†µí•´ í•™ìŠµìì˜ êµ¬ì–´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” êµ¬ì–´ ì–¸ì–´ í‰ê°€(SLA)ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë¡ ì€ ì˜¤ë¥˜ ì „íŒŒê°€ ì‰¬ìš´ ì—°ì† íŒŒì´í”„ë¼ì¸ì´ë‚˜ ë‹´í™” ìˆ˜ì¤€ì˜ ì¦ê±°ë¥¼ ë†“ì¹  ìˆ˜ ìˆëŠ” ë‹¨ì¼ ì˜¤ë””ì˜¤ ì°½ ê¸°ë°˜ì˜ ì¢…ë‹¨ ê°„ ëª¨ë¸ì— ì˜ì¡´í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì„¸ì…˜ ìˆ˜ì¤€ í‰ê°€ë¥¼ ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ ìˆ˜í–‰í•˜ëŠ” ìƒˆë¡œìš´ ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ì´ˆ ëª¨ë¸ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ Whisper ASR ëª¨ë¸ ê¸°ë°˜ì˜ ìŒí–¥ ì¸ì‹ ë³´ì •ì„ ìœ„í•œ ìŒì„± ì‚¬ì „ê³¼ ë‹¤ì¤‘ ëª©í‘œ í•™ìŠµì„ ê²°í•©í•˜ì—¬, ìˆ˜ì‘ì—… íŠ¹ì§• ì—†ì´ë„ SLAì˜ ì „ì²´ì  ë° íŠ¹ì„± ìˆ˜ì¤€ ëª©í‘œë¥¼ ê³µë™ í•™ìŠµí•©ë‹ˆë‹¤. ì „ì²´ ì‘ë‹µ ì„¸ì…˜ì„ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•˜ì—¬ ì´ì²´ì ì¸ êµ¬ì–´ ëŠ¥ë ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë›°ì–´ë‚˜ë©°, Speak & Improve ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ê¸°ì¡´ ìµœì²¨ë‹¨ ì‹œìŠ¤í…œì„ ëŠ¥ê°€í•˜ê³  CALL ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•œ ì»´íŒ©íŠ¸í•œ í‰ê°€ê¸°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ìë°œì  ë°œí™”ë¥¼ í†µí•´ í•™ìŠµìì˜ êµ¬ì–´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ë°˜ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. ì œì•ˆëœ ëª¨ë¸ì€ Whisper ASR ëª¨ë¸ ê¸°ë°˜ì˜ ìŒí–¥ ì¸ì‹ ë³´ì •ê³¼ ë‹¤ì¤‘ ëª©í‘œ í•™ìŠµì„ ê²°í•©í•˜ì—¬, ìˆ˜ì‘ì—… íŠ¹ì§• ì—†ì´ë„ ì „ì²´ì  ë° íŠ¹ì„± ìˆ˜ì¤€ì˜ êµ¬ì–´ ëŠ¥ë ¥ì„ ê³µë™ í•™ìŠµí•©ë‹ˆë‹¤.

- 3. ì „ì²´ ì‘ë‹µ ì„¸ì…˜ì„ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì€ í•™ìŠµìì˜ ì „ì²´ì ì¸ êµ¬ì–´ ëŠ¥ë ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

- 4. Speak & Improve ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ì ‘ê·¼ë²•ì€ ì´ì „ ìµœì²¨ë‹¨ ê³„ë‹¨ì‹ ì‹œìŠ¤í…œì„ ëŠ¥ê°€í•˜ë©°, CALL ì‘ìš© í”„ë¡œê·¸ë¨ì— ì í•©í•œ ì»´íŒ©íŠ¸í•œ í‰ê°€ê¸°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

- 5. ë³¸ ì—°êµ¬ëŠ” ê°•ë ¥í•œ êµì°¨ ë¶€ë¶„ ì¼ë°˜í™”ë¥¼ í†µí•´ CALL ì‘ìš© í”„ë¡œê·¸ë¨ì— ì í•©í•œ í‰ê°€ê¸°ë¥¼ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:22:42*