# World Modelling Improves Language Model Agents

**Korean Title:** ì„¸ê³„ ëª¨ë¸ë§ì´ ì–¸ì–´ ëª¨ë¸ ì—ì´ì „íŠ¸ë¥¼ ê°œì„ í•œë‹¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Scalable Planning RL Methods

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (85.6% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.4% similar)
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (84.9% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (84.8% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.02918v2 Announce Type: replace 
Abstract: Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.02918v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìƒíƒœ ê¸°ë°˜ í™˜ê²½ì—ì„œì˜ ë„êµ¬ ì‚¬ìš©ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì—ê²Œ ë…íŠ¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•˜ë©°, í™˜ê²½ì—ì„œ ë°˜ë³µì ì¸ ì‹œë„ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ê¸°ì¡´ì˜ í…ŒìŠ¤íŠ¸ ì‹œê°„ ê³„ì‚° ì „ëµì€ ë¹„ì‹¤ìš©ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMsë¥¼ ì‚¬í›„ í›ˆë ¨ ì¤‘ í•¨ìˆ˜ í˜¸ì¶œê³¼ í•¨ê»˜ ìƒíƒœ ì˜ˆì¸¡ ê¸°ëŠ¥ìœ¼ë¡œ ë³´ê°•í•˜ëŠ” ë°©ë²•ì¸ ë™ì  ëª¨ë¸ë§(DyMo)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMsëŠ” ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ í†µí•´ ìì‹ ì˜ í–‰ë™ì˜ ë¯¸ë˜ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Berkeley Function Calling Leaderboard V2ì—ì„œ DyMoëŠ” ì„±ê³µë¥ ì„ í–¥ìƒì‹œí‚¤ê³  í™˜ê°ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ ìê¸° ê²€ì¦ ìƒ˜í”Œë§(SVS)ì— í†µí•©í•˜ì—¬, ì‹œë„ íšŸìˆ˜ kì— ëŒ€í•œ pass^kë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³ , ëª¨ë¸ì´ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œë ¥ì„ ê±°ë¶€í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. DyMoì™€ SVSëŠ” í•¨ê»˜ LLMsì˜ ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ íš¨ê³¼ì„±ê³¼ ì‹ ë¢°ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì—°êµ¬ê°€ ì˜¤ë¼í´ í™˜ê²½ì„ ë°˜ë³µì ìœ¼ë¡œ ì¿¼ë¦¬í•˜ì§€ ì•Šê³  LLM ì¶”ë¡ ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ê³„íš ê°•í™” í•™ìŠµ(RL) ë°©ë²•ìœ¼ë¡œ ë‚˜ì•„ê°€ëŠ” ê¸¸ì„ ì œì‹œí•œë‹¤ê³  ë¯¿ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìƒíƒœê°€ ìˆëŠ” í™˜ê²½ì—ì„œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ìˆì–´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì§ë©´í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DyMoë¼ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. DyMoëŠ” LLMì— ìƒíƒœ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬, í›ˆë ¨ í›„ í•¨ìˆ˜ í˜¸ì¶œê³¼ í•¨ê»˜ ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ í†µí•´ ë¯¸ë˜ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Berkeley Function Calling Leaderboard V2ì—ì„œ ì„±ê³µë¥ ì„ ë†’ì´ê³  í™˜ê°ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ ìê¸° ê²€ì¦ ìƒ˜í”Œë§(SVS)ê³¼ í†µí•©í•˜ì—¬, ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œë ¥ì„ ê±°ë¶€í•˜ê³ , ì‹œë„ íšŸìˆ˜ kì— ë”°ë¥¸ pass^kë¥¼ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. DyMoì™€ SVSëŠ” LLMì˜ ë„êµ¬ ì‚¬ìš© íš¨ê³¼ì™€ ì‹ ë¢°ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ë°˜ë³µì ì¸ í™˜ê²½ ì¿¼ë¦¬ ì—†ì´ í™•ì¥ ê°€ëŠ¥í•œ ê³„íš ê°•í™” í•™ìŠµ ë°©ë²•ìœ¼ë¡œì˜ ê¸¸ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DyMoëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ìƒíƒœ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ë„êµ¬ ì‚¬ìš© ì‹œ ë¯¸ë˜ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

- 2. Berkeley Function Calling Leaderboard V2ì—ì„œ DyMoëŠ” ì„±ê³µë¥ ì„ ë†’ì´ê³  í™˜ê° í˜„ìƒì„ í¬ê²Œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

- 3. DyMoì™€ SVSë¥¼ í†µí•©í•˜ì—¬ ëª¨ë¸ì´ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œë ¥ì„ ê±°ë¶€í•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, pass^k ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.

- 4. ì´ ì—°êµ¬ëŠ” ë°˜ë³µì ì¸ í™˜ê²½ ì¿¼ë¦¬ ì—†ì´ LLM ì¶”ë¡ ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ê³„íš RL ë°©ë²•ì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:32:16*