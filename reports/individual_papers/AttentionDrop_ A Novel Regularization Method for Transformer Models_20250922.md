# AttentionDrop: A Novel Regularization Method for Transformer Models

**Korean Title:** ì–´í…ì…˜ë“œë¡­: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ìœ„í•œ ìƒˆë¡œìš´ ì •ê·œí™” ê¸°ë²•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Stochastic Regularization Techniques

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC) A Cognitive-Inspired Approach for Attention Management in Transformers]] (81.9% similar)
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (81.0% similar)
- [[2025-09-19/Superpose Task-specific Features for Model Merging_20250919|Superpose Task-specific Features for Model Merging]] (80.0% similar)
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (80.0% similar)
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods Reviving Transformer for Graph Clustering]] (80.0% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.12088v2 Announce Type: replace-cross 
Abstract: Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech processing. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. In this research, a unified family of stochastic regularization techniques has been proposed, i.e. AttentionDrop with its three different variants, which operate directly on the self-attention distributions. Hard Attention Masking randomly zeroes out top-k attention logits per query to encourage diverse context utilization, Blurred Attention Smoothing applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions, and Consistency-Regularized AttentionDrop enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss. Results achieved in the study demonstrate that AttentionDrop consistently improves accuracy, calibration, and adversarial robustness over standard Dropout, DropConnect, and R-Drop baselines

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2504.12088v2 ë°œí‘œ ìœ í˜•: êµì²´-í¬ë¡œìŠ¤  
ì´ˆë¡: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ëŠ” ìì—°ì–´ ì²˜ë¦¬, ì»´í“¨í„° ë¹„ì „, ìŒì„± ì²˜ë¦¬ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë“¤ì˜ ì—„ì²­ë‚œ ìš©ëŸ‰ì€ ì¢…ì¢… ê³¼ì í•©ì„ ì´ˆë˜í•˜ë©°, íŠ¹íˆ í›ˆë ¨ ë°ì´í„°ê°€ ì œí•œì ì´ê±°ë‚˜ ì¡ìŒì´ ë§ì€ ê²½ìš°ì— ê·¸ëŸ¬í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ìê¸° ì£¼ì˜ ë¶„í¬ì— ì§ì ‘ ì‘ìš©í•˜ëŠ” ì„¸ ê°€ì§€ ë³€í˜•ì„ ê°€ì§„ AttentionDropì´ë¼ëŠ” í†µí•©ëœ í™•ë¥ ì  ì •ê·œí™” ê¸°ë²•ì„ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. Hard Attention Maskingì€ ì¿¼ë¦¬ë‹¹ ìƒìœ„ kê°œì˜ ì£¼ì˜ ë¡œê·¸ì‡ì„ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë‹¤ì–‘í•œ ë¬¸ë§¥ í™œìš©ì„ ì´‰ì§„í•˜ë©°, Blurred Attention Smoothingì€ ê³¼ë„í•˜ê²Œ ë¾°ì¡±í•œ ë¶„í¬ë¥¼ í™•ì‚°ì‹œí‚¤ê¸° ìœ„í•´ ì£¼ì˜ ë¡œê·¸ì‡ì— ë™ì  ê°€ìš°ì‹œì•ˆ ì»¨ë³¼ë£¨ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤. ë˜í•œ, Consistency-Regularized AttentionDropì€ KL ê¸°ë°˜ì˜ ì¼ê´€ì„± ì†ì‹¤ì„ í†µí•´ ì—¬ëŸ¬ ë…ë¦½ì ì¸ AttentionDrop ë³€ë™ í•˜ì—ì„œ ì¶œë ¥ì˜ ì•ˆì •ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, AttentionDropì€ í‘œì¤€ ë“œë¡­ì•„ì›ƒ(Dropout), ë“œë¡­ì»¤ë„¥íŠ¸(DropConnect), R-Drop ê¸°ì¤€ì„ ì— ë¹„í•´ ì •í™•ë„, ë³´ì •, ì ëŒ€ì  ê²¬ê³ ì„±ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ìì—°ì–´ ì²˜ë¦¬, ì»´í“¨í„° ë¹„ì „, ìŒì„± ì²˜ë¦¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ì˜ ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ í™•ë¥ ì  ì •ê·œí™” ê¸°ë²•ì¸ AttentionDropì„ ì œì•ˆí•©ë‹ˆë‹¤. AttentionDropì€ ì…€í”„ ì–´í…ì…˜ ë¶„í¬ì— ì§ì ‘ ì‘ìš©í•˜ë©°, ì„¸ ê°€ì§€ ë³€í˜•ì„ í¬í•¨í•©ë‹ˆë‹¤. Hard Attention Maskingì€ ë‹¤ì–‘í•œ ë¬¸ë§¥ í™œìš©ì„ ìœ„í•´ íŠ¹ì • ì¿¼ë¦¬ì˜ ìƒìœ„ kê°œì˜ ì–´í…ì…˜ ë¡œì§“ì„ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , Blurred Attention Smoothingì€ ë™ì  ê°€ìš°ì‹œì•ˆ í•©ì„±ì„ í†µí•´ ê³¼ë„í•˜ê²Œ ì§‘ì¤‘ëœ ë¶„í¬ë¥¼ í™•ì‚°ì‹œí‚µë‹ˆë‹¤. Consistency-Regularized AttentionDropì€ KL ê¸°ë°˜ì˜ ì¼ê´€ì„± ì†ì‹¤ì„ í†µí•´ ì—¬ëŸ¬ ë…ë¦½ì ì¸ AttentionDrop ë³€í˜•ì—ì„œ ì¶œë ¥ì˜ ì•ˆì •ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, AttentionDropì€ ê¸°ì¡´ì˜ Dropout, DropConnect, R-Drop ëŒ€ë¹„ ì •í™•ì„±, ë³´ì •, ì ëŒ€ì  ê²¬ê³ ì„±ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transformer ê¸°ë°˜ ì•„í‚¤í…ì²˜ëŠ” ìì—°ì–´ ì²˜ë¦¬, ì»´í“¨í„° ë¹„ì „, ìŒì„± ì²˜ë¦¬ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ë§Œ, ë°ì´í„°ê°€ ì œí•œì ì´ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ ë§ì€ ê²½ìš° ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ìê¸° ì£¼ì˜ ë¶„í¬ì— ì§ì ‘ ì‘ìš©í•˜ëŠ” ì„¸ ê°€ì§€ ë³€í˜•ì˜ AttentionDropì´ë¼ëŠ” í™•ë¥ ì  ì •ê·œí™” ê¸°ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

- 3. Hard Attention Maskingì€ ì¿¼ë¦¬ë‹¹ ìƒìœ„ kê°œì˜ ì£¼ì˜ ë¡œê·¸ì‡ì„ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë‹¤ì–‘í•œ ë¬¸ë§¥ í™œìš©ì„ ì´‰ì§„í•©ë‹ˆë‹¤.

- 4. Blurred Attention Smoothingì€ ë™ì  ê°€ìš°ì‹œì•ˆ ì»¨ë³¼ë£¨ì…˜ì„ í†µí•´ ì§€ë‚˜ì¹˜ê²Œ ë¾°ì¡±í•œ ë¶„í¬ë¥¼ í™•ì‚°ì‹œí‚µë‹ˆë‹¤.

- 5. Consistency-Regularized AttentionDropì€ KL ê¸°ë°˜ì˜ ì¼ê´€ì„± ì†ì‹¤ì„ í†µí•´ ì—¬ëŸ¬ ë…ë¦½ì ì¸ AttentionDrop ë³€ë™ì—ì„œ ì¶œë ¥ì˜ ì•ˆì •ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:46:33*