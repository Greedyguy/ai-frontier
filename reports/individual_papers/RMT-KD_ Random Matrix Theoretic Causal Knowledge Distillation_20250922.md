# RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation

**Korean Title:** RMT-KD: ëœë¤ í–‰ë ¬ ì´ë¡ ì  ì¸ê³¼ ì§€ì‹ ì¦ë¥˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Random Matrix Theory

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Delta Knowledge Distillation for Large Language Models_20250919|Delta Knowledge Distillation for Large Language Models]] (83.4% similar)
- [[2025-09-19/Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities_20250919|Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities]] (81.1% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (80.3% similar)
- [[2025-09-19/TableDART_ Dynamic Adaptive Multi-Modal Routing for Table Understanding_20250919|TableDART Dynamic Adaptive Multi-Modal Routing for Table Understanding]] (79.8% similar)
- [[2025-09-19/Improving Internet Traffic Matrix Prediction via Time Series Clustering_20250919|Improving Internet Traffic Matrix Prediction via Time Series Clustering]] (79.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15724v1 Announce Type: new 
Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE, AG News, and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15724v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: BERT ë° ResNetê³¼ ê°™ì€ ëŒ€í˜• ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì§€ë§Œ, ê·¸ í¬ê¸°ì™€ ì—°ì‚° ìš”êµ¬ë¡œ ì¸í•´ ì—£ì§€ì—ì„œì˜ ë°°í¬ê°€ ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. ìš°ë¦¬ëŠ” RMT-KDë¼ëŠ” ì••ì¶• ë°©ë²•ì„ ì œì‹œí•˜ë©°, ì´ëŠ” ëœë¤ í–‰ë ¬ ì´ë¡ (RMT)ì„ í™œìš©í•œ ì§€ì‹ ì¦ë¥˜ë¥¼ í†µí•´ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤. ê°€ì§€ì¹˜ê¸°ë‚˜ íœ´ë¦¬ìŠ¤í‹± ë­í¬ ì„ íƒ ëŒ€ì‹ , RMT-KDëŠ” ìˆ¨ê²¨ì§„ í‘œí˜„ì˜ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ í†µí•´ ì‹ë³„ëœ ì •ë³´ ë°©í–¥ë§Œì„ ë³´ì¡´í•©ë‹ˆë‹¤. RMT ê¸°ë°˜ì˜ ì¸ê³¼ì  ì¶•ì†ŒëŠ” ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ìê¸° ì¦ë¥˜ì™€ í•¨ê»˜ ê³„ì¸µë³„ë¡œ ì ìš©ë©ë‹ˆë‹¤. GLUE, AG News, CIFAR-10ì—ì„œ RMT-KDëŠ” ìµœëŒ€ 80%ì˜ ë§¤ê°œë³€ìˆ˜ ê°ì†Œì™€ ë‹¨ 2%ì˜ ì •í™•ë„ ì†ì‹¤ë¡œ 2.8ë°° ë¹ ë¥¸ ì¶”ë¡ ê³¼ ê±°ì˜ ì ˆë°˜ì˜ ì „ë ¥ ì†Œë¹„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” RMT-KDë¥¼ ë„¤íŠ¸ì›Œí¬ ì¦ë¥˜ì— ëŒ€í•œ ìˆ˜í•™ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ í™•ë¦½í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

RMT-KDëŠ” ëŒ€í˜• ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í¬ê¸°ì™€ ê³„ì‚° ìš”êµ¬ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëœë¤ í–‰ë ¬ ì´ë¡ (RMT)ì„ í™œìš©í•œ ì§€ì‹ ì¦ë¥˜ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìˆ¨ê²¨ì§„ í‘œí˜„ì˜ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ í†µí•´ ì •ë³´ê°€ í’ë¶€í•œ ë°©í–¥ë§Œì„ ë³´ì¡´í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤. RMT ê¸°ë°˜ì˜ ì¸ê³¼ì  ì¶•ì†ŒëŠ” ê° ì¸µì— ì ìš©ë˜ë©°, ìê¸° ì¦ë¥˜ë¥¼ í†µí•´ ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. GLUE, AG News, CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ RMT-KDëŠ” ìµœëŒ€ 80%ì˜ ë§¤ê°œë³€ìˆ˜ ê°ì†Œì™€ 2%ì˜ ì •í™•ë„ ì†ì‹¤ë§Œìœ¼ë¡œ 2.8ë°° ë¹ ë¥¸ ì¶”ë¡ ê³¼ ì „ë ¥ ì†Œë¹„ ì ˆë°˜ ê°ì†Œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” RMT-KDê°€ ìˆ˜í•™ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ì¦ë¥˜ ë°©ë²•ì„ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RMT-KDëŠ” ëœë¤ í–‰ë ¬ ì´ë¡ ì„ í™œìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¤„ì´ëŠ” ì§€ì‹ ì¦ë¥˜ ë°©ë²•ì…ë‹ˆë‹¤.

- 2. RMT-KDëŠ” ìˆ¨ê²¨ì§„ í‘œí˜„ì˜ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ í†µí•´ ì •ë³´ê°€ ë§ì€ ë°©í–¥ë§Œì„ ë³´ì¡´í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤.

- 3. GLUE, AG News, CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ ìµœëŒ€ 80%ì˜ íŒŒë¼ë¯¸í„° ê°ì†Œì™€ 2%ì˜ ì •í™•ë„ ì†ì‹¤ë¡œ 2.8ë°° ë¹ ë¥¸ ì¶”ë¡ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 4. RMT-KDëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ì¦ë¥˜ ì ‘ê·¼ë²•ìœ¼ë¡œ, ì „ë ¥ ì†Œë¹„ë¥¼ ê±°ì˜ ì ˆë°˜ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

- 5. RMT-KDëŠ” ì¸µë³„ ìê¸° ì¦ë¥˜ë¥¼ í†µí•´ ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì¸ê³¼ì  ê°ì†Œë¥¼ ì ìš©í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:21:56*