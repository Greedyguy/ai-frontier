---
keywords:
  - Few-Shot Learning
  - Large Language Model
  - Weak Supervision
  - Pseudo-Label Correction
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2410.01508
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:16:16.185455",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "Large Language Model",
    "Weak Supervision",
    "Pseudo-Label Correction"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.85,
    "Large Language Model": 0.8,
    "Weak Supervision": 0.78,
    "Pseudo-Label Correction": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "In-Context Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "In-Context Learning is a form of Few-Shot Learning, which is crucial for linking to related concepts in learning paradigms.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and application of In-Context Learning, providing a strong link to related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Weak Supervision",
        "canonical": "Weak Supervision",
        "aliases": [
          "Weakly Supervised Learning"
        ],
        "category": "unique_technical",
        "rationale": "Weak Supervision is a key concept in the paper's methodology, offering a unique perspective on learning paradigms.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Pseudo-Label Correction",
        "canonical": "Pseudo-Label Correction",
        "aliases": [
          "Pseudo-Labeling"
        ],
        "category": "unique_technical",
        "rationale": "Pseudo-Label Correction is a novel approach in the paper, enhancing the understanding of label refinement processes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Weak Supervision",
      "resolved_canonical": "Weak Supervision",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Pseudo-Label Correction",
      "resolved_canonical": "Pseudo-Label Correction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Disentangling Latent Shifts of In-Context Learning with Weak Supervision

**Korean Title:** ë§¥ë½ ë‚´ í•™ìŠµì˜ ì ì¬ì  ë³€í™” ë¶„ë¦¬ë¥¼ ì•½í•œ ê°ë…ìœ¼ë¡œ í•´ê²°í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2410.01508.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2410.01508](https://arxiv.org/abs/2410.01508)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KITE_ Kernelized and Information Theoretic Exemplars for In-Context Learning_20250922|KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning]] (85.6% similar)
- [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (81.4% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (81.2% similar)
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release: Iterative LLM Unlearning with Self-generated Data]] (80.9% similar)
- [[2025-09-22/Global Pre-fixing, Local Adjusting_ A Simple yet Effective Contrastive Strategy for Continual Learning_20250922|Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Weak Supervision|Weak Supervision]], [[keywords/Pseudo-Label Correction|Pseudo-Label Correction]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.01508v2 Announce Type: replace-cross 
Abstract: In-context learning (ICL) enables large language models to perform few-shot learning by conditioning on labeled examples in the prompt. Despite its flexibility, ICL suffers from instability -- especially as prompt length increases with more demonstrations. To address this, we treat ICL as a source of weak supervision and propose a parameter-efficient method that disentangles demonstration-induced latent shifts from those of the query. An ICL-based teacher generates pseudo-labels on unlabeled queries, while a student predicts them using only the query input, updating a lightweight adapter. This captures demonstration effects in a compact, reusable form, enabling efficient inference while remaining composable with new demonstrations. Although trained on noisy teacher outputs, the student often outperforms its teacher through pseudo-label correction and coverage expansion, consistent with the weak-to-strong generalization effect. Empirically, our method improves generalization, stability, and efficiency across both in-domain and out-of-domain tasks, surpassing standard ICL and prior disentanglement methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2410.01508v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ë§¥ë½ ë‚´ í•™ìŠµ(ICL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ì—ì„œ ë ˆì´ë¸”ì´ ìˆëŠ” ì˜ˆì‹œë¥¼ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬ ëª‡ ê°€ì§€ ìƒ· í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ê·¸ ìœ ì—°ì„±ì—ë„ ë¶ˆêµ¬í•˜ê³ , ICLì€ ë¶ˆì•ˆì •ì„± ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. íŠ¹íˆ, ë” ë§ì€ ë°ëª¨ê°€ í¬í•¨ë˜ì–´ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì´ëŸ¬í•œ ë¬¸ì œê°€ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ICLì„ ì•½í•œ ê°ë…ì˜ ì›ì²œìœ¼ë¡œ ê°„ì£¼í•˜ê³ , ë°ëª¨ì— ì˜í•´ ìœ ë„ëœ ì ì¬ì  ë³€í™”ë¥¼ ì¿¼ë¦¬ì˜ ë³€í™”ì™€ ë¶„ë¦¬í•˜ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ICL ê¸°ë°˜ì˜ êµì‚¬ëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì§œ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê³ , í•™ìƒì€ ì¿¼ë¦¬ ì…ë ¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ì˜ˆì¸¡í•˜ë©°, ê²½ëŸ‰ ì–´ëŒ‘í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ëŠ” ë°ëª¨ íš¨ê³¼ë¥¼ ê°„ê²°í•˜ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ í¬ì°©í•˜ì—¬, ìƒˆë¡œìš´ ë°ëª¨ì™€ ì¡°í•© ê°€ëŠ¥í•˜ë©´ì„œë„ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë¹„ë¡ êµì‚¬ì˜ ì¶œë ¥ì´ ë…¸ì´ì¦ˆê°€ ë§ë”ë¼ë„, í•™ìƒì€ ì¢…ì¢… ê°€ì§œ ë ˆì´ë¸” ìˆ˜ì •ê³¼ ë²”ìœ„ í™•ì¥ì„ í†µí•´ êµì‚¬ë¥¼ ëŠ¥ê°€í•˜ë©°, ì´ëŠ” ì•½í•œ ê°ë…ì—ì„œ ê°•í•œ ì¼ë°˜í™”ë¡œì˜ íš¨ê³¼ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤. ì‹¤ì¦ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ë„ë©”ì¸ ë‚´ ë° ë„ë©”ì¸ ì™¸ ê³¼ì œ ì „ë°˜ì—ì„œ ì¼ë°˜í™”, ì•ˆì •ì„± ë° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œì¼œ, í‘œì¤€ ICL ë° ì´ì „ì˜ ë¶„ë¦¬ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë¶ˆì•ˆì •ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ICL(ë¬¸ë§¥ ë‚´ í•™ìŠµ)ì„ ì•½í•œ ê°ë…ì˜ ì›ì²œìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì‹œì—°ìœ¼ë¡œ ì¸í•œ ì ì¬ì  ë³€í™”ë¥¼ ì§ˆì˜ë¡œë¶€í„° ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•˜ë©°, ICL ê¸°ë°˜ êµì‚¬ê°€ ë¹„í‘œì‹œëœ ì§ˆì˜ì— ëŒ€í•´ ìƒì„±í•œ ê°€ì§œ ë ˆì´ë¸”ì„ í•™ìƒ ëª¨ë¸ì´ ê²½ëŸ‰ ì–´ëŒ‘í„°ë¥¼ í†µí•´ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ í•™ìƒ ëª¨ë¸ì€ êµì‚¬ì˜ ì¶œë ¥ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì´ëŠ” ì•½í•œ ê°ë…ì—ì„œ ê°•í•œ ì¼ë°˜í™”ë¡œì˜ ì „í™˜ íš¨ê³¼ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ ICL ë° ì´ì „ ë¶„ë¦¬ ë°©ë²•ë³´ë‹¤ ë” ë‚˜ì€ ì¼ë°˜í™”, ì•ˆì •ì„±, íš¨ìœ¨ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ICL(ë¬¸ë§¥ í•™ìŠµ)ì€ ë ˆì´ë¸”ì´ ìˆëŠ” ì˜ˆì œë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ì†Œìˆ˜ì˜ ìƒ· í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
- 2. ICLì€ ìœ ì—°ì„±ì—ë„ ë¶ˆêµ¬í•˜ê³  í”„ë¡¬í”„íŠ¸ ê¸¸ì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ë¶ˆì•ˆì •ì„±ì´ ì»¤ì§€ëŠ” ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ICLì„ ì•½í•œ ê°ë…ì˜ ì›ì²œìœ¼ë¡œ ë³´ê³ , ì‹œì—°ìœ¼ë¡œ ì¸í•œ ì ì¬ì  ë³€í™”ë¥¼ ì¿¼ë¦¬ì˜ ë³€í™”ì™€ ë¶„ë¦¬í•˜ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ICL ê¸°ë°˜ êµì‚¬ëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì§œ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê³ , í•™ìƒì€ ì¿¼ë¦¬ ì…ë ¥ë§Œìœ¼ë¡œ ì´ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ê²½ëŸ‰ ì–´ëŒ‘í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ì¼ë°˜í™”, ì•ˆì •ì„±, íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ë©°, í‘œì¤€ ICL ë° ì´ì „ì˜ ë¶„ë¦¬ ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:16:16*