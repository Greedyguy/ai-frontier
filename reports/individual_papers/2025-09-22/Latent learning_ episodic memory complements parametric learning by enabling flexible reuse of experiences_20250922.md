---
keywords:
  - Latent Learning
  - Episodic Memory
  - Retrieval Mechanism
  - Parametric Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.16189
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:45:02.480079",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Latent Learning",
    "Episodic Memory",
    "Retrieval Mechanism",
    "Parametric Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Latent Learning": 0.78,
    "Episodic Memory": 0.82,
    "Retrieval Mechanism": 0.79,
    "Parametric Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "latent learning",
        "canonical": "Latent Learning",
        "aliases": [
          "latent knowledge acquisition"
        ],
        "category": "unique_technical",
        "rationale": "Latent learning is a novel concept that addresses the gap in current machine learning systems' ability to generalize by learning information not immediately relevant to tasks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "episodic memory",
        "canonical": "Episodic Memory",
        "aliases": [
          "event memory",
          "experience memory"
        ],
        "category": "specific_connectable",
        "rationale": "Episodic memory is crucial for understanding how retrieval methods can enhance machine learning generalization, linking cognitive science with AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "retrieval mechanism",
        "canonical": "Retrieval Mechanism",
        "aliases": [
          "information retrieval",
          "data retrieval"
        ],
        "category": "specific_connectable",
        "rationale": "Retrieval mechanisms are key to improving generalization in machine learning by enabling flexible use of learned experiences.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "parametric learning",
        "canonical": "Parametric Learning",
        "aliases": [
          "parameter-based learning"
        ],
        "category": "broad_technical",
        "rationale": "Parametric learning is a fundamental concept in machine learning that contrasts with retrieval-based methods, providing a basis for comparison.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "generalization",
      "mechanism"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "latent learning",
      "resolved_canonical": "Latent Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "episodic memory",
      "resolved_canonical": "Episodic Memory",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "retrieval mechanism",
      "resolved_canonical": "Retrieval Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "parametric learning",
      "resolved_canonical": "Parametric Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences

**Korean Title:** 잠재 학습: 일화적 기억은 경험의 유연한 재사용을 가능하게 함으로써 매개변수 학습을 보완합니다.

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16189.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.16189](https://arxiv.org/abs/2509.16189)

## 🔗 유사한 논문
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (81.0% similar)
- [[2025-09-22/Search and Refine During Think_ Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning_20250922|Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning]] (81.0% similar)
- [[2025-09-22/Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery_20250922|Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery]] (80.3% similar)
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (80.1% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (79.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Parametric Learning|Parametric Learning]]
**🔗 Specific Connectable**: [[keywords/Episodic Memory|Episodic Memory]], [[keywords/Retrieval Mechanism|Retrieval Mechanism]]
**⚡ Unique Technical**: [[keywords/Latent Learning|Latent Learning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16189v1 Announce Type: new 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization.

## 🔍 Abstract (한글 번역)

arXiv:2509.16189v1 발표 유형: 신규  
초록: 기계 학습 시스템이 일반화에 실패하는 경우는 언제이며, 그들의 일반화를 개선할 수 있는 메커니즘은 무엇일까? 여기서 우리는 인지 과학에서 영감을 받아 기계 학습 시스템의 한 가지 약점이 잠재 학습을 나타내지 못하는 것이라고 주장한다. 잠재 학습이란 현재의 과제와는 관련이 없지만 미래의 과제에서 유용할 수 있는 정보를 학습하는 것이다. 우리는 이 관점이 언어 모델링에서의 역전 저주부터 에이전트 기반 탐색에 대한 새로운 발견에 이르기까지의 실패를 어떻게 연결하는지를 보여준다. 그런 다음 인지 과학이 이러한 문제에 대한 잠재적 해결책의 일부로서 일화 기억을 지적하는 방법을 강조한다. 이에 따라, 우리는 오라클 검색 메커니즘을 갖춘 시스템이 학습 경험을 보다 유연하게 활용하여 이러한 많은 도전 과제에서 더 나은 일반화를 달성할 수 있음을 보여준다. 또한 검색을 효과적으로 사용하기 위한 몇 가지 필수 구성 요소를 식별하며, 검색된 예제 전반에 걸쳐 정보를 사용할 수 있는 능력을 획득하기 위해 예제 내 맥락 학습의 중요성을 포함한다. 요약하면, 우리의 결과는 현재 기계 학습 시스템이 자연 지능에 비해 상대적으로 데이터 비효율적인 이유 중 하나를 설명하고, 검색 방법이 매개변수 학습을 보완하여 일반화를 개선할 수 있는 방법을 이해하는 데 도움을 준다.

## 📝 요약

이 논문은 기계 학습 시스템의 일반화 실패 원인과 개선 방안을 탐구합니다. 인지 과학에서 영감을 받아, 기계 학습 시스템이 잠재 학습을 잘 수행하지 못하는 점을 지적합니다. 이는 언어 모델링의 역전 저주부터 에이전트 기반 내비게이션의 새로운 발견에 이르기까지 다양한 실패 사례와 연결됩니다. 인지 과학은 이러한 문제 해결의 일환으로 일화 기억을 제안하며, 오라클 검색 메커니즘을 통해 학습 경험을 유연하게 활용하여 일반화를 개선할 수 있음을 보여줍니다. 또한, 검색을 효과적으로 활용하기 위한 필수 요소로, 검색된 예시 간의 정보를 활용하는 능력을 획득하기 위한 맥락 내 학습의 중요성을 강조합니다. 결과적으로, 이 연구는 현재 기계 학습 시스템의 데이터 비효율성 원인을 설명하고, 검색 방법이 매개 변수 학습을 보완하여 일반화를 향상시킬 수 있음을 이해하는 데 기여합니다.

## 🎯 주요 포인트

- 1. 기계 학습 시스템의 일반화 실패 원인 중 하나는 잠재 학습의 부족이다.
- 2. 인지 과학은 에피소드 기억이 일반화 문제 해결의 잠재적 해결책임을 시사한다.
- 3. 오라클 검색 메커니즘을 갖춘 시스템은 학습 경험을 유연하게 활용하여 일반화를 개선할 수 있다.
- 4. 검색을 효과적으로 사용하기 위해서는 예제 내 맥락 학습의 중요성이 강조된다.
- 5. 현재 기계 학습 시스템의 데이터 비효율성을 자연 지능과 비교하여 설명할 수 있다.


---

*Generated on 2025-09-23 10:45:02*