---
keywords:
  - Latent Learning
  - Episodic Memory
  - Retrieval Mechanism
  - Parametric Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.16189
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:45:02.480079",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Latent Learning",
    "Episodic Memory",
    "Retrieval Mechanism",
    "Parametric Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Latent Learning": 0.78,
    "Episodic Memory": 0.82,
    "Retrieval Mechanism": 0.79,
    "Parametric Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "latent learning",
        "canonical": "Latent Learning",
        "aliases": [
          "latent knowledge acquisition"
        ],
        "category": "unique_technical",
        "rationale": "Latent learning is a novel concept that addresses the gap in current machine learning systems' ability to generalize by learning information not immediately relevant to tasks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "episodic memory",
        "canonical": "Episodic Memory",
        "aliases": [
          "event memory",
          "experience memory"
        ],
        "category": "specific_connectable",
        "rationale": "Episodic memory is crucial for understanding how retrieval methods can enhance machine learning generalization, linking cognitive science with AI.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "retrieval mechanism",
        "canonical": "Retrieval Mechanism",
        "aliases": [
          "information retrieval",
          "data retrieval"
        ],
        "category": "specific_connectable",
        "rationale": "Retrieval mechanisms are key to improving generalization in machine learning by enabling flexible use of learned experiences.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "parametric learning",
        "canonical": "Parametric Learning",
        "aliases": [
          "parameter-based learning"
        ],
        "category": "broad_technical",
        "rationale": "Parametric learning is a fundamental concept in machine learning that contrasts with retrieval-based methods, providing a basis for comparison.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "generalization",
      "mechanism"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "latent learning",
      "resolved_canonical": "Latent Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "episodic memory",
      "resolved_canonical": "Episodic Memory",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "retrieval mechanism",
      "resolved_canonical": "Retrieval Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "parametric learning",
      "resolved_canonical": "Parametric Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences

**Korean Title:** ì ì¬ í•™ìŠµ: ì¼í™”ì  ê¸°ì–µì€ ê²½í—˜ì˜ ìœ ì—°í•œ ì¬ì‚¬ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ìœ¼ë¡œì¨ ë§¤ê°œë³€ìˆ˜ í•™ìŠµì„ ë³´ì™„í•©ë‹ˆë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16189.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.16189](https://arxiv.org/abs/2509.16189)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (81.0% similar)
- [[2025-09-22/Search and Refine During Think_ Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning_20250922|Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning]] (81.0% similar)
- [[2025-09-22/Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery_20250922|Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery]] (80.3% similar)
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (80.1% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (79.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Parametric Learning|Parametric Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Episodic Memory|Episodic Memory]], [[keywords/Retrieval Mechanism|Retrieval Mechanism]]
**âš¡ Unique Technical**: [[keywords/Latent Learning|Latent Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16189v1 Announce Type: new 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16189v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì´ ì¼ë°˜í™”ì— ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ëŠ” ì–¸ì œì´ë©°, ê·¸ë“¤ì˜ ì¼ë°˜í™”ë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë©”ì»¤ë‹ˆì¦˜ì€ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì¸ì§€ ê³¼í•™ì—ì„œ ì˜ê°ì„ ë°›ì•„ ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì˜ í•œ ê°€ì§€ ì•½ì ì´ ì ì¬ í•™ìŠµì„ ë‚˜íƒ€ë‚´ì§€ ëª»í•˜ëŠ” ê²ƒì´ë¼ê³  ì£¼ì¥í•œë‹¤. ì ì¬ í•™ìŠµì´ë€ í˜„ì¬ì˜ ê³¼ì œì™€ëŠ” ê´€ë ¨ì´ ì—†ì§€ë§Œ ë¯¸ë˜ì˜ ê³¼ì œì—ì„œ ìœ ìš©í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ì´ ê´€ì ì´ ì–¸ì–´ ëª¨ë¸ë§ì—ì„œì˜ ì—­ì „ ì €ì£¼ë¶€í„° ì—ì´ì „íŠ¸ ê¸°ë°˜ íƒìƒ‰ì— ëŒ€í•œ ìƒˆë¡œìš´ ë°œê²¬ì— ì´ë¥´ê¸°ê¹Œì§€ì˜ ì‹¤íŒ¨ë¥¼ ì–´ë–»ê²Œ ì—°ê²°í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì¸ì§€ ê³¼í•™ì´ ì´ëŸ¬í•œ ë¬¸ì œì— ëŒ€í•œ ì ì¬ì  í•´ê²°ì±…ì˜ ì¼ë¶€ë¡œì„œ ì¼í™” ê¸°ì–µì„ ì§€ì í•˜ëŠ” ë°©ë²•ì„ ê°•ì¡°í•œë‹¤. ì´ì— ë”°ë¼, ìš°ë¦¬ëŠ” ì˜¤ë¼í´ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜ì„ ê°–ì¶˜ ì‹œìŠ¤í…œì´ í•™ìŠµ ê²½í—˜ì„ ë³´ë‹¤ ìœ ì—°í•˜ê²Œ í™œìš©í•˜ì—¬ ì´ëŸ¬í•œ ë§ì€ ë„ì „ ê³¼ì œì—ì„œ ë” ë‚˜ì€ ì¼ë°˜í™”ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤. ë˜í•œ ê²€ìƒ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ í•„ìˆ˜ êµ¬ì„± ìš”ì†Œë¥¼ ì‹ë³„í•˜ë©°, ê²€ìƒ‰ëœ ì˜ˆì œ ì „ë°˜ì— ê±¸ì³ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ íšë“í•˜ê¸° ìœ„í•´ ì˜ˆì œ ë‚´ ë§¥ë½ í•™ìŠµì˜ ì¤‘ìš”ì„±ì„ í¬í•¨í•œë‹¤. ìš”ì•½í•˜ë©´, ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” í˜„ì¬ ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì´ ìì—° ì§€ëŠ¥ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ë°ì´í„° ë¹„íš¨ìœ¨ì ì¸ ì´ìœ  ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ëª…í•˜ê³ , ê²€ìƒ‰ ë°©ë²•ì´ ë§¤ê°œë³€ìˆ˜ í•™ìŠµì„ ë³´ì™„í•˜ì—¬ ì¼ë°˜í™”ë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì˜ ì¼ë°˜í™” ì‹¤íŒ¨ ì›ì¸ê³¼ ê°œì„  ë°©ì•ˆì„ íƒêµ¬í•©ë‹ˆë‹¤. ì¸ì§€ ê³¼í•™ì—ì„œ ì˜ê°ì„ ë°›ì•„, ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì´ ì ì¬ í•™ìŠµì„ ì˜ ìˆ˜í–‰í•˜ì§€ ëª»í•˜ëŠ” ì ì„ ì§€ì í•©ë‹ˆë‹¤. ì´ëŠ” ì–¸ì–´ ëª¨ë¸ë§ì˜ ì—­ì „ ì €ì£¼ë¶€í„° ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‚´ë¹„ê²Œì´ì…˜ì˜ ìƒˆë¡œìš´ ë°œê²¬ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ì‹¤íŒ¨ ì‚¬ë¡€ì™€ ì—°ê²°ë©ë‹ˆë‹¤. ì¸ì§€ ê³¼í•™ì€ ì´ëŸ¬í•œ ë¬¸ì œ í•´ê²°ì˜ ì¼í™˜ìœ¼ë¡œ ì¼í™” ê¸°ì–µì„ ì œì•ˆí•˜ë©°, ì˜¤ë¼í´ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•™ìŠµ ê²½í—˜ì„ ìœ ì—°í•˜ê²Œ í™œìš©í•˜ì—¬ ì¼ë°˜í™”ë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, ê²€ìƒ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•œ í•„ìˆ˜ ìš”ì†Œë¡œ, ê²€ìƒ‰ëœ ì˜ˆì‹œ ê°„ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ëŠ¥ë ¥ì„ íšë“í•˜ê¸° ìœ„í•œ ë§¥ë½ ë‚´ í•™ìŠµì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ ì—°êµ¬ëŠ” í˜„ì¬ ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì˜ ë°ì´í„° ë¹„íš¨ìœ¨ì„± ì›ì¸ì„ ì„¤ëª…í•˜ê³ , ê²€ìƒ‰ ë°©ë²•ì´ ë§¤ê°œ ë³€ìˆ˜ í•™ìŠµì„ ë³´ì™„í•˜ì—¬ ì¼ë°˜í™”ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì˜ ì¼ë°˜í™” ì‹¤íŒ¨ ì›ì¸ ì¤‘ í•˜ë‚˜ëŠ” ì ì¬ í•™ìŠµì˜ ë¶€ì¡±ì´ë‹¤.
- 2. ì¸ì§€ ê³¼í•™ì€ ì—í”¼ì†Œë“œ ê¸°ì–µì´ ì¼ë°˜í™” ë¬¸ì œ í•´ê²°ì˜ ì ì¬ì  í•´ê²°ì±…ì„ì„ ì‹œì‚¬í•œë‹¤.
- 3. ì˜¤ë¼í´ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜ì„ ê°–ì¶˜ ì‹œìŠ¤í…œì€ í•™ìŠµ ê²½í—˜ì„ ìœ ì—°í•˜ê²Œ í™œìš©í•˜ì—¬ ì¼ë°˜í™”ë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‹¤.
- 4. ê²€ìƒ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì˜ˆì œ ë‚´ ë§¥ë½ í•™ìŠµì˜ ì¤‘ìš”ì„±ì´ ê°•ì¡°ëœë‹¤.
- 5. í˜„ì¬ ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì˜ ë°ì´í„° ë¹„íš¨ìœ¨ì„±ì„ ìì—° ì§€ëŠ¥ê³¼ ë¹„êµí•˜ì—¬ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-23 10:45:02*