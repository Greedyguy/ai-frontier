---
keywords:
  - Knowledge Graph Foundation Models
  - Large Language Model
  - Zero-Shot Learning
  - Semantic Embeddings
  - Textual Relation Graph
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.20422
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:00:10.633987",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Knowledge Graph Foundation Models",
    "Large Language Model",
    "Zero-Shot Learning",
    "Semantic Embeddings",
    "Textual Relation Graph"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Knowledge Graph Foundation Models": 0.78,
    "Large Language Model": 0.85,
    "Zero-Shot Learning": 0.82,
    "Semantic Embeddings": 0.77,
    "Textual Relation Graph": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Knowledge Graph Foundation Models",
        "canonical": "Knowledge Graph Foundation Models",
        "aliases": [
          "KGFMs"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel concept in the integration of semantics and structure for knowledge graphs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology, linking to broader discussions on language model applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Zero-Shot Reasoning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the model's capability to generalize without prior exposure, a key feature in modern AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Semantic Embeddings",
        "canonical": "Semantic Embeddings",
        "aliases": [
          "Textual Embeddings"
        ],
        "category": "unique_technical",
        "rationale": "Critical for understanding the integration of textual data into graph models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Textual Relation Graph",
        "canonical": "Textual Relation Graph",
        "aliases": [
          "Textual Graph"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to combining text and graph data, enhancing model capabilities.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Knowledge Graph Foundation Models",
      "resolved_canonical": "Knowledge Graph Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Zero-Shot Reasoning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Semantic Embeddings",
      "resolved_canonical": "Semantic Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Textual Relation Graph",
      "resolved_canonical": "Textual Relation Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# SEMMA: A Semantic Aware Knowledge Graph Foundation Model

**Korean Title:** SEMMA: ì˜ë¯¸ ì¸ì‹ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.20422.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.20422](https://arxiv.org/abs/2505.20422)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (83.7% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (83.2% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (82.7% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (81.6% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Knowledge Graph Foundation Models|Knowledge Graph Foundation Models]], [[keywords/Semantic Embeddings|Semantic Embeddings]], [[keywords/Textual Relation Graph|Textual Relation Graph]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.20422v2 Announce Type: replace-cross 
Abstract: Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.20422v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ì§€ì‹ ê·¸ë˜í”„ ê¸°ì´ˆ ëª¨ë¸(KGFMs)ì€ ì „ì´ ê°€ëŠ¥í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ë³´ì§€ ëª»í•œ ê·¸ë˜í”„ì— ëŒ€í•œ ì œë¡œìƒ· ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ KGFMsëŠ” ê·¸ë˜í”„ êµ¬ì¡°ì—ë§Œ ì˜ì¡´í•˜ë©°, í…ìŠ¤íŠ¸ ì†ì„±ì— ë‚´ì¬ëœ í’ë¶€í•œ ì˜ë¯¸ ì‹ í˜¸ë¥¼ ê°„ê³¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” êµ¬ì¡°ì™€ í•¨ê»˜ ì „ì´ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì˜ë¯¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ì´ì¤‘ ëª¨ë“ˆ KGFMì¸ SEMMAë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. SEMMAëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í™œìš©í•˜ì—¬ ê´€ê³„ ì‹ë³„ìë¥¼ í’ë¶€í•˜ê²Œ í•˜ê³ , ì´ë¥¼ í†µí•´ ìƒì„±ëœ ì˜ë¯¸ ì„ë² ë”©ì´ í…ìŠ¤íŠ¸ ê´€ê³„ ê·¸ë˜í”„ë¥¼ í˜•ì„±í•˜ì—¬ êµ¬ì¡°ì  êµ¬ì„± ìš”ì†Œì™€ ìœµí•©ë©ë‹ˆë‹¤. 54ê°œì˜ ë‹¤ì–‘í•œ ì§€ì‹ ê·¸ë˜í”„ì—ì„œ SEMMAëŠ” ì™„ì „ ê·€ë‚©ì  ë§í¬ ì˜ˆì¸¡ì—ì„œ ULTRAì™€ ê°™ì€ ìˆœìˆ˜ êµ¬ì¡°ì  ê¸°ì¤€ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. íŠ¹íˆ, í…ŒìŠ¤íŠ¸ ì‹œì ì˜ ê´€ê³„ ì–´íœ˜ê°€ ì „í˜€ ë³´ì§€ ëª»í•œ ê²½ìš°ì™€ ê°™ì€ ë” ì–´ë ¤ìš´ ì¼ë°˜í™” ì„¤ì •ì—ì„œ êµ¬ì¡°ì  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ëŠ” ë°˜ë©´, SEMMAëŠ” 2ë°° ë” íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” êµ¬ì¡°ë§Œìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” í™˜ê²½ì—ì„œ ì¼ë°˜í™”ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ ì˜ë¯¸ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ì§€ì‹ ì¶”ë¡ ì—ì„œ êµ¬ì¡°ì  ë° ì–¸ì–´ì  ì‹ í˜¸ë¥¼ í†µí•©í•˜ëŠ” ê¸°ì´ˆ ëª¨ë¸ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë…¼ë¬¸ì€ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ëª¨ë¸(KGFMs)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ SEMMAë¼ëŠ” ì´ì¤‘ ëª¨ë“ˆ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ KGFMsëŠ” ì£¼ë¡œ ê·¸ë˜í”„ êµ¬ì¡°ì— ì˜ì¡´í•˜ì§€ë§Œ, SEMMAëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì† ì˜ë¯¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê´€ê³„ ì‹ë³„ìë¥¼ ê°•í™”í•˜ê³ , í…ìŠ¤íŠ¸ ê´€ê³„ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì—¬ êµ¬ì¡°ì  ìš”ì†Œì™€ ê²°í•©í•©ë‹ˆë‹¤. 54ê°œì˜ ë‹¤ì–‘í•œ ì§€ì‹ ê·¸ë˜í”„ì—ì„œ SEMMAëŠ” ê¸°ì¡´ êµ¬ì¡°ì  ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ í…ŒìŠ¤íŠ¸ ì‹œì ì— ì „í˜€ ë³´ì§€ ëª»í•œ ê´€ê³„ ì–´íœ˜ê°€ ì£¼ì–´ì§€ëŠ” ì–´ë ¤ìš´ ì¼ë°˜í™” ìƒí™©ì—ì„œë„ ë‘ ë°° ë” íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ ì˜ë¯¸ê°€ êµ¬ì¡°ë§Œìœ¼ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ëŠ” ì¼ë°˜í™” ë¬¸ì œì— í•„ìˆ˜ì ì„ì„ ê°•ì¡°í•˜ë©°, êµ¬ì¡°ì™€ ì–¸ì–´ì  ì‹ í˜¸ë¥¼ í†µí•©í•˜ëŠ” ëª¨ë¸ì˜ í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SEMMAëŠ” êµ¬ì¡°ì™€ í•¨ê»˜ ì „ì´ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì˜ë¯¸ë¥¼ í†µí•©í•˜ëŠ” ì´ì¤‘ ëª¨ë“ˆ KGFMì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 2. SEMMAëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ ê´€ê³„ ì‹ë³„ìë¥¼ í’ë¶€í•˜ê²Œ í•˜ê³ , ì´ë¥¼ í†µí•´ ìƒì„±ëœ ì˜ë¯¸ ì„ë² ë”©ì„ êµ¬ì¡°ì  êµ¬ì„± ìš”ì†Œì™€ ìœµí•©í•©ë‹ˆë‹¤.
- 3. 54ê°œì˜ ë‹¤ì–‘í•œ ì§€ì‹ ê·¸ë˜í”„ì—ì„œ SEMMAëŠ” ULTRAì™€ ê°™ì€ ìˆœìˆ˜ êµ¬ì¡°ì  ê¸°ì¤€ì„ ëŠ¥ê°€í•˜ì—¬ ì™„ì „ ìœ ë„ì  ë§í¬ ì˜ˆì¸¡ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 4. í…ŒìŠ¤íŠ¸ ì‹œ ê´€ê³„ ì–´íœ˜ê°€ ì „í˜€ ë³´ì´ì§€ ì•ŠëŠ” ì¼ë°˜í™” ì„¤ì •ì—ì„œ êµ¬ì¡°ì  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ëŠ” ë°˜ë©´, SEMMAëŠ” ë‘ ë°° ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.
- 5. í…ìŠ¤íŠ¸ ì˜ë¯¸ëŠ” êµ¬ì¡°ë§Œìœ¼ë¡œëŠ” ì‹¤íŒ¨í•˜ëŠ” ìƒí™©ì—ì„œ ì¼ë°˜í™”ì— ì¤‘ìš”í•˜ë©°, êµ¬ì¡°ì  ë° ì–¸ì–´ì  ì‹ í˜¸ë¥¼ í†µí•©í•˜ëŠ” ê¸°ì´ˆ ëª¨ë¸ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:00:10*