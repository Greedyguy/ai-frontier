---
keywords:
  - Knowledge Graph Foundation Models
  - Large Language Model
  - Zero-Shot Learning
  - Semantic Embeddings
  - Textual Relation Graph
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.20422
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:00:10.633987",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Knowledge Graph Foundation Models",
    "Large Language Model",
    "Zero-Shot Learning",
    "Semantic Embeddings",
    "Textual Relation Graph"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Knowledge Graph Foundation Models": 0.78,
    "Large Language Model": 0.85,
    "Zero-Shot Learning": 0.82,
    "Semantic Embeddings": 0.77,
    "Textual Relation Graph": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Knowledge Graph Foundation Models",
        "canonical": "Knowledge Graph Foundation Models",
        "aliases": [
          "KGFMs"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel concept in the integration of semantics and structure for knowledge graphs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's methodology, linking to broader discussions on language model applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Zero-Shot Reasoning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the model's capability to generalize without prior exposure, a key feature in modern AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Semantic Embeddings",
        "canonical": "Semantic Embeddings",
        "aliases": [
          "Textual Embeddings"
        ],
        "category": "unique_technical",
        "rationale": "Critical for understanding the integration of textual data into graph models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Textual Relation Graph",
        "canonical": "Textual Relation Graph",
        "aliases": [
          "Textual Graph"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to combining text and graph data, enhancing model capabilities.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Knowledge Graph Foundation Models",
      "resolved_canonical": "Knowledge Graph Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Zero-Shot Reasoning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Semantic Embeddings",
      "resolved_canonical": "Semantic Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Textual Relation Graph",
      "resolved_canonical": "Textual Relation Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# SEMMA: A Semantic Aware Knowledge Graph Foundation Model

**Korean Title:** SEMMA: 의미 인식 지식 그래프 기반 모델

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.20422.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.20422](https://arxiv.org/abs/2505.20422)

## 🔗 유사한 논문
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (83.7% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (83.2% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (82.7% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (81.6% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (81.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Knowledge Graph Foundation Models|Knowledge Graph Foundation Models]], [[keywords/Semantic Embeddings|Semantic Embeddings]], [[keywords/Textual Relation Graph|Textual Relation Graph]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.20422v2 Announce Type: replace-cross 
Abstract: Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.

## 🔍 Abstract (한글 번역)

arXiv:2505.20422v2 발표 유형: 교차 대체  
초록: 지식 그래프 기초 모델(KGFMs)은 전이 가능한 패턴을 학습하여 보지 못한 그래프에 대한 제로샷 추론을 가능하게 하는 데 유망한 성과를 보여주었습니다. 그러나 대부분의 기존 KGFMs는 그래프 구조에만 의존하며, 텍스트 속성에 내재된 풍부한 의미 신호를 간과하고 있습니다. 우리는 구조와 함께 전이 가능한 텍스트 의미를 체계적으로 통합하는 이중 모듈 KGFM인 SEMMA를 소개합니다. SEMMA는 대형 언어 모델(LLMs)을 활용하여 관계 식별자를 풍부하게 하고, 이를 통해 생성된 의미 임베딩이 텍스트 관계 그래프를 형성하여 구조적 구성 요소와 융합됩니다. 54개의 다양한 지식 그래프에서 SEMMA는 완전 귀납적 링크 예측에서 ULTRA와 같은 순수 구조적 기준을 능가합니다. 특히, 테스트 시점의 관계 어휘가 전혀 보지 못한 경우와 같은 더 어려운 일반화 설정에서 구조적 방법이 실패하는 반면, SEMMA는 2배 더 효과적임을 보여줍니다. 우리의 연구 결과는 구조만으로 실패하는 환경에서 일반화를 위해 텍스트 의미가 중요하다는 것을 보여주며, 지식 추론에서 구조적 및 언어적 신호를 통합하는 기초 모델의 필요성을 강조합니다.

## 📝 요약

논문은 지식 그래프 기반 모델(KGFMs)의 한계를 극복하기 위해 SEMMA라는 이중 모듈 모델을 제안합니다. 기존 KGFMs는 주로 그래프 구조에 의존하지만, SEMMA는 대형 언어 모델(LLMs)을 활용하여 텍스트 속 의미를 통합합니다. 이를 통해 관계 식별자를 강화하고, 텍스트 관계 그래프를 생성하여 구조적 요소와 결합합니다. 54개의 다양한 지식 그래프에서 SEMMA는 기존 구조적 모델보다 뛰어난 성능을 보였으며, 특히 테스트 시점에 전혀 보지 못한 관계 어휘가 주어지는 어려운 일반화 상황에서도 두 배 더 효과적임을 입증했습니다. 이는 텍스트 의미가 구조만으로는 해결할 수 없는 일반화 문제에 필수적임을 강조하며, 구조와 언어적 신호를 통합하는 모델의 필요성을 제시합니다.

## 🎯 주요 포인트

- 1. SEMMA는 구조와 함께 전이 가능한 텍스트 의미를 통합하는 이중 모듈 KGFM을 소개합니다.
- 2. SEMMA는 대형 언어 모델(LLM)을 활용하여 관계 식별자를 풍부하게 하고, 이를 통해 생성된 의미 임베딩을 구조적 구성 요소와 융합합니다.
- 3. 54개의 다양한 지식 그래프에서 SEMMA는 ULTRA와 같은 순수 구조적 기준을 능가하여 완전 유도적 링크 예측에서 뛰어난 성능을 보입니다.
- 4. 테스트 시 관계 어휘가 전혀 보이지 않는 일반화 설정에서 구조적 방법이 실패하는 반면, SEMMA는 두 배 더 효과적입니다.
- 5. 텍스트 의미는 구조만으로는 실패하는 상황에서 일반화에 중요하며, 구조적 및 언어적 신호를 통합하는 기초 모델의 필요성을 강조합니다.


---

*Generated on 2025-09-23 10:00:10*