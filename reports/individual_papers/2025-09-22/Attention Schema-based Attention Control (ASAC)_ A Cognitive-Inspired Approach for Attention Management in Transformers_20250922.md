# Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers

**Korean Title:** ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì£¼ì˜ ì œì–´(ASAC): íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì£¼ì˜ ê´€ë¦¬ì— ëŒ€í•œ ì¸ì§€ ì˜ê° ì ‘ê·¼ë²•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Attention Mechanism, Few-shot Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods Reviving Transformer for Graph Clustering]] (80.7% similar)
- [[2025-09-19/Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention A Scalable Multilevel Attention Mechanism for Text and Images]] (79.4% similar)
- [[2025-09-19/Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models_20250919|Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models]] (79.1% similar)
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (78.5% similar)
- [[2025-09-19/Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention_20250919|Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention]] (78.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16058v1 Announce Type: new 
Abstract: Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16058v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì¸ê°„ ì¸ì§€ì—ì„œ ì˜ê°ì„ ë°›ì•„ AIì—ì„œ ëª¨ë¸ ì„±ëŠ¥ê³¼ í™•ì¥ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©° í•„ìˆ˜ì ì¸ ìš”ì†Œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ë™ì‹œì—, ì¸ì§€ ê³¼í•™ì˜ ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ì´ë¡ (AST)ì€ ê°œì¸ì´ ì£¼ì˜ ìì²´ì˜ ëª¨ë¸ì„ ìƒì„±í•˜ì—¬ ì¸ì§€ ìì›ì„ íš¨ê³¼ì ìœ¼ë¡œ í• ë‹¹í•¨ìœ¼ë¡œì¨ ì£¼ì˜ë¥¼ ê´€ë¦¬í•œë‹¤ê³  ì œì•ˆí•©ë‹ˆë‹¤. ASTì—ì„œ ì˜ê°ì„ ë°›ì•„, ìš°ë¦¬ëŠ” ì¸ê³µ ì‹ ê²½ë§ì— ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ê°œë…ì„ í†µí•©í•œ ASAC(ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì£¼ì˜ ì œì–´)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ˆê¸° ì‹¤í—˜ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ ë‚´ì— ASAC ëª¨ë“ˆì„ ë‚´ì¥í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë“ˆì€ ì£¼ì˜ ì¶”ìƒí™”ê¸° ë° ì œì–´ê¸°ë¡œì„œ ë²¡í„° ì–‘ìí™” ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(VQVAE)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì£¼ì˜ ê´€ë¦¬ê°€ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì£¼ì˜ í• ë‹¹ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ì‹œìŠ¤í…œ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ASACì˜ ë¹„ì „ ë° ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ë©°, ë¶„ë¥˜ ì •í™•ë„ë¥¼ ê°œì„ í•˜ê³  í•™ìŠµ ê³¼ì •ì„ ê°€ì†í™”í•˜ëŠ” ëŠ¥ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œì˜ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ì‹¤í—˜ì€ ì£¼ì˜ ì œì–´ê¸°ê°€ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì¼ ë¿ë§Œ ì•„ë‹ˆë¼ í•™ìŠµì„ ê°€ì†í™”í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ìš±ì´, ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ê°•ê±´ì„±ê³¼ ë¶„í¬ ì™¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¤ì¤‘ ì‘ì—… ì„¤ì •ì—ì„œì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ì‹¤í—˜ì€ ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ëª¨ë“ˆì´ ì ëŒ€ì  ê³µê²©ì— ëŒ€í•œ íšŒë³µë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , í•™ìŠµ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì£¼ì˜ë¥¼ ìµœì í™”í•˜ë©°, íš¨ê³¼ì ì¸ ì „ì´ í•™ìŠµê³¼ ì ì€ ì˜ˆì‹œì—ì„œì˜ í•™ìŠµì„ ì´‰ì§„í•œë‹¤ëŠ” ê²ƒì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ìœ ë§í•œ ê²°ê³¼ëŠ” ì¸ì§€ ê³¼í•™ê³¼ ê¸°ê³„ í•™ìŠµ ê°„ì˜ ì—°ê²°ì„ í™•ë¦½í•˜ë©°, AI ì‹œìŠ¤í…œì—ì„œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ìœ¨ì ì¸ í™œìš©ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ ì¸ì§€ì—ì„œ ì˜ê°ì„ ë°›ì€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ AIì— ì ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ì¸ì§€ê³¼í•™ì˜ ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ì´ë¡ (AST)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ASAC(ì£¼ì˜ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì£¼ì˜ ì œì–´)ë¥¼ ë„ì…í•˜ì—¬ ì¸ê³µ ì‹ ê²½ë§ì— í†µí•©í–ˆìŠµë‹ˆë‹¤. ASAC ëª¨ë“ˆì€ VQVAEë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì˜ë¥¼ ì¶”ìƒí™”í•˜ê³  ì œì–´í•˜ë©°, ì´ë¥¼ í†µí•´ ì‹œìŠ¤í…œ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ASACëŠ” ë¹„ì „ ë° NLP ë¶„ì•¼ì—ì„œ ë¶„ë¥˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ê³  í•™ìŠµ ì†ë„ë¥¼ ê°€ì†í™”í•˜ë©°, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¤ì¤‘ ì‘ì—… í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ê°œì„ ê³¼ ì ëŒ€ì  ê³µê²©ì— ëŒ€í•œ íšŒë³µë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì¸ì§€ê³¼í•™ê³¼ ê¸°ê³„í•™ìŠµ ê°„ì˜ ì—°ê²°ì„ ì œì‹œí•˜ë©°, AI ì‹œìŠ¤í…œì—ì„œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ìœ¨ì  í™œìš©ì„ ìœ„í•œ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ASAC(Attention Schema-based Attention Control)ëŠ” ì¸ê°„ì˜ ì£¼ì˜ë ¥ ê´€ë¦¬ ì´ë¡ ì¸ Attention Schema Theory(AST)ë¥¼ ì¸ê³µ ì‹ ê²½ë§ì— í†µí•©í•˜ì—¬ ì£¼ì˜ë ¥ ê´€ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.

- 2. ASAC ëª¨ë“ˆì€ Transformer ì•„í‚¤í…ì²˜ì— í†µí•©ë˜ì–´ Vector-Quantized Variational AutoEncoder(VQVAE)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì˜ë ¥ ì¶”ìƒí™” ë° ì œì–´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- 3. ì‹¤í—˜ ê²°ê³¼, ASACëŠ” ë¹„ì „ ë° ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œ ë¶„ë¥˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ê³  í•™ìŠµ ì†ë„ë¥¼ ê°€ì†í™”í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- 4. ASACëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œì˜ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ì‹¤í—˜ì„ í†µí•´ ë¶„ë¥˜ ì •í™•ë„ í–¥ìƒ ë° í•™ìŠµ ê°€ì†í™”ë¿ë§Œ ì•„ë‹ˆë¼ ì¡ìŒ ë° ë¶„í¬ ì™¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê²¬ê³ ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

- 5. ì£¼ì˜ë ¥ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ëª¨ë“ˆì€ ì ëŒ€ì  ê³µê²©ì— ëŒ€í•œ íšŒë³µë ¥ì„ ë†’ì´ê³ , í•™ìŠµ íš¨ìœ¨ì„±ì„ ìµœì í™”í•˜ë©°, íš¨ê³¼ì ì¸ ì „ì´ í•™ìŠµ ë° ì ì€ ì˜ˆì œë¡œë¶€í„°ì˜ í•™ìŠµì„ ì´‰ì§„í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 13:47:21*