---
keywords:
  - LLM Cache Bandit
  - Query Heterogeneity
  - Knapsack Problem
  - Cost-Effective LLM Inference
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15515
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:29:44.209350",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "LLM Cache Bandit",
    "Query Heterogeneity",
    "Knapsack Problem",
    "Cost-Effective LLM Inference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "LLM Cache Bandit": 0.78,
    "Query Heterogeneity": 0.72,
    "Knapsack Problem": 0.68,
    "Cost-Effective LLM Inference": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LLM Cache Bandit",
        "canonical": "LLM Cache Bandit",
        "aliases": [
          "Large Language Model Cache Bandit"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and introduces a unique approach to optimizing cache strategies for LLMs.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Query Heterogeneity",
        "canonical": "Query Heterogeneity",
        "aliases": [
          "Heterogeneous Queries"
        ],
        "category": "unique_technical",
        "rationale": "Addressing query heterogeneity is a key challenge in the paper, impacting cache selection strategies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Knapsack Problem",
        "canonical": "Knapsack Problem",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "The paper frames cache selection as a knapsack problem, which is a well-known optimization problem.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.68
      },
      {
        "surface": "Cost-Effective LLM Inference",
        "canonical": "Cost-Effective LLM Inference",
        "aliases": [
          "Efficient LLM Inference"
        ],
        "category": "unique_technical",
        "rationale": "The focus on cost-effective inference is crucial for practical applications of LLMs.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "cache selection",
      "computational overhead"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LLM Cache Bandit",
      "resolved_canonical": "LLM Cache Bandit",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Query Heterogeneity",
      "resolved_canonical": "Query Heterogeneity",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Knapsack Problem",
      "resolved_canonical": "Knapsack Problem",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.68
      }
    },
    {
      "candidate_surface": "Cost-Effective LLM Inference",
      "resolved_canonical": "Cost-Effective LLM Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference

**Korean Title:** LLM ìºì‹œ ë°´ë”§ ì¬ê²€í† : ë¹„ìš© íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ì„ ìœ„í•œ ì¿¼ë¦¬ ì´ì§ˆì„± í•´ê²°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15515.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15515](https://arxiv.org/abs/2509.15515)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency_20250922|Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency]] (82.5% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (82.2% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (81.4% similar)
- [[2025-09-22/Adaptive Self-improvement LLM Agentic System for ML Library Development_20250922|Adaptive Self-improvement LLM Agentic System for ML Library Development]] (79.7% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (79.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Knapsack Problem|Knapsack Problem]]
**âš¡ Unique Technical**: [[keywords/LLM Cache Bandit|LLM Cache Bandit]], [[keywords/Query Heterogeneity|Query Heterogeneity]], [[keywords/Cost-Effective LLM Inference|Cost-Effective LLM Inference]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15515v1 Announce Type: new 
Abstract: This paper revisits the LLM cache bandit problem, with a special focus on addressing the query heterogeneity for cost-effective LLM inference. Previous works often assume uniform query sizes. Heterogeneous query sizes introduce a combinatorial structure for cache selection, making the cache replacement process more computationally and statistically challenging. We treat optimal cache selection as a knapsack problem and employ an accumulation-based strategy to effectively balance computational overhead and cache updates. In theoretical analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$ bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$ result in Berkeley, where $N$ is the total number of queries and $M$ is the cache size. Additionally, we also provide a problem-dependent bound, which was absent in previous works. The experiment rely on real-world data show that our algorithm reduces the total cost by approximately 12\%.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15515v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ ë¹„ìš© íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì¶”ë¡ ì„ ìœ„í•´ ì¿¼ë¦¬ ì´ì§ˆì„±ì„ í•´ê²°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ê³  LLM ìºì‹œ ë°´ë”§ ë¬¸ì œë¥¼ ì¬ê²€í† í•©ë‹ˆë‹¤. ì´ì „ ì—°êµ¬ë“¤ì€ ì¢…ì¢… ê· ì¼í•œ ì¿¼ë¦¬ í¬ê¸°ë¥¼ ê°€ì •í–ˆìŠµë‹ˆë‹¤. ì´ì§ˆì ì¸ ì¿¼ë¦¬ í¬ê¸°ëŠ” ìºì‹œ ì„ íƒì— ì¡°í•©ì  êµ¬ì¡°ë¥¼ ë„ì…í•˜ì—¬ ìºì‹œ êµì²´ ê³¼ì •ì„ ë” ë³µì¡í•˜ê³  í†µê³„ì ìœ¼ë¡œ ë„ì „ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìµœì ì˜ ìºì‹œ ì„ íƒì„ ë°°ë‚­ ë¬¸ì œë¡œ ê°„ì£¼í•˜ê³ , ê³„ì‚° ì˜¤ë²„í—¤ë“œì™€ ìºì‹œ ì—…ë°ì´íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê· í˜• ì¡ê¸° ìœ„í•´ ëˆ„ì  ê¸°ë°˜ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ì—ì„œ, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ í›„íšŒê°€ $O(\sqrt{MNT})$ ê²½ê³„ë¥¼ ë‹¬ì„±í•¨ì„ ì¦ëª…í•˜ë©°, ì´ëŠ” ë²„í´ë¦¬ì—ì„œì˜ $O(MN\sqrt{T})$ ê²°ê³¼ì— ë¹„í•´ $\sqrt{MN}$ ê³„ìˆ˜ë¥¼ ê°œì„ í•œ ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ $N$ì€ ì´ ì¿¼ë¦¬ ìˆ˜ì´ê³  $M$ì€ ìºì‹œ í¬ê¸°ì…ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, ì´ì „ ì—°êµ¬ë“¤ì—ì„œëŠ” ì—†ì—ˆë˜ ë¬¸ì œ ì˜ì¡´ì  ê²½ê³„ë„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ì´ ë¹„ìš©ì„ ì•½ 12% ì ˆê°í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ LLM ìºì‹œ ë°´ë”§ ë¬¸ì œë¥¼ ì¬ê²€í† í•˜ë©°, íŠ¹íˆ ë¹„ìš© íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ì„ ìœ„í•œ ì¿¼ë¦¬ ì´ì§ˆì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ê· ì¼í•œ ì¿¼ë¦¬ í¬ê¸°ë¥¼ ê°€ì •í–ˆìœ¼ë‚˜, ì´ì§ˆì ì¸ ì¿¼ë¦¬ í¬ê¸°ëŠ” ìºì‹œ ì„ íƒì— ì¡°í•©ì  êµ¬ì¡°ë¥¼ ë„ì…í•˜ì—¬ ìºì‹œ êµì²´ ê³¼ì •ì„ ë” ë³µì¡í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ìµœì ì˜ ìºì‹œ ì„ íƒì„ ë°°ë‚­ ë¬¸ì œë¡œ ê°„ì£¼í•˜ê³ , ê³„ì‚° ì˜¤ë²„í—¤ë“œì™€ ìºì‹œ ì—…ë°ì´íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê· í˜• ì¡ê¸° ìœ„í•´ ëˆ„ì  ê¸°ë°˜ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ì„ í†µí•´ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ í›„íšŒ(regret)ê°€ $O(\sqrt{MNT})$ ê²½ê³„ë¥¼ ë‹¬ì„±í•¨ì„ ì¦ëª…í•˜ë©°, ì´ëŠ” Berkeleyì˜ $O(MN\sqrt{T})$ ê²°ê³¼ì— ë¹„í•´ $\sqrt{MN}$ ê³„ìˆ˜ë¥¼ ê°œì„ í•œ ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, ì´ì „ ì—°êµ¬ì— ì—†ë˜ ë¬¸ì œ ì˜ì¡´ì  ê²½ê³„ë„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ì´ ë¹„ìš©ì„ ì•½ 12% ì ˆê°í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ ë…¼ë¬¸ì€ LLM ìºì‹œ ë°´ë”§ ë¬¸ì œë¥¼ ì¬ê²€í† í•˜ë©°, ë¹„ìš© íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ì„ ìœ„í•œ ì¿¼ë¦¬ ì´ì§ˆì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.
- 2. ì´ì§ˆì ì¸ ì¿¼ë¦¬ í¬ê¸°ëŠ” ìºì‹œ ì„ íƒì— ì¡°í•©ì  êµ¬ì¡°ë¥¼ ë„ì…í•˜ì—¬ ìºì‹œ êµì²´ ê³¼ì •ì„ ë” ë³µì¡í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
- 3. ìµœì ì˜ ìºì‹œ ì„ íƒì„ ë°°ë‚­ ë¬¸ì œë¡œ ê°„ì£¼í•˜ê³ , ê³„ì‚° ì˜¤ë²„í—¤ë“œì™€ ìºì‹œ ì—…ë°ì´íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê· í˜• ì¡ê¸° ìœ„í•´ ëˆ„ì  ê¸°ë°˜ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ í›„íšŒ(regret)ëŠ” $O(\sqrt{MNT})$ ê²½ê³„ë¥¼ ë‹¬ì„±í•˜ë©°, ì´ëŠ” ê¸°ì¡´ì˜ $O(MN\sqrt{T})$ ê²°ê³¼ì— ë¹„í•´ ê°œì„ ëœ ê²ƒì…ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ì´ ë¹„ìš©ì„ ì•½ 12% ì ˆê°í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:29:44*