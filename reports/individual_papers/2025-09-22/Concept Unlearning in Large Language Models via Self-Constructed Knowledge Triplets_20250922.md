---
keywords:
  - Concept Unlearning
  - Large Language Model
  - Knowledge Graph
  - Machine Unlearning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15621
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:50:55.913408",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Concept Unlearning",
    "Large Language Model",
    "Knowledge Graph",
    "Machine Unlearning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Concept Unlearning": 0.78,
    "Large Language Model": 0.82,
    "Knowledge Graph": 0.77,
    "Machine Unlearning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Concept Unlearning",
        "canonical": "Concept Unlearning",
        "aliases": [
          "CU"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to unlearning in LLMs, enhancing the understanding of knowledge removal.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion, linking to broader discussions on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Knowledge Graphs",
        "canonical": "Knowledge Graph",
        "aliases": [
          "KG"
        ],
        "category": "specific_connectable",
        "rationale": "Facilitates understanding of the internal representation of knowledge in LLMs.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Machine Unlearning",
        "canonical": "Machine Unlearning",
        "aliases": [
          "MU"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for addressing privacy and copyright issues in LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Concept Unlearning",
      "resolved_canonical": "Concept Unlearning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Knowledge Graphs",
      "resolved_canonical": "Knowledge Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Machine Unlearning",
      "resolved_canonical": "Machine Unlearning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ê°œë… ìŠê¸°: ìê°€ êµ¬ì¶• ì§€ì‹ ì‚¼ì¤‘í•­ì„ í†µí•œ ì ‘ê·¼

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15621.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15621](https://arxiv.org/abs/2509.15621)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models_20250922|Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models]] (88.1% similar)
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release: Iterative LLM Unlearning with Self-generated Data]] (86.5% similar)
- [[2025-09-18/CUFG_ Curriculum Unlearning Guided by the Forgetting Gradient_20250918|CUFG: Curriculum Unlearning Guided by the Forgetting Gradient]] (85.1% similar)
- [[2025-09-17/Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning_20250917|Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning]] (82.8% similar)
- [[2025-09-22/Pre-Forgettable Models_ Prompt Learning as a Native Mechanism for Unlearning_20250922|Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Graph|Knowledge Graph]]
**âš¡ Unique Technical**: [[keywords/Concept Unlearning|Concept Unlearning]], [[keywords/Machine Unlearning|Machine Unlearning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15621v1 Announce Type: cross 
Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a solution to privacy and copyright issues in large language models (LLMs). Existing MU methods aim to remove specific target sentences from an LLM while minimizing damage to unrelated knowledge. However, these approaches require explicit target sentences and do not support removing broader concepts, such as persons or events. To address this limitation, we introduce Concept Unlearning (CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to represent the LLM's internal knowledge and define CU as removing the forgetting target nodes and associated edges. This graph-based formulation enables a more intuitive unlearning and facilitates the design of more effective methods. We propose a novel method that prompts the LLM to generate knowledge triplets and explanatory sentences about the forgetting target and applies the unlearning process to these representations. Our approach enables more precise and comprehensive concept removal by aligning the unlearning process with the LLM's internal knowledge representations. Experiments on real-world and synthetic datasets demonstrate that our method effectively achieves concept-level unlearning while preserving unrelated knowledge.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15621v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê¸°ê³„ í•™ìŠµ ì œê±°(Machine Unlearning, MU)ëŠ” ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê°œì¸ì •ë³´ ë³´í˜¸ ë° ì €ì‘ê¶Œ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ì±…ìœ¼ë¡œ ìƒë‹¹í•œ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ MU ë°©ë²•ì€ íŠ¹ì • ëª©í‘œ ë¬¸ì¥ì„ LLMì—ì„œ ì œê±°í•˜ë©´ì„œ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì— ëŒ€í•œ ì†ìƒì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ëª…ì‹œì ì¸ ëª©í‘œ ë¬¸ì¥ì´ í•„ìš”í•˜ë©°, ì‚¬ëŒì´ë‚˜ ì‚¬ê±´ê³¼ ê°™ì€ ë” ë„“ì€ ê°œë…ì„ ì œê±°í•˜ëŠ” ê²ƒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” LLM ì œê±°ì˜ ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œì„œ ê°œë… ì œê±°(Concept Unlearning, CU)ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ë‚´ë¶€ ì§€ì‹ì„ í‘œí˜„í•˜ê³ , CUë¥¼ ìŠì–´ì•¼ í•  ëª©í‘œ ë…¸ë“œì™€ ê´€ë ¨ëœ ì—£ì§€ë¥¼ ì œê±°í•˜ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì´ ê·¸ë˜í”„ ê¸°ë°˜ì˜ ê³µì‹í™”ëŠ” ë³´ë‹¤ ì§ê´€ì ì¸ ì œê±°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ë³´ë‹¤ íš¨ê³¼ì ì¸ ë°©ë²•ì˜ ì„¤ê³„ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì´ ìŠì–´ì•¼ í•  ëª©í‘œì— ëŒ€í•œ ì§€ì‹ ì‚¼ì¤‘í•­ê³¼ ì„¤ëª… ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ê³ , ì´ëŸ¬í•œ í‘œí˜„ì— ì œê±° ê³¼ì •ì„ ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ì œê±° ê³¼ì •ì„ LLMì˜ ë‚´ë¶€ ì§€ì‹ í‘œí˜„ê³¼ ì¼ì¹˜ì‹œí‚´ìœ¼ë¡œì¨ ë³´ë‹¤ ì •ë°€í•˜ê³  í¬ê´„ì ì¸ ê°œë… ì œê±°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤ì œ ë° í•©ì„± ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ë°©ë²•ì´ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì„ ë³´ì¡´í•˜ë©´ì„œ ê°œë… ìˆ˜ì¤€ì˜ ì œê±°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ê°œì¸ì •ë³´ ë° ì €ì‘ê¶Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê¸°ê³„ í•™ìŠµ ì œê±°(MU) ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì ìƒˆë¡œìš´ ê°œë… ì œê±°(CU) ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ MU ë°©ë²•ì€ íŠ¹ì • ë¬¸ì¥ì„ ì œê±°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì§€ë§Œ, CUëŠ” ì‚¬ëŒì´ë‚˜ ì‚¬ê±´ê³¼ ê°™ì€ ë” ë„“ì€ ê°œë…ì„ ì œê±°í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ë‚´ë¶€ ì§€ì‹ì„ í‘œí˜„í•˜ê³ , ì œê±°í•  ê°œë…ì˜ ë…¸ë“œì™€ ê´€ë ¨ëœ ì—£ì§€ë¥¼ ì‚­ì œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ CUë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ LLMì´ ìƒì„±í•œ ì§€ì‹ ì‚¼ì¤‘í•­ê³¼ ì„¤ëª… ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì œê±° ê³¼ì •ì„ ìˆ˜í–‰í•˜ì—¬, ë³´ë‹¤ ì •ë°€í•˜ê³  í¬ê´„ì ì¸ ê°œë… ì œê±°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì„ ë³´ì¡´í•˜ë©´ì„œë„ ê°œë… ìˆ˜ì¤€ì˜ ì œê±°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ê³„ í•™ìŠµì—ì„œì˜ 'ê¸°ì–µ ì§€ìš°ê¸°'ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ì„œ í”„ë¼ì´ë²„ì‹œì™€ ì €ì‘ê¶Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ 'ê¸°ì–µ ì§€ìš°ê¸°' ë°©ë²•ì€ íŠ¹ì • ë¬¸ì¥ì„ ì œê±°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì§€ë§Œ, ê°œë… ì „ì²´ë¥¼ ì œê±°í•˜ëŠ” ë°ëŠ” í•œê³„ê°€ ìˆë‹¤.
- 3. 'ê°œë… ì§€ìš°ê¸°'ëŠ” ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ë‚´ë¶€ ì§€ì‹ì„ í‘œí˜„í•˜ê³ , ìŠí˜€ì•¼ í•  ë…¸ë“œì™€ ê´€ë ¨ëœ ì—£ì§€ë¥¼ ì œê±°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ì˜ëœë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ì–¸ì–´ ëª¨ë¸ì´ ìŠí˜€ì•¼ í•  ëŒ€ìƒì— ëŒ€í•œ ì§€ì‹ ì‚¼ì¤‘í•­ê³¼ ì„¤ëª… ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 'ê¸°ì–µ ì§€ìš°ê¸°'ë¥¼ ìˆ˜í–‰í•œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê°œë… ìˆ˜ì¤€ì˜ 'ê¸°ì–µ ì§€ìš°ê¸°'ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©´ì„œë„ ê´€ë ¨ ì—†ëŠ” ì§€ì‹ì€ ë³´ì¡´í•˜ëŠ” ë° ì„±ê³µí–ˆë‹¤.


---

*Generated on 2025-09-23 10:50:55*