---
keywords:
  - Large Language Model
  - Continued Pretraining
  - In-Context Learning
  - Curriculum Learning
  - Exponential Moving Average
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2506.00288
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:01:32.949388",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Continued Pretraining",
    "In-Context Learning",
    "Curriculum Learning",
    "Exponential Moving Average"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Continued Pretraining": 0.78,
    "In-Context Learning": 0.82,
    "Curriculum Learning": 0.8,
    "Exponential Moving Average": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on language adaptation and emergent abilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Continued Pretraining",
        "canonical": "Continued Pretraining",
        "aliases": [
          "CPT"
        ],
        "category": "unique_technical",
        "rationale": "Key process discussed for adapting language models to new languages.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "In-Context Learning",
        "canonical": "In-Context Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "Important concept for evaluating model capabilities in new languages.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Curriculum Learning",
        "canonical": "Curriculum Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Proposed as an effective alternative method to mitigate reliance on English data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.68,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Exponential Moving Average of Weights",
        "canonical": "Exponential Moving Average",
        "aliases": [
          "EMA"
        ],
        "category": "specific_connectable",
        "rationale": "Technique introduced to improve model training stability and performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "validation perplexity",
      "downstream capabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Continued Pretraining",
      "resolved_canonical": "Continued Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Curriculum Learning",
      "resolved_canonical": "Curriculum Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.68,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Exponential Moving Average of Weights",
      "resolved_canonical": "Exponential Moving Average",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation

**Korean Title:** ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì†ì ì¸ ì‚¬ì „ í•™ìŠµì„ í†µí•œ ì–¸ì–´ ì ì‘ì—ì„œì˜ ë°œí˜„ ëŠ¥ë ¥

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.00288.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2506.00288](https://arxiv.org/abs/2506.00288)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (86.0% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (84.1% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (83.3% similar)
- [[2025-09-19/ReCoVeR the Target Language_ Language Steering without Sacrificing Task Performance_20250919|ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance]] (82.9% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/In-Context Learning|In-Context Learning]], [[keywords/Curriculum Learning|Curriculum Learning]], [[keywords/Exponential Moving Average|Exponential Moving Average]]
**âš¡ Unique Technical**: [[keywords/Continued Pretraining|Continued Pretraining]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.00288v3 Announce Type: replace-cross 
Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.00288v3 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ì§€ì†ì ì¸ ì‚¬ì „ í•™ìŠµ(CPT)ì€ ê¸°ì¡´ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ì–¸ì–´ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ì¸ê¸° ìˆëŠ” ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì˜ì–´ ë°ì´í„°ë¥¼ ì¼ë¶€ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ê´€í–‰ì´ì§€ë§Œ, ê·¸ ì—­í• ì— ëŒ€í•œ ë©´ë°€í•œ ì—°êµ¬ëŠ” ì•„ì§ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì˜ì–´ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ê²€ì¦ í˜¼ë€ë„ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì§€ë§Œ, ëª©í‘œ ì–¸ì–´ì—ì„œì˜ í•˜ìœ„ ì‘ì—… ëŠ¥ë ¥ì˜ ë°œí˜„ì—ëŠ” ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë§¥ë½ ë‚´ í•™ìŠµ(ICL)ì„ ìœ„í•œ ì–¸ì–´ ë¹„ì˜ì¡´ì  ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í•˜ì˜€ìœ¼ë©°, ì˜ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šì„ ê²½ìš° CPT ì´ˆê¸°ì— ì¹˜ëª…ì ì¸ ë§ê°ì´ ë°œìƒí•¨ì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª©í‘œ ì–¸ì–´ì—ì„œì˜ í•˜ìœ„ ì‘ì—…ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì†ìƒì‹œí‚¤ë©°, ì´ëŠ” í›ˆë ¨ í›„ë°˜ê¹Œì§€ ì •í™•ë„ ì¸¡ë©´ì—ì„œëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•Šë”ë¼ë„ í˜¼ë€ë„ ì¸¡ë©´ì—ì„œ ì¸¡ì •ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì˜ í° ë³€í™”ì™€ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì˜ì–´ì˜ í•„ìš”ì„±ì„ ì¤„ì´ê¸° ìœ„í•œ íš¨ê³¼ì ì¸ ëŒ€ì•ˆìœ¼ë¡œ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµê³¼ ê°€ì¤‘ì¹˜ì˜ ì§€ìˆ˜ ì´ë™ í‰ê· (EMA)ì„ ë„ì…í•©ë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ì–¸ì–´ ì ì‘ì„ ìœ„í•œ CPTë¥¼ ìˆ˜í–‰í•  ë•Œ ë°œí˜„ ëŠ¥ë ¥ì´ ë‚˜íƒ€ë‚˜ëŠ” ì—­í•™ì„ ì¡°ëª…í•˜ë©°, ë¯¸ë˜ì— ë” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì„¤ê³„í•˜ê¸° ìœ„í•œ ê¸°ì´ˆë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì§€ì†ì  ì‚¬ì „ í›ˆë ¨(CPT)ì— ëŒ€í•´ ì—°êµ¬í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì˜ì–´ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ê²€ì¦ í¼í”Œë ‰ì‹œí‹°ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì§€ë§Œ, ëª©í‘œ ì–¸ì–´ì—ì„œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëŠ¥ë ¥ ë°œí˜„ì— ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì˜ì–´ë¥¼ í¬í•¨í•˜ì§€ ì•Šì„ ê²½ìš° ì´ˆê¸° CPTì—ì„œ ë§ê° í˜„ìƒì´ ë°œìƒí•˜ì—¬ ëª©í‘œ ì–¸ì–´ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì†ìƒëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµê³¼ ê°€ì¤‘ì¹˜ì˜ ì§€ìˆ˜ ì´ë™ í‰ê· (EMA)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–¸ì–´ ì ì‘ì„ ìœ„í•œ CPTì˜ ë™ì  ê³¼ì •ì„ ì´í•´í•˜ê³ , í–¥í›„ ë” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì„¤ê³„í•˜ëŠ” ë° ê¸°ì´ˆê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì§€ì†ì  ì‚¬ì „ í›ˆë ¨(CPT)ì´ ì‚¬ìš©ëœë‹¤.
- 2. ì˜ì–´ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ê²€ì¦ í˜¼ë€ë„ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì§€ë§Œ, ëª©í‘œ ì–¸ì–´ì—ì„œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëŠ¥ë ¥ ë°œí˜„ì—ëŠ” ì¤‘ìš”í•˜ë‹¤.
- 3. ì˜ì–´ë¥¼ í¬í•¨í•˜ì§€ ì•Šì„ ê²½ìš°, CPT ì´ˆê¸°ì— ë§ê° í˜„ìƒì´ ë°œìƒí•˜ë©°, ì´ëŠ” ëª©í‘œ ì–¸ì–´ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì†ìƒì‹œí‚¨ë‹¤.
- 4. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµê³¼ ì§€ìˆ˜ ì´ë™ í‰ê· (EMA)ì„ í†µí•´ ì˜ì–´ì˜ í•„ìš”ì„±ì„ ì¤„ì´ëŠ” íš¨ê³¼ì ì¸ ëŒ€ì•ˆì´ ì œì‹œë˜ì—ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ì–¸ì–´ ì ì‘ì„ ìœ„í•œ CPTì—ì„œ ë°œìƒí•˜ëŠ” ëŠ¥ë ¥ ë°œí˜„ì˜ ë™ì  ê³¼ì •ì„ ì¡°ëª…í•˜ê³ , í–¥í›„ ë” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì„¤ê³„í•˜ëŠ” ê¸°ì´ˆë¥¼ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-23 10:01:32*