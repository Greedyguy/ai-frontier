---
keywords:
  - Large Language Model
  - Continued Pretraining
  - In-Context Learning
  - Curriculum Learning
  - Exponential Moving Average
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2506.00288
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:01:32.949388",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Continued Pretraining",
    "In-Context Learning",
    "Curriculum Learning",
    "Exponential Moving Average"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Continued Pretraining": 0.78,
    "In-Context Learning": 0.82,
    "Curriculum Learning": 0.8,
    "Exponential Moving Average": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on language adaptation and emergent abilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Continued Pretraining",
        "canonical": "Continued Pretraining",
        "aliases": [
          "CPT"
        ],
        "category": "unique_technical",
        "rationale": "Key process discussed for adapting language models to new languages.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "In-Context Learning",
        "canonical": "In-Context Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "Important concept for evaluating model capabilities in new languages.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Curriculum Learning",
        "canonical": "Curriculum Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Proposed as an effective alternative method to mitigate reliance on English data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.68,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Exponential Moving Average of Weights",
        "canonical": "Exponential Moving Average",
        "aliases": [
          "EMA"
        ],
        "category": "specific_connectable",
        "rationale": "Technique introduced to improve model training stability and performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "validation perplexity",
      "downstream capabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Continued Pretraining",
      "resolved_canonical": "Continued Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Curriculum Learning",
      "resolved_canonical": "Curriculum Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.68,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Exponential Moving Average of Weights",
      "resolved_canonical": "Exponential Moving Average",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation

**Korean Title:** 대규모 언어 모델의 지속적인 사전 학습을 통한 언어 적응에서의 발현 능력

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.00288.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2506.00288](https://arxiv.org/abs/2506.00288)

## 🔗 유사한 논문
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (86.0% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (84.1% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (83.3% similar)
- [[2025-09-19/ReCoVeR the Target Language_ Language Steering without Sacrificing Task Performance_20250919|ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance]] (82.9% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (82.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/In-Context Learning|In-Context Learning]], [[keywords/Curriculum Learning|Curriculum Learning]], [[keywords/Exponential Moving Average|Exponential Moving Average]]
**⚡ Unique Technical**: [[keywords/Continued Pretraining|Continued Pretraining]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.00288v3 Announce Type: replace-cross 
Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.

## 🔍 Abstract (한글 번역)

arXiv:2506.00288v3 발표 유형: 교차 교체  
초록: 지속적인 사전 학습(CPT)은 기존의 대형 언어 모델(LLM)을 새로운 언어에 적응시키기 위한 인기 있는 접근 방식입니다. 이 과정에서 영어 데이터를 일부 포함하는 것이 일반적인 관행이지만, 그 역할에 대한 면밀한 연구는 아직 이루어지지 않았습니다. 본 연구에서는 영어를 포함하는 것이 검증 혼란도에는 영향을 미치지 않지만, 목표 언어에서의 하위 작업 능력의 발현에는 중요하다는 것을 보여줍니다. 우리는 맥락 내 학습(ICL)을 위한 언어 비의존적 벤치마크를 도입하였으며, 영어가 포함되지 않을 경우 CPT 초기에 치명적인 망각이 발생함을 밝혀냈습니다. 이는 목표 언어에서의 하위 작업에 대한 일반화 능력을 손상시키며, 이는 훈련 후반까지 정확도 측면에서는 나타나지 않더라도 혼란도 측면에서 측정됩니다. 이는 모델 매개변수의 큰 변화와 관련이 있습니다. 이러한 통찰을 바탕으로, 우리는 영어의 필요성을 줄이기 위한 효과적인 대안으로 커리큘럼 학습과 가중치의 지수 이동 평균(EMA)을 도입합니다. 전반적으로, 우리의 연구는 언어 적응을 위한 CPT를 수행할 때 발현 능력이 나타나는 역학을 조명하며, 미래에 더 효과적인 방법을 설계하기 위한 기초로 활용될 수 있습니다.

## 📝 요약

이 논문은 기존 대형 언어 모델(LLM)을 새로운 언어로 적응시키기 위한 방법으로 사용되는 지속적 사전 훈련(CPT)에 대해 연구합니다. 연구 결과, 영어 데이터를 포함하는 것이 검증 퍼플렉시티에는 영향을 미치지 않지만, 목표 언어에서의 다운스트림 능력 발현에 중요하다는 것을 발견했습니다. 또한, 영어를 포함하지 않을 경우 초기 CPT에서 망각 현상이 발생하여 목표 언어의 일반화 능력이 손상된다는 것을 보여줍니다. 이를 해결하기 위해 커리큘럼 학습과 가중치의 지수 이동 평균(EMA)을 제안합니다. 이 연구는 언어 적응을 위한 CPT의 동적 과정을 이해하고, 향후 더 효과적인 방법을 설계하는 데 기초가 될 수 있습니다.

## 🎯 주요 포인트

- 1. 기존 대형 언어 모델(LLM)을 새로운 언어로 적응시키기 위한 방법으로 지속적 사전 훈련(CPT)이 사용된다.
- 2. 영어 데이터를 포함하는 것이 검증 혼란도에는 영향을 미치지 않지만, 목표 언어에서의 다운스트림 능력 발현에는 중요하다.
- 3. 영어를 포함하지 않을 경우, CPT 초기에 망각 현상이 발생하며, 이는 목표 언어의 다운스트림 프롬프트에 대한 일반화 능력을 손상시킨다.
- 4. 커리큘럼 학습과 지수 이동 평균(EMA)을 통해 영어의 필요성을 줄이는 효과적인 대안이 제시되었다.
- 5. 본 연구는 언어 적응을 위한 CPT에서 발생하는 능력 발현의 동적 과정을 조명하고, 향후 더 효과적인 방법을 설계하는 기초를 제공한다.


---

*Generated on 2025-09-23 10:01:32*