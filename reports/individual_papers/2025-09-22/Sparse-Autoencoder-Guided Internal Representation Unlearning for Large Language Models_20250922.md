---
keywords:
  - Large Language Model
  - Sparse Autoencoder
  - Internal Representation Unlearning
  - Model Collapse
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15631
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:51:22.169849",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Sparse Autoencoder",
    "Internal Representation Unlearning",
    "Model Collapse"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Sparse Autoencoder": 0.78,
    "Internal Representation Unlearning": 0.82,
    "Model Collapse": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion, linking to broader discussions in NLP and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sparse Autoencoder",
        "canonical": "Sparse Autoencoder",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key technique proposed for unlearning, offering a novel approach to model modification.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Internal Representation Unlearning",
        "canonical": "Internal Representation Unlearning",
        "aliases": [
          "Representation Forgetting"
        ],
        "category": "unique_technical",
        "rationale": "Describes the novel method introduced in the paper, crucial for understanding the contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Model Collapse",
        "canonical": "Model Collapse",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Addresses a significant issue in model training and unlearning, relevant to model stability discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "unlearning techniques",
      "suppression-based methods",
      "activation of the target entity"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sparse Autoencoder",
      "resolved_canonical": "Sparse Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Internal Representation Unlearning",
      "resolved_canonical": "Internal Representation Unlearning",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Model Collapse",
      "resolved_canonical": "Model Collapse",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models

**Korean Title:** í¬ì†Œ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ë‚´ë¶€ í‘œí˜„ í•™ìŠµ ì œê±°ë¥¼ í†µí•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ê°œì„ 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15631.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15631](https://arxiv.org/abs/2509.15631)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release: Iterative LLM Unlearning with Self-generated Data]] (90.1% similar)
- [[2025-09-22/Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets_20250922|Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets]] (88.1% similar)
- [[2025-09-17/Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning_20250917|Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning]] (84.9% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.1% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (84.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Model Collapse|Model Collapse]]
**âš¡ Unique Technical**: [[keywords/Sparse Autoencoder|Sparse Autoencoder]], [[keywords/Internal Representation Unlearning|Internal Representation Unlearning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15631v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed across various applications, privacy and copyright concerns have heightened the need for more effective LLM unlearning techniques. Many existing unlearning methods aim to suppress undesirable outputs through additional training (e.g., gradient ascent), which reduces the probability of generating such outputs. While such suppression-based approaches can control model outputs, they may not eliminate the underlying knowledge embedded in the model's internal activations; muting a response is not the same as forgetting it. Moreover, such suppression-based methods often suffer from model collapse. To address these issues, we propose a novel unlearning method that directly intervenes in the model's internal activations. In our formulation, forgetting is defined as a state in which the activation of a forgotten target is indistinguishable from that of ``unknown'' entities. Our method introduces an unlearning objective that modifies the activation of the target entity away from those of known entities and toward those of unknown entities in a sparse autoencoder latent space. By aligning the target's internal activation with those of unknown entities, we shift the model's recognition of the target entity from ``known'' to ``unknown'', achieving genuine forgetting while avoiding over-suppression and model collapse. Empirically, we show that our method effectively aligns the internal activations of the forgotten target, a result that the suppression-based approaches do not reliably achieve. Additionally, our method effectively reduces the model's recall of target knowledge in question-answering tasks without significant damage to the non-target knowledge.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15631v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ì  ë” ë§ì´ ë°°ì¹˜ë¨ì— ë”°ë¼, í”„ë¼ì´ë²„ì‹œì™€ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì¸í•´ ë³´ë‹¤ íš¨ê³¼ì ì¸ LLM ìŠê¸° ê¸°ìˆ ì˜ í•„ìš”ì„±ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤. ë§ì€ ê¸°ì¡´ì˜ ìŠê¸° ë°©ë²•ì€ ì¶”ê°€ í•™ìŠµ(ì˜ˆ: ê¸°ìš¸ê¸° ìƒìŠ¹)ì„ í†µí•´ ë°”ëŒì§í•˜ì§€ ì•Šì€ ì¶œë ¥ì„ ì–µì œí•˜ì—¬ ê·¸ëŸ¬í•œ ì¶œë ¥ì´ ìƒì„±ë  í™•ë¥ ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ ì¶œë ¥ì„ ì œì–´í•  ìˆ˜ ìˆì§€ë§Œ, ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ë‚´ì¬ëœ ì§€ì‹ì„ ì œê±°í•˜ì§€ëŠ” ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ë‹µì„ ë¬´ìŒ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ê·¸ê²ƒì„ ìŠëŠ” ê²ƒê³¼ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²Œë‹¤ê°€, ì´ëŸ¬í•œ ì–µì œ ê¸°ë°˜ ë°©ë²•ì€ ì¢…ì¢… ëª¨ë¸ ë¶•ê´´ë¥¼ ê²ªìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ì§ì ‘ ê°œì…í•˜ëŠ” ìƒˆë¡œìš´ ìŠê¸° ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê³µì‹í™”ì—ì„œ, ìŠê¸°ëŠ” ìŠí˜€ì§„ ëŒ€ìƒì˜ í™œì„±í™”ê°€ "ì•Œ ìˆ˜ ì—†ëŠ”" ì—”í‹°í‹°ì™€ êµ¬ë³„ë˜ì§€ ì•ŠëŠ” ìƒíƒœë¡œ ì •ì˜ë©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ í¬ì†Œ ì˜¤í† ì¸ì½”ë” ì ì¬ ê³µê°„ì—ì„œ ëŒ€ìƒ ì—”í‹°í‹°ì˜ í™œì„±í™”ë¥¼ ì•Œë ¤ì§„ ì—”í‹°í‹°ë¡œë¶€í„° ë©€ì–´ì§€ê³  ì•Œ ìˆ˜ ì—†ëŠ” ì—”í‹°í‹°ë¡œ í–¥í•˜ë„ë¡ ìˆ˜ì •í•˜ëŠ” ìŠê¸° ëª©í‘œë¥¼ ë„ì…í•©ë‹ˆë‹¤. ëŒ€ìƒì˜ ë‚´ë¶€ í™œì„±í™”ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ì—”í‹°í‹°ì™€ ì •ë ¬í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ëŒ€ìƒ ì—”í‹°í‹°ë¥¼ "ì•Œë ¤ì§„" ìƒíƒœì—ì„œ "ì•Œ ìˆ˜ ì—†ëŠ”" ìƒíƒœë¡œ ì¸ì‹í•˜ë„ë¡ ì „í™˜í•˜ì—¬ ê³¼ë„í•œ ì–µì œì™€ ëª¨ë¸ ë¶•ê´´ë¥¼ í”¼í•˜ë©´ì„œ ì§„ì •í•œ ìŠê¸°ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ ì‹ ë¢°í•  ìˆ˜ ìˆê²Œ ë‹¬ì„±í•˜ì§€ ëª»í•˜ëŠ” ìŠí˜€ì§„ ëŒ€ìƒì˜ ë‚´ë¶€ í™œì„±í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì •ë ¬í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ë¹„ëŒ€ìƒ ì§€ì‹ì— ëŒ€í•œ í° ì†ìƒ ì—†ì´ ì§ˆë¬¸-ì‘ë‹µ ì‘ì—…ì—ì„œ ëŒ€ìƒ ì§€ì‹ì˜ ëª¨ë¸ íšŒìƒì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í”„ë¼ì´ë²„ì‹œ ë° ì €ì‘ê¶Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ 'ìŠê¸°' ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì–µì œ ê¸°ë°˜ ë°©ë²•ì€ ëª¨ë¸ì˜ ì¶œë ¥ë§Œì„ í†µì œí•˜ë©°, ëª¨ë¸ ë‚´ë¶€ì˜ ì§€ì‹ì„ ì™„ì „íˆ ì œê±°í•˜ì§€ ëª»í•˜ê³  ëª¨ë¸ ë¶•ê´´ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ì§ì ‘ ê°œì…í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìŠê³ ì í•˜ëŠ” ëŒ€ìƒì„ 'ì•Œë ¤ì§„' ìƒíƒœì—ì„œ 'ì•Œ ìˆ˜ ì—†ëŠ”' ìƒíƒœë¡œ ì „í™˜í•˜ì—¬ ì§„ì •í•œ ìŠê¸°ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì€ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ë²•ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ìƒ ì§€ì‹ì„ ìŠê²Œ í•˜ë©°, ë¹„ëŒ€ìƒ ì§€ì‹ì—ëŠ” í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ìš©ì´ ì¦ê°€í•¨ì— ë”°ë¼ í”„ë¼ì´ë²„ì‹œì™€ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì¸í•´ íš¨ê³¼ì ì¸ LLM ë§ê° ê¸°ìˆ ì˜ í•„ìš”ì„±ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì–µì œ ê¸°ë°˜ ë§ê° ë°©ë²•ì€ ëª¨ë¸ ì¶œë ¥ ì œì–´ì—ëŠ” íš¨ê³¼ì ì´ì§€ë§Œ, ëª¨ë¸ ë‚´ë¶€ì˜ ì§€ì‹ì„ ì™„ì „íˆ ì œê±°í•˜ì§€ ëª»í•˜ê³  ëª¨ë¸ ë¶•ê´´ ë¬¸ì œë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 3. ì œì•ˆëœ ìƒˆë¡œìš´ ë§ê° ë°©ë²•ì€ ëª¨ë¸ì˜ ë‚´ë¶€ í™œì„±í™”ì— ì§ì ‘ ê°œì…í•˜ì—¬, ìŠí˜€ì•¼ í•  ëŒ€ìƒì˜ í™œì„±í™”ë¥¼ "ì•Œë ¤ì§€ì§€ ì•Šì€" ì—”í‹°í‹°ì˜ í™œì„±í™”ì™€ êµ¬ë³„í•  ìˆ˜ ì—†ë„ë¡ ë§Œë“­ë‹ˆë‹¤.
- 4. ì´ ë°©ë²•ì€ í¬ì†Œ ì˜¤í† ì¸ì½”ë” ì ì¬ ê³µê°„ì—ì„œ ëŒ€ìƒ ì—”í‹°í‹°ì˜ í™œì„±í™”ë¥¼ ì¡°ì •í•˜ì—¬, ì§„ì •í•œ ë§ê°ì„ ë‹¬ì„±í•˜ë©´ì„œ ê³¼ë„í•œ ì–µì œì™€ ëª¨ë¸ ë¶•ê´´ë¥¼ í”¼í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì–µì œ ê¸°ë°˜ ì ‘ê·¼ë²•ì´ ì‹ ë¢°ì„± ìˆê²Œ ë‹¬ì„±í•˜ì§€ ëª»í•˜ëŠ” ë‚´ë¶€ í™œì„±í™” ì •ë ¬ì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©°, ë¹„ëŒ€ìƒ ì§€ì‹ì— í° ì†ìƒì„ ì£¼ì§€ ì•Šê³  ëŒ€ìƒ ì§€ì‹ì˜ íšŒìƒì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:51:22*