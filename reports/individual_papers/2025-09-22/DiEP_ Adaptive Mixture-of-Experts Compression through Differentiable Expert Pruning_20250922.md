---
keywords:
  - Mixture-of-Experts
  - Differentiable Expert Pruning
  - Natural Language Processing
  - Adaptive Gradient-based Pruning
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.16105
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:35:39.305060",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Differentiable Expert Pruning",
    "Natural Language Processing",
    "Adaptive Gradient-based Pruning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.78,
    "Differentiable Expert Pruning": 0.8,
    "Natural Language Processing": 0.85,
    "Adaptive Gradient-based Pruning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "Mixture-of-Experts is central to the paper's methodology and offers a unique approach to model compression.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Differentiable Expert Pruning",
        "canonical": "Differentiable Expert Pruning",
        "aliases": [
          "DiEP"
        ],
        "category": "unique_technical",
        "rationale": "This is the core technique introduced in the paper, offering a novel pruning strategy.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.8
      },
      {
        "surface": "Natural Language Processing",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "The paper's experiments are conducted within the NLP domain, providing context for application.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "adaptive gradient-based pruning",
        "canonical": "Adaptive Gradient-based Pruning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This technique is crucial for understanding the adaptive mechanisms in the proposed method.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Differentiable Expert Pruning",
      "resolved_canonical": "Differentiable Expert Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Natural Language Processing",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "adaptive gradient-based pruning",
      "resolved_canonical": "Adaptive Gradient-based Pruning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning

**Korean Title:** DiEP: ë¯¸ë¶„ ê°€ëŠ¥í•œ ì „ë¬¸ê°€ ê°€ì§€ì¹˜ê¸°ë¥¼ í†µí•œ ì ì‘í˜• ì „ë¬¸ê°€ í˜¼í•© ì••ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16105.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.16105](https://arxiv.org/abs/2509.16105)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (85.0% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (84.6% similar)
- [[2025-09-22/TrueMoE_ Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection_20250922|TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection]] (84.2% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (83.7% similar)
- [[2025-09-19/Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation_20250919|Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**ğŸ”— Specific Connectable**: [[keywords/Adaptive Gradient-based Pruning|Adaptive Gradient-based Pruning]]
**âš¡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Differentiable Expert Pruning|Differentiable Expert Pruning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16105v1 Announce Type: new 
Abstract: Despite the significant breakthrough of Mixture-of-Experts (MoE), the increasing scale of these MoE models presents huge memory and storage challenges. Existing MoE pruning methods, which involve reducing parameter size with a uniform sparsity across all layers, often lead to suboptimal outcomes and performance degradation due to varying expert redundancy in different MoE layers. To address this, we propose a non-uniform pruning strategy, dubbed \textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which adaptively adjusts pruning rates at the layer level while jointly learning inter-layer importance, effectively capturing the varying redundancy across different MoE layers. By transforming the global discrete search space into a continuous one, our method handles exponentially growing non-uniform expert combinations, enabling adaptive gradient-based pruning. Extensive experiments on five advanced MoE models demonstrate the efficacy of our method across various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original performance on Mixtral 8$\times$7B with only half the experts, outperforming other pruning methods by up to 7.1\% on the challenging MMLU dataset.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16105v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: Mixture-of-Experts (MoE)ì˜ ìƒë‹¹í•œ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ MoE ëª¨ë¸ì˜ ê·œëª¨ê°€ ì»¤ì§€ë©´ì„œ ë©”ëª¨ë¦¬ì™€ ì €ì¥ ê³µê°„ì— í° ë„ì „ ê³¼ì œê°€ ìƒê¸°ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì¸µì— ê±¸ì³ ê· ì¼í•œ í¬ì†Œì„±ì„ ì ìš©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê¸°ì¡´ì˜ MoE ê°€ì§€ì¹˜ê¸° ë°©ë²•ì€ ì„œë¡œ ë‹¤ë¥¸ MoE ì¸µì—ì„œ ì „ë¬¸ê°€ ì¤‘ë³µì„±ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ìµœì ì´ ì•„ë‹Œ ê²°ê³¼ì™€ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” \textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP})ì´ë¼ëŠ” ë¹„ê· ì¼ ê°€ì§€ì¹˜ê¸° ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì¸µ ìˆ˜ì¤€ì—ì„œ ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•˜ë©´ì„œ ì¸µ ê°„ ì¤‘ìš”ì„±ì„ ê³µë™ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ì„œë¡œ ë‹¤ë¥¸ MoE ì¸µì—ì„œì˜ ì¤‘ë³µì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. ì „ì—­ ì´ì‚° íƒìƒ‰ ê³µê°„ì„ ì—°ì†ì ì¸ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ì˜ ë°©ë²•ì€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë¹„ê· ì¼ ì „ë¬¸ê°€ ì¡°í•©ì„ ì²˜ë¦¬í•˜ê³ , ì ì‘í˜• ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ê°€ì§€ì¹˜ê¸°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°œì˜ ê³ ê¸‰ MoE ëª¨ë¸ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ë‹¤ì–‘í•œ NLP ì‘ì—…ì—ì„œ ìš°ë¦¬ì˜ ë°©ë²•ì˜ íš¨ëŠ¥ì„ ì…ì¦í•©ë‹ˆë‹¤. íŠ¹íˆ, \textbf{DiEP}ëŠ” Mixtral 8$\times$7Bì—ì„œ ì „ë¬¸ê°€ì˜ ì ˆë°˜ë§Œìœ¼ë¡œë„ ì›ë˜ ì„±ëŠ¥ì˜ ì•½ 92\%ë¥¼ ìœ ì§€í•˜ë©°, ë„ì „ì ì¸ MMLU ë°ì´í„°ì…‹ì—ì„œ ë‹¤ë¥¸ ê°€ì§€ì¹˜ê¸° ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 7.1\% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Mixture-of-Experts (MoE) ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë° ì €ì¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¹„ê· ì¼í•œ ê°€ì§€ì¹˜ê¸° ì „ëµì¸ DiEP(Differentiable Expert Pruning)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê· ì¼í•œ ê°€ì§€ì¹˜ê¸° ë°©ë²•ì€ ê° ì¸µì˜ ì „ë¬¸ê°€ ì¤‘ë³µë„ê°€ ë‹¤ë¦„ì—ë„ ë¶ˆêµ¬í•˜ê³  ëª¨ë“  ì¸µì— ë™ì¼í•œ í¬ì†Œì„±ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í–ˆìŠµë‹ˆë‹¤. DiEPëŠ” ì¸µë³„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•˜ë©° ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨ì„ ì¡°ì •í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì „ì—­ ì´ì‚° íƒìƒ‰ ê³µê°„ì„ ì—°ì†ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ì‘í˜• ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ê°€ì§€ì¹˜ê¸°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DiEPëŠ” ë‹¤ì–‘í•œ NLP ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ Mixtral 8Ã—7B ëª¨ë¸ì—ì„œ ì ˆë°˜ì˜ ì „ë¬¸ê°€ë§Œ ì‚¬ìš©í•˜ë©´ì„œë„ ì›ë˜ ì„±ëŠ¥ì˜ ì•½ 92%ë¥¼ ìœ ì§€í•˜ë©°, MMLU ë°ì´í„°ì…‹ì—ì„œ ë‹¤ë¥¸ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 7.1% ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Mixture-of-Experts(MoE) ëª¨ë¸ì˜ í™•ì¥ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ë° ì €ì¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¹„ê· ì¼í•œ ê°€ì§€ì¹˜ê¸° ì „ëµì¸ DiEPë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. DiEPëŠ” ë ˆì´ì–´ë³„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•˜ì—¬ ê° MoE ë ˆì´ì–´ì˜ ì¤‘ë³µì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ê³ , ë ˆì´ì–´ ìˆ˜ì¤€ì—ì„œ ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨ì„ ì¡°ì •í•©ë‹ˆë‹¤.
- 3. ê¸€ë¡œë²Œ ì´ì‚° íƒìƒ‰ ê³µê°„ì„ ì—°ì†ì ì¸ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ì‘í˜• ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ê°€ì§€ì¹˜ê¸°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. DiEPëŠ” Mixtral 8Ã—7B ëª¨ë¸ì—ì„œ ì ˆë°˜ì˜ ì „ë¬¸ê°€ë§Œìœ¼ë¡œë„ ì›ë˜ ì„±ëŠ¥ì˜ ì•½ 92%ë¥¼ ìœ ì§€í•˜ë©°, ë‹¤ë¥¸ ê°€ì§€ì¹˜ê¸° ë°©ë²•ë³´ë‹¤ MMLU ë°ì´í„°ì…‹ì—ì„œ ìµœëŒ€ 7.1% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ì–‘í•œ NLP ì‘ì—…ì—ì„œ 5ê°œì˜ ê³ ê¸‰ MoE ëª¨ë¸ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´ ê·¸ íš¨ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:35:39*