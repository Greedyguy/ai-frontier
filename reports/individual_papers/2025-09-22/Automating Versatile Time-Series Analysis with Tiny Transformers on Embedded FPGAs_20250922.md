---
keywords:
  - Tiny Transformers
  - FPGA
  - Quantization-aware Training
  - Time-Series Analysis
  - Automatic VHDL Generation
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2505.17662
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:08:36.384935",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Tiny Transformers",
    "FPGA",
    "Quantization-aware Training",
    "Time-Series Analysis",
    "Automatic VHDL Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Tiny Transformers": 0.78,
    "FPGA": 0.8,
    "Quantization-aware Training": 0.77,
    "Time-Series Analysis": 0.79,
    "Automatic VHDL Generation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Tiny Transformers",
        "canonical": "Tiny Transformers",
        "aliases": [
          "Compact Transformers",
          "Small Transformers"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a specific adaptation of Transformer models for resource-constrained environments, which is a novel area of research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Field-Programmable Gate Arrays",
        "canonical": "FPGA",
        "aliases": [
          "FPGAs"
        ],
        "category": "broad_technical",
        "rationale": "FPGA is a key hardware platform discussed in the paper, providing a basis for linking hardware-specific optimizations.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Quantization-aware training",
        "canonical": "Quantization-aware Training",
        "aliases": [
          "Quantization Training"
        ],
        "category": "specific_connectable",
        "rationale": "This technique is crucial for reducing model size and computational requirements, linking to efficient model deployment.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Time-series analysis",
        "canonical": "Time-Series Analysis",
        "aliases": [
          "Time Series Analysis"
        ],
        "category": "specific_connectable",
        "rationale": "A central theme of the paper, connecting various tasks such as forecasting and classification.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.68,
        "link_intent_score": 0.79
      },
      {
        "surface": "Automatic VHDL generation",
        "canonical": "Automatic VHDL Generation",
        "aliases": [
          "VHDL Automation"
        ],
        "category": "unique_technical",
        "rationale": "This process is a unique technical contribution facilitating seamless FPGA deployment.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "resource-constrained devices",
      "deployment framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Tiny Transformers",
      "resolved_canonical": "Tiny Transformers",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Field-Programmable Gate Arrays",
      "resolved_canonical": "FPGA",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Quantization-aware training",
      "resolved_canonical": "Quantization-aware Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Time-series analysis",
      "resolved_canonical": "Time-Series Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.68,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Automatic VHDL generation",
      "resolved_canonical": "Automatic VHDL Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs

**Korean Title:** ì„ë² ë””ë“œ FPGAì—ì„œ Tiny Transformerë¥¼ í™œìš©í•œ ë‹¤ëª©ì  ì‹œê³„ì—´ ë¶„ì„ ìë™í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.17662.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2505.17662](https://arxiv.org/abs/2505.17662)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (80.7% similar)
- [[2025-09-19/MaRVIn_ A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration_20250919|MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration]] (79.9% similar)
- [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (79.0% similar)
- [[2025-09-19/Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization_20250919|Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization]] (78.9% similar)
- [[2025-09-19/Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images]] (78.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/FPGA|FPGA]]
**ğŸ”— Specific Connectable**: [[keywords/Quantization-aware Training|Quantization-aware Training]], [[keywords/Time-Series Analysis|Time-Series Analysis]]
**âš¡ Unique Technical**: [[keywords/Tiny Transformers|Tiny Transformers]], [[keywords/Automatic VHDL Generation|Automatic VHDL Generation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.17662v5 Announce Type: replace 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.17662v5 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‹œê³„ì—´ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ë†’ì€ ë©”ëª¨ë¦¬ ë° ê³„ì‚° ìš”êµ¬ë¡œ ì¸í•´ ìì›ì´ ì œí•œëœ ì¥ì¹˜ì— ë°°í¬í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ì–´ë ¤ìš´ ê³¼ì œì…ë‹ˆë‹¤. ë§ˆì´í¬ë¡œì»¨íŠ¸ë¡¤ëŸ¬ ìœ ë‹›(MCU)ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì´ì „ ì—°êµ¬ëŠ” í•˜ë“œì›¨ì–´ íŠ¹ì • ìµœì í™”ë¥¼ íƒêµ¬í–ˆì§€ë§Œ, ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ì¢…ì¢… ì‘ì—…ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©° 8ë¹„íŠ¸ ê³ ì • ì†Œìˆ˜ì  ì •ë°€ë„ì— ì œí•œë©ë‹ˆë‹¤. í•„ë“œ í”„ë¡œê·¸ë˜ë¨¸ë¸” ê²Œì´íŠ¸ ì–´ë ˆì´(FPGA)ëŠ” ë°ì´í„° ì •ë°€ë„ì™€ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì„¸ë°€í•œ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë” í° ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ê¸°ì¡´ì˜ FPGA ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë°°í¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ë™ êµ¬ì„±ê³¼ í•¨ê»˜ ê³ ë°€ë„ í”Œë«í¼ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì„ë² ë””ë“œ FPGAì—ì„œ Tiny Transformersì˜ í†µí•©ë˜ê³  ì™„ì „ ìë™í™”ëœ ë°°í¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ì‹œê³„ì—´ ì‘ì—…(ì˜ˆì¸¡, ë¶„ë¥˜, ì´ìƒ íƒì§€)ì— ê±¸ì³ ì»´íŒ©íŠ¸í•œ ì¸ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ëŠ” 4ë¹„íŠ¸ê¹Œì§€ì˜ ì–‘ìí™” ì¸ì‹ í›ˆë ¨, Optunaë¥¼ ì‚¬ìš©í•œ í•˜ë“œì›¨ì–´ ì¸ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰, ì›í™œí•œ ë°°í¬ë¥¼ ìœ„í•œ ìë™ VHDL ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°œì˜ ì„ë² ë””ë“œ FPGA í”Œë«í¼ì—ì„œ ì—¬ì„¯ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì„ í†µí•´ ìš°ë¦¬ í”„ë ˆì„ì›Œí¬ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ìš°ë¦¬ í”„ë ˆì„ì›Œí¬ê°€ AMD Spartan-7ì—ì„œ ë°€ë¦¬ì´ˆ ì§€ì—° ì‹œê°„ìœ¼ë¡œ ì¶”ë¡ ë‹¹ 0.033 mJê¹Œì§€ ë‚®ì€ ì—ë„ˆì§€ë¥¼ ì†Œëª¨í•˜ëŠ” ì •ìˆ˜ ì „ìš©, ì‘ì—… íŠ¹í™” íŠ¸ëœìŠ¤í¬ë¨¸ ê°€ì†ê¸°ë¥¼ ìƒì„±í•˜ë©°, Lattice iCE40ì—ì„œì˜ ë°°í¬ ê°€ëŠ¥ì„±ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œëŠ” GitHub ì €ì¥ì†Œ(https://github.com/Edwina1030/TinyTransformer4TS)ì— ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì„ë² ë””ë“œ FPGAì—ì„œì˜ Tiny Transformer ëª¨ë¸ ë°°í¬ë¥¼ ìœ„í•œ í†µí•© ë° ìë™í™”ëœ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ì‹œê³„ì—´ ì‘ì—…(ì˜ˆì¸¡, ë¶„ë¥˜, ì´ìƒ íƒì§€)ì— ëŒ€í•´ 4ë¹„íŠ¸ê¹Œì§€ì˜ ì–‘ìí™” ì¸ì‹ í›ˆë ¨ê³¼ Optunaë¥¼ í™œìš©í•œ í•˜ë“œì›¨ì–´ ì¸ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰, ìë™ VHDL ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤. ë‘ ê°œì˜ ì„ë² ë””ë“œ FPGA í”Œë«í¼ì—ì„œ ì—¬ì„¯ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì„ í‰ê°€í•œ ê²°ê³¼, AMD Spartan-7ì—ì„œ 0.033 mJì˜ ì—ë„ˆì§€ ì†Œëª¨ì™€ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ì§€ì—° ì‹œê°„ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” Lattice iCE40ì—ì„œì˜ ë°°í¬ ê°€ëŠ¥ì„±ì— ëŒ€í•œ í†µì°°ë„ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transformer ê¸°ë°˜ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‹œê³„ì—´ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìì› ì œì•½ì´ ìˆëŠ” ì¥ì¹˜ì—ì„œì˜ ë°°í¬ëŠ” ì—¬ì „íˆ ë„ì „ì ì…ë‹ˆë‹¤.
- 2. ì´ ë…¼ë¬¸ì€ ì„ë² ë””ë“œ FPGAì—ì„œ Tiny Transformersë¥¼ ìœ„í•œ í†µí•©ë˜ê³  ì™„ì „ ìë™í™”ëœ ë°°í¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” 4ë¹„íŠ¸ê¹Œì§€ì˜ ì–‘ìí™” ì¸ì‹ í›ˆë ¨, Optunaë¥¼ ì‚¬ìš©í•œ í•˜ë“œì›¨ì–´ ì¸ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰, ìë™ VHDL ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.
- 4. AMD Spartan-7ì—ì„œì˜ ì¶”ë¡ ë‹¹ 0.033 mJì˜ ì—ë„ˆì§€ ì†Œë¹„ì™€ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ì§€ì—° ì‹œê°„ì„ ë‹¬ì„±í•˜ëŠ” ì •ìˆ˜ ì „ìš©, ì‘ì—…ë³„ Transformer ê°€ì†ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- 5. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œëŠ” GitHub ì €ì¥ì†Œì— ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:08:36*