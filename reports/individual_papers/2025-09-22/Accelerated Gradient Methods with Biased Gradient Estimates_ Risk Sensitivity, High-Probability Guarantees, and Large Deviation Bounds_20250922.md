---
keywords:
  - Generalized Momentum Methods
  - Risk-Sensitive Index
  - Large-Deviation Principle
  - Smooth Strongly Convex Functions
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.13628
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:25:10.267053",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Generalized Momentum Methods",
    "Risk-Sensitive Index",
    "Large-Deviation Principle",
    "Smooth Strongly Convex Functions"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Generalized Momentum Methods": 0.78,
    "Risk-Sensitive Index": 0.77,
    "Large-Deviation Principle": 0.79,
    "Smooth Strongly Convex Functions": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Generalized Momentum Methods",
        "canonical": "Generalized Momentum Methods",
        "aliases": [
          "GMMs"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's analysis and offers a unique perspective on momentum methods, which are crucial for linking with optimization literature.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Risk-Sensitive Index",
        "canonical": "Risk-Sensitive Index",
        "aliases": [
          "RSI"
        ],
        "category": "unique_technical",
        "rationale": "RSI is a novel metric introduced in the paper, providing a new dimension to robustness analysis in optimization.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Large-Deviation Principle",
        "canonical": "Large-Deviation Principle",
        "aliases": [
          "LDP"
        ],
        "category": "specific_connectable",
        "rationale": "The large-deviation principle is a significant concept in probability theory, relevant for linking with statistical analysis in optimization.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      },
      {
        "surface": "Smooth Strongly Convex Functions",
        "canonical": "Smooth Strongly Convex Functions",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "This is a fundamental concept in optimization theory, critical for understanding the context of the paper's results.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "gradient errors",
      "quadratic objectives",
      "Gaussian noise"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Generalized Momentum Methods",
      "resolved_canonical": "Generalized Momentum Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Risk-Sensitive Index",
      "resolved_canonical": "Risk-Sensitive Index",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Large-Deviation Principle",
      "resolved_canonical": "Large-Deviation Principle",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Smooth Strongly Convex Functions",
      "resolved_canonical": "Smooth Strongly Convex Functions",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds

**Korean Title:** í¸í–¥ëœ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°€ì† ê·¸ë˜ë””ì–¸íŠ¸ ë°©ë²•: ìœ„í—˜ ë¯¼ê°ì„±, ë†’ì€ í™•ë¥  ë³´ì¥, ë° í° í¸ì°¨ ê²½ê³„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.13628.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.13628](https://arxiv.org/abs/2509.13628)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (83.3% similar)
- [[2025-09-22/Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses_20250922|Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses]] (83.0% similar)
- [[2025-09-22/Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection_20250922|Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection]] (81.7% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (81.5% similar)
- [[2025-09-18/Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain_20250918|Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Smooth Strongly Convex Functions|Smooth Strongly Convex Functions]]
**ğŸ”— Specific Connectable**: [[keywords/Large-Deviation Principle|Large-Deviation Principle]]
**âš¡ Unique Technical**: [[keywords/Generalized Momentum Methods|Generalized Momentum Methods]], [[keywords/Risk-Sensitive Index|Risk-Sensitive Index]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13628v2 Announce Type: replace-cross 
Abstract: We study trade-offs between convergence rate and robustness to gradient errors in the context of first-order methods. Our focus is on generalized momentum methods (GMMs)--a broad class that includes Nesterov's accelerated gradient, heavy-ball, and gradient descent methods--for minimizing smooth strongly convex objectives. We allow stochastic gradient errors that may be adversarial and biased, and quantify robustness of these methods to gradient errors via the risk-sensitive index (RSI) from robust control theory. For quadratic objectives with i.i.d. Gaussian noise, we give closed form expressions for RSI in terms of solutions to 2x2 matrix Riccati equations, revealing a Pareto frontier between RSI and convergence rate over the choice of step-size and momentum parameters. We then prove a large-deviation principle for time-averaged suboptimality in the large iteration limit and show that the rate function is, up to a scaling, the convex conjugate of the RSI function. We further show that the rate function and RSI are linked to the $H_\infty$-norm--a measure of robustness to the worst-case deterministic gradient errors--so that stronger worst-case robustness (smaller $H_\infty$-norm) leads to sharper decay of the tail probabilities for the average suboptimality. Beyond quadratics, under potentially biased sub-Gaussian gradient errors, we derive non-asymptotic bounds on a finite-time analogue of the RSI, yielding finite-time high-probability guarantees and non-asymptotic large-deviation bounds for the averaged iterates. In the case of smooth strongly convex functions, we also observe an analogous trade-off between RSI and convergence-rate bounds. To our knowledge, these are the first non-asymptotic guarantees for GMMs with biased gradients and the first risk-sensitive analysis of GMMs. Finally, we provide numerical experiments on a robust regression problem to illustrate our results.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13628v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” 1ì°¨ ë°©ë²•ì˜ ë§¥ë½ì—ì„œ ìˆ˜ë ´ ì†ë„ì™€ ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ì— ëŒ€í•œ ê°•ê±´ì„± ê°„ì˜ ìƒì¶© ê´€ê³„ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì´ˆì ì€ ë§¤ë„ëŸ½ê³  ê°•í•˜ê²Œ ë³¼ë¡í•œ ëª©ì ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ ì¼ë°˜í™”ëœ ëª¨ë©˜í…€ ë°©ë²•(GMMs)ì…ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ì—ëŠ” Nesterovì˜ ê°€ì† ê·¸ë˜ë””ì–¸íŠ¸, ë¬´ê±°ìš´ ê³µ, ê·¸ë˜ë””ì–¸íŠ¸ í•˜ê°• ë°©ë²•ì´ í¬í•¨ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì ëŒ€ì ì´ê³  í¸í–¥ëœ í™•ë¥ ì  ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ë¥¼ í—ˆìš©í•˜ë©°, ê°•ê±´ ì œì–´ ì´ë¡ ì˜ ìœ„í—˜ ë¯¼ê° ì§€ìˆ˜(RSI)ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë°©ë²•ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤. ë…ë¦½ ë™ì¼ ë¶„í¬ì˜ ê°€ìš°ì‹œì•ˆ ì¡ìŒì„ ê°€ì§„ ì´ì°¨ ëª©ì ì— ëŒ€í•´, ìš°ë¦¬ëŠ” 2x2 í–‰ë ¬ ë¦¬ì¹´í‹° ë°©ì •ì‹ì˜ í•´ë¥¼ í†µí•´ RSIì— ëŒ€í•œ ë‹«íŒ í˜•ì‹ í‘œí˜„ì„ ì œê³µí•˜ì—¬, ìŠ¤í… í¬ê¸° ë° ëª¨ë©˜í…€ ë§¤ê°œë³€ìˆ˜ ì„ íƒì— ë”°ë¥¸ RSIì™€ ìˆ˜ë ´ ì†ë„ ê°„ì˜ íŒŒë ˆí†  ì „ì„ ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ìš°ë¦¬ëŠ” í° ë°˜ë³µ í•œê³„ì—ì„œ ì‹œê°„ í‰ê·  í•˜ìœ„ ìµœì ì„±ì— ëŒ€í•œ ëŒ€í¸ì°¨ ì›ë¦¬ë¥¼ ì¦ëª…í•˜ê³ , ê·¸ ë¹„ìœ¨ í•¨ìˆ˜ê°€ ìŠ¤ì¼€ì¼ë§ì— ë”°ë¼ RSI í•¨ìˆ˜ì˜ ë³¼ë¡ ì¼¤ë ˆì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ë¹„ìœ¨ í•¨ìˆ˜ì™€ RSIê°€ ìµœì•…ì˜ ê²°ì •ë¡ ì  ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ì— ëŒ€í•œ ê°•ê±´ì„±ì˜ ì²™ë„ì¸ $H_\infty$-ë…¸ë¦„ê³¼ ì—°ê²°ë˜ì–´ ìˆìŒì„ ë³´ì—¬, ë” ê°•í•œ ìµœì•…ì˜ ê°•ê±´ì„±(ë” ì‘ì€ $H_\infty$-ë…¸ë¦„)ì´ í‰ê·  í•˜ìœ„ ìµœì ì„±ì— ëŒ€í•œ ê¼¬ë¦¬ í™•ë¥ ì˜ ë” ë‚ ì¹´ë¡œìš´ ê°ì†Œë¥¼ ì´ˆë˜í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ì°¨ í•¨ìˆ˜ ì™¸ì—ë„, ì ì¬ì ìœ¼ë¡œ í¸í–¥ëœ ì„œë¸Œ ê°€ìš°ì‹œì•ˆ ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ í•˜ì—ì„œ, ìš°ë¦¬ëŠ” RSIì˜ ìœ í•œ ì‹œê°„ ìœ ì‚¬ì²´ì— ëŒ€í•œ ë¹„ëŒ€ì¹­ ê²½ê³„ë¥¼ ë„ì¶œí•˜ì—¬ ìœ í•œ ì‹œê°„ ê³ í™•ë¥  ë³´ì¥ê³¼ í‰ê·  ë°˜ë³µì— ëŒ€í•œ ë¹„ëŒ€ì¹­ ëŒ€í¸ì°¨ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë§¤ë„ëŸ½ê³  ê°•í•˜ê²Œ ë³¼ë¡í•œ í•¨ìˆ˜ì˜ ê²½ìš°, ìš°ë¦¬ëŠ” ë˜í•œ RSIì™€ ìˆ˜ë ´ ì†ë„ ê²½ê³„ ê°„ì˜ ìœ ì‚¬í•œ ìƒì¶© ê´€ê³„ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ì´ê²ƒë“¤ì€ í¸í–¥ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê°€ì§„ GMMsì— ëŒ€í•œ ìµœì´ˆì˜ ë¹„ëŒ€ì¹­ ë³´ì¥ì´ë©°, GMMsì— ëŒ€í•œ ìµœì´ˆì˜ ìœ„í—˜ ë¯¼ê° ë¶„ì„ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•´ ê°•ê±´í•œ íšŒê·€ ë¬¸ì œì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ 1ì°¨ ë°©ë²•ë¡ ì—ì„œ ìˆ˜ë ´ ì†ë„ì™€ ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ì— ëŒ€í•œ ê°•ê±´ì„± ê°„ì˜ ìƒì¶© ê´€ê³„ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ì¼ë°˜í™”ëœ ëª¨ë©˜í…€ ë°©ë²•(GMMs)ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ë„ëŸ½ê³  ê°•í•˜ê²Œ ë³¼ë¡í•œ ëª©ì ì„ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •ì—ì„œ, í¸í–¥ëœ í™•ë¥ ì  ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ê°€ ì¡´ì¬í•  ë•Œì˜ ê°•ê±´ì„±ì„ ìœ„í—˜ ë¯¼ê° ì§€ìˆ˜(RSI)ë¥¼ í†µí•´ ì •ëŸ‰í™”í•©ë‹ˆë‹¤. íŠ¹íˆ, i.i.d. ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì´ì°¨ ëª©ì  í•¨ìˆ˜ì— ëŒ€í•´ RSIì™€ ìˆ˜ë ´ ì†ë„ ê°„ì˜ íŒŒë ˆí†  ê²½ê³„ë¥¼ ì œì‹œí•˜ë©°, ì´ëŠ” ìŠ¤í… í¬ê¸°ì™€ ëª¨ë©˜í…€ ë§¤ê°œë³€ìˆ˜ ì„ íƒì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë˜í•œ, ì‹œê°„ í‰ê·  í•˜ìœ„ ìµœì ì„±ì— ëŒ€í•œ ëŒ€í¸ì°¨ ì›ë¦¬ë¥¼ ì¦ëª…í•˜ê³ , RSI í•¨ìˆ˜ì˜ ë³¼ë¡ ì¼¤ë ˆì™€ ê´€ë ¨ì´ ìˆìŒì„ ë³´ì…ë‹ˆë‹¤. ë” ë‚˜ì•„ê°€, $H_\infty$-ë…¸ë¦„ê³¼ì˜ ê´€ê³„ë¥¼ í†µí•´ ìµœì•…ì˜ ê²½ìš° ê°•ê±´ì„±ì´ ê°•í™”ë ìˆ˜ë¡ í‰ê·  í•˜ìœ„ ìµœì ì„±ì˜ ê¼¬ë¦¬ í™•ë¥ ì´ ë” ê¸‰ê²©íˆ ê°ì†Œí•¨ì„ í™•ì¸í•©ë‹ˆë‹¤. ë¹„ì´ì°¨ì  ê²½ìš°ì—ëŠ” í¸í–¥ëœ ì„œë¸Œ ê°€ìš°ì‹œì•ˆ ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ í•˜ì—ì„œ ìœ í•œ ì‹œê°„ ë‚´ RSIì˜ ë¹„ëŒ€ì¹­ì  ê²½ê³„ë¥¼ ë„ì¶œí•˜ì—¬ GMMsì˜ ë¹„ëŒ€ì¹­ì  ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ê°•ê±´ íšŒê·€ ë¬¸ì œì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ê²°ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¼ë°˜í™”ëœ ëª¨ë©˜í…€ ë°©ë²•(GMMs)ì€ ë§¤ë„ëŸ½ê³  ê°•í•˜ê²Œ ë³¼ë¡í•œ ëª©í‘œë¥¼ ìµœì†Œí™”í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, Nesterovì˜ ê°€ì† ê²½ì‚¬, ë¬´ê±°ìš´ ê³µ, ê²½ì‚¬ í•˜ê°• ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.
- 2. í™•ë¥ ì  ê²½ì‚¬ ì˜¤ë¥˜ê°€ ì ëŒ€ì ì´ê³  í¸í–¥ì ì¼ ìˆ˜ ìˆëŠ” ìƒí™©ì—ì„œ GMMsì˜ ê²½ì‚¬ ì˜¤ë¥˜ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ìœ„í—˜ ë¯¼ê° ì§€ìˆ˜(RSI)ë¥¼ í†µí•´ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.
- 3. ì´ì°¨ ëª©í‘œì™€ ë…ë¦½ ë™ì¼ ë¶„í¬ ê°€ìš°ì‹œì•ˆ ì¡ìŒì˜ ê²½ìš°, RSIì™€ ìˆ˜ë ´ ì†ë„ ê°„ì˜ íŒŒë ˆí†  ì „ì„ ì„ ë“œëŸ¬ë‚´ëŠ” 2x2 í–‰ë ¬ ë¦¬ì¹´í‹° ë°©ì •ì‹ì˜ í•´ë¥¼ í†µí•´ RSIì˜ ë‹«íŒ í˜•íƒœ í‘œí˜„ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. í‰ê·  í•˜ìœ„ ìµœì ì„±ì˜ í° í¸ì°¨ ì›ë¦¬ë¥¼ ì¦ëª…í•˜ê³ , ë¹„ìœ¨ í•¨ìˆ˜ê°€ RSI í•¨ìˆ˜ì˜ ë³¼ë¡ ì¼¤ë ˆì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. GMMsì— ëŒ€í•œ í¸í–¥ëœ ê²½ì‚¬ì™€ ê´€ë ¨ëœ ë¹„ëŒ€ì¹­ì  ë³´ì¦ê³¼ ìœ„í—˜ ë¯¼ê° ë¶„ì„ì„ ìµœì´ˆë¡œ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:25:10*