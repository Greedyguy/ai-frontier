# Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing

**Korean Title:** ìŠ¤í“¨ë¦¬ì–´ìŠ¤ ì‹ í˜¸ë¥¼ ë„˜ì–´: ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ê³¼ ì ì‘í˜• ì „ë¬¸ê°€ ë¼ìš°íŒ…ì„ í†µí•œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í¸í–¥ ì œê±°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Causal Mediation-based Debiasing

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (87.2% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (86.1% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (85.8% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (85.1% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (85.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15361v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities in integrating visual and textual information, yet frequently rely on spurious correlations, undermining their robustness and generalization in complex multimodal reasoning tasks. This paper addresses the critical challenge of superficial correlation bias in MLLMs through a novel causal mediation-based debiasing framework. Specially, we distinguishing core semantics from spurious textual and visual contexts via counterfactual examples to activate training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture with dynamic routing to selectively engages modality-specific debiasing experts. Empirical evaluation on multimodal sarcasm detection and sentiment analysis tasks demonstrates that our framework significantly surpasses unimodal debiasing strategies and existing state-of-the-art models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15361v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Models, MLLMs)ì€ ì‹œê°ì  ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ìˆì–´ ìƒë‹¹í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì¢…ì¢… ì˜ëª»ëœ ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ë³µì¡í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì¶”ë¡  ì‘ì—…ì—ì„œì˜ ê²¬ê³ ì„±ê³¼ ì¼ë°˜í™”ë¥¼ ì €í•´í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ MLLMsì—ì„œì˜ í”¼ìƒì  ìƒê´€ í¸í–¥ ë¬¸ì œë¥¼ ìƒˆë¡œìš´ ì¸ê³¼ ë§¤ê°œ ê¸°ë°˜ í¸í–¥ ì œê±° í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ í•´ê²°í•©ë‹ˆë‹¤. íŠ¹íˆ, ë°˜ì‚¬ì‹¤ì  ì˜ˆì œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸ë¥¼ ì˜ëª»ëœ í…ìŠ¤íŠ¸ ë° ì‹œê°ì  ë§¥ë½ì—ì„œ êµ¬ë³„í•˜ì—¬ í›ˆë ¨ ë‹¨ê³„ì—ì„œì˜ í¸í–¥ ì œê±°ë¥¼ í™œì„±í™”í•˜ê³ , ë™ì  ë¼ìš°íŒ…ì„ ê°–ì¶˜ ì „ë¬¸ê°€ í˜¼í•©(Mixture-of-Experts, MoE) ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ë³„ í¸í–¥ ì œê±° ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ ì°¸ì—¬ì‹œí‚µë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ í’ì ê°ì§€ ë° ê°ì • ë¶„ì„ ì‘ì—…ì— ëŒ€í•œ ì‹¤ì¦ì  í‰ê°€ ê²°ê³¼, ë³¸ í”„ë ˆì„ì›Œí¬ê°€ ë‹¨ì¼ ëª¨ë‹¬ í¸í–¥ ì œê±° ì „ëµ ë° ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ì„ í¬ê²Œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ í”¼ìƒì  ìƒê´€ í¸í–¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì¸ê³¼ ë§¤ê°œ ê¸°ë°˜ ë””ë°”ì´ì‹± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. í•µì‹¬ ê¸°ì—¬ëŠ” ë°˜ì‚¬ì‹¤ì  ì˜ˆì œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸ë¥¼ êµ¬ë³„í•˜ê³ , Mixture-of-Experts(MoE) ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ë³„ ë””ë°”ì´ì‹± ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ í’ì íƒì§€ ë° ê°ì • ë¶„ì„ ì‘ì—…ì—ì„œ ê¸°ì¡´ì˜ ë‹¨ì¼ ëª¨ë‹¬ ë””ë°”ì´ì‹± ì „ëµê³¼ ìµœì‹  ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ, ì¢…ì¢… í”¼ìƒì ì¸ ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ë³µì¡í•œ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ì‘ì—…ì—ì„œì˜ ê²¬ê³ ì„±ê³¼ ì¼ë°˜í™”ê°€ ì €í•˜ë©ë‹ˆë‹¤.

- 2. ë³¸ ë…¼ë¬¸ì€ ì¸ê³¼ ë§¤ê°œ ê¸°ë°˜ì˜ ìƒˆë¡œìš´ ë””ë°”ì´ì‹± í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ MLLMsì˜ í”¼ìƒì  ìƒê´€ê´€ê³„ í¸í–¥ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

- 3. ë°˜ì‚¬ì‹¤ì  ì˜ˆì œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸ë¥¼ í”¼ìƒì ì¸ í…ìŠ¤íŠ¸ ë° ì‹œê°ì  ë§¥ë½ì—ì„œ êµ¬ë¶„í•˜ì—¬ í›ˆë ¨ ë‹¨ê³„ì—ì„œì˜ ë””ë°”ì´ì‹±ì„ í™œì„±í™”í•©ë‹ˆë‹¤.

- 4. Mixture-of-Experts (MoE) ì•„í‚¤í…ì²˜ì™€ ë™ì  ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ë³„ ë””ë°”ì´ì‹± ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ ì°¸ì—¬ì‹œí‚µë‹ˆë‹¤.

- 5. ë‹¤ì¤‘ëª¨ë‹¬ í’ì ê°ì§€ ë° ê°ì • ë¶„ì„ ì‘ì—…ì— ëŒ€í•œ ì‹¤ì¦ì  í‰ê°€ì—ì„œ ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ê°€ ë‹¨ì¼ ëª¨ë‹¬ ë””ë°”ì´ì‹± ì „ëµ ë° ê¸°ì¡´ ìµœì‹  ëª¨ë¸ì„ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 13:53:56*