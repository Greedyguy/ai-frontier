---
keywords:
  - Multimodal Learning
  - Counterfactual Inference
  - Adaptive Expert Routing
  - Mixture-of-Experts
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15361
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:54:23.755837",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Counterfactual Inference",
    "Adaptive Expert Routing",
    "Mixture-of-Experts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Counterfactual Inference": 0.78,
    "Adaptive Expert Routing": 0.77,
    "Mixture-of-Experts": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that enhances connectivity by linking visual and textual modalities.",
        "novelty_score": 0.55,
        "connectivity_score": 0.89,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Counterfactual Inference",
        "canonical": "Counterfactual Inference",
        "aliases": [
          "Counterfactual Analysis"
        ],
        "category": "unique_technical",
        "rationale": "Counterfactual Inference is a unique technique that can link to causal reasoning and debiasing strategies.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "Adaptive Expert Routing",
        "canonical": "Adaptive Expert Routing",
        "aliases": [
          "Dynamic Expert Routing"
        ],
        "category": "unique_technical",
        "rationale": "This concept is specific to the paper's framework and connects to dynamic model architectures.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "Mixture-of-Experts is a well-known model architecture that enhances connectivity with modular learning approaches.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "debiasing",
      "superficial correlation bias",
      "empirical evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.89,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Counterfactual Inference",
      "resolved_canonical": "Counterfactual Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Adaptive Expert Routing",
      "resolved_canonical": "Adaptive Expert Routing",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing

**Korean Title:** ìŠ¤í“¨ë¦¬ì–´ìŠ¤ ì‹ í˜¸ë¥¼ ë„˜ì–´: ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ê³¼ ì ì‘í˜• ì „ë¬¸ê°€ ë¼ìš°íŒ…ì„ í†µí•œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í¸í–¥ ì œê±°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15361.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15361](https://arxiv.org/abs/2509.15361)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (87.2% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.6% similar)
- [[2025-09-22/Pointing to a Llama and Call it a Camel_ On the Sycophancy of Multimodal Large Language Models_20250922|Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models]] (86.4% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (86.1% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (85.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Mixture-of-Experts|Mixture-of-Experts]]
**âš¡ Unique Technical**: [[keywords/Counterfactual Inference|Counterfactual Inference]], [[keywords/Adaptive Expert Routing|Adaptive Expert Routing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15361v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities in integrating visual and textual information, yet frequently rely on spurious correlations, undermining their robustness and generalization in complex multimodal reasoning tasks. This paper addresses the critical challenge of superficial correlation bias in MLLMs through a novel causal mediation-based debiasing framework. Specially, we distinguishing core semantics from spurious textual and visual contexts via counterfactual examples to activate training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture with dynamic routing to selectively engages modality-specific debiasing experts. Empirical evaluation on multimodal sarcasm detection and sentiment analysis tasks demonstrates that our framework significantly surpasses unimodal debiasing strategies and existing state-of-the-art models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15361v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Models, MLLMs)ì€ ì‹œê°ì  ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ìˆì–´ ìƒë‹¹í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë‚˜, ì¢…ì¢… ì˜ëª»ëœ ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ë³µì¡í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì¶”ë¡  ì‘ì—…ì—ì„œì˜ ê²¬ê³ ì„±ê³¼ ì¼ë°˜í™”ë¥¼ ì €í•´í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì°¸ì‹ í•œ ì¸ê³¼ ë§¤ê°œ ê¸°ë°˜ì˜ í¸í–¥ ì œê±° í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ MLLMsì˜ í”¼ìƒì  ìƒê´€ê´€ê³„ í¸í–¥ì´ë¼ëŠ” ì¤‘ìš”í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ë°˜ì‚¬ë¡€(counterfactual) ì˜ˆì œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸ë¥¼ ì˜ëª»ëœ í…ìŠ¤íŠ¸ ë° ì‹œê°ì  ë§¥ë½ì—ì„œ êµ¬ë³„í•˜ì—¬ í›ˆë ¨ ë‹¨ê³„ì˜ í¸í–¥ ì œê±°ë¥¼ í™œì„±í™”í•˜ê³ , ë™ì  ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ëŠ” ì „ë¬¸ê°€ í˜¼í•©(Mixture-of-Experts, MoE) ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ë³„ í¸í–¥ ì œê±° ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ ì°¸ì—¬ì‹œí‚µë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ í’ì ê°ì§€ ë° ê°ì • ë¶„ì„ ì‘ì—…ì— ëŒ€í•œ ì‹¤ì¦ì  í‰ê°€ ê²°ê³¼, ë³¸ í”„ë ˆì„ì›Œí¬ê°€ ë‹¨ì¼ ëª¨ë‹¬ í¸í–¥ ì œê±° ì „ëµ ë° ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ì„ í¬ê²Œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì´ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ìˆì–´ ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ, ì¢…ì¢… í”¼ìƒì ì¸ ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ë³µì¡í•œ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ì‘ì—…ì—ì„œì˜ ê²¬ê³ ì„±ê³¼ ì¼ë°˜í™”ê°€ ì €í•´ëœë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì €ìë“¤ì€ ì¸ê³¼ ë§¤ê°œ ê¸°ë°˜ì˜ ìƒˆë¡œìš´ í¸í–¥ ì œê±° í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë°˜ì‚¬ì‹¤ì  ì˜ˆì œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸ë¥¼ í”¼ìƒì ì¸ í…ìŠ¤íŠ¸ ë° ì‹œê°ì  ë§¥ë½ì—ì„œ êµ¬ë¶„í•˜ê³ , Mixture-of-Experts(MoE) ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ë³„ í¸í–¥ ì œê±° ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ë‹¤ì¤‘ëª¨ë‹¬ í’ì íƒì§€ ë° ê°ì • ë¶„ì„ ì‘ì—…ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ê°€ ê¸°ì¡´ì˜ ë‹¨ì¼ëª¨ë‹¬ í¸í–¥ ì œê±° ì „ëµ ë° ìµœì‹  ëª¨ë¸ì„ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” ë° ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ, ì¢…ì¢… í”¼ìƒì ì¸ ìƒê´€ê´€ê³„ì— ì˜ì¡´í•˜ì—¬ ë³µì¡í•œ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ì‘ì—…ì—ì„œì˜ ê²¬ê³ ì„±ê³¼ ì¼ë°˜í™”ë¥¼ ì €í•´í•œë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì€ MLLMsì˜ í”¼ìƒì  ìƒê´€ê´€ê³„ í¸í–¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì¸ê³¼ ë§¤ê°œ ê¸°ë°˜ì˜ ë””ë°”ì´ì‹± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 3. ë°˜ì‚¬ì‹¤ì  ì˜ˆì œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸ë¥¼ í”¼ìƒì ì¸ í…ìŠ¤íŠ¸ ë° ì‹œê°ì  ë§¥ë½ê³¼ êµ¬ë³„í•˜ì—¬ í›ˆë ¨ ë‹¨ê³„ì—ì„œì˜ ë””ë°”ì´ì‹±ì„ í™œì„±í™”í•œë‹¤.
- 4. Mixture-of-Experts (MoE) ì•„í‚¤í…ì²˜ì™€ ë™ì  ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹°ë³„ ë””ë°”ì´ì‹± ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ ì°¸ì—¬ì‹œí‚¨ë‹¤.
- 5. ë‹¤ì¤‘ëª¨ë‹¬ í’ì íƒì§€ ë° ê°ì • ë¶„ì„ ì‘ì—…ì— ëŒ€í•œ ì‹¤ì¦ì  í‰ê°€ì—ì„œ ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ê°€ ë‹¨ì¼ ëª¨ë‹¬ ë””ë°”ì´ì‹± ì „ëµ ë° ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ì„ í¬ê²Œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-23 08:54:23*