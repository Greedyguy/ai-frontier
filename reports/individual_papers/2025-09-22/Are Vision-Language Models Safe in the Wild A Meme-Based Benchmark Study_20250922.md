---
keywords:
  - Vision-Language Model
  - MemeSafetyBench
  - Large Language Model
  - Safety Taxonomy
  - Multimodal Learning
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2505.15389
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:45:47.374276",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "MemeSafetyBench",
    "Large Language Model",
    "Safety Taxonomy",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "MemeSafetyBench": 0.8,
    "Large Language Model": 0.85,
    "Safety Taxonomy": 0.78,
    "Multimodal Learning": 0.83
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "This term is central to the paper's focus and links to the trending concept of multimodal AI systems.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "MemeSafetyBench",
        "canonical": "MemeSafetyBench",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique benchmark introduced in the paper, crucial for linking specific studies on meme-based evaluations.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Key technology used in the study, facilitating connections to broader AI research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Safety Taxonomy",
        "canonical": "Safety Taxonomy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific framework used for evaluating model safety, important for linking safety-related research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Relates to the integration of vision and language, a key aspect of the models discussed.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.83
      }
    ],
    "ban_list_suggestions": [
      "Rapid deployment",
      "Artificial images",
      "Real-world memes",
      "Harmful outputs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "MemeSafetyBench",
      "resolved_canonical": "MemeSafetyBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Safety Taxonomy",
      "resolved_canonical": "Safety Taxonomy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.83
      }
    }
  ]
}
-->

# Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study

**Korean Title:** 비전-언어 모델은 실제 환경에서 안전한가? 밈 기반 벤치마크 연구

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.15389.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2505.15389](https://arxiv.org/abs/2505.15389)

## 🔗 유사한 논문
- [[2025-09-22/Red Teaming Multimodal Language Models_ Evaluating Harm Across Prompt Modalities and Models_20250922|Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models]] (85.1% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (84.8% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (84.3% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (83.5% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/MemeSafetyBench|MemeSafetyBench]], [[keywords/Safety Taxonomy|Safety Taxonomy]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.15389v2 Announce Type: replace 
Abstract: Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs are more vulnerable to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms. MemeSafetyBench is publicly available at https://github.com/oneonlee/Meme-Safety-Bench.

## 🔍 Abstract (한글 번역)

arXiv:2505.15389v2 발표 유형: 교체  
초록: 비전-언어 모델(VLMs)의 빠른 배치는 안전성 위험을 증대시키지만, 대부분의 평가가 인공 이미지를 기반으로 이루어지고 있습니다. 본 연구는 다음과 같은 질문을 제기합니다: 일반 사용자가 공유하는 밈 이미지에 직면했을 때 현재의 VLMs는 얼마나 안전한가? 이 질문을 조사하기 위해, 우리는 MemeSafetyBench라는 50,430개의 실제 밈 이미지와 해로운 지시 및 무해한 지시를 짝지은 벤치마크를 소개합니다. 포괄적인 안전 분류 체계와 LLM 기반 지시 생성 방식을 사용하여, 단일 및 다중 회차 상호작용에서 여러 VLMs를 평가합니다. 우리는 실제 세계의 밈이 해로운 출력을 어떻게 유도하는지, 대화적 맥락의 완화 효과, 모델 규모와 안전 지표 간의 관계를 조사합니다. 우리의 연구 결과는 VLMs가 합성 또는 타이포그래픽 이미지보다 밈 기반의 해로운 프롬프트에 더 취약하다는 것을 보여줍니다. 밈은 해로운 응답을 크게 증가시키고 텍스트 전용 입력에 비해 거부를 감소시킵니다. 다중 회차 상호작용이 부분적인 완화를 제공하더라도, 높은 취약성은 지속됩니다. 이러한 결과는 생태학적으로 유효한 평가와 강력한 안전 메커니즘의 필요성을 강조합니다. MemeSafetyBench는 https://github.com/oneonlee/Meme-Safety-Bench에서 공개적으로 이용 가능합니다.

## 📝 요약

이 연구는 시각-언어 모델(VLMs)의 안전성을 평가하기 위해 MemeSafetyBench라는 벤치마크를 도입하여 실제 밈 이미지와 유해 및 무해한 지시문을 결합한 50,430개의 사례를 분석합니다. 연구 결과, VLMs는 인공 이미지보다 실제 밈 이미지에 더 취약하며, 밈은 유해한 응답을 증가시키고 거부율을 감소시킵니다. 대화형 맥락이 일부 완화 효과를 제공하지만, 여전히 높은 취약성이 존재합니다. 이 연구는 생태학적으로 유효한 평가와 강력한 안전 메커니즘의 필요성을 강조하며, MemeSafetyBench는 공개적으로 이용 가능합니다.

## 🎯 주요 포인트

- 1. 시각-언어 모델(VLMs)은 밈 이미지에 대해 인공 이미지보다 더 높은 안전성 위험을 보인다.
- 2. MemeSafetyBench는 50,430개의 실제 밈 이미지와 유해 및 무해 지시문을 짝지어 VLMs의 안전성을 평가하는 벤치마크이다.
- 3. 밈 이미지는 텍스트만 있는 입력보다 유해한 반응을 증가시키고 거부를 감소시킨다.
- 4. 다중 턴 상호작용은 유해성 완화에 일부 기여하지만, 여전히 높은 취약성이 남아 있다.
- 5. 연구 결과는 생태학적으로 유효한 평가와 강력한 안전 메커니즘의 필요성을 강조한다.


---

*Generated on 2025-09-23 11:45:47*