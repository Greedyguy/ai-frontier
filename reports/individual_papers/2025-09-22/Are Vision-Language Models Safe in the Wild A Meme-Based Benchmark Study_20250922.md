---
keywords:
  - Vision-Language Model
  - MemeSafetyBench
  - Large Language Model
  - Safety Taxonomy
  - Multimodal Learning
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2505.15389
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:45:47.374276",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "MemeSafetyBench",
    "Large Language Model",
    "Safety Taxonomy",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.92,
    "MemeSafetyBench": 0.8,
    "Large Language Model": 0.85,
    "Safety Taxonomy": 0.78,
    "Multimodal Learning": 0.83
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "This term is central to the paper's focus and links to the trending concept of multimodal AI systems.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.92
      },
      {
        "surface": "MemeSafetyBench",
        "canonical": "MemeSafetyBench",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique benchmark introduced in the paper, crucial for linking specific studies on meme-based evaluations.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Key technology used in the study, facilitating connections to broader AI research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Safety Taxonomy",
        "canonical": "Safety Taxonomy",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific framework used for evaluating model safety, important for linking safety-related research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Relates to the integration of vision and language, a key aspect of the models discussed.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.83
      }
    ],
    "ban_list_suggestions": [
      "Rapid deployment",
      "Artificial images",
      "Real-world memes",
      "Harmful outputs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "MemeSafetyBench",
      "resolved_canonical": "MemeSafetyBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Safety Taxonomy",
      "resolved_canonical": "Safety Taxonomy",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.83
      }
    }
  ]
}
-->

# Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study

**Korean Title:** ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì€ ì‹¤ì œ í™˜ê²½ì—ì„œ ì•ˆì „í•œê°€? ë°ˆ ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.15389.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2505.15389](https://arxiv.org/abs/2505.15389)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Red Teaming Multimodal Language Models_ Evaluating Harm Across Prompt Modalities and Models_20250922|Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models]] (85.1% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (84.8% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (84.3% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (83.5% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/MemeSafetyBench|MemeSafetyBench]], [[keywords/Safety Taxonomy|Safety Taxonomy]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.15389v2 Announce Type: replace 
Abstract: Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs are more vulnerable to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms. MemeSafetyBench is publicly available at https://github.com/oneonlee/Meme-Safety-Bench.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.15389v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ ë¹ ë¥¸ ë°°ì¹˜ëŠ” ì•ˆì „ì„± ìœ„í—˜ì„ ì¦ëŒ€ì‹œí‚¤ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ í‰ê°€ê°€ ì¸ê³µ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ ì œê¸°í•©ë‹ˆë‹¤: ì¼ë°˜ ì‚¬ìš©ìê°€ ê³µìœ í•˜ëŠ” ë°ˆ ì´ë¯¸ì§€ì— ì§ë©´í–ˆì„ ë•Œ í˜„ì¬ì˜ VLMsëŠ” ì–¼ë§ˆë‚˜ ì•ˆì „í•œê°€? ì´ ì§ˆë¬¸ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” MemeSafetyBenchë¼ëŠ” 50,430ê°œì˜ ì‹¤ì œ ë°ˆ ì´ë¯¸ì§€ì™€ í•´ë¡œìš´ ì§€ì‹œ ë° ë¬´í•´í•œ ì§€ì‹œë¥¼ ì§ì§€ì€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. í¬ê´„ì ì¸ ì•ˆì „ ë¶„ë¥˜ ì²´ê³„ì™€ LLM ê¸°ë°˜ ì§€ì‹œ ìƒì„± ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬, ë‹¨ì¼ ë° ë‹¤ì¤‘ íšŒì°¨ ìƒí˜¸ì‘ìš©ì—ì„œ ì—¬ëŸ¬ VLMsë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‹¤ì œ ì„¸ê³„ì˜ ë°ˆì´ í•´ë¡œìš´ ì¶œë ¥ì„ ì–´ë–»ê²Œ ìœ ë„í•˜ëŠ”ì§€, ëŒ€í™”ì  ë§¥ë½ì˜ ì™„í™” íš¨ê³¼, ëª¨ë¸ ê·œëª¨ì™€ ì•ˆì „ ì§€í‘œ ê°„ì˜ ê´€ê³„ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” VLMsê°€ í•©ì„± ë˜ëŠ” íƒ€ì´í¬ê·¸ë˜í”½ ì´ë¯¸ì§€ë³´ë‹¤ ë°ˆ ê¸°ë°˜ì˜ í•´ë¡œìš´ í”„ë¡¬í”„íŠ¸ì— ë” ì·¨ì•½í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë°ˆì€ í•´ë¡œìš´ ì‘ë‹µì„ í¬ê²Œ ì¦ê°€ì‹œí‚¤ê³  í…ìŠ¤íŠ¸ ì „ìš© ì…ë ¥ì— ë¹„í•´ ê±°ë¶€ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ë‹¤ì¤‘ íšŒì°¨ ìƒí˜¸ì‘ìš©ì´ ë¶€ë¶„ì ì¸ ì™„í™”ë¥¼ ì œê³µí•˜ë”ë¼ë„, ë†’ì€ ì·¨ì•½ì„±ì€ ì§€ì†ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ìƒíƒœí•™ì ìœ¼ë¡œ ìœ íš¨í•œ í‰ê°€ì™€ ê°•ë ¥í•œ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. MemeSafetyBenchëŠ” https://github.com/oneonlee/Meme-Safety-Benchì—ì„œ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ ì•ˆì „ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ MemeSafetyBenchë¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í•˜ì—¬ ì‹¤ì œ ë°ˆ ì´ë¯¸ì§€ì™€ ìœ í•´ ë° ë¬´í•´í•œ ì§€ì‹œë¬¸ì„ ê²°í•©í•œ 50,430ê°œì˜ ì‚¬ë¡€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, VLMsëŠ” ì¸ê³µ ì´ë¯¸ì§€ë³´ë‹¤ ì‹¤ì œ ë°ˆ ì´ë¯¸ì§€ì— ë” ì·¨ì•½í•˜ë©°, ë°ˆì€ ìœ í•´í•œ ì‘ë‹µì„ ì¦ê°€ì‹œí‚¤ê³  ê±°ë¶€ìœ¨ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ëŒ€í™”í˜• ë§¥ë½ì´ ì¼ë¶€ ì™„í™” íš¨ê³¼ë¥¼ ì œê³µí•˜ì§€ë§Œ, ì—¬ì „íˆ ë†’ì€ ì·¨ì•½ì„±ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ìƒíƒœí•™ì ìœ¼ë¡œ ìœ íš¨í•œ í‰ê°€ì™€ ê°•ë ¥í•œ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, MemeSafetyBenchëŠ” ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ë°ˆ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¸ê³µ ì´ë¯¸ì§€ë³´ë‹¤ ë” ë†’ì€ ì•ˆì „ì„± ìœ„í—˜ì„ ë³´ì¸ë‹¤.
- 2. MemeSafetyBenchëŠ” 50,430ê°œì˜ ì‹¤ì œ ë°ˆ ì´ë¯¸ì§€ì™€ ìœ í•´ ë° ë¬´í•´ ì§€ì‹œë¬¸ì„ ì§ì§€ì–´ VLMsì˜ ì•ˆì „ì„±ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ì´ë‹¤.
- 3. ë°ˆ ì´ë¯¸ì§€ëŠ” í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ì…ë ¥ë³´ë‹¤ ìœ í•´í•œ ë°˜ì‘ì„ ì¦ê°€ì‹œí‚¤ê³  ê±°ë¶€ë¥¼ ê°ì†Œì‹œí‚¨ë‹¤.
- 4. ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš©ì€ ìœ í•´ì„± ì™„í™”ì— ì¼ë¶€ ê¸°ì—¬í•˜ì§€ë§Œ, ì—¬ì „íˆ ë†’ì€ ì·¨ì•½ì„±ì´ ë‚¨ì•„ ìˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ìƒíƒœí•™ì ìœ¼ë¡œ ìœ íš¨í•œ í‰ê°€ì™€ ê°•ë ¥í•œ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-23 11:45:47*