---
keywords:
  - Speculative Decoding
  - Attention Mechanism
  - Transformer
  - Two-Stage Block-Attention Training
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.24544
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:01:12.106686",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Speculative Decoding",
    "Attention Mechanism",
    "Transformer",
    "Two-Stage Block-Attention Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Speculative Decoding": 0.78,
    "Attention Mechanism": 0.82,
    "Transformer": 0.79,
    "Two-Stage Block-Attention Training": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [
          "SD"
        ],
        "category": "unique_technical",
        "rationale": "Speculative Decoding is a distinctive method discussed in the paper, offering a unique approach to inference acceleration in LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cross-Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-Attention is a variant of the Attention Mechanism, crucial for understanding the architecture described in the paper.",
        "novelty_score": 0.58,
        "connectivity_score": 0.89,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Transformer Decoder",
        "canonical": "Transformer",
        "aliases": [
          "Decoder"
        ],
        "category": "broad_technical",
        "rationale": "The Transformer Decoder is a fundamental component of the architecture, linking to broader Transformer-based discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.92,
        "specificity_score": 0.65,
        "link_intent_score": 0.79
      },
      {
        "surface": "Two-Stage Block-Attention Training",
        "canonical": "Two-Stage Block-Attention Training",
        "aliases": [
          "Block-Attention Training"
        ],
        "category": "unique_technical",
        "rationale": "This new training method is specific to the paper and critical for achieving the reported results.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cross-Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.89,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Transformer Decoder",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.92,
        "specificity": 0.65,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Two-Stage Block-Attention Training",
      "resolved_canonical": "Two-Stage Block-Attention Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Cross-Attention Speculative Decoding

**Korean Title:** êµì°¨ ì£¼ì˜ ì¶”ë¡  ë””ì½”ë”©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.24544.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.24544](https://arxiv.org/abs/2505.24544)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (83.2% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (81.7% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (79.7% similar)
- [[2025-09-22/Causal2Vec_ Improving Decoder-only LLMs as Versatile Embedding Models_20250922|Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models]] (79.4% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop: A Novel Regularization Method for Transformer Models]] (79.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Two-Stage Block-Attention Training|Two-Stage Block-Attention Training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.24544v2 Announce Type: replace-cross 
Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.24544v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ì¶”ì¸¡ì  ë””ì½”ë”©(SD)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ ë„ë¦¬ ì±„íƒëœ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ, íŠ¹íˆ ì´ˆì•ˆ ëª¨ë¸ê³¼ ëŒ€ìƒ ëª¨ë¸ì´ ì˜ ì •ë ¬ë˜ì–´ ìˆì„ ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœì²¨ë‹¨ SD ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ ë³´ì¡° í’€ë§ ë˜ëŠ” ìœµí•© ë ˆì´ì–´ë¡œ ë³´ê°•ëœ, ë°€ì ‘í•˜ê²Œ ê²°í•©ëœ ìê¸°-ì–´í…ì…˜ ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°í•©ì€ ëª¨ë¸ ê°„ì˜ ì¼ë°˜í™”ë¥¼ ì ì  ë” ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìµœì´ˆë¡œ, ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, êµì°¨-ì–´í…ì…˜ ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” SD ëª¨ë¸ì¸ Budget EAGLE (Beagle)ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ í’€ë§ì´ë‚˜ ë³´ì¡° êµ¬ì„± ìš”ì†Œì˜ í•„ìš”ì„±ì„ ì œê±°í•˜ë©´ì„œ, ì•„í‚¤í…ì²˜ë¥¼ ë‹¨ìˆœí™”í•˜ê³ , í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ë©°, í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ë™ì•ˆ ì•ˆì •ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ ì§€í•˜ë©´ì„œ, ì„ ë„ì ì¸ ìê¸°-ì–´í…ì…˜ SD ëª¨ë¸(EAGLE-v2)ê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì˜ íš¨ê³¼ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¸”ë¡ ìˆ˜ì¤€ì˜ ì–´í…ì…˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì¸ ì´ë‹¨ê³„ ë¸”ë¡-ì–´í…ì…˜ í›ˆë ¨ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—¬ëŸ¬ LLMê³¼ ë°ì´í„°ì…‹ì— ê±¸ì¹œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ Beagleì´ EAGLE-v2ë³´ë‹¤ ê²½ìŸë ¥ ìˆëŠ” ì¶”ë¡  ì†ë„ í–¥ìƒê³¼ ë” ë†’ì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ì—¬, ì¶”ì¸¡ì  ë””ì½”ë”© ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ê°•ë ¥í•œ ëŒ€ì•ˆì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì¶”ë¡  ë°©ë²•ì¸ Budget EAGLE(Beagle)ì„ ì œì•ˆí•©ë‹ˆë‹¤. Beagleì€ ìµœì´ˆë¡œ êµì°¨ ì£¼ì˜ ê¸°ë°˜ì˜ Transformer ë””ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬, ê¸°ì¡´ì˜ ìê¸° ì£¼ì˜ ê¸°ë°˜ ëª¨ë¸(EAGLE-v2)ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ë³´ì¡° êµ¬ì„± ìš”ì†Œ ì—†ì´ ê°„ë‹¨í•œ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ í›ˆë ¨ ë°©ë²•ì¸ Two-Stage Block-Attention Trainingì„ ë„ì…í•˜ì—¬ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ LLMê³¼ ë°ì´í„°ì…‹ì„ í†µí•œ ì‹¤í—˜ ê²°ê³¼, Beagleì€ EAGLE-v2ë³´ë‹¤ ê²½ìŸë ¥ ìˆëŠ” ì¶”ë¡  ì†ë„ì™€ ë†’ì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë³´ì—¬ì£¼ë©°, ì¶”ë¡  ê°€ì†í™”ì— ìˆì–´ ê°•ë ¥í•œ ëŒ€ì•ˆì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Budget EAGLE (Beagle)ëŠ” ìµœì´ˆì˜ êµì°¨ ì£¼ì˜ ê¸°ë°˜ Transformer ë””ì½”ë” SD ëª¨ë¸ë¡œ, ê¸°ì¡´ì˜ ìê¸° ì£¼ì˜ SD ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ë³´ì¡° êµ¬ì„± ìš”ì†Œ ì—†ì´ ì•„í‚¤í…ì²˜ë¥¼ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.
- 2. Beagleì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê³  í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ë™ì•ˆ ì•ˆì •ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 3. ìƒˆë¡œìš´ Two-Stage Block-Attention Training ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë¸”ë¡ ìˆ˜ì¤€ ì£¼ì˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í›ˆë ¨ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ LLMê³¼ ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ Beagleì€ EAGLE-v2ë³´ë‹¤ ê²½ìŸë ¥ ìˆëŠ” ì¶”ë¡  ì†ë„ í–¥ìƒê³¼ ë” ë†’ì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:01:12*