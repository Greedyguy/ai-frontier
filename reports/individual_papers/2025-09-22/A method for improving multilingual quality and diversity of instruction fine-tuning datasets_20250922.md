---
keywords:
  - Multilingual Instruction Fine-Tuning
  - Large Language Model
  - Multilingual Data Quality and Diversity
  - Superficial Alignment Hypothesis
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15549
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:30:03.418124",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multilingual Instruction Fine-Tuning",
    "Large Language Model",
    "Multilingual Data Quality and Diversity",
    "Superficial Alignment Hypothesis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multilingual Instruction Fine-Tuning": 0.78,
    "Large Language Model": 0.82,
    "Multilingual Data Quality and Diversity": 0.79,
    "Superficial Alignment Hypothesis": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multilingual Instruction Fine-Tuning",
        "canonical": "Multilingual Instruction Fine-Tuning",
        "aliases": [
          "Multilingual IFT"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's focus on improving multilingual capabilities of LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are fundamental to the study and are a key area of research in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multilingual Data Quality and Diversity",
        "canonical": "Multilingual Data Quality and Diversity",
        "aliases": [
          "M-DaQ"
        ],
        "category": "unique_technical",
        "rationale": "Introduced as a novel method in the paper, it is crucial for improving multilinguality in LLMs.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.79
      },
      {
        "surface": "Superficial Alignment Hypothesis",
        "canonical": "Superficial Alignment Hypothesis",
        "aliases": [
          "SAH"
        ],
        "category": "unique_technical",
        "rationale": "The paper conducts a systematic investigation of this hypothesis in a multilingual setting.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multilingual Instruction Fine-Tuning",
      "resolved_canonical": "Multilingual Instruction Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multilingual Data Quality and Diversity",
      "resolved_canonical": "Multilingual Data Quality and Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Superficial Alignment Hypothesis",
      "resolved_canonical": "Superficial Alignment Hypothesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# A method for improving multilingual quality and diversity of instruction fine-tuning datasets

**Korean Title:** ë‹¤êµ­ì–´ í’ˆì§ˆ ë° ë‹¤ì–‘ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ êµìœ¡ ë¯¸ì„¸ ì¡°ì • ë°ì´í„°ì…‹ì˜ ë°©ë²•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15549.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15549](https://arxiv.org/abs/2509.15549)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (88.5% similar)
- [[2025-09-22/The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation_20250922|The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation]] (86.3% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.9% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (84.9% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Multilingual Instruction Fine-Tuning|Multilingual Instruction Fine-Tuning]], [[keywords/Multilingual Data Quality and Diversity|Multilingual Data Quality and Diversity]], [[keywords/Superficial Alignment Hypothesis|Superficial Alignment Hypothesis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15549v1 Announce Type: new 
Abstract: Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large language models (LLMs) to generalize effectively across diverse linguistic and cultural contexts. However, the scarcity of high-quality multilingual training data and corresponding building method remains a critical bottleneck. While data selection has shown promise in English settings, existing methods often fail to generalize across languages due to reliance on simplistic heuristics or language-specific assumptions. In this work, we introduce Multilingual Data Quality and Diversity (M-DaQ), a novel method for improving LLMs multilinguality, by selecting high-quality and semantically diverse multilingual IFT samples. We further conduct the first systematic investigation of the Superficial Alignment Hypothesis (SAH) in multilingual setting. Empirical results across 18 languages demonstrate that models fine-tuned with M-DaQ method achieve significant performance gains over vanilla baselines over 60% win rate. Human evaluations further validate these gains, highlighting the increment of cultural points in the response. We release the M-DaQ code to support future research.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15549v1 ë°œí‘œ ìœ í˜•: ìƒˆë¡œìš´ ê²ƒ  
ì´ˆë¡: ë‹¤êµ­ì–´ ì§€ì‹œ ë¯¸ì„¸ ì¡°ì •(IFT)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì–‘í•œ ì–¸ì–´ ë° ë¬¸í™”ì  ë§¥ë½ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³ í’ˆì§ˆì˜ ë‹¤êµ­ì–´ í›ˆë ¨ ë°ì´í„°ì™€ ì´ì— ìƒì‘í•˜ëŠ” êµ¬ì¶• ë°©ë²•ì˜ ë¶€ì¡±ì€ ì—¬ì „íˆ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒìœ¼ë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ì„ íƒì´ ì˜ì–´ í™˜ê²½ì—ì„œ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ì€ ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±ì´ë‚˜ ì–¸ì–´ë³„ ê°€ì •ì— ì˜ì¡´í•˜ì—¬ ì–¸ì–´ ê°„ ì¼ë°˜í™”ì— ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ í’ˆì§ˆ ë° ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ IFT ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ LLMì˜ ë‹¤êµ­ì–´ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì¸ ë‹¤êµ­ì–´ ë°ì´í„° í’ˆì§ˆ ë° ë‹¤ì–‘ì„±(M-DaQ)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë˜í•œ ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ í‘œë©´ì  ì •ë ¬ ê°€ì„¤(SAH)ì— ëŒ€í•œ ìµœì´ˆì˜ ì²´ê³„ì ì¸ ì¡°ì‚¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. 18ê°œ ì–¸ì–´ì— ê±¸ì¹œ ì‹¤ì¦ì  ê²°ê³¼ëŠ” M-DaQ ë°©ë²•ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì´ ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ì— ë¹„í•´ 60% ì´ìƒì˜ ìŠ¹ë¥ ë¡œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¸ê°„ í‰ê°€ ë˜í•œ ì´ëŸ¬í•œ ì„±ê³¼ë¥¼ ê²€ì¦í•˜ë©°, ì‘ë‹µì—ì„œ ë¬¸í™”ì  ìš”ì†Œì˜ ì¦ê°€ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ M-DaQ ì½”ë“œë¥¼ ê³µê°œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‹¤êµ­ì–´ ì¼ë°˜í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ M-DaQ(Multilingual Data Quality and Diversity)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±ì´ë‚˜ ì–¸ì–´ íŠ¹ìˆ˜ì  ê°€ì •ì— ì˜ì¡´í•˜ëŠ” ë°©ë²•ë“¤ì´ ë‹¤êµ­ì–´ì— ì¼ë°˜í™”ë˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, M-DaQëŠ” ê³ í’ˆì§ˆì˜ ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ í•™ìŠµ ë°ì´í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. 18ê°œ ì–¸ì–´ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, M-DaQë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì´ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ 60% ì´ìƒì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, ì¸ê°„ í‰ê°€ì—ì„œë„ ë¬¸í™”ì  ìš”ì†Œê°€ ì¦ê°€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì—°êµ¬ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ M-DaQ ì½”ë“œë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤êµ­ì–´ êµìœ¡ ë¯¸ì„¸ ì¡°ì •(IFT)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì–‘í•œ ì–¸ì–´ ë° ë¬¸í™”ì  ë§¥ë½ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì¼ë°˜í™”í•˜ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.
- 2. ê³ í’ˆì§ˆ ë‹¤êµ­ì–´ í›ˆë ¨ ë°ì´í„°ì˜ ë¶€ì¡±ì€ ì—¬ì „íˆ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ í’ˆì§ˆ ë° ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ IFT ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ LLMì˜ ë‹¤êµ­ì–´ì„±ì„ ê°œì„ í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì¸ M-DaQë¥¼ ì†Œê°œí•œë‹¤.
- 4. M-DaQ ë°©ë²•ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì€ 18ê°œ ì–¸ì–´ì—ì„œ 60% ì´ìƒì˜ ìŠ¹ë¥ ë¡œ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆë‹¤.
- 5. M-DaQ ì½”ë“œë¥¼ ê³µê°œí•˜ì—¬ í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•œë‹¤.


---

*Generated on 2025-09-23 11:30:03*