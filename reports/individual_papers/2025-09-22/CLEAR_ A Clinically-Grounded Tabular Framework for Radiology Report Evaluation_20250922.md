---
keywords:
  - Radiology Report Evaluation
  - Clinical Attributes
  - MIMIC-CXR Dataset
  - CheXpert Conditions
  - Attribute-level Comparison
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.16325
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:58:24.262505",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Radiology Report Evaluation",
    "Clinical Attributes",
    "MIMIC-CXR Dataset",
    "CheXpert Conditions",
    "Attribute-level Comparison"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Radiology Report Evaluation": 0.8,
    "Clinical Attributes": 0.78,
    "MIMIC-CXR Dataset": 0.82,
    "CheXpert Conditions": 0.79,
    "Attribute-level Comparison": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Radiology Report Evaluation",
        "canonical": "Radiology Report Evaluation",
        "aliases": [
          "Radiology Evaluation",
          "Report Evaluation"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's focus and provides a unique technical perspective on evaluating radiology reports.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Clinical Attributes",
        "canonical": "Clinical Attributes",
        "aliases": [
          "Medical Attributes",
          "Health Attributes"
        ],
        "category": "unique_technical",
        "rationale": "The concept of clinical attributes is crucial for understanding the paper's evaluation framework and its application in medical contexts.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "MIMIC-CXR",
        "canonical": "MIMIC-CXR Dataset",
        "aliases": [
          "MIMIC-CXR",
          "Chest X-ray Dataset"
        ],
        "category": "specific_connectable",
        "rationale": "MIMIC-CXR is a well-known dataset in the medical imaging community, providing a strong point of connection for related research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "CheXpert Conditions",
        "canonical": "CheXpert Conditions",
        "aliases": [
          "CheXpert",
          "Chest Conditions"
        ],
        "category": "specific_connectable",
        "rationale": "CheXpert Conditions are widely recognized in the field of radiology, facilitating connections with other works using this standard.",
        "novelty_score": 0.6,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "Attribute-level Comparison",
        "canonical": "Attribute-level Comparison",
        "aliases": [
          "Attribute Comparison",
          "Detailed Comparison"
        ],
        "category": "unique_technical",
        "rationale": "This concept is key to the paper's methodology, offering a detailed approach to evaluating radiology reports that can link to similar analytical frameworks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "evaluation",
      "report",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Radiology Report Evaluation",
      "resolved_canonical": "Radiology Report Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Clinical Attributes",
      "resolved_canonical": "Clinical Attributes",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "MIMIC-CXR",
      "resolved_canonical": "MIMIC-CXR Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "CheXpert Conditions",
      "resolved_canonical": "CheXpert Conditions",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Attribute-level Comparison",
      "resolved_canonical": "Attribute-level Comparison",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation

**Korean Title:** CLEAR: 방사선 보고서 평가를 위한 임상 기반 표 형식 프레임워크

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.16325.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.16325](https://arxiv.org/abs/2505.16325)

## 🔗 유사한 논문
- [[2025-09-18/Limitations of Public Chest Radiography Datasets for Artificial Intelligence_ Label Quality, Domain Shift, Bias and Evaluation Challenges_20250918|Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges]] (82.7% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (81.2% similar)
- [[2025-09-22/DPC-QA Net_ A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images_20250922|DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images]] (79.5% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (79.1% similar)
- [[2025-09-19/Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model_20250919|Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model]] (79.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/MIMIC-CXR Dataset|MIMIC-CXR Dataset]], [[keywords/CheXpert Conditions|CheXpert Conditions]]
**⚡ Unique Technical**: [[keywords/Radiology Report Evaluation|Radiology Report Evaluation]], [[keywords/Clinical Attributes|Clinical Attributes]], [[keywords/Attribute-level Comparison|Attribute-level Comparison]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.16325v2 Announce Type: replace-cross 
Abstract: Existing metrics often lack the granularity and interpretability to capture nuanced clinical differences between candidate and ground-truth radiology reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded tabular framework with Expert-curated labels and Attribute-level comparison for Radiology report evaluation (CLEAR). CLEAR not only examines whether a report can accurately identify the presence or absence of medical conditions, but also assesses whether it can precisely describe each positively identified condition across five key attributes: first occurrence, change, severity, descriptive location, and recommendation. Compared to prior works, CLEAR's multi-dimensional, attribute-level outputs enable a more comprehensive and clinically interpretable evaluation of report quality. Additionally, to measure the clinical alignment of CLEAR, we collaborate with five board-certified radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions. Our experiments show that CLEAR achieves high accuracy in extracting clinical attributes and provides automated metrics that are strongly aligned with clinical judgment.

## 🔍 Abstract (한글 번역)

arXiv:2505.16325v2 발표 유형: 교차 교체  
초록: 기존의 평가 지표는 후보와 실제 방사선 보고서 간의 미묘한 임상적 차이를 포착하기 위한 세분성과 해석 가능성이 부족하여 최적의 평가를 이루지 못하는 경우가 많습니다. 우리는 방사선 보고서 평가를 위한 전문가가 큐레이션한 레이블과 속성 수준 비교를 포함한 임상 기반의 표 형식 프레임워크(CLEAR)를 소개합니다. CLEAR는 보고서가 의학적 상태의 존재 여부를 정확히 식별할 수 있는지 여부를 조사할 뿐만 아니라, 긍정적으로 식별된 각 상태를 첫 발생, 변화, 심각도, 설명적 위치, 권고사항이라는 다섯 가지 주요 속성에 걸쳐 정확하게 설명할 수 있는지도 평가합니다. 이전 연구와 비교하여, CLEAR의 다차원 속성 수준 출력은 보고서 품질에 대한 보다 포괄적이고 임상적으로 해석 가능한 평가를 가능하게 합니다. 또한, CLEAR의 임상적 정렬을 측정하기 위해, 우리는 MIMIC-CXR의 100개의 흉부 X선 보고서를 6개의 큐레이션된 속성과 13개의 CheXpert 조건에 따라 주석을 달아 개발한 CLEAR-Bench라는 데이터셋을 만들기 위해 5명의 보드 인증 방사선 전문의와 협력합니다. 우리의 실험은 CLEAR가 임상 속성을 추출하는 데 높은 정확성을 달성하고 임상 판단과 강하게 일치하는 자동화된 지표를 제공함을 보여줍니다.

## 📝 요약

기존의 평가 지표는 후보와 실제 방사선 보고서 간의 미묘한 임상적 차이를 포착하는 데 한계가 있습니다. 이를 해결하기 위해, 우리는 방사선 보고서 평가를 위한 임상 기반의 표 형식 프레임워크인 CLEAR를 제안합니다. CLEAR는 의료 상태의 존재 여부뿐만 아니라, 첫 발생, 변화, 심각도, 설명적 위치, 권고 사항 등 다섯 가지 핵심 속성에 대해 정확하게 기술할 수 있는지를 평가합니다. 이전 연구와 비교하여, CLEAR는 다차원적이고 속성 수준의 출력을 통해 보고서 품질을 더 포괄적이고 임상적으로 해석 가능하게 평가합니다. 또한, CLEAR의 임상적 정렬성을 측정하기 위해, 5명의 보드 인증 방사선 전문의와 협력하여 100개의 MIMIC-CXR 흉부 X선 보고서로 구성된 CLEAR-Bench 데이터셋을 개발했습니다. 실험 결과, CLEAR는 임상 속성 추출에서 높은 정확도를 보였으며, 임상 판단과 강하게 일치하는 자동화된 지표를 제공합니다.

## 🎯 주요 포인트

- 1. 기존의 평가 지표는 후보와 실제 방사선 보고서 간의 미세한 임상적 차이를 포착하는 데 한계가 있다.
- 2. CLEAR는 방사선 보고서 평가를 위해 임상적으로 기반을 둔 표 형식의 프레임워크로, 전문가가 선정한 라벨과 속성 수준의 비교를 제공한다.
- 3. CLEAR는 의료 상태의 존재 여부뿐만 아니라, 긍정적으로 식별된 각 상태를 다섯 가지 주요 속성(최초 발생, 변화, 심각도, 설명적 위치, 권장 사항)으로 정확히 설명할 수 있는지를 평가한다.
- 4. CLEAR는 다차원적이고 속성 수준의 출력을 통해 보고서 품질에 대한 보다 포괄적이고 임상적으로 해석 가능한 평가를 가능하게 한다.
- 5. CLEAR-Bench 데이터셋을 통해 CLEAR는 임상 속성을 높은 정확도로 추출하고, 임상 판단과 강하게 일치하는 자동화된 지표를 제공한다.


---

*Generated on 2025-09-23 09:58:24*