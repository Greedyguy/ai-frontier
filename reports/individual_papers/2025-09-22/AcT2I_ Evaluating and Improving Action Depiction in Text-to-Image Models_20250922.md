---
keywords:
  - Text-to-Image Models
  - Large Language Model
  - Action Depiction
  - Knowledge Distillation
  - Temporal Details
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.16141
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:22:13.008154",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text-to-Image Models",
    "Large Language Model",
    "Action Depiction",
    "Knowledge Distillation",
    "Temporal Details"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text-to-Image Models": 0.8,
    "Large Language Model": 0.7,
    "Action Depiction": 0.75,
    "Knowledge Distillation": 0.78,
    "Temporal Details": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Text-to-Image models",
        "canonical": "Text-to-Image Models",
        "aliases": [
          "T2I models"
        ],
        "category": "unique_technical",
        "rationale": "Text-to-Image models are central to the paper's focus on action depiction and image generation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are used for knowledge distillation, enhancing the connectivity with existing NLP frameworks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "action depiction",
        "canonical": "Action Depiction",
        "aliases": [
          "depiction of actions"
        ],
        "category": "unique_technical",
        "rationale": "This concept is crucial for understanding the challenges in accurately rendering complex scenes in T2I models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "knowledge distillation",
        "canonical": "Knowledge Distillation",
        "aliases": [
          "distillation of knowledge"
        ],
        "category": "specific_connectable",
        "rationale": "Knowledge distillation is employed to improve T2I models, linking it to broader machine learning techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "temporal details",
        "canonical": "Temporal Details",
        "aliases": [
          "time-based details"
        ],
        "category": "unique_technical",
        "rationale": "Incorporating temporal details is highlighted as a significant improvement for image generation accuracy.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Text-to-Image models",
      "resolved_canonical": "Text-to-Image Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "action depiction",
      "resolved_canonical": "Action Depiction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "knowledge distillation",
      "resolved_canonical": "Knowledge Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "temporal details",
      "resolved_canonical": "Temporal Details",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models

**Korean Title:** AcT2I: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ëª¨ë¸ì—ì„œ í–‰ë™ ë¬˜ì‚¬ë¥¼ í‰ê°€í•˜ê³  ê°œì„ í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16141.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.16141](https://arxiv.org/abs/2509.16141)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (87.2% similar)
- [[2025-09-22/Structured Information for Improving Spatial Relationships in Text-to-Image Generation_20250922|Structured Information for Improving Spatial Relationships in Text-to-Image Generation]] (85.9% similar)
- [[2025-09-22/CIDER_ A Causal Cure for Brand-Obsessed Text-to-Image Models_20250922|CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models]] (84.6% similar)
- [[2025-09-19/TIDE_ Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement_20250919|TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement]] (82.9% similar)
- [[2025-09-18/BiasMap_ Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation_20250918|BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Distillation|Knowledge Distillation]]
**âš¡ Unique Technical**: [[keywords/Text-to-Image Models|Text-to-Image Models]], [[keywords/Action Depiction|Action Depiction]], [[keywords/Temporal Details|Temporal Details]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16141v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16141v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€(T2I) ëª¨ë¸ì€ ìµœê·¼ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ìˆì–´ ë†€ë¼ìš´ ì„±ê³µì„ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, í–‰ë™ê³¼ ìƒí˜¸ì‘ìš©ì´ ì£¼ìš” ì˜ë¯¸ì  ì´ˆì ì„ ì´ë£¨ëŠ” ë³µì¡í•œ ì¥ë©´ì„ ì •í™•í•˜ê²Œ ë Œë”ë§í•˜ëŠ” ë°ì—ëŠ” ì—¬ì „íˆ ë„ì „ ê³¼ì œê°€ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œì˜ ì£¼ìš” ê´€ì°°ì€ T2I ëª¨ë¸ì´ í–‰ë™ ë¬˜ì‚¬ì— ë‚´ì¬ëœ ë¯¸ë¬˜í•˜ê³  ì¢…ì¢… ì•”ë¬µì ì¸ ì†ì„±ì„ í¬ì°©í•˜ëŠ” ë° ìì£¼ ì–´ë ¤ì›€ì„ ê²ªì–´, ì£¼ìš” ë§¥ë½ì  ì„¸ë¶€ì‚¬í•­ì´ ê²°ì—¬ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê²Œ ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì²´ê³„ì ì¸ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í–‰ë™ ì¤‘ì‹¬ì˜ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” T2I ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì¸ AcT2Ië¥¼ ë„ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì£¼ìš” T2I ëª¨ë¸ë“¤ì´ AcT2Iì—ì„œ ì¢‹ì€ ì„±ê³¼ë¥¼ ë‚´ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ê²€ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë‹¨ì ì´ ê¸°ì¡´ T2I ëª¨ë¸ì˜ í›ˆë ¨ ì½”í¼ìŠ¤ì—ì„œ ë‚´ì¬ëœ ì†ì„±ê³¼ ë§¥ë½ì  ì˜ì¡´ì„±ì˜ ë¶ˆì™„ì „í•œ í‘œí˜„ì—ì„œ ê¸°ì¸í•œë‹¤ê³  ê°€ì„¤ì„ ì„¸ì›ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš°ë¦¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í›ˆë ¨ ì—†ëŠ” ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ ê°œë°œí•˜ì—¬ ì´ í•œê³„ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì„¸ ê°€ì§€ ì°¨ì›ì— ê±¸ì³ ë°€ì§‘ëœ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ê°•í™”í•˜ë©°, ì‹œê°„ì  ì„¸ë¶€ì‚¬í•­ì„ ì£¼ì…í•¨ìœ¼ë¡œì¨ ì´ë¯¸ì§€ ìƒì„± ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë¨ì„ ê´€ì°°í•˜ì˜€ê³ , ìš°ë¦¬ì˜ ìµœì  ëª¨ë¸ì€ 72%ì˜ ì¦ê°€ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ìˆì–´ í˜„ì¬ T2I ë°©ë²•ì˜ í•œê³„ë¥¼ ê°•ì¡°í•˜ë©°, ì²´ê³„ì ì¸ ë°©ì‹ìœ¼ë¡œ ì–¸ì–´ì  ì§€ì‹ì„ í†µí•©í•˜ëŠ” ê²ƒì´ ë¯¸ë¬˜í•˜ê³  ë§¥ë½ì ìœ¼ë¡œ ì •í™•í•œ ì´ë¯¸ì§€ ìƒì„±ì— ìƒë‹¹í•œ ë°œì „ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ìµœê·¼ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€(T2I) ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° í° ì„±ê³µì„ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë³µì¡í•œ ì¥ë©´ì—ì„œì˜ í–‰ë™ê³¼ ìƒí˜¸ì‘ìš©ì„ ì •í™•íˆ í‘œí˜„í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” T2I ëª¨ë¸ì´ í–‰ë™ ë¬˜ì‚¬ì— ë‚´ì¬ëœ ë¯¸ë¬˜í•œ ì†ì„±ì„ ì˜ í¬ì°©í•˜ì§€ ëª»í•´ ì¤‘ìš”í•œ ë§¥ë½ì  ì„¸ë¶€ì‚¬í•­ì´ ë¶€ì¡±í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤ëŠ” ì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ í–‰ë™ ì¤‘ì‹¬ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„± ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” AcT2I ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì£¼ìš” T2I ëª¨ë¸ë“¤ì´ AcT2Iì—ì„œ ì¢‹ì€ ì„±ê³¼ë¥¼ ë‚´ì§€ ëª»í–ˆìœ¼ë©°, ì´ëŠ” ê¸°ì¡´ T2I ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì†ì„±ê³¼ ë§¥ë½ì  ì˜ì¡´ì„±ì´ ë¶ˆì™„ì „í•˜ê²Œ í‘œí˜„ë˜ì—ˆê¸° ë•Œë¬¸ì´ë¼ê³  ê°€ì„¤ì„ ì„¸ì› ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í›ˆë ¨ ì—†ëŠ” ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, í”„ë¡¬í”„íŠ¸ì— ì‹œê°„ì  ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨ì‹œì¼œ ì´ë¯¸ì§€ ìƒì„± ì •í™•ë„ë¥¼ 72% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” í˜„ì¬ T2I ë°©ë²•ì˜ í•œê³„ë¥¼ ë³´ì—¬ì£¼ë©°, ì²´ê³„ì ì¸ ì–¸ì–´ ì§€ì‹ í†µí•©ì´ ë³µì¡í•˜ê³  ë§¥ë½ì ìœ¼ë¡œ ì •í™•í•œ ì´ë¯¸ì§€ ìƒì„±ì— í¬ê²Œ ê¸°ì—¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Text-to-Image(T2I) ëª¨ë¸ì€ ë³µì¡í•œ ì¥ë©´ì˜ ë™ì‘ê³¼ ìƒí˜¸ì‘ìš©ì„ ì •í™•íˆ í‘œí˜„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
- 2. AcT2Ië¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í•˜ì—¬ ë™ì‘ ì¤‘ì‹¬ì˜ í”„ë¡¬í”„íŠ¸ì—ì„œ T2I ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 3. ê¸°ì¡´ T2I ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ê°€ ë‚´ì¬ëœ ì†ì„±ê³¼ ë§¥ë½ì  ì˜ì¡´ì„±ì„ ì¶©ë¶„íˆ í‘œí˜„í•˜ì§€ ëª»í•´ ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
- 4. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ì— ì‹œê°„ì  ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„± ì •í™•ë„ë¥¼ 72% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 5. ì–¸ì–´ì  ì§€ì‹ì„ ì²´ê³„ì ìœ¼ë¡œ í†µí•©í•˜ë©´ ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ì´ë¯¸ì§€ ìƒì„±ì˜ ì •í™•ì„±ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:22:13*