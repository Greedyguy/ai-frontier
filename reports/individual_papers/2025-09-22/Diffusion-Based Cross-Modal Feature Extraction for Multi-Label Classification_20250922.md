---
keywords:
  - Diffusion-Based Feature Extraction
  - Transformer
  - Multi-label Classification
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15553
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:03:06.729609",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion-Based Feature Extraction",
    "Transformer",
    "Multi-label Classification",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion-Based Feature Extraction": 0.78,
    "Transformer": 0.85,
    "Multi-label Classification": 0.8,
    "Vision-Language Model": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diff-Feat",
        "canonical": "Diffusion-Based Feature Extraction",
        "aliases": [
          "Diffusion Feature Extraction",
          "Diff-Feat"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for feature extraction using diffusion models, which is central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a fundamental model architecture used in the proposed method, linking to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multi-label classification",
        "canonical": "Multi-label Classification",
        "aliases": [
          "Multi-label",
          "Multi-label Task"
        ],
        "category": "specific_connectable",
        "rationale": "The paper addresses challenges specific to multi-label classification, making it a key concept for linking.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language",
          "Vision-Language Task"
        ],
        "category": "evolved_concepts",
        "rationale": "The integration of vision and language tasks is central to the paper's approach, aligning with current trends.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "intermediate features",
      "downstream tasks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diff-Feat",
      "resolved_canonical": "Diffusion-Based Feature Extraction",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multi-label classification",
      "resolved_canonical": "Multi-label Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification

**Korean Title:** ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ë¥¼ ìœ„í•œ í™•ì‚° ê¸°ë°˜ êµì°¨ ëª¨ë‹¬ íŠ¹ì§• ì¶”ì¶œ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15553.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15553](https://arxiv.org/abs/2509.15553)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/DiffCut_ Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut_20250919|DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut]] (83.2% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (82.5% similar)
- [[2025-09-22/RespoDiff_ Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation_20250922|RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation]] (82.4% similar)
- [[2025-09-17/SpecDiff_ Accelerating Diffusion Model Inference with Self-Speculation_20250917|SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation]] (82.1% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multi-label Classification|Multi-label Classification]]
**âš¡ Unique Technical**: [[keywords/Diffusion-Based Feature Extraction|Diffusion-Based Feature Extraction]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15553v1 Announce Type: cross 
Abstract: Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \textit{Diff-Feat}, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious "Layer $12$" consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts. The code is available at https://github.com/lt-0123/Diff-Feat.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15553v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ëŠ” ê´‘ë²”ìœ„í•œ ì‘ìš© ë¶„ì•¼ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ë‹¤ì¤‘ ë¼ë²¨ ìƒí˜¸ì‘ìš©ì„ í¬ì°©í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ í‘œí˜„ì— ì˜ì¡´í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‚¬ì „ í•™ìŠµëœ í™•ì‚°-íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ì¤‘ê°„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ í›„ì† ì‘ì—…ì— ìœµí•©í•˜ëŠ” ê°„ë‹¨í•˜ì§€ë§Œ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì¸ \textit{Diff-Feat}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ë¹„ì „ ì‘ì—…ì˜ ê²½ìš°, í™•ì‚° ê³¼ì • ì¤‘ ê°€ì¥ ë³€ë³„ë ¥ ìˆëŠ” ì¤‘ê°„ íŠ¹ì§•ì€ ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ë°œìƒí•˜ë©° íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¤‘ê°„ ë¸”ë¡ì— ìœ„ì¹˜í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì–¸ì–´ ì‘ì—…ì˜ ê²½ìš°, ìµœê³ ì˜ íŠ¹ì§•ì€ ì¡ìŒì´ ì—†ëŠ” ë‹¨ê³„ì—ì„œ ë°œìƒí•˜ë©° ê°€ì¥ ê¹Šì€ ë¸”ë¡ì— ìœ„ì¹˜í•©ë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ë†€ë¼ìš´ í˜„ìƒì„ ê´€ì°°í–ˆëŠ”ë°, ì‹ ë¹„ë¡œìš´ "ë ˆì´ì–´ $12$"ê°€ ì´ë¯¸ì§€ì— ëŒ€í•œ ë‹¤ì–‘í•œ í›„ì† ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤ (DiT-XL/2-256$\times$256 ê¸°ì¤€). ìš°ë¦¬ëŠ” ëª‡ ê°€ì§€ í›„ë³´ ì¤‘ì—ì„œ ì§€ì—­ ìµœì ì˜ "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸"$\times$"ë¸”ë¡-ì‹œê°„ ë‹¨ê³„" ìŒì„ ì°¾ì•„ë‚´ëŠ” íœ´ë¦¬ìŠ¤í‹± ì§€ì—­ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê³ ì•ˆí•˜ì—¬, ì „ì²´ì ì¸ ê·¸ë¦¬ë“œ ê²€ìƒ‰ì„ í”¼í–ˆìŠµë‹ˆë‹¤. ì„ íƒëœ í‘œí˜„ì„ ë‹¨ìˆœ ìœµí•©-ì„ í˜• íˆ¬ì˜ í›„ ë”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤: MS-COCO-í–¥ìƒì—ì„œ 98.6% mAPì™€ Visual Genome 500ì—ì„œ 45.7% mAPë¥¼ ê¸°ë¡í•˜ë©°, ê°•ë ¥í•œ CNN, ê·¸ë˜í”„, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ì¤€ì„ ì„ í° ì°¨ì´ë¡œ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. t-SNEì™€ í´ëŸ¬ìŠ¤í„°ë§ ì§€í‘œëŠ” \textit{Diff-Feat}ê°€ ë‹¨ì¼ ëª¨ë‹¬ ëŒ€ì•ˆë³´ë‹¤ ë” ë°€ì§‘ëœ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•¨ì„ ì¶”ê°€ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/lt-0123/Diff-Featì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ \textit{Diff-Feat}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì‚¬ì „ í•™ìŠµëœ í™•ì‚°-íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë¡œë¶€í„° ì¤‘ê°„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— í™œìš©í•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ìœ¼ë¡œ, ë¹„ì „ ì‘ì—…ì—ì„œëŠ” ì¤‘ê°„ ë‹¨ê³„ì˜ ì¤‘ê°„ ë¸”ë¡ì—ì„œ ê°€ì¥ êµ¬ë³„ë ¥ ìˆëŠ” íŠ¹ì§•ì´ ë‚˜íƒ€ë‚˜ê³ , ì–¸ì–´ ì‘ì—…ì—ì„œëŠ” ê°€ì¥ ê¹Šì€ ë¸”ë¡ì—ì„œ ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ë‹¨ê³„ì—ì„œ ìµœì ì˜ íŠ¹ì§•ì´ ë‚˜íƒ€ë‚œë‹¤ëŠ” ì ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ "Layer 12"ê°€ ì¼ê´€ë˜ê²Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, íš¨ìœ¨ì ì¸ ì§€ì—­ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ìµœì ì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ì™€ ë¸”ë¡-íƒ€ì„ìŠ¤í… ì¡°í•©ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ MS-COCO-enhancedì—ì„œ 98.6% mAP, Visual Genome 500ì—ì„œ 45.7% mAPë¥¼ ê¸°ë¡í•˜ë©°, ê¸°ì¡´ì˜ CNN, ê·¸ë˜í”„, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì„ í¬ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤. \textit{Diff-Feat}ëŠ” ë‹¨ì¼ ëª¨ë‹¬ë³´ë‹¤ ë” ë°€ì§‘ëœ ì˜ë¯¸ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Diff-FeatëŠ” ì‚¬ì „ í•™ìŠµëœ diffusion-Transformer ëª¨ë¸ì—ì„œ ì¤‘ê°„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ì— í™œìš©í•˜ëŠ” ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ë¹„ì „ ì‘ì—…ì—ì„œëŠ” ì¤‘ê°„ ë‹¨ê³„ì™€ Transformerì˜ ì¤‘ê°„ ë¸”ë¡ì—ì„œ ê°€ì¥ ë³€ë³„ë ¥ ìˆëŠ” íŠ¹ì§•ì´ ë‚˜íƒ€ë‚˜ë©°, ì–¸ì–´ ì‘ì—…ì—ì„œëŠ” ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ë‹¨ê³„ì™€ ê°€ì¥ ê¹Šì€ ë¸”ë¡ì—ì„œ ìµœìƒì˜ íŠ¹ì§•ì´ ë°œê²¬ë©ë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ "Layer 12"ê°€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” í˜„ìƒì´ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ì™€ ë¸”ë¡-íƒ€ì„ìŠ¤í… ìŒì„ ìµœì í™”í•˜ê¸° ìœ„í•œ íœ´ë¦¬ìŠ¤í‹± ë¡œì»¬ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬, ë³µì¡í•œ ê·¸ë¦¬ë“œ ê²€ìƒ‰ì„ í”¼í–ˆìŠµë‹ˆë‹¤.
- 5. ì„ íƒëœ í‘œí˜„ì„ ë‹¨ìˆœí•œ ìœµí•©-ì„ í˜• íˆ¬ì˜ ë° ì¶”ê°€í•˜ì—¬ MS-COCO-enhancedì—ì„œ 98.6% mAP, Visual Genome 500ì—ì„œ 45.7% mAPë¥¼ ê¸°ë¡í•˜ë©°, ê¸°ì¡´ì˜ ê°•ë ¥í•œ CNN, ê·¸ë˜í”„, Transformer ê¸°ë°˜ì„ í¬ê²Œ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:03:06*