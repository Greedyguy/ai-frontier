---
keywords:
  - Riemannian Stochastic Gradient Descent
  - Batch Size Optimization
  - Stochastic Oracle Complexity
  - Learning Rate Scheduling
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2501.18164
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:03:43.434432",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Riemannian Stochastic Gradient Descent",
    "Batch Size Optimization",
    "Stochastic Oracle Complexity",
    "Learning Rate Scheduling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Riemannian Stochastic Gradient Descent": 0.78,
    "Batch Size Optimization": 0.79,
    "Stochastic Oracle Complexity": 0.77,
    "Learning Rate Scheduling": 0.81
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Riemannian Stochastic Gradient Descent",
        "canonical": "Riemannian Stochastic Gradient Descent",
        "aliases": [
          "RSGD"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific optimization technique relevant to the paper's focus on convergence behavior.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "increasing batch size",
        "canonical": "Batch Size Optimization",
        "aliases": [
          "dynamic batch size",
          "variable batch size"
        ],
        "category": "specific_connectable",
        "rationale": "Optimizing batch size is crucial for improving convergence rates, linking to broader optimization strategies.",
        "novelty_score": 0.68,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "stochastic first-order oracle complexity",
        "canonical": "Stochastic Oracle Complexity",
        "aliases": [
          "SFO complexity"
        ],
        "category": "unique_technical",
        "rationale": "This term is specific to the computational analysis of the optimization method discussed.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "cosine annealing decay",
        "canonical": "Learning Rate Scheduling",
        "aliases": [
          "cosine decay",
          "annealing schedule"
        ],
        "category": "specific_connectable",
        "rationale": "Learning rate scheduling is a key concept in training optimization, linking to various decay strategies.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.81
      }
    ],
    "ban_list_suggestions": [
      "convergence rate",
      "computational time"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Riemannian Stochastic Gradient Descent",
      "resolved_canonical": "Riemannian Stochastic Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "increasing batch size",
      "resolved_canonical": "Batch Size Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "stochastic first-order oracle complexity",
      "resolved_canonical": "Stochastic Oracle Complexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "cosine annealing decay",
      "resolved_canonical": "Learning Rate Scheduling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.81
      }
    }
  ]
}
-->

# Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size

**Korean Title:** ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ë ´ ì†ë„ í–¥ìƒ: ë°°ì¹˜ í¬ê¸° ì¦ê°€ì˜ íš¨ê³¼

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2501.18164.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2501.18164](https://arxiv.org/abs/2501.18164)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (84.1% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (83.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (82.3% similar)
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (81.1% similar)
- [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Batch Size Optimization|Batch Size Optimization]], [[keywords/Learning Rate Scheduling|Learning Rate Scheduling]]
**âš¡ Unique Technical**: [[keywords/Riemannian Stochastic Gradient Descent|Riemannian Stochastic Gradient Descent]], [[keywords/Stochastic Oracle Complexity|Stochastic Oracle Complexity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.18164v4 Announce Type: replace 
Abstract: We theoretically analyzed the convergence behavior of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster convergence than using a constant batch size, not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. The convergence rate improves from $O(T^{-1}+C)$ with a constant batch size to $O(T^{-1})$ with an increasing batch size, where $T$ denotes the total number of iterations and $C$ is a constant. Using principal component analysis and low-rank matrix completion, we investigated, both theoretically and numerically, how an increasing batch size affects computational time as quantified by stochastic first-order oracle (SFO) complexity. An increasing batch size was found to reduce the SFO complexity of RSGD. Furthermore, an increasing batch size was found to offer the advantages of both small and large constant batch sizes.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2501.18164v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(RSGD)ì˜ ìˆ˜ë ´ í–‰ë™ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìœ¼ë©°, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë¹ ë¥¸ ìˆ˜ë ´ì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¼ì •í•œ í•™ìŠµë¥ ë¿ë§Œ ì•„ë‹ˆë¼ ì½”ì‚¬ì¸ ê°ì†Œ ë° ë‹¤í•­ì‹ ê°ì†Œì™€ ê°™ì€ ê°ì†Œí•˜ëŠ” í•™ìŠµë¥ ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ìˆ˜ë ´ ì†ë„ëŠ” ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ì˜ ê²½ìš° $O(T^{-1}+C)$ì—ì„œ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ì˜ ê²½ìš° $O(T^{-1})$ë¡œ ê°œì„ ë˜ë©°, ì—¬ê¸°ì„œ $T$ëŠ” ì´ ë°˜ë³µ íšŸìˆ˜ì´ê³  $C$ëŠ” ìƒìˆ˜ì…ë‹ˆë‹¤. ì£¼ì„±ë¶„ ë¶„ì„ê³¼ ì €ìˆœìœ„ í–‰ë ¬ ì™„ì„±ì„ ì‚¬ìš©í•˜ì—¬, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ì„±ìœ¼ë¡œ ì •ëŸ‰í™”ëœ ê³„ì‚° ì‹œê°„ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ì´ë¡ ì  ë° ìˆ˜ì¹˜ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” RSGDì˜ SFO ë³µì¡ì„±ì„ ì¤„ì´ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë˜í•œ, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ì‘ì€ ë° í° ì¼ì •í•œ ë°°ì¹˜ í¬ê¸°ì˜ ì¥ì ì„ ëª¨ë‘ ì œê³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(RSGD)ì˜ ìˆ˜ë ´ í–‰ë™ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ê²½ìš° ê³ ì • ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê³ ì • í•™ìŠµë¥ ë¿ë§Œ ì•„ë‹ˆë¼ ì½”ì‚¬ì¸ ê°ì†Œ ë° ë‹¤í•­ì‹ ê°ì†Œì™€ ê°™ì€ ê°ì†Œ í•™ìŠµë¥ ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ìˆ˜ë ´ ì†ë„ëŠ” ê³ ì • ë°°ì¹˜ í¬ê¸°ì—ì„œ $O(T^{-1}+C)$ì—ì„œ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ì—ì„œ $O(T^{-1})$ë¡œ ê°œì„ ë©ë‹ˆë‹¤. ë˜í•œ ì£¼ì„±ë¶„ ë¶„ì„ê³¼ ì €ë­í¬ í–‰ë ¬ ì™„ì„±ì„ í†µí•´ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ì„±ìœ¼ë¡œ ì¸¡ì •ëœ ê³„ì‚° ì‹œê°„ì„ ì–´ë–»ê²Œ ì¤„ì´ëŠ”ì§€ë¥¼ ì´ë¡ ì  ë° ìˆ˜ì¹˜ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” RSGDì˜ SFO ë³µì¡ì„±ì„ ì¤„ì´ë©°, ì‘ì€ ê³ ì • ë°°ì¹˜ í¬ê¸°ì™€ í° ê³ ì • ë°°ì¹˜ í¬ê¸°ì˜ ì¥ì ì„ ëª¨ë‘ ì œê³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¦¬ë§Œ ê¸°í•˜í•™ì  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(RSGD)ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œ ë” ë¹ ë¥´ë‹¤.
- 2. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ì¼ì •í•œ í•™ìŠµë¥ ë¿ë§Œ ì•„ë‹ˆë¼ ì½”ì‚¬ì¸ ê°ì‡  ë° ë‹¤í•­ì‹ ê°ì‡ ì™€ ê°™ì€ ê°ì‡  í•™ìŠµë¥ ì—ì„œë„ ìˆ˜ë ´ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤.
- 3. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” RSGDì˜ í™•ë¥ ì  ì¼ì°¨ ì˜¤ë¼í´(SFO) ë³µì¡ì„±ì„ ê°ì†Œì‹œí‚¨ë‹¤.
- 4. ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ëŠ” ì‘ì€ ë°°ì¹˜ í¬ê¸°ì™€ í° ë°°ì¹˜ í¬ê¸°ì˜ ì¥ì ì„ ëª¨ë‘ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-23 11:03:43*