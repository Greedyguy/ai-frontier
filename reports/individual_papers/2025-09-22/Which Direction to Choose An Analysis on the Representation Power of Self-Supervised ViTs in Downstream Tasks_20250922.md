---
keywords:
  - Self-supervised Learning
  - Transformer
  - Contrastive Learning
  - Masked Image Modeling
  - Few-Shot Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15272
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:55:08.627810",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Transformer",
    "Contrastive Learning",
    "Masked Image Modeling",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.88,
    "Transformer": 0.8,
    "Contrastive Learning": 0.79,
    "Masked Image Modeling": 0.75,
    "Few-Shot Learning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Self-Supervised Learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "SSL"
        ],
        "category": "specific_connectable",
        "rationale": "Self-supervised learning is a key pre-training strategy discussed in the paper, linking it to various downstream tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      },
      {
        "surface": "Vision Transformers",
        "canonical": "Transformer",
        "aliases": [
          "ViTs"
        ],
        "category": "broad_technical",
        "rationale": "Vision Transformers are central to the paper's analysis, providing a basis for understanding representation power in computer vision tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Contrastive Learning",
        "canonical": "Contrastive Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Contrastive learning is a dominant pre-training objective in self-supervised learning, relevant to the paper's focus.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Masked Image Modeling",
        "canonical": "Masked Image Modeling",
        "aliases": [
          "MIM"
        ],
        "category": "unique_technical",
        "rationale": "Masked image modeling is a specific technique analyzed for its effectiveness in self-supervised learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Few-shot learning is a context where the representation power of ViTs is evaluated, making it a relevant concept.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.68,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "pre-training strategy",
      "downstream tasks",
      "transformation layers"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Self-Supervised Learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Vision Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Contrastive Learning",
      "resolved_canonical": "Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Masked Image Modeling",
      "resolved_canonical": "Masked Image Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.68,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks

**Korean Title:** ì–´ëŠ ë°©í–¥ì„ ì„ íƒí•´ì•¼ í• ê¹Œ? ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ìê¸° ì§€ë„ í•™ìŠµ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViTs)ì˜ í‘œí˜„ë ¥ì— ëŒ€í•œ ë¶„ì„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15272.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15272](https://arxiv.org/abs/2509.15272)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/[Re] Improving Interpretation Faithfulness for Vision Transformers_20250918|[Re] Improving Interpretation Faithfulness for Vision Transformers]] (82.9% similar)
- [[2025-09-22/Large Vision Models Can Solve Mental Rotation Problems_20250922|Large Vision Models Can Solve Mental Rotation Problems]] (82.2% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (82.2% similar)
- [[2025-09-17/Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions_20250917|Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions]] (81.8% similar)
- [[2025-09-17/Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation_20250917|Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Contrastive Learning|Contrastive Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Masked Image Modeling|Masked Image Modeling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15272v1 Announce Type: new 
Abstract: Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently demonstrated considerable potential as a pre-training strategy for a variety of computer vision tasks, including image classification and segmentation, both in standard and few-shot downstream contexts. Two pre-training objectives dominate the landscape of SSL techniques: Contrastive Learning and Masked Image Modeling. Features (or tokens) extracted from the final transformer attention block -- specifically, the keys, queries, and values -- as well as features obtained after the final block's feed-forward layer, have become a common foundation for addressing downstream tasks. However, in many existing approaches, these pre-trained ViT features are further processed through additional transformation layers, often involving lightweight heads or combined with distillation, to achieve superior task performance. Although such methods can improve task outcomes, to the best of our knowledge, a comprehensive analysis of the intrinsic representation capabilities of unaltered ViT features has yet to be conducted. This study aims to bridge this gap by systematically evaluating the use of these unmodified features across image classification and segmentation tasks, in both standard and few-shot contexts. The classification and segmentation rules that we use are either hyperplane based (as in logistic regression) or cosine-similarity based, both of which rely on the presence of interpretable directions in the ViT's latent space. Based on the previous rules and without the use of additional feature transformations, we conduct an analysis across token types, tasks, and pre-trained ViT models. This study provides insights into the optimal choice for token type and decision rule based on the task, context, and the pre-training objective, while reporting detailed findings on two widely-used datasets.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15272v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViTs)ë¥¼ ìœ„í•œ ìê°€ ì§€ë„ í•™ìŠµ(SSL)ì€ ìµœê·¼ ì´ë¯¸ì§€ ë¶„ë¥˜ ë° ì„¸ë¶„í™”ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì—ì„œ, í‘œì¤€ ë° ì†Œìˆ˜ ìƒ· ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í™˜ê²½ ëª¨ë‘ì—ì„œ ì‚¬ì „ í•™ìŠµ ì „ëµìœ¼ë¡œ ìƒë‹¹í•œ ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. SSL ê¸°ë²•ì˜ ë‘ ê°€ì§€ ì£¼ìš” ì‚¬ì „ í•™ìŠµ ëª©í‘œëŠ” ëŒ€ì¡° í•™ìŠµê³¼ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ëª¨ë¸ë§ì…ë‹ˆë‹¤. ìµœì¢… íŠ¸ëœìŠ¤í¬ë¨¸ ì£¼ì˜ ë¸”ë¡ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•(ë˜ëŠ” í† í°) -- íŠ¹íˆ í‚¤, ì¿¼ë¦¬, ê°’ -- ë° ìµœì¢… ë¸”ë¡ì˜ í”¼ë“œ í¬ì›Œë“œ ë ˆì´ì–´ ì´í›„ì— ì–»ì–´ì§„ íŠ¹ì§•ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ê³µí†µ ê¸°ë°˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì—ì„œ ì´ëŸ¬í•œ ì‚¬ì „ í•™ìŠµëœ ViT íŠ¹ì§•ì€ ì¢…ì¢… ê²½ëŸ‰ í—¤ë“œ ë˜ëŠ” ì¦ë¥˜ì™€ ê²°í•©ëœ ì¶”ê°€ ë³€í™˜ ë ˆì´ì–´ë¥¼ í†µí•´ ë” ì²˜ë¦¬ë˜ì–´ ìš°ìˆ˜í•œ ì‘ì—… ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì´ ì‘ì—… ê²°ê³¼ë¥¼ ê°œì„ í•  ìˆ˜ ìˆì§€ë§Œ, ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ìˆ˜ì •ë˜ì§€ ì•Šì€ ViT íŠ¹ì§•ì˜ ë‚´ì¬ì  í‘œí˜„ ëŠ¥ë ¥ì— ëŒ€í•œ í¬ê´„ì ì¸ ë¶„ì„ì€ ì•„ì§ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” í‘œì¤€ ë° ì†Œìˆ˜ ìƒ· í™˜ê²½ ëª¨ë‘ì—ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë° ì„¸ë¶„í™” ì‘ì—… ì „ë°˜ì— ê±¸ì³ ì´ëŸ¬í•œ ìˆ˜ì •ë˜ì§€ ì•Šì€ íŠ¹ì§•ì˜ ì‚¬ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•¨ìœ¼ë¡œì¨ ì´ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë¶„ë¥˜ ë° ì„¸ë¶„í™” ê·œì¹™ì€ í•´ì„ ê°€ëŠ¥í•œ ë°©í–¥ì˜ ì¡´ì¬ì— ì˜ì¡´í•˜ëŠ” í•˜ì´í¼í”Œë ˆì¸ ê¸°ë°˜(ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ê°™ì€) ë˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ê¸°ë°˜ì…ë‹ˆë‹¤. ì´ì „ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ì ì¸ íŠ¹ì§• ë³€í™˜ ì—†ì´, ìš°ë¦¬ëŠ” í† í° ìœ í˜•, ì‘ì—… ë° ì‚¬ì „ í•™ìŠµëœ ViT ëª¨ë¸ ì „ë°˜ì— ê±¸ì³ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì‘ì—…, ë§¥ë½ ë° ì‚¬ì „ í•™ìŠµ ëª©í‘œì— ë”°ë¼ í† í° ìœ í˜• ë° ê²°ì • ê·œì¹™ì˜ ìµœì  ì„ íƒì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•˜ë©°, ë‘ ê°€ì§€ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì— ëŒ€í•œ ìƒì„¸í•œ ê²°ê³¼ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Vision Transformers(ViTs)ì˜ ìê¸° ì§€ë„ í•™ìŠµ(SSL)ì—ì„œ ë³€í˜•ë˜ì§€ ì•Šì€ ViT íŠ¹ì§•ì˜ ë‚´ì¬ì  í‘œí˜„ ëŠ¥ë ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ SSL ê¸°ë²•ì€ ëŒ€ì¡° í•™ìŠµê³¼ ë§ˆìŠ¤í‚¹ ì´ë¯¸ì§€ ëª¨ë¸ë§ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ë©°, ìµœì¢… íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì„ ì¶”ê°€ ë³€í™˜ ì—†ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ì™€ ì„¸ë¶„í™” ì‘ì—…ì— ì ìš©í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” í•˜ì´í¼í”Œë ˆì¸ ê¸°ë°˜ ë˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ì˜ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ í† í° ìœ í˜•, ì‘ì—…, ì‚¬ì „ í›ˆë ¨ëœ ViT ëª¨ë¸ì„ ë¶„ì„í•˜ë©°, ìµœì ì˜ í† í° ìœ í˜•ê³¼ ê²°ì • ê·œì¹™ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‘ ê°€ì§€ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ì„ ìƒì„¸íˆ ë³´ê³ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT)ì˜ ìê°€ ì§€ë„ í•™ìŠµ(SSL)ì€ ì´ë¯¸ì§€ ë¶„ë¥˜ ë° ì„¸ë¶„í™”ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì—ì„œ íš¨ê³¼ì ì¸ ì‚¬ì „ í•™ìŠµ ì „ëµìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì£¼ë¡œ ëŒ€ì¡° í•™ìŠµê³¼ ë§ˆìŠ¤í‚¹ ì´ë¯¸ì§€ ëª¨ë¸ë§ì„ í†µí•´ ViT íŠ¹ì§•ì„ ì¶”ê°€ ë³€í™˜ì¸µìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” ë³€í˜•ë˜ì§€ ì•Šì€ ViT íŠ¹ì§•ì˜ ë‚´ì¬ì  í‘œí˜„ ëŠ¥ë ¥ì„ ì´ë¯¸ì§€ ë¶„ë¥˜ ë° ì„¸ë¶„í™” ì‘ì—…ì—ì„œ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
- 4. ì—°êµ¬ëŠ” í•˜ì´í¼í”Œë ˆì¸ ê¸°ë°˜ ë˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ê¸°ë°˜ì˜ ë¶„ë¥˜ ë° ì„¸ë¶„í™” ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ViTì˜ ì ì¬ ê³µê°„ì—ì„œ í•´ì„ ê°€ëŠ¥í•œ ë°©í–¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì‘ì—…, ë¬¸ë§¥, ì‚¬ì „ í•™ìŠµ ëª©í‘œì— ë”°ë¼ ìµœì ì˜ í† í° ìœ í˜• ë° ê²°ì • ê·œì¹™ ì„ íƒì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:55:08*