---
keywords:
  - Large Language Model
  - Multilingual Dialogue Evaluation
  - Open-Domain Dialogue
  - Meta-Evaluation Benchmark
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2505.22777
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:47:30.519637",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multilingual Dialogue Evaluation",
    "Open-Domain Dialogue",
    "Meta-Evaluation Benchmark"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multilingual Dialogue Evaluation": 0.78,
    "Open-Domain Dialogue": 0.8,
    "Meta-Evaluation Benchmark": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's framework and evaluation process.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multilingual Dialogue Evaluation",
        "canonical": "Multilingual Dialogue Evaluation",
        "aliases": [
          "multilingual evaluation",
          "cross-lingual evaluation"
        ],
        "category": "unique_technical",
        "rationale": "The paper introduces a new benchmark specifically focused on multilingual dialogue evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Open-Domain Dialogue",
        "canonical": "Open-Domain Dialogue",
        "aliases": [
          "open-domain chat",
          "open dialogue"
        ],
        "category": "specific_connectable",
        "rationale": "Open-domain dialogue is a key focus of the evaluation framework introduced in the paper.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Meta-Evaluation Benchmark",
        "canonical": "Meta-Evaluation Benchmark",
        "aliases": [
          "evaluation benchmark",
          "meta-evaluation"
        ],
        "category": "unique_technical",
        "rationale": "The paper proposes a new meta-evaluation benchmark to improve dialogue evaluation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multilingual Dialogue Evaluation",
      "resolved_canonical": "Multilingual Dialogue Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Open-Domain Dialogue",
      "resolved_canonical": "Open-Domain Dialogue",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Meta-Evaluation Benchmark",
      "resolved_canonical": "Meta-Evaluation Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators

**Korean Title:** MEDAL: ë‹¤êµ­ì–´ ì˜¤í”ˆ ë„ë©”ì¸ ëŒ€í™” í‰ê°€ìë¡œì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ë²¤ì¹˜ë§ˆí‚¹í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.22777.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2505.22777](https://arxiv.org/abs/2505.22777)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MUG-Eval_ A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language_20250922|MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language]] (87.3% similar)
- [[2025-09-19/Ticket-Bench_ A Kickoff for Multilingual and Regionalized Agent Evaluation_20250919|Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation]] (86.3% similar)
- [[2025-09-22/Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context_20250922|Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context]] (85.6% similar)
- [[2025-09-19/Judging with Many Minds_ Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge_20250919|Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge]] (85.3% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Open-Domain Dialogue|Open-Domain Dialogue]]
**âš¡ Unique Technical**: [[keywords/Multilingual Dialogue Evaluation|Multilingual Dialogue Evaluation]], [[keywords/Meta-Evaluation Benchmark|Meta-Evaluation Benchmark]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.22777v3 Announce Type: replace 
Abstract: Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, commonsense, or relevance.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.22777v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì˜¤í”ˆ ë„ë©”ì¸ ì±—ë´‡ì˜ í’ˆì§ˆ í‰ê°€ê°€ ì ì  ë” ìë™ ì‹¬íŒ ì—­í• ì„ í•˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì˜ì¡´í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ë©”íƒ€ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ëŠ” ì •ì ì´ê³ , êµ¬ì‹ì´ë©°, ë‹¤êµ­ì–´ ë²”ìœ„ê°€ ë¶€ì¡±í•˜ì—¬ í‰ê°€ì—ì„œ ë¯¸ì„¸í•œ ì•½ì ì„ ì™„ì „íˆ í¬ì°©í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë³´ë‹¤ ëŒ€í‘œì ì´ê³  ë‹¤ì–‘í•œ ì˜¤í”ˆ ë„ë©”ì¸ ëŒ€í™” í‰ê°€ ë²¤ì¹˜ë§ˆí¬ë¥¼ íë ˆì´íŒ…í•˜ê¸° ìœ„í•œ ìë™í™”ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ì¸ MEDALì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì—¬ëŸ¬ ìµœì²¨ë‹¨ LLMì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‹œë“œ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì‚¬ìš©ì-ì±—ë´‡ ë‹¤êµ­ì–´ ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê°•ë ¥í•œ LLM(GPT-4.1)ì„ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ì˜ ì„±ëŠ¥ì„ ë‹¤ì°¨ì›ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ëˆˆì— ë„ëŠ” ì–¸ì–´ ê°„ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë°œê²¬í•©ë‹ˆë‹¤. ì´ ëŒ€ê·œëª¨ í‰ê°€ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ë©”íƒ€ í‰ê°€ ë‹¤êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ë¥¼ íë ˆì´íŒ…í•˜ê³ , ë¯¸ë¬˜í•œ í’ˆì§ˆ íŒë‹¨ìœ¼ë¡œ ì¸ê°„ ì£¼ì„ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì—¬ëŸ¬ ì¶”ë¡  ë° ë¹„ì¶”ë¡  LLMì´ ì˜¤í”ˆ ë„ë©”ì¸ ëŒ€í™”ì˜ í‰ê°€ìë¡œì„œì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. MEDALì„ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ëŠ” ìµœì²¨ë‹¨ ì‹¬íŒì´ ê³µê° ë¶€ì¡±, ìƒì‹ ë¶€ì¡± ë˜ëŠ” ê´€ë ¨ì„± ë¶€ì¡±ê³¼ ê°™ì€ ë¯¸ë¬˜í•œ ë¬¸ì œë¥¼ ì‹ ë¢°ì„± ìˆê²Œ ê°ì§€í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜¤í”ˆ ë„ë©”ì¸ ì±—ë´‡ì˜ í‰ê°€ì—ì„œ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì˜ ìë™ í‰ê°€ì ì—­í• ì— ëŒ€í•œ í•œê³„ë¥¼ ì§€ì í•˜ê³ , ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ MEDALì´ë¼ëŠ” ìë™í™”ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MEDALì€ ë‹¤ì–‘í•œ ìµœì²¨ë‹¨ LLMì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì‚¬ìš©ì-ì±—ë´‡ ëŒ€í™”ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ì±—ë´‡ ì„±ëŠ¥ì˜ ë‹¤ì°¨ì› ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. íŠ¹íˆ, GPT-4.1ì„ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ê°„ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë°œê²¬í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë‹¤êµ­ì–´ ë©”íƒ€ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì—¬ëŸ¬ LLMì˜ í‰ê°€ìë¡œì„œì˜ ëŠ¥ë ¥ì„ ê²€ì¦í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, MEDALì„ í†µí•´ ìµœì‹  í‰ê°€ìë“¤ì´ ê³µê° ë¶€ì¡±, ìƒì‹ ê²°ì—¬, ê´€ë ¨ì„± ë¶€ì¡± ë“±ì˜ ë¯¸ë¬˜í•œ ë¬¸ì œë¥¼ ì‹ ë¢°ì„± ìˆê²Œ ê°ì§€í•˜ì§€ ëª»í•œë‹¤ëŠ” ì ì„ ë°í˜€ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ë©”íƒ€ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ëŠ” ì •ì ì´ê³  ì˜¤ë˜ë˜ì—ˆìœ¼ë©° ë‹¤êµ­ì–´ ì§€ì›ì´ ë¶€ì¡±í•˜ì—¬ í‰ê°€ì˜ ë¯¸ë¬˜í•œ ì•½ì ì„ ì™„ì „íˆ í¬ì°©í•˜ì§€ ëª»í•œë‹¤.
- 2. MEDALì€ ë‹¤ì–‘í•œ ì˜¤í”ˆ ë„ë©”ì¸ ëŒ€í™” í‰ê°€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ìë™í™”ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ì´ë‹¤.
- 3. ìµœì‹  LLMë“¤ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì‚¬ìš©ì-ì±—ë´‡ ëŒ€í™”ë¥¼ ìƒì„±í•˜ê³ , GPT-4.1ì„ ì‚¬ìš©í•´ ì±—ë´‡ì˜ ì„±ëŠ¥ì„ ë‹¤ì°¨ì›ì ìœ¼ë¡œ ë¶„ì„í•œë‹¤.
- 4. ëŒ€ê·œëª¨ í‰ê°€ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë©”íƒ€ í‰ê°€ ë‹¤êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë§Œë“¤ê³ , ì¸ê°„ì´ ì„¸ì‹¬í•œ í’ˆì§ˆ íŒë‹¨ìœ¼ë¡œ ìƒ˜í”Œì„ ì£¼ì„í•œë‹¤.
- 5. MEDALì„ í†µí•´ ìµœì‹  ì‹¬íŒë“¤ì´ ê³µê° ë¶€ì¡±, ìƒì‹ ë¶€ì¡±, ê´€ë ¨ì„± ë¶€ì¡±ê³¼ ê°™ì€ ë¯¸ë¬˜í•œ ë¬¸ì œë¥¼ ì‹ ë¢°ì„± ìˆê²Œ ê°ì§€í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•œë‹¤.


---

*Generated on 2025-09-23 11:47:30*