---
keywords:
  - Random Matrix Theory
  - Knowledge Distillation
  - Self-supervised Learning
  - Large Language Model
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15724
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:32:26.604306",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Random Matrix Theory",
    "Knowledge Distillation",
    "Self-supervised Learning",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Random Matrix Theory": 0.78,
    "Knowledge Distillation": 0.82,
    "Self-supervised Learning": 0.75,
    "Large Language Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Random Matrix Theory",
        "canonical": "Random Matrix Theory",
        "aliases": [
          "RMT"
        ],
        "category": "unique_technical",
        "rationale": "Random Matrix Theory is central to the paper's proposed method and offers a unique perspective on knowledge distillation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Knowledge Distillation",
        "canonical": "Knowledge Distillation",
        "aliases": [
          "KD"
        ],
        "category": "specific_connectable",
        "rationale": "Knowledge Distillation is a key process in the paper, linking it to broader machine learning compression techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Self-distillation",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "self-distillation"
        ],
        "category": "specific_connectable",
        "rationale": "Self-distillation is a form of self-supervised learning, enhancing the model's efficiency and stability.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      },
      {
        "surface": "Large Deep Learning Models",
        "canonical": "Large Language Model",
        "aliases": [
          "large models",
          "deep models"
        ],
        "category": "broad_technical",
        "rationale": "The paper addresses the challenges of deploying large models, a common issue in the field.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "compression method",
      "parameter reduction"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Random Matrix Theory",
      "resolved_canonical": "Random Matrix Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Knowledge Distillation",
      "resolved_canonical": "Knowledge Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Self-distillation",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Large Deep Learning Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation

**Korean Title:** RMT-KD: ëœë¤ í–‰ë ¬ ì´ë¡ ì  ì¸ê³¼ ì§€ì‹ ì¦ë¥˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15724.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15724](https://arxiv.org/abs/2509.15724)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Delta Knowledge Distillation for Large Language Models_20250919|Delta Knowledge Distillation for Large Language Models]] (83.4% similar)
- [[2025-09-22/Temperature-Driven Robust Disease Detection in Brain and Gastrointestinal Disorders via Context-Aware Adaptive Knowledge Distillation_20250922|Temperature-Driven Robust Disease Detection in Brain and Gastrointestinal Disorders via Context-Aware Adaptive Knowledge Distillation]] (81.4% similar)
- [[2025-09-19/Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities_20250919|Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities]] (81.1% similar)
- [[2025-09-22/Efficient Multimodal Dataset Distillation via Generative Models_20250922|Efficient Multimodal Dataset Distillation via Generative Models]] (80.6% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (80.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Knowledge Distillation|Knowledge Distillation]], [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Random Matrix Theory|Random Matrix Theory]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15724v1 Announce Type: new 
Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE, AG News, and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15724v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: BERTì™€ ResNetê³¼ ê°™ì€ ëŒ€í˜• ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì§€ë§Œ, ê·¸ í¬ê¸°ì™€ ì—°ì‚° ìš”êµ¬ ë•Œë¬¸ì— ì—£ì§€ì—ì„œì˜ ë°°í¬ê°€ ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. ìš°ë¦¬ëŠ” RMT-KDë¼ëŠ” ì••ì¶• ë°©ë²•ì„ ì œì‹œí•˜ë©°, ì´ëŠ” ëœë¤ í–‰ë ¬ ì´ë¡ (RMT)ì„ í™œìš©í•œ ì§€ì‹ ì¦ë¥˜ë¥¼ í†µí•´ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤. ê°€ì§€ì¹˜ê¸°ë‚˜ íœ´ë¦¬ìŠ¤í‹± ìˆœìœ„ ì„ íƒ ëŒ€ì‹ , RMT-KDëŠ” ìˆ¨ê²¨ì§„ í‘œí˜„ì˜ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì— ì˜í•´ ì‹ë³„ëœ ì •ë³´ ë°©í–¥ë§Œì„ ë³´ì¡´í•©ë‹ˆë‹¤. RMT ê¸°ë°˜ì˜ ì¸ê³¼ì  ì¶•ì†ŒëŠ” ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ìê¸° ì¦ë¥˜ì™€ í•¨ê»˜ ì¸µë³„ë¡œ ì ìš©ë©ë‹ˆë‹¤. GLUE, AG News, CIFAR-10ì—ì„œ RMT-KDëŠ” ìµœëŒ€ 80%ì˜ ë§¤ê°œë³€ìˆ˜ ê°ì†Œì™€ ë‹¨ 2%ì˜ ì •í™•ë„ ì†ì‹¤ë¡œ 2.8ë°° ë¹ ë¥¸ ì¶”ë¡ ê³¼ ê±°ì˜ ì ˆë°˜ì˜ ì „ë ¥ ì†Œë¹„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” RMT-KDë¥¼ ë„¤íŠ¸ì›Œí¬ ì¦ë¥˜ì— ëŒ€í•œ ìˆ˜í•™ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ í™•ë¦½í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

RMT-KDëŠ” ëŒ€í˜• ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í¬ê¸°ì™€ ì—°ì‚° ìš”êµ¬ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì••ì¶• ë°©ë²•ìœ¼ë¡œ, ëœë¤ í–‰ë ¬ ì´ë¡ (RMT)ì„ í™œìš©í•œ ì§€ì‹ ì¦ë¥˜ ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê°€ì§€ì¹˜ê¸°ë‚˜ ì§ê´€ì  ìˆœìœ„ ì„ íƒ ëŒ€ì‹ , ìˆ¨ê²¨ì§„ í‘œí˜„ì˜ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ í†µí•´ ìœ ìš©í•œ ë°©í–¥ë§Œì„ ë³´ì¡´í•©ë‹ˆë‹¤. ê° ì¸µì— RMT ê¸°ë°˜ ì¸ê³¼ ê°ì†Œë¥¼ ì ìš©í•˜ê³ , ìê¸° ì¦ë¥˜ë¥¼ í†µí•´ ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. GLUE, AG News, CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ ìµœëŒ€ 80%ì˜ ë§¤ê°œë³€ìˆ˜ ê°ì†Œì™€ 2%ì˜ ì •í™•ë„ ì†ì‹¤ë¡œ, 2.8ë°° ë¹ ë¥¸ ì¶”ë¡ ê³¼ ì ˆë°˜ ìˆ˜ì¤€ì˜ ì „ë ¥ ì†Œëª¨ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. RMT-KDëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ì¦ë¥˜ ì ‘ê·¼ë²•ìœ¼ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RMT-KDëŠ” ëœë¤ í–‰ë ¬ ì´ë¡ ì„ í™œìš©í•œ ì§€ì‹ ì¦ë¥˜ ë°©ë²•ìœ¼ë¡œ, ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
- 2. RMT-KDëŠ” ì€ë‹‰ í‘œí˜„ì˜ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ í†µí•´ ì •ë³´ê°€ ìˆëŠ” ë°©í–¥ë§Œì„ ë³´ì¡´í•©ë‹ˆë‹¤.
- 3. GLUE, AG News, CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ ìµœëŒ€ 80%ì˜ íŒŒë¼ë¯¸í„° ê°ì†Œì™€ 2%ì˜ ì •í™•ë„ ì†ì‹¤ë§Œìœ¼ë¡œ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 4. RMT-KDëŠ” ì¶”ë¡  ì†ë„ë¥¼ 2.8ë°° í–¥ìƒì‹œí‚¤ê³  ì „ë ¥ ì†Œë¹„ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
- 5. RMT-KDëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ì¦ë¥˜ ì ‘ê·¼ë²•ìœ¼ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:32:26*