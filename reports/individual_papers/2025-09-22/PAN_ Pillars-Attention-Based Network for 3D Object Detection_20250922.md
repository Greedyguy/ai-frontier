---
keywords:
  - 3D Object Detection
  - Multimodal Learning
  - Radar Point Cloud
  - Attention Mechanism
  - Bird's-eye-view
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15935
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:15:34.202122",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "3D Object Detection",
    "Multimodal Learning",
    "Radar Point Cloud",
    "Attention Mechanism",
    "Bird's-eye-view"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "3D Object Detection": 0.9,
    "Multimodal Learning": 0.82,
    "Radar Point Cloud": 0.75,
    "Attention Mechanism": 0.8,
    "Bird's-eye-view": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "3D Object Detection",
        "canonical": "3D Object Detection",
        "aliases": [
          "3D Detection"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's contribution and connects to other works in 3D perception.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Camera-radar fusion",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Sensor Fusion"
        ],
        "category": "specific_connectable",
        "rationale": "Represents a key method discussed in the paper, linking to broader multimodal approaches.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Radar Point Cloud",
        "canonical": "Radar Point Cloud",
        "aliases": [
          "Radar Data"
        ],
        "category": "unique_technical",
        "rationale": "Specific to the paper's focus on radar data for object detection.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Self-attention mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Self-attention"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the use of attention mechanisms, linking to broader neural network architectures.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Bird's-eye-view (BEV)",
        "canonical": "Bird's-eye-view",
        "aliases": [
          "BEV"
        ],
        "category": "unique_technical",
        "rationale": "Essential for understanding the spatial perspective used in the detection algorithm.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "algorithm",
      "method",
      "performance",
      "results",
      "approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "3D Object Detection",
      "resolved_canonical": "3D Object Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Camera-radar fusion",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Radar Point Cloud",
      "resolved_canonical": "Radar Point Cloud",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Self-attention mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Bird's-eye-view (BEV)",
      "resolved_canonical": "Bird's-eye-view",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# PAN: Pillars-Attention-Based Network for 3D Object Detection

**Korean Title:** PAN: 3D 객체 탐지를 위한 기둥-주의 기반 네트워크

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15935.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15935](https://arxiv.org/abs/2509.15935)

## 🔗 유사한 논문
- [[2025-09-22/DAOcc_ 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction_20250922|DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction]] (84.7% similar)
- [[2025-09-22/RadarGaussianDet3D_ An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars_20250922|RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars]] (83.4% similar)
- [[2025-09-22/Recovering Parametric Scenes from Very Few Time-of-Flight Pixels_20250922|Recovering Parametric Scenes from Very Few Time-of-Flight Pixels]] (83.2% similar)
- [[2025-09-22/A re-calibration method for object detection with multi-modal alignment bias in autonomous driving_20250922|A re-calibration method for object detection with multi-modal alignment bias in autonomous driving]] (83.1% similar)
- [[2025-09-22/Sparse Multiview Open-Vocabulary 3D Detection_20250922|Sparse Multiview Open-Vocabulary 3D Detection]] (82.2% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/3D Object Detection|3D Object Detection]], [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Radar Point Cloud|Radar Point Cloud]], [[keywords/Bird's-eye-view|Bird's-eye-view]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15935v1 Announce Type: new 
Abstract: Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar fusion for the 3D object detection task in real-time under adverse weather and lighting conditions. However, currently, in the literature, it is possible to find few works focusing on this modality and, most importantly, developing new architectures to explore the advantages of the radar point cloud, such as accurate distance estimation and speed information. Therefore, this work presents a novel and efficient 3D object detection algorithm using cameras and radars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of radar before fusing the features into a detection head. A new backbone is introduced, which maps the radar pillar features into an embedded dimension. A self-attention mechanism allows the backbone to model the dependencies between the radar points. We are using a simplified convolutional layer to replace the FPN-based convolutional layers used in the PointPillars-based architectures with the main goal of reducing inference time. Our results show that with this modification, our approach achieves the new state-of-the-art in the 3D object detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50, while also setting a new benchmark for inference time on the nuScenes dataset for the same category.

## 🔍 Abstract (한글 번역)

arXiv:2509.15935v1 발표 유형: 신규  
초록: 카메라-레이더 융합은 악천후 및 조명 조건 하에서 실시간 3D 객체 탐지 작업을 위한 카메라-라이더 융합의 견고하고 저비용 대안입니다. 그러나 현재 문헌에서는 이 모달리티에 초점을 맞추고, 특히 레이더 포인트 클라우드의 장점인 정확한 거리 추정 및 속도 정보를 탐구하기 위한 새로운 아키텍처를 개발하는 연구를 거의 찾을 수 없습니다. 따라서 본 연구는 조감도(BEV)에서 카메라와 레이더를 사용한 새로운 효율적인 3D 객체 탐지 알고리즘을 제안합니다. 본 알고리즘은 탐지 헤드에 특징을 융합하기 전에 레이더의 장점을 활용합니다. 레이더 필러 특징을 임베디드 차원으로 매핑하는 새로운 백본을 도입했습니다. 셀프 어텐션 메커니즘은 백본이 레이더 포인트 간의 의존성을 모델링할 수 있도록 합니다. 우리는 추론 시간을 줄이기 위한 주요 목표로 PointPillars 기반 아키텍처에서 사용되는 FPN 기반 컨볼루션 레이어를 대체하기 위해 단순화된 컨볼루션 레이어를 사용하고 있습니다. 우리의 결과는 이 수정으로 인해 ResNet-50을 사용할 때 NDS 메트릭에서 58.2를 달성하여 3D 객체 탐지 문제에서 새로운 최첨단 성과를 달성했으며, 같은 범주에 대해 nuScenes 데이터셋에서 추론 시간에 대한 새로운 벤치마크를 설정했음을 보여줍니다.

## 📝 요약

이 논문은 악천후와 조명 조건에서도 실시간으로 3D 객체 탐지를 수행하는 카메라-레이더 융합 알고리즘을 제안합니다. 기존 연구에서는 카메라-라이다 융합이 주로 사용되었으나, 이 연구는 레이더의 거리 및 속도 정보의 장점을 활용하여 새로운 백본과 자가 주의 메커니즘을 도입했습니다. 이를 통해 레이더 포인트 간의 의존성을 모델링하고, 간소화된 합성곱 층을 사용하여 추론 시간을 단축했습니다. 결과적으로, ResNet-50을 사용한 경우 NDS 지표에서 58.2를 기록하며 nuScenes 데이터셋에서 새로운 기준을 세웠습니다.

## 🎯 주요 포인트

- 1. 카메라-레이더 융합은 악천후와 조명 조건에서도 실시간 3D 객체 탐지를 위한 견고하고 저비용의 대안으로 제시됩니다.
- 2. 본 연구는 레이더 포인트 클라우드의 정확한 거리 추정 및 속도 정보의 장점을 활용하는 새로운 3D 객체 탐지 알고리즘을 제안합니다.
- 3. 새로운 백본 구조를 도입하여 레이더 필러 특징을 임베디드 차원으로 매핑하고, 셀프 어텐션 메커니즘을 통해 레이더 포인트 간의 의존성을 모델링합니다.
- 4. 간소화된 합성곱 층을 사용하여 추론 시간을 줄이고, ResNet-50을 사용하여 NDS 메트릭에서 58.2를 기록하며 새로운 벤치마크를 설정합니다.
- 5. 본 접근법은 nuScenes 데이터셋에서 추론 시간에 대한 새로운 기준을 세우며, 3D 객체 탐지 문제에서 최신 상태의 성능을 달성합니다.


---

*Generated on 2025-09-23 12:15:34*