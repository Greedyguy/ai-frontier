---
keywords:
  - Self-supervised Learning
  - Contrastive Learning
  - Consistency Regularization
  - Multimodal Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.16170
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:23:10.478617",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Contrastive Learning",
    "Consistency Regularization",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.8,
    "Contrastive Learning": 0.75,
    "Consistency Regularization": 0.7,
    "Multimodal Learning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Self-supervised compensation",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "HSSC"
        ],
        "category": "specific_connectable",
        "rationale": "The concept of self-supervised compensation aligns closely with self-supervised learning techniques, enhancing connectivity with existing research in this area.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Modality-invariant contrastive learning",
        "canonical": "Contrastive Learning",
        "aliases": [
          "contrastive modality learning"
        ],
        "category": "specific_connectable",
        "rationale": "This technique is a specific application of contrastive learning, which is a well-connected concept in machine learning literature.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      },
      {
        "surface": "Hybrid consistency constraint",
        "canonical": "Consistency Regularization",
        "aliases": [
          "hybrid constraint"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a novel approach to ensuring model stability across modalities, making it a unique contribution to the field.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Cross-modal fusion",
        "canonical": "Multimodal Learning",
        "aliases": [
          "cross-modal integration"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-modal fusion is a key aspect of multimodal learning, facilitating connections with other research in integrating multiple data modalities.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.68,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "modality reconstruction",
      "reverse attention adapter"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Self-supervised compensation",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Modality-invariant contrastive learning",
      "resolved_canonical": "Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Hybrid consistency constraint",
      "resolved_canonical": "Consistency Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Cross-modal fusion",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.68,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation

**Korean Title:** UniMRSeg: ê³„ì¸µì  ìê°€ ì§€ë„ ë³´ìƒì„ í†µí•œ í†µí•© ëª¨ë‹¬ë¦¬í‹°-ë¦´ë™ìŠ¤ ì„¸ë¶„í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16170.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.16170](https://arxiv.org/abs/2509.16170)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/SLaM-DiMM_ Shared Latent Modeling for Diffusion Based Missing Modality Synthesis in MRI_20250922|SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality Synthesis in MRI]] (83.8% similar)
- [[2025-09-22/FMD-TransUNet_ Abdominal Multi-Organ Segmentation Based on Frequency Domain Multi-Axis Representation Learning and Dual Attention Mechanisms_20250922|FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency Domain Multi-Axis Representation Learning and Dual Attention Mechanisms]] (83.6% similar)
- [[2025-09-22/Uncertainty-Gated Deformable Network for Breast Tumor Segmentation in MR Images_20250922|Uncertainty-Gated Deformable Network for Breast Tumor Segmentation in MR Images]] (83.5% similar)
- [[2025-09-22/UNIV_ Unified Foundation Model for Infrared and Visible Modalities_20250922|UNIV: Unified Foundation Model for Infrared and Visible Modalities]] (83.2% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Contrastive Learning|Contrastive Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Consistency Regularization|Consistency Regularization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16170v1 Announce Type: new 
Abstract: Multi-modal image segmentation faces real-world deployment challenges from incomplete/corrupted modalities degrading performance. While existing methods address training-inference modality gaps via specialized per-combination models, they introduce high deployment costs by requiring exhaustive model subsets and model-modality matching. In this work, we propose a unified modality-relax segmentation network (UniMRSeg) through hierarchical self-supervised compensation (HSSC). Our approach hierarchically bridges representation gaps between complete and incomplete modalities across input, feature and output levels. %
First, we adopt modality reconstruction with the hybrid shuffled-masking augmentation, encouraging the model to learn the intrinsic modality characteristics and generate meaningful representations for missing modalities through cross-modal fusion. %
Next, modality-invariant contrastive learning implicitly compensates the feature space distance among incomplete-complete modality pairs. Furthermore, the proposed lightweight reverse attention adapter explicitly compensates for the weak perceptual semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid consistency constraint to ensure stable prediction under all modality combinations without large performance fluctuations. Without bells and whistles, UniMRSeg significantly outperforms the state-of-the-art methods under diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D semantic segmentation, RGB-D/T salient object segmentation. The code will be released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16170v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë‹¬ ì´ë¯¸ì§€ ë¶„í• ì€ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì†ìƒëœ ëª¨ë‹¬ë¦¬í‹°ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ë°°í¬ ë¬¸ì œì— ì§ë©´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¡°í•©ë³„ íŠ¹í™” ëª¨ë¸ì„ í†µí•´ í›ˆë ¨-ì¶”ë¡  ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ì°¨ì´ë¥¼ í•´ê²°í•˜ì§€ë§Œ, ì´ëŠ” ëª¨ë¸ í•˜ìœ„ ì§‘í•©ê³¼ ëª¨ë¸-ëª¨ë‹¬ë¦¬í‹° ë§¤ì¹­ì„ í•„ìš”ë¡œ í•˜ì—¬ ë†’ì€ ë°°í¬ ë¹„ìš©ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³„ì¸µì  ìê¸° ì§€ë„ ë³´ìƒì„ í†µí•œ í†µí•© ëª¨ë‹¬ë¦¬í‹° ì™„í™” ë¶„í•  ë„¤íŠ¸ì›Œí¬(UniMRSeg)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ì…ë ¥, íŠ¹ì§•, ì¶œë ¥ ìˆ˜ì¤€ì—ì„œ ì™„ì „í•œ ëª¨ë‹¬ë¦¬í‹°ì™€ ë¶ˆì™„ì „í•œ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ í‘œí˜„ ê²©ì°¨ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.  
ìš°ì„ , í•˜ì´ë¸Œë¦¬ë“œ ì…”í”Œ ë§ˆìŠ¤í‚¹ ì¦ê°•ì„ ì‚¬ìš©í•œ ëª¨ë‹¬ë¦¬í‹° ì¬êµ¬ì„±ì„ ì±„íƒí•˜ì—¬ ëª¨ë¸ì´ ë³¸ì§ˆì ì¸ ëª¨ë‹¬ë¦¬í‹° íŠ¹ì„±ì„ í•™ìŠµí•˜ê³  êµì°¨ ëª¨ë‹¬ ìœµí•©ì„ í†µí•´ ëˆ„ë½ëœ ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.  
ë‹¤ìŒìœ¼ë¡œ, ëª¨ë‹¬ë¦¬í‹° ë¶ˆë³€ ëŒ€ì¡° í•™ìŠµì€ ë¶ˆì™„ì „-ì™„ì „ ëª¨ë‹¬ë¦¬í‹° ìŒ ê°„ì˜ íŠ¹ì§• ê³µê°„ ê±°ë¦¬ë¥¼ ì•”ë¬µì ìœ¼ë¡œ ë³´ìƒí•©ë‹ˆë‹¤. ë˜í•œ, ì œì•ˆëœ ê²½ëŸ‰ ì—­ ì£¼ì˜ ì–´ëŒ‘í„°ëŠ” ê³ ì •ëœ ì¸ì½”ë”ì—ì„œ ì•½í•œ ì§€ê°ì  ì˜ë¯¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ìƒí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, UniMRSegëŠ” ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ì¡°í•©ì—ì„œ í° ì„±ëŠ¥ ë³€ë™ ì—†ì´ ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ í•˜ì´ë¸Œë¦¬ë“œ ì¼ê´€ì„± ì œì•½ í•˜ì— ë¯¸ì„¸ ì¡°ì •ë©ë‹ˆë‹¤. íŠ¹ë³„í•œ ê¸°êµ ì—†ì´ UniMRSegëŠ” MRI ê¸°ë°˜ ë‡Œì¢…ì–‘ ë¶„í• , RGB-D ì˜ë¯¸ ë¶„í• , RGB-D/T ì£¼ëª© ê°ì²´ ë¶„í• ì—ì„œ ë‹¤ì–‘í•œ ëˆ„ë½ ëª¨ë‹¬ë¦¬í‹° ì‹œë‚˜ë¦¬ì˜¤ í•˜ì—ì„œ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ì„ í¬ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/Xiaoqi-Zhao-DLUT/UniMRSegì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ì´ë¯¸ì§€ ë¶„í• ì˜ ì‹¤ì œ ì ìš©ì—ì„œ ë°œìƒí•˜ëŠ” ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì†ìƒëœ ëª¨ë‹¬ë¦¬í‹°ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ UniMRSegë¼ëŠ” í†µí•© ëª¨ë‹¬ë¦¬í‹° ì™„í™” ë¶„í•  ë„¤íŠ¸ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ë¡ ì€ ê³„ì¸µì  ìê¸° ì§€ë„ ë³´ìƒì„ í†µí•´ ì…ë ¥, íŠ¹ì§•, ì¶œë ¥ ìˆ˜ì¤€ì—ì„œ ì™„ì „í•œ ëª¨ë‹¬ë¦¬í‹°ì™€ ë¶ˆì™„ì „í•œ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ í‘œí˜„ ê²©ì°¨ë¥¼ ì¤„ì…ë‹ˆë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ì…”í”Œ ë§ˆìŠ¤í‚¹ ì¦ê°•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹° ì¬êµ¬ì„±ì„ ìˆ˜í–‰í•˜ê³ , ëª¨ë‹¬ë¦¬í‹° ë¶ˆë³€ ëŒ€ì¡° í•™ìŠµê³¼ ê²½ëŸ‰ ì—­ ì£¼ì˜ ì–´ëŒ‘í„°ë¥¼ í†µí•´ íŠ¹ì§• ê³µê°„ì˜ ê±°ë¦¬ë¥¼ ë³´ìƒí•©ë‹ˆë‹¤. ë˜í•œ, í•˜ì´ë¸Œë¦¬ë“œ ì¼ê´€ì„± ì œì•½ì„ í†µí•´ ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ì¡°í•©ì—ì„œ ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ MRI ê¸°ë°˜ ë‡Œì¢…ì–‘ ë¶„í• , RGB-D ì˜ë¯¸ë¡ ì  ë¶„í• , RGB-D/T ì£¼ëª© ê°ì²´ ë¶„í• ì—ì„œ ìµœì‹  ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. UniMRSegëŠ” ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì†ìƒëœ ëª¨ë‹¬ë¦¬í‹°ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³„ì¸µì  ìê°€ ì§€ë„ ë³´ìƒì„ í™œìš©í•œ í†µí•© ëª¨ë‹¬ë¦¬í‹° ë¦´ë™ìŠ¤ ì„¸ë¶„í™” ë„¤íŠ¸ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. í•˜ì´ë¸Œë¦¬ë“œ ì…”í”Œ ë§ˆìŠ¤í‚¹ ì¦ê°•ì„ í†µí•œ ëª¨ë‹¬ë¦¬í‹° ì¬êµ¬ì„±ì„ í†µí•´ ëª¨ë¸ì´ ëª¨ë‹¬ë¦¬í‹°ì˜ ë³¸ì§ˆì  íŠ¹ì„±ì„ í•™ìŠµí•˜ê³  êµì°¨ ëª¨ë‹¬ ìœµí•©ì„ í†µí•´ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
- 3. ëª¨ë‹¬ë¦¬í‹° ë¶ˆë³€ ëŒ€ì¡° í•™ìŠµì€ ë¶ˆì™„ì „-ì™„ì „ ëª¨ë‹¬ë¦¬í‹° ìŒ ê°„ì˜ íŠ¹ì§• ê³µê°„ ê±°ë¦¬ë¥¼ ì•”ë¬µì ìœ¼ë¡œ ë³´ìƒí•©ë‹ˆë‹¤.
- 4. ê²½ëŸ‰ì˜ ì—­ ì£¼ì˜ ì–´ëŒ‘í„°ëŠ” ê³ ì •ëœ ì¸ì½”ë”ì—ì„œ ì•½í•œ ì§€ê° ì˜ë¯¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ìƒí•©ë‹ˆë‹¤.
- 5. UniMRSegëŠ” ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ì¡°í•©ì—ì„œ ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ í•˜ì´ë¸Œë¦¬ë“œ ì¼ê´€ì„± ì œì•½ í•˜ì— ë¯¸ì„¸ ì¡°ì •ë˜ë©°, ë‹¤ì–‘í•œ ëˆ„ë½ ëª¨ë‹¬ë¦¬í‹° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìµœì²¨ë‹¨ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:23:10*