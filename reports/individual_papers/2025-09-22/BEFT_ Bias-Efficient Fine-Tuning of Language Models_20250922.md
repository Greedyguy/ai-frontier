---
keywords:
  - Bias-Efficient Fine-Tuning
  - Large Language Model
  - Parameter-Efficient Fine-Tuning
  - Bias Terms
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15974
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:24:08.933960",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Bias-Efficient Fine-Tuning",
    "Large Language Model",
    "Parameter-Efficient Fine-Tuning",
    "Bias Terms"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Bias-Efficient Fine-Tuning": 0.8,
    "Large Language Model": 0.85,
    "Parameter-Efficient Fine-Tuning": 0.78,
    "Bias Terms": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Bias-Efficient Fine-Tuning",
        "canonical": "Bias-Efficient Fine-Tuning",
        "aliases": [
          "BEFT"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper, crucial for understanding the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's experiments and relevant to a wide range of research in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Parameter-Efficient Fine-Tuning",
        "canonical": "Parameter-Efficient Fine-Tuning",
        "aliases": [
          "PEFT"
        ],
        "category": "specific_connectable",
        "rationale": "A key concept in the paper, connecting to broader discussions of efficiency in model training.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Bias Terms",
        "canonical": "Bias Terms",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Understanding bias terms is essential for grasping the fine-tuning process discussed.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Bias-Efficient Fine-Tuning",
      "resolved_canonical": "Bias-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Parameter-Efficient Fine-Tuning",
      "resolved_canonical": "Parameter-Efficient Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Bias Terms",
      "resolved_canonical": "Bias Terms",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# BEFT: Bias-Efficient Fine-Tuning of Language Models

**Korean Title:** BEFT: ì–¸ì–´ ëª¨ë¸ì˜ í¸í–¥ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15974.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15974](https://arxiv.org/abs/2509.15974)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (84.4% similar)
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (83.3% similar)
- [[2025-09-22/Benchmarking Debiasing Methods for LLM-based Parameter Estimates_20250922|Benchmarking Debiasing Methods for LLM-based Parameter Estimates]] (83.1% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (82.7% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]], [[keywords/Bias Terms|Bias Terms]]
**âš¡ Unique Technical**: [[keywords/Bias-Efficient Fine-Tuning|Bias-Efficient Fine-Tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15974v1 Announce Type: cross 
Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and competitive performance, especially in low-data regimes. Bias-only fine-tuning has the potential for unprecedented parameter efficiency. However, the link between fine-tuning different bias terms (i.e., bias terms in the query, key, or value projections) and downstream performance remains unclear. The existing approaches, e.g., based on the magnitude of bias change or empirical Fisher information, provide limited guidance for selecting the particular bias term for effective fine-tuning. In this paper, we propose an approach for selecting the bias term to be fine-tuned, forming the foundation of our bias-efficient fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against other bias-selection approaches, across a wide range of large language models (LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B parameters. Our results demonstrate the effectiveness and superiority of our bias-efficient approach on diverse downstream tasks, including classification, multiple-choice, and generation tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15974v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëª¨ë“  ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ê¸°ë²• ì¤‘ì—ì„œ íŠ¹íˆ ì €ë°ì´í„° í™˜ê²½ì—ì„œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ì„±ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ìœ¼ë¡œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ë°”ì´ì–´ìŠ¤ ì „ìš© ë¯¸ì„¸ ì¡°ì •ì€ ì „ë¡€ ì—†ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ì œê³µí•  ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì„œë¡œ ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ í•­ëª©(ì¦‰, ì¿¼ë¦¬, í‚¤, ë˜ëŠ” ê°’ í”„ë¡œì ì…˜ì˜ ë°”ì´ì–´ìŠ¤ í•­ëª©)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ ê°„ì˜ ì—°ê²°ì€ ì—¬ì „íˆ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ë²•, ì˜ˆë¥¼ ë“¤ì–´ ë°”ì´ì–´ìŠ¤ ë³€í™”ì˜ í¬ê¸°ë‚˜ ê²½í—˜ì  í”¼ì…” ì •ë³´ì— ê¸°ë°˜í•œ ì ‘ê·¼ë²•ì€ íš¨ê³¼ì ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ íŠ¹ì • ë°”ì´ì–´ìŠ¤ í•­ëª© ì„ íƒì— ëŒ€í•œ ì œí•œëœ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¯¸ì„¸ ì¡°ì •í•  ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(BEFT)ì˜ ê¸°ì´ˆë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” 110Mì—ì„œ 6.7B íŒŒë¼ë¯¸í„°ì— ì´ë¥´ëŠ” ì¸ì½”ë” ì „ìš© ë° ë””ì½”ë” ì „ìš© ì•„í‚¤í…ì²˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ëŒ€ìƒìœ¼ë¡œ ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ ì„ íƒ ì ‘ê·¼ë²•ê³¼ ë¹„êµí•˜ì—¬ ìš°ë¦¬ì˜ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ì ‘ê·¼ë²•ì„ ê´‘ë²”ìœ„í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ë¶„ë¥˜, ë‹¤ì§€ì„ ë‹¤, ìƒì„± ì‘ì—…ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ìš°ë¦¬ì˜ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ì ‘ê·¼ë²•ì˜ íš¨ê³¼ì„±ê³¼ ìš°ìˆ˜ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì •(PEFT) ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ ëª¨ë“  ë°”ì´ì–´ìŠ¤ í•­ëª©ì˜ ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, ë°ì´í„°ê°€ ì ì€ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê³  ì„±ëŠ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¿¼ë¦¬, í‚¤, ê°’ í”„ë¡œì ì…˜ì˜ ë°”ì´ì–´ìŠ¤ í•­ëª©ê³¼ ì„±ëŠ¥ ê°„ì˜ ê´€ê³„ëŠ” ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ íŠ¹ì • ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ì„ íƒí•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë°”ì´ì–´ìŠ¤ í•­ëª© ì„ íƒì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(BEFT)ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì´ ì ‘ê·¼ë²•ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•œ ê²°ê³¼, ë¶„ë¥˜, ì„ íƒí˜•, ìƒì„± ì‘ì—… ë“± ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëª¨ë“  ë°”ì´ì–´ìŠ¤ í•­ëª©ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì€ ë‚®ì€ ë°ì´í„° í™˜ê²½ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
- 2. ë°”ì´ì–´ìŠ¤ í•­ëª©ë§Œì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì€ ì „ë¡€ ì—†ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ì œê³µí•  ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” íš¨ê³¼ì ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ íŠ¹ì • ë°”ì´ì–´ìŠ¤ í•­ëª© ì„ íƒì„ ìœ„í•œ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ë©°, ì´ë¥¼ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(BEFT)ì˜ ê¸°ì´ˆë¡œ ì‚¼ìŠµë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°”ì´ì–´ìŠ¤ íš¨ìœ¨ì  ì ‘ê·¼ë²•ì€ ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ ì„ íƒ ì ‘ê·¼ë²•ê³¼ ë¹„êµ í‰ê°€ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ íš¨ê³¼ì ì´ê³  ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:24:08*