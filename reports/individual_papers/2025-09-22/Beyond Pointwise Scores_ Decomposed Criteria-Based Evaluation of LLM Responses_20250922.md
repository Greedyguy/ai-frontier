---
keywords:
  - Decomposed Criteria-Based Evaluation
  - Large Language Model
  - Precision in LLM Evaluation
  - Recall in LLM Evaluation
  - Multi-Jurisdictional Reasoning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.16093
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:28:41.075807",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Decomposed Criteria-Based Evaluation",
    "Large Language Model",
    "Precision in LLM Evaluation",
    "Recall in LLM Evaluation",
    "Multi-Jurisdictional Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Decomposed Criteria-Based Evaluation": 0.78,
    "Large Language Model": 0.85,
    "Precision in LLM Evaluation": 0.7,
    "Recall in LLM Evaluation": 0.7,
    "Multi-Jurisdictional Reasoning": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "DeCE",
        "canonical": "Decomposed Criteria-Based Evaluation",
        "aliases": [
          "DeCE"
        ],
        "category": "unique_technical",
        "rationale": "DeCE is a novel evaluation framework introduced in the paper, offering a new approach to LLM evaluation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "LLM",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion, providing a broad technical context.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "precision",
        "canonical": "Precision in LLM Evaluation",
        "aliases": [
          "factual accuracy",
          "relevance"
        ],
        "category": "specific_connectable",
        "rationale": "Precision is a key component of the DeCE framework, crucial for understanding LLM evaluation.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "recall",
        "canonical": "Recall in LLM Evaluation",
        "aliases": [
          "coverage of required concepts"
        ],
        "category": "specific_connectable",
        "rationale": "Recall complements precision in the DeCE framework, essential for comprehensive LLM evaluation.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "multi-jurisdictional reasoning",
        "canonical": "Multi-Jurisdictional Reasoning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is specific to the legal QA task discussed, highlighting the complexity of the evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "BLEU",
      "ROUGE",
      "model-agnostic",
      "domain-general"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "DeCE",
      "resolved_canonical": "Decomposed Criteria-Based Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LLM",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "precision",
      "resolved_canonical": "Precision in LLM Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "recall",
      "resolved_canonical": "Recall in LLM Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "multi-jurisdictional reasoning",
      "resolved_canonical": "Multi-Jurisdictional Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses

**Korean Title:** í¬ì¸íŠ¸ì™€ì´ì¦ˆ ì ìˆ˜ë¥¼ ë„˜ì–´ì„œ: LLM ì‘ë‹µì˜ ë¶„í•´ëœ ê¸°ì¤€ ê¸°ë°˜ í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16093.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.16093](https://arxiv.org/abs/2509.16093)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (83.5% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (83.3% similar)
- [[2025-09-19/LLM Agents at the Roundtable_ A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring_20250919|LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring]] (83.0% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (82.2% similar)
- [[2025-09-22/Calibrating LLM Confidence by Probing Perturbed Representation Stability_20250922|Calibrating LLM Confidence by Probing Perturbed Representation Stability]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Precision in LLM Evaluation|Precision in LLM Evaluation]], [[keywords/Recall in LLM Evaluation|Recall in LLM Evaluation]]
**âš¡ Unique Technical**: [[keywords/Decomposed Criteria-Based Evaluation|Decomposed Criteria-Based Evaluation]], [[keywords/Multi-Jurisdictional Reasoning|Multi-Jurisdictional Reasoning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16093v1 Announce Type: cross 
Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score. We introduce DeCE, a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements. DeCE is model-agnostic and domain-general, requiring no predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate different LLMs on a real-world legal QA task involving multi-jurisdictional reasoning and citation grounding. DeCE achieves substantially stronger correlation with expert judgments ($r=0.78$), compared to traditional metrics ($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Importantly, only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation framework in expert domains.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16093v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë²•ë¥ ì´ë‚˜ ì˜í•™ê³¼ ê°™ì€ ê³ ìœ„í—˜ ë¶„ì•¼ì—ì„œ ì¥ë¬¸ì˜ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ê·¼ë³¸ì ì¸ ë„ì „ ê³¼ì œë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. BLEU ë° ROUGEì™€ ê°™ì€ í‘œì¤€ ì§€í‘œëŠ” ì˜ë¯¸ì  ì •í™•ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•˜ë©°, í˜„ì¬ì˜ LLM ê¸°ë°˜ í‰ê°€ìë“¤ì€ ì¢…ì¢… ë‹µë³€ í’ˆì§ˆì˜ ë¯¸ë¬˜í•œ ì¸¡ë©´ì„ ë‹¨ì¼í•œ ë¹„ì°¨ë³„ì  ì ìˆ˜ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì •ë°€ë„(ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ê´€ë ¨ì„±)ì™€ ì¬í˜„ìœ¨(í•„ìš”í•œ ê°œë…ì˜ í¬ê´„ì„±)ì„ ë¶„ë¦¬í•˜ì—¬, ê³¨ë“œ ë‹µë³€ ìš”êµ¬ì‚¬í•­ì—ì„œ ìë™ìœ¼ë¡œ ì¶”ì¶œí•œ ì‚¬ë¡€ë³„ ê¸°ì¤€ì„ ì‚¬ìš©í•˜ëŠ” ë¶„í•´ëœ LLM í‰ê°€ í”„ë ˆì„ì›Œí¬ì¸ DeCEë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. DeCEëŠ” ì‚¬ì „ ì •ì˜ëœ ë¶„ë¥˜ ì²´ê³„ë‚˜ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì‘ì„±ëœ ë£¨ë¸Œë¦­ì´ í•„ìš” ì—†ëŠ” ëª¨ë¸ ë¶ˆê°€ì§€ë¡ ì ì´ë©° ë„ë©”ì¸ ì¼ë°˜ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì¤‘ ê´€í• ê¶Œì  ì¶”ë¡ ê³¼ ì¸ìš© ê·¼ê±°ë¥¼ í¬í•¨í•˜ëŠ” ì‹¤ì œ ë²•ë¥  QA ì‘ì—…ì—ì„œ ë‹¤ì–‘í•œ LLMì„ í‰ê°€í•˜ê¸° ìœ„í•´ DeCEë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. DeCEëŠ” ì „í†µì ì¸ ì§€í‘œ($r=0.12$), ì ë³„ LLM ì ìˆ˜($r=0.35$), í˜„ëŒ€ ë‹¤ì°¨ì› í‰ê°€ì($r=0.48$)ì— ë¹„í•´ ì „ë¬¸ê°€ íŒë‹¨ê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ìƒë‹¹íˆ ê°•í•©ë‹ˆë‹¤($r=0.78$). ë˜í•œ í•´ì„ ê°€ëŠ¥í•œ ì ˆì¶©ì ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤: ì¼ë°˜ ëª¨ë¸ì€ ì¬í˜„ìœ¨ì„ ì„ í˜¸í•˜ëŠ” ë°˜ë©´, ì „ë¬¸ ëª¨ë¸ì€ ì •ë°€ë„ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì ì€, LLMì´ ìƒì„±í•œ ê¸°ì¤€ ì¤‘ ì „ë¬¸ê°€ ìˆ˜ì •ì´ í•„ìš”í•œ ê²ƒì€ ë‹¨ 11.95%ì— ë¶ˆê³¼í•˜ì—¬ DeCEì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. DeCEëŠ” ì „ë¬¸ê°€ ë„ë©”ì¸ì—ì„œ í•´ì„ ê°€ëŠ¥í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ LLM í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë²•ë¥  ë° ì˜í•™ê³¼ ê°™ì€ ê³ ìœ„í—˜ ë¶„ì•¼ì—ì„œ ì¥ë¬¸ì˜ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ë° ìˆì–´ ê¸°ì¡´ì˜ BLEU ë° ROUGEì™€ ê°™ì€ ì§€í‘œê°€ ì˜ë¯¸ì  ì •í™•ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DeCEë¼ëŠ” í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. DeCEëŠ” ì •ë°€ë„(ì‚¬ì‹¤ ì •í™•ì„±ê³¼ ê´€ë ¨ì„±)ì™€ ì¬í˜„ìœ¨(í•„ìš”í•œ ê°œë…ì˜ í¬ê´„ì„±)ì„ ë¶„ë¦¬í•˜ì—¬ í‰ê°€í•˜ë©°, ì´ëŠ” ê¸ˆ ë‹µë³€ ìš”êµ¬ì‚¬í•­ì—ì„œ ìë™ìœ¼ë¡œ ì¶”ì¶œëœ ê¸°ì¤€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” íŠ¹ì • ëª¨ë¸ì´ë‚˜ ë¶„ì•¼ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ì‚¬ì „ ì •ì˜ëœ ë¶„ë¥˜ ì²´ê³„ë‚˜ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì‘ì„±ëœ ê¸°ì¤€ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤ì œ ë²•ë¥  QA ì‘ì—…ì— ì ìš©í•œ ê²°ê³¼, DeCEëŠ” ì „ë¬¸ê°€ íŒë‹¨ê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ê¸°ì¡´ ì§€í‘œë³´ë‹¤ í›¨ì”¬ ë†’ì•˜ìœ¼ë©°, í•´ì„ ê°€ëŠ¥í•œ í‰ê°€ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. DeCEëŠ” ì „ë¬¸ê°€ ë„ë©”ì¸ì—ì„œ í•´ì„ ê°€ëŠ¥í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DeCEëŠ” ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ ë¶„ë¦¬í•˜ì—¬ í‰ê°€í•˜ëŠ” LLM í‰ê°€ í”„ë ˆì„ì›Œí¬ë¡œ, ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ê´€ë ¨ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
- 2. DeCEëŠ” íŠ¹ì • ëª¨ë¸ì´ë‚˜ ë„ë©”ì¸ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ì‚¬ì „ ì •ì˜ëœ ë¶„ë¥˜ ì²´ê³„ë‚˜ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì‘ì„±ëœ ê¸°ì¤€ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 3. ë²•ë¥  ë¶„ì•¼ì˜ ì‹¤ì œ QA ì‘ì—…ì—ì„œ DeCEëŠ” ì „ë¬¸ê°€ íŒë‹¨ê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ë§¤ìš° ë†’ì•„ ê¸°ì¡´ í‰ê°€ ì§€í‘œë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. ì¼ë°˜ ëª¨ë¸ì€ ì¬í˜„ìœ¨ì„, ì „ë¬¸ ëª¨ë¸ì€ ì •ë°€ë„ë¥¼ ì¤‘ì‹œí•˜ëŠ” ê²½í–¥ì„ ë³´ì—¬ì£¼ëŠ” í•´ì„ ê°€ëŠ¥í•œ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.
- 5. LLMì´ ìƒì„±í•œ ê¸°ì¤€ ì¤‘ ì „ë¬¸ê°€ ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš°ëŠ” 11.95%ì— ë¶ˆê³¼í•˜ì—¬ DeCEì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:28:41*