---
keywords:
  - Large Language Model
  - NVIDIA H100/H200
  - AMD MI250
  - Parallelism Strategies
  - Activation Recomputation
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.10371
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:23:35.456602",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "NVIDIA H100/H200",
    "AMD MI250",
    "Parallelism Strategies",
    "Activation Recomputation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "NVIDIA H100/H200": 0.75,
    "AMD MI250": 0.72,
    "Parallelism Strategies": 0.8,
    "Activation Recomputation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on distributed training and efficiency analysis.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "NVIDIA H100/H200",
        "canonical": "NVIDIA H100/H200",
        "aliases": [
          "NVIDIA GPUs",
          "H100",
          "H200"
        ],
        "category": "unique_technical",
        "rationale": "Specific hardware platforms analyzed in the study, relevant for linking to hardware-focused research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "AMD MI250",
        "canonical": "AMD MI250",
        "aliases": [
          "AMD GPUs",
          "MI250"
        ],
        "category": "unique_technical",
        "rationale": "Another specific hardware platform analyzed, important for discussions on hardware efficiency.",
        "novelty_score": 0.68,
        "connectivity_score": 0.58,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Parallelism Strategies",
        "canonical": "Parallelism Strategies",
        "aliases": [
          "Parallelism",
          "Parallel Strategies"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in distributed training, relevant for linking to optimization techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Activation Recomputation",
        "canonical": "Activation Recomputation",
        "aliases": [
          "Recomputation",
          "Activation Checkpointing"
        ],
        "category": "specific_connectable",
        "rationale": "Optimization technique discussed in the paper, relevant for performance improvement links.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "Performance",
      "Efficiency",
      "Thermal"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "NVIDIA H100/H200",
      "resolved_canonical": "NVIDIA H100/H200",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "AMD MI250",
      "resolved_canonical": "AMD MI250",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.58,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Parallelism Strategies",
      "resolved_canonical": "Parallelism Strategies",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Activation Recomputation",
      "resolved_canonical": "Activation Recomputation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective

**Korean Title:** ë¶„ì‚° í•™ìŠµì˜ íš¨ìœ¨ì„± íŠ¹ì„±í™”: ì „ë ¥, ì„±ëŠ¥ ë° ì—´ ê´€ì ì—ì„œ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.10371.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.10371](https://arxiv.org/abs/2509.10371)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (84.9% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (84.1% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (83.7% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (83.6% similar)
- [[2025-09-19/{\lambda}Scale_ Enabling Fast Scaling for Serverless Large Language Model Inference_20250919|{\lambda}Scale: Enabling Fast Scaling for Serverless Large Language Model Inference]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Parallelism Strategies|Parallelism Strategies]], [[keywords/Activation Recomputation|Activation Recomputation]]
**âš¡ Unique Technical**: [[keywords/NVIDIA H100/H200|NVIDIA H100/H200]], [[keywords/AMD MI250|AMD MI250]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.10371v2 Announce Type: replace-cross 
Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at https://github.com/sitar-lab/CharLLM-PPT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.10371v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê¸‰ì†í•œ í™•ì¥ì€ ë‹¨ì¼ ë…¸ë“œ ë¶„ì„ì˜ í•œê³„ë¥¼ í›¨ì”¬ ë„˜ì–´ì„œëŠ” í›ˆë ¨ ì‘ì—…ì„ ìš”êµ¬í•˜ë©°, ì´ëŸ¬í•œ ëª¨ë¸ì´ ëŒ€ê·œëª¨, ë‹¤ì¤‘ GPU ì‹œìŠ¤í…œì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NVIDIA H100/H200 ë° AMD MI250 GPUë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ì—… ë¶€í•˜ì™€ í•˜ë“œì›¨ì–´ í”Œë«í¼ ì „ë°˜ì— ê±¸ì³ LLM í›ˆë ¨ì˜ í¬ê´„ì ì¸ íŠ¹ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í…ì„œ, íŒŒì´í”„ë¼ì¸, ë°ì´í„°, ì „ë¬¸ê°€ ë“± ë‹¤ì–‘í•œ ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ í•˜ì—ì„œ ë°€ì§‘ ë° í¬ì†Œ ëª¨ë¸ì„ ë¶„ì„í•˜ê³ , ì´ë“¤ì´ í•˜ë“œì›¨ì–´ í™œìš©ë„, ì „ë ¥ ì†Œë¹„ ë° ì—´ì  í–‰ë™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ í™œì„±í™” ì¬ê³„ì‚° ë° ê³„ì‚°-í†µì‹  ì¤‘ì²©ê³¼ ê°™ì€ ìµœì í™”ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ì„±ëŠ¥ì´ ë‹¨ìˆœíˆ í•˜ë“œì›¨ì–´ ìš©ëŸ‰ í™•ì¥ì— ì˜í•´ ê²°ì •ë˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë©”ëª¨ë¦¬ê°€ ë” í° GPUë¥¼ ì ê²Œ ì‚¬ìš©í•˜ëŠ” ìŠ¤ì¼€ì¼ì—… ì‹œìŠ¤í…œì€ í†µì‹ ì— ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œ ìŠ¤ì¼€ì¼ì•„ì›ƒ ì‹œìŠ¤í…œë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ì‹ ì¤‘í•˜ê²Œ ì¡°ì •ëœ êµ¬ì„± í•˜ì—ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ìŠ¤ì¼€ì¼ì•„ì›ƒ ë°°í¬ê°€ ë” ìš°ìˆ˜í•œ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ë˜í•œ í…ì„œì™€ íŒŒì´í”„ë¼ì¸ê³¼ ê°™ì€ íŠ¹ì • ë³‘ë ¬ ì²˜ë¦¬ ì¡°í•©ì€ ë¹„íš¨ìœ¨ì ì¸ ë°ì´í„° ì²­í‚¹ìœ¼ë¡œ ì¸í•´ ëŒ€ì—­í­ í™œìš©ë„ê°€ ë‚®ì•„ì§€ë©°, ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸°ë¥¼ ì¼ì • ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ë©´ ê¸‰ì‘ìŠ¤ëŸ¬ìš´ ì‹¤í–‰ê³¼ í”¼í¬ ì „ë ¥ ê¸‰ì¦ì„ ìœ ë°œí•˜ì—¬ ì—´ ìŠ¤ë¡œí‹€ë§ì„ ì•…í™”ì‹œí‚µë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ë ¥ì€ í•˜ë“œì›¨ì–´, ì‹œìŠ¤í…œ í† í´ë¡œì§€ ë° ëª¨ë¸ ì‹¤í–‰ ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì´ í›ˆë ¨ ì„±ëŠ¥ì„ ì–´ë–»ê²Œ í˜•ì„±í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¯¸ë˜ì˜ LLM ì‹œìŠ¤í…œ ë° ì‘ì—… ë¶€í•˜ì˜ í™•ì¥ì„±ê³¼ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì‹œìŠ¤í…œ ë° í•˜ë“œì›¨ì–´ ì„¤ê³„ì— ëŒ€í•œ ê¶Œì¥ ì‚¬í•­ì„ ì œì‹œí•˜ë©° ê²°ë¡ ì„ ë§ºìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ì˜ ì†ŒìŠ¤ ì½”ë“œëŠ” https://github.com/sitar-lab/CharLLM-PPTì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í›ˆë ¨ì„ ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ì—… ë¶€í•˜ì™€ í•˜ë“œì›¨ì–´ í”Œë«í¼(NVIDIA H100/H200, AMD MI250 GPU í¬í•¨)ì—ì„œ í¬ê´„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ë°€ì§‘ ë° í¬ì†Œ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë³‘ë ¬í™” ì „ëµ(í…ì„œ, íŒŒì´í”„ë¼ì¸, ë°ì´í„°, ì „ë¬¸ê°€) í•˜ì— ë¶„ì„í•˜ì—¬ í•˜ë“œì›¨ì–´ í™œìš©, ì „ë ¥ ì†Œë¹„, ì—´ ê±°ë™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ í™œì„±í™” ì¬ê³„ì‚° ë° ê³„ì‚°-í†µì‹  ì¤‘ì²©ê³¼ ê°™ì€ ìµœì í™”ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ì‚¬í•­ìœ¼ë¡œëŠ”, í•˜ë“œì›¨ì–´ ìš©ëŸ‰ì˜ í™•ì¥ë§Œìœ¼ë¡œ ì„±ëŠ¥ì´ ê²°ì •ë˜ì§€ ì•Šìœ¼ë©°, ê³ ìš©ëŸ‰ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤ì¼€ì¼ì—… ì‹œìŠ¤í…œì´ í†µì‹ ì— ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ íŠ¹ì • ë³‘ë ¬í™” ì¡°í•©ì´ ëŒ€ì—­í­ í™œìš©ì„ ì €í•´í•  ìˆ˜ ìˆìœ¼ë©°, ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸°ì˜ ì¦ê°€ê°€ ì—´ ìŠ¤ë¡œí‹€ë§ì„ ì•…í™”ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ì€ í•˜ë“œì›¨ì–´, ì‹œìŠ¤í…œ í† í´ë¡œì§€, ëª¨ë¸ ì‹¤í–‰ ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì´ í›ˆë ¨ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í–¥í›„ LLM ì‹œìŠ¤í…œê³¼ ì‘ì—… ë¶€í•˜ì˜ í™•ì¥ì„±ê³¼ ì‹ ë¢°ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ ë° í•˜ë“œì›¨ì–´ ì„¤ê³„ì— ëŒ€í•œ ê¶Œì¥ ì‚¬í•­ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì€ ë‹¨ì¼ ë…¸ë“œ ë¶„ì„ì˜ í•œê³„ë¥¼ ë„˜ì–´ ë‹¤ì¤‘ GPU ì‹œìŠ¤í…œì—ì„œì˜ ëª¨ë¸ ë™ì‘ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- 2. ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ì—… ë¶€í•˜ì™€ í•˜ë“œì›¨ì–´ í”Œë«í¼ì—ì„œ LLM í›ˆë ¨ì„ ë¶„ì„í•˜ì—¬ í•˜ë“œì›¨ì–´ í™œìš©ë„, ì „ë ¥ ì†Œë¹„ ë° ì—´ ê±°ë™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.
- 3. í•˜ë“œì›¨ì–´ ìš©ëŸ‰ í™•ì¥ì´ ì„±ëŠ¥ì„ ê²°ì •í•˜ì§€ ì•Šìœ¼ë©°, íŠ¹ì • ì¡°ê±´ì—ì„œ ì†Œìˆ˜ì˜ ê³ ìš©ëŸ‰ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹œìŠ¤í…œì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. í…ì„œì™€ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì¡°í•©ì€ ë¹„íš¨ìœ¨ì ì¸ ë°ì´í„° ì²­í‚¹ìœ¼ë¡œ ëŒ€ì—­í­ í™œìš©ì„ ì €í•´í•  ìˆ˜ ìˆìœ¼ë©°, ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ ì—´ ìŠ¤ë¡œí‹€ë§ì„ ì•…í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. í–¥í›„ LLM ì‹œìŠ¤í…œì˜ í™•ì¥ì„±ê³¼ ì‹ ë¢°ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ ë° í•˜ë“œì›¨ì–´ ì„¤ê³„ì— ëŒ€í•œ ê¶Œì¥ ì‚¬í•­ì„ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:23:35*