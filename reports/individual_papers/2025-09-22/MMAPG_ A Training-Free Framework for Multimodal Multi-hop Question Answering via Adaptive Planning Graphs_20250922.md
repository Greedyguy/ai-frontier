---
keywords:
  - Multimodal Learning
  - Adaptive Planning Graph
  - Vision-Language Model
  - Retrieval Augmented Generation
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2508.16051
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:40:27.668038",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Adaptive Planning Graph",
    "Vision-Language Model",
    "Retrieval Augmented Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.8,
    "Adaptive Planning Graph": 0.82,
    "Vision-Language Model": 0.78,
    "Retrieval Augmented Generation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Multi-hop question answering",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal QA",
          "Multimodal Question Answering"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the broader concept of integrating multiple data types, enhancing links to multimodal research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adaptive Planning Graph",
        "canonical": "Adaptive Planning Graph",
        "aliases": [
          "APG"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework that is central to the paper's methodology, offering unique linking potential.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Integration"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents a trending concept in combining visual and textual data, crucial for multimodal applications.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "retrieval and reasoning modules",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG Modules"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of enhancing reasoning with retrieval, relevant for improving QA systems.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.68,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "training-free framework",
      "task-specific training",
      "current state"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Multi-hop question answering",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adaptive Planning Graph",
      "resolved_canonical": "Adaptive Planning Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "retrieval and reasoning modules",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.68,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs

**Korean Title:** MMAPG: ì ì‘í˜• ê³„íš ê·¸ë˜í”„ë¥¼ í†µí•œ ë‹¤ì¤‘ ëª¨ë‹¬ ë‹¤ì¤‘ í™‰ ì§ˆë¬¸ ì‘ë‹µì„ ìœ„í•œ í›ˆë ¨ ë¶ˆí•„ìš” í”„ë ˆì„ì›Œí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.16051.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2508.16051](https://arxiv.org/abs/2508.16051)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (84.0% similar)
- [[2025-09-22/Advances in Multimodal Adaptation and Generalization_ From Traditional Approaches to Foundation Models_20250922|Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models]] (83.7% similar)
- [[2025-09-19/Reconstruction Alignment Improves Unified Multimodal Models_20250919|Reconstruction Alignment Improves Unified Multimodal Models]] (82.7% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (82.4% similar)
- [[2025-09-22/Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models_20250922|Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**âš¡ Unique Technical**: [[keywords/Adaptive Planning Graph|Adaptive Planning Graph]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.16051v2 Announce Type: replace 
Abstract: Multimodal Multi-hop question answering requires integrating information from diverse sources, such as images and texts, to derive answers. Existing methods typically rely on sequential retrieval and reasoning, where each step builds on the previous output. However, this single-path paradigm makes them vulnerable to errors due to misleading intermediate steps. Moreover, developing multimodal models can be computationally expensive, often requiring extensive training. To address these limitations, we propose a training-free framework guided by an Adaptive Planning Graph, which consists of planning, retrieval and reasoning modules. The planning module analyzes the current state of the Adaptive Planning Graph, determines the next action and where to expand the graph, which enables dynamic and flexible exploration of reasoning paths. To handle retrieval of text to unspecified target modalities, we devise modality-specific strategies that dynamically adapt to distinct data types. Our approach preserves the characteristics of multimodal information without costly task-specific training, enabling seamless integration with up-to-date models. Finally, the experiments on MultimodalQA and WebQA show that our approach matches or outperforms existing models that rely on training.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.16051v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë‹¤ì¤‘ëª¨ë‹¬ ë‹¤ì¤‘ë‹¨ê³„ ì§ˆë¬¸ ì‘ë‹µì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì¶œì²˜ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë‹µì„ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ìˆœì°¨ì  ê²€ìƒ‰ ë° ì¶”ë¡ ì— ì˜ì¡´í•˜ë©°, ê° ë‹¨ê³„ëŠ” ì´ì „ ì¶œë ¥ì— ê¸°ë°˜í•˜ì—¬ ì§„í–‰ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë‹¨ì¼ ê²½ë¡œ íŒ¨ëŸ¬ë‹¤ì„ì€ ì¤‘ê°„ ë‹¨ê³„ì—ì„œì˜ ì˜¤ë¥˜ë¡œ ì¸í•´ ì˜¤ë‹µì— ì·¨ì•½í•©ë‹ˆë‹¤. ê²Œë‹¤ê°€, ë‹¤ì¤‘ëª¨ë‹¬ ëª¨ë¸ ê°œë°œì€ ì¢…ì¢… ê´‘ë²”ìœ„í•œ í›ˆë ¨ì´ í•„ìš”í•˜ì—¬ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê³„íš, ê²€ìƒ‰ ë° ì¶”ë¡  ëª¨ë“ˆë¡œ êµ¬ì„±ëœ ì ì‘í˜• ê³„íš ê·¸ë˜í”„ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê³„íš ëª¨ë“ˆì€ ì ì‘í˜• ê³„íš ê·¸ë˜í”„ì˜ í˜„ì¬ ìƒíƒœë¥¼ ë¶„ì„í•˜ê³ , ë‹¤ìŒ í–‰ë™ê³¼ ê·¸ë˜í”„ í™•ì¥ ìœ„ì¹˜ë¥¼ ê²°ì •í•˜ì—¬ ì¶”ë¡  ê²½ë¡œì˜ ë™ì ì´ê³  ìœ ì—°í•œ íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ëª…ì‹œë˜ì§€ ì•Šì€ ëŒ€ìƒ ëª¨ë‹¬ë¦¬í‹°ë¡œì˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ìœ í˜•ì— ë™ì ìœ¼ë¡œ ì ì‘í•˜ëŠ” ëª¨ë‹¬ë¦¬í‹°ë³„ ì „ëµì„ ê³ ì•ˆí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…ë³„ í›ˆë ¨ ì—†ì´ ë‹¤ì¤‘ëª¨ë‹¬ ì •ë³´ì˜ íŠ¹ì„±ì„ ë³´ì¡´í•˜ë©°, ìµœì‹  ëª¨ë¸ê³¼ì˜ ì›í™œí•œ í†µí•©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, MultimodalQA ë° WebQAì— ëŒ€í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì´ í›ˆë ¨ì— ì˜ì¡´í•˜ëŠ” ê¸°ì¡´ ëª¨ë¸ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì–‘í•œ ì†ŒìŠ¤(ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸)ì—ì„œ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë‹µì„ ë„ì¶œí•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ë©€í‹°í™‰ ì§ˆë¬¸ ì‘ë‹µ ì‹œìŠ¤í…œì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ìˆœì°¨ì  ê²€ìƒ‰ê³¼ ì¶”ë¡ ì— ì˜ì¡´í•˜ì—¬ ì¤‘ê°„ ë‹¨ê³„ì˜ ì˜¤ë¥˜ì— ì·¨ì•½í•˜ê³ , ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ê°œë°œì— ë§ì€ ë¹„ìš©ì´ ë“­ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ Adaptive Planning Graphë¥¼ í™œìš©í•œ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ê³„íš, ê²€ìƒ‰, ì¶”ë¡  ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ë™ì ì´ê³  ìœ ì—°í•œ ì¶”ë¡  ê²½ë¡œ íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ìœ„í•œ ëª¨ë‹¬ë¦¬í‹°ë³„ ì „ëµì„ í†µí•´ ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì— ì ì‘í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë¹„ì‹¼ í›ˆë ¨ ì—†ì´ ìµœì‹  ëª¨ë¸ê³¼ì˜ í†µí•©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì‹¤í—˜ ê²°ê³¼ MultimodalQAì™€ WebQAì—ì„œ ê¸°ì¡´ ëª¨ë¸ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë©€í‹°ëª¨ë‹¬ ë©€í‹°í™‰ ì§ˆë¬¸ ì‘ë‹µì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë‹µì„ ë„ì¶œí•´ì•¼ í•œë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ì€ ìˆœì°¨ì  ê²€ìƒ‰ê³¼ ì¶”ë¡ ì— ì˜ì¡´í•˜ì—¬ ì¤‘ê°„ ë‹¨ê³„ì˜ ì˜¤ë¥˜ì— ì·¨ì•½í•˜ë‹¤.
- 3. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” Adaptive Planning Graphë¥¼ í™œìš©í•˜ì—¬ ë™ì ì´ê³  ìœ ì—°í•œ ì¶”ë¡  ê²½ë¡œ íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 4. í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ìœ„í•œ ëª¨ë‹¬ë¦¬í‹°ë³„ ì „ëµì„ í†µí•´ ë°ì´í„° ìœ í˜•ì— ë§ê²Œ ë™ì ìœ¼ë¡œ ì ì‘í•œë‹¤.
- 5. MultimodalQAì™€ WebQA ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ëª¨ë¸ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-23 09:40:27*