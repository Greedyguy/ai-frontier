---
keywords:
  - Large Language Model
  - Supervised Fine-Tuning
  - Model-Informed Dynamic Data Optimization
  - Closed-Loop Learning
  - Dynamic Learning Principles
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2508.21589
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:09:50.652365",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Supervised Fine-Tuning",
    "Model-Informed Dynamic Data Optimization",
    "Closed-Loop Learning",
    "Dynamic Learning Principles"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Supervised Fine-Tuning": 0.78,
    "Model-Informed Dynamic Data Optimization": 0.8,
    "Closed-Loop Learning": 0.77,
    "Dynamic Learning Principles": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on optimizing fine-tuning processes.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "unique_technical",
        "rationale": "Key process discussed for enhancing LLM performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Model-informed dynamic data optimization",
        "canonical": "Model-Informed Dynamic Data Optimization",
        "aliases": [
          "Middo"
        ],
        "category": "unique_technical",
        "rationale": "Describes the novel framework introduced in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Closed-loop learning",
        "canonical": "Closed-Loop Learning",
        "aliases": [
          "Closed-loop optimization"
        ],
        "category": "specific_connectable",
        "rationale": "Represents the iterative process crucial to the framework.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Dynamic learning principles",
        "canonical": "Dynamic Learning Principles",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights the evolving nature of the learning process in the framework.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Model-informed dynamic data optimization",
      "resolved_canonical": "Model-Informed Dynamic Data Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Closed-loop learning",
      "resolved_canonical": "Closed-Loop Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Dynamic learning principles",
      "resolved_canonical": "Dynamic Learning Principles",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning

**Korean Title:** ë¯¸ë„: íë£¨í”„ í•™ìŠµì„ í†µí•œ í–¥ìƒëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ëª¨ë¸ ì •ë³´ ê¸°ë°˜ ë™ì  ë°ì´í„° ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.21589.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2508.21589](https://arxiv.org/abs/2508.21589)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A method for improving multilingual quality and diversity of instruction fine-tuning datasets_20250922|A method for improving multilingual quality and diversity of instruction fine-tuning datasets]] (88.5% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.9% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (86.7% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (85.8% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (85.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Closed-Loop Learning|Closed-Loop Learning]]
**âš¡ Unique Technical**: [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]], [[keywords/Model-Informed Dynamic Data Optimization|Model-Informed Dynamic Data Optimization]], [[keywords/Dynamic Learning Principles|Dynamic Learning Principles]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.21589v3 Announce Type: replace-cross 
Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are coming soon. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.21589v3 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ì§€ë„ í•™ìŠµ ì„¸ë¶€ ì¡°ì •(SFT) ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ê·¼ë³¸ì ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ í•™ìŠµ ë°ì´í„°ì— ì˜ì¡´í•©ë‹ˆë‹¤. ë°ì´í„° ì„ íƒê³¼ ë°ì´í„° í•©ì„±ì€ ë°ì´í„° í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì¼ë°˜ì ì¸ ì „ëµì´ì§€ë§Œ, ê¸°ì¡´ ì ‘ê·¼ë²•ì€ ëª¨ë¸ì˜ ëŠ¥ë ¥ì´ ë°œì „í•¨ì— ë”°ë¼ ì ì‘í•˜ì§€ ëª»í•˜ëŠ” ì •ì  ë°ì´í„°ì…‹ íë ˆì´ì…˜ì˜ í•œê³„ë¥¼ ìì£¼ ì§ë©´í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë¸ ì¸ì‹ ë°ì´í„° ì„ íƒê³¼ ë¬¸ë§¥ ë³´ì¡´ ë°ì´í„° ì •ì œë¥¼ ì‚¬ìš©í•˜ëŠ” ìê°€ ì§„í™” ëª¨ë¸ ì •ë³´ ê¸°ë°˜ ë™ì  ë°ì´í„° ìµœì í™” í”„ë ˆì„ì›Œí¬ì¸ Middoë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì¼íšŒì„± í•„í„°ë§/í•©ì„± ë°©ë²•ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” íì‡„ ë£¨í”„ ìµœì í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤: (1) ìê¸° ì°¸ì¡° ì§„ë‹¨ ëª¨ë“ˆì€ ë³µì¡ì„±(ì†ì‹¤ íŒ¨í„´), ë‹¤ì–‘ì„±(ì„ë² ë”© í´ëŸ¬ìŠ¤í„° ë™ì—­í•™), í’ˆì§ˆ(ìê¸° ì •ë ¬ ì ìˆ˜)ì´ë¼ëŠ” ì„¸ ì¶•ì˜ ëª¨ë¸ ì‹ í˜¸ë¥¼ í†µí•´ ìµœì í™”ë˜ì§€ ì•Šì€ ìƒ˜í”Œì„ ëŠ¥ë™ì ìœ¼ë¡œ ì‹ë³„í•©ë‹ˆë‹¤; (2) ì ì‘í˜• ìµœì í™” ì—”ì§„ì€ ì˜ë¯¸ì  ë¬´ê²°ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìµœì í™”ë˜ì§€ ì•Šì€ ìƒ˜í”Œì„ êµìœ¡ì ìœ¼ë¡œ ê°€ì¹˜ ìˆëŠ” í•™ìŠµ í¬ì¸íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤; (3) ì´ ìµœì í™” ê³¼ì •ì€ ë™ì  í•™ìŠµ ì›ì¹™ì„ í†µí•´ ëª¨ë¸ì˜ ëŠ¥ë ¥ê³¼ í•¨ê»˜ ì§€ì†ì ìœ¼ë¡œ ì§„í™”í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ìš°ë¦¬ì˜ MiddoëŠ” ì‹œë“œ ë°ì´í„°ì˜ í’ˆì§ˆì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ê³ , ì›ë˜ ë°ì´í„°ì…‹ ê·œëª¨ë¥¼ ìœ ì§€í•˜ë©´ì„œ í‰ê·  7.15%ì˜ ì •í™•ë„ í–¥ìƒì„ í†µí•´ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë°ì´í„°ì™€ ëª¨ë¸ì˜ ë™ì  ì¸ê°„-AI ê³µë™ ì§„í™”ë¥¼ í†µí•œ ì§€ì† ê°€ëŠ¥í•œ LLM í•™ìŠµì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ í™•ë¦½í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°ì´í„°ì…‹, ëª¨ë¸ ë° ì½”ë“œëŠ” ê³§ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°ì´í„°ì…‹, ëª¨ë¸ ë° ì½”ë“œëŠ” https://github.com/Word2VecT/Middoì—ì„œ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ê³ í’ˆì§ˆ í›ˆë ¨ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì§€ë„ í•™ìŠµì„ ê°œì„ í•˜ê¸° ìœ„í•´ Middoë¼ëŠ” ë™ì  ë°ì´í„° ìµœì í™” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì •ì  ë°ì´í„° ì„ íƒ ë° í•©ì„± ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, MiddoëŠ” ëª¨ë¸ ì¸ì‹ ë°ì´í„° ì„ íƒê³¼ ë§¥ë½ ë³´ì¡´ ë°ì´í„° ì •ì œë¥¼ í™œìš©í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì‚¼ì¤‘ ì¶• ëª¨ë¸ ì‹ í˜¸ë¥¼ í†µí•´ ë¹„ìµœì  ìƒ˜í”Œì„ ì‹ë³„í•˜ê³ , ì´ë¥¼ êµìœ¡ì ìœ¼ë¡œ ê°€ì¹˜ ìˆëŠ” í›ˆë ¨ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì ì‘í˜• ìµœì í™” ì—”ì§„ì„ í¬í•¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MiddoëŠ” ë°ì´í„° í’ˆì§ˆì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ë©°, LLMì˜ ì„±ëŠ¥ì„ í‰ê·  7.15% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë°ì´í„°ì™€ ëª¨ë¸ì˜ ë™ì  ì¸ê°„-AI ê³µì§„í™”ë¥¼ í†µí•´ ì§€ì† ê°€ëŠ¥í•œ LLM í›ˆë ¨ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MiddoëŠ” ëª¨ë¸ ì¸ì‹ ë°ì´í„° ì„ íƒê³¼ ë¬¸ë§¥ ë³´ì¡´ ë°ì´í„° ì •ì œë¥¼ ì‚¬ìš©í•˜ëŠ” ìê°€ ì§„í™”í˜• ë™ì  ë°ì´í„° ìµœì í™” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì†ì‹¤ íŒ¨í„´, ì„ë² ë”© í´ëŸ¬ìŠ¤í„° ë™ì—­í•™, ìê¸° ì •ë ¬ ì ìˆ˜ë¥¼ í†µí•´ ë¹„ìµœì  ìƒ˜í”Œì„ ì‹ë³„í•˜ëŠ” ìê¸° ì°¸ì¡° ì§„ë‹¨ ëª¨ë“ˆì„ í¬í•¨í•©ë‹ˆë‹¤.
- 3. ì ì‘í˜• ìµœì í™” ì—”ì§„ì€ ë¹„ìµœì  ìƒ˜í”Œì„ êµìœ¡ì ìœ¼ë¡œ ê°€ì¹˜ ìˆëŠ” í›ˆë ¨ í¬ì¸íŠ¸ë¡œ ë³€í™˜í•˜ë©°, ì˜ë¯¸ì  ë¬´ê²°ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 4. MiddoëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì§„í™”í•˜ë©°, í‰ê· ì ìœ¼ë¡œ ì •í™•ë„ë¥¼ 7.15% í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ë™ì  ì¸ê°„-AI ë°ì´í„° ë° ëª¨ë¸ì˜ ê³µë™ ì§„í™”ë¥¼ í†µí•œ ì§€ì† ê°€ëŠ¥í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:09:50*