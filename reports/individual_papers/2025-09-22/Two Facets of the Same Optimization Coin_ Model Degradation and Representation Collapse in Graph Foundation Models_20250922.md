---
keywords:
  - Graph Foundation Models
  - Representation Collapse
  - Model Degradation
  - Few-Shot Learning
  - Zero-Shot Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.08401
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:12:02.103879",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Foundation Models",
    "Representation Collapse",
    "Model Degradation",
    "Few-Shot Learning",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Graph Foundation Models": 0.8,
    "Representation Collapse": 0.75,
    "Model Degradation": 0.72,
    "Few-Shot Learning": 0.78,
    "Zero-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Graph Foundation Models",
        "canonical": "Graph Foundation Models",
        "aliases": [
          "GFM"
        ],
        "category": "unique_technical",
        "rationale": "Graph Foundation Models are a novel concept specific to this paper, providing a unique angle on graph-based learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Representation Collapse",
        "canonical": "Representation Collapse",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term describes a specific issue in model optimization, crucial for understanding the paper's contributions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Model Degradation",
        "canonical": "Model Degradation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Model Degradation is a key challenge addressed in the paper, relevant for linking optimization issues.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is a trending topic and relevant to the paper's experiments, enhancing cross-domain connections.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept that aligns with the paper's focus on generalization capabilities.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "model",
      "optimization",
      "supervision"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Graph Foundation Models",
      "resolved_canonical": "Graph Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Representation Collapse",
      "resolved_canonical": "Representation Collapse",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Model Degradation",
      "resolved_canonical": "Model Degradation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models

**Korean Title:** 동일한 최적화 동전의 두 가지 측면: 그래프 기초 모델에서의 모델 저하와 표현 붕괴

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.08401.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.08401](https://arxiv.org/abs/2509.08401)

## 🔗 유사한 논문
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (82.8% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.6% similar)
- [[2025-09-22/Advances in Multimodal Adaptation and Generalization_ From Traditional Approaches to Foundation Models_20250922|Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models]] (82.3% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (81.8% similar)
- [[2025-09-22/CoDoL_ Conditional Domain Prompt Learning for Out-of-Distribution Generalization_20250922|CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization]] (81.5% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Graph Foundation Models|Graph Foundation Models]], [[keywords/Representation Collapse|Representation Collapse]], [[keywords/Model Degradation|Model Degradation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.08401v4 Announce Type: replace 
Abstract: Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.

## 🔍 Abstract (한글 번역)

arXiv:2509.08401v4 발표 유형: 교체  
초록: LLM의 성공에 영감을 받아, GFM은 다중 도메인 텍스트 속성 그래프로부터 최적의 임베딩 함수를 학습하여 다운스트림 교차 작업 일반화 능력을 목표로 설계되었습니다. 다양한 아키텍처 중에서, 그래프 VQ-MAE는 점점 더 다양해지는 GFM의 환경에서 두드러집니다. 이는 여러 도메인의 토폴로지와 텍스트 속성을 명확한 의미 경계를 가진 이산 임베딩 공간으로 공동 인코딩할 수 있는 능력에 기인합니다. 그 잠재력에도 불구하고, 도메인 일반화 충돌은 미세한 함정을 초래합니다. 본 논문에서는 두 가지를 구체화하며, 이는 마치 동일한 GFM 최적화 동전의 양면과 같습니다 - 측면 1 모델 저하: 인코더와 코드북이 입력의 다양성을 포착하지 못함; 측면 2 표현 붕괴: 숨겨진 임베딩과 코드북 벡터가 좁은 표현 하위 공간의 제약으로 인해 의미적 분리를 유지하지 못함. 이 두 가지 함정(측면)은 디코더를 집합적으로 손상시키고 저품질의 재구성된 감독을 생성하여 사전 훈련 중 GFM 최적화 딜레마(동전)를 초래합니다. 실증적 조사를 통해, 우리는 위의 도전 과제를 정보 병목 현상과 정규화 결핍에 기인한다고 봅니다. 이를 해결하기 위해, 우리는 MoT를 제안합니다 - (1) 두 가지 함정을 위한 정보 조정, 이는 엣지 기반 의미 융합 전략과 도메인 인식 라우팅을 통한 혼합 코드북을 활용하여 정보 용량을 향상시킵니다. (2) 최적화 동전을 위한 정규화 조정, 이는 제안된 정보 조정에서 기울기 감독을 더욱 향상시키기 위해 두 가지 추가 정규화를 활용합니다. 특히, 유연한 아키텍처로서 MoT는 GFM의 확장 법칙을 준수하여 제어 가능한 모델 규모를 제공합니다. SOTA 기준선과 비교하여, 6개 도메인에 걸친 22개 데이터셋에서의 실험은 MoT가 감독, 소수 샷, 제로 샷 시나리오에서 유의미한 개선을 달성함을 보여줍니다.

## 📝 요약

이 논문은 대규모 언어 모델(LLM)의 성공에 영감을 받아, 다중 도메인 텍스트 속성 그래프에서 최적의 임베딩 함수를 학습하여 다양한 작업에 일반화할 수 있는 그래프 기반 모델(GFM)을 제안합니다. 특히, 그래프 VQ-MAE는 여러 도메인의 토폴로지와 텍스트 속성을 명확한 의미적 경계를 가진 이산 임베딩 공간으로 인코딩하는 능력으로 주목받습니다. 그러나 도메인 일반화의 문제로 인해 모델 성능 저하와 표현 붕괴라는 두 가지 문제가 발생합니다. 이를 해결하기 위해 정보 병목과 정규화 결핍을 극복하는 MoT 방법론을 제안합니다. MoT는 정보 용량을 개선하기 위한 엣지 기반 의미 융합 전략과 도메인 인식 라우팅을 사용하는 혼합 코드북을 도입하며, 추가적인 정규화를 통해 최적화 문제를 해결합니다. 실험 결과, MoT는 6개 도메인의 22개 데이터셋에서 기존 최첨단 모델 대비 유의미한 성능 향상을 보였습니다.

## 🎯 주요 포인트

- 1. 그래프 VQ-MAE는 다중 도메인에서의 토폴로지와 텍스트 속성을 명확한 의미 경계로 인코딩하여 GFM에서 두드러집니다.
- 2. 도메인 일반화 갈등으로 인해 인식하기 어려운 문제점이 발생하며, 이는 모델 열화와 표현 붕괴로 나타납니다.
- 3. Information Bottleneck와 Regularization Deficit이 이러한 문제의 원인으로 지목됩니다.
- 4. MoT는 정보 용량을 개선하기 위해 엣지 기반 의미 융합 전략과 도메인 인식 라우팅을 사용하는 혼합 코드북을 제안합니다.
- 5. MoT는 6개 도메인에서 22개의 데이터셋을 대상으로 한 실험에서 SOTA 기준보다 유의미한 성능 향상을 보였습니다.


---

*Generated on 2025-09-23 11:12:02*