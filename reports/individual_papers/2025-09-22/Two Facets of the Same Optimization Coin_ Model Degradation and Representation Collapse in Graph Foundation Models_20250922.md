---
keywords:
  - Graph Foundation Models
  - Representation Collapse
  - Model Degradation
  - Few-Shot Learning
  - Zero-Shot Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.08401
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:12:02.103879",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Foundation Models",
    "Representation Collapse",
    "Model Degradation",
    "Few-Shot Learning",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Graph Foundation Models": 0.8,
    "Representation Collapse": 0.75,
    "Model Degradation": 0.72,
    "Few-Shot Learning": 0.78,
    "Zero-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Graph Foundation Models",
        "canonical": "Graph Foundation Models",
        "aliases": [
          "GFM"
        ],
        "category": "unique_technical",
        "rationale": "Graph Foundation Models are a novel concept specific to this paper, providing a unique angle on graph-based learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Representation Collapse",
        "canonical": "Representation Collapse",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This term describes a specific issue in model optimization, crucial for understanding the paper's contributions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Model Degradation",
        "canonical": "Model Degradation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Model Degradation is a key challenge addressed in the paper, relevant for linking optimization issues.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is a trending topic and relevant to the paper's experiments, enhancing cross-domain connections.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept that aligns with the paper's focus on generalization capabilities.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "model",
      "optimization",
      "supervision"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Graph Foundation Models",
      "resolved_canonical": "Graph Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Representation Collapse",
      "resolved_canonical": "Representation Collapse",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Model Degradation",
      "resolved_canonical": "Model Degradation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models

**Korean Title:** ë™ì¼í•œ ìµœì í™” ë™ì „ì˜ ë‘ ê°€ì§€ ì¸¡ë©´: ê·¸ë˜í”„ ê¸°ì´ˆ ëª¨ë¸ì—ì„œì˜ ëª¨ë¸ ì €í•˜ì™€ í‘œí˜„ ë¶•ê´´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.08401.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.08401](https://arxiv.org/abs/2509.08401)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (82.8% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.6% similar)
- [[2025-09-22/Advances in Multimodal Adaptation and Generalization_ From Traditional Approaches to Foundation Models_20250922|Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models]] (82.3% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (81.8% similar)
- [[2025-09-22/CoDoL_ Conditional Domain Prompt Learning for Out-of-Distribution Generalization_20250922|CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Graph Foundation Models|Graph Foundation Models]], [[keywords/Representation Collapse|Representation Collapse]], [[keywords/Model Degradation|Model Degradation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.08401v4 Announce Type: replace 
Abstract: Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.08401v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: LLMì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„, GFMì€ ë‹¤ì¤‘ ë„ë©”ì¸ í…ìŠ¤íŠ¸ ì†ì„± ê·¸ë˜í”„ë¡œë¶€í„° ìµœì ì˜ ì„ë² ë”© í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ êµì°¨ ì‘ì—… ì¼ë°˜í™” ëŠ¥ë ¥ì„ ëª©í‘œë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì¤‘ì—ì„œ, ê·¸ë˜í”„ VQ-MAEëŠ” ì ì  ë” ë‹¤ì–‘í•´ì§€ëŠ” GFMì˜ í™˜ê²½ì—ì„œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ ë„ë©”ì¸ì˜ í† í´ë¡œì§€ì™€ í…ìŠ¤íŠ¸ ì†ì„±ì„ ëª…í™•í•œ ì˜ë¯¸ ê²½ê³„ë¥¼ ê°€ì§„ ì´ì‚° ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ê³µë™ ì¸ì½”ë”©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì— ê¸°ì¸í•©ë‹ˆë‹¤. ê·¸ ì ì¬ë ¥ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë„ë©”ì¸ ì¼ë°˜í™” ì¶©ëŒì€ ë¯¸ì„¸í•œ í•¨ì •ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ êµ¬ì²´í™”í•˜ë©°, ì´ëŠ” ë§ˆì¹˜ ë™ì¼í•œ GFM ìµœì í™” ë™ì „ì˜ ì–‘ë©´ê³¼ ê°™ìŠµë‹ˆë‹¤ - ì¸¡ë©´ 1 ëª¨ë¸ ì €í•˜: ì¸ì½”ë”ì™€ ì½”ë“œë¶ì´ ì…ë ¥ì˜ ë‹¤ì–‘ì„±ì„ í¬ì°©í•˜ì§€ ëª»í•¨; ì¸¡ë©´ 2 í‘œí˜„ ë¶•ê´´: ìˆ¨ê²¨ì§„ ì„ë² ë”©ê³¼ ì½”ë“œë¶ ë²¡í„°ê°€ ì¢ì€ í‘œí˜„ í•˜ìœ„ ê³µê°„ì˜ ì œì•½ìœ¼ë¡œ ì¸í•´ ì˜ë¯¸ì  ë¶„ë¦¬ë¥¼ ìœ ì§€í•˜ì§€ ëª»í•¨. ì´ ë‘ ê°€ì§€ í•¨ì •(ì¸¡ë©´)ì€ ë””ì½”ë”ë¥¼ ì§‘í•©ì ìœ¼ë¡œ ì†ìƒì‹œí‚¤ê³  ì €í’ˆì§ˆì˜ ì¬êµ¬ì„±ëœ ê°ë…ì„ ìƒì„±í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ ì¤‘ GFM ìµœì í™” ë”œë ˆë§ˆ(ë™ì „)ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ì‹¤ì¦ì  ì¡°ì‚¬ë¥¼ í†µí•´, ìš°ë¦¬ëŠ” ìœ„ì˜ ë„ì „ ê³¼ì œë¥¼ ì •ë³´ ë³‘ëª© í˜„ìƒê³¼ ì •ê·œí™” ê²°í•ì— ê¸°ì¸í•œë‹¤ê³  ë´…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” MoTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤ - (1) ë‘ ê°€ì§€ í•¨ì •ì„ ìœ„í•œ ì •ë³´ ì¡°ì •, ì´ëŠ” ì—£ì§€ ê¸°ë°˜ ì˜ë¯¸ ìœµí•© ì „ëµê³¼ ë„ë©”ì¸ ì¸ì‹ ë¼ìš°íŒ…ì„ í†µí•œ í˜¼í•© ì½”ë“œë¶ì„ í™œìš©í•˜ì—¬ ì •ë³´ ìš©ëŸ‰ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. (2) ìµœì í™” ë™ì „ì„ ìœ„í•œ ì •ê·œí™” ì¡°ì •, ì´ëŠ” ì œì•ˆëœ ì •ë³´ ì¡°ì •ì—ì„œ ê¸°ìš¸ê¸° ê°ë…ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì¶”ê°€ ì •ê·œí™”ë¥¼ í™œìš©í•©ë‹ˆë‹¤. íŠ¹íˆ, ìœ ì—°í•œ ì•„í‚¤í…ì²˜ë¡œì„œ MoTëŠ” GFMì˜ í™•ì¥ ë²•ì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ ì œì–´ ê°€ëŠ¥í•œ ëª¨ë¸ ê·œëª¨ë¥¼ ì œê³µí•©ë‹ˆë‹¤. SOTA ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬, 6ê°œ ë„ë©”ì¸ì— ê±¸ì¹œ 22ê°œ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ì€ MoTê°€ ê°ë…, ì†Œìˆ˜ ìƒ·, ì œë¡œ ìƒ· ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìœ ì˜ë¯¸í•œ ê°œì„ ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„, ë‹¤ì¤‘ ë„ë©”ì¸ í…ìŠ¤íŠ¸ ì†ì„± ê·¸ë˜í”„ì—ì„œ ìµœì ì˜ ì„ë² ë”© í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì— ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ê·¸ë˜í”„ ê¸°ë°˜ ëª¨ë¸(GFM)ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ê·¸ë˜í”„ VQ-MAEëŠ” ì—¬ëŸ¬ ë„ë©”ì¸ì˜ í† í´ë¡œì§€ì™€ í…ìŠ¤íŠ¸ ì†ì„±ì„ ëª…í™•í•œ ì˜ë¯¸ì  ê²½ê³„ë¥¼ ê°€ì§„ ì´ì‚° ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë„ë©”ì¸ ì¼ë°˜í™”ì˜ ë¬¸ì œë¡œ ì¸í•´ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì™€ í‘œí˜„ ë¶•ê´´ë¼ëŠ” ë‘ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì •ë³´ ë³‘ëª©ê³¼ ì •ê·œí™” ê²°í•ì„ ê·¹ë³µí•˜ëŠ” MoT ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. MoTëŠ” ì •ë³´ ìš©ëŸ‰ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì—£ì§€ ê¸°ë°˜ ì˜ë¯¸ ìœµí•© ì „ëµê³¼ ë„ë©”ì¸ ì¸ì‹ ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ëŠ” í˜¼í•© ì½”ë“œë¶ì„ ë„ì…í•˜ë©°, ì¶”ê°€ì ì¸ ì •ê·œí™”ë¥¼ í†µí•´ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MoTëŠ” 6ê°œ ë„ë©”ì¸ì˜ 22ê°œ ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ ëŒ€ë¹„ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê·¸ë˜í”„ VQ-MAEëŠ” ë‹¤ì¤‘ ë„ë©”ì¸ì—ì„œì˜ í† í´ë¡œì§€ì™€ í…ìŠ¤íŠ¸ ì†ì„±ì„ ëª…í™•í•œ ì˜ë¯¸ ê²½ê³„ë¡œ ì¸ì½”ë”©í•˜ì—¬ GFMì—ì„œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.
- 2. ë„ë©”ì¸ ì¼ë°˜í™” ê°ˆë“±ìœ¼ë¡œ ì¸í•´ ì¸ì‹í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œì ì´ ë°œìƒí•˜ë©°, ì´ëŠ” ëª¨ë¸ ì—´í™”ì™€ í‘œí˜„ ë¶•ê´´ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
- 3. Information Bottleneckì™€ Regularization Deficitì´ ì´ëŸ¬í•œ ë¬¸ì œì˜ ì›ì¸ìœ¼ë¡œ ì§€ëª©ë©ë‹ˆë‹¤.
- 4. MoTëŠ” ì •ë³´ ìš©ëŸ‰ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì—£ì§€ ê¸°ë°˜ ì˜ë¯¸ ìœµí•© ì „ëµê³¼ ë„ë©”ì¸ ì¸ì‹ ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ëŠ” í˜¼í•© ì½”ë“œë¶ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. MoTëŠ” 6ê°œ ë„ë©”ì¸ì—ì„œ 22ê°œì˜ ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ SOTA ê¸°ì¤€ë³´ë‹¤ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:12:02*