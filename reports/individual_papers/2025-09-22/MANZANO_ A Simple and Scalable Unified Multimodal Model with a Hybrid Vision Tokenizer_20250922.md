---
keywords:
  - Multimodal Learning
  - Hybrid Vision Tokenizer
  - Computer Vision
  - Vision-Language Model
  - Diffusion Decoder
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.16197
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:58:49.347755",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Hybrid Vision Tokenizer",
    "Computer Vision",
    "Vision-Language Model",
    "Diffusion Decoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Hybrid Vision Tokenizer": 0.78,
    "Computer Vision": 0.8,
    "Vision-Language Model": 0.82,
    "Diffusion Decoder": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Unified Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Unified Multimodal LLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of integrating multiple modalities in language models, which is a trending topic.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Hybrid Vision Tokenizer",
        "canonical": "Hybrid Vision Tokenizer",
        "aliases": [
          "Vision Tokenizer"
        ],
        "category": "unique_technical",
        "rationale": "Represents a unique technical component of the proposed model, crucial for its functionality.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision Encoder",
        "canonical": "Computer Vision",
        "aliases": [
          "Image Encoder"
        ],
        "category": "broad_technical",
        "rationale": "Essential for understanding the integration of visual data in the model.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Text-to-Image Generation",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Text-to-Image"
        ],
        "category": "evolved_concepts",
        "rationale": "Highlights the model's capability to generate visual content from text, a key feature.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.76,
        "link_intent_score": 0.82
      },
      {
        "surface": "Diffusion Decoder",
        "canonical": "Diffusion Decoder",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel component that translates image tokens into pixels, crucial for understanding the model's architecture.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance trade-off",
      "training recipe"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Unified Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Hybrid Vision Tokenizer",
      "resolved_canonical": "Hybrid Vision Tokenizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision Encoder",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Text-to-Image Generation",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.76,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Diffusion Decoder",
      "resolved_canonical": "Diffusion Decoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer

**Korean Title:** MANZANO: 하이브리드 비전 토크나이저를 갖춘 간단하고 확장 가능한 통합 멀티모달 모델

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16197.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.16197](https://arxiv.org/abs/2509.16197)

## 🔗 유사한 논문
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (84.3% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (83.5% similar)
- [[2025-09-22/Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding_20250922|Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding]] (82.5% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (82.0% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (81.7% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Computer Vision|Computer Vision]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Hybrid Vision Tokenizer|Hybrid Vision Tokenizer]], [[keywords/Diffusion Decoder|Diffusion Decoder]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16197v1 Announce Type: cross 
Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.

## 🔍 Abstract (한글 번역)

arXiv:2509.16197v1 발표 유형: 교차  
초록: 통합 멀티모달 대형 언어 모델(LLMs)은 시각적 콘텐츠를 이해하고 생성할 수 있는 잠재력을 지니고 있습니다. 그러나 기존의 오픈 소스 모델들은 이러한 기능들 간의 성능 균형 문제를 자주 겪습니다. 우리는 하이브리드 이미지 토크나이저와 잘 구성된 학습 레시피를 결합하여 이러한 긴장을 상당히 줄이는 간단하고 확장 가능한 통합 프레임워크인 Manzano를 제시합니다. 단일 공유 비전 인코더는 이미지-텍스트 이해를 위한 연속 임베딩과 텍스트-이미지 생성을 위한 이산 토큰을 생성하는 두 개의 경량 어댑터에 정보를 제공합니다. 통합 자율회귀 LLM은 텍스트와 이미지 토큰의 형태로 고수준의 의미를 예측하며, 보조 확산 디코더가 이후에 이미지 토큰을 픽셀로 변환합니다. 이 아키텍처는 이해 및 생성 데이터를 아우르는 통합 학습 레시피와 함께 두 가지 기능의 확장 가능한 공동 학습을 가능하게 합니다. Manzano는 통합 모델 중에서 최첨단 결과를 달성하며, 특히 텍스트가 풍부한 평가에서 전문 모델들과 경쟁력을 갖추고 있습니다. 우리의 연구는 최소한의 작업 충돌과 모델 크기 확장에서의 일관된 이득을 보여주며, 하이브리드 토크나이저의 설계 선택을 검증합니다.

## 📝 요약

Manzano는 시각 콘텐츠를 이해하고 생성할 수 있는 통합 멀티모달 대형 언어 모델(LLM)로, 기존 오픈소스 모델의 성능 저하 문제를 해결합니다. 하이브리드 이미지 토크나이저와 잘 구성된 학습 방식을 결합하여 이미지-텍스트 이해와 텍스트-이미지 생성 간의 긴장을 줄였습니다. 단일 비전 인코더가 두 개의 경량 어댑터에 연속 임베딩과 이산 토큰을 생성하며, 통합 자가회귀 LLM이 텍스트와 이미지 토큰을 예측합니다. 보조 확산 디코더는 이미지 토큰을 픽셀로 변환합니다. 이 아키텍처는 이해와 생성 데이터를 통합 학습하여 확장 가능한 공동 학습을 가능하게 합니다. Manzano는 통합 모델 중 최첨단 성과를 내며, 특히 텍스트가 많은 평가에서 전문 모델과 경쟁력을 갖습니다. 연구 결과, 하이브리드 토크나이저 설계가 유효하며, 모델 크기 확장에서 일관된 성능 향상을 보였습니다.

## 🎯 주요 포인트

- 1. Manzano는 하이브리드 이미지 토크나이저와 잘 구성된 학습 레시피를 결합하여 시각적 콘텐츠 이해와 생성 간의 성능 저하 문제를 크게 줄였습니다.
- 2. 단일 공유 비전 인코더가 이미지-텍스트 이해를 위한 연속 임베딩과 텍스트-이미지 생성을 위한 이산 토큰을 생성하는 두 개의 경량 어댑터를 지원합니다.
- 3. 통합된 자가 회귀 LLM은 텍스트 및 이미지 토큰 형태로 고수준의 의미를 예측하며, 보조 확산 디코더가 이미지 토큰을 픽셀로 변환합니다.
- 4. Manzano는 통합 모델 중 최첨단 결과를 달성했으며, 특히 텍스트가 많은 평가에서 전문 모델과 경쟁력이 있습니다.
- 5. 연구 결과, 하이브리드 토크나이저 설계 선택의 타당성을 입증하며 모델 크기 확장에서 일관된 성능 향상을 보여주었습니다.


---

*Generated on 2025-09-23 10:58:49*