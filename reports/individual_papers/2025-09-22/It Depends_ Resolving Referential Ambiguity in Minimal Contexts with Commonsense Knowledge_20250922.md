---
keywords:
  - Large Language Model
  - Referential Ambiguity
  - Commonsense Knowledge
  - Multilingual Evaluation Dataset
  - Direct Preference Optimization
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.16107
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:36:00.333457",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Referential Ambiguity",
    "Commonsense Knowledge",
    "Multilingual Evaluation Dataset",
    "Direct Preference Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Referential Ambiguity": 0.8,
    "Commonsense Knowledge": 0.78,
    "Multilingual Evaluation Dataset": 0.75,
    "Direct Preference Optimization": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, connecting to broader discussions in NLP and AI.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "referential ambiguity",
        "canonical": "Referential Ambiguity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key focus of the paper, offering a unique angle on language processing.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "commonsense knowledge",
        "canonical": "Commonsense Knowledge",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for understanding how LLMs resolve ambiguity.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "multilingual evaluation dataset",
        "canonical": "Multilingual Evaluation Dataset",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights the paper's contribution to dataset development.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Direct Preference Optimization",
        "canonical": "Direct Preference Optimization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific method used to improve model performance, relevant for linking technical methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "simplified language",
      "human annotations",
      "communication styles"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "referential ambiguity",
      "resolved_canonical": "Referential Ambiguity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "commonsense knowledge",
      "resolved_canonical": "Commonsense Knowledge",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multilingual evaluation dataset",
      "resolved_canonical": "Multilingual Evaluation Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Direct Preference Optimization",
      "resolved_canonical": "Direct Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge

**Korean Title:** ì˜ì¡´ì„±: ìƒì‹ ì§€ì‹ì„ í™œìš©í•œ ìµœì†Œí•œì˜ ë¬¸ë§¥ì—ì„œì˜ ì§€ì‹œì  ëª¨í˜¸ì„± í•´ê²°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16107.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.16107](https://arxiv.org/abs/2509.16107)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Correct-Detect_ Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs_20250917|Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs]] (88.3% similar)
- [[2025-09-17/Do Large Language Models Understand Word Senses?_20250917|Do Large Language Models Understand Word Senses?]] (87.5% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (87.4% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (86.5% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (86.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Commonsense Knowledge|Commonsense Knowledge]]
**âš¡ Unique Technical**: [[keywords/Referential Ambiguity|Referential Ambiguity]], [[keywords/Multilingual Evaluation Dataset|Multilingual Evaluation Dataset]], [[keywords/Direct Preference Optimization|Direct Preference Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16107v1 Announce Type: new 
Abstract: Ambiguous words or underspecified references require interlocutors to resolve them, often by relying on shared context and commonsense knowledge. Therefore, we systematically investigate whether Large Language Models (LLMs) can leverage commonsense to resolve referential ambiguity in multi-turn conversations and analyze their behavior when ambiguity persists. Further, we study how requests for simplified language affect this capacity. Using a novel multilingual evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that current LLMs struggle to resolve ambiguity effectively: they tend to commit to a single interpretation or cover all possible references, rather than hedging or seeking clarification. This limitation becomes more pronounced under simplification prompts, which drastically reduce the use of commonsense reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct Preference Optimization substantially improves ambiguity resolution across all request types. These results underscore the need for advanced fine-tuning to improve LLMs' handling of ambiguity and to ensure robust performance across diverse communication styles.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16107v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëª¨í˜¸í•œ ë‹¨ì–´ë‚˜ ëª…í™•í•˜ì§€ ì•Šì€ ì°¸ì¡°ëŠ” ëŒ€í™”ìë“¤ì´ ì´ë¥¼ í•´ê²°í•´ì•¼ í•˜ë©°, ì¢…ì¢… ê³µìœ ëœ ë§¥ë½ê³¼ ìƒì‹ì  ì§€ì‹ì„ í™œìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ìƒì‹ì„ í™œìš©í•˜ì—¬ ë‹¤ì¤‘ í„´ ëŒ€í™”ì—ì„œ ì°¸ì¡°ì˜ ëª¨í˜¸ì„±ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ê³ , ëª¨í˜¸ì„±ì´ ì§€ì†ë  ë•Œ ê·¸ë“¤ì˜ í–‰ë™ì„ ë¶„ì„í•©ë‹ˆë‹¤. ë˜í•œ, ê°„ë‹¨í•œ ì–¸ì–´ì— ëŒ€í•œ ìš”ì²­ì´ ì´ ëŠ¥ë ¥ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì—°êµ¬í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ë‹¤êµ­ì–´ í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, Llama-3.1-8Bë¥¼ LLM-as-Judgeì™€ ì¸ê°„ ì£¼ì„ì„ í†µí•´ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” í˜„ì¬ì˜ LLMë“¤ì´ ëª¨í˜¸ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤: ê·¸ë“¤ì€ ë‹¨ì¼ í•´ì„ì— ì§‘ì°©í•˜ê±°ë‚˜ ëª¨ë“  ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ í¬í•¨í•˜ë ¤ê³  í•˜ë©°, ëª¨í˜¸ì„±ì„ ì¤„ì´ê±°ë‚˜ ëª…í™•í™”ë¥¼ ì¶”êµ¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ëŠ” ê°„ì†Œí™” ìš”ì²­ì— ë”°ë¼ ë”ìš± ë‘ë“œëŸ¬ì§€ë©°, ì´ëŠ” ìƒì‹ì  ì¶”ë¡ ê³¼ ë‹¤ì–‘í•œ ì‘ë‹µ ì „ëµì˜ ì‚¬ìš©ì„ ê¸‰ê²©íˆ ê°ì†Œì‹œí‚µë‹ˆë‹¤. Llama-3.1-8Bë¥¼ ì§ì ‘ ì„ í˜¸ ìµœì í™”ë¥¼ í†µí•´ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ëª¨ë“  ìš”ì²­ ìœ í˜•ì—ì„œ ëª¨í˜¸ì„± í•´ê²°ì´ í¬ê²Œ ê°œì„ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLMì˜ ëª¨í˜¸ì„± ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ë‹¤ì–‘í•œ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ê³ ê¸‰ ë¯¸ì„¸ ì¡°ì •ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì¤‘ íšŒì°¨ ëŒ€í™”ì—ì„œ ì°¸ì¡°ì˜ ëª¨í˜¸ì„±ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ëª¨í˜¸ì„±ì´ ì§€ì†ë  ë•Œ ì–´ë–»ê²Œ í–‰ë™í•˜ëŠ”ì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ê°„ë‹¨í•œ ì–¸ì–´ ìš”ì²­ì´ ì´ëŸ¬í•œ ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ë‹¤êµ­ì–´ í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, Llama-3.1-8B ëª¨ë¸ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ í˜„ì¬ LLMì€ ëª¨í˜¸ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ë‹¨ì¼ í•´ì„ì— ì§‘ì°©í•˜ê±°ë‚˜ ëª¨ë“  ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ í¬ê´„í•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ì–¸ì–´ ìš”ì²­ì€ ìƒì‹ì  ì¶”ë¡ ê³¼ ë‹¤ì–‘í•œ ì‘ë‹µ ì „ëµ ì‚¬ìš©ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. Llama-3.1-8B ëª¨ë¸ì„ Direct Preference Optimizationìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ëª¨ë“  ìš”ì²­ ìœ í˜•ì—ì„œ ëª¨í˜¸ì„± í•´ê²°ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ëª¨í˜¸ì„± ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ê³ ê¸‰ ë¯¸ì„¸ ì¡°ì •ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ë‹¤ì¤‘ íšŒì°¨ ëŒ€í™”ì—ì„œ ì°¸ì¡° ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ëŠ” ë° ìˆì–´ ê³µí†µì˜ ë§¥ë½ê³¼ ìƒì‹ì  ì§€ì‹ì„ í™œìš©í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
- 2. ë‹¨ìˆœí™”ëœ ì–¸ì–´ ìš”ì²­ì€ LLMì˜ ëª¨í˜¸ì„± í•´ê²° ëŠ¥ë ¥ì„ ë”ìš± ê°ì†Œì‹œí‚¤ë©°, ìƒì‹ì  ì¶”ë¡ ê³¼ ë‹¤ì–‘í•œ ì‘ë‹µ ì „ëµì˜ ì‚¬ìš©ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.
- 3. Llama-3.1-8B ëª¨ë¸ì„ Direct Preference Optimizationìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ëª¨ë“  ìš”ì²­ ìœ í˜•ì—ì„œ ëª¨í˜¸ì„± í•´ê²° ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.
- 4. í˜„ì¡´í•˜ëŠ” LLMë“¤ì€ ëª¨í˜¸ì„±ì— ì§ë©´í–ˆì„ ë•Œ ë‹¨ì¼ í•´ì„ì— ì§‘ì°©í•˜ê±°ë‚˜ ëª¨ë“  ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ í¬ê´„í•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ëª…í™•ì„±ì„ ì¶”êµ¬í•˜ê±°ë‚˜ ìœ ë³´í•˜ëŠ” ì „ëµì„ ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 5. ë‹¤ì–‘í•œ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ LLMì˜ ëª¨í˜¸ì„± ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê°œì„ í•˜ëŠ” ê³ ê¸‰ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:36:00*