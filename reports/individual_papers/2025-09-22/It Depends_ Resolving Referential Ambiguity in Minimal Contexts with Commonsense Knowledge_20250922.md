---
keywords:
  - Large Language Model
  - Referential Ambiguity
  - Commonsense Knowledge
  - Multilingual Evaluation Dataset
  - Direct Preference Optimization
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.16107
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:36:00.333457",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Referential Ambiguity",
    "Commonsense Knowledge",
    "Multilingual Evaluation Dataset",
    "Direct Preference Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Referential Ambiguity": 0.8,
    "Commonsense Knowledge": 0.78,
    "Multilingual Evaluation Dataset": 0.75,
    "Direct Preference Optimization": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, connecting to broader discussions in NLP and AI.",
        "novelty_score": 0.2,
        "connectivity_score": 0.95,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "referential ambiguity",
        "canonical": "Referential Ambiguity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Key focus of the paper, offering a unique angle on language processing.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "commonsense knowledge",
        "canonical": "Commonsense Knowledge",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Essential for understanding how LLMs resolve ambiguity.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "multilingual evaluation dataset",
        "canonical": "Multilingual Evaluation Dataset",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights the paper's contribution to dataset development.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Direct Preference Optimization",
        "canonical": "Direct Preference Optimization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific method used to improve model performance, relevant for linking technical methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "simplified language",
      "human annotations",
      "communication styles"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.95,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "referential ambiguity",
      "resolved_canonical": "Referential Ambiguity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "commonsense knowledge",
      "resolved_canonical": "Commonsense Knowledge",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multilingual evaluation dataset",
      "resolved_canonical": "Multilingual Evaluation Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Direct Preference Optimization",
      "resolved_canonical": "Direct Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge

**Korean Title:** 의존성: 상식 지식을 활용한 최소한의 문맥에서의 지시적 모호성 해결

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16107.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.16107](https://arxiv.org/abs/2509.16107)

## 🔗 유사한 논문
- [[2025-09-17/Correct-Detect_ Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs_20250917|Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs]] (88.3% similar)
- [[2025-09-17/Do Large Language Models Understand Word Senses?_20250917|Do Large Language Models Understand Word Senses?]] (87.5% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (87.4% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (86.5% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (86.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Commonsense Knowledge|Commonsense Knowledge]]
**⚡ Unique Technical**: [[keywords/Referential Ambiguity|Referential Ambiguity]], [[keywords/Multilingual Evaluation Dataset|Multilingual Evaluation Dataset]], [[keywords/Direct Preference Optimization|Direct Preference Optimization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16107v1 Announce Type: new 
Abstract: Ambiguous words or underspecified references require interlocutors to resolve them, often by relying on shared context and commonsense knowledge. Therefore, we systematically investigate whether Large Language Models (LLMs) can leverage commonsense to resolve referential ambiguity in multi-turn conversations and analyze their behavior when ambiguity persists. Further, we study how requests for simplified language affect this capacity. Using a novel multilingual evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that current LLMs struggle to resolve ambiguity effectively: they tend to commit to a single interpretation or cover all possible references, rather than hedging or seeking clarification. This limitation becomes more pronounced under simplification prompts, which drastically reduce the use of commonsense reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct Preference Optimization substantially improves ambiguity resolution across all request types. These results underscore the need for advanced fine-tuning to improve LLMs' handling of ambiguity and to ensure robust performance across diverse communication styles.

## 🔍 Abstract (한글 번역)

arXiv:2509.16107v1 발표 유형: 신규  
초록: 모호한 단어나 명확하지 않은 참조는 대화자들이 이를 해결해야 하며, 종종 공유된 맥락과 상식적 지식을 활용하게 됩니다. 따라서, 우리는 대형 언어 모델(LLMs)이 상식을 활용하여 다중 턴 대화에서 참조의 모호성을 해결할 수 있는지 체계적으로 조사하고, 모호성이 지속될 때 그들의 행동을 분석합니다. 또한, 간단한 언어에 대한 요청이 이 능력에 어떻게 영향을 미치는지 연구합니다. 새로운 다국어 평가 데이터셋을 사용하여, 우리는 DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, Llama-3.1-8B를 LLM-as-Judge와 인간 주석을 통해 테스트합니다. 우리의 연구 결과는 현재의 LLM들이 모호성을 효과적으로 해결하는 데 어려움을 겪고 있음을 나타냅니다: 그들은 단일 해석에 집착하거나 모든 가능한 참조를 포함하려고 하며, 모호성을 줄이거나 명확화를 추구하지 않습니다. 이러한 한계는 간소화 요청에 따라 더욱 두드러지며, 이는 상식적 추론과 다양한 응답 전략의 사용을 급격히 감소시킵니다. Llama-3.1-8B를 직접 선호 최적화를 통해 미세 조정하면 모든 요청 유형에서 모호성 해결이 크게 개선됩니다. 이러한 결과는 LLM의 모호성 처리 능력을 향상시키고 다양한 의사소통 스타일에서 강력한 성능을 보장하기 위해 고급 미세 조정의 필요성을 강조합니다.

## 📝 요약

이 논문은 대규모 언어 모델(LLM)이 다중 회차 대화에서 참조의 모호성을 해결할 수 있는지, 그리고 모호성이 지속될 때 어떻게 행동하는지를 체계적으로 조사합니다. 연구는 간단한 언어 요청이 이러한 능력에 미치는 영향을 분석합니다. 새로운 다국어 평가 데이터셋을 사용하여 DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, Llama-3.1-8B 모델을 평가했습니다. 결과적으로 현재 LLM은 모호성을 효과적으로 해결하는 데 어려움을 겪으며, 단일 해석에 집착하거나 모든 가능한 참조를 포괄하려는 경향이 있습니다. 간단한 언어 요청은 상식적 추론과 다양한 응답 전략 사용을 크게 줄입니다. Llama-3.1-8B 모델을 Direct Preference Optimization으로 미세 조정하면 모든 요청 유형에서 모호성 해결이 크게 개선되었습니다. 이 연구는 LLM의 모호성 처리 능력을 향상시키기 위한 고급 미세 조정의 필요성을 강조합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 다중 회차 대화에서 참조 모호성을 해결하는 데 있어 공통의 맥락과 상식적 지식을 활용하는 데 어려움을 겪고 있습니다.
- 2. 단순화된 언어 요청은 LLM의 모호성 해결 능력을 더욱 감소시키며, 상식적 추론과 다양한 응답 전략의 사용을 크게 줄입니다.
- 3. Llama-3.1-8B 모델을 Direct Preference Optimization으로 미세 조정하면 모든 요청 유형에서 모호성 해결 능력이 크게 향상됩니다.
- 4. 현존하는 LLM들은 모호성에 직면했을 때 단일 해석에 집착하거나 모든 가능한 참조를 포괄하려는 경향이 있으며, 명확성을 추구하거나 유보하는 전략을 잘 사용하지 않습니다.
- 5. 다양한 의사소통 스타일에서 강력한 성능을 보장하기 위해 LLM의 모호성 처리 능력을 개선하는 고급 미세 조정이 필요합니다.


---

*Generated on 2025-09-23 11:36:00*