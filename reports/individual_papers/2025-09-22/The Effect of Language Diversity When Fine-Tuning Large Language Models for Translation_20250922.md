---
keywords:
  - Large Language Model
  - Language Diversity
  - Translation Quality
  - Language-Agnostic Representations
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2505.13090
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:44:43.240298",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Language Diversity",
    "Translation Quality",
    "Language-Agnostic Representations"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Language Diversity": 0.78,
    "Translation Quality": 0.8,
    "Language-Agnostic Representations": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to broader discussions on language model capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "language diversity",
        "canonical": "Language Diversity",
        "aliases": [
          "linguistic diversity"
        ],
        "category": "unique_technical",
        "rationale": "Key focus of the paper, offering unique insights into translation model performance.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "translation quality",
        "canonical": "Translation Quality",
        "aliases": [
          "quality of translation"
        ],
        "category": "unique_technical",
        "rationale": "Directly related to the paper's evaluation of model performance improvements.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "language-agnostic representations",
        "canonical": "Language-Agnostic Representations",
        "aliases": [
          "language-independent representations"
        ],
        "category": "unique_technical",
        "rationale": "Describes a significant outcome of increased language diversity in fine-tuning.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "fine-tuning",
      "translation directions",
      "supervised pairs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "language diversity",
      "resolved_canonical": "Language Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "translation quality",
      "resolved_canonical": "Translation Quality",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "language-agnostic representations",
      "resolved_canonical": "Language-Agnostic Representations",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation

**Korean Title:** ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ë²ˆì—­ì„ ìœ„í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.13090.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2505.13090](https://arxiv.org/abs/2505.13090)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A method for improving multilingual quality and diversity of instruction fine-tuning datasets_20250922|A method for improving multilingual quality and diversity of instruction fine-tuning datasets]] (86.3% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.2% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (84.8% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (84.5% similar)
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Language Diversity|Language Diversity]], [[keywords/Translation Quality|Translation Quality]], [[keywords/Language-Agnostic Representations|Language-Agnostic Representations]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.13090v2 Announce Type: replace 
Abstract: Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.13090v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: LLM ë¯¸ì„¸ ì¡°ì •ì—ì„œ ì–¸ì–´ ë‹¤ì–‘ì„±ì— ëŒ€í•œ ì´ì „ ì—°êµ¬ëŠ” ìƒë°˜ëœ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤. ì¼ë¶€ ì—°êµ¬ëŠ” ì´ì ì´ ìˆë‹¤ê³  ë³´ê³ í•˜ëŠ” ë°˜ë©´, ë‹¤ë¥¸ ì—°êµ¬ëŠ” ì´ì ì„ ì°¾ì§€ ëª»í•©ë‹ˆë‹¤. 132ê°œì˜ ë²ˆì—­ ë°©í–¥ì— ê±¸ì¹œ í†µì œëœ ë¯¸ì„¸ ì¡°ì • ì‹¤í—˜ì„ í†µí•´ ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¯¸ì„¸ ì¡°ì • ì¤‘ ì–¸ì–´ ë‹¤ì–‘ì„±ì„ í™•ì¥í•˜ë©´ ë¹„ì§€ë„ ë° -- ë†€ëê²Œë„ -- ì§€ë„ ìŒ ëª¨ë‘ì—ì„œ ë²ˆì—­ í’ˆì§ˆì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ëœ ë‹¤ì–‘í•œ ëª¨ë¸ì´ ì´ëŸ¬í•œ ì§€ë„ ìŒì—ë§Œ ë¯¸ì„¸ ì¡°ì •ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ë‚˜íƒ€ë‚œ ê²°ê³¼ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ íŠ¹ì • ë‹¤ì–‘ì„± ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ì´ì ì´ ì •ì²´ë˜ê±°ë‚˜ ê°ì†Œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ì¦ê°€í•˜ë©´ ë” ë§ì€ ì–¸ì–´ ë¹„íŠ¹ì´ì  í‘œí˜„ì´ ìƒì„±ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ í‘œí˜„ ì ì‘ì€ ë” í° ë‹¤ì–‘ì„±ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì„¤ëª…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë¯¸ì„¸ ì¡°ì •ì—ì„œ ì–¸ì–´ ë‹¤ì–‘ì„±ì˜ íš¨ê³¼ì— ëŒ€í•œ ê¸°ì¡´ ì—°êµ¬ì˜ ìƒë°˜ëœ ê²°ê³¼ë¥¼ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. 132ê°œì˜ ë²ˆì—­ ë°©í–¥ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´, ì–¸ì–´ ë‹¤ì–‘ì„±ì„ í™•ì¥í•˜ë©´ ë¹„ì§€ë„ ë° ì§€ë„ ë²ˆì—­ ìŒ ëª¨ë‘ì—ì„œ ë²ˆì—­ í’ˆì§ˆì´ í–¥ìƒë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì§€ë„ ìŒì—ë§Œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ì¼ì • ìˆ˜ì¤€ì„ ë„˜ì–´ì„œë©´ ê·¸ ì´ì ì´ ê°ì†Œí•˜ê±°ë‚˜ ì •ì²´ë˜ëŠ” ê²½í–¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ì¦ê°€í•˜ë©´ ì–¸ì–´ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” í‘œí˜„ì´ ìƒì„±ë˜ì–´, ì´ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì„¤ëª…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LLM ë¯¸ì„¸ ì¡°ì •ì—ì„œ ì–¸ì–´ ë‹¤ì–‘ì„±ì˜ íš¨ê³¼ì— ëŒ€í•œ ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ìƒë°˜ëœ ê²°ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ë³¸ ì—°êµ¬ì—ì„œëŠ” 132ê°œì˜ ë²ˆì—­ ë°©í–¥ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´ ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•˜ì˜€ë‹¤.
- 2. ì–¸ì–´ ë‹¤ì–‘ì„±ì„ í™•ì¥í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ë¹„ì§€ë„ ë° ì§€ë„ ë²ˆì—­ ìŒ ëª¨ë‘ì—ì„œ ë²ˆì—­ í’ˆì§ˆì´ í–¥ìƒë˜ë©°, ì´ëŠ” ì§€ë„ ìŒì—ë§Œ ë¯¸ì„¸ ì¡°ì •ëœ ëœ ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œë„ ë‚˜íƒ€ë‚œë‹¤.
- 3. ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ì¼ì • ìˆ˜ì¤€ì„ ë„˜ì–´ì„œë©´ ê·¸ ì´ì ì´ ì •ì²´ë˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.
- 4. ì–¸ì–´ ë‹¤ì–‘ì„±ì˜ ì¦ê°€ëŠ” ë³´ë‹¤ ì–¸ì–´ ë¹„ì¢…ì†ì ì¸ í‘œí˜„ì„ ë§Œë“¤ì–´ë‚´ë©°, ì´ëŠ” ë” í° ë‹¤ì–‘ì„±ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì„¤ëª…í•˜ëŠ” ë° ê¸°ì—¬í•œë‹¤.


---

*Generated on 2025-09-23 11:44:43*