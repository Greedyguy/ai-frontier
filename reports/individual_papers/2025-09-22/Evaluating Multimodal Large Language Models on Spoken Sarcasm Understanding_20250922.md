---
keywords:
  - Multimodal Learning
  - Sarcasm Detection
  - Zero-Shot Learning
  - Few-Shot Learning
  - Collaborative Gating Fusion
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15476
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:28:56.724408",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Sarcasm Detection",
    "Zero-Shot Learning",
    "Few-Shot Learning",
    "Collaborative Gating Fusion"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Sarcasm Detection": 0.79,
    "Zero-Shot Learning": 0.78,
    "Few-Shot Learning": 0.75,
    "Collaborative Gating Fusion": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal LLMs"
        ],
        "category": "specific_connectable",
        "rationale": "This term bridges the gap between language models and multimodal data, enhancing cross-disciplinary connections.",
        "novelty_score": 0.65,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Sarcasm Detection",
        "canonical": "Sarcasm Detection",
        "aliases": [
          "Sarcasm Understanding"
        ],
        "category": "unique_technical",
        "rationale": "A specific task within natural language processing that connects to sentiment analysis and emotion recognition.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A trending approach that allows models to generalize to tasks without task-specific data, linking to transfer learning.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This method is crucial for understanding model performance with limited data, connecting to data efficiency techniques.",
        "novelty_score": 0.58,
        "connectivity_score": 0.83,
        "specificity_score": 0.77,
        "link_intent_score": 0.75
      },
      {
        "surface": "Collaborative Gating Fusion Module",
        "canonical": "Collaborative Gating Fusion",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel approach for integrating multimodal representations, enhancing model architecture discussions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experimental results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Sarcasm Detection",
      "resolved_canonical": "Sarcasm Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.83,
        "specificity": 0.77,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Collaborative Gating Fusion Module",
      "resolved_canonical": "Collaborative Gating Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding

**Korean Title:** 구어적 풍자의 이해에 대한 다중 모드 대형 언어 모델 평가

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15476.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15476](https://arxiv.org/abs/2509.15476)

## 🔗 유사한 논문
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.4% similar)
- [[2025-09-22/Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment_20250922|Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment]] (85.2% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (84.7% similar)
- [[2025-09-22/Pointing to a Llama and Call it a Camel_ On the Sycophancy of Multimodal Large Language Models_20250922|Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models]] (84.3% similar)
- [[2025-09-19/Large Multi-modal Models Can Interpret Features in Large Multi-modal Models_20250919|Large Multi-modal Models Can Interpret Features in Large Multi-modal Models]] (84.3% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Sarcasm Detection|Sarcasm Detection]], [[keywords/Collaborative Gating Fusion|Collaborative Gating Fusion]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15476v1 Announce Type: new 
Abstract: Sarcasm detection remains a challenge in natural language understanding, as sarcastic intent often relies on subtle cross-modal cues spanning text, speech, and vision. While prior work has primarily focused on textual or visual-textual sarcasm, comprehensive audio-visual-textual sarcasm understanding remains underexplored. In this paper, we systematically evaluate large language models (LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In addition to direct classification, we explore models as feature encoders, integrating their representations through a collaborative gating fusion module. Experimental results show that audio-based models achieve the strongest unimodal performance, while text-audio and audio-vision combinations outperform unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show competitive zero-shot and fine-tuned performance. Our findings highlight the potential of MLLMs for cross-lingual, audio-visual-textual sarcasm understanding.

## 🔍 Abstract (한글 번역)

arXiv:2509.15476v1 발표 유형: 신규  
초록: 풍자 감지(sarcasm detection)는 자연어 이해에서 여전히 도전 과제로 남아 있으며, 풍자적 의도는 종종 텍스트, 음성, 시각을 아우르는 미묘한 다중 모달 단서에 의존합니다. 이전 연구는 주로 텍스트 또는 시각-텍스트 풍자에 초점을 맞췄으나, 포괄적인 오디오-비주얼-텍스트 풍자 이해는 여전히 충분히 탐구되지 않았습니다. 본 논문에서는 영어(MUStARD++)와 중국어(MCSD 1.0)에 대해 대규모 언어 모델(LLMs)과 다중 모달 LLMs을 제로샷(zero-shot), 몇 샷(few-shot), LoRA 미세 조정 설정에서 풍자 감지를 위해 체계적으로 평가합니다. 직접적인 분류 외에도, 우리는 모델을 특징 인코더로 활용하여 협력적 게이팅 융합 모듈을 통해 그들의 표현을 통합하는 방법을 탐구합니다. 실험 결과, 오디오 기반 모델이 가장 강력한 단일 모달 성능을 달성했으며, 텍스트-오디오 및 오디오-비전 조합이 단일 모달 및 삼중 모달 모델을 능가하는 것으로 나타났습니다. 또한, Qwen-Omni와 같은 MLLMs은 경쟁력 있는 제로샷 및 미세 조정 성능을 보여줍니다. 우리의 연구 결과는 MLLMs이 교차 언어, 오디오-비주얼-텍스트 풍자 이해에 잠재력을 가지고 있음을 강조합니다.

## 📝 요약

이 논문은 자연어 이해에서의 풍자 감지 문제를 다룹니다. 기존 연구는 주로 텍스트나 시각-텍스트 풍자에 집중했지만, 이 논문은 오디오-비주얼-텍스트 풍자 이해를 탐구합니다. 영어와 중국어 데이터셋에서 대형 언어 모델(LLM)과 멀티모달 LLM을 사용하여 제로샷, 퓨샷, LoRA 미세 조정 설정에서 풍자 감지를 평가했습니다. 실험 결과, 오디오 기반 모델이 단일 모달 성능에서 가장 우수했으며, 텍스트-오디오 및 오디오-비전 조합이 단일 모달 및 삼중 모달 모델보다 뛰어났습니다. 또한, MLLM인 Qwen-Omni는 경쟁력 있는 제로샷 및 미세 조정 성능을 보였습니다. 연구는 MLLM의 다국어, 오디오-비주얼-텍스트 풍자 이해 가능성을 강조합니다.

## 🎯 주요 포인트

- 1. 자연어 이해에서 풍자 감지의 어려움은 텍스트, 음성, 시각을 아우르는 미묘한 단서에 의존하기 때문입니다.
- 2. 기존 연구는 주로 텍스트 또는 시각-텍스트 풍자에 집중했지만, 종합적인 음성-시각-텍스트 풍자 이해는 충분히 탐구되지 않았습니다.
- 3. 본 논문은 영어와 중국어에서 대형 언어 모델(LLM)과 다중 모달 LLM을 활용하여 풍자 감지를 평가합니다.
- 4. 실험 결과, 음성 기반 모델이 가장 강력한 단일 모달 성능을 보였으며, 텍스트-음성 및 음성-시각 조합이 단일 모달 및 삼중 모달 모델을 능가했습니다.
- 5. MLLM은 교차 언어, 음성-시각-텍스트 풍자 이해에 잠재력이 있음을 보여줍니다.


---

*Generated on 2025-09-23 11:28:56*