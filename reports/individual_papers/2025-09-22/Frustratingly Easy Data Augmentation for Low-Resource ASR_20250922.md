---
keywords:
  - Automatic Speech Recognition
  - Low-Resource Languages
  - Text-to-Speech
  - Wav2Vec2-XLSR-53
  - Large Language Model
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15373
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:27:57.007379",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Automatic Speech Recognition",
    "Low-Resource Languages",
    "Text-to-Speech",
    "Wav2Vec2-XLSR-53",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Automatic Speech Recognition": 0.8,
    "Low-Resource Languages": 0.85,
    "Text-to-Speech": 0.78,
    "Wav2Vec2-XLSR-53": 0.8,
    "Large Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Automatic Speech Recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "unique_technical",
        "rationale": "ASR is a central topic of the paper and links to related work in speech processing.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Low-Resource Languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "Low-Resource ASR"
        ],
        "category": "unique_technical",
        "rationale": "Focus on low-resource languages is crucial for linking to other research in language diversity and resource-scarce contexts.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Text-to-Speech",
        "canonical": "Text-to-Speech",
        "aliases": [
          "TTS"
        ],
        "category": "specific_connectable",
        "rationale": "TTS is a key technology in the paper, linking to broader work in speech synthesis.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Wav2Vec2-XLSR-53",
        "canonical": "Wav2Vec2-XLSR-53",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This specific model is central to the methodology and connects to research on self-supervised learning in speech.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are used for text generation, linking to a wide range of NLP applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Automatic Speech Recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Low-Resource Languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Text-to-Speech",
      "resolved_canonical": "Text-to-Speech",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Wav2Vec2-XLSR-53",
      "resolved_canonical": "Wav2Vec2-XLSR-53",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Frustratingly Easy Data Augmentation for Low-Resource ASR

**Korean Title:** ë‚®ì€ ìì› ASRì„ ìœ„í•œ ì¢Œì ˆìŠ¤ëŸ½ê²Œ ì‰¬ìš´ ë°ì´í„° ì¦ê°•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15373.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15373](https://arxiv.org/abs/2509.15373)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/AS-ASR_ A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition_20250922|AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition]] (84.0% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (83.7% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (82.9% similar)
- [[2025-09-22/LESS_ Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models Using in-the-wild Data_20250922|LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models Using in-the-wild Data]] (82.3% similar)
- [[2025-09-19/Listening, Imagining \& Refining_ A Heuristic Optimized ASR Correction Framework with LLMs_20250919|Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Text-to-Speech|Text-to-Speech]]
**âš¡ Unique Technical**: [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]], [[keywords/Low-Resource Languages|Low-Resource Languages]], [[keywords/Wav2Vec2-XLSR-53|Wav2Vec2-XLSR-53]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15373v1 Announce Type: new 
Abstract: This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel text--using gloss-based replacement, random replacement, or an LLM-based approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15373v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì´ ë…¼ë¬¸ì€ ìì›ì´ ë¶€ì¡±í•œ ìë™ ìŒì„± ì¸ì‹(ASR)ì„ ìœ„í•œ ì„¸ ê°€ì§€ ë…ë¦½ì ì¸ ë°ì´í„° ì¦ê°• ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê¸°ìˆ ì€ ë¨¼ì € ê¸€ë¡œìŠ¤ ê¸°ë°˜ ëŒ€ì²´, ë¬´ì‘ìœ„ ëŒ€ì²´, ë˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ ë‹¤ìŒ í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜(TTS)ì„ ì ìš©í•˜ì—¬ í•©ì„± ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë°©ë²•ë“¤ì„ ì›ë˜ ì£¼ì„ì´ ë‹¬ë¦° ë°ì´í„°ë§Œì„ í™œìš©í•˜ì—¬ ìì›ì´ ê·¹ë„ë¡œ ì œí•œëœ ë„¤ ê°œì˜ ì–¸ì–´(Vatlongos, Nashta, Shinekhen Buryat, Kakabe)ì— ì ìš©í•©ë‹ˆë‹¤. ì›ë³¸ ì˜¤ë””ì˜¤ì™€ ìƒì„±ëœ í•©ì„± ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ Wav2Vec2-XLSR-53 ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ Nashtaì˜ ê²½ìš° 14.3%ì˜ ì ˆëŒ€ WER ê°ì†Œë¥¼ í¬í•¨í•˜ì—¬ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ë“¤ì€ ëª¨ë“  ìì›ì´ ë¶€ì¡±í•œ ë„¤ ê°œì˜ ì–¸ì–´ì—ì„œ íš¨ê³¼ì ì„ì„ ì¦ëª…í–ˆìœ¼ë©°, ì˜ì–´ì™€ ê°™ì€ ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ì„œë„ ìœ ìš©ì„±ì„ ë³´ì—¬ì£¼ì–´ ê·¸ ê´‘ë²”ìœ„í•œ ì ìš© ê°€ëŠ¥ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì›ì´ ë¶€ì¡±í•œ ìë™ ìŒì„± ì¸ì‹ì„ ìœ„í•œ ì„¸ ê°€ì§€ ë…ë¦½ì ì¸ ë°ì´í„° ì¦ê°• ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ê¸€ë¡œìŠ¤ ê¸°ë°˜ ëŒ€ì²´, ë¬´ì‘ìœ„ ëŒ€ì²´, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ì ‘ê·¼ë²•ì„ í†µí•´ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„, ì´ë¥¼ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜(TTS)ìœ¼ë¡œ í•©ì„± ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë°©ë²•ë“¤ì€ ì›ë˜ì˜ ì£¼ì„ ë°ì´í„°ë§Œì„ í™œìš©í•˜ì—¬ ìì›ì´ ë§¤ìš° ì œí•œëœ ë„¤ ê°œì˜ ì–¸ì–´(Vatlongos, Nashta, Shinekhen Buryat, Kakabe)ì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ Wav2Vec2-XLSR-53 ëª¨ë¸ì„ ì›ë³¸ ì˜¤ë””ì˜¤ì™€ ìƒì„±ëœ í•©ì„± ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •í•œ ê²°ê³¼, Nashta ì–¸ì–´ì—ì„œ 14.3%ì˜ ì ˆëŒ€ì ì¸ WER ê°ì†Œë¥¼ í¬í•¨í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ë“¤ì€ ë„¤ ê°œì˜ ì €ìì› ì–¸ì–´ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ì–´ì™€ ê°™ì€ ê³ ìì› ì–¸ì–´ì—ë„ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì£¼ë©°, ê´‘ë²”ìœ„í•œ ì ìš© ê°€ëŠ¥ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ìì› ë¶€ì¡± í™˜ê²½ì—ì„œì˜ ìë™ ìŒì„± ì¸ì‹ì„ ìœ„í•œ ì„¸ ê°€ì§€ ë°ì´í„° ì¦ê°• ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ ê¸°ë²•ì€ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„, ì´ë¥¼ ìŒì„± í•©ì„±(TTS)ì„ í†µí•´ í•©ì„± ì˜¤ë””ì˜¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- 3. ì´ ë°©ë²•ë“¤ì€ ë§¤ìš° ì œí•œëœ ìì›ì„ ê°€ì§„ ë„¤ ê°œì˜ ì–¸ì–´(Vatlongos, Nashta, Shinekhen Buryat, Kakabe)ì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ì›ë³¸ ì˜¤ë””ì˜¤ì™€ ìƒì„±ëœ í•©ì„± ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ Wav2Vec2-XLSR-53 ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•œ ê²°ê³¼, Nashta ì–¸ì–´ì—ì„œ 14.3%ì˜ ì ˆëŒ€ WER ê°ì†Œë¥¼ í¬í•¨í•œ ì„±ëŠ¥ í–¥ìƒì´ ìˆì—ˆìŠµë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ìì› ë¶€ì¡± ì–¸ì–´ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ì–´ì™€ ê°™ì€ ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ì„œë„ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:27:57*