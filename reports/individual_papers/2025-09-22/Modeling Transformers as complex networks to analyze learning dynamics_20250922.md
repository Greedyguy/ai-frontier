# Modeling Transformers as complex networks to analyze learning dynamics

**Korean Title:** íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ë¡œ ëª¨ë¸ë§í•˜ì—¬ í•™ìŠµ ë™ì—­í•™ ë¶„ì„í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Stable Hierarchy of Information Spreader Components

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (84.9% similar)
- [[2025-09-19/From Capabilities to Performance_ Evaluating Key Functional Properties of LLM Architectures in Penetration Testing_20250919|From Capabilities to Performance Evaluating Key Functional Properties of LLM Architectures in Penetration Testing]] (83.3% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (83.0% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (82.8% similar)
- [[2025-09-22/Knowledge-Driven Hallucination in Large Language Models_ An Empirical Study on Process Modeling_20250922|Knowledge-Driven Hallucination in Large Language Models An Empirical Study on Process Modeling]] (82.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15269v1 Announce Type: cross 
Abstract: The process by which Large Language Models (LLMs) acquire complex capabilities during training remains a key open question in mechanistic interpretability. This project investigates whether these learning dynamics can be characterized through the lens of Complex Network Theory (CNT). I introduce a novel methodology to represent a Transformer-based LLM as a directed, weighted graph where nodes are the model's computational components (attention heads and MLPs) and edges represent causal influence, measured via an intervention-based ablation technique. By tracking the evolution of this component-graph across 143 training checkpoints of the Pythia-14M model on a canonical induction task, I analyze a suite of graph-theoretic metrics. The results reveal that the network's structure evolves through distinct phases of exploration, consolidation, and refinement. Specifically, I identify the emergence of a stable hierarchy of information spreader components and a dynamic set of information gatherer components, whose roles reconfigure at key learning junctures. This work demonstrates that a component-level network perspective offers a powerful macroscopic lens for visualizing and understanding the self-organizing principles that drive the formation of functional circuits in LLMs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15269v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ í›ˆë ¨ ì¤‘ ë³µì¡í•œ ëŠ¥ë ¥ì„ íšë“í•˜ëŠ” ê³¼ì •ì€ ê¸°ê³„ì  í•´ì„ ê°€ëŠ¥ì„±ì—ì„œ ì—¬ì „íˆ ì¤‘ìš”í•œ ë¯¸í•´ê²° ë¬¸ì œë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ì´ëŸ¬í•œ í•™ìŠµ ë™íƒœê°€ ë³µì¡ ë„¤íŠ¸ì›Œí¬ ì´ë¡ (CNT)ì˜ ê´€ì ì—ì„œ íŠ¹ì„±í™”ë  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì €ëŠ” Transformer ê¸°ë°˜ LLMì„ ë°©í–¥ì„± ê°€ì¤‘ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë…¸ë“œëŠ” ëª¨ë¸ì˜ ê³„ì‚° êµ¬ì„± ìš”ì†Œ(ì–´í…ì…˜ í—¤ë“œì™€ MLP)ì´ê³ , ì—£ì§€ëŠ” ê°œì… ê¸°ë°˜ ì œê±° ê¸°ë²•ì„ í†µí•´ ì¸¡ì •ëœ ì¸ê³¼ì  ì˜í–¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Pythia-14M ëª¨ë¸ì˜ í‘œì¤€ ê·€ë‚© ì‘ì—…ì— ëŒ€í•œ 143ê°œì˜ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ì— ê±¸ì³ ì´ êµ¬ì„± ìš”ì†Œ ê·¸ë˜í”„ì˜ ì§„í™”ë¥¼ ì¶”ì í•¨ìœ¼ë¡œì¨, ì €ëŠ” ì¼ë ¨ì˜ ê·¸ë˜í”„ ì´ë¡ ì  ì§€í‘œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì¡°ê°€ íƒìƒ‰, í†µí•©, ì •ì œì˜ ëšœë ·í•œ ë‹¨ê³„ë¥¼ í†µí•´ ì§„í™”í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ì •ë³´ í™•ì‚° êµ¬ì„± ìš”ì†Œì˜ ì•ˆì •ì ì¸ ê³„ì¸µ êµ¬ì¡°ì˜ ì¶œí˜„ê³¼ ì •ë³´ ìˆ˜ì§‘ êµ¬ì„± ìš”ì†Œì˜ ë™ì  ì§‘í•©ì„ ì‹ë³„í–ˆìœ¼ë©°, ì´ë“¤ì˜ ì—­í• ì€ ì£¼ìš” í•™ìŠµ ì‹œì ì—ì„œ ì¬êµ¬ì„±ë©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” êµ¬ì„± ìš”ì†Œ ìˆ˜ì¤€ì˜ ë„¤íŠ¸ì›Œí¬ ê´€ì ì´ LLMì—ì„œ ê¸°ëŠ¥ì  íšŒë¡œ í˜•ì„±ì„ ì£¼ë„í•˜ëŠ” ìê¸° ì¡°ì§ ì›ë¦¬ë¥¼ ì‹œê°í™”í•˜ê³  ì´í•´í•˜ëŠ” ë° ê°•ë ¥í•œ ê±°ì‹œì  ë Œì¦ˆë¥¼ ì œê³µí•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ í›ˆë ¨ ì¤‘ ë³µì¡í•œ ëŠ¥ë ¥ì„ íšë“í•˜ëŠ” ê³¼ì •ì„ ë³µì¡ ë„¤íŠ¸ì›Œí¬ ì´ë¡ (CNT) ê´€ì ì—ì„œ ë¶„ì„í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ LLMì„ ë°©í–¥ì„± ê°€ì¤‘ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì—¬, ëª¨ë¸ì˜ ê³„ì‚° êµ¬ì„± ìš”ì†Œ(ì–´í…ì…˜ í—¤ë“œì™€ MLP)ë¥¼ ë…¸ë“œë¡œ, ê°œì… ê¸°ë°˜ ì ˆì œ ê¸°ë²•ì„ í†µí•´ ì¸ê³¼ì  ì˜í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ê°„ì„ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Pythia-14M ëª¨ë¸ì˜ 143ê°œ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ì—ì„œ êµ¬ì„± ìš”ì†Œ ê·¸ë˜í”„ì˜ ì§„í™”ë¥¼ ì¶”ì í•˜ë©°, ê·¸ë˜í”„ ì´ë¡ ì  ì§€í‘œë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ê°€ íƒìƒ‰, í†µí•©, ì •ì œì˜ ëšœë ·í•œ ë‹¨ê³„ë¥¼ ê±°ì³ ì§„í™”í•˜ë©°, ì •ë³´ í™•ì‚° êµ¬ì„± ìš”ì†Œì˜ ì•ˆì •ì  ê³„ì¸µê³¼ ë™ì  ì •ë³´ ìˆ˜ì§‘ êµ¬ì„± ìš”ì†Œê°€ ì£¼ìš” í•™ìŠµ ì‹œì ì—ì„œ ì¬êµ¬ì„±ë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” êµ¬ì„± ìš”ì†Œ ìˆ˜ì¤€ì˜ ë„¤íŠ¸ì›Œí¬ ê´€ì ì´ LLMì˜ ê¸°ëŠ¥ì  íšŒë¡œ í˜•ì„±ì„ ì£¼ë„í•˜ëŠ” ìê¸° ì¡°ì§ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê°•ë ¥í•œ ê±°ì‹œì  ì‹œê°ì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•™ìŠµ ì—­í•™ì„ ë³µì¡ ë„¤íŠ¸ì›Œí¬ ì´ë¡ (CNT)ì˜ ê´€ì ì—ì„œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤.

- 2. Transformer ê¸°ë°˜ LLMì„ ë°©í–¥ì„± ìˆëŠ” ê°€ì¤‘ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 3. Pythia-14M ëª¨ë¸ì˜ 143ê°œ í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ í†µí•´ ê·¸ë˜í”„ ì´ë¡ ì  ë©”íŠ¸ë¦­ì„ ë¶„ì„í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ê°€ íƒìƒ‰, í†µí•©, ì •ì œì˜ ë‹¨ê³„ë¥¼ ê±°ì³ ì§„í™”í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

- 4. ì •ë³´ í™•ì‚° êµ¬ì„± ìš”ì†Œì˜ ì•ˆì •ì ì¸ ê³„ì¸µ êµ¬ì¡°ì™€ ì •ë³´ ìˆ˜ì§‘ êµ¬ì„± ìš”ì†Œì˜ ë™ì  ì§‘í•©ì´ í•™ìŠµì˜ ì£¼ìš” ì‹œì ì—ì„œ ì¬êµ¬ì„±ë˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

- 5. êµ¬ì„± ìš”ì†Œ ìˆ˜ì¤€ì˜ ë„¤íŠ¸ì›Œí¬ ê´€ì ì´ LLMì˜ ê¸°ëŠ¥ì  íšŒë¡œ í˜•ì„±ì„ ì£¼ë„í•˜ëŠ” ìê¸° ì¡°ì§í™” ì›ë¦¬ë¥¼ ì‹œê°í™”í•˜ê³  ì´í•´í•˜ëŠ” ê°•ë ¥í•œ ê±°ì‹œì  ê´€ì ì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

*Generated on 2025-09-22 13:51:43*