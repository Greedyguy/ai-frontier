---
keywords:
  - Dual-Masked Autoencoder
  - Unsupervised Multivariate Time Series Representation Learning
  - Masked Time-Series Modeling
  - Self-supervised Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.16078
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:41:42.150482",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Dual-Masked Autoencoder",
    "Unsupervised Multivariate Time Series Representation Learning",
    "Masked Time-Series Modeling",
    "Self-supervised Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Dual-Masked Autoencoder": 0.8,
    "Unsupervised Multivariate Time Series Representation Learning": 0.75,
    "Masked Time-Series Modeling": 0.78,
    "Self-supervised Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Dual-Masked Autoencoder",
        "canonical": "Dual-Masked Autoencoder",
        "aliases": [
          "DMAE"
        ],
        "category": "unique_technical",
        "rationale": "A novel framework specifically introduced in the paper, providing a unique approach to MTS representation learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Unsupervised Multivariate Time Series Representation Learning",
        "canonical": "Unsupervised Multivariate Time Series Representation Learning",
        "aliases": [
          "Unsupervised MTS Representation Learning"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's focus, this concept is key for understanding the context of the proposed method.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Masked Time-Series Modeling",
        "canonical": "Masked Time-Series Modeling",
        "aliases": [
          "Masked Time-Series"
        ],
        "category": "unique_technical",
        "rationale": "Describes the innovative approach used in the paper, crucial for linking to related techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Self-supervised Learning",
        "canonical": "Self-supervised Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This concept is integral to the methodology and connects well with existing literature on learning paradigms.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "representation",
      "learning",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Dual-Masked Autoencoder",
      "resolved_canonical": "Dual-Masked Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Unsupervised Multivariate Time Series Representation Learning",
      "resolved_canonical": "Unsupervised Multivariate Time Series Representation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Masked Time-Series Modeling",
      "resolved_canonical": "Masked Time-Series Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Self-supervised Learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning

**Korean Title:** MTS-DMAE: 비지도 다변량 시계열 표현 학습을 위한 이중 마스크 자동 인코더

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16078.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.16078](https://arxiv.org/abs/2509.16078)

## 🔗 유사한 논문
- [[2025-09-22/MoCA_ Multi-modal Cross-masked Autoencoder for Digital Health Measurements_20250922|MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements]] (82.7% similar)
- [[2025-09-18/Masked Feature Modeling Enhances Adaptive Segmentation_20250918|Masked Feature Modeling Enhances Adaptive Segmentation]] (81.5% similar)
- [[2025-09-22/VMDNet_ Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding_20250922|VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding]] (80.7% similar)
- [[2025-09-17/Bridging Past and Future_ Distribution-Aware Alignment for Time Series Forecasting_20250917|Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting]] (80.6% similar)
- [[2025-09-18/Beyond Marginals_ Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection_20250918|Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection]] (80.3% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**⚡ Unique Technical**: [[keywords/Dual-Masked Autoencoder|Dual-Masked Autoencoder]], [[keywords/Unsupervised Multivariate Time Series Representation Learning|Unsupervised Multivariate Time Series Representation Learning]], [[keywords/Masked Time-Series Modeling|Masked Time-Series Modeling]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.16078v1 Announce Type: new 
Abstract: Unsupervised multivariate time series (MTS) representation learning aims to extract compact and informative representations from raw sequences without relying on labels, enabling efficient transfer to diverse downstream tasks. In this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked time-series modeling framework for unsupervised MTS representation learning. DMAE formulates two complementary pretext tasks: (1) reconstructing masked values based on visible attributes, and (2) estimating latent representations of masked features, guided by a teacher encoder. To further improve representation quality, we introduce a feature-level alignment constraint that encourages the predicted latent representations to align with the teacher's outputs. By jointly optimizing these objectives, DMAE learns temporally coherent and semantically rich representations. Comprehensive evaluations across classification, regression, and forecasting tasks demonstrate that our approach achieves consistent and superior performance over competitive baselines.

## 🔍 Abstract (한글 번역)

arXiv:2509.16078v1 발표 유형: 신규  
초록: 비지도 다변량 시계열(MTS) 표현 학습은 레이블에 의존하지 않고 원시 시퀀스로부터 간결하고 정보가 풍부한 표현을 추출하여 다양한 다운스트림 작업으로의 효율적인 전이를 가능하게 하는 것을 목표로 합니다. 이 논문에서는 비지도 MTS 표현 학습을 위한 새로운 마스크드 시계열 모델링 프레임워크인 Dual-Masked Autoencoder (DMAE)를 제안합니다. DMAE는 두 가지 상호 보완적인 사전 과제를 공식화합니다: (1) 보이는 속성을 기반으로 마스크된 값을 재구성하는 것, 그리고 (2) 교사 인코더에 의해 안내되는 마스크된 특징의 잠재 표현을 추정하는 것. 표현의 품질을 더욱 향상시키기 위해, 우리는 예측된 잠재 표현이 교사의 출력과 정렬되도록 유도하는 특징 수준의 정렬 제약을 도입합니다. 이러한 목표를 공동으로 최적화함으로써, DMAE는 시간적으로 일관되고 의미적으로 풍부한 표현을 학습합니다. 분류, 회귀 및 예측 작업 전반에 걸친 종합적인 평가를 통해 우리의 접근 방식이 경쟁력 있는 기준선보다 일관되고 우수한 성능을 달성함을 입증합니다.

## 📝 요약

이 논문에서는 비지도 다변량 시계열 표현 학습을 위한 새로운 프레임워크인 Dual-Masked Autoencoder (DMAE)를 제안합니다. DMAE는 두 가지 보조 과제를 통해 시계열 데이터를 모델링합니다: (1) 보이는 속성을 기반으로 마스킹된 값을 재구성하고, (2) 교사 인코더의 지도를 받아 마스킹된 특징의 잠재 표현을 추정합니다. 또한, 예측된 잠재 표현이 교사의 출력과 정렬되도록 하는 특징 수준의 정렬 제약을 도입하여 표현의 품질을 향상시킵니다. 이 방법론은 시계열의 시간적 일관성과 의미적 풍부함을 학습하며, 분류, 회귀, 예측 작업에서 기존 기법보다 우수한 성능을 보입니다.

## 🎯 주요 포인트

- 1. Dual-Masked Autoencoder (DMAE)는 비지도 다변량 시계열 표현 학습을 위한 새로운 프레임워크를 제안합니다.
- 2. DMAE는 두 가지 보완적인 사전 과제를 통해 마스킹된 값을 재구성하고, 마스킹된 특징의 잠재 표현을 추정합니다.
- 3. 특징 수준의 정렬 제약을 도입하여 예측된 잠재 표현이 교사 인코더의 출력과 정렬되도록 유도합니다.
- 4. DMAE는 시간적으로 일관되고 의미적으로 풍부한 표현을 학습합니다.
- 5. 분류, 회귀, 예측 작업에서 DMAE는 경쟁력 있는 기준선보다 일관되고 우수한 성능을 보여줍니다.


---

*Generated on 2025-09-23 10:41:42*