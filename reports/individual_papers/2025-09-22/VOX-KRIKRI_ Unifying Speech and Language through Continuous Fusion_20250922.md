---
keywords:
  - Large Language Model
  - Multimodal Learning
  - Attention Mechanism
  - Speech-enabled Large Language Models
  - Automatic Speech Recognition
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15667
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:32:38.905015",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multimodal Learning",
    "Attention Mechanism",
    "Speech-enabled Large Language Models",
    "Automatic Speech Recognition"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multimodal Learning": 0.88,
    "Attention Mechanism": 0.9,
    "Speech-enabled Large Language Models": 0.8,
    "Automatic Speech Recognition": 0.87
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Connects to a wide range of topics in Natural Language Processing and Machine Learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multimodal Fusion",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Integration"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the integration of multiple data types, a key aspect of the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.88
      },
      {
        "surface": "Cross-modal Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross-modal Attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the use of attention mechanisms across different modalities.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.82,
        "link_intent_score": 0.9
      },
      {
        "surface": "Speech-enabled LLMs",
        "canonical": "Speech-enabled Large Language Models",
        "aliases": [
          "Speech LLMs"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel integration of speech capabilities into language models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Automatic Speech Recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "specific_connectable",
        "rationale": "A core application area that connects to both speech and language processing.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.87
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multimodal Fusion",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Cross-modal Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.82,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Speech-enabled LLMs",
      "resolved_canonical": "Speech-enabled Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Automatic Speech Recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.87
      }
    }
  ]
}
-->

# VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion

**Korean Title:** VOX-KRIKRI: 연속적 융합을 통한 음성과 언어의 통합

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15667.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15667](https://arxiv.org/abs/2509.15667)

## 🔗 유사한 논문
- [[2025-09-22/Speech Language Models for Under-Represented Languages_ Insights from Wolof_20250922|Speech Language Models for Under-Represented Languages: Insights from Wolof]] (81.3% similar)
- [[2025-09-22/GLip_ A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition_20250922|GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition]] (80.9% similar)
- [[2025-09-22/Think, Verbalize, then Speak_ Bridging Complex Thoughts and Comprehensible Speech_20250922|Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech]] (80.8% similar)
- [[2025-09-19/Cross-Modal Knowledge Distillation for Speech Large Language Models_20250919|Cross-Modal Knowledge Distillation for Speech Large Language Models]] (80.5% similar)
- [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (80.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]]
**⚡ Unique Technical**: [[keywords/Speech-enabled Large Language Models|Speech-enabled Large Language Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15667v1 Announce Type: new 
Abstract: We present a multimodal fusion framework that bridges pre-trained decoder-based large language models (LLM) and acoustic encoder-decoder architectures such as Whisper, with the aim of building speech-enabled LLMs. Instead of directly using audio embeddings, we explore an intermediate audio-conditioned text space as a more effective mechanism for alignment. Our method operates fully in continuous text representation spaces, fusing Whisper's hidden decoder states with those of an LLM through cross-modal attention, and supports both offline and streaming modes. We introduce \textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that our approach effectively aligns representations across modalities. These results highlight continuous space fusion as a promising path for multilingual and low-resource speech LLMs, while achieving state-of-the-art results for Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative improvement across benchmarks.

## 🔍 Abstract (한글 번역)

arXiv:2509.15667v1 발표 유형: 새로운  
초록: 우리는 사전 훈련된 디코더 기반 대형 언어 모델(LLM)과 Whisper와 같은 음향 인코더-디코더 아키텍처를 연결하는 다중 모달 융합 프레임워크를 제시하며, 이를 통해 음성 지원 LLM을 구축하고자 합니다. 오디오 임베딩을 직접 사용하는 대신, 우리는 정렬을 위한 보다 효과적인 메커니즘으로 중간 오디오 조건 텍스트 공간을 탐색합니다. 우리의 방법은 연속적인 텍스트 표현 공간에서 완전히 작동하며, Whisper의 숨겨진 디코더 상태를 LLM의 상태와 교차 모달 주의를 통해 융합하고, 오프라인 및 스트리밍 모드를 모두 지원합니다. 우리는 \textit{VoxKrikri}, 최초의 그리스어 음성 LLM을 소개하며, 분석을 통해 우리의 접근 방식이 모달리티 간 표현을 효과적으로 정렬함을 보여줍니다. 이러한 결과는 다국어 및 저자원 음성 LLM을 위한 유망한 경로로서 연속 공간 융합을 강조하며, 그리스어 자동 음성 인식에서 최첨단 결과를 달성하여 벤치마크 전반에 걸쳐 평균 약 20%의 상대적 개선을 제공합니다.

## 📝 요약

이 논문은 사전 훈련된 대형 언어 모델(LLM)과 음성 인코더-디코더 구조를 연결하는 다중 모달 융합 프레임워크를 제안합니다. 오디오 임베딩 대신 오디오 조건부 텍스트 공간을 활용하여 더 효과적인 정렬을 시도합니다. Whisper의 숨겨진 디코더 상태와 LLM의 상태를 교차 모달 주의 메커니즘으로 결합하여 연속적인 텍스트 표현 공간에서 작동하며, 오프라인 및 스트리밍 모드를 지원합니다. 이 방법론을 통해 최초의 그리스어 음성 LLM인 \textit{VoxKrikri}를 소개하며, 다중 언어 및 저자원 음성 LLM에 대한 가능성을 보여줍니다. 그리스어 자동 음성 인식에서 최첨단 결과를 달성하며, 평균 약 20%의 상대적 성능 향상을 기록했습니다.

## 🎯 주요 포인트

- 1. 본 연구는 사전 훈련된 디코더 기반 대형 언어 모델(LLM)과 Whisper와 같은 음향 인코더-디코더 아키텍처를 연결하는 다중 모달 융합 프레임워크를 제안합니다.
- 2. 오디오 임베딩을 직접 사용하지 않고, 오디오 조건부 텍스트 공간을 통해 더 효과적인 정렬 메커니즘을 탐색합니다.
- 3. Whisper의 숨겨진 디코더 상태와 LLM의 상태를 교차 모달 주의를 통해 융합하여 연속 텍스트 표현 공간에서 작동하며, 오프라인 및 스트리밍 모드를 지원합니다.
- 4. 최초의 그리스어 음성 LLM인 \textit{VoxKrikri}를 소개하며, 제안된 접근 방식이 모달리티 간 표현을 효과적으로 정렬함을 보여줍니다.
- 5. 본 연구는 그리스어 자동 음성 인식에서 최첨단 성과를 달성하며, 벤치마크 전반에 걸쳐 평균 약 20%의 상대적 개선을 제공합니다.


---

*Generated on 2025-09-23 11:32:38*