---
keywords:
  - Large Language Model
  - Cross-Lingual Interaction-aware Multilingual Balancing
  - Cross-Lingual Interaction
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15556
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:03:26.311437",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Lingual Interaction-aware Multilingual Balancing",
    "Cross-Lingual Interaction"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cross-Lingual Interaction-aware Multilingual Balancing": 0.8,
    "Cross-Lingual Interaction": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus, connecting with existing discussions on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Climb",
        "canonical": "Cross-Lingual Interaction-aware Multilingual Balancing",
        "aliases": [
          "Climb framework"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework specifically developed for optimizing multilingual data allocation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "cross-lingual interactions",
        "canonical": "Cross-Lingual Interaction",
        "aliases": [
          "inter-language dependencies"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for understanding multilingual data allocation and its impact on model performance.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "multilingual capabilities",
      "language proportions",
      "training corpora"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Climb",
      "resolved_canonical": "Cross-Lingual Interaction-aware Multilingual Balancing",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "cross-lingual interactions",
      "resolved_canonical": "Cross-Lingual Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining

**Korean Title:** ë‹¤ì¤‘ì–¸ì–´ ì¡°í™” íƒêµ¬: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ì„ ìœ„í•œ ë‹¤ì¤‘ì–¸ì–´ ë°ì´í„° í• ë‹¹

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15556.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15556](https://arxiv.org/abs/2509.15556)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (86.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (85.9% similar)
- [[2025-09-19/ReCoVeR the Target Language_ Language Steering without Sacrificing Task Performance_20250919|ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance]] (85.3% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (84.9% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (84.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Cross-Lingual Interaction|Cross-Lingual Interaction]]
**âš¡ Unique Technical**: [[keywords/Cross-Lingual Interaction-aware Multilingual Balancing|Cross-Lingual Interaction-aware Multilingual Balancing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15556v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become integral to a wide range of applications worldwide, driving an unprecedented global demand for effective multilingual capabilities. Central to achieving robust multilingual performance is the strategic allocation of language proportions within training corpora. However, determining optimal language ratios is highly challenging due to intricate cross-lingual interactions and sensitivity to dataset scale. This paper introduces Climb (Cross-Lingual Interaction-aware Multilingual Balancing), a novel framework designed to systematically optimize multilingual data allocation. At its core, Climb introduces a cross-lingual interaction-aware language ratio, explicitly quantifying each language's effective allocation by capturing inter-language dependencies. Leveraging this ratio, Climb proposes a principled two-step optimization procedure--first equalizing marginal benefits across languages, then maximizing the magnitude of the resulting language allocation vectors--significantly simplifying the inherently complex multilingual optimization problem. Extensive experiments confirm that Climb can accurately measure cross-lingual interactions across various multilingual settings. LLMs trained with Climb-derived proportions consistently achieve state-of-the-art multilingual performance, even achieving competitive performance with open-sourced LLMs trained with more tokens.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15556v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì „ ì„¸ê³„ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì— í•„ìˆ˜ì ì¸ ìš”ì†Œê°€ ë˜ì—ˆìœ¼ë©°, íš¨ê³¼ì ì¸ ë‹¤êµ­ì–´ ê¸°ëŠ¥ì— ëŒ€í•œ ì „ë¡€ ì—†ëŠ” ê¸€ë¡œë²Œ ìˆ˜ìš”ë¥¼ ì´‰ì§„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê°•ë ¥í•œ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” í›ˆë ¨ ì½”í¼ìŠ¤ ë‚´ì—ì„œ ì–¸ì–´ ë¹„ìœ¨ì˜ ì „ëµì  í• ë‹¹ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœì ì˜ ì–¸ì–´ ë¹„ìœ¨ì„ ê²°ì •í•˜ëŠ” ê²ƒì€ ë³µì¡í•œ ì–¸ì–´ ê°„ ìƒí˜¸ì‘ìš©ê³¼ ë°ì´í„°ì…‹ ê·œëª¨ì— ëŒ€í•œ ë¯¼ê°ì„± ë•Œë¬¸ì— ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤êµ­ì–´ ë°ì´í„° í• ë‹¹ì„ ì²´ê³„ì ìœ¼ë¡œ ìµœì í™”í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ Climb (Cross-Lingual Interaction-aware Multilingual Balancing)ì„ ì†Œê°œí•©ë‹ˆë‹¤. Climbì˜ í•µì‹¬ì€ ì–¸ì–´ ê°„ ì¢…ì†ì„±ì„ í¬ì°©í•˜ì—¬ ê° ì–¸ì–´ì˜ íš¨ê³¼ì ì¸ í• ë‹¹ì„ ëª…ì‹œì ìœ¼ë¡œ ì •ëŸ‰í™”í•˜ëŠ” ì–¸ì–´ ê°„ ìƒí˜¸ì‘ìš© ì¸ì‹ ì–¸ì–´ ë¹„ìœ¨ì„ ë„ì…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë¹„ìœ¨ì„ í™œìš©í•˜ì—¬ Climbì€ ì›ì¹™ì ì¸ ë‘ ë‹¨ê³„ ìµœì í™” ì ˆì°¨ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ë¡œ ì–¸ì–´ ê°„ í•œê³„ ì´ìµì„ ê· ë“±í™”í•˜ê³ , ë‘ ë²ˆì§¸ë¡œ ê²°ê³¼ ì–¸ì–´ í• ë‹¹ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ìµœëŒ€í™”í•˜ì—¬ ë³¸ì§ˆì ìœ¼ë¡œ ë³µì¡í•œ ë‹¤êµ­ì–´ ìµœì í™” ë¬¸ì œë¥¼ í¬ê²Œ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ Climbì´ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ ì„¤ì •ì—ì„œ ì–¸ì–´ ê°„ ìƒí˜¸ì‘ìš©ì„ ì •í™•í•˜ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. Climbì—ì„œ ë„ì¶œëœ ë¹„ìœ¨ë¡œ í›ˆë ¨ëœ LLMì€ ì¼ê´€ë˜ê²Œ ìµœì²¨ë‹¨ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ë” ë§ì€ í† í°ìœ¼ë¡œ í›ˆë ¨ëœ ì˜¤í”ˆ ì†ŒìŠ¤ LLMê³¼ë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ Climbì„ ì†Œê°œí•©ë‹ˆë‹¤. Climbì€ ì–¸ì–´ ê°„ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•œ ì–¸ì–´ ë¹„ìœ¨ì„ ë„ì…í•˜ì—¬ ê° ì–¸ì–´ì˜ íš¨ê³¼ì ì¸ í• ë‹¹ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Climbì€ ì–¸ì–´ ê°„ í•œê³„ ì´ìµì„ ê· ë“±í™”í•˜ê³ , ê²°ê³¼ ì–¸ì–´ í• ë‹¹ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë‘ ë‹¨ê³„ ìµœì í™” ì ˆì°¨ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, Climbì„ í†µí•´ í•™ìŠµëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ë” ë§ì€ í† í°ìœ¼ë¡œ í•™ìŠµëœ ê³µê°œ LLMê³¼ë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ClimbëŠ” ë‹¤êµ­ì–´ ë°ì´í„° í• ë‹¹ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ì–¸ì–´ ê°„ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•œ ì–¸ì–´ ë¹„ìœ¨ì„ ë„ì…í•©ë‹ˆë‹¤.
- 2. ClimbëŠ” ì–¸ì–´ ê°„ ì˜ì¡´ì„±ì„ í¬ì°©í•˜ì—¬ ê° ì–¸ì–´ì˜ íš¨ê³¼ì ì¸ í• ë‹¹ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.
- 3. Climbì˜ ìµœì í™” ì ˆì°¨ëŠ” ì–¸ì–´ ê°„ í•œê³„ ì´ìµì„ ê· ë“±í™”í•˜ê³ , ê²°ê³¼ ì–¸ì–´ í• ë‹¹ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
- 4. Climbë¥¼ í†µí•´ í›ˆë ¨ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 5. ClimbëŠ” ë” ë§ì€ í† í°ìœ¼ë¡œ í›ˆë ¨ëœ ì˜¤í”ˆì†ŒìŠ¤ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:03:26*