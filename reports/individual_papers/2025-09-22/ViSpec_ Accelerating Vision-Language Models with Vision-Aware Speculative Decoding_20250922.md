---
keywords:
  - Vision-Aware Speculative Decoding
  - Vision-Language Model
  - Speculative Decoding
  - Multimodal Learning
  - Attention Mechanism
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15235
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:37:40.397111",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Aware Speculative Decoding",
    "Vision-Language Model",
    "Speculative Decoding",
    "Multimodal Learning",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Aware Speculative Decoding": 0.78,
    "Vision-Language Model": 0.82,
    "Speculative Decoding": 0.77,
    "Multimodal Learning": 0.75,
    "Attention Mechanism": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Aware Speculative Decoding",
        "canonical": "Vision-Aware Speculative Decoding",
        "aliases": [
          "ViSpec"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework specifically designed to enhance the performance of vision-language models.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Central to the paper's focus on improving multimodal model efficiency.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key technique discussed for accelerating model inference.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multimodal Coherence",
        "canonical": "Multimodal Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Enhancing coherence across modalities is a significant aspect of the proposed method.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Integral to the integration of image tokens in the draft model.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "large language models",
      "multimodal datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Aware Speculative Decoding",
      "resolved_canonical": "Vision-Aware Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multimodal Coherence",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding

**Korean Title:** ViSpec: ë¹„ì „ ì¸ì‹ ì¶”ì¸¡ ë””ì½”ë”©ì„ í†µí•œ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ê°€ì†í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15235.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15235](https://arxiv.org/abs/2509.15235)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (84.9% similar)
- [[2025-09-22/ViLU_ Learning Vision-Language Uncertainties for Failure Prediction_20250922|ViLU: Learning Vision-Language Uncertainties for Failure Prediction]] (84.4% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (84.0% similar)
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (83.9% similar)
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (83.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Vision-Aware Speculative Decoding|Vision-Aware Speculative Decoding]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15235v1 Announce Type: cross 
Abstract: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (<1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft model's attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target model's hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15235v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¶”ì¸¡ ë””ì½”ë”©ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ ë„ë¦¬ ì±„íƒëœ ê¸°ìˆ ì´ì§€ë§Œ, ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì—ì˜ ì ìš©ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ê¸°ì¡´ ë°©ë²•ì€ ë‹¨ì§€ ë¯¸ë¯¸í•œ ì†ë„ í–¥ìƒ(<1.5ë°°)ë§Œì„ ë‹¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ëŠ¥ì´ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì¤‘ì‹¬ì´ ë¨ì— ë”°ë¼ ì ì  ë” ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€í˜• VLMì´ í…ìŠ¤íŠ¸ ì´í•´ë¥¼ ì†ìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ê³„ì¸µë³„ë¡œ ì¤‘ë³µëœ ì´ë¯¸ì§€ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•„í„°ë§í•  ìˆ˜ ìˆëŠ” ë°˜ë©´, ì‘ì€ ì´ˆì•ˆ ëª¨ë¸ì€ ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” VLMì— ë§ì¶˜ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ Vision-Aware Speculative Decoding (ViSpec)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ViSpecì€ ê²½ëŸ‰ì˜ ë¹„ì „ ì–´ëŒ‘í„° ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í† í°ì„ ì••ì¶•ëœ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ì´ˆì•ˆ ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— ì›ë˜ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë§¤ë„ëŸ½ê²Œ í†µí•©í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, ê° ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•œ ì „ì—­ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ëª¨ë“  í›„ì† í…ìŠ¤íŠ¸ í† í°ì— ì´ íŠ¹ì§•ì„ ì¶”ê°€í•˜ì—¬ ë‹¤ì¤‘ ëª¨ë‹¬ ì¼ê´€ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ê¸´ ë³´ì¡° ì‘ë‹µì„ í¬í•¨í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ì…‹ì˜ ë¶€ì¡±ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´, ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì¬í™œìš©í•˜ê³  ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ VLMìœ¼ë¡œ í™•ì¥ëœ ì¶œë ¥ì„ ìƒì„±í•˜ì—¬ íŠ¹ìˆ˜í•œ í•™ìŠµ ë°ì´í„°ì…‹ì„ íë ˆì´ì…˜í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í•™ìŠµ ì „ëµì€ ì´ˆì•ˆ ëª¨ë¸ì´ ëŒ€ìƒ ëª¨ë¸ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ í•™ìŠµ ì§€ë¦„ê¸¸ì„ ì°¾ëŠ” ìœ„í—˜ì„ ì™„í™”í•˜ë©°, ì´ëŠ” ëŒ€ìƒ ëª¨ë¸ ì¶œë ¥ë§Œìœ¼ë¡œ í•™ìŠµí•  ê²½ìš° ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ViSpecì´ VLMì˜ ì¶”ì¸¡ ë””ì½”ë”©ì—ì„œ ìµœì´ˆë¡œ ìƒë‹¹í•œ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆìŒì„ ê²€ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¶”ì¸¡ ë””ì½”ë”© ê¸°ë²•ì„ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLM)ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ VLMì—ì„œëŠ” ì†ë„ í–¥ìƒì´ ë¯¸ë¯¸í–ˆìœ¼ë‚˜, ì œì•ˆëœ Vision-Aware Speculative Decoding(ViSpec) í”„ë ˆì„ì›Œí¬ëŠ” ê²½ëŸ‰ì˜ ë¹„ì „ ì–´ëŒ‘í„° ëª¨ë“ˆì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì••ì¶•í•˜ê³ , ì´ë¥¼ ì´ˆì•ˆ ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— í†µí•©í•©ë‹ˆë‹¤. ë˜í•œ, ê° ì´ë¯¸ì§€ì— ëŒ€í•œ ê¸€ë¡œë²Œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ í† í°ì— ì¶”ê°€í•¨ìœ¼ë¡œì¨ ë‹¤ì¤‘ ëª¨ë‹¬ ì¼ê´€ì„±ì„ ë†’ì…ë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì¬êµ¬ì„±í•˜ê³  í™•ì¥ëœ ì¶œë ¥ì„ ìƒì„±í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë§ˆë ¨í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ í†µí•´ ì´ˆì•ˆ ëª¨ë¸ì´ ëª©í‘œ ëª¨ë¸ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ì§ì ‘ í™œìš©í•˜ì§€ ì•Šë„ë¡ í•˜ì—¬ í•™ìŠµì˜ ì§€ë¦„ê¸¸ì„ ë°©ì§€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ViSpecëŠ” VLMì˜ ì¶”ì¸¡ ë””ì½”ë”©ì—ì„œ ìµœì´ˆë¡œ ìœ ì˜ë¯¸í•œ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision-Aware Speculative Decoding (ViSpec)ëŠ” ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì„ ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì••ì¶•í•˜ì—¬ í…ìŠ¤íŠ¸ ì´í•´ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. ViSpecëŠ” ê²½ëŸ‰ì˜ ë¹„ì „ ì–´ëŒ‘í„° ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í† í°ì„ ì••ì¶•í•˜ê³ , ì´ë¥¼ ì´ˆì•ˆ ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— í†µí•©í•˜ì—¬ ì›ë˜ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
- 3. ê° ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ê¸€ë¡œë²Œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ëª¨ë“  í›„ì† í…ìŠ¤íŠ¸ í† í°ì— ì¶”ê°€í•˜ì—¬ ë‹¤ì¤‘ ëª¨ë‹¬ ì¼ê´€ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.
- 4. ViSpecëŠ” ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì¬í™œìš©í•˜ê³  íƒ€ê²Ÿ VLMì„ ì‚¬ìš©í•˜ì—¬ í™•ì¥ëœ ì¶œë ¥ì„ ìƒì„±í•¨ìœ¼ë¡œì¨, ê¸´ ë³´ì¡° ì‘ë‹µì„ í¬í•¨í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ì…‹ì˜ ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 5. ViSpecëŠ” VLM ì¶”ë¡  ì†ë„ë¥¼ ì‹¤ì§ˆì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ë©°, ì´ëŠ” VLM ì¶”ë¡  ê°€ì†í™”ì—ì„œ ì²« ë²ˆì§¸ë¡œ ì˜ë¯¸ ìˆëŠ” ì„±ê³¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:37:40*