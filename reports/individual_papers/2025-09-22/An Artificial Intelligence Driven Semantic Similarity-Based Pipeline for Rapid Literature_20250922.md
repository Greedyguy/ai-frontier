---
keywords:
  - Semantic Similarity
  - Transformer
  - Cosine Similarity
  - Embedding Models
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15292
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:40:36.375361",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Semantic Similarity",
    "Transformer",
    "Cosine Similarity",
    "Embedding Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Semantic Similarity": 0.85,
    "Transformer": 0.89,
    "Cosine Similarity": 0.82,
    "Embedding Models": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "semantic similarity",
        "canonical": "Semantic Similarity",
        "aliases": [
          "similarity measure",
          "semantic closeness"
        ],
        "category": "specific_connectable",
        "rationale": "Semantic similarity is crucial for linking related literature and concepts, enhancing connectivity.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "transformer based embeddings",
        "canonical": "Transformer",
        "aliases": [
          "transformer embeddings",
          "transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a fundamental technology in NLP, providing a strong link to related research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.92,
        "specificity_score": 0.68,
        "link_intent_score": 0.89
      },
      {
        "surface": "cosine similarity",
        "canonical": "Cosine Similarity",
        "aliases": [
          "cosine measure",
          "cosine distance"
        ],
        "category": "specific_connectable",
        "rationale": "Cosine similarity is a widely used metric for measuring semantic similarity, facilitating connections.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "embedding models",
        "canonical": "Embedding Models",
        "aliases": [
          "vector embeddings",
          "embedding techniques"
        ],
        "category": "specific_connectable",
        "rationale": "Embedding models are key in transforming text into numerical representations, linking various NLP tasks.",
        "novelty_score": 0.58,
        "connectivity_score": 0.87,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "automated pipeline",
      "literature review"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "semantic similarity",
      "resolved_canonical": "Semantic Similarity",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "transformer based embeddings",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.92,
        "specificity": 0.68,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "cosine similarity",
      "resolved_canonical": "Cosine Similarity",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "embedding models",
      "resolved_canonical": "Embedding Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.87,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature

**Korean Title:** ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ì„± ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ì‹ ì†í•œ ë¬¸í—Œ ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15292.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15292](https://arxiv.org/abs/2509.15292)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/OpenLens AI_ Fully Autonomous Research Agent for Health Infomatics_20250919|OpenLens AI: Fully Autonomous Research Agent for Health Infomatics]] (79.6% similar)
- [[2025-09-22/Efficient Extractive Text Summarization for Online News Articles Using Machine Learning_20250922|Efficient Extractive Text Summarization for Online News Articles Using Machine Learning]] (79.6% similar)
- [[2025-09-19/MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs]] (78.2% similar)
- [[2025-09-22/AI Copilots for Reproducibility in Science_ A Case Study_20250922|AI Copilots for Reproducibility in Science: A Case Study]] (77.9% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility: Process-Supervised Rewrite for RAG]] (77.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Semantic Similarity|Semantic Similarity]], [[keywords/Cosine Similarity|Cosine Similarity]], [[keywords/Embedding Models|Embedding Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15292v1 Announce Type: new 
Abstract: We propose an automated pipeline for performing literature reviews using semantic similarity. Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity. By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input. Three embedding models were evaluated. A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline. Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15292v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ìš°ë¦¬ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸í—Œ ë¦¬ë·°ë¥¼ ìˆ˜í–‰í•˜ëŠ” ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì „í†µì ì¸ ì²´ê³„ì  ë¦¬ë·° ì‹œìŠ¤í…œì´ë‚˜ ìµœì í™” ê¸°ë°˜ ë°©ë²•ê³¼ ë‹¬ë¦¬, ì´ ì—°êµ¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œì™€ ë†’ì€ ê´€ë ¨ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ì„ ì œê³µí•¨ìœ¼ë¡œì¨ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ê³ , ì˜¤í”ˆ ì•¡ì„¸ìŠ¤ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë…¼ë¬¸ì„ ê°€ì ¸ì˜¤ë©°, ì…ë ¥ê³¼ì˜ ì˜ë¯¸ì  ê·¼ì ‘ì„±ì— ë”°ë¼ ì´ë¥¼ ìˆœìœ„í™”í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ì„ë² ë”© ëª¨ë¸ì´ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ í†µê³„ì  ì„ê³„ê°’ ì ‘ê·¼ë²•ì„ ì ìš©í•˜ì—¬ ê´€ë ¨ ë…¼ë¬¸ì„ í•„í„°ë§í•˜ì—¬ íš¨ê³¼ì ì¸ ë¬¸í—Œ ë¦¬ë·° íŒŒì´í”„ë¼ì¸ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. íœ´ë¦¬ìŠ¤í‹± í”¼ë“œë°±ì´ë‚˜ ì‹¤ì œ ê´€ë ¨ì„± ë ˆì´ë¸”ì´ ì—†ëŠ” ìƒí™©ì—ì„œë„, ì œì•ˆëœ ì‹œìŠ¤í…œì€ ì˜ˆë¹„ ì—°êµ¬ ë° íƒìƒ‰ì  ë¶„ì„ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ì‹¤ìš©ì ì¸ ë„êµ¬ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¬¸í—Œ ë¦¬ë·°ë¥¼ ìë™í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì „í†µì ì¸ ì²´ê³„ì  ë¦¬ë·° ì‹œìŠ¤í…œì´ë‚˜ ìµœì í™” ê¸°ë°˜ ë°©ë²•ê³¼ ë‹¬ë¦¬, ì´ ì—°êµ¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œì™€ ë†’ì€ ê´€ë ¨ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ê³ , ì˜¤í”ˆ ì•¡ì„¸ìŠ¤ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë…¼ë¬¸ì„ ê²€ìƒ‰í•œ í›„ ì…ë ¥ê³¼ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ë”°ë¼ ë…¼ë¬¸ì„ ìˆœìœ„í™”í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ì„ë² ë”© ëª¨ë¸ì„ í‰ê°€í–ˆìœ¼ë©°, í†µê³„ì  ì„ê³„ê°’ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•´ ê´€ë ¨ ë…¼ë¬¸ì„ í•„í„°ë§í•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ íœ´ë¦¬ìŠ¤í‹± í”¼ë“œë°±ì´ë‚˜ ì‹¤ì œ ê´€ë ¨ì„± ë ˆì´ë¸” ì—†ì´ë„ ì˜ˆë¹„ ì—°êµ¬ ë° íƒìƒ‰ì  ë¶„ì„ì— ìœ ìš©í•œ ë„êµ¬ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ë¬¸í—Œ ë¦¬ë·°ë¥¼ ìë™í™”í•˜ê¸° ìœ„í•´ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ í™œìš©í•œ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œì™€ ë†’ì€ ê´€ë ¨ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
- 3. ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ê³ , ì˜¤í”ˆ ì•¡ì„¸ìŠ¤ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë…¼ë¬¸ì„ ê²€ìƒ‰ ë° ìˆœìœ„í™”í•©ë‹ˆë‹¤.
- 4. ì„¸ ê°€ì§€ ì„ë² ë”© ëª¨ë¸ì„ í‰ê°€í•˜ê³ , í†µê³„ì  ì„ê³„ê°’ ì„¤ì • ë°©ë²•ì„ í†µí•´ ê´€ë ¨ ë…¼ë¬¸ì„ í•„í„°ë§í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ì‹œìŠ¤í…œì€ ì´ˆê¸° ì—°êµ¬ ë° íƒìƒ‰ì  ë¶„ì„ì— ìœ ë§í•œ ë„êµ¬ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 08:40:36*