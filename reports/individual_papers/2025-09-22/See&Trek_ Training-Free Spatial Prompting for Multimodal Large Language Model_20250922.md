---
keywords:
  - Multimodal Learning
  - Visual Diversity
  - Motion Reconstruction
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.16087
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:28:20.536989",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Visual Diversity",
    "Motion Reconstruction",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Visual Diversity": 0.7,
    "Motion Reconstruction": 0.72,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a trending concept that enhances the understanding of models using multiple data types, making it highly relevant for linking.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Visual Diversity",
        "canonical": "Visual Diversity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is unique to the paper's approach, focusing on enhancing spatial understanding through diverse visual inputs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      },
      {
        "surface": "Motion Reconstruction",
        "canonical": "Motion Reconstruction",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Motion Reconstruction is a specialized technique used in the paper to simulate visual trajectories, crucial for spatial reasoning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are an evolved concept that integrates visual and linguistic data, relevant for linking multimodal approaches.",
        "novelty_score": 0.58,
        "connectivity_score": 0.85,
        "specificity_score": 0.76,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Training-Free",
      "GPU-Free"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Visual Diversity",
      "resolved_canonical": "Visual Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Motion Reconstruction",
      "resolved_canonical": "Motion Reconstruction",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.85,
        "specificity": 0.76,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model

**Korean Title:** See&Trek: ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” ê³µê°„ í”„ë¡¬í”„íŠ¸ ê¸°ë²•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16087.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.16087](https://arxiv.org/abs/2509.16087)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (85.3% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (84.5% similar)
- [[2025-09-22/GRE Suite_ Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains_20250922|GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains]] (83.5% similar)
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (83.0% similar)
- [[2025-09-22/Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models_20250922|Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Visual Diversity|Visual Diversity]], [[keywords/Motion Reconstruction|Motion Reconstruction]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16087v1 Announce Type: cross 
Abstract: We introduce SEE&amp;TREK, the first training-free prompting framework tailored to enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints. While prior efforts have incorporated modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains underexplored. SEE&amp;TREK addresses this gap by focusing on two core principles: increasing visual diversity and motion reconstruction. For visual diversity, we conduct Maximum Semantic Richness Sampling, which employs an off-the-shell perception model to extract semantically rich keyframes that capture scene structure. For motion reconstruction, we simulate visual trajectories and encode relative spatial positions into keyframes to preserve both spatial relations and temporal coherence. Our method is training&amp;GPU-free, requiring only a single forward pass, and can be seamlessly integrated into existing MLLM'S. Extensive experiments on the VSI-B ENCH and STI-B ENCH show that S EE &amp;T REK consistently boosts various MLLM S performance across diverse spatial reasoning tasks with the most +3.5% improvement, offering a promising path toward stronger spatial intelligence.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16087v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ìš°ë¦¬ëŠ” ì‹œê° ì „ìš© ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ê³µê°„ ì´í•´ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë§ì¶¤í™”ëœ ìµœì´ˆì˜ í›ˆë ¨ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ í”„ë ˆì„ì›Œí¬ì¸ SEE&TREKì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ì „ì˜ ë…¸ë ¥ë“¤ì€ ê¹Šì´ ë˜ëŠ” í¬ì¸íŠ¸ í´ë¼ìš°ë“œì™€ ê°™ì€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í¬í•¨í•˜ì—¬ ê³µê°„ ì¶”ë¡ ì„ ê°œì„ í•˜ë ¤ê³  í–ˆì§€ë§Œ, ìˆœìˆ˜í•œ ì‹œê°ì  ê³µê°„ ì´í•´ëŠ” ì—¬ì „íˆ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SEE&TREKì€ ì‹œê°ì  ë‹¤ì–‘ì„± ì¦ê°€ì™€ ìš´ë™ ì¬êµ¬ì„±ì´ë¼ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ì›ì¹™ì— ì´ˆì ì„ ë§ì¶”ì–´ ì´ ê²©ì°¨ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì‹œê°ì  ë‹¤ì–‘ì„±ì„ ìœ„í•´, ìš°ë¦¬ëŠ” ì¥ë©´ êµ¬ì¡°ë¥¼ í¬ì°©í•˜ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ í’ë¶€í•œ í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì˜¤í”„ë”ì…¸ ì¸ì‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ìµœëŒ€ ì˜ë¯¸ í’ë¶€ì„± ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìš´ë™ ì¬êµ¬ì„±ì„ ìœ„í•´, ìš°ë¦¬ëŠ” ì‹œê°ì  ê¶¤ì ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  ìƒëŒ€ì  ê³µê°„ ìœ„ì¹˜ë¥¼ í‚¤í”„ë ˆì„ì— ì¸ì½”ë”©í•˜ì—¬ ê³µê°„ì  ê´€ê³„ì™€ ì‹œê°„ì  ì¼ê´€ì„±ì„ ëª¨ë‘ ë³´ì¡´í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ í›ˆë ¨ ë° GPUê°€ í•„ìš” ì—†ìœ¼ë©° ë‹¨ì¼ ìˆœë°©í–¥ íŒ¨ìŠ¤ë§Œ í•„ìš”í•˜ë©° ê¸°ì¡´ MLLMì— ì›í™œí•˜ê²Œ í†µí•©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. VSI-BENCH ë° STI-BENCHì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì—ì„œ SEE&TREKì€ ë‹¤ì–‘í•œ ê³µê°„ ì¶”ë¡  ì‘ì—…ì—ì„œ ë‹¤ì–‘í•œ MLLMì˜ ì„±ëŠ¥ì„ ì¼ê´€ë˜ê²Œ í–¥ìƒì‹œì¼œ ìµœëŒ€ 3.5%ì˜ ê°œì„ ì„ ì œê³µí•˜ë©°, ë” ê°•ë ¥í•œ ê³µê°„ ì§€ëŠ¥ì„ í–¥í•œ ìœ ë§í•œ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

SEE&TREKì€ ì‹œê°ì  ì œì•½ í•˜ì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ê³µê°„ ì´í•´ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìµœì´ˆì˜ í›ˆë ¨ ë¶ˆí•„ìš” í”„ë¡¬í”„íŠ¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ê¹Šì´ ì •ë³´ë‚˜ í¬ì¸íŠ¸ í´ë¼ìš°ë“œì™€ ê°™ì€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í–ˆì§€ë§Œ, ìˆœìˆ˜í•œ ì‹œê°ì  ê³µê°„ ì´í•´ëŠ” ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SEE&TREKì€ ì‹œê°ì  ë‹¤ì–‘ì„± ì¦ëŒ€ì™€ ì›€ì§ì„ ì¬êµ¬ì„±ì´ë¼ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ì›ì¹™ì— ì´ˆì ì„ ë§ì¶”ì–´ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì‹œê°ì  ë‹¤ì–‘ì„±ì„ ìœ„í•´, ì¥ë©´ êµ¬ì¡°ë¥¼ í¬ì°©í•˜ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ í’ë¶€í•œ í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•˜ëŠ” ìµœëŒ€ ì˜ë¯¸ í’ë¶€ì„± ìƒ˜í”Œë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì›€ì§ì„ ì¬êµ¬ì„±ì„ ìœ„í•´, ì‹œê°ì  ê¶¤ì ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  ìƒëŒ€ì  ê³µê°„ ìœ„ì¹˜ë¥¼ í‚¤í”„ë ˆì„ì— ì¸ì½”ë”©í•˜ì—¬ ê³µê°„ ê´€ê³„ì™€ ì‹œê°„ì  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í›ˆë ¨ì´ë‚˜ GPUê°€ í•„ìš” ì—†ìœ¼ë©°, ê¸°ì¡´ MLLMì— ì‰½ê²Œ í†µí•©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. VSI-BENCHì™€ STI-BENCHì—ì„œì˜ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼, SEE&TREKì€ ë‹¤ì–‘í•œ ê³µê°„ ì¶”ë¡  ì‘ì—…ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 3.5% í–¥ìƒì‹œí‚¤ë©°, ê°•ë ¥í•œ ê³µê°„ ì§€ëŠ¥ìœ¼ë¡œì˜ ìœ ë§í•œ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SEE&TREKì€ ì‹œê°ì  ì œì•½ í•˜ì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMS)ì˜ ê³µê°„ ì´í•´ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìµœì´ˆì˜ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í”„ë¡¬í”„íŒ… í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì‹œê°ì  ë‹¤ì–‘ì„± ì¦ê°€ì™€ ìš´ë™ ì¬êµ¬ì„±ì„ í•µì‹¬ ì›ì¹™ìœ¼ë¡œ í•˜ì—¬ ìˆœìˆ˜í•œ ì‹œê°ì  ê³µê°„ ì´í•´ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
- 3. ìµœëŒ€ ì˜ë¯¸ í’ë¶€ ìƒ˜í”Œë§ì„ í†µí•´ ì¥ë©´ êµ¬ì¡°ë¥¼ í¬ì°©í•˜ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ í’ë¶€í•œ í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ì‹œê°ì  ë‹¤ì–‘ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 4. ìš´ë™ ì¬êµ¬ì„±ì„ ìœ„í•´ ì‹œê°ì  ê¶¤ì ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  í‚¤í”„ë ˆì„ì— ìƒëŒ€ì  ê³µê°„ ìœ„ì¹˜ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ê³µê°„ì  ê´€ê³„ì™€ ì‹œê°„ì  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 5. SEE&TREKì€ í›ˆë ¨ ë° GPUê°€ í•„ìš” ì—†ìœ¼ë©°, ë‹¤ì–‘í•œ ê³µê°„ ì¶”ë¡  ê³¼ì œì—ì„œ MLLMSì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 3.5% í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:28:20*