---
keywords:
  - Large Language Model
  - Cross-Lingual Reward Model
  - Mathematical Reasoning
  - Multilingual Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15811
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:16:32.119910",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Lingual Reward Model",
    "Mathematical Reasoning",
    "Multilingual Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cross-Lingual Reward Model": 0.78,
    "Mathematical Reasoning": 0.8,
    "Multilingual Language Model": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to broader discussions on language model capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-Lingual Reward Model",
        "canonical": "Cross-Lingual Reward Model",
        "aliases": [
          "Cross-Lingual RM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to reward modeling across languages, enhancing multilingual reasoning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mathematical Reasoning",
        "canonical": "Mathematical Reasoning",
        "aliases": [
          "Math Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Focuses on a specific application of LLMs, relevant to specialized reasoning tasks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multilingual Models",
        "canonical": "Multilingual Language Model",
        "aliases": [
          "Multilingual LLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the multilingual aspect of LLMs, connecting to cross-lingual research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-Lingual Reward Model",
      "resolved_canonical": "Cross-Lingual Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mathematical Reasoning",
      "resolved_canonical": "Mathematical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multilingual Models",
      "resolved_canonical": "Multilingual Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning

**Korean Title:** Best-of-L: ìˆ˜í•™ì  ì¶”ë¡ ì„ ìœ„í•œ êµì°¨ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ë§

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15811.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15811](https://arxiv.org/abs/2509.15811)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Language Mixing in Reasoning Language Models_ Patterns, Impact, and Internal Causes_20250922|Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes]] (87.0% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (85.2% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (84.9% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (84.9% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Mathematical Reasoning|Mathematical Reasoning]], [[keywords/Multilingual Language Model|Multilingual Language Model]]
**âš¡ Unique Technical**: [[keywords/Cross-Lingual Reward Model|Cross-Lingual Reward Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15811v1 Announce Type: cross 
Abstract: While the reasoning abilities of large language models (LLMs) continue to advance, it remains unclear how such ability varies across languages in multilingual LLMs and whether different languages produce reasoning paths that complement each other. To investigate this question, we train a reward model to rank generated responses for a given question across languages. Our results show that our cross-lingual reward model substantially improves mathematical reasoning performance compared to using reward modeling within a single language, benefiting even high-resource languages. While English often exhibits the highest performance in multilingual models, we find that cross-lingual sampling particularly benefits English under low sampling budgets. Our findings reveal new opportunities to improve multilingual reasoning by leveraging the complementary strengths of diverse languages.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15811v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì´ ê³„ì† ë°œì „í•˜ê³  ìˆì§€ë§Œ, ë‹¤êµ­ì–´ LLMì—ì„œ ì´ëŸ¬í•œ ëŠ¥ë ¥ì´ ì–¸ì–´ì— ë”°ë¼ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€, ê·¸ë¦¬ê³  ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ê°€ ì„œë¡œ ë³´ì™„í•˜ëŠ” ì¶”ë¡  ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ”ì§€ ì—¬ë¶€ëŠ” ì—¬ì „íˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì— ê±¸ì³ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„±ëœ ì‘ë‹µì„ ìˆœìœ„ ë§¤ê¸°ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ë‹¨ì¼ ì–¸ì–´ ë‚´ì—ì„œ ë³´ìƒ ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë¹„êµí•˜ì—¬, êµì°¨ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ì´ ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ ìƒë‹¹íˆ í–¥ìƒì‹œí‚¤ë©°, ì‹¬ì§€ì–´ ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ë„ ì´ì ì´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ì–´ëŠ” ì¢…ì¢… ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìš°ë¦¬ëŠ” êµì°¨ ì–¸ì–´ ìƒ˜í”Œë§ì´ ë‚®ì€ ìƒ˜í”Œë§ ì˜ˆì‚° í•˜ì—ì„œ íŠ¹íˆ ì˜ì–´ì— ì´ì ì„ ì¤€ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°œê²¬ì€ ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìƒí˜¸ ë³´ì™„ì ì¸ ê°•ì ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì¶”ë¡ ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤êµ­ì–´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì–¸ì–´ ê°„ ì¶”ë¡  ëŠ¥ë ¥ì˜ ì°¨ì´ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬íŒ€ì€ ì–¸ì–´ë³„ë¡œ ìƒì„±ëœ ì‘ë‹µì„ í‰ê°€í•˜ëŠ” ë³´ìƒ ëª¨ë¸ì„ ê°œë°œí•˜ì—¬, ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ, ì˜ì–´ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì €ë¹„ìš© ìƒ˜í”Œë§ í™˜ê²½ì—ì„œëŠ” ì–¸ì–´ ê°„ ìƒ˜í”Œë§ì´ ì˜ì–´ì˜ ì„±ëŠ¥ì„ ë”ìš± ê°œì„ ì‹œí‚µë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìƒí˜¸ ë³´ì™„ì  ê°•ì ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì¶”ë¡ ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì€ ì–¸ì–´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ë©°, ë‹¤ì–‘í•œ ì–¸ì–´ê°€ ì„œë¡œ ë³´ì™„ì ì¸ ì¶”ë¡  ê²½ë¡œë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.
- 2. ì—°êµ¬ ê²°ê³¼, êµì°¨ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ì´ ë‹¨ì¼ ì–¸ì–´ ë³´ìƒ ëª¨ë¸ì— ë¹„í•´ ìˆ˜í•™ì  ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ì´ëŠ” ìì›ì´ í’ë¶€í•œ ì–¸ì–´ì—ë„ ì´ì ì„ ì œê³µí•œë‹¤.
- 3. ì˜ì–´ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ì¢…ì¢… ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, êµì°¨ ì–¸ì–´ ìƒ˜í”Œë§ì€ ë‚®ì€ ìƒ˜í”Œë§ ì˜ˆì‚° í•˜ì—ì„œ íŠ¹íˆ ì˜ì–´ì— ì´ì ì„ ì¤€ë‹¤.
- 4. ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìƒí˜¸ ë³´ì™„ì  ê°•ì ì„ í™œìš©í•˜ì—¬ ë‹¤êµ­ì–´ ì¶”ë¡ ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ë°œê²¬í–ˆë‹¤.


---

*Generated on 2025-09-23 09:16:32*