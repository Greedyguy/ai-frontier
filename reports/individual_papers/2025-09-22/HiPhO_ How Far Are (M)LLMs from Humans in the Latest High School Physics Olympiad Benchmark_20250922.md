---
keywords:
  - Multimodal Learning
  - Physics Olympiad
  - Large Language Model
  - Human-aligned Evaluation
  - Closed-source Models
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.07894
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:41:31.963636",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Physics Olympiad",
    "Large Language Model",
    "Human-aligned Evaluation",
    "Closed-source Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.8,
    "Physics Olympiad": 0.78,
    "Large Language Model": 0.75,
    "Human-aligned Evaluation": 0.77,
    "Closed-source Models": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal physical reasoning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal reasoning",
          "Multimodal physical reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the integration of multiple modalities in learning, crucial for physics problem-solving.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "High School Physics Olympiad",
        "canonical": "Physics Olympiad",
        "aliases": [
          "Physics competition",
          "Olympiad exams"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific benchmark for evaluating model performance against human contestants.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "(M)LLMs",
        "canonical": "Large Language Model",
        "aliases": [
          "Multimodal LLMs",
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating language models in physics contexts.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      },
      {
        "surface": "Human-aligned evaluation",
        "canonical": "Human-aligned Evaluation",
        "aliases": [
          "Human evaluation",
          "Human-aligned assessment"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the paper's approach to aligning model evaluation with human standards.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Closed-source reasoning MLLMs",
        "canonical": "Closed-source Models",
        "aliases": [
          "Closed-source MLLMs",
          "Proprietary MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Differentiates between open and closed-source models in terms of performance.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance gap",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal physical reasoning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "High School Physics Olympiad",
      "resolved_canonical": "Physics Olympiad",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "(M)LLMs",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Human-aligned evaluation",
      "resolved_canonical": "Human-aligned Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Closed-source reasoning MLLMs",
      "resolved_canonical": "Closed-source Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?

**Korean Title:** HiPhO: ìµœì‹  ê³ ë“±í•™êµ ë¬¼ë¦¬ ì˜¬ë¦¼í”¼ì•„ë“œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ (M)LLMì€ ì¸ê°„ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ê°€?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.07894.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.07894](https://arxiv.org/abs/2509.07894)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Multi-Physics_ A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems_20250922|Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems]] (84.7% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (81.5% similar)
- [[2025-09-22/How Good are Foundation Models in Step-by-Step Embodied Reasoning?_20250922|How Good are Foundation Models in Step-by-Step Embodied Reasoning?]] (80.3% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (79.3% similar)
- [[2025-09-18/The NazoNazo Benchmark_ A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs_20250918|The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs]] (79.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Closed-source Models|Closed-source Models]]
**âš¡ Unique Technical**: [[keywords/Physics Olympiad|Physics Olympiad]], [[keywords/Human-aligned Evaluation|Human-aligned Evaluation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.07894v4 Announce Type: replace 
Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight the performance gap between open-source models and top students, the strong reasoning abilities of closed-source models, and the remaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for multimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO with a public leaderboard at https://phyarena.github.io/.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.07894v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìµœê·¼ (M)LLMì˜ ë¬¼ë¦¬ì  ëŠ¥ë ¥ì— ëŒ€í•œ ê´€ì‹¬ì´ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ë¬¼ë¦¬í•™ ë²¤ì¹˜ë§ˆí¬ëŠ” ë‘ ê°€ì§€ ì£¼ìš”í•œ ê²°í•¨ì´ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, ë¬¼ë¦¬í•™ ì˜¬ë¦¼í”¼ì•„ë“œì™€ ê°™ì€ ì‹¤ì œ ë¬¼ë¦¬í•™ ëŒ€íšŒë¥¼ ì²´ê³„ì ì´ê³  ìµœì‹ ì˜ ë°©ì‹ìœ¼ë¡œ ë‹¤ë£¨ì§€ ëª»í•˜ê³  ìˆìœ¼ë©°, ë‘˜ì§¸, ì¸ê°„ê³¼ì˜ ì§ì ‘ì ì¸ ì„±ëŠ¥ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì¸ê°„ê³¼ì˜ í‰ê°€ë¥¼ ë§ì¶˜ ê³ ë“±í•™êµ ë¬¼ë¦¬í•™ ì˜¬ë¦¼í”¼ì•„ë“œì— ì „ë…í•˜ëŠ” ìµœì´ˆì˜ ë²¤ì¹˜ë§ˆí¬ì¸ HiPhOë¥¼ ì œì‹œí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, HiPhOëŠ” ì„¸ ê°€ì§€ ì£¼ìš” í˜ì‹ ì„ ê°•ì¡°í•©ë‹ˆë‹¤. (1) í¬ê´„ì ì¸ ë°ì´í„°: 2024-2025ë…„ì˜ ìµœì‹  ì˜¬ë¦¼í”¼ì•„ë“œ ì‹œí—˜ 13ê°œë¥¼ êµ­ì œ ë° ì§€ì—­ ëŒ€íšŒë¥¼ ì•„ìš°ë¥´ë©°, í…ìŠ¤íŠ¸ ì „ìš© ë¬¸ì œë¶€í„° ë‹¤ì´ì–´ê·¸ë¨ ê¸°ë°˜ ë¬¸ì œê¹Œì§€ ë‹¤ì–‘í•œ í˜•ì‹ì„ í¬í•¨í•˜ì—¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (2) ì „ë¬¸ì ì¸ í‰ê°€: ê³µì‹ ì±„ì  ë°©ì‹ì„ ì±„íƒí•˜ì—¬ ë‹µë³€ ë° ë‹¨ê³„ ìˆ˜ì¤€ì—ì„œ ì„¸ë°€í•œ ì±„ì ì„ ìˆ˜í–‰í•˜ë©°, ì¸ê°„ ì±„ì ìì™€ ì™„ì „íˆ ì¼ì¹˜í•˜ì—¬ ê³ í’ˆì§ˆì˜ ë¶„ì•¼ë³„ í‰ê°€ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. (3) ì¸ê°„ ì°¸ê°€ìì™€ì˜ ë¹„êµ: ê³µì‹ ë©”ë‹¬ ê¸°ì¤€ì— ë”°ë¼ ëª¨ë¸ì— ê¸ˆ, ì€, ë™ë©”ë‹¬ì„ ë¶€ì—¬í•˜ì—¬ (M)LLMê³¼ ì¸ê°„ ì°¸ê°€ì ê°„ì˜ ì§ì ‘ì ì¸ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. 30ê°œì˜ ìµœì²¨ë‹¨ (M)LLMì— ëŒ€í•œ ëŒ€ê·œëª¨ í‰ê°€ ê²°ê³¼, 13ê°œì˜ ì‹œí—˜ì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ MLLMì€ ëŒ€ë¶€ë¶„ ë™ë©”ë‹¬ ìˆ˜ì¤€ì— ë¨¸ë¬¼ê±°ë‚˜ ê·¸ ì´í•˜ì´ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ LLMì€ ì—¬ëŸ¬ ê¸ˆë©”ë‹¬ì„ íšë“í•˜ë©° ìœ ë§í•œ ì§„ì „ì„ ë³´ì´ê³ , íì‡„í˜• ì¶”ë¡  MLLMì€ 6ê°œì—ì„œ 12ê°œì˜ ê¸ˆë©”ë‹¬ì„ íšë“í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ì—¬ì „íˆ ë§Œì ê³¼ í° ì°¨ì´ê°€ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ê³¼ ìµœìƒìœ„ í•™ìƒ ê°„ì˜ ì„±ëŠ¥ ê²©ì°¨, íì‡„í˜• ëª¨ë¸ì˜ ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥, ê·¸ë¦¬ê³  ê°œì„ ì˜ ì—¬ì§€ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. HiPhOëŠ” ë‹¤ì¤‘ ëª¨ë“œ ë¬¼ë¦¬ì  ì¶”ë¡ ì„ ìœ„í•œ ì¸ê°„ ì •ë ¬ ì˜¬ë¦¼í”¼ì•„ë“œ ë²¤ì¹˜ë§ˆí¬ë¡œ, https://github.com/SciYu/HiPhOì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë˜ë©°, https://phyarena.github.io/ì—ì„œ ê³µê°œ ë¦¬ë”ë³´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ìµœê·¼ ë¬¼ë¦¬í•™ ë¶„ì•¼ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ëŠ¥ë ¥ì´ ì£¼ëª©ë°›ê³  ìˆì§€ë§Œ, ê¸°ì¡´ì˜ ë¬¼ë¦¬í•™ ë²¤ì¹˜ë§ˆí¬ëŠ” ì‹¤ì œ ë¬¼ë¦¬í•™ ëŒ€íšŒì™€ì˜ ë¹„êµë‚˜ ì¸ê°„ê³¼ì˜ ì§ì ‘ì ì¸ ì„±ëŠ¥ ë¹„êµê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê³ ë“±í•™êµ ë¬¼ë¦¬ ì˜¬ë¦¼í”¼ì•„ë“œë¥¼ ìœ„í•œ ìµœì´ˆì˜ ë²¤ì¹˜ë§ˆí¬ì¸ HiPhOë¥¼ ì œì‹œí•©ë‹ˆë‹¤. HiPhOëŠ” 2024-2025ë…„ ìµœì‹  ì˜¬ë¦¼í”¼ì•„ë“œ ì‹œí—˜ 13ê°œë¥¼ í¬í•¨í•˜ë©°, í…ìŠ¤íŠ¸ì™€ ë‹¤ì´ì–´ê·¸ë¨ ê¸°ë°˜ ë¬¸ì œë¥¼ ì•„ìš°ë¥´ëŠ” ì¢…í•©ì ì¸ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ê³µì‹ ì±„ì  ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ ì¸ê°„ í‰ê°€ìì™€ ì¼ì¹˜í•˜ëŠ” ì„¸ë°€í•œ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë©°, ëª¨ë¸ì˜ ì„±ê³¼ë¥¼ ì¸ê°„ ì°¸ê°€ìì™€ ì§ì ‘ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ë©”ë‹¬ ê¸°ì¤€ì„ ì ìš©í•©ë‹ˆë‹¤. 30ê°œì˜ ìµœì‹  MLLMì„ í‰ê°€í•œ ê²°ê³¼, ì˜¤í”ˆ ì†ŒìŠ¤ MLLMì€ ëŒ€ë¶€ë¶„ ë™ë©”ë‹¬ ìˆ˜ì¤€ì— ë¨¸ë¬¼ë €ê³ , íì‡„ ì†ŒìŠ¤ ëª¨ë¸ì€ ìµœëŒ€ 12ê°œì˜ ê¸ˆë©”ë‹¬ì„ íšë“í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ê³¼ ìµœìƒìœ„ í•™ìƒ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´, íì‡„ ì†ŒìŠ¤ ëª¨ë¸ì˜ ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥, ê·¸ë¦¬ê³  ê°œì„ ì˜ ì—¬ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. HiPhOëŠ” ê³µê°œ ì†ŒìŠ¤ë¡œ ì œê³µë˜ë©°, ê´€ë ¨ ë¦¬ë”ë³´ë“œë„ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. HiPhOëŠ” ê³ ë“±í•™êµ ë¬¼ë¦¬ ì˜¬ë¦¼í”¼ì•„ë“œë¥¼ ìœ„í•œ ìµœì´ˆì˜ ì¸ê°„ ì •ë ¬ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ë¡œ, ìµœì‹  ì˜¬ë¦¼í”¼ì•„ë“œ ì‹œí—˜ì„ í¬ê´„ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ ë¬¼ë¦¬í•™ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.
- 2. HiPhOëŠ” ê³µì‹ ì±„ì  ê¸°ì¤€ì„ ì±„íƒí•˜ì—¬ ë‹µë³€ ë° ë‹¨ê³„ ìˆ˜ì¤€ì—ì„œ ì„¸ë¶„í™”ëœ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë©°, ì¸ê°„ ì‹¬ì‚¬ìœ„ì›ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” í‰ê°€ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
- 3. ëª¨ë¸ì˜ ì„±ê³¼ë¥¼ ì¸ê°„ ì°¸ê°€ìì™€ ì§ì ‘ ë¹„êµí•˜ê¸° ìœ„í•´ ê³µì‹ ë©”ë‹¬ ê¸°ì¤€ì— ë”°ë¼ ê¸ˆ, ì€, ë™ë©”ë‹¬ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
- 4. 30ê°œì˜ ìµœì‹  (M)LLMì— ëŒ€í•œ ëŒ€ê·œëª¨ í‰ê°€ ê²°ê³¼, ì˜¤í”ˆ ì†ŒìŠ¤ MLLMì€ ëŒ€ë¶€ë¶„ ë™ë©”ë‹¬ ìˆ˜ì¤€ì— ë¨¸ë¬¼ê³  ìˆìœ¼ë©°, íì‡„í˜• ì†ŒìŠ¤ ëª¨ë¸ì€ ìµœëŒ€ 12ê°œì˜ ê¸ˆë©”ë‹¬ì„ íšë“í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. HiPhOëŠ” ë¬¼ë¦¬ì  ì¶”ë¡ ì„ ìœ„í•œ ë©€í‹°ëª¨ë‹¬ ì¸ê°„ ì •ë ¬ ì˜¬ë¦¼í”¼ì•„ë“œ ë²¤ì¹˜ë§ˆí¬ë¡œ, ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë˜ë©° ê³µê°œ ë¦¬ë”ë³´ë“œë¥¼ í†µí•´ ì„±ê³¼ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:41:31*