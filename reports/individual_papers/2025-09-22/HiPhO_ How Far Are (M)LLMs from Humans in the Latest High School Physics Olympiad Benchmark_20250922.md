---
keywords:
  - Multimodal Learning
  - Physics Olympiad
  - Large Language Model
  - Human-aligned Evaluation
  - Closed-source Models
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.07894
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:41:31.963636",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Physics Olympiad",
    "Large Language Model",
    "Human-aligned Evaluation",
    "Closed-source Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.8,
    "Physics Olympiad": 0.78,
    "Large Language Model": 0.75,
    "Human-aligned Evaluation": 0.77,
    "Closed-source Models": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal physical reasoning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal reasoning",
          "Multimodal physical reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the integration of multiple modalities in learning, crucial for physics problem-solving.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "High School Physics Olympiad",
        "canonical": "Physics Olympiad",
        "aliases": [
          "Physics competition",
          "Olympiad exams"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific benchmark for evaluating model performance against human contestants.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "(M)LLMs",
        "canonical": "Large Language Model",
        "aliases": [
          "Multimodal LLMs",
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating language models in physics contexts.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      },
      {
        "surface": "Human-aligned evaluation",
        "canonical": "Human-aligned Evaluation",
        "aliases": [
          "Human evaluation",
          "Human-aligned assessment"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the paper's approach to aligning model evaluation with human standards.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Closed-source reasoning MLLMs",
        "canonical": "Closed-source Models",
        "aliases": [
          "Closed-source MLLMs",
          "Proprietary MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Differentiates between open and closed-source models in terms of performance.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance gap",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal physical reasoning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "High School Physics Olympiad",
      "resolved_canonical": "Physics Olympiad",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "(M)LLMs",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Human-aligned evaluation",
      "resolved_canonical": "Human-aligned Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Closed-source reasoning MLLMs",
      "resolved_canonical": "Closed-source Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?

**Korean Title:** HiPhO: 최신 고등학교 물리 올림피아드 벤치마크에서 (M)LLM은 인간과 얼마나 차이가 나는가?

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.07894.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.07894](https://arxiv.org/abs/2509.07894)

## 🔗 유사한 논문
- [[2025-09-22/Multi-Physics_ A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems_20250922|Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems]] (84.7% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (81.5% similar)
- [[2025-09-22/How Good are Foundation Models in Step-by-Step Embodied Reasoning?_20250922|How Good are Foundation Models in Step-by-Step Embodied Reasoning?]] (80.3% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (79.3% similar)
- [[2025-09-18/The NazoNazo Benchmark_ A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs_20250918|The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs]] (79.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Closed-source Models|Closed-source Models]]
**⚡ Unique Technical**: [[keywords/Physics Olympiad|Physics Olympiad]], [[keywords/Human-aligned Evaluation|Human-aligned Evaluation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.07894v4 Announce Type: replace 
Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight the performance gap between open-source models and top students, the strong reasoning abilities of closed-source models, and the remaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for multimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO with a public leaderboard at https://phyarena.github.io/.

## 🔍 Abstract (한글 번역)

arXiv:2509.07894v4 발표 유형: 교체  
초록: 최근 (M)LLM의 물리적 능력에 대한 관심이 증가하고 있습니다. 그러나 기존의 물리학 벤치마크는 두 가지 주요한 결함이 있습니다. 첫째, 물리학 올림피아드와 같은 실제 물리학 대회를 체계적이고 최신의 방식으로 다루지 못하고 있으며, 둘째, 인간과의 직접적인 성능 비교를 가능하게 하지 못합니다. 이러한 격차를 해소하기 위해, 우리는 인간과의 평가를 맞춘 고등학교 물리학 올림피아드에 전념하는 최초의 벤치마크인 HiPhO를 제시합니다. 구체적으로, HiPhO는 세 가지 주요 혁신을 강조합니다. (1) 포괄적인 데이터: 2024-2025년의 최신 올림피아드 시험 13개를 국제 및 지역 대회를 아우르며, 텍스트 전용 문제부터 다이어그램 기반 문제까지 다양한 형식을 포함하여 수집합니다. (2) 전문적인 평가: 공식 채점 방식을 채택하여 답변 및 단계 수준에서 세밀한 채점을 수행하며, 인간 채점자와 완전히 일치하여 고품질의 분야별 평가를 보장합니다. (3) 인간 참가자와의 비교: 공식 메달 기준에 따라 모델에 금, 은, 동메달을 부여하여 (M)LLM과 인간 참가자 간의 직접적인 비교를 가능하게 합니다. 30개의 최첨단 (M)LLM에 대한 대규모 평가 결과, 13개의 시험에서 오픈 소스 MLLM은 대부분 동메달 수준에 머물거나 그 이하이며, 오픈 소스 LLM은 여러 금메달을 획득하며 유망한 진전을 보이고, 폐쇄형 추론 MLLM은 6개에서 12개의 금메달을 획득할 수 있으며, 대부분의 모델은 여전히 만점과 큰 차이가 있음을 보여줍니다. 이러한 결과는 오픈 소스 모델과 최상위 학생 간의 성능 격차, 폐쇄형 모델의 강력한 추론 능력, 그리고 개선의 여지를 강조합니다. HiPhO는 다중 모드 물리적 추론을 위한 인간 정렬 올림피아드 벤치마크로, https://github.com/SciYu/HiPhO에서 오픈 소스로 제공되며, https://phyarena.github.io/에서 공개 리더보드를 제공합니다.

## 📝 요약

최근 물리학 분야에서 대형 언어 모델(MLLMs)의 능력이 주목받고 있지만, 기존의 물리학 벤치마크는 실제 물리학 대회와의 비교나 인간과의 직접적인 성능 비교가 부족합니다. 이를 해결하기 위해, 우리는 고등학교 물리 올림피아드를 위한 최초의 벤치마크인 HiPhO를 제시합니다. HiPhO는 2024-2025년 최신 올림피아드 시험 13개를 포함하며, 텍스트와 다이어그램 기반 문제를 아우르는 종합적인 데이터를 제공합니다. 또한, 공식 채점 기준을 사용하여 인간 평가자와 일치하는 세밀한 평가를 수행하며, 모델의 성과를 인간 참가자와 직접 비교할 수 있도록 메달 기준을 적용합니다. 30개의 최신 MLLM을 평가한 결과, 오픈 소스 MLLM은 대부분 동메달 수준에 머물렀고, 폐쇄 소스 모델은 최대 12개의 금메달을 획득할 수 있었습니다. 이는 오픈 소스 모델과 최상위 학생 간의 성능 차이, 폐쇄 소스 모델의 강력한 추론 능력, 그리고 개선의 여지를 보여줍니다. HiPhO는 공개 소스로 제공되며, 관련 리더보드도 공개되어 있습니다.

## 🎯 주요 포인트

- 1. HiPhO는 고등학교 물리 올림피아드를 위한 최초의 인간 정렬 평가 벤치마크로, 최신 올림피아드 시험을 포괄적으로 수집하여 물리학 문제를 다루고 있습니다.
- 2. HiPhO는 공식 채점 기준을 채택하여 답변 및 단계 수준에서 세분화된 평가를 수행하며, 인간 심사위원과 완전히 일치하는 평가를 보장합니다.
- 3. 모델의 성과를 인간 참가자와 직접 비교하기 위해 공식 메달 기준에 따라 금, 은, 동메달을 부여합니다.
- 4. 30개의 최신 (M)LLM에 대한 대규모 평가 결과, 오픈 소스 MLLM은 대부분 동메달 수준에 머물고 있으며, 폐쇄형 소스 모델은 최대 12개의 금메달을 획득할 수 있습니다.
- 5. HiPhO는 물리적 추론을 위한 멀티모달 인간 정렬 올림피아드 벤치마크로, 오픈 소스로 제공되며 공개 리더보드를 통해 성과를 비교할 수 있습니다.


---

*Generated on 2025-09-23 09:41:31*