---
keywords:
  - Extractive Summarization
  - Transformer
  - Neural Network
  - Cornell Newsroom Dataset
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15614
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:31:48.594130",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Extractive Summarization",
    "Transformer",
    "Neural Network",
    "Cornell Newsroom Dataset"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Extractive Summarization": 0.79,
    "Transformer": 0.8,
    "Neural Network": 0.77,
    "Cornell Newsroom Dataset": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "extractive text summarization",
        "canonical": "Extractive Summarization",
        "aliases": [
          "text summarization",
          "extractive summarization"
        ],
        "category": "specific_connectable",
        "rationale": "Extractive summarization is a key technique in NLP for condensing information, linking to broader themes in content management.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "BERT embeddings",
        "canonical": "Transformer",
        "aliases": [
          "BERT",
          "BERT model"
        ],
        "category": "broad_technical",
        "rationale": "BERT is a foundational model in NLP, facilitating connections to other Transformer-based techniques.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "LSTM networks",
        "canonical": "Neural Network",
        "aliases": [
          "LSTM",
          "Long Short-Term Memory"
        ],
        "category": "broad_technical",
        "rationale": "LSTM networks are a type of neural network crucial for tasks involving sequential data, enhancing connections in temporal data processing.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Cornell Newsroom dataset",
        "canonical": "Cornell Newsroom Dataset",
        "aliases": [
          "Newsroom dataset"
        ],
        "category": "unique_technical",
        "rationale": "This dataset is specific to the domain of news summarization, providing a unique link to empirical studies in the field.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "content management",
      "user engagement"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "extractive text summarization",
      "resolved_canonical": "Extractive Summarization",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "BERT embeddings",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "LSTM networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Cornell Newsroom dataset",
      "resolved_canonical": "Cornell Newsroom Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Efficient Extractive Text Summarization for Online News Articles Using Machine Learning

**Korean Title:** ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìœ„í•œ ê¸°ê³„ í•™ìŠµ ê¸°ë°˜ì˜ íš¨ìœ¨ì ì¸ ì¶”ì¶œì  í…ìŠ¤íŠ¸ ìš”ì•½

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15614.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15614](https://arxiv.org/abs/2509.15614)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Re-FRAME the Meeting Summarization SCOPE_ Fact-Based Summarization and Personalization via Questions_20250922|Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions]] (81.1% similar)
- [[2025-09-22/Deep learning and abstractive summarisation for radiological reports_ an empirical study for adapting the PEGASUS models' family with scarce data_20250922|Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data]] (80.7% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (80.6% similar)
- [[2025-09-19/MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs]] (80.4% similar)
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (80.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Extractive Summarization|Extractive Summarization]]
**âš¡ Unique Technical**: [[keywords/Cornell Newsroom Dataset|Cornell Newsroom Dataset]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15614v1 Announce Type: new 
Abstract: In the age of information overload, content management for online news articles relies on efficient summarization to enhance accessibility and user engagement. This article addresses the challenge of extractive text summarization by employing advanced machine learning techniques to generate concise and coherent summaries while preserving the original meaning. Using the Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we developed a pipeline leveraging BERT embeddings to transform textual data into numerical representations. By framing the task as a binary classification problem, we explored various models, including logistic regression, feed-forward neural networks, and long short-term memory (LSTM) networks. Our findings demonstrate that LSTM networks, with their ability to capture sequential dependencies, outperform baseline methods like Lede-3 and simpler models in F1 score and ROUGE-1 metrics. This study underscores the potential of automated summarization in improving content management systems for online news platforms, enabling more efficient content organization and enhanced user experiences.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15614v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì •ë³´ ê³¼ë¶€í•˜ì˜ ì‹œëŒ€ì— ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ ì½˜í…ì¸  ê´€ë¦¬ëŠ” ì ‘ê·¼ì„± í–¥ìƒê³¼ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ìœ„í•´ íš¨ìœ¨ì ì¸ ìš”ì•½ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ì›ë˜ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°„ê²°í•˜ê³  ì¼ê´€ëœ ìš”ì•½ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ê³ ê¸‰ ê¸°ê³„ í•™ìŠµ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì¶”ì¶œì  í…ìŠ¤íŠ¸ ìš”ì•½ì˜ ë„ì „ì„ ë‹¤ë£¹ë‹ˆë‹¤. 130ë§Œ ê°œì˜ ê¸°ì‚¬-ìš”ì•½ ìŒìœ¼ë¡œ êµ¬ì„±ëœ Cornell Newsroom ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” BERT ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ì  í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì„ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì„¤ì •í•˜ì—¬ ë¡œì§€ìŠ¤í‹± íšŒê·€, í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§, ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬(LSTM) ë„¤íŠ¸ì›Œí¬ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ìˆœì°¨ì  ì˜ì¡´ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆëŠ” LSTM ë„¤íŠ¸ì›Œí¬ê°€ Lede-3ì™€ ê°™ì€ ê¸°ì¤€ ë°©ë²• ë° ë” ê°„ë‹¨í•œ ëª¨ë¸ë³´ë‹¤ F1 ì ìˆ˜ì™€ ROUGE-1 ì§€í‘œì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì˜¨ë¼ì¸ ë‰´ìŠ¤ í”Œë«í¼ì˜ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ì—¬ ë” íš¨ìœ¨ì ì¸ ì½˜í…ì¸  ì¡°ì§ê³¼ í–¥ìƒëœ ì‚¬ìš©ì ê²½í—˜ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìë™ ìš”ì•½ì˜ ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì •ë³´ ê³¼ë¶€í•˜ ì‹œëŒ€ì— ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íš¨ìœ¨ì ì¸ ìš”ì•½ì„ í†µí•´ ì ‘ê·¼ì„±ê³¼ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. Cornell Newsroom ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ BERT ì„ë² ë”©ì„ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í™”í•˜ê³ , ì´ë¥¼ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì ‘ê·¼í–ˆìŠµë‹ˆë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€, í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§, LSTM ë„¤íŠ¸ì›Œí¬ ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ íƒìƒ‰í•œ ê²°ê³¼, LSTM ë„¤íŠ¸ì›Œí¬ê°€ ìˆœì°¨ì  ì˜ì¡´ì„±ì„ ì˜ í¬ì°©í•˜ì—¬ F1 ì ìˆ˜ì™€ ROUGE-1 ì§€í‘œì—ì„œ Lede-3 ë“± ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ìë™ ìš”ì•½ì´ ì˜¨ë¼ì¸ ë‰´ìŠ¤ í”Œë«í¼ì˜ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ê°œì„ í•  ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì •ë³´ ê³¼ë¶€í•˜ ì‹œëŒ€ì— ì˜¨ë¼ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ì€ ì ‘ê·¼ì„±ê³¼ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì¤‘ìš”í•©ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” BERT ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í™”í•˜ê³ , ì´ë¥¼ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ì„¤ì •í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 3. LSTM ë„¤íŠ¸ì›Œí¬ëŠ” ìˆœì°¨ì  ì˜ì¡´ì„±ì„ í¬ì°©í•˜ëŠ” ëŠ¥ë ¥ ë•ë¶„ì— F1 ì ìˆ˜ì™€ ROUGE-1 ì§€í‘œì—ì„œ ê¸°ë³¸ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. ìë™ ìš”ì•½ì€ ì˜¨ë¼ì¸ ë‰´ìŠ¤ í”Œë«í¼ì˜ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ê³  ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:31:48*