---
keywords:
  - Retrieval Augmented Generation
  - Relation-Aware Model
  - Structured Relation Semantics
  - Heterogeneous Objects
  - Semantic Consistency
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15883
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:18:18.617609",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval Augmented Generation",
    "Relation-Aware Model",
    "Structured Relation Semantics",
    "Heterogeneous Objects",
    "Semantic Consistency"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Retrieval Augmented Generation": 0.82,
    "Relation-Aware Model": 0.77,
    "Structured Relation Semantics": 0.75,
    "Heterogeneous Objects": 0.72,
    "Semantic Consistency": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "retrieval-augmented image captioning",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG",
          "retrieval-augmented captioning"
        ],
        "category": "specific_connectable",
        "rationale": "Links to retrieval-augmented methods in multimodal contexts, enhancing connectivity with related works.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "relation-aware model",
        "canonical": "Relation-Aware Model",
        "aliases": [
          "relation-aware architecture"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to modeling relationships in image captioning, offering unique insights.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.81,
        "link_intent_score": 0.77
      },
      {
        "surface": "structured relation semantics",
        "canonical": "Structured Relation Semantics",
        "aliases": [
          "relation semantics",
          "structured semantics"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on the specific technique of extracting structured relationships, relevant for semantic analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.79,
        "link_intent_score": 0.75
      },
      {
        "surface": "heterogeneous objects",
        "canonical": "Heterogeneous Objects",
        "aliases": [
          "diverse objects",
          "varied objects"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the model's capability to identify diverse objects, a key feature in image analysis.",
        "novelty_score": 0.68,
        "connectivity_score": 0.66,
        "specificity_score": 0.76,
        "link_intent_score": 0.72
      },
      {
        "surface": "semantic consistency",
        "canonical": "Semantic Consistency",
        "aliases": [
          "consistent semantics"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for ensuring coherent image captions, linking to broader semantic analysis topics.",
        "novelty_score": 0.58,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "image captioning",
      "model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "retrieval-augmented image captioning",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "relation-aware model",
      "resolved_canonical": "Relation-Aware Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.81,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "structured relation semantics",
      "resolved_canonical": "Structured Relation Semantics",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.79,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "heterogeneous objects",
      "resolved_canonical": "Heterogeneous Objects",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.66,
        "specificity": 0.76,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "semantic consistency",
      "resolved_canonical": "Semantic Consistency",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning

**Korean Title:** RACap: 경량 검색 증강 이미지 캡셔닝을 위한 관계 인식 프롬프트 기법

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15883.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15883](https://arxiv.org/abs/2509.15883)

## 🔗 유사한 논문
- [[2025-09-19/RAcQUEt_ Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs_20250919|RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs]] (81.5% similar)
- [[2025-09-19/Enhancing Retrieval Augmentation via Adversarial Collaboration_20250919|Enhancing Retrieval Augmentation via Adversarial Collaboration]] (81.1% similar)
- [[2025-09-19/Causal-Counterfactual RAG_ The Integration of Causal-Counterfactual Reasoning into RAG_20250919|Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG]] (80.6% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (80.5% similar)
- [[2025-09-19/Reconstruction Alignment Improves Unified Multimodal Models_20250919|Reconstruction Alignment Improves Unified Multimodal Models]] (80.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Semantic Consistency|Semantic Consistency]]
**⚡ Unique Technical**: [[keywords/Relation-Aware Model|Relation-Aware Model]], [[keywords/Structured Relation Semantics|Structured Relation Semantics]], [[keywords/Heterogeneous Objects|Heterogeneous Objects]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15883v1 Announce Type: cross 
Abstract: Recent retrieval-augmented image captioning methods incorporate external knowledge to compensate for the limitations in comprehending complex scenes. However, current approaches face challenges in relation modeling: (1) the representation of semantic prompts is too coarse-grained to capture fine-grained relationships; (2) these methods lack explicit modeling of image objects and their semantic relationships. To address these limitations, we propose RACap, a relation-aware retrieval-augmented model for image captioning, which not only mines structured relation semantics from retrieval captions, but also identifies heterogeneous objects from the image. RACap effectively retrieves structured relation features that contain heterogeneous visual information to enhance the semantic consistency and relational expressiveness. Experimental results show that RACap, with only 10.8M trainable parameters, achieves superior performance compared to previous lightweight captioning models.

## 🔍 Abstract (한글 번역)

arXiv:2509.15883v1 발표 유형: 교차  
초록: 최근의 검색 증강 이미지 캡셔닝 방법은 복잡한 장면을 이해하는 데 있어 한계를 보완하기 위해 외부 지식을 통합합니다. 그러나 현재 접근 방식은 관계 모델링에서 다음과 같은 문제에 직면하고 있습니다: (1) 의미 프롬프트의 표현이 너무 거칠어 세밀한 관계를 포착하지 못합니다; (2) 이러한 방법은 이미지 객체와 그 의미 관계를 명시적으로 모델링하지 못합니다. 이러한 한계를 해결하기 위해, 우리는 이미지 캡셔닝을 위한 관계 인식 검색 증강 모델인 RACap을 제안합니다. 이는 검색 캡션에서 구조화된 관계 의미를 발굴할 뿐만 아니라 이미지에서 이질적인 객체를 식별합니다. RACap은 이질적인 시각 정보를 포함하는 구조화된 관계 특징을 효과적으로 검색하여 의미 일관성과 관계 표현력을 향상시킵니다. 실험 결과, RACap은 단 10.8M의 학습 가능한 매개변수로 이전의 경량 캡셔닝 모델에 비해 우수한 성능을 달성함을 보여줍니다.

## 📝 요약

최근의 검색 보강 이미지 캡셔닝 방법은 복잡한 장면 이해의 한계를 보완하기 위해 외부 지식을 활용하지만, 관계 모델링에서 어려움을 겪고 있습니다. 이러한 문제를 해결하기 위해, 우리는 RACap이라는 관계 인식 검색 보강 모델을 제안합니다. RACap은 검색된 캡션에서 구조화된 관계 의미를 추출하고, 이미지에서 이질적인 객체를 식별합니다. 이를 통해 이질적인 시각 정보를 포함한 구조화된 관계 특징을 효과적으로 검색하여 의미적 일관성과 관계 표현력을 향상시킵니다. 실험 결과, RACap은 1,080만 개의 학습 가능한 파라미터만으로도 이전의 경량 캡셔닝 모델보다 우수한 성능을 보였습니다.

## 🎯 주요 포인트

- 1. 최근 이미지 캡션 생성 방법은 외부 지식을 활용하여 복잡한 장면 이해의 한계를 보완하고자 한다.
- 2. 기존 방법들은 관계 모델링에서 세밀한 관계를 포착하지 못하는 문제를 가지고 있다.
- 3. RACap은 이미지에서 이질적인 객체를 식별하고, 검색된 캡션에서 구조화된 관계 의미를 추출하는 모델이다.
- 4. RACap은 이질적인 시각 정보를 포함한 구조화된 관계 특징을 효과적으로 검색하여 의미적 일관성과 관계 표현력을 향상시킨다.
- 5. RACap은 10.8M의 훈련 가능한 파라미터로 이전의 경량 캡션 생성 모델보다 우수한 성능을 보여준다.


---

*Generated on 2025-09-23 09:18:18*