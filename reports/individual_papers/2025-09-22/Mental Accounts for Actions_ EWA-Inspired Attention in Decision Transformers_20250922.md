---
keywords:
  - Decision Transformers
  - Online Decision Transformers
  - Experience-Weighted Attraction
  - Attention Mechanism
  - Entropy-Regularized Training
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15498
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:27:28.740684",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Decision Transformers",
    "Online Decision Transformers",
    "Experience-Weighted Attraction",
    "Attention Mechanism",
    "Entropy-Regularized Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Decision Transformers": 0.78,
    "Online Decision Transformers": 0.77,
    "Experience-Weighted Attraction": 0.75,
    "Attention Mechanism": 0.8,
    "Entropy-Regularized Training": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Decision Transformers",
        "canonical": "Decision Transformers",
        "aliases": [
          "DTs"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel approach in reinforcement learning by framing it as supervised sequence modeling.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Online Decision Transformers",
        "canonical": "Online Decision Transformers",
        "aliases": [
          "ODTs"
        ],
        "category": "unique_technical",
        "rationale": "An extension of Decision Transformers that incorporates online learning, enhancing exploration capabilities.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Experience-Weighted Attraction",
        "canonical": "Experience-Weighted Attraction",
        "aliases": [
          "EWA"
        ],
        "category": "unique_technical",
        "rationale": "A cognitive model that influences the proposed method, providing a unique perspective on action-outcome memory.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Central to the paper's methodology, connecting it to broader research on attention in machine learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Entropy-Regularized Training",
        "canonical": "Entropy-Regularized Training",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific training technique used to enhance the exploration capabilities of the model.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "Transformers",
      "Reinforcement Learning",
      "Soft Actor-Critic"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Decision Transformers",
      "resolved_canonical": "Decision Transformers",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Online Decision Transformers",
      "resolved_canonical": "Online Decision Transformers",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Experience-Weighted Attraction",
      "resolved_canonical": "Experience-Weighted Attraction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Entropy-Regularized Training",
      "resolved_canonical": "Entropy-Regularized Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers

**Korean Title:** 행동에 대한 정신적 계좌: 결정 변환기에서 EWA에 영감을 받은 주의 집중

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15498.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15498](https://arxiv.org/abs/2509.15498)

## 🔗 유사한 논문
- [[2025-09-17/Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection_20250917|Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection]] (80.7% similar)
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers]] (80.3% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop: A Novel Regularization Method for Transformer Models]] (80.1% similar)
- [[2025-09-17/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250917|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (80.0% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (79.9% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Decision Transformers|Decision Transformers]], [[keywords/Online Decision Transformers|Online Decision Transformers]], [[keywords/Experience-Weighted Attraction|Experience-Weighted Attraction]], [[keywords/Entropy-Regularized Training|Entropy-Regularized Training]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15498v1 Announce Type: new 
Abstract: Transformers have emerged as a compelling architecture for sequential decision-making by modeling trajectories via self-attention. In reinforcement learning (RL), they enable return-conditioned control without relying on value function approximation. Decision Transformers (DTs) exploit this by casting RL as supervised sequence modeling, but they are restricted to offline data and lack exploration. Online Decision Transformers (ODTs) address this limitation through entropy-regularized training on on-policy rollouts, offering a stable alternative to traditional RL methods like Soft Actor-Critic, which depend on bootstrapped targets and reward shaping. Despite these advantages, ODTs use standard attention, which lacks explicit memory of action-specific outcomes. This leads to inefficiencies in learning long-term action effectiveness. Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we propose Experience-Weighted Attraction with Vector Quantization for Online Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains per-action mental accounts summarizing recent successes and failures. Continuous actions are routed via direct grid lookup to a compact vector-quantized codebook, where each code stores a scalar attraction updated online through decay and reward-based reinforcement. These attractions modulate attention by biasing the columns associated with action tokens, requiring no change to the backbone or training objective. On standard continuous-control benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT, particularly in early training. The module is computationally efficient, interpretable via per-code traces, and supported by theoretical guarantees that bound the attraction dynamics and its impact on attention drift.

## 🔍 Abstract (한글 번역)

arXiv:2509.15498v1 발표 유형: 신규  
초록: 트랜스포머는 자기 주의를 통해 경로를 모델링하여 순차적 의사결정을 위한 매력적인 아키텍처로 부상했습니다. 강화 학습(RL)에서는 가치 함수 근사에 의존하지 않고 반환 조건부 제어를 가능하게 합니다. Decision Transformers(DTs)는 이를 활용하여 RL을 지도 순차 모델링으로 변환하지만, 오프라인 데이터에 제한되고 탐색이 부족합니다. Online Decision Transformers(ODTs)는 정책 롤아웃에 대한 엔트로피 정규화 학습을 통해 이 제한을 해결하며, 부트스트랩 목표와 보상 형성에 의존하는 Soft Actor-Critic과 같은 전통적인 RL 방법에 대한 안정적인 대안을 제공합니다. 이러한 장점에도 불구하고, ODTs는 표준 주의를 사용하며, 이는 행동별 결과에 대한 명시적인 기억이 부족합니다. 이는 장기적인 행동 효과 학습에서 비효율성을 초래합니다. Experience-Weighted Attraction(EWA)와 같은 인지 모델에서 영감을 받아, 우리는 Online Decision Transformers를 위한 벡터 양자화 경험 가중 매력(EWA-VQ-ODT)을 제안합니다. 이는 최근의 성공과 실패를 요약하는 행동별 정신 계정을 유지하는 경량 모듈입니다. 연속적인 행동은 직접 그리드 조회를 통해 압축된 벡터 양자화 코드북으로 라우팅되며, 각 코드는 감쇠와 보상 기반 강화 학습을 통해 온라인으로 업데이트되는 스칼라 매력을 저장합니다. 이러한 매력은 행동 토큰과 관련된 열에 편향을 주어 주의를 조절하며, 백본이나 학습 목표의 변경이 필요하지 않습니다. 표준 연속 제어 벤치마크에서 EWA-VQ-ODT는 특히 초기 학습에서 ODT에 비해 샘플 효율성과 평균 반환을 개선합니다. 이 모듈은 계산적으로 효율적이며, 코드별 추적을 통해 해석 가능하고, 매력 동역학과 주의 드리프트에 미치는 영향을 제한하는 이론적 보장을 지원받습니다.

## 📝 요약

트랜스포머는 자기 주의를 통해 경로를 모델링하여 순차적 의사 결정에 유망한 구조로 부상했습니다. 강화 학습(RL)에서, 트랜스포머는 가치 함수 근사 없이 반환 조건부 제어를 가능하게 합니다. Decision Transformers(DTs)는 이를 활용하여 RL을 지도 학습 시퀀스 모델링으로 전환하지만, 오프라인 데이터에 제한되고 탐색이 부족합니다. Online Decision Transformers(ODTs)는 정책 롤아웃에 대한 엔트로피 정규화 훈련을 통해 이 제한을 극복하여 Soft Actor-Critic과 같은 전통적인 RL 방법에 대한 안정적인 대안을 제공합니다. 그러나 ODT는 표준 주의를 사용하여 행동별 결과에 대한 명시적 기억이 부족하여 장기적인 행동 효과 학습에 비효율적입니다. 이에 영감을 받아, 우리는 Experience-Weighted Attraction with Vector Quantization for Online Decision Transformers(EWA-VQ-ODT)를 제안합니다. 이 모듈은 최근 성공과 실패를 요약하는 행동별 정신 계정을 유지하며, 연속적인 행동을 벡터 양자화된 코드북으로 라우팅하여 주의력을 조정합니다. EWA-VQ-ODT는 표준 연속 제어 벤치마크에서 샘플 효율성과 평균 반환을 개선하며, 특히 초기 훈련에서 두드러집니다. 이 모듈은 계산적으로 효율적이며, 이론적 보장을 통해 주의력 드리프트에 대한 영향을 제한합니다.

## 🎯 주요 포인트

- 1. 트랜스포머는 자기 주의를 통해 궤적을 모델링하여 순차적 의사결정에 유리한 구조로 부상했습니다.
- 2. 온라인 의사결정 트랜스포머(ODT)는 온정책 롤아웃에 대한 엔트로피 정규화 훈련을 통해 탐색 부족 문제를 해결합니다.
- 3. EWA-VQ-ODT는 각 행동의 최근 성공과 실패를 요약하는 정신 계정을 유지하는 경량 모듈로, 행동별 결과에 대한 명시적 기억을 보완합니다.
- 4. EWA-VQ-ODT는 표준 연속 제어 벤치마크에서 샘플 효율성과 평균 수익을 개선하며, 특히 초기 훈련에서 두드러집니다.
- 5. 이 모듈은 계산적으로 효율적이며, 이론적 보장을 통해 주의 이동에 대한 영향을 제한합니다.


---

*Generated on 2025-09-23 10:27:28*