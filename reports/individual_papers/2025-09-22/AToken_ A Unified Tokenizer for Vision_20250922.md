---
keywords:
  - Unified Visual Tokenizer
  - Transformer
  - Multimodal Latent Space
  - Visual Generation
  - Multimodal Learning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.14476
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:15:45.709314",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Unified Visual Tokenizer",
    "Transformer",
    "Multimodal Latent Space",
    "Visual Generation",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Unified Visual Tokenizer": 0.88,
    "Transformer": 0.85,
    "Multimodal Latent Space": 0.8,
    "Visual Generation": 0.78,
    "Multimodal Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AToken",
        "canonical": "Unified Visual Tokenizer",
        "aliases": [
          "AToken"
        ],
        "category": "unique_technical",
        "rationale": "AToken represents a novel approach to unified visual tokenization across multiple modalities, which is central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Transformer architecture",
        "canonical": "Transformer",
        "aliases": [
          "Transformer model"
        ],
        "category": "broad_technical",
        "rationale": "The use of a Transformer architecture is a key technical component that enables the unified processing of visual inputs.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "4D latent space",
        "canonical": "Multimodal Latent Space",
        "aliases": [
          "4D space"
        ],
        "category": "specific_connectable",
        "rationale": "The concept of a 4D latent space is crucial for linking the unified tokenization process across different visual modalities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual generation tasks",
        "canonical": "Visual Generation",
        "aliases": [
          "Image generation",
          "Text-to-video generation"
        ],
        "category": "specific_connectable",
        "rationale": "Visual generation tasks are a significant application area for the proposed tokenizer, enhancing its connectivity to related research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal LLMs",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Large Language Models"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal LLMs are a trending topic and relevant to the paper's discussion on understanding tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "high-fidelity reconstruction",
      "semantic understanding",
      "state-of-the-art"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AToken",
      "resolved_canonical": "Unified Visual Tokenizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Transformer architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "4D latent space",
      "resolved_canonical": "Multimodal Latent Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual generation tasks",
      "resolved_canonical": "Visual Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal LLMs",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# AToken: A Unified Tokenizer for Vision

**Korean Title:** AToken: 비전을 위한 통합 토크나이저

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.14476.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.14476](https://arxiv.org/abs/2509.14476)

## 🔗 유사한 논문
- [[2025-09-22/GPSToken_ Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation_20250922|GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation]] (84.0% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (81.6% similar)
- [[2025-09-22/MANZANO_ A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer_20250922|MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer]] (80.9% similar)
- [[2025-09-17/Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions_20250917|Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions]] (80.9% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (80.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Multimodal Latent Space|Multimodal Latent Space]], [[keywords/Visual Generation|Visual Generation]], [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Unified Visual Tokenizer|Unified Visual Tokenizer]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14476v2 Announce Type: replace-cross 
Abstract: We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 40.2% MSRVTT retrieval for videos, and 28.28 PSNR with 90.9% classification accuracy for 3D.. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.

## 🔍 Abstract (한글 번역)

arXiv:2509.14476v2 발표 유형: 교차 교체  
초록: 우리는 이미지, 비디오 및 3D 자산 전반에 걸쳐 고충실도 재구성과 의미 이해를 모두 달성하는 최초의 통합 시각 토크나이저인 AToken을 소개합니다. 기존의 토크나이저는 단일 모달리티에 대한 재구성 또는 이해에 특화되어 있는 반면, AToken은 이러한 다양한 시각적 입력을 공유된 4D 잠재 공간으로 인코딩하여 두 가지 작업과 모달리티를 단일 프레임워크로 통합합니다. 구체적으로, 우리는 임의의 해상도와 시간적 길이의 시각적 입력을 처리하기 위해 4D 회전 위치 임베딩을 갖춘 순수 트랜스포머 아키텍처를 도입합니다. 안정적인 훈련을 보장하기 위해, 우리는 인식적 손실과 Gram 행렬 손실을 결합한 적대적이지 않은 훈련 목표를 도입하여 최첨단 재구성 품질을 달성합니다. 점진적인 훈련 커리큘럼을 사용하여, AToken은 단일 이미지, 비디오 및 3D에서 점차 확장되며 연속 및 이산 잠재 토큰을 모두 지원합니다. AToken은 이미지에 대해 0.21 rFID와 82.2% ImageNet 정확도를, 비디오에 대해 3.01 rFVD와 40.2% MSRVTT 검색을, 3D에 대해 28.28 PSNR과 90.9% 분류 정확도를 달성합니다. 다운스트림 응용 프로그램에서, AToken은 시각적 생성 작업(예: 연속 및 이산 토큰을 사용한 이미지 생성, 텍스트-비디오 생성, 이미지-3D 합성)과 이해 작업(예: 다중 모달 LLM)을 가능하게 하여 모든 벤치마크에서 경쟁력 있는 성능을 달성합니다. 이러한 결과는 통합된 시각적 토큰화를 기반으로 구축된 차세대 다중 모달 AI 시스템에 대한 통찰력을 제공합니다.

## 📝 요약

AToken은 이미지, 비디오, 3D 자산에서 높은 재구성 정확도와 의미 이해를 동시에 달성하는 최초의 통합 시각 토크나이저입니다. 기존 토크나이저와 달리, AToken은 다양한 시각 입력을 4차원 잠재 공간에 인코딩하여 재구성과 이해를 통합합니다. 이를 위해 4D 회전 위치 임베딩을 사용하는 순수 트랜스포머 아키텍처를 도입하였으며, 안정적인 훈련을 위해 지각적 손실과 Gram 행렬 손실을 결합한 비대립적 훈련 목표를 설정했습니다. AToken은 이미지, 비디오, 3D 자산에 대해 점진적인 훈련을 통해 연속 및 이산 잠재 토큰을 지원하며, 다양한 벤치마크에서 경쟁력 있는 성능을 발휘합니다. 이러한 결과는 통합 시각 토크나이제이션에 기반한 차세대 멀티모달 AI 시스템의 발전 가능성을 보여줍니다.

## 🎯 주요 포인트

- 1. AToken은 이미지, 비디오, 3D 자산에 대해 고품질 재구성과 의미 이해를 동시에 달성하는 최초의 통합 시각 토크나이저입니다.
- 2. AToken은 다양한 시각 입력을 4D 잠재 공간에 인코딩하여 단일 프레임워크에서 재구성과 이해 작업을 통합합니다.
- 3. 순수한 트랜스포머 아키텍처와 4D 회전 위치 임베딩을 도입하여 임의의 해상도와 시간 길이의 시각 입력을 처리합니다.
- 4. AToken은 점진적인 훈련 커리큘럼을 통해 단일 이미지, 비디오, 3D에서 확장되며, 연속 및 이산 잠재 토큰을 모두 지원합니다.
- 5. AToken은 다양한 벤치마크에서 경쟁력 있는 성능을 발휘하며, 차세대 멀티모달 AI 시스템의 발전 가능성을 보여줍니다.


---

*Generated on 2025-09-23 10:15:45*