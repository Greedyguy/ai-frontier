---
keywords:
  - Unified Visual Tokenizer
  - Transformer
  - Multimodal Latent Space
  - Visual Generation
  - Multimodal Learning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.14476
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:15:45.709314",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Unified Visual Tokenizer",
    "Transformer",
    "Multimodal Latent Space",
    "Visual Generation",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Unified Visual Tokenizer": 0.88,
    "Transformer": 0.85,
    "Multimodal Latent Space": 0.8,
    "Visual Generation": 0.78,
    "Multimodal Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AToken",
        "canonical": "Unified Visual Tokenizer",
        "aliases": [
          "AToken"
        ],
        "category": "unique_technical",
        "rationale": "AToken represents a novel approach to unified visual tokenization across multiple modalities, which is central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Transformer architecture",
        "canonical": "Transformer",
        "aliases": [
          "Transformer model"
        ],
        "category": "broad_technical",
        "rationale": "The use of a Transformer architecture is a key technical component that enables the unified processing of visual inputs.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "4D latent space",
        "canonical": "Multimodal Latent Space",
        "aliases": [
          "4D space"
        ],
        "category": "specific_connectable",
        "rationale": "The concept of a 4D latent space is crucial for linking the unified tokenization process across different visual modalities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual generation tasks",
        "canonical": "Visual Generation",
        "aliases": [
          "Image generation",
          "Text-to-video generation"
        ],
        "category": "specific_connectable",
        "rationale": "Visual generation tasks are a significant application area for the proposed tokenizer, enhancing its connectivity to related research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal LLMs",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Large Language Models"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal LLMs are a trending topic and relevant to the paper's discussion on understanding tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "high-fidelity reconstruction",
      "semantic understanding",
      "state-of-the-art"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AToken",
      "resolved_canonical": "Unified Visual Tokenizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Transformer architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "4D latent space",
      "resolved_canonical": "Multimodal Latent Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual generation tasks",
      "resolved_canonical": "Visual Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal LLMs",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# AToken: A Unified Tokenizer for Vision

**Korean Title:** AToken: ë¹„ì „ì„ ìœ„í•œ í†µí•© í† í¬ë‚˜ì´ì €

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.14476.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.14476](https://arxiv.org/abs/2509.14476)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/GPSToken_ Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation_20250922|GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation]] (84.0% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (81.6% similar)
- [[2025-09-22/MANZANO_ A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer_20250922|MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer]] (80.9% similar)
- [[2025-09-17/Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions_20250917|Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions]] (80.9% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (80.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Latent Space|Multimodal Latent Space]], [[keywords/Visual Generation|Visual Generation]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Unified Visual Tokenizer|Unified Visual Tokenizer]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14476v2 Announce Type: replace-cross 
Abstract: We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 40.2% MSRVTT retrieval for videos, and 28.28 PSNR with 90.9% classification accuracy for 3D.. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14476v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” ì´ë¯¸ì§€, ë¹„ë””ì˜¤ ë° 3D ìì‚° ì „ë°˜ì— ê±¸ì³ ê³ ì¶©ì‹¤ë„ ì¬êµ¬ì„±ê³¼ ì˜ë¯¸ ì´í•´ë¥¼ ëª¨ë‘ ë‹¬ì„±í•˜ëŠ” ìµœì´ˆì˜ í†µí•© ì‹œê° í† í¬ë‚˜ì´ì €ì¸ ATokenì„ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í† í¬ë‚˜ì´ì €ëŠ” ë‹¨ì¼ ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ì¬êµ¬ì„± ë˜ëŠ” ì´í•´ì— íŠ¹í™”ë˜ì–´ ìˆëŠ” ë°˜ë©´, ATokenì€ ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì‹œê°ì  ì…ë ¥ì„ ê³µìœ ëœ 4D ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë‘ ê°€ì§€ ì‘ì—…ê³¼ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ë¡œ í†µí•©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì„ì˜ì˜ í•´ìƒë„ì™€ ì‹œê°„ì  ê¸¸ì´ì˜ ì‹œê°ì  ì…ë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ 4D íšŒì „ ìœ„ì¹˜ ì„ë² ë”©ì„ ê°–ì¶˜ ìˆœìˆ˜ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ì•ˆì •ì ì¸ í›ˆë ¨ì„ ë³´ì¥í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì¸ì‹ì  ì†ì‹¤ê³¼ Gram í–‰ë ¬ ì†ì‹¤ì„ ê²°í•©í•œ ì ëŒ€ì ì´ì§€ ì•Šì€ í›ˆë ¨ ëª©í‘œë¥¼ ë„ì…í•˜ì—¬ ìµœì²¨ë‹¨ ì¬êµ¬ì„± í’ˆì§ˆì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì ì§„ì ì¸ í›ˆë ¨ ì»¤ë¦¬í˜ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬, ATokenì€ ë‹¨ì¼ ì´ë¯¸ì§€, ë¹„ë””ì˜¤ ë° 3Dì—ì„œ ì ì°¨ í™•ì¥ë˜ë©° ì—°ì† ë° ì´ì‚° ì ì¬ í† í°ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤. ATokenì€ ì´ë¯¸ì§€ì— ëŒ€í•´ 0.21 rFIDì™€ 82.2% ImageNet ì •í™•ë„ë¥¼, ë¹„ë””ì˜¤ì— ëŒ€í•´ 3.01 rFVDì™€ 40.2% MSRVTT ê²€ìƒ‰ì„, 3Dì— ëŒ€í•´ 28.28 PSNRê³¼ 90.9% ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ, ATokenì€ ì‹œê°ì  ìƒì„± ì‘ì—…(ì˜ˆ: ì—°ì† ë° ì´ì‚° í† í°ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±, í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±, ì´ë¯¸ì§€-3D í•©ì„±)ê³¼ ì´í•´ ì‘ì—…(ì˜ˆ: ë‹¤ì¤‘ ëª¨ë‹¬ LLM)ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” í†µí•©ëœ ì‹œê°ì  í† í°í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ì°¨ì„¸ëŒ€ ë‹¤ì¤‘ ëª¨ë‹¬ AI ì‹œìŠ¤í…œì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ATokenì€ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, 3D ìì‚°ì—ì„œ ë†’ì€ ì¬êµ¬ì„± ì •í™•ë„ì™€ ì˜ë¯¸ ì´í•´ë¥¼ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ìµœì´ˆì˜ í†µí•© ì‹œê° í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤. ê¸°ì¡´ í† í¬ë‚˜ì´ì €ì™€ ë‹¬ë¦¬, ATokenì€ ë‹¤ì–‘í•œ ì‹œê° ì…ë ¥ì„ 4ì°¨ì› ì ì¬ ê³µê°„ì— ì¸ì½”ë”©í•˜ì—¬ ì¬êµ¬ì„±ê³¼ ì´í•´ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ 4D íšŒì „ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ìˆœìˆ˜ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•˜ì˜€ìœ¼ë©°, ì•ˆì •ì ì¸ í›ˆë ¨ì„ ìœ„í•´ ì§€ê°ì  ì†ì‹¤ê³¼ Gram í–‰ë ¬ ì†ì‹¤ì„ ê²°í•©í•œ ë¹„ëŒ€ë¦½ì  í›ˆë ¨ ëª©í‘œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ATokenì€ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, 3D ìì‚°ì— ëŒ€í•´ ì ì§„ì ì¸ í›ˆë ¨ì„ í†µí•´ ì—°ì† ë° ì´ì‚° ì ì¬ í† í°ì„ ì§€ì›í•˜ë©°, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” í†µí•© ì‹œê° í† í¬ë‚˜ì´ì œì´ì…˜ì— ê¸°ë°˜í•œ ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì˜ ë°œì „ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ATokenì€ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, 3D ìì‚°ì— ëŒ€í•´ ê³ í’ˆì§ˆ ì¬êµ¬ì„±ê³¼ ì˜ë¯¸ ì´í•´ë¥¼ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ìµœì´ˆì˜ í†µí•© ì‹œê° í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.
- 2. ATokenì€ ë‹¤ì–‘í•œ ì‹œê° ì…ë ¥ì„ 4D ì ì¬ ê³µê°„ì— ì¸ì½”ë”©í•˜ì—¬ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ì—ì„œ ì¬êµ¬ì„±ê³¼ ì´í•´ ì‘ì—…ì„ í†µí•©í•©ë‹ˆë‹¤.
- 3. ìˆœìˆ˜í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì™€ 4D íšŒì „ ìœ„ì¹˜ ì„ë² ë”©ì„ ë„ì…í•˜ì—¬ ì„ì˜ì˜ í•´ìƒë„ì™€ ì‹œê°„ ê¸¸ì´ì˜ ì‹œê° ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- 4. ATokenì€ ì ì§„ì ì¸ í›ˆë ¨ ì»¤ë¦¬í˜ëŸ¼ì„ í†µí•´ ë‹¨ì¼ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, 3Dì—ì„œ í™•ì¥ë˜ë©°, ì—°ì† ë° ì´ì‚° ì ì¬ í† í°ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
- 5. ATokenì€ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì˜ ë°œì „ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:15:45*