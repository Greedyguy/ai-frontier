---
keywords:
  - Multimodal Learning
  - Visual Perception Reward
  - Reinforcement Learning with Verifiable Rewards
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2506.07218
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:02:52.706437",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Visual Perception Reward",
    "Reinforcement Learning with Verifiable Rewards",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Visual Perception Reward": 0.78,
    "Reinforcement Learning with Verifiable Rewards": 0.77,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of integrating multiple modalities in language models, which is crucial for advanced reasoning tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Visual Perception Reward",
        "canonical": "Visual Perception Reward",
        "aliases": [
          "Perception-R1"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specifically designed to enhance visual perception in multimodal models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning with Verifiable Rewards",
        "canonical": "Reinforcement Learning with Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "A specific technique applied to improve reasoning in multimodal models, relevant for linking advanced learning methods.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents the integration of visual and language data, a key area of development in multimodal AI.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Visual Perception Reward",
      "resolved_canonical": "Visual Perception Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning with Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning with Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward

**Korean Title:** 지각-R1: 시각적 지각 보상을 통한 MLLM의 다중 모드 추론 능력 향상

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.07218.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2506.07218](https://arxiv.org/abs/2506.07218)

## 🔗 유사한 논문
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (86.6% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (86.6% similar)
- [[2025-09-22/BaseReward_ A Strong Baseline for Multimodal Reward Model_20250922|BaseReward: A Strong Baseline for Multimodal Reward Model]] (86.2% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (86.2% similar)
- [[2025-09-22/Pointing to a Llama and Call it a Camel_ On the Sycophancy of Multimodal Large Language Models_20250922|Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models]] (86.1% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Visual Perception Reward|Visual Perception Reward]], [[keywords/Reinforcement Learning with Verifiable Rewards|Reinforcement Learning with Verifiable Rewards]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.07218v2 Announce Type: replace-cross 
Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.

## 🔍 Abstract (한글 번역)

arXiv:2506.07218v2 발표 유형: 교체-교차  
초록: 다중모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)의 다중모달 추론 능력을 향상시키는 것은 도전적인 과제로, 커뮤니티 내에서 점점 더 많은 관심을 받고 있습니다. 최근 여러 연구에서는 MLLMs의 추론 능력을 향상시키기 위해 검증 가능한 보상(Reinforcement Learning with Verifiable Rewards, RLVR)을 다중모달 도메인에 적용하고 있습니다. 그러나 이러한 연구들은 MLLMs의 다중모달 지각 능력 향상을 크게 간과하고 있으며, 이는 복잡한 다중모달 추론의 핵심 전제 조건이자 기초 구성 요소로 작용합니다. McNemar 검정을 통해 기존의 RLVR 방법이 MLLMs의 다중모달 지각 능력을 효과적으로 향상시키지 못하여, 다중모달 추론의 추가적인 개선을 제한하고 있음을 발견했습니다. 이러한 한계를 해결하기 위해, 우리는 Perception-R1을 제안합니다. 이는 MLLMs가 시각적 콘텐츠를 정확하게 인식하도록 명시적으로 장려하는 새로운 시각적 지각 보상을 도입하여, 다중모달 지각 및 추론 능력을 효과적으로 강화할 수 있습니다. 구체적으로, 우리는 먼저 다중모달 문제의 CoT 경로에서 텍스트 기반의 시각적 주석을 수집하여 보상 할당을 위한 시각적 참조로 사용합니다. RLVR 훈련 중에는, 판단 LLM을 사용하여 시각적 주석과 MLLM이 생성한 응답 간의 일관성을 평가하고, 이러한 일관성 판단에 기반하여 시각적 지각 보상을 할당합니다. 여러 다중모달 추론 벤치마크에서의 광범위한 실험은 Perception-R1의 효과를 입증하며, 단 1,442개의 훈련 데이터만을 사용하여 대부분의 벤치마크에서 최첨단 성능을 달성했습니다.

## 📝 요약

이 논문은 다중모달 대형 언어 모델(MLLMs)의 다중모달 추론 능력을 향상시키기 위한 연구로, 기존의 강화 학습 방법(RLVR)이 다중모달 인식 능력을 충분히 개선하지 못한다는 문제를 지적합니다. 이를 해결하기 위해 새로운 시각적 인식 보상 체계인 Perception-R1을 제안합니다. 이 방법은 시각적 콘텐츠를 정확히 인식하도록 MLLMs를 유도하며, 텍스트 시각 주석을 활용해 보상을 할당합니다. 실험 결과, Perception-R1은 적은 양의 훈련 데이터로도 여러 벤치마크에서 최첨단 성능을 달성했습니다.

## 🎯 주요 포인트

- 1. 다중모달 대형 언어 모델(MLLM)의 다중모달 추론 능력 향상이 중요한 과제로 주목받고 있습니다.
- 2. 기존의 강화 학습 방법은 MLLM의 다중모달 인식 능력 향상에 효과적이지 않다는 한계가 있습니다.
- 3. Perception-R1은 시각적 인식 보상을 도입하여 MLLM의 시각적 콘텐츠 인식을 정확하게 하도록 유도합니다.
- 4. CoT 경로에서 수집한 텍스트 시각 주석을 기반으로 시각적 인식 보상을 할당하는 새로운 방법을 제안합니다.
- 5. Perception-R1은 여러 다중모달 추론 벤치마크에서 최첨단 성능을 달성하며, 1,442개의 훈련 데이터만으로도 효과적임을 입증했습니다.


---

*Generated on 2025-09-23 10:02:52*