---
keywords:
  - Multimodal Learning
  - Visual Perception Reward
  - Reinforcement Learning with Verifiable Rewards
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2506.07218
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:02:52.706437",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Visual Perception Reward",
    "Reinforcement Learning with Verifiable Rewards",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Visual Perception Reward": 0.78,
    "Reinforcement Learning with Verifiable Rewards": 0.77,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of integrating multiple modalities in language models, which is crucial for advanced reasoning tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Visual Perception Reward",
        "canonical": "Visual Perception Reward",
        "aliases": [
          "Perception-R1"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specifically designed to enhance visual perception in multimodal models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning with Verifiable Rewards",
        "canonical": "Reinforcement Learning with Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "A specific technique applied to improve reasoning in multimodal models, relevant for linking advanced learning methods.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents the integration of visual and language data, a key area of development in multimodal AI.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Visual Perception Reward",
      "resolved_canonical": "Visual Perception Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning with Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning with Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward

**Korean Title:** ì§€ê°-R1: ì‹œê°ì  ì§€ê° ë³´ìƒì„ í†µí•œ MLLMì˜ ë‹¤ì¤‘ ëª¨ë“œ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.07218.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2506.07218](https://arxiv.org/abs/2506.07218)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (86.6% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (86.6% similar)
- [[2025-09-22/BaseReward_ A Strong Baseline for Multimodal Reward Model_20250922|BaseReward: A Strong Baseline for Multimodal Reward Model]] (86.2% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (86.2% similar)
- [[2025-09-22/Pointing to a Llama and Call it a Camel_ On the Sycophancy of Multimodal Large Language Models_20250922|Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models]] (86.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Visual Perception Reward|Visual Perception Reward]], [[keywords/Reinforcement Learning with Verifiable Rewards|Reinforcement Learning with Verifiable Rewards]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.07218v2 Announce Type: replace-cross 
Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.07218v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Models, MLLMs)ì˜ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì€ ë„ì „ì ì¸ ê³¼ì œë¡œ, ì»¤ë®¤ë‹ˆí‹° ë‚´ì—ì„œ ì ì  ë” ë§ì€ ê´€ì‹¬ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ìµœê·¼ ì—¬ëŸ¬ ì—°êµ¬ì—ì„œëŠ” MLLMsì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ(Reinforcement Learning with Verifiable Rewards, RLVR)ì„ ë‹¤ì¤‘ëª¨ë‹¬ ë„ë©”ì¸ì— ì ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì—°êµ¬ë“¤ì€ MLLMsì˜ ë‹¤ì¤‘ëª¨ë‹¬ ì§€ê° ëŠ¥ë ¥ í–¥ìƒì„ í¬ê²Œ ê°„ê³¼í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ë³µì¡í•œ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡ ì˜ í•µì‹¬ ì „ì œ ì¡°ê±´ì´ì ê¸°ì´ˆ êµ¬ì„± ìš”ì†Œë¡œ ì‘ìš©í•©ë‹ˆë‹¤. McNemar ê²€ì •ì„ í†µí•´ ê¸°ì¡´ì˜ RLVR ë°©ë²•ì´ MLLMsì˜ ë‹¤ì¤‘ëª¨ë‹¬ ì§€ê° ëŠ¥ë ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ì§€ ëª»í•˜ì—¬, ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡ ì˜ ì¶”ê°€ì ì¸ ê°œì„ ì„ ì œí•œí•˜ê³  ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Perception-R1ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” MLLMsê°€ ì‹œê°ì  ì½˜í…ì¸ ë¥¼ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ ì¥ë ¤í•˜ëŠ” ìƒˆë¡œìš´ ì‹œê°ì  ì§€ê° ë³´ìƒì„ ë„ì…í•˜ì—¬, ë‹¤ì¤‘ëª¨ë‹¬ ì§€ê° ë° ì¶”ë¡  ëŠ¥ë ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°•í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë¨¼ì € ë‹¤ì¤‘ëª¨ë‹¬ ë¬¸ì œì˜ CoT ê²½ë¡œì—ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ì‹œê°ì  ì£¼ì„ì„ ìˆ˜ì§‘í•˜ì—¬ ë³´ìƒ í• ë‹¹ì„ ìœ„í•œ ì‹œê°ì  ì°¸ì¡°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. RLVR í›ˆë ¨ ì¤‘ì—ëŠ”, íŒë‹¨ LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  ì£¼ì„ê³¼ MLLMì´ ìƒì„±í•œ ì‘ë‹µ ê°„ì˜ ì¼ê´€ì„±ì„ í‰ê°€í•˜ê³ , ì´ëŸ¬í•œ ì¼ê´€ì„± íŒë‹¨ì— ê¸°ë°˜í•˜ì—¬ ì‹œê°ì  ì§€ê° ë³´ìƒì„ í• ë‹¹í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ Perception-R1ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ë©°, ë‹¨ 1,442ê°œì˜ í›ˆë ¨ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì—°êµ¬ë¡œ, ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ ë°©ë²•(RLVR)ì´ ë‹¤ì¤‘ëª¨ë‹¬ ì¸ì‹ ëŠ¥ë ¥ì„ ì¶©ë¶„íˆ ê°œì„ í•˜ì§€ ëª»í•œë‹¤ëŠ” ë¬¸ì œë¥¼ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì‹œê°ì  ì¸ì‹ ë³´ìƒ ì²´ê³„ì¸ Perception-R1ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‹œê°ì  ì½˜í…ì¸ ë¥¼ ì •í™•íˆ ì¸ì‹í•˜ë„ë¡ MLLMsë¥¼ ìœ ë„í•˜ë©°, í…ìŠ¤íŠ¸ ì‹œê° ì£¼ì„ì„ í™œìš©í•´ ë³´ìƒì„ í• ë‹¹í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, Perception-R1ì€ ì ì€ ì–‘ì˜ í›ˆë ¨ ë°ì´í„°ë¡œë„ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì´ ì¤‘ìš”í•œ ê³¼ì œë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ ë°©ë²•ì€ MLLMì˜ ë‹¤ì¤‘ëª¨ë‹¬ ì¸ì‹ ëŠ¥ë ¥ í–¥ìƒì— íš¨ê³¼ì ì´ì§€ ì•Šë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
- 3. Perception-R1ì€ ì‹œê°ì  ì¸ì‹ ë³´ìƒì„ ë„ì…í•˜ì—¬ MLLMì˜ ì‹œê°ì  ì½˜í…ì¸  ì¸ì‹ì„ ì •í™•í•˜ê²Œ í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
- 4. CoT ê²½ë¡œì—ì„œ ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ ì‹œê° ì£¼ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°ì  ì¸ì‹ ë³´ìƒì„ í• ë‹¹í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 5. Perception-R1ì€ ì—¬ëŸ¬ ë‹¤ì¤‘ëª¨ë‹¬ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, 1,442ê°œì˜ í›ˆë ¨ ë°ì´í„°ë§Œìœ¼ë¡œë„ íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:02:52*