---
keywords:
  - Large Language Model
  - Competitive Programming
  - Ollama Runtime
  - Kattis Problem Set
  - Gemini Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15283
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:52:57.876308",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Competitive Programming",
    "Ollama Runtime",
    "Kattis Problem Set",
    "Gemini Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Competitive Programming": 0.78,
    "Ollama Runtime": 0.65,
    "Kattis Problem Set": 0.72,
    "Gemini Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "This term is central to the study and connects with existing research on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "competitive programming tasks",
        "canonical": "Competitive Programming",
        "aliases": [
          "programming challenges"
        ],
        "category": "specific_connectable",
        "rationale": "Links to a specific application domain of LLMs, relevant for connecting with programming-focused studies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Ollama runtime",
        "canonical": "Ollama Runtime",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique technical term specific to the study's methodology, useful for linking implementation details.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.65
      },
      {
        "surface": "Kattis corpus",
        "canonical": "Kattis Problem Set",
        "aliases": [
          "Kattis"
        ],
        "category": "unique_technical",
        "rationale": "Specific dataset used in the study, important for linking to other works using the same corpus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Gemini 1.5",
        "canonical": "Gemini Model",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific model compared in the study, relevant for linking to proprietary model discussions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "competitive programming tasks",
      "resolved_canonical": "Competitive Programming",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Ollama runtime",
      "resolved_canonical": "Ollama Runtime",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "Kattis corpus",
      "resolved_canonical": "Kattis Problem Set",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Gemini 1.5",
      "resolved_canonical": "Gemini Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges

**Korean Title:** ì§€ì—­ LLMì˜ ë³µì¡í•œ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ í•´ê²°ì— ëŒ€í•œ í•œê³„ í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15283.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15283](https://arxiv.org/abs/2509.15283)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (85.5% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (85.2% similar)
- [[2025-09-19/Automating Modelica Module Generation Using Large Language Models_ A Case Study on Building Control Description Language_20250919|Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language]] (84.4% similar)
- [[2025-09-19/CodeLSI_ Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning_20250919|CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning]] (83.6% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Competitive Programming|Competitive Programming]]
**âš¡ Unique Technical**: [[keywords/Ollama Runtime|Ollama Runtime]], [[keywords/Kattis Problem Set|Kattis Problem Set]], [[keywords/Gemini Model|Gemini Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15283v1 Announce Type: cross 
Abstract: This study examines the performance of today's open-source, locally hosted large-language models (LLMs) in handling complex competitive programming tasks with extended problem descriptions and contexts. Building on the original Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit the pipeline to work entirely offline through the Ollama runtime, collapsing FACE's sprawling per-problem directory tree into a handful of consolidated JSON files, and adding robust checkpointing so multi-day runs can resume after failures. The enhanced framework generates, submits, and records solutions for the full Kattis corpus of 3,589 problems across eight code-oriented models ranging from 6.7-9 billion parameters. The submission results show that the overall pass@1 accuracy is modest for the local models, with the best models performing at approximately half the acceptance rate of the proprietary models, Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between private, cost-controlled LLM deployments and state-of-the-art proprietary services, yet also highlight the rapid progress of open models and the practical benefits of an evaluation workflow that organizations can replicate on in-house hardware.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15283v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì´ ì—°êµ¬ëŠ” ì˜¤ëŠ˜ë‚ ì˜ ì˜¤í”ˆ ì†ŒìŠ¤, ë¡œì»¬ í˜¸ìŠ¤íŒ…ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ í™•ì¥ëœ ë¬¸ì œ ì„¤ëª…ê³¼ ë§¥ë½ì„ í¬í•¨í•œ ë³µì¡í•œ ê²½ìŸ í”„ë¡œê·¸ë˜ë° ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ì„±ëŠ¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. AI ê¸°ë°˜ ì½”ë“œ ìƒì„± í‰ê°€ë¥¼ ìœ„í•œ ì›ë˜ì˜ í”„ë ˆì„ì›Œí¬(FACE)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì €ìë“¤ì€ Ollama ëŸ°íƒ€ì„ì„ í†µí•´ íŒŒì´í”„ë¼ì¸ì„ ì™„ì „íˆ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ê°œì¡°í•˜ì—¬ FACEì˜ ë°©ëŒ€í•œ ë¬¸ì œë³„ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ë¥¼ ëª‡ ê°œì˜ í†µí•©ëœ JSON íŒŒì¼ë¡œ ì¶•ì†Œí•˜ê³ , ë‹¤ì¤‘ì¼ ì‹¤í–‰ì´ ì‹¤íŒ¨ í›„ì—ë„ ì¬ê°œë  ìˆ˜ ìˆë„ë¡ ê°•ë ¥í•œ ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. í–¥ìƒëœ í”„ë ˆì„ì›Œí¬ëŠ” 6.7-9ì–µ ë§¤ê°œë³€ìˆ˜ ë²”ìœ„ì˜ 8ê°œ ì½”ë“œ ì§€í–¥ ëª¨ë¸ì— ê±¸ì³ 3,589ê°œì˜ ë¬¸ì œë¡œ êµ¬ì„±ëœ ì „ì²´ Kattis ì½”í¼ìŠ¤ì— ëŒ€í•œ ì†”ë£¨ì…˜ì„ ìƒì„±, ì œì¶œ ë° ê¸°ë¡í•©ë‹ˆë‹¤. ì œì¶œ ê²°ê³¼ëŠ” ë¡œì»¬ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ pass@1 ì •í™•ë„ê°€ ë¯¸ë¯¸í•˜ë©°, ìµœê³ ì˜ ëª¨ë¸ì´ Gemini 1.5 ë° ChatGPT-4ì™€ ê°™ì€ ë…ì  ëª¨ë¸ì˜ ìˆ˜ìš©ë¥ ì˜ ì ˆë°˜ ì •ë„ì— ë¶ˆê³¼í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì‚¬ì„¤, ë¹„ìš© í†µì œëœ LLM ë°°í¬ì™€ ìµœì²¨ë‹¨ ë…ì  ì„œë¹„ìŠ¤ ê°„ì˜ ì§€ì†ì ì¸ ê²©ì°¨ë¥¼ ë“œëŸ¬ë‚´ì§€ë§Œ, ë˜í•œ ì˜¤í”ˆ ëª¨ë¸ì˜ ë¹ ë¥¸ ë°œì „ê³¼ ì¡°ì§ì´ ìì²´ í•˜ë“œì›¨ì–´ì—ì„œ ë³µì œí•  ìˆ˜ ìˆëŠ” í‰ê°€ ì›Œí¬í”Œë¡œìš°ì˜ ì‹¤ì§ˆì ì¸ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì˜¤ëŠ˜ë‚ ì˜ ì˜¤í”ˆ ì†ŒìŠ¤, ë¡œì»¬ í˜¸ìŠ¤íŒ…ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë³µì¡í•œ í”„ë¡œê·¸ë˜ë° ë¬¸ì œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ê¸°ì¡´ AI ê¸°ë°˜ ì½”ë“œ ìƒì„± í‰ê°€ í”„ë ˆì„ì›Œí¬(FACE)ë¥¼ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‘ë™í•˜ë„ë¡ ìˆ˜ì •í•˜ì—¬ Ollama ëŸ°íƒ€ì„ì„ í†µí•´ ë¬¸ì œ í•´ê²° ê³¼ì •ì„ í†µí•©í•˜ê³ , ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. 6.7-9ì–µ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ 8ê°œì˜ ì½”ë“œ ì§€í–¥ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 3,589ê°œì˜ Kattis ë¬¸ì œì— ëŒ€í•œ ì†”ë£¨ì…˜ì„ ìƒì„±, ì œì¶œ, ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ë¡œì»¬ ëª¨ë¸ì˜ pass@1 ì •í™•ë„ëŠ” ë‚®ì•˜ìœ¼ë©°, ìƒìœ„ ëª¨ë¸ë„ Gemini 1.5 ë° ChatGPT-4ì™€ ë¹„êµí•˜ì—¬ ì ˆë°˜ ìˆ˜ì¤€ì˜ ìˆ˜ìš©ë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì‚¬ì„¤ LLMê³¼ ìµœì‹  ìƒìš© ì„œë¹„ìŠ¤ ê°„ì˜ ê²©ì°¨ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ, ì˜¤í”ˆ ëª¨ë¸ì˜ ë¹ ë¥¸ ë°œì „ê³¼ ìì²´ í•˜ë“œì›¨ì–´ì—ì„œ í‰ê°€ ì›Œí¬í”Œë¡œë¥¼ ë³µì œí•  ìˆ˜ ìˆëŠ” ì‹¤ì§ˆì  ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ë³µì¡í•œ ê²½ìŸ í”„ë¡œê·¸ë˜ë° ê³¼ì œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ìˆì–´ ì˜¤í”ˆ ì†ŒìŠ¤, ë¡œì»¬ í˜¸ìŠ¤íŒ… ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 2. ì—°êµ¬ì§„ì€ FACE í”„ë ˆì„ì›Œí¬ë¥¼ ì˜¤í”„ë¼ì¸ì—ì„œ ì™„ì „íˆ ì‘ë™í•˜ë„ë¡ ê°œì¡°í•˜ì—¬ Ollama ëŸ°íƒ€ì„ì„ í†µí•´ ë¬¸ì œë³„ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ë¥¼ í†µí•©ëœ JSON íŒŒì¼ë¡œ ì¶•ì†Œí•˜ê³ , ê²¬ê³ í•œ ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼, ë¡œì»¬ ëª¨ë¸ì˜ ì „ì²´ pass@1 ì •í™•ë„ëŠ” ë³´í†µ ìˆ˜ì¤€ì´ë©°, ìµœê³ ì˜ ëª¨ë¸ë„ Gemini 1.5ì™€ ChatGPT-4 ê°™ì€ ë…ì  ëª¨ë¸ì˜ ì ˆë°˜ ì •ë„ì˜ ìˆ˜ë½ë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. ì´ ê²°ê³¼ëŠ” ë¹„ìš©ì´ í†µì œëœ LLM ë°°í¬ì™€ ìµœì²¨ë‹¨ ë…ì  ì„œë¹„ìŠ¤ ê°„ì˜ ì§€ì†ì ì¸ ê²©ì°¨ë¥¼ ë“œëŸ¬ë‚´ì§€ë§Œ, ì˜¤í”ˆ ëª¨ë¸ì˜ ë¹ ë¥¸ ë°œì „ê³¼ ì¡°ì§ì´ ìì²´ í•˜ë“œì›¨ì–´ì—ì„œ ë³µì œí•  ìˆ˜ ìˆëŠ” í‰ê°€ ì›Œí¬í”Œë¡œì˜ ì‹¤ì§ˆì ì¸ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 08:52:57*