# Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges

**Korean Title:** ë³µì¡í•œ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ í•´ê²°ì—ì„œ ì§€ì—­ LLMì˜ í•œê³„ í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: AI-driven Code Generation Evaluation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Automating Modelica Module Generation Using Large Language Models_ A Case Study on Building Control Description Language_20250919|Automating Modelica Module Generation Using Large Language Models A Case Study on Building Control Description Language]] (84.4% similar)
- [[2025-09-19/CodeLSI_ Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning_20250919|CodeLSI Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning]] (83.6% similar)
- [[2025-09-18/An LLM Agentic Approach for Legal-Critical Software_ A Case Study for Tax Prep Software_20250918|An LLM Agentic Approach for Legal-Critical Software A Case Study for Tax Prep Software]] (83.4% similar)
- [[2025-09-19/From Capabilities to Performance_ Evaluating Key Functional Properties of LLM Architectures in Penetration Testing_20250919|From Capabilities to Performance Evaluating Key Functional Properties of LLM Architectures in Penetration Testing]] (83.2% similar)
- [[2025-09-18/CARGO_ A Framework for Confidence-Aware Routing of Large Language Models_20250918|CARGO A Framework for Confidence-Aware Routing of Large Language Models]] (83.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15283v1 Announce Type: cross 
Abstract: This study examines the performance of today's open-source, locally hosted large-language models (LLMs) in handling complex competitive programming tasks with extended problem descriptions and contexts. Building on the original Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit the pipeline to work entirely offline through the Ollama runtime, collapsing FACE's sprawling per-problem directory tree into a handful of consolidated JSON files, and adding robust checkpointing so multi-day runs can resume after failures. The enhanced framework generates, submits, and records solutions for the full Kattis corpus of 3,589 problems across eight code-oriented models ranging from 6.7-9 billion parameters. The submission results show that the overall pass@1 accuracy is modest for the local models, with the best models performing at approximately half the acceptance rate of the proprietary models, Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between private, cost-controlled LLM deployments and state-of-the-art proprietary services, yet also highlight the rapid progress of open models and the practical benefits of an evaluation workflow that organizations can replicate on in-house hardware.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15283v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì´ ì—°êµ¬ëŠ” ì˜¤ëŠ˜ë‚ ì˜ ì˜¤í”ˆ ì†ŒìŠ¤, ë¡œì»¬ í˜¸ìŠ¤íŒ… ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ í™•ì¥ëœ ë¬¸ì œ ì„¤ëª…ê³¼ ë§¥ë½ì„ ê°€ì§„ ë³µì¡í•œ ê²½ìŸ í”„ë¡œê·¸ë˜ë° ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ì„±ëŠ¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. AI ê¸°ë°˜ ì½”ë“œ ìƒì„± í‰ê°€ë¥¼ ìœ„í•œ ì›ë˜ì˜ í”„ë ˆì„ì›Œí¬(FACE)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì €ìë“¤ì€ Ollama ëŸ°íƒ€ì„ì„ í†µí•´ íŒŒì´í”„ë¼ì¸ì„ ì™„ì „íˆ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ê°œì¡°í•˜ê³ , FACEì˜ ë°©ëŒ€í•œ ë¬¸ì œë³„ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ë¥¼ ì†Œìˆ˜ì˜ í†µí•©ëœ JSON íŒŒì¼ë¡œ ì¶•ì†Œí•˜ë©°, ë‹¤ì¼ê°„ì˜ ì‹¤í–‰ì´ ì‹¤íŒ¨ í›„ì—ë„ ì¬ê°œë  ìˆ˜ ìˆë„ë¡ ê°•ë ¥í•œ ì²´í¬í¬ì¸íŒ…ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. í–¥ìƒëœ í”„ë ˆì„ì›Œí¬ëŠ” 6.7-9ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ì¶˜ 8ê°œì˜ ì½”ë“œ ì§€í–¥ ëª¨ë¸ì— ê±¸ì³ 3,589ê°œì˜ ë¬¸ì œë¡œ êµ¬ì„±ëœ ì „ì²´ Kattis ì½”í¼ìŠ¤ì— ëŒ€í•œ ì†”ë£¨ì…˜ì„ ìƒì„±, ì œì¶œ ë° ê¸°ë¡í•©ë‹ˆë‹¤. ì œì¶œ ê²°ê³¼ëŠ” ë¡œì»¬ ëª¨ë¸ì˜ ì „ì²´ pass@1 ì •í™•ë„ê°€ ë³´í†µ ìˆ˜ì¤€ì„ì„ ë³´ì—¬ì£¼ë©°, ìµœê³ ì˜ ëª¨ë¸ì€ Gemini 1.5 ë° ChatGPT-4ì™€ ê°™ì€ ë…ì  ëª¨ë¸ì˜ ìˆ˜ìš©ë¥ ì˜ ì•½ ì ˆë°˜ ìˆ˜ì¤€ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì‚¬ì„¤, ë¹„ìš© í†µì œëœ LLM ë°°í¬ì™€ ìµœì²¨ë‹¨ ë…ì  ì„œë¹„ìŠ¤ ê°„ì˜ ì§€ì†ì ì¸ ê²©ì°¨ë¥¼ ë“œëŸ¬ë‚´ì§€ë§Œ, ë˜í•œ ì˜¤í”ˆ ëª¨ë¸ì˜ ë¹ ë¥¸ ë°œì „ê³¼ ì¡°ì§ì´ ìì²´ í•˜ë“œì›¨ì–´ì—ì„œ ë³µì œí•  ìˆ˜ ìˆëŠ” í‰ê°€ ì›Œí¬í”Œë¡œì˜ ì‹¤ì§ˆì ì¸ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì˜¤ëŠ˜ë‚ ì˜ ì˜¤í”ˆ ì†ŒìŠ¤, ë¡œì»¬ í˜¸ìŠ¤íŒ… ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë³µì¡í•œ ê²½ìŸ í”„ë¡œê·¸ë˜ë° ê³¼ì œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. AI ê¸°ë°˜ ì½”ë“œ ìƒì„± í‰ê°€ í”„ë ˆì„ì›Œí¬(FACE)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, Ollama ëŸ°íƒ€ì„ì„ í†µí•´ ì˜¤í”„ë¼ì¸ì—ì„œ ì‘ë™í•˜ë„ë¡ íŒŒì´í”„ë¼ì¸ì„ ê°œì¡°í•˜ê³ , JSON íŒŒì¼ë¡œ í†µí•©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. 6.7-9ì–µ ë§¤ê°œë³€ìˆ˜ì˜ 8ê°œ ì½”ë“œ ì§€í–¥ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Kattisì˜ 3,589ê°œ ë¬¸ì œì— ëŒ€í•œ ì†”ë£¨ì…˜ì„ ìƒì„±, ì œì¶œ, ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ë¡œì»¬ ëª¨ë¸ì˜ pass@1 ì •í™•ë„ëŠ” ë‹¤ì†Œ ë‚®ìœ¼ë©°, ìµœê³ ì˜ ëª¨ë¸ë„ Gemini 1.5ì™€ ChatGPT-4ì˜ ì ˆë°˜ ìˆ˜ì¤€ì— ê·¸ì³¤ìŠµë‹ˆë‹¤. ì´ëŠ” ì‚¬ì„¤ ëª¨ë¸ê³¼ ìµœì‹  ë…ì  ì„œë¹„ìŠ¤ ê°„ì˜ ê²©ì°¨ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ, ì˜¤í”ˆ ëª¨ë¸ì˜ ë¹ ë¥¸ ë°œì „ê³¼ ìì²´ í•˜ë“œì›¨ì–´ì—ì„œ í‰ê°€ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³µì œí•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ë³µì¡í•œ ê²½ìŸ í”„ë¡œê·¸ë˜ë° ê³¼ì œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ìˆì–´ ì˜¤í”ˆ ì†ŒìŠ¤, ë¡œì»¬ í˜¸ìŠ¤íŒ… ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

- 2. ì—°êµ¬ì§„ì€ FACE í”„ë ˆì„ì›Œí¬ë¥¼ ì˜¤í”„ë¼ì¸ì—ì„œ ì™„ì „íˆ ì‘ë™í•˜ë„ë¡ ê°œì¡°í•˜ì—¬ Ollama ëŸ°íƒ€ì„ì„ í†µí•´ ë¬¸ì œë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í†µí•©ëœ JSON íŒŒì¼ë¡œ ì¶•ì†Œí•˜ê³ , ê²¬ê³ í•œ ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

- 3. ì—°êµ¬ ê²°ê³¼, ë¡œì»¬ ëª¨ë¸ì˜ ì „ì²´ pass@1 ì •í™•ë„ëŠ” ë³´í†µ ìˆ˜ì¤€ì´ë©°, ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë„ Gemini 1.5 ë° ChatGPT-4ì™€ ê°™ì€ ë…ì  ëª¨ë¸ì˜ ì ˆë°˜ ì •ë„ì˜ ìˆ˜ìš©ë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

- 4. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì‚¬ì„¤ LLM ë°°í¬ì™€ ìµœì²¨ë‹¨ ë…ì  ì„œë¹„ìŠ¤ ê°„ì˜ ì§€ì†ì ì¸ ê²©ì°¨ë¥¼ ë“œëŸ¬ë‚´ì§€ë§Œ, ì˜¤í”ˆ ëª¨ë¸ì˜ ë¹ ë¥¸ ë°œì „ê³¼ ì¡°ì§ì´ ìì²´ í•˜ë“œì›¨ì–´ì—ì„œ ë³µì œí•  ìˆ˜ ìˆëŠ” í‰ê°€ ì›Œí¬í”Œë¡œì˜ ì‹¤ì§ˆì  ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 13:52:55*