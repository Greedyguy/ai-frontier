---
keywords:
  - Empathy-R1 Framework
  - Chain-of-Empathy Reasoning
  - Reinforcement Learning
  - Empathy-QA Dataset
  - Long Counseling Texts
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.14851
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:16:42.491336",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Empathy-R1 Framework",
    "Chain-of-Empathy Reasoning",
    "Reinforcement Learning",
    "Empathy-QA Dataset",
    "Long Counseling Texts"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Empathy-R1 Framework": 0.8,
    "Chain-of-Empathy Reasoning": 0.78,
    "Reinforcement Learning": 0.85,
    "Empathy-QA Dataset": 0.77,
    "Long Counseling Texts": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Empathy-R1",
        "canonical": "Empathy-R1 Framework",
        "aliases": [
          "Empathy-R1"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel framework specifically designed for mental health support, offering potential for unique insights and connections.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Chain-of-Empathy",
        "canonical": "Chain-of-Empathy Reasoning",
        "aliases": [
          "CoE"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new reasoning process inspired by cognitive-behavioral therapy, enhancing the interpretability of AI responses.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental technique in AI that enhances the framework's response quality, relevant for linking with other AI methodologies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Empathy-QA",
        "canonical": "Empathy-QA Dataset",
        "aliases": [
          "Empathy-QA"
        ],
        "category": "unique_technical",
        "rationale": "A large-scale dataset critical for training and evaluating the framework, offering opportunities for data-driven insights.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "Long Counseling Texts",
        "canonical": "Long Counseling Texts",
        "aliases": [
          "LCTs"
        ],
        "category": "specific_connectable",
        "rationale": "Focuses on a specific application area for AI, linking mental health support with language processing challenges.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Empathy-R1",
      "resolved_canonical": "Empathy-R1 Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Chain-of-Empathy",
      "resolved_canonical": "Chain-of-Empathy Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Empathy-QA",
      "resolved_canonical": "Empathy-QA Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Long Counseling Texts",
      "resolved_canonical": "Long Counseling Texts",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support

**Korean Title:** 공감-R1: 장기 정신 건강 지원을 위한 공감 연쇄 및 강화 학습 프레임워크

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.14851.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.14851](https://arxiv.org/abs/2509.14851)

## 🔗 유사한 논문
- [[2025-09-18/Empathy-R1_ A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support_20250918|Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support]] (99.3% similar)
- [[2025-09-19/LLM Agents at the Roundtable_ A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring_20250919|LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring]] (82.1% similar)
- [[2025-09-22/Fleming-R1_ Toward Expert-Level Medical Reasoning via Reinforcement Learning_20250922|Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning]] (81.0% similar)
- [[2025-09-22/Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter_20250922|Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter]] (81.0% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (80.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Long Counseling Texts|Long Counseling Texts]]
**⚡ Unique Technical**: [[keywords/Empathy-R1 Framework|Empathy-R1 Framework]], [[keywords/Chain-of-Empathy Reasoning|Chain-of-Empathy Reasoning]], [[keywords/Empathy-QA Dataset|Empathy-QA Dataset]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14851v2 Announce Type: replace-cross 
Abstract: Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.

## 🔍 Abstract (한글 번역)

arXiv:2509.14851v2 발표 유형: 교차 교체  
초록: 공감은 특히 긴 상담 텍스트(LCT)를 다룰 때 효과적인 정신 건강 지원에 필수적입니다. 그러나 기존의 대형 언어 모델(LLM)은 의미적으로 유창한 답변을 생성하지만, 특히 중국어 맥락에서 진정한 심리적 지원에 필요한 구조화된 추론이 부족한 경우가 많습니다. 이러한 격차를 해소하기 위해 우리는 LCT의 응답 품질을 향상시키기 위해 공감 체인(CoE) 추론 프로세스와 강화 학습(RL)을 통합한 새로운 프레임워크인 Empathy-R1을 소개합니다. 인지 행동 치료에서 영감을 받은 우리의 CoE 패러다임은 모델이 도움을 요청하는 사람의 감정, 원인 및 의도를 순차적으로 추론하도록 안내하여 사고 과정을 투명하고 해석 가능하게 만듭니다. 우리의 프레임워크는 새로운 대규모 중국어 데이터셋인 Empathy-QA와 2단계 학습 프로세스를 통해 강화됩니다. 먼저, 지도형 미세 조정을 통해 CoE의 추론 구조를 주입합니다. 그 후, 전용 보상 모델에 의해 안내되는 RL은 최종 응답의 치료적 관련성과 맥락적 적절성을 개선합니다. 실험 결과 Empathy-R1은 주요 자동 지표에서 강력한 성능을 달성했습니다. 더 중요한 것은 인간 평가에서 강력한 기준선보다 명확한 선호도를 보이며 새로운 벤치마크에서 Win@1 비율 44.30%를 달성하여 그 우수성을 확인했다는 것입니다. 해석 가능하고 맥락적으로 미묘한 응답을 가능하게 함으로써 Empathy-R1은 정신 건강 지원을 위한 책임 있고 진정으로 유익한 AI 개발에 있어 중요한 진전을 나타냅니다.

## 📝 요약

Empathy-R1은 장문의 상담 텍스트(LCTs)에 대한 효과적인 정신 건강 지원을 위해 개발된 새로운 프레임워크입니다. 기존의 대형 언어 모델(LLMs)이 심리적 지원에 필요한 구조적 추론을 결여한 반면, Empathy-R1은 Chain-of-Empathy(CoE) 추론 과정과 강화 학습(RL)을 결합하여 응답의 질을 향상시킵니다. CoE는 인지행동치료에서 영감을 받아, 도움을 요청하는 사람의 감정, 원인, 의도를 순차적으로 추론하여 투명하고 해석 가능한 사고 과정을 제공합니다. 이 프레임워크는 대규모 중국어 데이터셋 Empathy-QA와 두 단계의 훈련 과정을 통해 강화됩니다. 실험 결과, Empathy-R1은 자동 평가 지표에서 우수한 성능을 보였으며, 인간 평가에서도 강력한 기준 모델보다 선호되며 Win@1 비율 44.30%를 기록했습니다. 이는 정신 건강 지원을 위한 책임 있고 유익한 AI 개발에 있어 중요한 발전을 의미합니다.

## 🎯 주요 포인트

- 1. Empathy-R1은 장문의 상담 텍스트(LCTs)에 대한 응답 품질을 향상시키기 위해 공감의 연쇄 추론(CoE)과 강화 학습(RL)을 통합한 새로운 프레임워크입니다.
- 2. CoE 패러다임은 인지행동치료에서 영감을 받아 도움을 요청하는 사람의 감정, 원인, 의도를 순차적으로 추론하여 모델의 사고 과정을 투명하고 해석 가능하게 만듭니다.
- 3. Empathy-R1은 새로운 대규모 중국어 데이터셋 Empathy-QA와 두 단계의 훈련 과정을 통해 강화됩니다.
- 4. Empathy-R1은 자동 평가 지표에서 강력한 성능을 보이며, 인간 평가에서도 강력한 기준선보다 우수한 결과를 보여줍니다.
- 5. Empathy-R1은 해석 가능하고 상황에 맞는 응답을 가능하게 하여 정신 건강 지원을 위한 책임 있고 유익한 AI 개발에 중요한 진전을 나타냅니다.


---

*Generated on 2025-09-23 10:16:42*