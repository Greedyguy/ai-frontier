---
keywords:
  - Vision-Language Model
  - Cache of Thought
  - Master-Apprentice Framework
  - Multimodal Learning
  - Few-Shot Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2502.20587
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:04:59.570545",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Cache of Thought",
    "Master-Apprentice Framework",
    "Multimodal Learning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.91,
    "Cache of Thought": 0.89,
    "Master-Apprentice Framework": 0.85,
    "Multimodal Learning": 0.83,
    "Few-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's framework and connect to the broader trend of multimodal AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.91
      },
      {
        "surface": "Cache of Thought",
        "canonical": "Cache of Thought",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Cache of Thought is a unique framework introduced in the paper, offering a novel approach to collaborative inference.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.89
      },
      {
        "surface": "Master Apprentice Framework",
        "canonical": "Master-Apprentice Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The Master-Apprentice Framework is a novel concept in the paper, facilitating the interaction between large and small VLMs.",
        "novelty_score": 0.87,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multimodal Retrieval",
        "canonical": "Multimodal Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Multimodal Retrieval is a key component of the proposed framework, aligning with the trending concept of Multimodal Learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.76,
        "link_intent_score": 0.83
      },
      {
        "surface": "In-Context Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Contextual Learning"
        ],
        "category": "specific_connectable",
        "rationale": "In-Context Learning is related to Few-Shot Learning, enhancing the performance of smaller models in the framework.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.79,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "response quality",
      "query results",
      "general reasoning benchmarks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.91
      }
    },
    {
      "candidate_surface": "Cache of Thought",
      "resolved_canonical": "Cache of Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "Master Apprentice Framework",
      "resolved_canonical": "Master-Apprentice Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.87,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multimodal Retrieval",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.76,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.79,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning

**Korean Title:** "ì‚¬ê³ ì˜ ìºì‹œ: ë¹„ìš© íš¨ìœ¨ì ì¸ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ë§ˆìŠ¤í„°-ê²¬ìŠµìƒ í”„ë ˆì„ì›Œí¬"

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.20587.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2502.20587](https://arxiv.org/abs/2502.20587)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (85.6% similar)
- [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (84.3% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (84.1% similar)
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (84.1% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Cache of Thought|Cache of Thought]], [[keywords/Master-Apprentice Framework|Master-Apprentice Framework]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.20587v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved remarkable success in a wide range of vision applications of increasing complexity and scales, yet choosing the right VLM model size involves a trade-off between response quality and cost. While smaller VLMs are cheaper to run, they typically produce responses only marginally better than random guessing on benchmarks such as MMMU.
  In this paper, we propose Cache of Thought (CoT), a master apprentice framework for collaborative inference between large and small VLMs. CoT manages high quality query results from large VLMs (master) in a cache, which are then selected via a novel multi modal retrieval and in-context learning to aid the performance of small VLMs (apprentice). We extensively evaluate CoT on various widely recognized and challenging general reasoning benchmarks, and show that CoT increases overall reasoning performance by up to 7.7% under the same budget, and specifically boosts the performance of apprentice VLMs by up to 36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.20587v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë¹„ì „ ì–¸ì–´ ëª¨ë¸(Vision Language Models, VLMs)ì€ ì ì  ë” ë³µì¡í•˜ê³  ê·œëª¨ê°€ ì»¤ì§€ëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ì‘ìš© ë¶„ì•¼ì—ì„œ ë†€ë¼ìš´ ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ì ì ˆí•œ VLM ëª¨ë¸ í¬ê¸°ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì€ ì‘ë‹µ í’ˆì§ˆê³¼ ë¹„ìš© ê°„ì˜ ê· í˜•ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ì‘ì€ VLMì€ ìš´ì˜ ë¹„ìš©ì´ ì €ë ´í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ MMMUì™€ ê°™ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¸¡ë³´ë‹¤ ì•½ê°„ ë” ë‚˜ì€ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.  
ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ë° ì†Œí˜• VLM ê°„ì˜ í˜‘ë ¥ì  ì¶”ë¡ ì„ ìœ„í•œ ë§ˆìŠ¤í„° ê²¬ìŠµìƒ í”„ë ˆì„ì›Œí¬ì¸ Cache of Thought (CoT)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CoTëŠ” ëŒ€í˜• VLM(ë§ˆìŠ¤í„°)ì˜ ê³ í’ˆì§ˆ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ê³ , ì´ë¥¼ í†µí•´ ì†Œí˜• VLM(ê²¬ìŠµìƒ)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ìƒˆë¡œìš´ ë‹¤ì¤‘ ëª¨ë‹¬ ê²€ìƒ‰ ë° ë§¥ë½ ë‚´ í•™ìŠµì„ í†µí•´ ì„ íƒí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” CoTë¥¼ ë‹¤ì–‘í•œ ë„ë¦¬ ì¸ì •ë°›ëŠ” ë„ì „ì ì¸ ì¼ë°˜ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê´‘ë²”ìœ„í•˜ê²Œ í‰ê°€í•˜ì˜€ìœ¼ë©°, ë™ì¼í•œ ì˜ˆì‚° í•˜ì—ì„œ ì „ì²´ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœëŒ€ 7.7% í–¥ìƒì‹œí‚¤ê³ , íŠ¹íˆ ê²¬ìŠµìƒ VLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 36.6% í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/UIUC-MONET/Cache-of-Thoughtsì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ë° ì†Œí˜• ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM) ê°„ì˜ í˜‘ë ¥ ì¶”ë¡ ì„ ìœ„í•œ Cache of Thought(CoT)ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CoTëŠ” ëŒ€í˜• VLM(ë§ˆìŠ¤í„°)ì˜ ê³ í’ˆì§ˆ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ê³ , ì´ë¥¼ ì†Œí˜• VLM(ê²¬ìŠµìƒ)ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë‹¤ì¤‘ ëª¨ë‹¬ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ í•™ìŠµì„ í†µí•´ ì„ íƒí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¼ë°˜ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ CoTë¥¼ í‰ê°€í•œ ê²°ê³¼, ë™ì¼í•œ ì˜ˆì‚° í•˜ì— ì „ì²´ ì¶”ë¡  ì„±ëŠ¥ì´ ìµœëŒ€ 7.7% í–¥ìƒë˜ì—ˆìœ¼ë©°, íŠ¹íˆ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì´ ìµœëŒ€ 36.6% ì¦ê°€í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision Language Models (VLMs)ëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ëª¨ë¸ í¬ê¸° ì„ íƒ ì‹œ ì‘ë‹µ í’ˆì§ˆê³¼ ë¹„ìš© ì‚¬ì´ì˜ ê· í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤.
- 2. Cache of Thought (CoT)ëŠ” ëŒ€í˜• VLM(ë§ˆìŠ¤í„°)ê³¼ ì†Œí˜• VLM(ê²¬ìŠµìƒ) ê°„ì˜ í˜‘ë ¥ ì¶”ë¡ ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, ëŒ€í˜• VLMì˜ ê³ í’ˆì§ˆ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ê³  ì´ë¥¼ í†µí•´ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 3. CoTëŠ” ë‹¤ì–‘í•œ ì¼ë°˜ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 7.7%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€ 36.6%ê¹Œì§€ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. CoTëŠ” ìƒˆë¡œìš´ ë‹¤ì¤‘ ëª¨ë‹¬ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ í•™ìŠµì„ í†µí•´ ì†Œí˜• VLMì˜ ì„±ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.
- 5. CoTì˜ ì½”ë“œëŠ” https://github.com/UIUC-MONET/Cache-of-Thoughtsì—ì„œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:04:59*