---
keywords:
  - Self-supervised Learning
  - Bi-level Optimization
  - Random Quantization
  - Pseudo-label Generation
  - Label Refinement
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15430
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:28:22.493711",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Bi-level Optimization",
    "Random Quantization",
    "Pseudo-label Generation",
    "Label Refinement"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.85,
    "Bi-level Optimization": 0.78,
    "Random Quantization": 0.72,
    "Pseudo-label Generation": 0.8,
    "Label Refinement": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Self-supervised learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "SSL"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's methodology, linking it to broader discussions on self-supervised approaches.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Bi-level optimization",
        "canonical": "Bi-level Optimization",
        "aliases": [
          "Bilevel optimization"
        ],
        "category": "unique_technical",
        "rationale": "A unique methodological approach in the paper, offering a distinct link to optimization techniques.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Random quantization",
        "canonical": "Random Quantization",
        "aliases": [
          "Random-projection quantizer"
        ],
        "category": "unique_technical",
        "rationale": "A specific technique used in the paper, relevant for discussions on quantization methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Pseudo-label generation",
        "canonical": "Pseudo-label Generation",
        "aliases": [
          "Pseudo-labeling"
        ],
        "category": "specific_connectable",
        "rationale": "Key to the paper's approach, connecting to broader themes in label generation strategies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.68,
        "link_intent_score": 0.8
      },
      {
        "surface": "Label refinement",
        "canonical": "Label Refinement",
        "aliases": [
          "Label enhancement"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding iterative improvement in labeling, linking to similar refinement processes.",
        "novelty_score": 0.5,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Self-supervised learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Bi-level optimization",
      "resolved_canonical": "Bi-level Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Random quantization",
      "resolved_canonical": "Random Quantization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Pseudo-label generation",
      "resolved_canonical": "Pseudo-label Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.68,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Label refinement",
      "resolved_canonical": "Label Refinement",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition

**Korean Title:** BiRQ: 자기 지도 음성 인식을 위한 이중 수준 자기 라벨링 랜덤 양자화

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15430.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15430](https://arxiv.org/abs/2509.15430)

## 🔗 유사한 논문
- [[2025-09-22/Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization_20250922|Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization]] (81.6% similar)
- [[2025-09-17/Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures_20250917|Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures]] (80.6% similar)
- [[2025-09-18/Evolving Language Models without Labels_ Majority Drives Selection, Novelty Promotes Variation_20250918|Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation]] (80.4% similar)
- [[2025-09-18/BabyHuBERT_ Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings_20250918|BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings]] (80.2% similar)
- [[2025-09-22/MEC-Quant_ Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training_20250922|MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training]] (80.2% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Pseudo-label Generation|Pseudo-label Generation]], [[keywords/Label Refinement|Label Refinement]]
**⚡ Unique Technical**: [[keywords/Bi-level Optimization|Bi-level Optimization]], [[keywords/Random Quantization|Random Quantization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15430v1 Announce Type: new 
Abstract: Speech is a rich signal, and labeled audio-text pairs are costly, making self-supervised learning essential for scalable representation learning. A core challenge in speech SSL is generating pseudo-labels that are both informative and efficient: strong labels, such as those used in HuBERT, improve downstream performance but rely on external encoders and multi-stage pipelines, while efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels. We propose BiRQ, a bilevel SSL framework that combines the efficiency of BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key idea is to reuse part of the model itself as a pseudo-label generator: intermediate representations are discretized by a random-projection quantizer to produce enhanced labels, while anchoring labels derived directly from the raw input stabilize training and prevent collapse. Training is formulated as an efficient first-order bilevel optimization problem, solved end-to-end with differentiable Gumbel-softmax selection. This design eliminates the need for external label encoders, reduces memory cost, and enables iterative label refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ while maintaining low complexity and computational efficiency. We validate our method on various datasets, including 960-hour LibriSpeech, 150-hour AMI meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.

## 🔍 Abstract (한글 번역)

arXiv:2509.15430v1 발표 유형: 신규  
초록: 음성은 풍부한 신호이며, 레이블이 있는 오디오-텍스트 쌍은 비용이 많이 들기 때문에, 확장 가능한 표현 학습을 위해 자기 지도 학습(self-supervised learning, SSL)이 필수적입니다. 음성 SSL의 핵심 과제는 정보가 풍부하면서도 효율적인 의사 레이블을 생성하는 것입니다. HuBERT에서 사용되는 강력한 레이블은 다운스트림 성능을 향상시키지만, 외부 인코더와 다단계 파이프라인에 의존합니다. 반면, BEST-RQ와 같은 효율적인 방법은 단순성을 유지하지만 약한 레이블을 사용합니다. 우리는 BEST-RQ의 효율성과 HuBERT 스타일의 레이블 개선의 이점을 결합한 이중 수준(bilevel) SSL 프레임워크인 BiRQ를 제안합니다. 핵심 아이디어는 모델의 일부를 의사 레이블 생성기로 재사용하는 것입니다: 중간 표현은 랜덤 프로젝션 양자화기를 통해 이산화되어 향상된 레이블을 생성하며, 원시 입력에서 직접 파생된 앵커 레이블은 훈련을 안정화하고 붕괴를 방지합니다. 훈련은 효율적인 1차 이중 수준 최적화 문제로 공식화되어, 미분 가능한 Gumbel-softmax 선택을 통해 종단 간(end-to-end)으로 해결됩니다. 이 설계는 외부 레이블 인코더의 필요성을 제거하고, 메모리 비용을 줄이며, 종단 간 방식으로 반복적인 레이블 개선을 가능하게 합니다. BiRQ는 낮은 복잡성과 계산 효율성을 유지하면서 BEST-RQ보다 일관되게 향상된 성능을 보여줍니다. 우리는 960시간의 LibriSpeech, 150시간의 AMI 회의, 5,000시간의 YODAS를 포함한 다양한 데이터셋에서 우리의 방법을 검증하여 BEST-RQ 대비 일관된 성능 향상을 입증합니다.

## 📝 요약

이 논문은 음성 자기 지도 학습(SSL)에서 효율적이고 정보가 풍부한 의사 레이블 생성의 중요성을 다룹니다. 기존의 HuBERT는 강력한 레이블을 사용하지만 외부 인코더와 복잡한 파이프라인이 필요하며, BEST-RQ는 간단하지만 약한 레이블을 생성합니다. 이를 해결하기 위해 제안된 BiRQ는 BEST-RQ의 효율성과 HuBERT 스타일의 레이블 개선을 결합한 이중 수준 SSL 프레임워크입니다. 모델의 일부를 의사 레이블 생성기로 재사용하여 중간 표현을 무작위 투영 양자화기를 통해 개선된 레이블로 변환하고, 원시 입력에서 직접 파생된 앵커 레이블로 훈련을 안정화합니다. 이 방법은 외부 레이블 인코더가 필요 없으며, 메모리 비용을 줄이고, 레이블을 반복적으로 개선할 수 있습니다. BiRQ는 BEST-RQ보다 일관되게 성능을 향상시키며, LibriSpeech, AMI, YODAS 등 다양한 데이터셋에서 그 효과를 검증했습니다.

## 🎯 주요 포인트

- 1. BiRQ는 BEST-RQ의 효율성과 HuBERT 스타일의 라벨 개선 이점을 결합한 이중 수준의 자기 지도 학습(SSL) 프레임워크입니다.
- 2. 모델의 일부를 의사 라벨 생성기로 재사용하여 중간 표현을 무작위 투영 양자화기로 이산화하여 개선된 라벨을 생성합니다.
- 3. 훈련은 효율적인 1차 이중 수준 최적화 문제로 공식화되어, 외부 라벨 인코더의 필요성을 제거하고 메모리 비용을 줄입니다.
- 4. BiRQ는 BEST-RQ보다 일관되게 성능을 개선하면서도 낮은 복잡성과 계산 효율성을 유지합니다.
- 5. 다양한 데이터셋에서 BiRQ의 성능을 검증하여 BEST-RQ 대비 일관된 성능 향상을 입증했습니다.


---

*Generated on 2025-09-23 11:28:22*