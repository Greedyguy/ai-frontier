---
keywords:
  - Visual Foundation Models
  - Range-view Segmentation
  - SemanticKITTI
  - Zero-Shot Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15886
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:13:56.694126",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Visual Foundation Models",
    "Range-view Segmentation",
    "SemanticKITTI",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Visual Foundation Models": 0.78,
    "Range-view Segmentation": 0.7,
    "SemanticKITTI": 0.75,
    "Zero-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Foundation Models",
        "canonical": "Visual Foundation Models",
        "aliases": [
          "VFMs"
        ],
        "category": "unique_technical",
        "rationale": "VFMs are central to the paper's approach and represent a novel application in LiDAR segmentation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.72,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Range-view segmentation",
        "canonical": "Range-view Segmentation",
        "aliases": [
          "Range-view LiDAR Segmentation"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific method explored in the paper, linking LiDAR data with segmentation techniques.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.7
      },
      {
        "surface": "SemanticKITTI",
        "canonical": "SemanticKITTI",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "SemanticKITTI is a benchmark dataset mentioned in the paper, crucial for evaluating segmentation methods.",
        "novelty_score": 0.45,
        "connectivity_score": 0.8,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "Zero-shot recognition",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot recognition"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot capabilities are relevant to the paper's exploration of VFMs and their applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Foundation Models",
      "resolved_canonical": "Visual Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.72,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Range-view segmentation",
      "resolved_canonical": "Range-view Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "SemanticKITTI",
      "resolved_canonical": "SemanticKITTI",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.8,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Zero-shot recognition",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation

**Korean Title:** RangeSAM: ì‹œê°ì  ê¸°ì´ˆ ëª¨ë¸ì„ í™œìš©í•œ ë²”ìœ„ ë·° ê¸°ë°˜ LiDAR ì„¸ë¶„í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15886.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15886](https://arxiv.org/abs/2509.15886)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/TASAM_ Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation_20250922|TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation]] (82.5% similar)
- [[2025-09-22/FloorSAM_ SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion_20250922|FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion]] (82.2% similar)
- [[2025-09-22/pFedSAM_ Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation_20250922|pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation]] (81.9% similar)
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (81.7% similar)
- [[2025-09-22/Spatial Understanding from Videos_ Structured Prompts Meet Simulation Data_20250922|Spatial Understanding from Videos: Structured Prompts Meet Simulation Data]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/SemanticKITTI|SemanticKITTI]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Visual Foundation Models|Visual Foundation Models]], [[keywords/Range-view Segmentation|Range-view Segmentation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15886v1 Announce Type: new 
Abstract: Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15886v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¶„í• ì€ ììœ¨ ì£¼í–‰ ë° 3D ì¥ë©´ ì´í•´ì— ìˆì–´ í•µì‹¬ì ì…ë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ë”¥ ì•„í‚¤í…ì²˜ì™€ì˜ í˜¸í™˜ì„± ë° ì„¸ë°€í•œ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ í¬ì°©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ ë•ë¶„ì— ë³µì…€ ë° í¬ì¸íŠ¸ ê¸°ë°˜ ë°©ë²•ì´ ì£¼ë¥¼ ì´ë£¨ê³  ìˆì§€ë§Œ, ì´ë“¤ì€ ì¢…ì¢… ë†’ì€ ê³„ì‚° ë¹„ìš©, ë¶ˆê·œì¹™í•œ ë©”ëª¨ë¦¬ ì ‘ê·¼, ì œí•œëœ ì‹¤ì‹œê°„ íš¨ìœ¨ì„±ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ë°˜ë©´, ìƒëŒ€ì ìœ¼ë¡œ ëœ íƒêµ¬ëœ ë²”ìœ„-ë·° ë°©ë²•ì€ ì„±ìˆ™í•œ 2D ì˜ë¯¸ ë¶„í•  ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë¹ ë¥´ê³  ì •í™•í•œ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìº¡ì…˜ ì‘ì„±, ì œë¡œìƒ· ì¸ì‹, ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì„ ìœ„í•œ ë¹„ì£¼ì–¼ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(VFM)ì˜ ê¸‰ì†í•œ ë°œì „ì— í˜ì…ì–´, ìš°ë¦¬ëŠ” í˜„ì¬ ë¶„í•  ì‘ì—…ì„ ìœ„í•œ ìµœì²¨ë‹¨ VFMì¸ SAM2ê°€ ë²”ìœ„ ë·°ì—ì„œ LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¶„í• ì„ ìœ„í•œ ê°•ë ¥í•œ ë°±ë³¸ ì—­í• ì„ í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” SAM2ë¥¼ 3D ë¶„í• ì— ì ì‘ì‹œí‚¤ëŠ” ìµœì´ˆì˜ ë²”ìœ„-ë·° í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•˜ë©°, íš¨ìœ¨ì ì¸ 2D íŠ¹ì§• ì¶”ì¶œì„ í‘œì¤€ íˆ¬ì˜/ì—­íˆ¬ì˜ê³¼ ê²°í•©í•˜ì—¬ í¬ì¸íŠ¸ í´ë¼ìš°ë“œì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. SAM2ë¥¼ ë²”ìœ„-ë·° í‘œí˜„ì— ìµœì í™”í•˜ê¸° ìœ„í•´ ì¸ì½”ë”ì— ëª‡ ê°€ì§€ êµ¬ì¡°ì  ìˆ˜ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤: (1) LiDAR ë²”ìœ„ ì´ë¯¸ì§€ì— ë‚´ì¬ëœ ìˆ˜í‰ ê³µê°„ ì¢…ì†ì„±ì„ ê°•ì¡°í•˜ëŠ” ìƒˆë¡œìš´ ëª¨ë“ˆ, (2) êµ¬í˜• íˆ¬ì˜ì˜ ê¸°í•˜í•™ì  íŠ¹ì„±ì— ë§ì¶˜ ë§ì¶¤í˜• êµ¬ì„±, (3) ë²”ìœ„-ë·° ì˜ì‚¬ ì´ë¯¸ì§€ì— ì¡´ì¬í•˜ëŠ” ë…íŠ¹í•œ ê³µê°„ íŒ¨í„´ê³¼ ë¶ˆì—°ì†ì„±ì„ í¬ì°©í•˜ë„ë¡ ì„¤ê³„ëœ ì¸ì½”ë” ë°±ë³¸ì˜ ì ì‘ëœ ë©”ì»¤ë‹ˆì¦˜. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ SemanticKITTIì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œ 2D ì¤‘ì‹¬ íŒŒì´í”„ë¼ì¸ì˜ ì†ë„, í™•ì¥ì„±, ë°°í¬ì˜ ë‹¨ìˆœì„±ì˜ ì´ì ì„ ëˆ„ë¦½ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” 3D ì¸ì‹ì„ ìœ„í•œ ë²”ìš© ë°±ë³¸ìœ¼ë¡œì„œì˜ VFMì˜ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•˜ë©°, í†µí•©ëœ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ê¸°ë°˜ LiDAR ë¶„í• ë¡œì˜ ê¸¸ì„ ì—½ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ VFMì„ ì‚¬ìš©í•˜ëŠ” ë²”ìœ„-ë·° ë¶„í•  ë°©ë²•ì´ ìœ ë§í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•œë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ììœ¨ì£¼í–‰ê³¼ 3D ì¥ë©´ ì´í•´ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¶„í• ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë³µì…€ ë° í¬ì¸íŠ¸ ê¸°ë°˜ ë°©ë²•ì€ ì„¸ë°€í•œ ê¸°í•˜í•™ì  ì •ë³´ë¥¼ í¬ì°©í•  ìˆ˜ ìˆì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ê³¼ ì‹¤ì‹œê°„ íš¨ìœ¨ì„±ì˜ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ì— ë¹„í•´, ë²”ìœ„ ë·° ë°©ë²•ì€ 2D ì˜ë¯¸ ë¶„í•  ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë¹ ë¥´ê³  ì •í™•í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ìµœì‹  ì‹œê°ì  ê¸°ì´ˆ ëª¨ë¸(VFM)ì¸ SAM2ë¥¼ LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¶„í• ì— ì ìš©í•˜ì—¬, íš¨ìœ¨ì ì¸ 2D íŠ¹ì§• ì¶”ì¶œê³¼ í‘œì¤€ íˆ¬ì˜/ì—­íˆ¬ì˜ì„ ê²°í•©í•œ ìµœì´ˆì˜ ë²”ìœ„ ë·° í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. SAM2ë¥¼ ë²”ìœ„ ë·° í‘œí˜„ì— ìµœì í™”í•˜ê¸° ìœ„í•´, ì¸ì½”ë”ì— ìˆ˜í‰ ê³µê°„ ì˜ì¡´ì„±ì„ ê°•ì¡°í•˜ëŠ” ëª¨ë“ˆ, êµ¬í˜• íˆ¬ì˜ì˜ ê¸°í•˜í•™ì  íŠ¹ì„±ì— ë§ì¶˜ ì„¤ì •, ë²”ìœ„ ë·° ì´ë¯¸ì§€ì˜ ë…íŠ¹í•œ ê³µê°„ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ SemanticKITTIì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°, 2D ì¤‘ì‹¬ íŒŒì´í”„ë¼ì¸ì˜ ì†ë„ì™€ í™•ì¥ì„±, ë°°í¬ì˜ ê°„í¸í•¨ì„ ì œê³µí•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, VFMì„ í™œìš©í•œ ë²”ìœ„ ë·° ë¶„í•  ë°©ë²•ì´ ìœ ë§í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì„¸ë¶„í™”ëŠ” ììœ¨ ì£¼í–‰ê³¼ 3D ì¥ë©´ ì´í•´ì— ì¤‘ìš”í•˜ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë†’ì€ ê³„ì‚° ë¹„ìš©ê³¼ ì‹¤ì‹œê°„ íš¨ìœ¨ì„±ì˜ í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
- 2. ë²”ìœ„ ë·° ë°©ë²•ì€ 2D ì˜ë¯¸ë¡ ì  ì„¸ë¶„í™” ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë¹ ë¥´ê³  ì •í™•í•œ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. SAM2ë¥¼ 3D ì„¸ë¶„í™”ì— ì ì‘ì‹œí‚¨ ìµœì´ˆì˜ ë²”ìœ„ ë·° í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•˜ë©°, íš¨ìœ¨ì ì¸ 2D íŠ¹ì§• ì¶”ì¶œê³¼ í‘œì¤€ íˆ¬ì˜/ì—­íˆ¬ì˜ì„ ê²°í•©í•˜ì—¬ í¬ì¸íŠ¸ í´ë¼ìš°ë“œì— ì ìš©í•©ë‹ˆë‹¤.
- 4. LiDAR ë²”ìœ„ ì´ë¯¸ì§€ì˜ ìˆ˜í‰ ê³µê°„ ì˜ì¡´ì„±ì„ ê°•ì¡°í•˜ëŠ” ëª¨ë“ˆ ë“± ëª‡ ê°€ì§€ êµ¬ì¡°ì  ìˆ˜ì •ì´ SAM2ì˜ ì¸ì½”ë”ì— êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. VFMsë¥¼ ì‚¬ìš©í•œ ë²”ìœ„ ë·° ì„¸ë¶„í™” ë°©ë²•ì€ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ë©°, 3D ì¸ì‹ì„ ìœ„í•œ ë²”ìš© ë°±ë³¸ìœ¼ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:13:56*