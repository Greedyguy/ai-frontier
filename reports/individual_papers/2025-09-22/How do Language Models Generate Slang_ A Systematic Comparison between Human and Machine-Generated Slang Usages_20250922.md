---
keywords:
  - Large Language Model
  - Slang Detection
  - Slang Interpretation
  - Machine-Generated Slang
  - Model Distillation
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15518
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:02:20.680369",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Slang Detection",
    "Slang Interpretation",
    "Machine-Generated Slang",
    "Model Distillation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Slang Detection": 0.78,
    "Slang Interpretation": 0.75,
    "Machine-Generated Slang": 0.72,
    "Model Distillation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's analysis, linking to a broad technical concept in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Slang Detection",
        "canonical": "Slang Detection",
        "aliases": [
          "Slang Identification"
        ],
        "category": "unique_technical",
        "rationale": "A specific task within NLP that the paper addresses, providing a unique link.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Slang Interpretation",
        "canonical": "Slang Interpretation",
        "aliases": [
          "Slang Understanding"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on understanding slang, a unique aspect of language processing discussed in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Machine-Generated Slang",
        "canonical": "Machine-Generated Slang",
        "aliases": [
          "AI-Generated Slang"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the comparison between human and machine outputs, a unique technical focus.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Model Distillation",
        "canonical": "Model Distillation",
        "aliases": [
          "Knowledge Distillation"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant for linking to methods of transferring knowledge within models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "slang usages",
      "human-attested"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Slang Detection",
      "resolved_canonical": "Slang Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Slang Interpretation",
      "resolved_canonical": "Slang Interpretation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Machine-Generated Slang",
      "resolved_canonical": "Machine-Generated Slang",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Model Distillation",
      "resolved_canonical": "Model Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages

**Korean Title:** ì–¸ì–´ ëª¨ë¸ì€ ì–´ë–»ê²Œ ì†ì–´ë¥¼ ìƒì„±í•˜ëŠ”ê°€: ì¸ê°„ê³¼ ê¸°ê³„ ìƒì„± ì†ì–´ ì‚¬ìš©ì˜ ì²´ê³„ì  ë¹„êµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15518.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15518](https://arxiv.org/abs/2509.15518)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (88.3% similar)
- [[2025-09-17/Do Large Language Models Understand Word Senses?_20250917|Do Large Language Models Understand Word Senses?]] (87.5% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (86.4% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (86.1% similar)
- [[2025-09-18/Catch Me If You Can? Not Yet_ LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors_20250918|Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Model Distillation|Model Distillation]]
**âš¡ Unique Technical**: [[keywords/Slang Detection|Slang Detection]], [[keywords/Slang Interpretation|Slang Interpretation]], [[keywords/Machine-Generated Slang|Machine-Generated Slang]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15518v1 Announce Type: cross 
Abstract: Slang is a commonly used type of informal language that poses a daunting challenge to NLP systems. Recent advances in large language models (LLMs), however, have made the problem more approachable. While LLM agents are becoming more widely applied to intermediary tasks such as slang detection and slang interpretation, their generalizability and reliability are heavily dependent on whether these models have captured structural knowledge about slang that align well with human attested slang usages. To answer this question, we contribute a systematic comparison between human and machine-generated slang usages. Our evaluative framework focuses on three core aspects: 1) Characteristics of the usages that reflect systematic biases in how machines perceive slang, 2) Creativity reflected by both lexical coinages and word reuses employed by the slang usages, and 3) Informativeness of the slang usages when used as gold-standard examples for model distillation. By comparing human-attested slang usages from the Online Slang Dictionary (OSD) and slang generated by GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our results suggest that while LLMs have captured significant knowledge about the creative aspects of slang, such knowledge does not align with humans sufficiently to enable LLMs for extrapolative tasks such as linguistic analyses.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15518v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì†ì–´ëŠ” ë¹„ê³µì‹ì ì¸ ì–¸ì–´ì˜ í•œ ìœ í˜•ìœ¼ë¡œ, ìì—°ì–´ ì²˜ë¦¬(NLP) ì‹œìŠ¤í…œì— ìƒë‹¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ì€ ì´ ë¬¸ì œë¥¼ ë³´ë‹¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. LLM ì—ì´ì „íŠ¸ê°€ ì†ì–´ ê°ì§€ ë° í•´ì„ê³¼ ê°™ì€ ì¤‘ê°„ ì‘ì—…ì— ì ì  ë” ë„ë¦¬ ì ìš©ë˜ê³  ìˆì§€ë§Œ, ì´ëŸ¬í•œ ëª¨ë¸ì´ ì¸ê°„ì´ ì…ì¦í•œ ì†ì–´ ì‚¬ìš©ê³¼ ì˜ ë§ëŠ” êµ¬ì¡°ì  ì§€ì‹ì„ í¬ì°©í–ˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ê·¸ ì¼ë°˜í™” ê°€ëŠ¥ì„±ê³¼ ì‹ ë¢°ì„±ì´ í¬ê²Œ ì¢Œìš°ë©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì¸ê°„ê³¼ ê¸°ê³„ ìƒì„± ì†ì–´ ì‚¬ìš© ê°„ì˜ ì²´ê³„ì ì¸ ë¹„êµë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í‰ê°€ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ í•µì‹¬ ì¸¡ë©´ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤: 1) ê¸°ê³„ê°€ ì†ì–´ë¥¼ ì¸ì‹í•˜ëŠ” ë°©ì‹ì—ì„œ ì²´ê³„ì ì¸ í¸í–¥ì„ ë°˜ì˜í•˜ëŠ” ì‚¬ìš©ì˜ íŠ¹ì„±, 2) ì†ì–´ ì‚¬ìš©ì— ì˜í•´ ì‚¬ìš©ëœ ì–´íœ˜ì  ì‹ ì¡°ì–´ì™€ ë‹¨ì–´ ì¬ì‚¬ìš©ì— ì˜í•´ ë°˜ì˜ëœ ì°½ì˜ì„±, 3) ëª¨ë¸ ì¦ë¥˜ë¥¼ ìœ„í•œ ê¸ˆí‘œì¤€ ì˜ˆë¡œ ì‚¬ìš©ë  ë•Œ ì†ì–´ ì‚¬ìš©ì˜ ì •ë³´ì„±. ì˜¨ë¼ì¸ ì†ì–´ ì‚¬ì „(OSD)ì—ì„œ ì¸ê°„ì´ ì…ì¦í•œ ì†ì–´ ì‚¬ìš©ê³¼ GPT-4o ë° Llama-3ì— ì˜í•´ ìƒì„±ëœ ì†ì–´ë¥¼ ë¹„êµí•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” LLMì´ ì†ì–´ë¥¼ ì¸ì‹í•˜ëŠ” ë°©ì‹ì—ì„œ ìƒë‹¹í•œ í¸í–¥ì„ ë°œê²¬í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” LLMì´ ì†ì–´ì˜ ì°½ì˜ì  ì¸¡ë©´ì— ëŒ€í•œ ìƒë‹¹í•œ ì§€ì‹ì„ í¬ì°©í–ˆì§€ë§Œ, ê·¸ëŸ¬í•œ ì§€ì‹ì´ ì–¸ì–´ ë¶„ì„ê³¼ ê°™ì€ ì™¸ì‚½ì  ì‘ì—…ì„ ê°€ëŠ¥í•˜ê²Œ í•  ë§Œí¼ ì¸ê°„ê³¼ ì¶©ë¶„íˆ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë¹„ê³µì‹ ì–¸ì–´ì¸ ì†ì–´ë¥¼ ì–´ë–»ê²Œ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ”ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì¸ê°„ê³¼ ê¸°ê³„ê°€ ìƒì„±í•œ ì†ì–´ ì‚¬ìš©ì„ ë¹„êµí•˜ì—¬ LLMì˜ ì¼ë°˜í™” ë° ì‹ ë¢°ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” LLMì´ ì†ì–´ì˜ ì°½ì˜ì  ì¸¡ë©´ì„ ì–´ëŠ ì •ë„ ì´í•´í•˜ì§€ë§Œ, ì¸ê°„ì˜ ì†ì–´ ì‚¬ìš©ê³¼ ì¶©ë¶„íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ ì–¸ì–´ ë¶„ì„ê³¼ ê°™ì€ í™•ì¥ì  ì‘ì—…ì—ëŠ” í•œê³„ê°€ ìˆìŒì„ ë°œê²¬í•œ ê²ƒì…ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì†ì–´ì˜ ì²´ê³„ì  í¸í–¥, ì°½ì˜ì„±, ì •ë³´ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë©°, LLMì´ ì†ì–´ì˜ ì°½ì˜ì„±ì„ ì–´ëŠ ì •ë„ í¬ì°©í–ˆìœ¼ë‚˜ ì¸ê°„ê³¼ì˜ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ í•œê³„ê°€ ì¡´ì¬í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì†ì–´ëŠ” NLP ì‹œìŠ¤í…œì— ë„ì „ì ì¸ ê³¼ì œë¥¼ ì œì‹œí•˜ì§€ë§Œ, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ìœ¼ë¡œ ë¬¸ì œ í•´ê²°ì´ ë” ì ‘ê·¼ ê°€ëŠ¥í•´ì¡Œë‹¤.
- 2. LLM ì—ì´ì „íŠ¸ëŠ” ì†ì–´ íƒì§€ ë° í•´ì„ê³¼ ê°™ì€ ì¤‘ê°„ ì‘ì—…ì— ë„ë¦¬ ì ìš©ë˜ê³  ìˆì§€ë§Œ, ì´ë“¤ì˜ ì¼ë°˜í™” ë° ì‹ ë¢°ì„±ì€ ì¸ê°„ì˜ ì†ì–´ ì‚¬ìš©ê³¼ ì˜ ë§ëŠ” êµ¬ì¡°ì  ì§€ì‹ì„ í¬ì°©í–ˆëŠ”ì§€ì— í¬ê²Œ ì˜ì¡´í•œë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” ì¸ê°„ê³¼ ê¸°ê³„ê°€ ìƒì„±í•œ ì†ì–´ ì‚¬ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•˜ì—¬, ê¸°ê³„ê°€ ì†ì–´ë¥¼ ì¸ì‹í•˜ëŠ” ë° ìˆì–´ ì²´ê³„ì ì¸ í¸í–¥ì„ ë“œëŸ¬ë‚´ëŠ” ì‚¬ìš©ì˜ íŠ¹ì„±, ì†ì–´ ì‚¬ìš©ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ì°½ì˜ì„±, ê·¸ë¦¬ê³  ëª¨ë¸ ì¦ë¥˜ë¥¼ ìœ„í•œ ê¸ˆí‘œì¤€ ì˜ˆë¡œì„œì˜ ì •ë³´ì„±ì„ í‰ê°€í•œë‹¤.
- 4. ì—°êµ¬ ê²°ê³¼, LLMì€ ì†ì–´ì˜ ì°½ì˜ì  ì¸¡ë©´ì— ëŒ€í•œ ìƒë‹¹í•œ ì§€ì‹ì„ í¬ì°©í–ˆì§€ë§Œ, ì¸ê°„ê³¼ ì¶©ë¶„íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ ì–¸ì–´ ë¶„ì„ê³¼ ê°™ì€ ì™¸ì‚½ì  ì‘ì—…ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŒì„ ì‹œì‚¬í•œë‹¤.


---

*Generated on 2025-09-23 09:02:20*