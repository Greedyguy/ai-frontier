---
keywords:
  - Deep Learning
  - Graph Neural Network
  - Zero-Shot Learning
  - Relational Inductive Bias
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.16151
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:44:22.883951",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Graph Neural Network",
    "Zero-Shot Learning",
    "Relational Inductive Bias"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.78,
    "Graph Neural Network": 0.85,
    "Zero-Shot Learning": 0.82,
    "Relational Inductive Bias": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep reinforcement learning",
        "canonical": "Deep Learning",
        "aliases": [
          "Deep RL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is a foundational concept that connects to various machine learning techniques, including reinforcement learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Graph-based Reinforcement Learning",
        "canonical": "Graph Neural Network",
        "aliases": [
          "Graph RL"
        ],
        "category": "specific_connectable",
        "rationale": "Graph Neural Networks are crucial for understanding the graph-based approach in reinforcement learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Zero-shot adapt",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot adaptation"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept that enhances the adaptability of models to unseen data.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Relational inductive bias",
        "canonical": "Relational Inductive Bias",
        "aliases": [
          "Relational bias"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique concept that underpins the reasoning capabilities of agents in graph-based environments.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Automated Cyber Defense",
      "Partially Observable Markov Decision Problem"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep reinforcement learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Graph-based Reinforcement Learning",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Zero-shot adapt",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Relational inductive bias",
      "resolved_canonical": "Relational Inductive Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents

**Korean Title:** ì¼ë°˜í™” ê°€ëŠ¥í•œ ê·¸ë˜í”„ ê¸°ë°˜ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16151.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.16151](https://arxiv.org/abs/2509.16151)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (83.1% similar)
- [[2025-09-22/Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem_20250922|Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem]] (82.7% similar)
- [[2025-09-18/$Agent^2$_ An Agent-Generates-Agent Framework for Reinforcement Learning Automation_20250918|$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation]] (82.1% similar)
- [[2025-09-19/A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks_20250919|A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks]] (81.3% similar)
- [[2025-09-22/Revealing Human Internal Attention Patterns from Gameplay Analysis for Reinforcement Learning_20250922|Revealing Human Internal Attention Patterns from Gameplay Analysis for Reinforcement Learning]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Relational Inductive Bias|Relational Inductive Bias]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16151v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for automated cyber defense (ACD). The traditional RL approach represents networks as a list of computers in various states of safety or threat. Unfortunately, these models are forced to overfit to specific network topologies, rendering them ineffective when faced with even small environmental perturbations. In this work, we frame ACD as a two-player context-based partially observable Markov decision problem with observations represented as attributed graphs. This approach allows our agents to reason through the lens of relational inductive bias. Agents learn how to reason about hosts interacting with other system entities in a more general manner, and their actions are understood as edits to the graph representing the environment. By introducing this bias, we will show that our agents can better reason about the states of networks and zero-shot adapt to new ones. We show that this approach outperforms the state-of-the-art by a wide margin, and makes our agents capable of defending never-before-seen networks against a wide range of adversaries in a variety of complex, and multi-agent environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16151v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì€ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´(ACD)ë¥¼ ìœ„í•œ ì‹¤í˜„ ê°€ëŠ¥í•œ ì „ëµìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì „í†µì ì¸ RL ì ‘ê·¼ ë°©ì‹ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ì–‘í•œ ì•ˆì „ ë˜ëŠ” ìœ„í˜‘ ìƒíƒœì— ìˆëŠ” ì»´í“¨í„° ëª©ë¡ìœ¼ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë¶ˆí–‰íˆë„ ì´ëŸ¬í•œ ëª¨ë¸ì€ íŠ¹ì • ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ì— ê³¼ì í•©ë˜ë„ë¡ ê°•ìš”ë˜ì–´, í™˜ê²½ì˜ ì‘ì€ ë³€í™”ì—ë„ íš¨ê³¼ì ì´ì§€ ëª»í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ACDë¥¼ ì†ì„± ê·¸ë˜í”„ë¡œ í‘œí˜„ëœ ê´€ì¸¡ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‘ í”Œë ˆì´ì–´ ê¸°ë°˜ì˜ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ë¬¸ì œë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì—ì´ì „íŠ¸ê°€ ê´€ê³„ì  ê·€ë‚© í¸í–¥ì˜ ê´€ì ì—ì„œ ì¶”ë¡ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ì‹œìŠ¤í…œì˜ ë‹¤ë¥¸ ì—”í‹°í‹°ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” í˜¸ìŠ¤íŠ¸ì— ëŒ€í•´ ë³´ë‹¤ ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ë©°, ê·¸ë“¤ì˜ í–‰ë™ì€ í™˜ê²½ì„ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ì— ëŒ€í•œ ìˆ˜ì •ìœ¼ë¡œ ì´í•´ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í¸í–¥ì„ ë„ì…í•¨ìœ¼ë¡œì¨, ì—ì´ì „íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ëŒ€í•´ ë” ì˜ ì¶”ë¡ í•˜ê³  ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ ì œë¡œìƒ· ì ì‘í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤„ ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì ‘ê·¼ ë°©ì‹ì´ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, ë‹¤ì–‘í•œ ë³µì¡í•˜ê³  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ ì ëŒ€ìì— ë§ì„œ ì´ì „ì— ë³¸ ì  ì—†ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ë°©ì–´í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì„ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´(ACD)ì— ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ RL ëª¨ë¸ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ íŠ¹ì • êµ¬ì¡°ë¡œ ê³ ì •í•˜ì—¬ í™˜ê²½ ë³€í™”ì— ì·¨ì•½í•œ ë°˜ë©´, ë³¸ ì—°êµ¬ëŠ” ACDë¥¼ ë§¥ë½ ê¸°ë°˜ì˜ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ë¬¸ì œë¡œ ì„¤ì •í•˜ê³ , ê´€ì°°ì„ ì†ì„± ê·¸ë˜í”„ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ê´€ê³„ì  ê·€ë‚© í¸í–¥ì„ í™œìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ ì¼ë°˜í™”í•˜ê³ , ìƒˆë¡œìš´ í™˜ê²½ì— ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ìµœì‹  ê¸°ìˆ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¤ì–‘í•œ ë³µì¡í•œ í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë°©ì–´í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¬ì¸µ ê°•í™” í•™ìŠµì€ ìë™í™”ëœ ì‚¬ì´ë²„ ë°©ì–´ë¥¼ ìœ„í•œ ìœ ë§í•œ ì „ëµìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ ëª¨ë¸ì€ íŠ¹ì • ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ì— ê³¼ì í•©ë˜ì–´ í™˜ê²½ ë³€í™”ì— íš¨ê³¼ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” ACDë¥¼ ì†ì„± ê·¸ë˜í”„ë¡œ í‘œí˜„ëœ ì´ì¤‘ í”Œë ˆì´ì–´ ë¬¸ë§¥ ê¸°ë°˜ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ë¬¸ì œë¡œ ì •ì˜í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ì ‘ê·¼ë²•ì€ ì—ì´ì „íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ ë” ì˜ ì´í•´í•˜ê³  ìƒˆë¡œìš´ í™˜ê²½ì— ì ì‘í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- 5. ì´ ë°©ë²•ì€ ìµœì‹  ê¸°ìˆ ì„ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, ë‹¤ì–‘í•œ ë³µì¡í•œ í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°©ì–´í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:44:22*