---
keywords:
  - Large Language Model
  - Natural Language Explanations
  - Uncertainty Quantification
  - Question Answering
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15403
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:48:31.159066",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Natural Language Explanations",
    "Uncertainty Quantification",
    "Question Answering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Natural Language Explanations": 0.78,
    "Uncertainty Quantification": 0.8,
    "Question Answering": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large-scale language models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus, linking to existing research on LLMs and their applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Natural Language Explanations",
        "canonical": "Natural Language Explanations",
        "aliases": [
          "NLE",
          "language-based explanations"
        ],
        "category": "unique_technical",
        "rationale": "Key concept introduced for explaining LLM behavior, offering a unique perspective in NLP.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Uncertainty Quantification",
        "canonical": "Uncertainty Quantification",
        "aliases": [
          "uncertainty estimation",
          "confidence assessment"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding model reliability, connecting to broader discussions in AI safety.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Question Answering",
        "canonical": "Question Answering",
        "aliases": [
          "QA",
          "question-response tasks"
        ],
        "category": "specific_connectable",
        "rationale": "A primary application domain for LLMs, facilitating connections to various NLP tasks.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Natural Language Explanations",
      "resolved_canonical": "Natural Language Explanations",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Uncertainty Quantification",
      "resolved_canonical": "Uncertainty Quantification",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Question Answering",
      "resolved_canonical": "Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì§ˆë¬¸ ì‘ë‹µì— ëŒ€í•œ ìì—°ì–´ ì„¤ëª…ì—ì„œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15403.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15403](https://arxiv.org/abs/2509.15403)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (85.3% similar)
- [[2025-09-22/Efficient Real-time Refinement of Language Model Text Generation_20250922|Efficient Real-time Refinement of Language Model Text Generation]] (84.0% similar)
- [[2025-09-22/Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data_20250922|Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data]] (83.5% similar)
- [[2025-09-22/Knowledge-Driven Hallucination in Large Language Models_ An Empirical Study on Process Modeling_20250922|Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling]] (83.5% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Uncertainty Quantification|Uncertainty Quantification]], [[keywords/Question Answering|Question Answering]]
**âš¡ Unique Technical**: [[keywords/Natural Language Explanations|Natural Language Explanations]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15403v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong capabilities, enabling concise, context-aware answers in question answering (QA) tasks. The lack of transparency in complex LLMs has inspired extensive research aimed at developing methods to explain large language behaviors. Among existing explanation methods, natural language explanations stand out due to their ability to explain LLMs in a self-explanatory manner and enable the understanding of model behaviors even when the models are closed-source. However, despite these promising advancements, there is no existing work studying how to provide valid uncertainty guarantees for these generated natural language explanations. Such uncertainty quantification is critical in understanding the confidence behind these explanations. Notably, generating valid uncertainty estimates for natural language explanations is particularly challenging due to the auto-regressive generation process of LLMs and the presence of noise in medical inquiries. To bridge this gap, in this work, we first propose a novel uncertainty estimation framework for these generated natural language explanations, which provides valid uncertainty guarantees in a post-hoc and model-agnostic manner. Additionally, we also design a novel robust uncertainty estimation method that maintains valid uncertainty guarantees even under noise. Extensive experiments on QA tasks demonstrate the desired performance of our methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15403v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì§ˆë¬¸ ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ê°„ê²°í•˜ê³  ë§¥ë½ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë³µì¡í•œ LLMì˜ íˆ¬ëª…ì„± ë¶€ì¡±ì€ ëŒ€í˜• ì–¸ì–´ í–‰ë™ì„ ì„¤ëª…í•˜ëŠ” ë°©ë²•ì„ ê°œë°œí•˜ê¸° ìœ„í•œ ê´‘ë²”ìœ„í•œ ì—°êµ¬ë¥¼ ì´‰ë°œí–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì„¤ëª… ë°©ë²• ì¤‘ì—ì„œ ìì—°ì–´ ì„¤ëª…ì€ LLMì„ ìê¸° ì„¤ëª… ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ëª¨ë¸ì´ ë¹„ê³µê°œ ì†ŒìŠ¤ì¼ ë•Œì—ë„ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤ëŠ” ì ì—ì„œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ìœ ë§í•œ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ , ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•´ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•œ ê¸°ì¡´ ì—°êµ¬ëŠ” ì—†ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ëŠ” ì´ëŸ¬í•œ ì„¤ëª… ë’¤ì— ìˆëŠ” ì‹ ë¢°ë„ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. íŠ¹íˆ, ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ LLMì˜ ìë™ íšŒê·€ ìƒì„± ê³¼ì •ê³¼ ì˜ë£Œ ë¬¸ì˜ì—ì„œì˜ ë…¸ì´ì¦ˆ ì¡´ì¬ë¡œ ì¸í•´ íŠ¹íˆ ì–´ë µìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¨¼ì € ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìƒˆë¡œìš´ ë¶ˆí™•ì‹¤ì„± ì¶”ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ë©°, ì´ëŠ” ì‚¬í›„ì ì´ê³  ëª¨ë¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ë°©ì‹ìœ¼ë¡œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ, ë…¸ì´ì¦ˆê°€ ìˆëŠ” ìƒí™©ì—ì„œë„ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ìœ ì§€í•˜ëŠ” ìƒˆë¡œìš´ ê²¬ê³ í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. QA ì‘ì—…ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ë°©ë²•ì´ ì›í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìì—°ì–´ ì„¤ëª…ì€ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•˜ëŠ” ë° ìœ ìš©í•˜ì§€ë§Œ, ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì´ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ì‚¬í›„ì ì´ê³  ëª¨ë¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ë¶ˆí™•ì‹¤ì„± ì¶”ì • í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì¡ìŒì´ ìˆëŠ” ìƒí™©ì—ì„œë„ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ìœ ì§€í•˜ëŠ” ê°•ë ¥í•œ ì¶”ì • ë°©ë²•ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ QA ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì§ˆë¬¸ ì‘ë‹µ(QA) ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ê°„ê²°í•˜ê³  ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. ìì—°ì–´ ì„¤ëª…ì€ LLMì˜ í–‰ë™ì„ ìê°€ ì„¤ëª… ë°©ì‹ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆê²Œ í•˜ë©°, ëª¨ë¸ì´ ë¹„ê³µê°œ ì†ŒìŠ¤ì¼ ë•Œë„ ìœ ìš©í•©ë‹ˆë‹¤.
- 3. í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•˜ëŠ” ì—°êµ¬ëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤.
- 4. ë³¸ ì—°êµ¬ëŠ” ìƒì„±ëœ ìì—°ì–´ ì„¤ëª…ì— ëŒ€í•œ ìƒˆë¡œìš´ ë¶ˆí™•ì‹¤ì„± ì¶”ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬, ì‚¬í›„ì ì´ê³  ëª¨ë¸ ë¹„ì¢…ì†ì ì¸ ë°©ì‹ìœ¼ë¡œ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ìƒí™©ì—ì„œë„ ìœ íš¨í•œ ë¶ˆí™•ì‹¤ì„± ë³´ì¥ì„ ìœ ì§€í•˜ëŠ” ê°•ë ¥í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:48:31*