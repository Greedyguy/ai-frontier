---
keywords:
  - Hierarchical Reinforcement Learning
  - Model Predictive Control
  - Multi-Agent Systems
  - Structured Regions of Interest
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15799
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:13:54.990414",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hierarchical Reinforcement Learning",
    "Model Predictive Control",
    "Multi-Agent Systems",
    "Structured Regions of Interest"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hierarchical Reinforcement Learning": 0.78,
    "Model Predictive Control": 0.82,
    "Multi-Agent Systems": 0.8,
    "Structured Regions of Interest": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Hierarchical Reinforcement Learning",
        "canonical": "Hierarchical Reinforcement Learning",
        "aliases": [
          "HRL"
        ],
        "category": "unique_technical",
        "rationale": "Hierarchical Reinforcement Learning is a distinct approach that enhances connectivity by bridging high-level decision-making with low-level execution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Model Predictive Control",
        "canonical": "Model Predictive Control",
        "aliases": [
          "MPC"
        ],
        "category": "specific_connectable",
        "rationale": "Model Predictive Control is a well-established technique that ensures safe and feasible motion, connecting structured learning with control systems.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multi-Agent Systems",
        "canonical": "Multi-Agent Systems",
        "aliases": [
          "MAS"
        ],
        "category": "specific_connectable",
        "rationale": "Multi-Agent Systems are crucial for understanding interactions in complex environments, enhancing connectivity with other multi-agent frameworks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Structured Regions of Interest",
        "canonical": "Structured Regions of Interest",
        "aliases": [
          "ROIs"
        ],
        "category": "unique_technical",
        "rationale": "Structured Regions of Interest are key to high-level policy selection, providing a unique perspective on spatial decision-making.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "end-to-end learning",
      "sample efficiency",
      "reward",
      "safety",
      "consistency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Hierarchical Reinforcement Learning",
      "resolved_canonical": "Hierarchical Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Model Predictive Control",
      "resolved_canonical": "Model Predictive Control",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multi-Agent Systems",
      "resolved_canonical": "Multi-Agent Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Structured Regions of Interest",
      "resolved_canonical": "Structured Regions of Interest",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control

**Korean Title:** ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì œì–´ë¥¼ ìœ„í•œ ì €ìˆ˜ì¤€ MPCë¥¼ í™œìš©í•œ ê³„ì¸µì  ê°•í™” í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15799.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15799](https://arxiv.org/abs/2509.15799)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems_20250922|Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems]] (84.9% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (84.7% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (84.6% similar)
- [[2025-09-19/Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control_20250919|Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control]] (84.3% similar)
- [[2025-09-19/ActivePusher_ Active Learning and Planning with Residual Physics for Nonprehensile Manipulation_20250919|ActivePusher: Active Learning and Planning with Residual Physics for Nonprehensile Manipulation]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Model Predictive Control|Model Predictive Control]], [[keywords/Multi-Agent Systems|Multi-Agent Systems]]
**âš¡ Unique Technical**: [[keywords/Hierarchical Reinforcement Learning|Hierarchical Reinforcement Learning]], [[keywords/Structured Regions of Interest|Structured Regions of Interest]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15799v1 Announce Type: cross 
Abstract: Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15799v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë™ì ì´ê³  ì œì•½ì´ ë§ì€ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê³  ì¡°ì •ëœ í–‰ë™ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ í•™ìŠµ ê¸°ë°˜ ì œì–´ì— ìˆì–´ ì—¬ì „íˆ ì£¼ìš” ê³¼ì œì…ë‹ˆë‹¤. ìˆœìˆ˜í•œ ì¢…ë‹¨ ê°„ í•™ìŠµì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ë‚®ê³  ì‹ ë¢°ì„±ì´ ì œí•œì ì´ë©°, ëª¨ë¸ ê¸°ë°˜ ë°©ë²•ì€ ì‚¬ì „ì— ì •ì˜ëœ ì°¸ì¡°ì— ì˜ì¡´í•˜ê³  ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê°•í™” í•™ìŠµ(RL)ì„ í†µí•œ ì „ìˆ ì  ì˜ì‚¬ ê²°ì •ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ì œì–´(MPC)ë¥¼ í†µí•œ ì €ìˆ˜ì¤€ ì‹¤í–‰ì„ ê²°í•©í•œ ê³„ì¸µì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ê²½ìš°, ì´ëŠ” ê³ ìˆ˜ì¤€ ì •ì±…ì´ ê´€ì‹¬ ì˜ì—­(ROIs)ì˜ êµ¬ì¡°í™”ëœ ì˜ì—­ì—ì„œ ì¶”ìƒì ì¸ ëª©í‘œë¥¼ ì„ íƒí•˜ëŠ” ë°˜ë©´, MPCëŠ” ë™ì ìœ¼ë¡œ ì‹¤í˜„ ê°€ëŠ¥í•˜ê³  ì•ˆì „í•œ ì›€ì§ì„ì„ ë³´ì¥í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í¬ì‹ì-í”¼ì‹ì ë²¤ì¹˜ë§ˆí¬ì—ì„œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ë³´ìƒ, ì•ˆì „ì„± ë° ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ì¢…ë‹¨ ê°„ ë° ì°¨í ê¸°ë°˜ RL ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ì—¬ êµ¬ì¡°í™”ëœ í•™ìŠµê³¼ ëª¨ë¸ ê¸°ë°˜ ì œì–´ë¥¼ ê²°í•©í•œ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë™ì ì´ê³  ì œì•½ì´ ë§ì€ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê³  ì¡°ì •ëœ í–‰ë™ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•™ìŠµ ê¸°ë°˜ ì œì–´ì˜ ë„ì „ ê³¼ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì €ìë“¤ì€ ê°•í™”í•™ìŠµ(RL)ì„ í†µí•œ ì „ìˆ ì  ì˜ì‚¬ê²°ì •ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ì œì–´(MPC)ë¥¼ í†µí•œ ì €ìˆ˜ì¤€ ì‹¤í–‰ì„ ê²°í•©í•œ ê³„ì¸µì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ê²½ìš°, ê³ ìˆ˜ì¤€ ì •ì±…ì€ êµ¬ì¡°í™”ëœ ê´€ì‹¬ ì˜ì—­(ROIs)ì—ì„œ ì¶”ìƒì ì¸ ëª©í‘œë¥¼ ì„ íƒí•˜ê³ , MPCëŠ” ë™ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•ˆì „í•œ ì›€ì§ì„ì„ ë³´ì¥í•©ë‹ˆë‹¤. í¬ì‹ì-í”¼ì‹ì ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì´ ì ‘ê·¼ë²•ì€ ë³´ìƒ, ì•ˆì „ì„±, ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ê¸°ì¡´ì˜ RL ê¸°ë°˜ ë°©ë²•ì„ ëŠ¥ê°€í•˜ë©°, êµ¬ì¡°í™”ëœ í•™ìŠµê³¼ ëª¨ë¸ ê¸°ë°˜ ì œì–´ì˜ ê²°í•©ì´ ê°€ì ¸ì˜¤ëŠ” ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í•™ìŠµ ê¸°ë°˜ ì œì–´ì—ì„œ ì•ˆì „í•˜ê³  ì¡°ì •ëœ í–‰ë™ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ì£¼ìš” ê³¼ì œì…ë‹ˆë‹¤.
- 2. ìˆœìˆ˜í•œ ì¢…ë‹¨ ê°„ í•™ìŠµì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ì‹ ë¢°ì„±ì´ ë¶€ì¡±í•œ ë°˜ë©´, ëª¨ë¸ ê¸°ë°˜ ë°©ë²•ì€ ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
- 3. ê°•í™” í•™ìŠµì„ í†µí•œ ì „ìˆ ì  ì˜ì‚¬ ê²°ì •ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ì œì–´ë¥¼ í†µí•œ ì €ìˆ˜ì¤€ ì‹¤í–‰ì„ ê²°í•©í•œ ê³„ì¸µì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œëŠ” ê³ ìˆ˜ì¤€ ì •ì±…ì´ ê´€ì‹¬ ì˜ì—­ì—ì„œ ì¶”ìƒì  ëª©í‘œë¥¼ ì„ íƒí•˜ê³ , MPCê°€ ë™ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•ˆì „í•œ ì›€ì§ì„ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 5. í¬ì‹ì-í”¼ì‹ì ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ ë³´ìƒ, ì•ˆì „ì„±, ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:13:54*