---
keywords:
  - 3D Geometry-Aware Policy
  - Multi-View Images
  - Spatial Encoder
  - Language Instructions
  - Sensor-Agnostic Solution
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15733
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:11:48.877727",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "3D Geometry-Aware Policy",
    "Multi-View Images",
    "Spatial Encoder",
    "Language Instructions",
    "Sensor-Agnostic Solution"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "3D Geometry-Aware Policy": 0.78,
    "Multi-View Images": 0.8,
    "Spatial Encoder": 0.75,
    "Language Instructions": 0.77,
    "Sensor-Agnostic Solution": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "3D Geometry-Aware Policy",
        "canonical": "3D Geometry-Aware Policy",
        "aliases": [
          "3D Geometry Policy"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's contribution and represents a novel approach to robotic manipulation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multi-View Images",
        "canonical": "Multi-View Images",
        "aliases": [
          "Multi-View Observations"
        ],
        "category": "specific_connectable",
        "rationale": "Multi-view images are crucial for understanding 3D geometry, linking to broader topics in computer vision.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spatial Encoder",
        "canonical": "Spatial Encoder",
        "aliases": [
          "Spatial Feature Encoder"
        ],
        "category": "unique_technical",
        "rationale": "The spatial encoder is a key component of the proposed method, enabling depth and camera parameter estimation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Language Instructions",
        "canonical": "Language Instructions",
        "aliases": [
          "Natural Language Instructions"
        ],
        "category": "specific_connectable",
        "rationale": "Integrating language instructions with visual data is a growing area in multimodal learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "Sensor-Agnostic Solution",
        "canonical": "Sensor-Agnostic Solution",
        "aliases": [
          "Sensor-Free Approach"
        ],
        "category": "unique_technical",
        "rationale": "This highlights the method's adaptability to various environments without specific sensors.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "robotic manipulation",
      "depth sensors",
      "real-world robots"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "3D Geometry-Aware Policy",
      "resolved_canonical": "3D Geometry-Aware Policy",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multi-View Images",
      "resolved_canonical": "Multi-View Images",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spatial Encoder",
      "resolved_canonical": "Spatial Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Language Instructions",
      "resolved_canonical": "Language Instructions",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Sensor-Agnostic Solution",
      "resolved_canonical": "Sensor-Agnostic Solution",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation

**Korean Title:** GP3: ë¡œë´‡ ì¡°ì‘ì„ ìœ„í•œ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ë¥¼ í™œìš©í•œ 3D ê¸°í•˜í•™ ì¸ì‹ ì •ì±…

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15733.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15733](https://arxiv.org/abs/2509.15733)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/GAF_ Gaussian Action Field as a Dynamic World Model for Robotic Manipulation_20250919|GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation]] (85.0% similar)
- [[2025-09-17/Prompt2Auto_ From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning_20250917|Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning]] (84.6% similar)
- [[2025-09-18/PRISM-DP_ Spatial Pose-based Observations for Diffusion-Policies via Segmentation, Mesh Generation, and Pose Tracking_20250918|PRISM-DP: Spatial Pose-based Observations for Diffusion-Policies via Segmentation, Mesh Generation, and Pose Tracking]] (84.2% similar)
- [[2025-09-18/Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning_20250918|Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning]] (84.0% similar)
- [[2025-09-19/Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiDAR-Camera Fusion_20250919|Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiDAR-Camera Fusion]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multi-View Images|Multi-View Images]], [[keywords/Language Instructions|Language Instructions]]
**âš¡ Unique Technical**: [[keywords/3D Geometry-Aware Policy|3D Geometry-Aware Policy]], [[keywords/Spatial Encoder|Spatial Encoder]], [[keywords/Sensor-Agnostic Solution|Sensor-Agnostic Solution]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15733v1 Announce Type: cross 
Abstract: Effective robotic manipulation relies on a precise understanding of 3D scene geometry, and one of the most straightforward ways to acquire such geometry is through multi-view observations. Motivated by this, we present GP3 -- a 3D geometry-aware robotic manipulation policy that leverages multi-view input. GP3 employs a spatial encoder to infer dense spatial features from RGB observations, which enable the estimation of depth and camera parameters, leading to a compact yet expressive 3D scene representation tailored for manipulation. This representation is fused with language instructions and translated into continuous actions via a lightweight policy head. Comprehensive experiments demonstrate that GP3 consistently outperforms state-of-the-art methods on simulated benchmarks. Furthermore, GP3 transfers effectively to real-world robots without depth sensors or pre-mapped environments, requiring only minimal fine-tuning. These results highlight GP3 as a practical, sensor-agnostic solution for geometry-aware robotic manipulation.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15733v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: íš¨ê³¼ì ì¸ ë¡œë´‡ ì¡°ì‘ì€ 3D ì¥ë©´ ê¸°í•˜í•™ì— ëŒ€í•œ ì •í™•í•œ ì´í•´ì— ì˜ì¡´í•˜ë©°, ì´ëŸ¬í•œ ê¸°í•˜í•™ì„ íšë“í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” ë‹¤ì¤‘ ì‹œì  ê´€ì°°ì„ í†µí•´ì„œì…ë‹ˆë‹¤. ì´ì— ì˜ê°ì„ ë°›ì•„, ìš°ë¦¬ëŠ” ë‹¤ì¤‘ ì‹œì  ì…ë ¥ì„ í™œìš©í•˜ëŠ” 3D ê¸°í•˜í•™ ì¸ì‹ ë¡œë´‡ ì¡°ì‘ ì •ì±…ì¸ GP3ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. GP3ëŠ” RGB ê´€ì°°ë¡œë¶€í„° ë°€ì§‘ ê³µê°„ íŠ¹ì§•ì„ ì¶”ë¡ í•˜ê¸° ìœ„í•´ ê³µê°„ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ë©°, ì´ë¥¼ í†µí•´ ê¹Šì´ ë° ì¹´ë©”ë¼ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ì—¬ ì¡°ì‘ì— ë§ì¶˜ ê°„ê²°í•˜ë©´ì„œë„ í‘œí˜„ë ¥ ìˆëŠ” 3D ì¥ë©´ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ í‘œí˜„ì€ ì–¸ì–´ ì§€ì‹œì™€ ê²°í•©ë˜ì–´ ê²½ëŸ‰ ì •ì±… í—¤ë“œë¥¼ í†µí•´ ì—°ì†ì ì¸ í–‰ë™ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. í¬ê´„ì ì¸ ì‹¤í—˜ ê²°ê³¼, GP3ëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ì„ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ìš±ì´, GP3ëŠ” ê¹Šì´ ì„¼ì„œë‚˜ ì‚¬ì „ ë§¤í•‘ëœ í™˜ê²½ ì—†ì´ë„ ì‹¤ì œ ë¡œë´‡ì— íš¨ê³¼ì ìœ¼ë¡œ ì „ì´ë˜ë©°, ìµœì†Œí•œì˜ ë¯¸ì„¸ ì¡°ì •ë§Œì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” GP3ê°€ ê¸°í•˜í•™ ì¸ì‹ ë¡œë´‡ ì¡°ì‘ì„ ìœ„í•œ ì‹¤ìš©ì ì´ê³  ì„¼ì„œì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì†”ë£¨ì…˜ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ ì‹œì  ê´€ì°°ì„ í™œìš©í•˜ì—¬ 3D ì¥ë©´ ê¸°í•˜í•™ì„ ì´í•´í•˜ëŠ” ë¡œë´‡ ì¡°ì‘ ì •ì±…ì¸ GP3ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. GP3ëŠ” RGB ê´€ì°°ë¡œë¶€í„° ë°€ì§‘ ê³µê°„ íŠ¹ì§•ì„ ì¶”ë¡ í•˜ëŠ” ê³µê°„ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹Šì´ì™€ ì¹´ë©”ë¼ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³ , ì´ë¥¼ í†µí•´ ì¡°ì‘ì— ì í•©í•œ 3D ì¥ë©´ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ í‘œí˜„ì€ ì–¸ì–´ ì§€ì‹œì™€ ê²°í•©ë˜ì–´ ì—°ì†ì ì¸ í–‰ë™ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, GP3ëŠ” ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì‹  ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©°, ê¹Šì´ ì„¼ì„œë‚˜ ì‚¬ì „ ë§¤í•‘ í™˜ê²½ ì—†ì´ë„ ì‹¤ì œ ë¡œë´‡ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. GP3ëŠ” ì„¼ì„œì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì‹¤ìš©ì ì¸ ê¸°í•˜í•™ ì¸ì‹ ë¡œë´‡ ì¡°ì‘ ì†”ë£¨ì…˜ìœ¼ë¡œ ì£¼ëª©ë°›ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GP3ëŠ” ë‹¤ì¤‘ ì‹œì  ì…ë ¥ì„ í™œìš©í•˜ì—¬ 3D ê¸°í•˜í•™ì„ ì´í•´í•˜ëŠ” ë¡œë´‡ ì¡°ì‘ ì •ì±…ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ê³µê°„ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ RGB ê´€ì°°ì—ì„œ ê¹Šì´ì™€ ì¹´ë©”ë¼ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°€ì§‘ ê³µê°„ íŠ¹ì§•ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.
- 3. GP3ëŠ” ì–¸ì–´ ì§€ì‹œì™€ ê²°í•©ëœ 3D ì¥ë©´ í‘œí˜„ì„ ê²½ëŸ‰ ì •ì±… í—¤ë“œë¥¼ í†µí•´ ì—°ì†ì ì¸ í–‰ë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- 4. ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. ê¹Šì´ ì„¼ì„œë‚˜ ì‚¬ì „ ë§¤í•‘ëœ í™˜ê²½ ì—†ì´ë„ ìµœì†Œí•œì˜ ë¯¸ì„¸ ì¡°ì •ë§Œìœ¼ë¡œ ì‹¤ì œ ë¡œë´‡ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:11:48*