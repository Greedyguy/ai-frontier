---
keywords:
  - Large Language Model
  - Emotional Support Conversations
  - Chain-of-Strategy Optimization
  - Monte Carlo Tree Search
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2503.05362
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:42:26.859233",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Emotional Support Conversations",
    "Chain-of-Strategy Optimization",
    "Monte Carlo Tree Search"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Emotional Support Conversations": 0.7,
    "Chain-of-Strategy Optimization": 0.8,
    "Monte Carlo Tree Search": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion and connect broadly with existing research in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Emotional Support Conversations",
        "canonical": "Emotional Support Conversations",
        "aliases": [
          "ESC"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific application context for LLMs, highlighting a unique domain of interaction.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Chain-of-Strategy Optimization",
        "canonical": "Chain-of-Strategy Optimization",
        "aliases": [
          "CSO"
        ],
        "category": "unique_technical",
        "rationale": "This novel approach is central to the paper's contribution and represents a unique method for improving LLMs.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Monte Carlo Tree Search",
        "canonical": "Monte Carlo Tree Search",
        "aliases": [
          "MCTS"
        ],
        "category": "specific_connectable",
        "rationale": "A well-known method used in the paper to enhance strategy optimization, linking to broader AI techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "strategy selection accuracy",
      "preference bias",
      "fine-tuning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Emotional Support Conversations",
      "resolved_canonical": "Emotional Support Conversations",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Chain-of-Strategy Optimization",
      "resolved_canonical": "Chain-of-Strategy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Monte Carlo Tree Search",
      "resolved_canonical": "Monte Carlo Tree Search",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter

**Korean Title:** ì „ëµ ìµœì í™”ì˜ ì—°ì‡„ê°€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë” ë‚˜ì€ ê°ì • ì§€ì›ìë¡œ ë§Œë“ ë‹¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.05362.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2503.05362](https://arxiv.org/abs/2503.05362)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Empathy-R1_ A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support_20250922|Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support]] (81.0% similar)
- [[2025-09-22/Beyond Linear Steering_ Unified Multi-Attribute Control for Language Models_20250922|Beyond Linear Steering: Unified Multi-Attribute Control for Language Models]] (80.5% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (80.3% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (80.1% similar)
- [[2025-09-22/Calibrating LLM Confidence by Probing Perturbed Representation Stability_20250922|Calibrating LLM Confidence by Probing Perturbed Representation Stability]] (80.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Monte Carlo Tree Search|Monte Carlo Tree Search]]
**âš¡ Unique Technical**: [[keywords/Emotional Support Conversations|Emotional Support Conversations]], [[keywords/Chain-of-Strategy Optimization|Chain-of-Strategy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.05362v3 Announce Type: replace 
Abstract: The growing emotional stress in modern society has increased the demand for Emotional Support Conversations (ESC). While Large Language Models (LLMs) show promise for ESC, they face two key challenges: (1) low strategy selection accuracy, and (2) preference bias, limiting their adaptability to emotional needs of users. Existing supervised fine-tuning (SFT) struggles to address these issues, as it rigidly trains models on single gold-standard responses without modeling nuanced strategy trade-offs. To overcome these limitations, we propose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes strategy selection preferences at each dialogue turn. We first leverage Monte Carlo Tree Search to construct ESC-Pro, a high-quality preference dataset with turn-level strategy-response pairs. Training on ESC-Pro with CSO improves both strategy accuracy and bias mitigation, enabling LLMs to generate more empathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B, Gemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT, highlighting the efficacy of fine-grained, turn-level preference modeling in ESC.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2503.05362v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: í˜„ëŒ€ ì‚¬íšŒì—ì„œ ì¦ê°€í•˜ëŠ” ì •ì„œì  ìŠ¤íŠ¸ë ˆìŠ¤ëŠ” ì •ì„œì  ì§€ì› ëŒ€í™”(ESC)ì— ëŒ€í•œ ìˆ˜ìš”ë¥¼ ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ESCì— ìœ ë§í•œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ë‘ ê°€ì§€ ì£¼ìš” ê³¼ì œì— ì§ë©´í•´ ìˆìŠµë‹ˆë‹¤: (1) ë‚®ì€ ì „ëµ ì„ íƒ ì •í™•ë„, (2) ì‚¬ìš©ì ì •ì„œì  ìš”êµ¬ì— ëŒ€í•œ ì ì‘ì„±ì„ ì œí•œí•˜ëŠ” ì„ í˜¸ í¸í–¥. ê¸°ì¡´ì˜ ì§€ë„ í•™ìŠµ(SFT)ì€ ë‹¨ì¼ í‘œì¤€ ì‘ë‹µì— ëª¨ë¸ì„ ì—„ê²©í•˜ê²Œ í›ˆë ¨ì‹œí‚¤ë©´ì„œ ë¯¸ë¬˜í•œ ì „ëµì  ì ˆì¶©ì„ ëª¨ë¸ë§í•˜ì§€ ëª»í•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê° ëŒ€í™” í„´ì—ì„œ ì „ëµ ì„ íƒ ì„ í˜¸ë„ë¥¼ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ ì „ëµ ì²´ì¸ ìµœì í™”(CSO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë¨¼ì € ëª¬í…Œì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰ì„ í™œìš©í•˜ì—¬ í„´ ìˆ˜ì¤€ì˜ ì „ëµ-ì‘ë‹µ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ê³ í’ˆì§ˆ ì„ í˜¸ ë°ì´í„°ì…‹ì¸ ESC-Proë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. CSOë¥¼ ì‚¬ìš©í•˜ì—¬ ESC-Proì—ì„œ í›ˆë ¨í•˜ë©´ ì „ëµ ì •í™•ë„ì™€ í¸í–¥ ì™„í™”ê°€ ëª¨ë‘ í–¥ìƒë˜ì–´, LLMì´ ë” ê³µê°ì ì´ê³  ìƒí™©ì— ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. LLaMA-3.1-8B, Gemma-2-9B, Qwen2.5-7Bì— ëŒ€í•œ ì‹¤í—˜ì€ CSOê°€ í‘œì¤€ SFTë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•˜ë©°, ESCì—ì„œ ì„¸ë°€í•˜ê³  í„´ ìˆ˜ì¤€ì˜ ì„ í˜¸ ëª¨ë¸ë§ì˜ íš¨ìœ¨ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

í˜„ëŒ€ ì‚¬íšŒì˜ ê°ì •ì  ìŠ¤íŠ¸ë ˆìŠ¤ ì¦ê°€ë¡œ ê°ì • ì§€ì› ëŒ€í™”(ESC)ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ESCì— ìœ ë§í•˜ì§€ë§Œ, ì „ëµ ì„ íƒ ì •í™•ë„ê°€ ë‚®ê³  ì‚¬ìš©ì ê°ì •ì— ëŒ€í•œ ì ì‘ì„±ì´ ì œí•œë˜ëŠ” í¸í–¥ ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì§€ë„ í•™ìŠµì€ ë‹¨ì¼ ì •ë‹µì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëŒ€í™”ì˜ ê° í„´ì—ì„œ ì „ëµ ì„ íƒì„ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ ì „ëµ ì²´ì¸ ìµœì í™”(CSO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ëª¬í…Œì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰ì„ í™œìš©í•´ í„´ ìˆ˜ì¤€ì˜ ì „ëµ-ì‘ë‹µ ìŒì„ í¬í•¨í•œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ESC-Proë¥¼ êµ¬ì¶•í•˜ê³ , ì´ë¥¼ í†µí•´ CSOë¥¼ í›ˆë ¨í•˜ì—¬ ì „ëµ ì •í™•ë„ì™€ í¸í–¥ ì™„í™”ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CSOê°€ ê¸°ì¡´ì˜ ì§€ë„ í•™ìŠµë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ê°ì • ì§€ì› ëŒ€í™”ì—ì„œì˜ ì„¸ë°€í•œ ì„ í˜¸ë„ ëª¨ë¸ë§ì˜ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í˜„ëŒ€ ì‚¬íšŒì˜ ê°ì •ì  ìŠ¤íŠ¸ë ˆìŠ¤ ì¦ê°€ë¡œ ì¸í•´ ê°ì • ì§€ì› ëŒ€í™”(ESC)ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆë‹¤.
- 2. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ESCì— ìœ ë§í•˜ì§€ë§Œ, ì „ëµ ì„ íƒ ì •í™•ë„ ë‚®ìŒê³¼ ì„ í˜¸ í¸í–¥ì´ë¼ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œì— ì§ë©´í•˜ê³  ìˆë‹¤.
- 3. ê¸°ì¡´ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT)ì€ ë‹¨ì¼ ì •ë‹µì— ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œ ì „ëµ ì„ íƒì˜ ë¯¸ë¬˜í•œ ê· í˜•ì„ ëª¨ë¸ë§í•˜ì§€ ëª»í•œë‹¤.
- 4. ì²´ì¸ ì˜¤ë¸Œ ìŠ¤íŠ¸ë˜í‹°ì§€ ìµœì í™”(CSO)ëŠ” ê° ëŒ€í™” í„´ì—ì„œ ì „ëµ ì„ íƒ ì„ í˜¸ë„ë¥¼ ìµœì í™”í•˜ì—¬ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•œë‹¤.
- 5. CSOëŠ” ESC-Pro ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì „ëµ ì •í™•ë„ì™€ í¸í–¥ ì™„í™”ë¥¼ ê°œì„ í•˜ê³ , ë” ê³µê°ì ì´ê³  ìƒí™©ì— ë§ëŠ” ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•œë‹¤.


---

*Generated on 2025-09-23 11:42:26*