---
keywords:
  - Deep Learning
  - Dynamic Policy Fusion
  - Zero-Shot Learning
  - Human Feedback
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2409.20016
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:33:18.178090",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Dynamic Policy Fusion",
    "Zero-Shot Learning",
    "Human Feedback"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.7,
    "Dynamic Policy Fusion": 0.78,
    "Zero-Shot Learning": 0.82,
    "Human Feedback": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep reinforcement learning",
        "canonical": "Deep Learning",
        "aliases": [
          "Deep RL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is a foundational technology for reinforcement learning, enhancing connectivity with other machine learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Dynamic policy fusion",
        "canonical": "Dynamic Policy Fusion",
        "aliases": [
          "Policy Fusion"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach proposed in the paper, crucial for understanding user alignment without re-interaction.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-shot approach",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept that aligns with the paper's approach of not requiring additional interactions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Human feedback",
        "canonical": "Human Feedback",
        "aliases": [
          "User Feedback"
        ],
        "category": "unique_technical",
        "rationale": "Human Feedback is central to adapting policies to user-specific needs, enhancing the paper's methodology.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "task policy",
      "user-specific needs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep reinforcement learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Dynamic policy fusion",
      "resolved_canonical": "Dynamic Policy Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-shot approach",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Human feedback",
      "resolved_canonical": "Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Dynamic Policy Fusion for User Alignment Without Re-Interaction

**Korean Title:** ì‚¬ìš©ì ì •ë ¬ì„ ìœ„í•œ ë™ì  ì •ì±… ìœµí•©: ì¬ìƒí˜¸ì‘ìš© ì—†ì´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2409.20016.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2409.20016](https://arxiv.org/abs/2409.20016)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (83.4% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (82.5% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (81.9% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (81.7% similar)
- [[2025-09-19/Emergent Alignment via Competition_20250919|Emergent Alignment via Competition]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Dynamic Policy Fusion|Dynamic Policy Fusion]], [[keywords/Human Feedback|Human Feedback]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2409.20016v4 Announce Type: replace 
Abstract: Deep reinforcement learning (RL) policies, although optimal in terms of task rewards, may not align with the personal preferences of human users. To ensure this alignment, a naive solution would be to retrain the agent using a reward function that encodes the user's specific preferences. However, such a reward function is typically not readily available, and as such, retraining the agent from scratch can be prohibitively expensive. We propose a more practical approach - to adapt the already trained policy to user-specific needs with the help of human feedback. To this end, we infer the user's intent through trajectory-level feedback and combine it with the trained task policy via a theoretically grounded dynamic policy fusion approach. As our approach collects human feedback on the very same trajectories used to learn the task policy, it does not require any additional interactions with the environment, making it a zero-shot approach. We empirically demonstrate in a number of environments that our proposed dynamic policy fusion approach consistently achieves the intended task while simultaneously adhering to user-specific needs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2409.20016v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL) ì •ì±…ì€ ê³¼ì œ ë³´ìƒ ì¸¡ë©´ì—ì„œ ìµœì ì¼ ìˆ˜ ìˆì§€ë§Œ, ì¸ê°„ ì‚¬ìš©ìì˜ ê°œì¸ì  ì„ í˜¸ì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¼ì¹˜ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•œ ë‹¨ìˆœí•œ í•´ê²°ì±…ì€ ì‚¬ìš©ìì˜ íŠ¹ì • ì„ í˜¸ë¥¼ ì•”í˜¸í™”í•˜ëŠ” ë³´ìƒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ì¬í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë³´ìƒ í•¨ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ì—†ìœ¼ë©°, ë”°ë¼ì„œ ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì¬í•™ìŠµí•˜ëŠ” ê²ƒì€ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë³´ë‹¤ ì‹¤ìš©ì ì¸ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¯¸ í•™ìŠµëœ ì •ì±…ì„ ì¸ê°„ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìë³„ ìš”êµ¬ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ê¶¤ì  ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì¶”ë¡ í•˜ê³ , ì´ ì •ë³´ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ê¸°ë°˜ì„ ë‘” ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ë²•ì„ í†µí•´ í•™ìŠµëœ ê³¼ì œ ì •ì±…ê³¼ ê²°í•©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ê³¼ì œ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ëœ ë™ì¼í•œ ê¶¤ì ì—ì„œ ì¸ê°„ì˜ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ë¯€ë¡œ í™˜ê²½ê³¼ì˜ ì¶”ê°€ì ì¸ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” ì œë¡œìƒ· ì ‘ê·¼ë²•ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ì œì•ˆëœ ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ë²•ì´ ì˜ë„ëœ ê³¼ì œë¥¼ ì¼ê´€ë˜ê²Œ ë‹¬ì„±í•˜ë©´ì„œ ë™ì‹œì— ì‚¬ìš©ìë³„ ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ëŠ” ê²ƒì„ ì‹¤ì¦ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL) ì •ì±…ì´ ì‚¬ìš©ì ê°œì¸ì˜ ì„ í˜¸ì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒì„ ì§€ì í•˜ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ì í”¼ë“œë°±ì„ í™œìš©í•œ ì •ì±… ì ì‘ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì •ì±…ì„ ì‚¬ìš©ì ìš”êµ¬ì— ë§ê²Œ ì¡°ì •í•˜ê¸° ìœ„í•´ ê²½ë¡œ ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì¶”ë¡ í•˜ê³ , ì´ ì •ë³´ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ê¸°ë°˜í•œ ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ë²•ìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì¶”ê°€ì ì¸ í™˜ê²½ ìƒí˜¸ì‘ìš© ì—†ì´ ê¸°ì¡´ ê²½ë¡œì—ì„œ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì´ ì‚¬ìš©ì ìš”êµ¬ë¥¼ ì¶©ì¡±í•˜ë©´ì„œë„ ê³¼ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¬ì¸µ ê°•í™” í•™ìŠµ ì •ì±…ì€ ì¸ê°„ ì‚¬ìš©ìì˜ ê°œì¸ì  ì„ í˜¸ì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
- 2. ì‚¬ìš©ì ì„ í˜¸ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì¬í›ˆë ¨í•˜ëŠ” ê²ƒì€ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ì´ë¯¸ í›ˆë ¨ëœ ì •ì±…ì„ ì‚¬ìš©ì ìš”êµ¬ì— ë§ê²Œ ì¸ê°„ í”¼ë“œë°±ì„ í†µí•´ ì ì‘ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.
- 4. ê²½ë¡œ ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì¶”ë¡ í•˜ê³ , ì´ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ê¸°ë°˜í•œ ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ë²•ìœ¼ë¡œ ê²°í•©í•œë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ í™˜ê²½ê³¼ì˜ ì¶”ê°€ ìƒí˜¸ì‘ìš© ì—†ì´ ì‚¬ìš©ì ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ë©´ì„œë„ ì˜ë„ëœ ì‘ì—…ì„ ë‹¬ì„±í•œë‹¤.


---

*Generated on 2025-09-23 09:33:18*