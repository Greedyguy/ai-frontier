---
keywords:
  - Multimodal Learning
  - Reward Models
  - Vision-Language Model
  - Reinforcement Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.16127
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:21:17.998556",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Reward Models",
    "Vision-Language Model",
    "Reinforcement Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.78,
    "Reward Models": 0.82,
    "Vision-Language Model": 0.77,
    "Reinforcement Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects advancements in multimodal capabilities with language models, a trending area in AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.87,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reward Models",
        "canonical": "Reward Models",
        "aliases": [
          "RMs"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's contribution, linking to the development of models aligning AI with human preferences.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VL Model"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents a key integration of vision and language processing, relevant to the paper's benchmarks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Essential for understanding the practical application of the proposed model in real-world scenarios.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "state-of-the-art",
      "experimental analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.87,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reward Models",
      "resolved_canonical": "Reward Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# BaseReward: A Strong Baseline for Multimodal Reward Model

**Korean Title:** ê¸°ë³¸ ë³´ìƒ: ë‹¤ì¤‘ ëª¨ë‹¬ ë³´ìƒ ëª¨ë¸ì„ ìœ„í•œ ê°•ë ¥í•œ ê¸°ì¤€ì 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16127.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.16127](https://arxiv.org/abs/2509.16127)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (88.8% similar)
- [[2025-09-22/reWordBench_ Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs_20250922|reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs]] (86.4% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.2% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (85.2% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Reward Models|Reward Models]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16127v1 Announce Type: new 
Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including \textit{reward modeling paradigms} (e.g., Naive-RM, Critic-based RM, and Generative RM), \textit{reward head architecture}, \textit{training strategies}, \textit{data curation} (covering over ten multimodal and text-only preference datasets), \textit{backbone model} and \textit{model scale}, and \textit{ensemble methods}.
  Based on these experimental insights, we introduce \textbf{BaseReward}, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16127v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë“œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Models, MLLMs)ì˜ ê¸‰ì†í•œ ë°œì „ì€ ì´ë¥¼ ì¸ê°„ì˜ ì„ í˜¸ë„ì— ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê³¼ì œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ë³´ìƒ ëª¨ë¸(Reward Models, RMs)ì€ ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•µì‹¬ ê¸°ìˆ ì´ì§€ë§Œ, ìµœì²¨ë‹¨ ë‹¤ì¤‘ ëª¨ë“œ ë³´ìƒ ëª¨ë¸(Multimodal Reward Models, MRMs)ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì²´ê³„ì ì¸ ê°€ì´ë“œëŠ” í•™ê³„ì™€ ì‚°ì—…ê³„ ëª¨ë‘ì— í˜„ì¬ ë¶€ì¡±í•œ ìƒí™©ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì² ì €í•œ ì‹¤í—˜ ë¶„ì„ì„ í†µí•´ ê³ ì„±ëŠ¥ MRMì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ëª…í™•í•œ "ë ˆì‹œí”¼"ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” MRM ê°œë°œ íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” \textit{ë³´ìƒ ëª¨ë¸ë§ íŒ¨ëŸ¬ë‹¤ì„} (ì˜ˆ: Naive-RM, Critic-based RM, Generative RM), \textit{ë³´ìƒ í—¤ë“œ ì•„í‚¤í…ì²˜}, \textit{í›ˆë ¨ ì „ëµ}, \textit{ë°ì´í„° íë ˆì´ì…˜} (10ê°œ ì´ìƒì˜ ë‹¤ì¤‘ ëª¨ë“œ ë° í…ìŠ¤íŠ¸ ì „ìš© ì„ í˜¸ ë°ì´í„°ì…‹ í¬í•¨), \textit{ë°±ë³¸ ëª¨ë¸} ë° \textit{ëª¨ë¸ ê·œëª¨}, \textit{ì•™ìƒë¸” ë°©ë²•}ì´ í¬í•¨ë©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‹¤í—˜ì  í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” \textbf{BaseReward}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ë³´ìƒ ëª¨ë¸ë§ì„ ìœ„í•œ ê°•ë ¥í•˜ê³  íš¨ìœ¨ì ì¸ ê¸°ì¤€ì„ ì…ë‹ˆë‹¤. BaseRewardëŠ” ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•˜ë©°, {Qwen2.5-VL} ë°±ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”ëœ 2ì¸µ ë³´ìƒ í—¤ë“œë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•˜ê³ , ê³ í’ˆì§ˆ ë‹¤ì¤‘ ëª¨ë“œ ë° í…ìŠ¤íŠ¸ ì „ìš© ì„ í˜¸ ë°ì´í„°ì˜ ì‹ ì¤‘í•˜ê²Œ íë ˆì´ì…˜ëœ í˜¼í•©ë¬¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” BaseRewardê°€ MM-RLHF-Reward Bench, VL-Reward Bench, Multimodal Reward Benchì™€ ê°™ì€ ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ SOTAë¥¼ í™•ë¦½í•˜ë©° ì´ì „ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ìš±ì´, ì •ì  ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„˜ì–´ ì‹¤ì§ˆì ì¸ ìœ ìš©ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” BaseRewardë¥¼ ì‹¤ì œ ê°•í™” í•™ìŠµ íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ì—¬ ë‹¤ì–‘í•œ ì¸ì‹, ì¶”ë¡  ë° ëŒ€í™” ì‘ì—…ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ ì„±ê³µì ìœ¼ë¡œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ìµœìƒê¸‰ MRMì„ ì œê³µí•  ë¿ë§Œ ì•„ë‹ˆë¼, ì°¨ì„¸ëŒ€ MLLMì„ ìœ„í•œ ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ì„ ê°œë°œí•˜ê¸° ìœ„í•œ ëª…í™•í•˜ê³  ê²½í—˜ì ìœ¼ë¡œ ë’·ë°›ì¹¨ëœ ê°€ì´ë“œë¥¼ ì»¤ë®¤ë‹ˆí‹°ì— ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ë³´ìƒ ëª¨ë¸(RMs)ì˜ ê°œë°œì„ ìœ„í•œ ì²´ê³„ì ì¸ ê°€ì´ë“œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì €ìë“¤ì€ ë‹¤ì–‘í•œ ë³´ìƒ ëª¨ë¸ë§ íŒ¨ëŸ¬ë‹¤ì„, ë³´ìƒ í—¤ë“œ ì•„í‚¤í…ì²˜, í›ˆë ¨ ì „ëµ, ë°ì´í„° íë ˆì´ì…˜, ë°±ë³¸ ëª¨ë¸ ë° ëª¨ë¸ ê·œëª¨, ì•™ìƒë¸” ë°©ë²• ë“±ì„ ì² ì €íˆ ë¶„ì„í•˜ì—¬ ê³ ì„±ëŠ¥ ë©€í‹°ëª¨ë‹¬ ë³´ìƒ ëª¨ë¸(MRMs)ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, ì €ìë“¤ì€ Qwen2.5-VL ë°±ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ BaseRewardë¼ëŠ” ê°•ë ¥í•œ ê¸°ì¤€ ëª¨ë¸ì„ ì†Œê°œí•˜ë©°, ì´ëŠ” ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, BaseRewardëŠ” ì‹¤ì œ ê°•í™” í•™ìŠµ íŒŒì´í”„ë¼ì¸ì— í†µí•©ë˜ì–´ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì°¨ì„¸ëŒ€ MLLMsë¥¼ ìœ„í•œ ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ ê°œë°œì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë©€í‹°ëª¨ë‹¬ ë³´ìƒ ëª¨ë¸(MRM) ê°œë°œì„ ìœ„í•œ ì²´ê³„ì ì¸ ê°€ì´ë“œë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ê° êµ¬ì„± ìš”ì†Œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.
- 2. BaseRewardë¼ëŠ” ê°•ë ¥í•˜ê³  íš¨ìœ¨ì ì¸ ë©€í‹°ëª¨ë‹¬ ë³´ìƒ ëª¨ë¸ì˜ ê¸°ì¤€ì„ ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 3. BaseRewardëŠ” ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ SOTAë¥¼ ìˆ˜ë¦½í•˜ë©°, ì´ì „ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. BaseRewardë¥¼ ì‹¤ì œ ê°•í™” í•™ìŠµ íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ì°¨ì„¸ëŒ€ MLLMì„ ìœ„í•œ ê°•ë ¥í•œ ë³´ìƒ ëª¨ë¸ ê°œë°œì„ ìœ„í•œ ëª…í™•í•˜ê³  ì‹¤ì¦ì ì¸ ê°€ì´ë“œë¥¼ ì»¤ë®¤ë‹ˆí‹°ì— ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:21:17*