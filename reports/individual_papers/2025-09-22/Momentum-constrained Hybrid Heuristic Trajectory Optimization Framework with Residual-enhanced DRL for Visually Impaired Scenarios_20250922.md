---
keywords:
  - Deep Learning
  - Momentum-constrained Trajectory Optimization
  - LSTM-based Temporal Feature Modeling
  - Assistive Navigation
  - Residual-enhanced Actor-Critic Network
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15582
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:06:28.362134",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Momentum-constrained Trajectory Optimization",
    "LSTM-based Temporal Feature Modeling",
    "Assistive Navigation",
    "Residual-enhanced Actor-Critic Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.85,
    "Momentum-constrained Trajectory Optimization": 0.78,
    "LSTM-based Temporal Feature Modeling": 0.82,
    "Assistive Navigation": 0.74,
    "Residual-enhanced Actor-Critic Network": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "deep reinforcement learning",
        "canonical": "Deep Learning",
        "aliases": [
          "DRL"
        ],
        "category": "broad_technical",
        "rationale": "Deep reinforcement learning is a subset of deep learning, which is a foundational concept in the field.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "momentum-constrained trajectory optimization",
        "canonical": "Momentum-constrained Trajectory Optimization",
        "aliases": [
          "MTO"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique approach specific to the paper's framework, enhancing trajectory optimization techniques.",
        "novelty_score": 0.72,
        "connectivity_score": 0.67,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "LSTM-based temporal feature modeling",
        "canonical": "LSTM-based Temporal Feature Modeling",
        "aliases": [
          "LSTM Temporal Modeling"
        ],
        "category": "specific_connectable",
        "rationale": "LSTM-based modeling is crucial for understanding temporal dependencies, linking to time-series analysis.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.77,
        "link_intent_score": 0.82
      },
      {
        "surface": "assistive navigation",
        "canonical": "Assistive Navigation",
        "aliases": [
          "Navigation for Visually Impaired"
        ],
        "category": "unique_technical",
        "rationale": "This term directly relates to the paper's application domain, offering a unique perspective on navigation technology.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.83,
        "link_intent_score": 0.74
      },
      {
        "surface": "residual-enhanced actor-critic network",
        "canonical": "Residual-enhanced Actor-Critic Network",
        "aliases": [
          "ResB-PPO"
        ],
        "category": "unique_technical",
        "rationale": "This network architecture is specific to the paper's methodology, enhancing reinforcement learning strategies.",
        "novelty_score": 0.71,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "trajectory sampling",
      "cost evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "deep reinforcement learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "momentum-constrained trajectory optimization",
      "resolved_canonical": "Momentum-constrained Trajectory Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.67,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LSTM-based temporal feature modeling",
      "resolved_canonical": "LSTM-based Temporal Feature Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.77,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "assistive navigation",
      "resolved_canonical": "Assistive Navigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.83,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "residual-enhanced actor-critic network",
      "resolved_canonical": "Residual-enhanced Actor-Critic Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.71,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios

**Korean Title:** ëª¨ë©˜í…€ ì œì•½ í•˜ì´ë¸Œë¦¬ë“œ íœ´ë¦¬ìŠ¤í‹± ê¶¤ì  ìµœì í™” í”„ë ˆì„ì›Œí¬ì™€ ì”ì—¬ ê°•í™” ì‹¬ì¸µ ê°•í™” í•™ìŠµì„ í†µí•œ ì‹œê° ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15582.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15582](https://arxiv.org/abs/2509.15582)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (82.2% similar)
- [[2025-09-19/PA-MPPI_ Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments_20250919|PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments]] (81.8% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (81.7% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (81.4% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (81.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/LSTM-based Temporal Feature Modeling|LSTM-based Temporal Feature Modeling]]
**âš¡ Unique Technical**: [[keywords/Momentum-constrained Trajectory Optimization|Momentum-constrained Trajectory Optimization]], [[keywords/Assistive Navigation|Assistive Navigation]], [[keywords/Residual-enhanced Actor-Critic Network|Residual-enhanced Actor-Critic Network]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15582v1 Announce Type: cross 
Abstract: This paper proposes a momentum-constrained hybrid heuristic trajectory optimization framework (MHHTOF) tailored for assistive navigation in visually impaired scenarios, integrating trajectory sampling generation, optimization and evaluation with residual-enhanced deep reinforcement learning (DRL). In the first stage, heuristic trajectory sampling cluster (HTSC) is generated in the Frenet coordinate system using third-order interpolation with fifth-order polynomials and momentum-constrained trajectory optimization (MTO) constraints to ensure smoothness and feasibility. After first stage cost evaluation, the second stage leverages a residual-enhanced actor-critic network with LSTM-based temporal feature modeling to adaptively refine trajectory selection in the Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with weight transfer aligns semantic priorities across stages, supporting human-centered optimization. Experimental results demonstrate that the proposed LSTM-ResB-PPO achieves significantly faster convergence, attaining stable policy performance in approximately half the training iterations required by the PPO baseline, while simultaneously enhancing both reward outcomes and training stability. Compared to baseline method, the selected model reduces average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle risks by over 77%. These findings validate the framework's effectiveness in enhancing robustness, safety, and real-time feasibility in complex assistive planning tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15582v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì´ ë…¼ë¬¸ì€ ì‹œê° ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë³´ì¡° ë‚´ë¹„ê²Œì´ì…˜ì„ ìœ„í•´ ì„¤ê³„ëœ ëª¨ë©˜í…€ ì œì•½ í•˜ì´ë¸Œë¦¬ë“œ íœ´ë¦¬ìŠ¤í‹± ê²½ë¡œ ìµœì í™” í”„ë ˆì„ì›Œí¬(MHHTOF)ë¥¼ ì œì•ˆí•˜ë©°, ì”ì—¬ ê°•í™” í•™ìŠµ(DRL)ì„ í™œìš©í•œ ê²½ë¡œ ìƒ˜í”Œë§ ìƒì„±, ìµœì í™” ë° í‰ê°€ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” í”„ë ˆë„¤ ì¢Œí‘œê³„ì—ì„œ 5ì°¨ ë‹¤í•­ì‹ê³¼ ëª¨ë©˜í…€ ì œì•½ ê²½ë¡œ ìµœì í™”(MTO) ì œì•½ì„ ì‚¬ìš©í•œ 3ì°¨ ë³´ê°„ë²•ì„ í†µí•´ íœ´ë¦¬ìŠ¤í‹± ê²½ë¡œ ìƒ˜í”Œë§ í´ëŸ¬ìŠ¤í„°(HTSC)ë¥¼ ìƒì„±í•˜ì—¬ ë§¤ë„ëŸ¬ì›€ê³¼ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì˜ ë¹„ìš© í‰ê°€ í›„, ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” LSTM ê¸°ë°˜ì˜ ì‹œê°„ì  íŠ¹ì§• ëª¨ë¸ë§ì„ ê°–ì¶˜ ì”ì—¬ ê°•í™” ì•¡í„°-í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ë°ì¹´ë¥´íŠ¸ ì¢Œí‘œê³„ì—ì„œ ê²½ë¡œ ì„ íƒì„ ì ì‘ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì „ì´ë¥¼ í†µí•œ ì´ì¤‘ ë‹¨ê³„ ë¹„ìš© ëª¨ë¸ë§ ë©”ì»¤ë‹ˆì¦˜(DCMM)ì€ ë‹¨ê³„ ê°„ ì˜ë¯¸ì  ìš°ì„ ìˆœìœ„ë¥¼ ì •ë ¬í•˜ì—¬ ì¸ê°„ ì¤‘ì‹¬ì˜ ìµœì í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ì œì•ˆëœ LSTM-ResB-PPOê°€ PPO ê¸°ì¤€ì„ ë³´ë‹¤ ì•½ ì ˆë°˜ì˜ í›ˆë ¨ ë°˜ë³µì—ì„œ ì•ˆì •ì ì¸ ì •ì±… ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ë³´ìƒ ê²°ê³¼ì™€ í›ˆë ¨ ì•ˆì •ì„±ì„ ë™ì‹œì— í–¥ìƒì‹œí‚¤ë©´ì„œ ìˆ˜ë ´ ì†ë„ê°€ í˜„ì €íˆ ë¹ ë¦„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¤€ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ì„ íƒëœ ëª¨ë¸ì€ í‰ê·  ë¹„ìš©ê³¼ ë¹„ìš© ë³€ë™ì„ ê°ê° 30.3% ë° 53.3% ê°ì†Œì‹œí‚¤ê³ , ìì•„ ë° ì¥ì• ë¬¼ ìœ„í—˜ì„ 77% ì´ìƒ ë‚®ì¶¥ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë³µì¡í•œ ë³´ì¡° ê³„íš ì‘ì—…ì—ì„œ í”„ë ˆì„ì›Œí¬ì˜ ê²¬ê³ ì„±, ì•ˆì „ì„± ë° ì‹¤ì‹œê°„ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ë³´ì¡° ë‚´ë¹„ê²Œì´ì…˜ì— ì í•©í•œ ëª¨ë©˜í…€ ì œì•½ í•˜ì´ë¸Œë¦¬ë“œ íœ´ë¦¬ìŠ¤í‹± ê²½ë¡œ ìµœì í™” í”„ë ˆì„ì›Œí¬(MHHTOF)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ê²½ë¡œ ìƒ˜í”Œë§ ìƒì„±, ìµœì í™” ë° í‰ê°€ë¥¼ ì”ì°¨ ê°•í™” í•™ìŠµ(DRL)ê³¼ í†µí•©í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” í”„ë ˆë„¤ ì¢Œí‘œê³„ì—ì„œ 3ì°¨ ë³´ê°„ê³¼ 5ì°¨ ë‹¤í•­ì‹ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¦¬ìŠ¤í‹± ê²½ë¡œ ìƒ˜í”Œë§ í´ëŸ¬ìŠ¤í„°(HTSC)ë¥¼ ìƒì„±í•˜ê³ , ëª¨ë©˜í…€ ì œì•½ ê²½ë¡œ ìµœì í™”(MTO)ë¡œ ë¶€ë“œëŸ¬ì›€ê³¼ ì‹¤í–‰ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” LSTM ê¸°ë°˜ì˜ ì”ì°¨ ê°•í™” ì•¡í„°-í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ê²½ë¡œ ì„ íƒì„ ì ì‘ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ LSTM-ResB-PPOëŠ” PPO ê¸°ì¤€ë³´ë‹¤ í›ˆë ¨ ìˆ˜ë ´ ì†ë„ê°€ ë‘ ë°° ë¹ ë¥´ê³ , ë³´ìƒ ê²°ê³¼ì™€ í›ˆë ¨ ì•ˆì •ì„±ì„ ë™ì‹œì— í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ í‰ê·  ë¹„ìš©ê³¼ ë¹„ìš© ë³€ë™ì„±ì„ ê°ê° 30.3%, 53.3% ì¤„ì´ê³ , ì¥ì• ë¬¼ ìœ„í—˜ì„ 77% ì´ìƒ ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë³µì¡í•œ ë³´ì¡° ê³„íš ì‘ì—…ì—ì„œ í”„ë ˆì„ì›Œí¬ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ ë…¼ë¬¸ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ë³´ì¡° ë‚´ë¹„ê²Œì´ì…˜ì— ì í•©í•œ ëª¨ë©˜í…€ ì œì•½ í•˜ì´ë¸Œë¦¬ë“œ íœ´ë¦¬ìŠ¤í‹± ê²½ë¡œ ìµœì í™” í”„ë ˆì„ì›Œí¬(MHHTOF)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” í”„ë ˆë„¤ ì¢Œí‘œê³„ì—ì„œ 3ì°¨ ë³´ê°„ë²•ê³¼ 5ì°¨ ë‹¤í•­ì‹ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¦¬ìŠ¤í‹± ê²½ë¡œ ìƒ˜í”Œë§ í´ëŸ¬ìŠ¤í„°(HTSC)ë¥¼ ìƒì„±í•˜ê³ , ëª¨ë©˜í…€ ì œì•½ ê²½ë¡œ ìµœì í™”(MTO) ì œì•½ì„ í†µí•´ ê²½ë¡œì˜ ë¶€ë“œëŸ¬ì›€ê³¼ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 3. ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” LSTM ê¸°ë°˜ì˜ ì‹œê³„ì—´ íŠ¹ì§• ëª¨ë¸ë§ì„ í™œìš©í•œ ì”ì°¨ ê°•í™” ì•¡í„°-í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ê²½ë¡œ ì„ íƒì„ ì ì‘ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ LSTM-ResB-PPOëŠ” PPO ê¸°ì¤€ì„ ë³´ë‹¤ ì•½ ì ˆë°˜ì˜ í›ˆë ¨ ë°˜ë³µì—ì„œ ì•ˆì •ì ì¸ ì •ì±… ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ë³´ìƒ ê²°ê³¼ì™€ í›ˆë ¨ ì•ˆì •ì„±ì„ ë™ì‹œì— í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 5. ì„ íƒëœ ëª¨ë¸ì€ ê¸°ì¤€ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ í‰ê·  ë¹„ìš©ê³¼ ë¹„ìš© ë³€ë™ì„±ì„ ê°ê° 30.3%ì™€ 53.3% ê°ì†Œì‹œí‚¤ê³ , ìì•„ ë° ì¥ì• ë¬¼ ìœ„í—˜ì„ 77% ì´ìƒ ë‚®ì¶¥ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:06:28*