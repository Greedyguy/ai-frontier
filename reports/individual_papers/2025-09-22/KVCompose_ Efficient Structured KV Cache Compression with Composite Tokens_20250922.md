---
keywords:
  - Large Language Model
  - KV Cache Compression
  - Attention Mechanism
  - Composite Tokens
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.05165
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:11:17.227846",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "KV Cache Compression",
    "Attention Mechanism",
    "Composite Tokens"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "KV Cache Compression": 0.7,
    "Attention Mechanism": 0.8,
    "Composite Tokens": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on KV cache compression, linking to broader discussions in NLP and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "KV Cache Compression",
        "canonical": "KV Cache Compression",
        "aliases": [
          "Key-Value Cache Compression"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique technical concept introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Attention Mechanism is a key component in the proposed method for estimating token importance.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Composite Tokens",
        "canonical": "Composite Tokens",
        "aliases": [
          "Composite Token"
        ],
        "category": "unique_technical",
        "rationale": "Composite Tokens are a novel concept in the paper, essential for the proposed KV cache compression strategy.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "method",
      "framework",
      "approach"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "KV Cache Compression",
      "resolved_canonical": "KV Cache Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Composite Tokens",
      "resolved_canonical": "Composite Tokens",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# KVCompose: Efficient Structured KV Cache Compression with Composite Tokens

**Korean Title:** KVCompose: ë³µí•© í† í°ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ êµ¬ì¡°ì  KV ìºì‹œ ì••ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.05165.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.05165](https://arxiv.org/abs/2509.05165)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (89.9% similar)
- [[2025-09-22/UniGist_ Towards General and Hardware-aligned Sequence-level Long Context Compression_20250922|UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression]] (84.9% similar)
- [[2025-09-22/ConCISE_ Confidence-guided Compression in Step-by-step Efficient Reasoning_20250922|ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning]] (83.3% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (83.2% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/KV Cache Compression|KV Cache Compression]], [[keywords/Composite Tokens|Composite Tokens]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.05165v2 Announce Type: replace 
Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.
  We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.05165v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ íš¨ìœ¨ì ì¸ ìê¸°íšŒê·€ ë””ì½”ë”©ì„ ìœ„í•´ í‚¤-ê°’(KV) ìºì‹œë¥¼ í™œìš©í•˜ì§€ë§Œ, ìºì‹œ í¬ê¸°ëŠ” ë¬¸ë§¥ ê¸¸ì´ì™€ ëª¨ë¸ ê¹Šì´ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì¶”ë¡ ì—ì„œ ì£¼ìš” ë³‘ëª© í˜„ìƒì´ ë©ë‹ˆë‹¤. ì´ì „ì˜ KV ìºì‹œ ì••ì¶• ë°©ë²•ë“¤ì€ ì—„ê²©í•œ íœ´ë¦¬ìŠ¤í‹±ì„ ê°•ìš”í•˜ê±°ë‚˜, ì£¼ì˜ í—¤ë“œë³„ ê°€ë³€ì„±ìœ¼ë¡œ ì¸í•´ í…ì„œ ë ˆì´ì•„ì›ƒì„ ë°©í•´í•˜ê±°ë‚˜, íŠ¹ìˆ˜í•œ ê³„ì‚° ì»¤ë„ì„ ìš”êµ¬í•©ë‹ˆë‹¤.  
ìš°ë¦¬ëŠ” ì£¼ì˜ ê¸°ë°˜, ê³„ì¸µ ì ì‘í˜• ë³µí•© í† í°ì— ê¸°ë°˜í•œ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ KV ìºì‹œ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ì£¼ì˜ ì ìˆ˜ë¥¼ ì§‘ê³„í•˜ì—¬ í† í°ì˜ ì¤‘ìš”ì„±ì„ ì¶”ì •í•˜ê³ , í—¤ë“œë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í† í°ì„ ì„ íƒí•˜ë©°, ê¸°ì¡´ ì¶”ë¡  ì—”ì§„ì´ ìš”êµ¬í•˜ëŠ” ê· ì¼í•œ ìºì‹œ êµ¬ì¡°ë¥¼ ì¤€ìˆ˜í•˜ëŠ” ë³µí•© í† í°ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤. ì „ì—­ í• ë‹¹ ë©”ì»¤ë‹ˆì¦˜ì€ ê³„ì¸µ ê°„ ìœ ì§€ ì˜ˆì‚°ì„ ì¡°ì •í•˜ì—¬ ì •ë³´ê°€ ë§ì€ í† í°ì„ ê°€ì§„ ê³„ì¸µì— ë” ë§ì€ ìš©ëŸ‰ì„ í• ë‹¹í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ë©°, ì´ì „ì˜ êµ¬ì¡°ì  ë° ë°˜êµ¬ì¡°ì  ë°©ë²•ë“¤ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤. ë¬´ì—‡ë³´ë‹¤ë„, ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ í‘œì¤€ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ê³¼ ì™„ì „íˆ í˜¸í™˜ë˜ë©°, íš¨ìœ¨ì ì¸ ê¸´ ë¬¸ë§¥ LLM ë°°í¬ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì ì¸ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ë””ì½”ë”©ì„ ìœ„í•œ KV ìºì‹œ ì••ì¶• ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ê³ ì •ëœ ê·œì¹™ì„ ê°•ìš”í•˜ê±°ë‚˜ íŠ¹ìˆ˜í•œ ê³„ì‚° ì»¤ë„ì„ ìš”êµ¬í•˜ëŠ” ë°˜ë©´, ì œì•ˆëœ ë°©ë²•ì€ ì£¼ì˜ ê¸°ë°˜ì˜ ë ˆì´ì–´ ì ì‘í˜• ë³µí•© í† í°ì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œë¥¼ ì••ì¶•í•©ë‹ˆë‹¤. ì£¼ì˜ ì ìˆ˜ë¥¼ í†µí•´ í† í° ì¤‘ìš”ë„ë¥¼ ì¶”ì •í•˜ê³ , ê° í—¤ë“œë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í† í°ì„ ì„ íƒí•˜ì—¬ ê¸°ì¡´ ì¶”ë¡  ì—”ì§„ê³¼ í˜¸í™˜ë˜ëŠ” êµ¬ì¡°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ê¸€ë¡œë²Œ í• ë‹¹ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë ˆì´ì–´ë³„ë¡œ ì •ë³´ê°€ ë§ì€ í† í°ì— ë” ë§ì€ ìš©ëŸ‰ì„ í• ë‹¹í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ë©´ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©°, ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. íŠ¹íˆ, í‘œì¤€ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ê³¼ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë˜ì–´ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì ì¸ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ë””ì½”ë”©ì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” KV ìºì‹œëŠ” ë¬¸ë§¥ ê¸¸ì´ì™€ ëª¨ë¸ ê¹Šì´ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì¶”ë¡ ì—ì„œ ì£¼ìš” ë³‘ëª© í˜„ìƒì´ ëœë‹¤.
- 2. ê¸°ì¡´ì˜ KV ìºì‹œ ì••ì¶• ë°©ë²•ì€ ì—„ê²©í•œ íœ´ë¦¬ìŠ¤í‹±ì„ ì ìš©í•˜ê±°ë‚˜, ì£¼ì˜ í—¤ë“œë³„ ë³€ë™ì„±ìœ¼ë¡œ í…ì„œ ë ˆì´ì•„ì›ƒì„ ë°©í•´í•˜ê±°ë‚˜, íŠ¹ìˆ˜í•œ ê³„ì‚° ì»¤ë„ì„ ìš”êµ¬í•œë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ì£¼ì˜ ê¸°ë°˜, ê³„ì¸µ ì ì‘í˜• ë³µí•© í† í°ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ KV ìºì‹œ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•œë‹¤.
- 4. ì´ ë°©ë²•ì€ ì£¼ì˜ ì ìˆ˜ë¥¼ ì§‘ê³„í•˜ì—¬ í† í° ì¤‘ìš”ë„ë¥¼ ì¶”ì •í•˜ê³ , í—¤ë“œë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í† í°ì„ ì„ íƒí•˜ì—¬ ê¸°ì¡´ ì¶”ë¡  ì—”ì§„ì´ ìš”êµ¬í•˜ëŠ” ê· ì¼í•œ ìºì‹œ êµ¬ì¡°ì— ë§ê²Œ ë³µí•© í† í°ìœ¼ë¡œ ì •ë ¬í•œë‹¤.
- 5. ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì€ ë©”ëª¨ë¦¬ ì ˆê°ì„ ì´ë£¨ë©´ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©°, ê¸°ì¡´ì˜ êµ¬ì¡°ì  ë° ë°˜êµ¬ì¡°ì  ë°©ë²•ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•œë‹¤.


---

*Generated on 2025-09-23 11:11:17*