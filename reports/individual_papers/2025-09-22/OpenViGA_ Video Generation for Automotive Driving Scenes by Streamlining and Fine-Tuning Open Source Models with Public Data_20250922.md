---
keywords:
  - Video Generation
  - World Model
  - Image Tokenizer
  - Pre-trained Models
  - Automotive Driving Scenes
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15479
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:58:16.912892",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Video Generation",
    "World Model",
    "Image Tokenizer",
    "Pre-trained Models",
    "Automotive Driving Scenes"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Video Generation": 0.78,
    "World Model": 0.81,
    "Image Tokenizer": 0.74,
    "Pre-trained Models": 0.79,
    "Automotive Driving Scenes": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "video generation",
        "canonical": "Video Generation",
        "aliases": [
          "video synthesis",
          "video creation"
        ],
        "category": "broad_technical",
        "rationale": "Video generation is a key process in creating realistic driving scenes and connects to broader computer vision tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "world model",
        "canonical": "World Model",
        "aliases": [
          "environment model",
          "scene model"
        ],
        "category": "specific_connectable",
        "rationale": "World models are crucial for predicting future states in video generation, linking to predictive modeling.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.81
      },
      {
        "surface": "image tokenizer",
        "canonical": "Image Tokenizer",
        "aliases": [
          "visual tokenizer",
          "image encoding"
        ],
        "category": "unique_technical",
        "rationale": "Image tokenization is a unique technical step in the pipeline, essential for processing visual data.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.77,
        "link_intent_score": 0.74
      },
      {
        "surface": "pre-trained open source models",
        "canonical": "Pre-trained Models",
        "aliases": [
          "open source models",
          "pre-trained frameworks"
        ],
        "category": "specific_connectable",
        "rationale": "Using pre-trained models is a common practice that enhances connectivity with existing machine learning frameworks.",
        "novelty_score": 0.52,
        "connectivity_score": 0.82,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      },
      {
        "surface": "automotive driving scenes",
        "canonical": "Automotive Driving Scenes",
        "aliases": [
          "driving scenarios",
          "vehicle scenes"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific application area that connects to autonomous driving and simulation research.",
        "novelty_score": 0.67,
        "connectivity_score": 0.73,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "video generation",
      "resolved_canonical": "Video Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "world model",
      "resolved_canonical": "World Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "image tokenizer",
      "resolved_canonical": "Image Tokenizer",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.77,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "pre-trained open source models",
      "resolved_canonical": "Pre-trained Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.82,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "automotive driving scenes",
      "resolved_canonical": "Automotive Driving Scenes",
      "decision": "linked",
      "scores": {
        "novelty": 0.67,
        "connectivity": 0.73,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data

**Korean Title:** OpenViGA: 공개 데이터로 오픈 소스 모델을 최적화하여 자동차 주행 장면을 위한 비디오 생성

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15479.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15479](https://arxiv.org/abs/2509.15479)

## 🔗 유사한 논문
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (81.9% similar)
- [[2025-09-19/SPATIALGEN_ Layout-guided 3D Indoor Scene Generation_20250919|SPATIALGEN: Layout-guided 3D Indoor Scene Generation]] (81.0% similar)
- [[2025-09-18/FlightDiffusion_ Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video_20250918|FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video]] (80.6% similar)
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (80.2% similar)
- [[2025-09-19/A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation_20250919|A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation]] (79.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Video Generation|Video Generation]]
**🔗 Specific Connectable**: [[keywords/World Model|World Model]], [[keywords/Pre-trained Models|Pre-trained Models]]
**⚡ Unique Technical**: [[keywords/Image Tokenizer|Image Tokenizer]], [[keywords/Automotive Driving Scenes|Automotive Driving Scenes]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15479v1 Announce Type: new 
Abstract: Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.

## 🔍 Abstract (한글 번역)

arXiv:2509.15479v1 발표 유형: 신규  
초록: 최근의 성공적인 비디오 생성 시스템은 짧은 비디오 입력으로부터 현실적인 자동차 운전 장면을 예측하고 생성하며, 토큰화, 미래 상태 예측(월드 모델), 비디오 디코딩을 전용 모델에 할당합니다. 이러한 접근 방식은 종종 상당한 훈련 자원이 필요한 대형 모델을 사용하며, 설계 선택에 대한 통찰력이 제한적이고, 공개적으로 사용 가능한 코드와 데이터셋이 부족합니다. 본 연구에서는 이러한 결점을 해결하고 자동차 운전 장면을 위한 오픈 비디오 생성 시스템인 OpenViGA를 제시합니다. 우리의 기여는 다음과 같습니다: GAIA-1과 같은 여러 이전 비디오 생성 연구와 달리, 우리는 시스템의 세 가지 구성 요소(이미지 토크나이저, 월드 모델, 비디오 디코더)에 대한 심층 분석을 별도의 정량적 및 정성적 평가를 통해 제공합니다. 두 번째로, 우리는 다양한 도메인에서 강력한 사전 훈련된 오픈 소스 모델을 기반으로 하여, 공개적으로 사용 가능한 자동차 데이터(BDD100K)를 GPU 하드웨어에서 학문적 규모로 미세 조정합니다. 세 번째로, 구성 요소의 인터페이스를 간소화하여 일관된 비디오 생성 시스템을 구축합니다. 네 번째로, 기본 모델과 데이터의 공개 사용 가능성 덕분에 완전한 재현성을 허용합니다. 마지막으로, 우리는 Github에 코드와 모델을 공개합니다. 256x256 이미지 크기에서 4 fps로, 우리는 알고리즘 지연이 단 한 프레임인 상태에서 프레임별로 현실적인 운전 장면 비디오를 예측할 수 있습니다.

## 📝 요약

이 논문에서는 자동차 주행 장면을 생성하는 비디오 생성 시스템인 OpenViGA를 소개합니다. 기존의 대규모 모델과 달리, OpenViGA는 이미지 토크나이저, 월드 모델, 비디오 디코더의 세 가지 구성 요소를 정량적 및 정성적으로 분석합니다. 또한, 다양한 도메인의 강력한 오픈 소스 모델을 사용하고, 공개된 자동차 데이터(BDD100K)를 활용하여 학문적 규모의 GPU 하드웨어에서 미세 조정합니다. 시스템 구성 요소 간의 인터페이스를 최적화하여 일관된 비디오 생성 시스템을 구축하였으며, 모든 모델과 데이터를 공개하여 재현성을 보장합니다. 최종적으로, 256x256 해상도에서 4fps로 현실적인 주행 장면 비디오를 프레임 단위로 예측할 수 있으며, 코드와 모델을 Github에 공개합니다.

## 🎯 주요 포인트

- 1. OpenViGA는 자동차 주행 장면의 비디오 생성을 위한 오픈 시스템으로, 이미지 토크나이저, 월드 모델, 비디오 디코더의 세 가지 구성 요소를 심층 분석합니다.
- 2. 다양한 도메인에서 사전 훈련된 오픈 소스 모델을 기반으로 하여, 공개된 자동차 데이터(BDD100K)를 사용해 학술적 규모의 GPU 하드웨어에서 미세 조정합니다.
- 3. 구성 요소의 인터페이스를 간소화하여 일관된 비디오 생성 시스템을 구축하였습니다.
- 4. 기반 모델과 데이터의 공개로 인해 완전한 재현성을 보장하며, 코드와 모델을 Github에 공개합니다.
- 5. 256x256 이미지 크기와 4 fps에서 알고리즘 지연이 한 프레임에 불과한 현실적인 주행 장면 비디오를 프레임별로 예측할 수 있습니다.


---

*Generated on 2025-09-23 11:58:16*