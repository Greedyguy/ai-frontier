---
keywords:
  - Supervised Fine-Tuning
  - Off-Policy Learning
  - Importance Sampling
  - Policy Gap
  - Data Rewriting
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15157
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:13:37.740956",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Supervised Fine-Tuning",
    "Off-Policy Learning",
    "Importance Sampling",
    "Policy Gap",
    "Data Rewriting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Supervised Fine-Tuning": 0.75,
    "Off-Policy Learning": 0.8,
    "Importance Sampling": 0.78,
    "Policy Gap": 0.7,
    "Data Rewriting": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Supervised Fine-Tuning",
        "canonical": "Supervised Fine-Tuning",
        "aliases": [
          "SFT"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific technique central to the paper's methodology, offering unique insights into off-policy learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Off-Policy Learning",
        "canonical": "Off-Policy Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key concept in the paper, linking to broader discussions on policy optimization in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Importance Sampling",
        "canonical": "Importance Sampling",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A fundamental technique discussed in the paper, connecting to statistical methods in machine learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Policy Gap",
        "canonical": "Policy Gap",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Central to the paper's problem statement, offering a unique angle on policy optimization challenges.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Data Rewriting",
        "canonical": "Data Rewriting",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to mitigate policy gaps, crucial for understanding the paper's contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Supervised Fine-Tuning",
      "resolved_canonical": "Supervised Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Off-Policy Learning",
      "resolved_canonical": "Off-Policy Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Importance Sampling",
      "resolved_canonical": "Importance Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Policy Gap",
      "resolved_canonical": "Policy Gap",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Data Rewriting",
      "resolved_canonical": "Data Rewriting",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning

**Korean Title:** ê°­ì— ìœ ì˜í•˜ë¼: ì•ˆì •ì ì¸ ì˜¤í”„-ì •ì±… ê°ë… ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ë°ì´í„° ì¬ì‘ì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15157.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15157](https://arxiv.org/abs/2509.15157)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250918|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (97.8% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (86.4% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (85.4% similar)
- [[2025-09-22/Distribution-Aligned Decoding for Efficient LLM Task Adaptation_20250922|Distribution-Aligned Decoding for Efficient LLM Task Adaptation]] (84.0% similar)
- [[2025-09-19/From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery: Reinforced Distillation of Large Language Model Agents]] (83.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Off-Policy Learning|Off-Policy Learning]], [[keywords/Importance Sampling|Importance Sampling]]
**âš¡ Unique Technical**: [[keywords/Supervised Fine-Tuning|Supervised Fine-Tuning]], [[keywords/Policy Gap|Policy Gap]], [[keywords/Data Rewriting|Data Rewriting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15157v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to skewed weights, high variance, and unstable optimization. Existing methods mitigate this issue with KL penalties or clipping, which passively restrict updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap before training. For each problem, correct model-generated solutions are kept as on-policy data, while incorrect ones are rewritten through guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy, reducing variance and improving stability. To handle residual mismatch after rewriting, we additionally apply importance sampling during training, forming a two-stage approach that combines data-level alignment with lightweight optimization-level correction. Experiments on five mathematical reasoning benchmarks show consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. Data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15157v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT)ì€ ì „ë¬¸ê°€ ì‹œì—°ì´ ê³ ì •ëœ í–‰ë™ ì •ì±…ì—ì„œ ë‚˜ì˜¤ëŠ” ë°˜ë©´, í›ˆë ¨ì€ ëª©í‘œ ì •ì±…ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ì˜¤í”„ ì •ì±… í•™ìŠµ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”ë„ ìƒ˜í”Œë§ì€ ì´ëŸ¬í•œ ë¶„í¬ ë¶ˆì¼ì¹˜ë¥¼ ìˆ˜ì •í•˜ê¸° ìœ„í•œ í‘œì¤€ ë„êµ¬ì´ì§€ë§Œ, í° ì •ì±… ê²©ì°¨ëŠ” ì™œê³¡ëœ ê°€ì¤‘ì¹˜, ë†’ì€ ë¶„ì‚°, ë¶ˆì•ˆì •í•œ ìµœì í™”ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ KL íŒ¨ë„í‹°ë‚˜ í´ë¦¬í•‘ì„ í†µí•´ ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ëŠ”ë°, ì´ëŠ” ê²©ì°¨ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¤„ì´ê¸°ë³´ë‹¤ëŠ” ìˆ˜ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ì œí•œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í›ˆë ¨ ì „ì— ì •ì±… ê²©ì°¨ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¤„ì´ëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë°ì´í„° ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê° ë¬¸ì œì— ëŒ€í•´, ëª¨ë¸ì´ ìƒì„±í•œ ì˜¬ë°”ë¥¸ ì†”ë£¨ì…˜ì€ ì˜¨ ì •ì±… ë°ì´í„°ë¡œ ìœ ì§€ë˜ë©°, ì˜ëª»ëœ ì†”ë£¨ì…˜ì€ ì•ˆë‚´ëœ ì¬í•´ê²°ì„ í†µí•´ ì¬ì‘ì„±ë˜ê³ , í•„ìš”í•  ë•Œë§Œ ì „ë¬¸ê°€ ì‹œì—°ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ ë¶„í¬ë¥¼ ëª©í‘œ ì •ì±…ê³¼ ì¼ì¹˜ì‹œì¼œ ë¶„ì‚°ì„ ì¤„ì´ê³  ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì¬ì‘ì„± í›„ ì”ì—¬ ë¶ˆì¼ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í›ˆë ¨ ì¤‘ì— ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì¶”ê°€ë¡œ ì ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì¤€ì˜ ì •ë ¬ê³¼ ê°€ë²¼ìš´ ìµœì í™” ìˆ˜ì¤€ì˜ ìˆ˜ì •ì„ ê²°í•©í•œ 2ë‹¨ê³„ ì ‘ê·¼ ë°©ì‹ì„ í˜•ì„±í•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°œì˜ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ì€ ê¸°ë³¸ SFTì™€ ìµœì‹  ë™ì  ë¯¸ì„¸ ì¡°ì •(DFT) ì ‘ê·¼ ë°©ì‹ì„ ëª¨ë‘ ëŠ¥ê°€í•˜ëŠ” ì¼ê´€ë˜ê³  ìƒë‹¹í•œ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë°ì´í„°ì™€ ì½”ë“œëŠ” https://github.com/NKU-HLT/Off-Policy-SFTì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ìƒˆë¡œìš´ ë°ì´í„° ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ KL íŒ¨ë„í‹°ë‚˜ í´ë¦¬í•‘ì„ ì‚¬ìš©í•˜ì—¬ ì •ì±… ì°¨ì´ë¥¼ ìˆ˜ë™ì ìœ¼ë¡œ ì œí•œí–ˆì§€ë§Œ, ì œì•ˆëœ ë°©ë²•ì€ í›ˆë ¨ ì „ì— ì •ì±… ì°¨ì´ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ìƒì„±í•œ ì˜¬ë°”ë¥¸ ì†”ë£¨ì…˜ì€ ì˜¨-ì •ì±… ë°ì´í„°ë¡œ ìœ ì§€í•˜ê³ , ì˜ëª»ëœ ì†”ë£¨ì…˜ì€ ì „ë¬¸ê°€ ì‹œì—°ì— ì˜ì¡´í•˜ì—¬ ë‹¤ì‹œ í•´ê²°í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ í›ˆë ¨ ë¶„í¬ë¥¼ ëª©í‘œ ì •ì±…ê³¼ ì¼ì¹˜ì‹œì¼œ ë¶„ì‚°ì„ ì¤„ì´ê³  ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì¬ì‘ì„± í›„ ë‚¨ì€ ë¶ˆì¼ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì¶”ê°€ë¡œ ì ìš©í•˜ì—¬ ë°ì´í„° ì •ë ¬ê³¼ ìµœì í™” ìˆ˜ì¤€ì˜ ë³´ì •ì„ ê²°í•©í•œ 2ë‹¨ê³„ ì ‘ê·¼ë²•ì„ í˜•ì„±í•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°€ì§€ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ì¼ê´€ë˜ê³  ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë°ì´í„°ì™€ ì½”ë“œëŠ” https://github.com/NKU-HLT/Off-Policy-SFTì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT)ì€ ê³ ì •ëœ í–‰ë™ ì •ì±…ì—ì„œ ì „ë¬¸ê°€ ì‹œì—°ì„ ë°›ê³  ëª©í‘œ ì •ì±…ì„ ìµœì í™”í•˜ëŠ” ì˜¤í”„-ì •ì±… í•™ìŠµ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ KL íŒ¨ë„í‹°ë‚˜ í´ë¦¬í•‘ì„ ì‚¬ìš©í•˜ì—¬ ì •ì±… ê°„ì˜ ì°¨ì´ë¥¼ ìˆ˜ë™ì ìœ¼ë¡œ ì œí•œí•˜ì§€ë§Œ, ì´ëŠ” ì ê·¹ì ìœ¼ë¡œ ì°¨ì´ë¥¼ ì¤„ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” í›ˆë ¨ ì „ì— ì •ì±… ì°¨ì´ë¥¼ ëŠ¥ë™ì ìœ¼ë¡œ ì¤„ì´ëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë°ì´í„° ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ë°ì´í„° ì¬ì‘ì„± í›„ ë‚¨ì•„ìˆëŠ” ë¶ˆì¼ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì¤‘ ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ì¶”ê°€ë¡œ ì ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì¤€ì˜ ì •ë ¬ê³¼ ìµœì í™” ìˆ˜ì¤€ì˜ ê²½ëŸ‰ ìˆ˜ì •ì´ ê²°í•©ëœ ë‘ ë‹¨ê³„ ì ‘ê·¼ë²•ì„ í˜•ì„±í•©ë‹ˆë‹¤.
- 5. ë‹¤ì„¯ ê°€ì§€ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ì˜ SFT ë° ìµœì‹ ì˜ ë™ì  ë¯¸ì„¸ ì¡°ì •(DFT) ì ‘ê·¼ë²•ë³´ë‹¤ ì¼ê´€ë˜ê³  ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:13:37*