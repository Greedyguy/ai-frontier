---
keywords:
  - Synthetic Bootstrapped Pretraining
  - Large Language Model
  - Inter-document Correlations
  - Bayesian Interpretation
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15248
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:48:34.835179",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Synthetic Bootstrapped Pretraining",
    "Large Language Model",
    "Inter-document Correlations",
    "Bayesian Interpretation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Synthetic Bootstrapped Pretraining": 0.8,
    "Large Language Model": 0.85,
    "Inter-document Correlations": 0.78,
    "Bayesian Interpretation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Synthetic Bootstrapped Pretraining",
        "canonical": "Synthetic Bootstrapped Pretraining",
        "aliases": [
          "SBP"
        ],
        "category": "unique_technical",
        "rationale": "This is the core innovation of the paper, offering a novel approach to language model pretraining.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "language model",
        "canonical": "Large Language Model",
        "aliases": [
          "LM"
        ],
        "category": "broad_technical",
        "rationale": "Language models are central to the paper's methodology and connect to various related works in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "inter-document correlations",
        "canonical": "Inter-document Correlations",
        "aliases": [
          "document relations"
        ],
        "category": "specific_connectable",
        "rationale": "Understanding inter-document correlations is key to the proposed method's improvement over traditional pretraining.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Bayesian interpretation",
        "canonical": "Bayesian Interpretation",
        "aliases": [
          "Bayesian perspective"
        ],
        "category": "specific_connectable",
        "rationale": "The Bayesian interpretation provides a theoretical framework that could link to other probabilistic models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "pretraining setup",
      "performance improvement"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Synthetic Bootstrapped Pretraining",
      "resolved_canonical": "Synthetic Bootstrapped Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "language model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "inter-document correlations",
      "resolved_canonical": "Inter-document Correlations",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Bayesian interpretation",
      "resolved_canonical": "Bayesian Interpretation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Synthetic bootstrapped pretraining

**Korean Title:** í•©ì„± ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‚¬ì „í›ˆë ¨

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15248.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15248](https://arxiv.org/abs/2509.15248)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (80.6% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (79.6% similar)
- [[2025-09-22/Bayesian Concept Bottleneck Models with LLM Priors_20250922|Bayesian Concept Bottleneck Models with LLM Priors]] (78.5% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (78.5% similar)
- [[2025-09-19/From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery: Reinforced Distillation of Large Language Model Agents]] (78.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Inter-document Correlations|Inter-document Correlations]], [[keywords/Bayesian Interpretation|Bayesian Interpretation]]
**âš¡ Unique Technical**: [[keywords/Synthetic Bootstrapped Pretraining|Synthetic Bootstrapped Pretraining]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15248v1 Announce Type: cross 
Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15248v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ìš°ë¦¬ëŠ” ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³  ì´ë¥¼ í™œìš©í•˜ì—¬ ê³µë™ í•™ìŠµì„ ìœ„í•œ ë°©ëŒ€í•œ ìƒˆë¡œìš´ ì½”í¼ìŠ¤ë¥¼ í•©ì„±í•˜ëŠ” ì–¸ì–´ ëª¨ë¸(LM) ì‚¬ì „ í•™ìŠµ ì ˆì°¨ì¸ Synthetic Bootstrapped Pretraining (SBP)ì„ ì†Œê°œí•©ë‹ˆë‹¤. í‘œì¤€ ì‚¬ì „ í•™ìŠµì€ ë‹¨ì¼ ë¬¸ì„œ ë‚´ì—ì„œ í† í° ê°„ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ í•™ìŠµí•˜ë„ë¡ LMsë¥¼ êµìœ¡í•˜ì§€ë§Œ, ì´ëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” í’ë¶€í•˜ê³  í•™ìŠµ ê°€ëŠ¥í•œ ë¬¸ì„œ ê°„ ìƒê´€ ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ë„ë¡ ì„¤ê³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê³„ì‚°ì´ ì¼ì¹˜í•˜ëŠ” ì‚¬ì „ í•™ìŠµ ì„¤ì •ì„ ì„¤ê³„í•˜ê³  1ì¡° ê°œì˜ í† í°ì—ì„œ 3B-ë§¤ê°œë³€ìˆ˜ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° ì‚¬ì „ í•™ìŠµí•˜ì—¬ SBPë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. SBPëŠ” ê°•ë ¥í•œ ë°˜ë³µ ê¸°ì¤€ì„ ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ê³ , 20ë°° ë” ë§ì€ ê³ ìœ  ë°ì´í„°ë¥¼ ì´ìš©í•  ìˆ˜ ìˆëŠ” ì˜¤ë¼í´ ìƒí•œì„ ì— ì˜í•´ ë‹¬ì„± ê°€ëŠ¥í•œ ì„±ëŠ¥ í–¥ìƒì˜ ìƒë‹¹ ë¶€ë¶„ì„ ì œê³µí•©ë‹ˆë‹¤. ì§ˆì  ë¶„ì„ ê²°ê³¼ í•©ì„±ëœ ë¬¸ì„œê°€ ë‹¨ìˆœí•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. SBPëŠ” ì²˜ìŒì— ì‹œë“œ ìë£Œì—ì„œ í•µì‹¬ ê°œë…ì„ ì¶”ìƒí™”í•œ ë‹¤ìŒ ê·¸ ìœ„ì— ìƒˆë¡œìš´ ì„œìˆ ì„ ì‘ì„±í•©ë‹ˆë‹¤. ê°•ë ¥í•œ ê²½í—˜ì  ì„±ëŠ¥ ì™¸ì—ë„ SBPëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë² ì´ì§€ì•ˆ í•´ì„ì„ í—ˆìš©í•©ë‹ˆë‹¤: í•©ì„±ê¸°ëŠ” ê´€ë ¨ ë¬¸ì„œ ê°„ì— ê³µìœ ë˜ëŠ” ì ì¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ëŠ” ë²•ì„ ì•”ë¬µì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Synthetic Bootstrapped Pretraining (SBP)ëŠ” ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì´ë¥¼ í†µí•´ ì–¸ì–´ ëª¨ë¸ì„ ê³µë™ í•™ìŠµí•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì‚¬ì „ í•™ìŠµì€ ë‹¨ì¼ ë¬¸ì„œ ë‚´ì˜ ì¸ê³¼ì  ìƒê´€ê´€ê³„ì— ì¤‘ì ì„ ë‘ì§€ë§Œ, SBPëŠ” ë¬¸ì„œ ê°„ì˜ í’ë¶€í•œ ìƒê´€ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. SBPëŠ” 3B-íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ 1ì¡° í† í°ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ ê°œì„ ì„ í™•ì¸í–ˆìœ¼ë©°, ì´ëŠ” 20ë°° ë” ë§ì€ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì´ìƒì ì¸ ìƒí•œì„ ì˜ ì„±ëŠ¥ ê°œì„ ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. SBPëŠ” ë¬¸ì„œì˜ í•µì‹¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ê³  ìƒˆë¡œìš´ ì„œìˆ ì„ ì°½ì‘í•˜ë©°, ë² ì´ì§€ì•ˆ í•´ì„ì„ í†µí•´ ê´€ë ¨ ë¬¸ì„œ ê°„ì˜ ì ì¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ëŠ” í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Synthetic Bootstrapped Pretraining (SBP)ëŠ” ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ë¥¼ í•©ì„±í•˜ê³  ì´ë¥¼ ê³µë™ í•™ìŠµì— í™œìš©í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í•™ìŠµ ì ˆì°¨ì…ë‹ˆë‹¤.
- 2. SBPëŠ” ë‹¨ì¼ ë¬¸ì„œ ë‚´ í† í° ê°„ ì¸ê³¼ì  ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ê¸°ì¡´ ì‚¬ì „ í•™ìŠµê³¼ ë‹¬ë¦¬, ë¬¸ì„œ ê°„ì˜ í’ë¶€í•œ ìƒê´€ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 3. 3B-íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ìµœëŒ€ 1ì¡° ê°œì˜ í† í°ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµí•œ ê²°ê³¼, SBPëŠ” ê°•ë ¥í•œ ë°˜ë³µ ê¸°ë°˜ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê³ , 20ë°° ë” ë§ì€ ê³ ìœ  ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ìƒì ì¸ ìƒí•œì„ ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 4. SBPëŠ” ë‹¨ìˆœí•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ë„˜ì–´ í•µì‹¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ê³  ìƒˆë¡œìš´ ì„œìˆ ì„ êµ¬ì„±í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. SBPëŠ” ê´€ë ¨ ë¬¸ì„œ ê°„ì— ê³µìœ ë˜ëŠ” ì ì¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ëŠ” í•™ìŠµì„ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ ë² ì´ì§€ì•ˆ í•´ì„ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 08:48:34*