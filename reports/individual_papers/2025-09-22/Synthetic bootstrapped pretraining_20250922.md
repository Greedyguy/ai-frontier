# Synthetic bootstrapped pretraining

**Korean Title:** í•©ì„± ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‚¬ì „ í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Bayesian Interpretation in Pretraining

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (80.6% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (78.5% similar)
- [[2025-09-19/From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (78.5% similar)
- [[2025-09-18/Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (78.5% similar)
- [[2025-09-18/Patent Language Model Pretraining with ModernBERT_20250918|Patent Language Model Pretraining with ModernBERT]] (77.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15248v1 Announce Type: cross 
Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15248v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ìš”ì•½: ìš°ë¦¬ëŠ” ë¬¸ì„œë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµí•œ í›„, ì´ë¥¼ í™œìš©í•˜ì—¬ ê³µë™ í•™ìŠµì„ ìœ„í•œ ë°©ëŒ€í•œ ìƒˆë¡œìš´ ì½”í¼ìŠ¤ë¥¼ í•©ì„±í•˜ëŠ” ì–¸ì–´ ëª¨ë¸(LM) ì‚¬ì „ í•™ìŠµ ì ˆì°¨ì¸ Synthetic Bootstrapped Pretraining (SBP)ì„ ì†Œê°œí•©ë‹ˆë‹¤. í‘œì¤€ ì‚¬ì „ í•™ìŠµì€ LMsê°€ ë‹¨ì¼ ë¬¸ì„œ ë‚´ì—ì„œ í† í° ê°„ì˜ ì¸ê³¼ì  ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ë„ë¡ ê°€ë¥´ì¹˜ì§€ë§Œ, ì´ëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” í’ë¶€í•˜ê³  í•™ìŠµ ê°€ëŠ¥í•œ ë¬¸ì„œ ê°„ ìƒê´€ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ë„ë¡ ì„¤ê³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê³„ì‚°ì´ ì¼ì¹˜í•˜ëŠ” ì‚¬ì „ í•™ìŠµ ì„¤ì •ì„ ì„¤ê³„í•˜ê³ , 1ì¡° ê°œì˜ í† í°ì— ëŒ€í•´ ì²˜ìŒë¶€í„° 3B-íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•˜ì—¬ SBPë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. SBPëŠ” ê°•ë ¥í•œ ë°˜ë³µ ê¸°ì¤€ì„ ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ê°œì„ ì„ ì´ë£¨ë©°, 20ë°° ë” ë§ì€ ê³ ìœ  ë°ì´í„°ë¥¼ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆëŠ” ì˜¤ë¼í´ ìƒí•œì„ ì´ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í–¥ìƒì˜ ìƒë‹¹ ë¶€ë¶„ì„ ì œê³µí•©ë‹ˆë‹¤. ì§ˆì  ë¶„ì„ì„ í†µí•´ í•©ì„±ëœ ë¬¸ì„œë“¤ì´ ë‹¨ìˆœí•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SBPëŠ” ë¨¼ì € ì‹œë“œ ìë£Œì—ì„œ í•µì‹¬ ê°œë…ì„ ì¶”ìƒí™”í•œ í›„, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì„œìˆ ì„ ë§Œë“­ë‹ˆë‹¤. ê°•ë ¥í•œ ê²½í—˜ì  ì„±ëŠ¥ ì™¸ì—ë„, SBPëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë² ì´ì§€ì•ˆ í•´ì„ì„ í—ˆìš©í•©ë‹ˆë‹¤: í•©ì„±ê¸°ëŠ” ê´€ë ¨ ë¬¸ì„œë“¤ ê°„ì— ê³µìœ ë˜ëŠ” ì ì¬ ê°œë…ì„ ì¶”ìƒì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Synthetic Bootstrapped Pretraining(SBP)ì€ ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì´ë¥¼ í†µí•´ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê¸°ì¡´ ì‚¬ì „ í•™ìŠµì€ ë¬¸ì„œ ë‚´ í† í° ê°„ ì¸ê³¼ ê´€ê³„ì— ì§‘ì¤‘í•˜ì§€ë§Œ, SBPëŠ” ë¬¸ì„œ ê°„ ìƒê´€ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. 3B-íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ 1ì¡° í† í°ìœ¼ë¡œ í•™ìŠµí•œ ê²°ê³¼, SBPëŠ” ë°˜ë³µ ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë©°, 20ë°° ë§ì€ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì´ìƒì  ìƒí•œì„ ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. SBPëŠ” ë¬¸ì„œì˜ í•µì‹¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ê³  ìƒˆë¡œìš´ ì„œìˆ ì„ ìƒì„±í•˜ë©°, ë² ì´ì§€ì•ˆ í•´ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Synthetic Bootstrapped Pretraining(SBP)ëŠ” ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ìƒˆë¡œìš´ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ë¥¼ í•©ì„±í•˜ê³  ì´ë¥¼ ê³µë™ í•™ìŠµì— í™œìš©í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í•™ìŠµ ì ˆì°¨ì…ë‹ˆë‹¤.

- 2. SBPëŠ” ë‹¨ì¼ ë¬¸ì„œ ë‚´ì˜ ì¸ê³¼ì  ìƒê´€ê´€ê³„ í•™ìŠµì„ ë„˜ì–´ì„œ, ë¬¸ì„œ ê°„ì˜ í’ë¶€í•œ ìƒê´€ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ ë„ëª¨í•©ë‹ˆë‹¤.

- 3. SBPë¥¼ í†µí•´ 3B-íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ìµœëŒ€ 1ì¡° ê°œì˜ í† í°ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµí•œ ê²°ê³¼, ê°•ë ¥í•œ ë°˜ë³µ ê¸°ì¤€ì„ ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ê³ , 20ë°° ë” ë§ì€ ê³ ìœ  ë°ì´í„°ë¥¼ ê°€ì§„ ê²½ìš°ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í–¥ìƒì˜ ìƒë‹¹ ë¶€ë¶„ì„ ì œê³µí•©ë‹ˆë‹¤.

- 4. SBPë¡œ í•©ì„±ëœ ë¬¸ì„œëŠ” ë‹¨ìˆœí•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ë„˜ì–´ì„œ, í•µì‹¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì„œìˆ ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.

- 5. SBPëŠ” ê´€ë ¨ ë¬¸ì„œ ê°„ì— ê³µìœ ë˜ëŠ” ì ì¬ ê°œë…ì„ ì¶”ìƒí™”í•˜ëŠ” í•™ìŠµì„ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ ë² ì´ì§€ì•ˆ í•´ì„ì„ í—ˆìš©í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 13:48:57*