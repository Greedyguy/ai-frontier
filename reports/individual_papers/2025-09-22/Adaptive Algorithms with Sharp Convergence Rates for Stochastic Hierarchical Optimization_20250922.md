---
keywords:
  - Stochastic Hierarchical Optimization
  - Nonconvex-Strongly-Concave Minimax Optimization
  - Nonconvex-Strongly-Convex Bilevel Optimization
  - Momentum Normalization Technique
  - Deep Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15399
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:23:34.937980",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Stochastic Hierarchical Optimization",
    "Nonconvex-Strongly-Concave Minimax Optimization",
    "Nonconvex-Strongly-Convex Bilevel Optimization",
    "Momentum Normalization Technique",
    "Deep Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Stochastic Hierarchical Optimization": 0.78,
    "Nonconvex-Strongly-Concave Minimax Optimization": 0.77,
    "Nonconvex-Strongly-Convex Bilevel Optimization": 0.79,
    "Momentum Normalization Technique": 0.76,
    "Deep Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "stochastic hierarchical optimization",
        "canonical": "Stochastic Hierarchical Optimization",
        "aliases": [
          "stochastic optimization",
          "hierarchical optimization"
        ],
        "category": "unique_technical",
        "rationale": "This term captures the specific focus of the paper on optimization problems with stochastic elements and hierarchical structure, which is central to the proposed algorithms.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "nonconvex-strongly-concave minimax optimization",
        "canonical": "Nonconvex-Strongly-Concave Minimax Optimization",
        "aliases": [
          "minimax optimization",
          "nonconvex optimization"
        ],
        "category": "unique_technical",
        "rationale": "This term specifies a class of optimization problems addressed by the paper, highlighting the unique challenge of nonconvex and strongly-concave structures.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "nonconvex-strongly-convex bilevel optimization",
        "canonical": "Nonconvex-Strongly-Convex Bilevel Optimization",
        "aliases": [
          "bilevel optimization",
          "nonconvex optimization"
        ],
        "category": "unique_technical",
        "rationale": "This term identifies another specific optimization problem class tackled by the paper, emphasizing the hierarchical and nonconvex-strongly-convex nature.",
        "novelty_score": 0.72,
        "connectivity_score": 0.66,
        "specificity_score": 0.83,
        "link_intent_score": 0.79
      },
      {
        "surface": "momentum normalization technique",
        "canonical": "Momentum Normalization Technique",
        "aliases": [
          "momentum normalization",
          "normalization technique"
        ],
        "category": "specific_connectable",
        "rationale": "This technique is a key component of the proposed algorithms, providing a potential link to other works on optimization techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.76
      },
      {
        "surface": "deep learning tasks",
        "canonical": "Deep Learning",
        "aliases": [
          "DL tasks",
          "deep learning applications"
        ],
        "category": "broad_technical",
        "rationale": "Deep learning is a major application area for the proposed algorithms, facilitating connections to broader machine learning contexts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "adaptive algorithms",
      "convergence rates",
      "gradient noise"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "stochastic hierarchical optimization",
      "resolved_canonical": "Stochastic Hierarchical Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "nonconvex-strongly-concave minimax optimization",
      "resolved_canonical": "Nonconvex-Strongly-Concave Minimax Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "nonconvex-strongly-convex bilevel optimization",
      "resolved_canonical": "Nonconvex-Strongly-Convex Bilevel Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.66,
        "specificity": 0.83,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "momentum normalization technique",
      "resolved_canonical": "Momentum Normalization Technique",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.76
      }
    },
    {
      "candidate_surface": "deep learning tasks",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization

**Korean Title:** ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì˜ ë‚ ì¹´ë¡œìš´ ìˆ˜ë ´ ì†ë„ë¥¼ ê°–ì¶˜ í™•ë¥ ì  ê³„ì¸µ ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15399.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15399](https://arxiv.org/abs/2509.15399)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises_20250922|Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises]] (87.1% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (86.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (84.5% similar)
- [[2025-09-18/Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (83.4% similar)
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Momentum Normalization Technique|Momentum Normalization Technique]]
**âš¡ Unique Technical**: [[keywords/Stochastic Hierarchical Optimization|Stochastic Hierarchical Optimization]], [[keywords/Nonconvex-Strongly-Concave Minimax Optimization|Nonconvex-Strongly-Concave Minimax Optimization]], [[keywords/Nonconvex-Strongly-Convex Bilevel Optimization|Nonconvex-Strongly-Convex Bilevel Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15399v1 Announce Type: new 
Abstract: Hierarchical optimization refers to problems with interdependent decision variables and objectives, such as minimax and bilevel formulations. While various algorithms have been proposed, existing methods and analyses lack adaptivity in stochastic optimization settings: they cannot achieve optimal convergence rates across a wide spectrum of gradient noise levels without prior knowledge of the noise magnitude. In this paper, we propose novel adaptive algorithms for two important classes of stochastic hierarchical optimization problems: nonconvex-strongly-concave minimax optimization and nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound on the stochastic gradient noise. Notably, these rates are obtained without prior knowledge of the noise level, thereby enabling automatic adaptivity in both low and high-noise regimes. To our knowledge, this work provides the first adaptive and sharp convergence guarantees for stochastic hierarchical optimization. Our algorithm design combines the momentum normalization technique with novel adaptive parameter choices. Extensive experiments on synthetic and deep learning tasks demonstrate the effectiveness of our proposed algorithms.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15399v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ê³„ì¸µì  ìµœì í™”ëŠ” ì˜ì‚¬ ê²°ì • ë³€ìˆ˜ì™€ ëª©í‘œê°€ ìƒí˜¸ ì˜ì¡´ì ì¸ ë¬¸ì œë¥¼ ë‹¤ë£¨ë©°, ìµœì†ŒìµœëŒ€(minimax) ë° ì´ìˆ˜ì¤€(bilevel) ê³µì‹í™”ì™€ ê°™ì€ ë¬¸ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì œì•ˆë˜ì—ˆì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ê³¼ ë¶„ì„ì€ í™•ë¥ ì  ìµœì í™” í™˜ê²½ì—ì„œ ì ì‘ì„±ì„ ê²°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¦‰, ì¡ìŒì˜ í¬ê¸°ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ ë„“ì€ ë²”ìœ„ì˜ ê¸°ìš¸ê¸° ì¡ìŒ ìˆ˜ì¤€ì—ì„œ ìµœì ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ í´ë˜ìŠ¤ì˜ í™•ë¥ ì  ê³„ì¸µì  ìµœì í™” ë¬¸ì œì— ëŒ€í•œ ìƒˆë¡œìš´ ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤: ë¹„ë³¼ë¡-ê°•ë³¼ë¡ ìµœì†ŒìµœëŒ€ ìµœì í™” ë° ë¹„ë³¼ë¡-ê°•ë³¼ë¡ ì´ìˆ˜ì¤€ ìµœì í™”. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ê¸°ìš¸ê¸° ë…¸ë¦„ì— ëŒ€í•´ $T$ ë°˜ë³µì—ì„œ $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ì˜ ì˜ˆë¦¬í•œ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ì—¬ê¸°ì„œ $\bar{\sigma}$ëŠ” í™•ë¥ ì  ê¸°ìš¸ê¸° ì¡ìŒì˜ ìƒí•œì…ë‹ˆë‹¤. íŠ¹íˆ, ì´ëŸ¬í•œ ì†ë„ëŠ” ì¡ìŒ ìˆ˜ì¤€ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ ì–»ì–´ì§€ë©°, ì €ì¡ìŒ ë° ê³ ì¡ìŒ í™˜ê²½ ëª¨ë‘ì—ì„œ ìë™ ì ì‘ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ì´ ì—°êµ¬ëŠ” í™•ë¥ ì  ê³„ì¸µì  ìµœì í™”ì— ëŒ€í•œ ìµœì´ˆì˜ ì ì‘í˜• ë° ì˜ˆë¦¬í•œ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ëŠ” ëª¨ë©˜í…€ ì •ê·œí™” ê¸°ë²•ê³¼ ìƒˆë¡œìš´ ì ì‘í˜• ë§¤ê°œë³€ìˆ˜ ì„ íƒì„ ê²°í•©í•©ë‹ˆë‹¤. í•©ì„± ë° ì‹¬ì¸µ í•™ìŠµ ê³¼ì œì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³„ì¸µì  ìµœì í™” ë¬¸ì œì—ì„œì˜ ìƒˆë¡œìš´ ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ í™•ë¥ ì  ìµœì í™”ì—ì„œì˜ ì ì‘ì„±ì„ ê²°ì—¬í•˜ì—¬ ë‹¤ì–‘í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì—ì„œ ìµœì ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¹„ë³¼ë¡-ê°•ë³¼ë¡ ìµœì†ŒìµœëŒ€ ìµœì í™”ì™€ ë¹„ë³¼ë¡-ê°•ë³¼ë¡ ì´ìˆ˜ì¤€ ìµœì í™” ë¬¸ì œì— ëŒ€í•´ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ì— ëŒ€í•´ $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŠ” ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ë„ ìë™ ì ì‘ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ëŠ” ëª¨ë©˜í…€ ì •ê·œí™” ê¸°ë²•ê³¼ ìƒˆë¡œìš´ ì ì‘í˜• ë§¤ê°œë³€ìˆ˜ ì„ íƒì„ ê²°í•©í•˜ì—¬ ì´ë£¨ì–´ì¡Œìœ¼ë©°, ì‹¤í—˜ì„ í†µí•´ ê·¸ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³„ì¸µì  ìµœì í™” ë¬¸ì œì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¡ìŒ ìˆ˜ì¤€ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ ìµœì ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¹„ë³¼ë¡-ê°•í•œ ì˜¤ëª© ìµœì†Œê·¹ëŒ€í™” ë° ë¹„ë³¼ë¡-ê°•í•œ ë³¼ë¡ ì´ìˆ˜ì¤€ ìµœì í™” ë¬¸ì œì— ëŒ€í•œ ìƒˆë¡œìš´ ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ì¡ìŒ ìˆ˜ì¤€ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ë„ ìë™ ì ì‘ì„±ì„ ì œê³µí•˜ë©°, ìˆ˜ë ´ ì†ë„ëŠ” $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. ëª¨ë©˜í…€ ì •ê·œí™” ê¸°ë²•ê³¼ ìƒˆë¡œìš´ ì ì‘í˜• ë§¤ê°œë³€ìˆ˜ ì„ íƒì„ ê²°í•©í•˜ì—¬ ì„¤ê³„ëœ ì•Œê³ ë¦¬ì¦˜ì€ ê³„ì¸µì  ìµœì í™”ì˜ ì ì‘í˜• ë° ë‚ ì¹´ë¡œìš´ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. í•©ì„± ë° ë”¥ëŸ¬ë‹ ì‘ì—…ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:23:34*