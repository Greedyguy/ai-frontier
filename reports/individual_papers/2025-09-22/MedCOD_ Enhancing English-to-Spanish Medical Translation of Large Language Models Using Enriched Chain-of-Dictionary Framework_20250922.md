---
keywords:
  - Large Language Model
  - Unified Medical Language System
  - Medical Translation
  - Structured Prompting
  - LoRA-based Fine-tuning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.00934
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:10:08.356721",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Unified Medical Language System",
    "Medical Translation",
    "Structured Prompting",
    "LoRA-based Fine-tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Unified Medical Language System": 0.8,
    "Medical Translation": 0.78,
    "Structured Prompting": 0.77,
    "LoRA-based Fine-tuning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Model"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking advancements in language model applications.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Unified Medical Language System",
        "canonical": "Unified Medical Language System",
        "aliases": [
          "UMLS"
        ],
        "category": "unique_technical",
        "rationale": "Key domain-specific resource for medical translation, enhancing specificity.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Medical Translation",
        "canonical": "Medical Translation",
        "aliases": [
          "Healthcare Translation"
        ],
        "category": "unique_technical",
        "rationale": "Focus of the study, crucial for linking medical and linguistic domains.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Structured Prompting",
        "canonical": "Structured Prompting",
        "aliases": [
          "Prompt Engineering"
        ],
        "category": "specific_connectable",
        "rationale": "Important technique for improving LLM performance in specific tasks.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "LoRA-based Fine-tuning",
        "canonical": "LoRA-based Fine-tuning",
        "aliases": [
          "Low-Rank Adaptation Fine-tuning"
        ],
        "category": "unique_technical",
        "rationale": "Innovative fine-tuning method enhancing model adaptability and performance.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.88,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "structured knowledge",
      "performance gains"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Unified Medical Language System",
      "resolved_canonical": "Unified Medical Language System",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Medical Translation",
      "resolved_canonical": "Medical Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Structured Prompting",
      "resolved_canonical": "Structured Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "LoRA-based Fine-tuning",
      "resolved_canonical": "LoRA-based Fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.88,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework

**Korean Title:** MedCOD: ì‚¬ì „ ì²´ì¸ ê°•í™” í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ì˜ë£Œ ë²ˆì—­ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.00934.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.00934](https://arxiv.org/abs/2509.00934)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (83.3% similar)
- [[2025-09-22/OpenWHO_ A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages_20250922|OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages]] (82.1% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (82.0% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (81.9% similar)
- [[2025-09-22/Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation_20250922|Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Structured Prompting|Structured Prompting]]
**âš¡ Unique Technical**: [[keywords/Unified Medical Language System|Unified Medical Language System]], [[keywords/Medical Translation|Medical Translation]], [[keywords/LoRA-based Fine-tuning|LoRA-based Fine-tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.00934v2 Announce Type: replace-cross 
Abstract: We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed to improve English-to-Spanish medical translation by integrating domain-specific structured knowledge into large language models (LLMs). MedCOD integrates domain-specific knowledge from both the Unified Medical Language System (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance structured prompting and fine-tuning. We constructed a parallel corpus of 2,999 English-Spanish MedlinePlus articles and a 100-sentence test set annotated with structured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B, Qwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that incorporated multilingual variants, medical synonyms, and UMLS-derived definitions, combined with LoRA-based fine-tuning. Experimental results demonstrate that MedCOD significantly improves translation quality across all models. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23, chrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o and GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model adaptation independently contribute to performance gains, with their combination yielding the highest improvements. These findings highlight the potential of structured knowledge integration to enhance LLMs for medical translation tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.00934v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì— ë¶„ì•¼ë³„ êµ¬ì¡°í™”ëœ ì§€ì‹ì„ í†µí•©í•˜ì—¬ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ì˜í•™ ë²ˆì—­ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ì›Œí¬ì¸ MedCOD(ì˜í•™ ì‚¬ì „ ì²´ì¸)ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. MedCODëŠ” êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ì™€ ë¯¸ì„¸ ì¡°ì •ì„ ê°•í™”í•˜ê¸° ìœ„í•´ í†µí•© ì˜ë£Œ ì–¸ì–´ ì‹œìŠ¤í…œ(UMLS)ê³¼ LLM-ì§€ì‹-ê¸°ë°˜(LLM-KB) íŒ¨ëŸ¬ë‹¤ì„ ëª¨ë‘ì—ì„œ ë¶„ì•¼ë³„ ì§€ì‹ì„ í†µí•©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” 2,999ê°œì˜ ì˜ì–´-ìŠ¤í˜ì¸ì–´ MedlinePlus ê¸°ì‚¬ì™€ êµ¬ì¡°í™”ëœ ì˜ë£Œ ë§¥ë½ìœ¼ë¡œ ì£¼ì„ì´ ë‹¬ë¦° 100ë¬¸ì¥ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ë„¤ ê°€ì§€ ì˜¤í”ˆ ì†ŒìŠ¤ LLM(Phi-4, Qwen2.5-14B, Qwen2.5-7B, LLaMA-3.1-8B)ì€ ë‹¤êµ­ì–´ ë³€í˜•, ì˜í•™ì  ë™ì˜ì–´ ë° UMLSì—ì„œ íŒŒìƒëœ ì •ì˜ë¥¼ í†µí•©í•œ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ì™€ LoRA ê¸°ë°˜ì˜ ë¯¸ì„¸ ì¡°ì •ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” MedCODê°€ ëª¨ë“  ëª¨ë¸ì—ì„œ ë²ˆì—­ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, MedCODì™€ ë¯¸ì„¸ ì¡°ì •ì„ ì‚¬ìš©í•œ Phi-4ëŠ” BLEU 44.23, chrF++ 28.91, COMET 0.863ì„ ë‹¬ì„±í•˜ì—¬ GPT-4o ë° GPT-4o-miniì™€ ê°™ì€ ê°•ë ¥í•œ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. ì†Œê±° ì—°êµ¬ëŠ” MedCOD í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ì ì‘ì´ ëª¨ë‘ ë…ë¦½ì ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•˜ë©°, ì´ë“¤ì˜ ì¡°í•©ì´ ê°€ì¥ ë†’ì€ ê°œì„ ì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ êµ¬ì¡°í™”ëœ ì§€ì‹ í†µí•©ì´ ì˜í•™ ë²ˆì—­ ì‘ì—…ì„ ìœ„í•œ LLMì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

MedCOD(ì˜ë£Œ ì‚¬ì „ ì²´ì¸)ì€ ì˜ì–´ì—ì„œ ìŠ¤í˜ì¸ì–´ë¡œì˜ ì˜ë£Œ ë²ˆì—­ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ë„ë©”ì¸ íŠ¹í™” êµ¬ì¡°ì  ì§€ì‹ì„ í†µí•©í•œ í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. MedCODëŠ” UMLSì™€ LLM-KB íŒ¨ëŸ¬ë‹¤ì„ì˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ êµ¬ì¡°ì  í”„ë¡¬í”„íŠ¸ì™€ ë¯¸ì„¸ ì¡°ì •ì„ ê°•í™”í•©ë‹ˆë‹¤. 2,999ê°œì˜ ì˜ì–´-ìŠ¤í˜ì¸ì–´ MedlinePlus ê¸°ì‚¬ì™€ 100ë¬¸ì¥ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ êµ¬ì¶•í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼, MedCODëŠ” ëª¨ë“  ëª¨ë¸ì—ì„œ ë²ˆì—­ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ Phi-4 ëª¨ë¸ì€ BLEU 44.23, chrF++ 28.91, COMET 0.863ì„ ê¸°ë¡í•˜ë©° ê¸°ì¡´ ê°•ë ¥í•œ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” êµ¬ì¡°ì  ì§€ì‹ í†µí•©ì´ ì˜ë£Œ ë²ˆì—­ ì‘ì—…ì—ì„œ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MedCODëŠ” ì˜ì–´-ìŠ¤í˜ì¸ì–´ ì˜ë£Œ ë²ˆì—­ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë„ë©”ì¸ë³„ êµ¬ì¡°í™”ëœ ì§€ì‹ì„ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— í†µí•©í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. MedCODëŠ” UMLSì™€ LLM-KB íŒ¨ëŸ¬ë‹¤ì„ì˜ ë„ë©”ì¸ë³„ ì§€ì‹ì„ í†µí•©í•˜ì—¬ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ì™€ ë¯¸ì„¸ ì¡°ì •ì„ ê°•í™”í•©ë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, MedCODëŠ” ëª¨ë“  ëª¨ë¸ì—ì„œ ë²ˆì—­ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, íŠ¹íˆ Phi-4 ëª¨ë¸ì´ BLEU 44.23, chrF++ 28.91, COMET 0.863ì„ ê¸°ë¡í•˜ì—¬ ê°•ë ¥í•œ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.
- 4. MedCOD í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ì ì‘ì€ ê°ê° ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•˜ë©°, ë‘ ìš”ì†Œì˜ ê²°í•©ì´ ê°€ì¥ í° ê°œì„ ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
- 5. êµ¬ì¡°í™”ëœ ì§€ì‹ í†µí•©ì´ ì˜ë£Œ ë²ˆì—­ ì‘ì—…ì—ì„œ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:10:08*