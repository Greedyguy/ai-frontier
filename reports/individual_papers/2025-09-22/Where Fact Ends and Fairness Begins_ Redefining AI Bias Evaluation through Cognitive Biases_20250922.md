---
keywords:
  - AI Bias Evaluation
  - Cognitive Biases
  - Fact-or-Fair Benchmark
  - Representativeness Bias
  - Normative Fairness
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2502.05849
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:40:36.333374",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AI Bias Evaluation",
    "Cognitive Biases",
    "Fact-or-Fair Benchmark",
    "Representativeness Bias",
    "Normative Fairness"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AI Bias Evaluation": 0.82,
    "Cognitive Biases": 0.79,
    "Fact-or-Fair Benchmark": 0.85,
    "Representativeness Bias": 0.77,
    "Normative Fairness": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AI Bias Evaluation",
        "canonical": "AI Bias Evaluation",
        "aliases": [
          "Bias Assessment",
          "Bias Measurement"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's theme of distinguishing fact from fairness in AI outputs.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Cognitive Biases",
        "canonical": "Cognitive Biases",
        "aliases": [
          "Cognitive Distortions"
        ],
        "category": "specific_connectable",
        "rationale": "Links psychological concepts to AI fairness, enhancing interdisciplinary connections.",
        "novelty_score": 0.58,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "Fact-or-Fair Benchmark",
        "canonical": "Fact-or-Fair Benchmark",
        "aliases": [
          "Fact-or-Fair Test Suite"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel benchmark for evaluating AI models, crucial for the paper's contributions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Representativeness Bias",
        "canonical": "Representativeness Bias",
        "aliases": [
          "Representativeness Heuristic"
        ],
        "category": "specific_connectable",
        "rationale": "A specific cognitive bias discussed in the context of AI fairness, aiding in psychological linkage.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.76,
        "link_intent_score": 0.77
      },
      {
        "surface": "Normative Fairness",
        "canonical": "Normative Fairness",
        "aliases": [
          "Ethical Fairness"
        ],
        "category": "unique_technical",
        "rationale": "Differentiates from factual correctness, a key distinction in the paper.",
        "novelty_score": 0.63,
        "connectivity_score": 0.7,
        "specificity_score": 0.79,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "fairness evaluation",
      "AI models",
      "socially harmful"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AI Bias Evaluation",
      "resolved_canonical": "AI Bias Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Cognitive Biases",
      "resolved_canonical": "Cognitive Biases",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Fact-or-Fair Benchmark",
      "resolved_canonical": "Fact-or-Fair Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Representativeness Bias",
      "resolved_canonical": "Representativeness Bias",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.76,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Normative Fairness",
      "resolved_canonical": "Normative Fairness",
      "decision": "linked",
      "scores": {
        "novelty": 0.63,
        "connectivity": 0.7,
        "specificity": 0.79,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases

**Korean Title:** 사실의 경계와 공정성의 시작: 인지 편향을 통한 AI 편향 평가의 재정의

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.05849.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2502.05849](https://arxiv.org/abs/2502.05849)

## 🔗 유사한 논문
- [[2025-09-22/Algorithmic Fairness_ Not a Purely Technical but Socio-Technical Property_20250922|Algorithmic Fairness: Not a Purely Technical but Socio-Technical Property]] (85.5% similar)
- [[2025-09-22/The Psychology of Falsehood_ A Human-Centric Survey of Misinformation Detection_20250922|The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection]] (83.2% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (82.6% similar)
- [[2025-09-17/APFEx_ Adaptive Pareto Front Explorer for Intersectional Fairness_20250917|APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness]] (82.4% similar)
- [[2025-09-22/Understanding AI Evaluation Patterns_ How Different GPT Models Assess Vision-Language Descriptions_20250922|Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions]] (81.9% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Cognitive Biases|Cognitive Biases]], [[keywords/Representativeness Bias|Representativeness Bias]]
**⚡ Unique Technical**: [[keywords/AI Bias Evaluation|AI Bias Evaluation]], [[keywords/Fact-or-Fair Benchmark|Fact-or-Fair Benchmark]], [[keywords/Normative Fairness|Normative Fairness]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.05849v2 Announce Type: replace 
Abstract: Recent failures such as Google Gemini generating people of color in Nazi-era uniforms illustrate how AI outputs can be factually plausible yet socially harmful. AI models are increasingly evaluated for "fairness," yet existing benchmarks often conflate two fundamentally different dimensions: factual correctness and normative fairness. A model may generate responses that are factually accurate but socially unfair, or conversely, appear fair while distorting factual reality. We argue that identifying the boundary between fact and fair is essential for meaningful fairness evaluation. We introduce Fact-or-Fair, a benchmark with (i) objective queries aligned with descriptive, fact-based judgments, and (ii) subjective queries aligned with normative, fairness-based judgments. Our queries are constructed from 19 statistics and are grounded in cognitive psychology, drawing on representativeness bias, attribution bias, and ingroup-outgroup bias to explain why models often misalign fact and fairness. Experiments across ten frontier models reveal different levels of fact-fair trade-offs. By reframing fairness evaluation, we provide both a new theoretical lens and a practical benchmark to advance the responsible model assessments. Our test suite is publicly available at https://github.com/uclanlp/Fact-or-Fair.

## 🔍 Abstract (한글 번역)

arXiv:2502.05849v2 발표 유형: 교체  
초록: 구글 제미니가 나치 시대의 유니폼을 입은 유색 인종을 생성하는 최근의 실패 사례는 AI 출력이 사실적으로 그럴듯하면서도 사회적으로 해로울 수 있음을 보여줍니다. AI 모델은 점점 더 "공정성"에 대해 평가되고 있지만, 기존의 벤치마크는 종종 두 가지 근본적으로 다른 차원인 사실적 정확성과 규범적 공정성을 혼동합니다. 모델은 사실적으로 정확하지만 사회적으로 불공정한 응답을 생성할 수 있으며, 반대로 사실적 현실을 왜곡하면서 공정해 보일 수도 있습니다. 우리는 사실과 공정성의 경계를 식별하는 것이 의미 있는 공정성 평가에 필수적이라고 주장합니다. 우리는 (i) 기술적, 사실 기반 판단에 맞춘 객관적 질문과 (ii) 규범적, 공정성 기반 판단에 맞춘 주관적 질문을 포함하는 Fact-or-Fair라는 벤치마크를 소개합니다. 우리의 질문은 19개의 통계 자료로 구성되어 있으며, 인지 심리학에 기반을 두고 대표성 편향, 귀인 편향, 내집단-외집단 편향을 활용하여 모델이 왜 사실과 공정성을 종종 잘못 정렬하는지를 설명합니다. 10개의 최첨단 모델에 대한 실험은 사실-공정성 간의 다양한 트레이드오프 수준을 드러냅니다. 공정성 평가를 재구성함으로써, 우리는 책임 있는 모델 평가를 진전시키기 위한 새로운 이론적 관점과 실용적인 벤치마크를 제공합니다. 우리의 테스트 스위트는 https://github.com/uclanlp/Fact-or-Fair에서 공개적으로 이용 가능합니다.

## 📝 요약

이 논문은 AI 모델의 공정성 평가에서 사실적 정확성과 규범적 공정성을 구분하는 것이 중요하다고 주장합니다. 기존의 평가는 이 두 차원을 혼동하여, 사실적으로 정확하지만 사회적으로 불공정한 결과를 초래할 수 있습니다. 이를 해결하기 위해, 저자들은 Fact-or-Fair라는 벤치마크를 제안합니다. 이 벤치마크는 사실 기반의 객관적 질문과 공정성 기반의 주관적 질문으로 구성되어 있으며, 인지 심리학의 편향 이론을 활용해 모델이 사실과 공정성을 혼동하는 이유를 설명합니다. 실험 결과, 다양한 모델들이 사실과 공정성 간의 상충 관계를 다르게 나타냄을 확인했습니다. 이 연구는 공정성 평가를 재구성하여 책임 있는 모델 평가를 위한 새로운 이론적 관점과 실용적 도구를 제공합니다.

## 🎯 주요 포인트

- 1. AI 모델의 출력은 사실적으로 그럴듯하지만 사회적으로 해로울 수 있으며, 이는 공정성 평가의 중요성을 강조합니다.
- 2. 기존의 공정성 벤치마크는 사실적 정확성과 규범적 공정성을 혼동하는 경향이 있습니다.
- 3. Fact-or-Fair 벤치마크는 사실 기반의 객관적 쿼리와 공정성 기반의 주관적 쿼리를 통해 사실과 공정성의 경계를 식별합니다.
- 4. 인지 심리학에 기반한 쿼리는 대표성 편향, 귀인 편향, 내집단-외집단 편향을 활용하여 모델의 사실과 공정성 불일치를 설명합니다.
- 5. 새로운 이론적 관점과 실용적 벤치마크를 제공하여 책임 있는 모델 평가를 촉진합니다.


---

*Generated on 2025-09-23 11:40:36*