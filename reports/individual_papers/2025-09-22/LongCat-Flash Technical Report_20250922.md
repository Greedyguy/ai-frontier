---
keywords:
  - Mixture-of-Experts
  - Zero-computation Experts
  - Shortcut-connected Mixture-of-Experts
  - Scalable Efficiency
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.01322
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:10:31.422239",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Zero-computation Experts",
    "Shortcut-connected Mixture-of-Experts",
    "Scalable Efficiency"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.82,
    "Zero-computation Experts": 0.78,
    "Shortcut-connected Mixture-of-Experts": 0.77,
    "Scalable Efficiency": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "Mixture-of-Experts models are a significant architectural choice in machine learning, facilitating connections with other models using similar architectures.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Zero-computation Experts",
        "canonical": "Zero-computation Experts",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This novel design within the LongCat-Flash model is unique and could be pivotal for understanding its efficiency innovations.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "Shortcut-connected MoE",
        "canonical": "Shortcut-connected Mixture-of-Experts",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This specific architectural enhancement is critical for the model's efficiency and provides a unique linking opportunity.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "scalable efficiency",
        "canonical": "Scalable Efficiency",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Scalable efficiency is a broad technical concept that underpins many advancements in large-scale models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "computational efficiency",
      "advanced agentic capabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Zero-computation Experts",
      "resolved_canonical": "Zero-computation Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Shortcut-connected MoE",
      "resolved_canonical": "Shortcut-connected Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "scalable efficiency",
      "resolved_canonical": "Scalable Efficiency",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# LongCat-Flash Technical Report

**Korean Title:** ë¡±ìº£-í”Œë˜ì‹œ ê¸°ìˆ  ë³´ê³ ì„œ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.01322.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.01322](https://arxiv.org/abs/2509.01322)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment]] (80.8% similar)
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (80.8% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (80.1% similar)
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (79.8% similar)
- [[2025-09-18/CSMoE_ An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts_20250918|CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts]] (79.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Scalable Efficiency|Scalable Efficiency]]
**ğŸ”— Specific Connectable**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]]
**âš¡ Unique Technical**: [[keywords/Zero-computation Experts|Zero-computation Experts]], [[keywords/Shortcut-connected Mixture-of-Experts|Shortcut-connected Mixture-of-Experts]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.01322v2 Announce Type: replace-cross 
Abstract: We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.01322v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´

ì´ˆë¡: ìš°ë¦¬ëŠ” ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ê³ ê¸‰ ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ ëª¨ë‘ ê°–ì¶˜ 5600ì–µ ë§¤ê°œë³€ìˆ˜ì˜ ì „ë¬¸ê°€ í˜¼í•©(MoE) ì–¸ì–´ ëª¨ë¸ì¸ LongCat-Flashë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. í™•ì¥ ê°€ëŠ¥í•œ íš¨ìœ¨ì„±ì˜ í•„ìš”ì„±ì—ì„œ ì¶œë°œí•˜ì—¬, LongCat-FlashëŠ” ë‘ ê°€ì§€ ìƒˆë¡œìš´ ì„¤ê³„ë¥¼ ì±„íƒí•©ë‹ˆë‹¤: (a) ì œë¡œ ê³„ì‚° ì „ë¬¸ê°€, ì´ëŠ” ë™ì  ê³„ì‚° ì˜ˆì‚° í• ë‹¹ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ë¬¸ë§¥ì  ìš”êµ¬ì— ë”°ë¼ í† í°ë‹¹ 18.6B-31.3B(í‰ê·  27B)ë¥¼ í™œì„±í™”í•˜ì—¬ ìì› ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤. (b) ìˆì»· ì—°ê²° MoE, ì´ëŠ” ê³„ì‚°-í†µì‹  ì¤‘ì²© ì°½ì„ í™•ì¥í•˜ì—¬, ìœ ì‚¬í•œ ê·œëª¨ì˜ ëª¨ë¸ì— ë¹„í•´ ì¶”ë¡  íš¨ìœ¨ì„±ê³¼ ì²˜ë¦¬ëŸ‰ì—ì„œ ì£¼ëª©í•  ë§Œí•œ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€í˜• ëª¨ë¸ì„ ìœ„í•œ í¬ê´„ì ì¸ í™•ì¥ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí•˜ì—¬, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ì´, ëª¨ë¸ ì„±ì¥ ì´ˆê¸°í™”, ë‹¤ê°ì  ì•ˆì •ì„± ìŠ¤ìœ„íŠ¸, ê²°ì •ë¡ ì  ê³„ì‚°ì„ ê²°í•©í•˜ì—¬ ì•ˆì •ì ì´ê³  ì¬í˜„ ê°€ëŠ¥í•œ í›ˆë ¨ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. íŠ¹íˆ, í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì™€ ì¸í”„ë¼ ë…¸ë ¥ ê°„ì˜ ì‹œë„ˆì§€ë¥¼ í™œìš©í•˜ì—¬, 30ì¼ ì´ë‚´ì— 20ì¡° ì´ìƒì˜ í† í°ì— ëŒ€í•œ ëª¨ë¸ í›ˆë ¨ì„ ì™„ë£Œí•˜ê³ , ì´ˆë‹¹ 100ê°œ ì´ìƒì˜ í† í°(TPS)ì„ ì´ˆë‹¹ 0.70ë‹¬ëŸ¬ì˜ ë¹„ìš©ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ì¶œë ¥ í† í° ë°±ë§Œ ê°œë‹¹ ë¹„ìš©ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. LongCat-Flashë¥¼ ì—ì´ì „íŠ¸ ì§€ëŠ¥ìœ¼ë¡œ ë°œì „ì‹œí‚¤ê¸° ìœ„í•´, ìµœì í™”ëœ í˜¼í•©ë¬¼ì— ëŒ€í•œ ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ì„ ìˆ˜í–‰í•˜ê³ , ì¶”ë¡ , ì½”ë“œ ë° ì§€ì¹¨ì— ëŒ€í•œ ëª©í‘œ ì¤‘ê°„ ë° í›„ì† í›ˆë ¨ì„ ìˆ˜í–‰í•˜ë©°, í•©ì„± ë°ì´í„° ë° ë„êµ¬ ì‚¬ìš© ì‘ì—…ìœ¼ë¡œ ì¶”ê°€ ê°•í™”í•©ë‹ˆë‹¤. í¬ê´„ì ì¸ í‰ê°€ ê²°ê³¼, LongCat-FlashëŠ” ë¹„ì‚¬ê³  ê¸°ë°˜ ëª¨ë¸ë¡œì„œ ë‹¤ë¥¸ ì„ ë„ì ì¸ ëª¨ë¸ë“¤ ì‚¬ì´ì—ì„œ ë§¤ìš° ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ì œê³µí•˜ë©°, ì—ì´ì „íŠ¸ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ê°•ì ì„ ë³´ì…ë‹ˆë‹¤. LongCat-Flashì˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ëŠ” ì»¤ë®¤ë‹ˆí‹° ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.

LongCat Chat: https://longcat.ai  
Hugging Face: https://huggingface.co/meituan-longcat  
GitHub: https://github.com/meituan-longcat

## ğŸ“ ìš”ì•½

LongCat-FlashëŠ” 5600ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ Mixture-of-Experts(MoE) ì–¸ì–´ ëª¨ë¸ë¡œ, ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ê³ ê¸‰ ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ ëª©í‘œë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” (a) Zero-computation Expertsë¥¼ í†µí•´ ë¬¸ë§¥ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê³„ì‚° ìì›ì„ í• ë‹¹í•˜ì—¬ í‰ê·  27ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ í™œì„±í™”í•˜ê³ , (b) Shortcut-connected MoEë¥¼ í†µí•´ ê³„ì‚°ê³¼ í†µì‹ ì˜ ì¤‘ì²©ì„ í™•ëŒ€í•˜ì—¬ ì¶”ë¡  íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¨ ì ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ 30ì¼ ë‚´ì— 20ì¡° ê°œ ì´ìƒì˜ í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, 1ì´ˆë‹¹ 100ê°œ ì´ìƒì˜ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LongCat-FlashëŠ” ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ê³¼ ì¶”ê°€ì ì¸ ì½”ë“œ ë° ëª…ë ¹ì–´ í•™ìŠµì„ í†µí•´ ì—ì´ì „íŠ¸ ì§€ëŠ¥ì„ ê°•í™”í•˜ì˜€ìœ¼ë©°, í‰ê°€ ê²°ê³¼ ë‹¤ë¥¸ ì„ ë„ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë˜ì–´ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì˜ ë°œì „ì„ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LongCat-FlashëŠ” 5600ì–µ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ Mixture-of-Experts(MoE) ì–¸ì–´ ëª¨ë¸ë¡œ, ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ê³ ê¸‰ ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ ëª©í‘œë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 2. Zero-computation Experts ì„¤ê³„ë¥¼ í†µí•´ ë¬¸ë§¥ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê³„ì‚° ìì›ì„ í• ë‹¹í•˜ì—¬ í‰ê·  270ì–µ ë§¤ê°œë³€ìˆ˜ë¥¼ í™œì„±í™”í•˜ê³  ìì› ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.
- 3. Shortcut-connected MoE ì„¤ê³„ëŠ” ê³„ì‚°-í†µì‹  ì¤‘ì²© ì°½ì„ í™•ì¥í•˜ì—¬ ìœ ì‚¬ ê·œëª¨ ëª¨ë¸ ëŒ€ë¹„ ì¶”ë¡  íš¨ìœ¨ì„±ê³¼ ì²˜ë¦¬ëŸ‰ì—ì„œ ìœ ì˜ë¯¸í•œ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. 30ì¼ ë‚´ì— 20ì¡° ê°œ ì´ìƒì˜ í† í°ì„ í•™ìŠµí•˜ê³ , ì´ˆë‹¹ 100ê°œ ì´ìƒì˜ í† í°ì„ ì¶”ë¡ í•˜ë©°, ë°±ë§Œ ê°œ ì¶œë ¥ í† í°ë‹¹ \$0.70ì˜ ë¹„ìš©ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 5. LongCat-FlashëŠ” ì—ì´ì „íŠ¸ ì§€ëŠ¥ì„ ëª©í‘œë¡œ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµê³¼ ì¤‘ê°„ ë° í›„ì† í•™ìŠµì„ í†µí•´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, íŠ¹íˆ ì—ì´ì „íŠ¸ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ê°•ì ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:10:31*