---
keywords:
  - Double Descent
  - Quantum Kernel Methods
  - Random Matrix Theory
  - Linear Regression
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2501.10077
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:18:19.983495",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Double Descent",
    "Quantum Kernel Methods",
    "Random Matrix Theory",
    "Linear Regression"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Double Descent": 0.78,
    "Quantum Kernel Methods": 0.77,
    "Random Matrix Theory": 0.8,
    "Linear Regression": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "double descent",
        "canonical": "Double Descent",
        "aliases": [
          "double descent phenomenon"
        ],
        "category": "unique_technical",
        "rationale": "Double Descent is a key concept in understanding model performance in overparameterized regimes, relevant for linking with statistical learning theory.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "quantum kernel methods",
        "canonical": "Quantum Kernel Methods",
        "aliases": [
          "quantum kernel",
          "quantum feature spaces"
        ],
        "category": "unique_technical",
        "rationale": "Quantum Kernel Methods are central to the paper's exploration of quantum machine learning, offering a unique perspective on model behavior.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      },
      {
        "surface": "random matrix theory",
        "canonical": "Random Matrix Theory",
        "aliases": [
          "RMT"
        ],
        "category": "specific_connectable",
        "rationale": "Random Matrix Theory provides theoretical insights into the behavior of large systems, linking to statistical properties in machine learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "linear regression models",
        "canonical": "Linear Regression",
        "aliases": [
          "linear models"
        ],
        "category": "broad_technical",
        "rationale": "Linear Regression is a foundational technique in both classical and quantum contexts, facilitating connections with broader statistical methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "test error peak",
      "real-world datasets",
      "system sizes"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "double descent",
      "resolved_canonical": "Double Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "quantum kernel methods",
      "resolved_canonical": "Quantum Kernel Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "random matrix theory",
      "resolved_canonical": "Random Matrix Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "linear regression models",
      "resolved_canonical": "Linear Regression",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Double descent in quantum kernel methods

**Korean Title:** ì–‘ì ì»¤ë„ ë°©ë²•ì—ì„œì˜ ì´ì¤‘ í•˜ê°•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2501.10077.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2501.10077](https://arxiv.org/abs/2501.10077)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Learning quantum many-body data locally_ A provably scalable framework_20250917|Learning quantum many-body data locally: A provably scalable framework]] (85.6% similar)
- [[2025-09-19/Trainability of Quantum Models Beyond Known Classical Simulability_20250919|Trainability of Quantum Models Beyond Known Classical Simulability]] (83.4% similar)
- [[2025-09-22/Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning_20250922|Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning]] (83.1% similar)
- [[2025-09-17/Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment_20250917|Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment]] (82.6% similar)
- [[2025-09-22/Efficient Learning for Linear Properties of Bounded-Gate Quantum Circuits_20250922|Efficient Learning for Linear Properties of Bounded-Gate Quantum Circuits]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Linear Regression|Linear Regression]]
**ğŸ”— Specific Connectable**: [[keywords/Random Matrix Theory|Random Matrix Theory]]
**âš¡ Unique Technical**: [[keywords/Double Descent|Double Descent]], [[keywords/Quantum Kernel Methods|Quantum Kernel Methods]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.10077v2 Announce Type: replace-cross 
Abstract: The double descent phenomenon challenges traditional statistical learning theory by revealing scenarios where larger models do not necessarily lead to reduced performance on unseen data. While this counterintuitive behavior has been observed in a variety of classical machine learning models, particularly modern neural network architectures, it remains elusive within the context of quantum machine learning. In this work, we analytically demonstrate that linear regression models in quantum feature spaces can exhibit double descent behavior by drawing on insights from classical linear regression and random matrix theory. Additionally, our numerical experiments on quantum kernel methods across different real-world datasets and system sizes further confirm the existence of a test error peak, a characteristic feature of double descent. Our findings provide evidence that quantum models can operate in the modern, overparameterized regime without experiencing overfitting, potentially opening pathways to improved learning performance beyond traditional statistical learning theory.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2501.10077v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ë”ë¸” ë””ì„¼íŠ¸ í˜„ìƒì€ ë” í° ëª¨ë¸ì´ ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°˜ë“œì‹œ ì´ˆë˜í•˜ì§€ ì•ŠëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë“œëŸ¬ë‚´ë©´ì„œ ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì— ë„ì „í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§ê´€ì— ë°˜í•˜ëŠ” í–‰ë™ì€ ë‹¤ì–‘í•œ ê³ ì „ì  ê¸°ê³„ í•™ìŠµ ëª¨ë¸, íŠ¹íˆ í˜„ëŒ€ì˜ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œ ê´€ì°°ë˜ì—ˆì§€ë§Œ, ì–‘ì ê¸°ê³„ í•™ìŠµì˜ ë§¥ë½ì—ì„œëŠ” ì—¬ì „íˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ ì „ì  ì„ í˜• íšŒê·€ì™€ ëœë¤ í–‰ë ¬ ì´ë¡ ì˜ í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ ì–‘ì íŠ¹ì§• ê³µê°„ì—ì„œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ë”ë¸” ë””ì„¼íŠ¸ í–‰ë™ì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ë¶„ì„ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ ì‹œìŠ¤í…œ í¬ê¸°ì— ê±¸ì³ ì–‘ì ì»¤ë„ ë°©ë²•ì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì€ ë”ë¸” ë””ì„¼íŠ¸ì˜ íŠ¹ì§•ì ì¸ íŠ¹ì„±ì¸ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ í”¼í¬ì˜ ì¡´ì¬ë¥¼ ì¶”ê°€ë¡œ í™•ì¸í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ì–‘ì ëª¨ë¸ì´ ê³¼ì í•©ì„ ê²½í—˜í•˜ì§€ ì•Šê³  í˜„ëŒ€ì˜ ê³¼ë§¤ê°œë³€ìˆ˜í™”ëœ ì²´ì œì—ì„œ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì„ ë„˜ì–´ì„  í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì˜ ê²½ë¡œë¥¼ ì—´ì–´ì¤„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–‘ì ê¸°ê³„ í•™ìŠµì—ì„œ 'ë”ë¸” ë””ì„¼íŠ¸' í˜„ìƒì„ ë¶„ì„í•©ë‹ˆë‹¤. ì „í†µì ì¸ í†µê³„ í•™ìŠµ ì´ë¡ ì—ì„œëŠ” ëª¨ë¸ì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ê³  ë³´ì§€ë§Œ, ë”ë¸” ë””ì„¼íŠ¸ëŠ” ì´ì™€ ë°˜ëŒ€ë˜ëŠ” ê²½ìš°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì €ìë“¤ì€ ì–‘ì íŠ¹ì§• ê³µê°„ì—ì„œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ì´ëŸ¬í•œ í˜„ìƒì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ ì‹œìŠ¤í…œ í¬ê¸°ì—ì„œ ì–‘ì ì»¤ë„ ë°©ë²•ì„ í†µí•´ ì´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–‘ì ëª¨ë¸ì´ ê³¼ì í•© ì—†ì´ ê³¼ë§¤ê°œë³€ìˆ˜í™”ëœ ìƒíƒœì—ì„œë„ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì „í†µì ì¸ í•™ìŠµ ì´ë¡ ì„ ë„˜ì–´ì„  ì„±ëŠ¥ í–¥ìƒì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë”ë¸” ë””ì„¼íŠ¸ í˜„ìƒì€ ë” í° ëª¨ë¸ì´ í•­ìƒ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì—ì„œ ì „í†µì ì¸ í†µê³„ì  í•™ìŠµ ì´ë¡ ì— ë„ì „í•©ë‹ˆë‹¤.
- 2. ì–‘ì íŠ¹ì„± ê³µê°„ì—ì„œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ë”ë¸” ë””ì„¼íŠ¸ í˜„ìƒì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ë¶„ì„ì ìœ¼ë¡œ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 3. ë‹¤ì–‘í•œ ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ ì‹œìŠ¤í…œ í¬ê¸°ì—ì„œ ì–‘ì ì»¤ë„ ë°©ë²•ì„ ì‚¬ìš©í•œ ì‹¤í—˜ì„ í†µí•´ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ í”¼í¬ê°€ ì¡´ì¬í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- 4. ì–‘ì ëª¨ë¸ì´ ê³¼ì í•© ì—†ì´ í˜„ëŒ€ì˜ ê³¼ë§¤ê°œë³€ìˆ˜í™”ëœ í™˜ê²½ì—ì„œ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- 5. ì´ëŸ¬í•œ ë°œê²¬ì€ ì „í†µì ì¸ í†µê³„ì  í•™ìŠµ ì´ë¡ ì„ ë„˜ì–´ì„  í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì˜ ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:18:19*