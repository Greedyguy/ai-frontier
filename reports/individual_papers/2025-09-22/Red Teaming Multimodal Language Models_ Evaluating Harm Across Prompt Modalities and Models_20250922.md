---
keywords:
  - Multimodal Learning
  - Adversarial Attacks
  - Ethical AI
  - AI Safety
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15478
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:29:19.503693",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Adversarial Attacks",
    "Ethical AI",
    "AI Safety"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Adversarial Attacks": 0.77,
    "Ethical AI": 0.78,
    "AI Safety": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal large language models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to the growing field of integrating multiple data modalities in language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "adversarial prompts",
        "canonical": "Adversarial Attacks",
        "aliases": [
          "adversarial inputs"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the challenges in model robustness against adversarial inputs.",
        "novelty_score": 0.68,
        "connectivity_score": 0.79,
        "specificity_score": 0.81,
        "link_intent_score": 0.77
      },
      {
        "surface": "harm categories",
        "canonical": "Ethical AI",
        "aliases": [
          "AI ethics",
          "harm assessment"
        ],
        "category": "evolved_concepts",
        "rationale": "Relates to the ethical implications of AI outputs, a critical area of study.",
        "novelty_score": 0.72,
        "connectivity_score": 0.76,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "safety mechanisms",
        "canonical": "AI Safety",
        "aliases": [
          "safety protocols",
          "safety measures"
        ],
        "category": "broad_technical",
        "rationale": "Addresses the implementation of safety protocols in AI systems.",
        "novelty_score": 0.5,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "model outputs",
      "statistical analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal large language models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "adversarial prompts",
      "resolved_canonical": "Adversarial Attacks",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.79,
        "specificity": 0.81,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "harm categories",
      "resolved_canonical": "Ethical AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.76,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "safety mechanisms",
      "resolved_canonical": "AI Safety",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models

**Korean Title:** ë‹¤ì¤‘ ëª¨ë‹¬ ì–¸ì–´ ëª¨ë¸ì˜ ë ˆë“œ íŒ€ ì‘ì—…: í”„ë¡¬í”„íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì™€ ëª¨ë¸ ì „ë°˜ì— ê±¸ì¹œ í•´ì•… í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15478.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15478](https://arxiv.org/abs/2509.15478)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection_20250918|Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection]] (86.1% similar)
- [[2025-09-22/From Judgment to Interference_ Early Stopping LLM Harmful Outputs via Streaming Content Monitoring_20250922|From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring]] (85.7% similar)
- [[2025-09-22/Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study_20250922|Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study]] (85.1% similar)
- [[2025-09-22/Toxicity Red-Teaming_ Benchmarking LLM Safety in Singapore's Low-Resource Languages_20250922|Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages]] (84.5% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/AI Safety|AI Safety]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Adversarial Attacks|Adversarial Attacks]]
**ğŸš€ Evolved Concepts**: [[keywords/Ethical AI|Ethical AI]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15478v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly used in real world applications, yet their safety under adversarial conditions remains underexplored. This study evaluates the harmlessness of four leading MLLMs (GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to adversarial prompts across text-only and multimodal formats. A team of 26 red teamers generated 726 prompts targeting three harm categories: illegal activity, disinformation, and unethical behaviour. These prompts were submitted to each model, and 17 annotators rated 2,904 model outputs for harmfulness using a 5-point scale. Results show significant differences in vulnerability across models and modalities. Pixtral 12B exhibited the highest rate of harmful responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%). Contrary to expectations, text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones. Statistical analysis confirmed that both model type and input modality were significant predictors of harmfulness. These findings underscore the urgent need for robust, multimodal safety benchmarks as MLLMs are deployed more widely.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15478v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ì‹¤ì œ ì‘ìš©ì—ì„œ ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì ëŒ€ì  ì¡°ê±´ì—ì„œì˜ ì•ˆì „ì„±ì€ ì—¬ì „íˆ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” í…ìŠ¤íŠ¸ ì „ìš© ë° ë‹¤ì¤‘ ëª¨ë‹¬ í˜•ì‹ì—ì„œ ì ëŒ€ì  í”„ë¡¬í”„íŠ¸ì— ë…¸ì¶œë  ë•Œ ë„¤ ê°€ì§€ ì£¼ìš” MLLMs(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, Qwen VL Plus)ì˜ ë¬´í•´ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. 26ëª…ì˜ ë ˆë“œ íŒ€ì›ì´ ë¶ˆë²• í™œë™, í—ˆìœ„ ì •ë³´, ë¹„ìœ¤ë¦¬ì  í–‰ë™ì˜ ì„¸ ê°€ì§€ í•´ì•… ë²”ì£¼ë¥¼ ëª©í‘œë¡œ 726ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í”„ë¡¬í”„íŠ¸ëŠ” ê° ëª¨ë¸ì— ì œì¶œë˜ì—ˆìœ¼ë©°, 17ëª…ì˜ ì£¼ì„ìê°€ 5ì  ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ 2,904ê°œì˜ ëª¨ë¸ ì¶œë ¥ì„ ìœ í•´ì„± ì¸¡ë©´ì—ì„œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ëª¨ë¸ ë° ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ì·¨ì•½ì„±ì— ìˆì–´ ìœ ì˜ë¯¸í•œ ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Pixtral 12BëŠ” ê°€ì¥ ë†’ì€ ìœ í•´ ì‘ë‹µ ë¹„ìœ¨(~62%)ì„ ë³´ì˜€ìœ¼ë©°, Claude Sonnet 3.5ëŠ” ê°€ì¥ ì €í•­ë ¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤(~10%). ì˜ˆìƒê³¼ ë‹¬ë¦¬, í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ ë‹¤ì¤‘ ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ëŠ” ë° ì•½ê°„ ë” íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤. í†µê³„ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ ìœ í˜•ê³¼ ì…ë ¥ ëª¨ë‹¬ë¦¬í‹°ê°€ ìœ í•´ì„±ì˜ ì¤‘ìš”í•œ ì˜ˆì¸¡ ë³€ìˆ˜ì„ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ ê²°ê³¼ëŠ” MLLMsê°€ ë” ë„ë¦¬ ë°°í¬ë¨ì— ë”°ë¼ ê°•ë ¥í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì•ˆì „ ë²¤ì¹˜ë§ˆí¬ì˜ ê¸´ê¸‰í•œ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë„¤ ê°€ì§€ ì£¼ìš” ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ì•ˆì „ì„±ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. 26ëª…ì˜ ë ˆë“œ íŒ€ì´ ë¶ˆë²• í™œë™, í—ˆìœ„ ì •ë³´, ë¹„ìœ¤ë¦¬ì  í–‰ë™ì„ í¬í•¨í•œ ì„¸ ê°€ì§€ í•´ì•… ë²”ì£¼ë¥¼ ëª©í‘œë¡œ 726ê°œì˜ ê³µê²©ì  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì— ëŒ€í•´ 2,904ê°œì˜ ì¶œë ¥ë¬¼ì„ í‰ê°€í•œ ê²°ê³¼, Pixtral 12Bê°€ ê°€ì¥ ë†’ì€ í•´ì•… ì‘ë‹µë¥ (ì•½ 62%)ì„ ë³´ì˜€ê³ , Claude Sonnet 3.5ê°€ ê°€ì¥ ì €í•­ë ¥ì´ ë†’ì•˜ìŠµë‹ˆë‹¤(ì•½ 10%). í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ ë‹¤ì¤‘ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ëª¨ë¸ ìœ í˜•ê³¼ ì…ë ¥ ë°©ì‹ì´ í•´ì•…ì„±ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, MLLMsì˜ ì•ˆì „ì„± ê¸°ì¤€ ë§ˆë ¨ì´ ì‹œê¸‰í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ë„¤ ê°€ì§€ ì£¼ìš” ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ì•ˆì „ì„±ì„ í‰ê°€í•˜ë©°, íŠ¹íˆ ì•…ì˜ì ì¸ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ëª¨ë¸ì˜ ë°˜ì‘ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.
- 2. 26ëª…ì˜ ë ˆë“œ íŒ€ì›ì´ ìƒì„±í•œ 726ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ë¶ˆë²• í™œë™, í—ˆìœ„ ì •ë³´, ë¹„ìœ¤ë¦¬ì  í–‰ë™ì˜ ì„¸ ê°€ì§€ í•´ì•… ë²”ì£¼ë¥¼ ëŒ€ìƒìœ¼ë¡œ ëª¨ë¸ì˜ ë°˜ì‘ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.
- 3. Pixtral 12B ëª¨ë¸ì€ ì•½ 62%ì˜ ë†’ì€ í•´ë¡œìš´ ë°˜ì‘ë¥ ì„ ë³´ì˜€ìœ¼ë©°, Claude Sonnet 3.5 ëª¨ë¸ì€ ì•½ 10%ë¡œ ê°€ì¥ ì €í•­ë ¥ì´ ê°•í–ˆìŠµë‹ˆë‹¤.
- 4. í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ ë‹¤ì¤‘ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ë¼ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” MLLMsì˜ ê´‘ë²”ìœ„í•œ ë°°í¬ì— ì•ì„œ ê°•ë ¥í•œ ë‹¤ì¤‘ëª¨ë‹¬ ì•ˆì „ ê¸°ì¤€ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:29:19*