---
keywords:
  - Large Language Model
  - Chain-of-Thought
  - Confidence-guided Compression
  - Confidence Injection
  - Early Stopping
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.04881
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:55:17.027096",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Chain-of-Thought",
    "Confidence-guided Compression",
    "Confidence Injection",
    "Early Stopping"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Chain-of-Thought": 0.81,
    "Confidence-guided Compression": 0.72,
    "Confidence Injection": 0.7,
    "Early Stopping": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Reasoning Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LRM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to existing discussions on large models in AI, particularly in reasoning tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain-of-Thought prompting",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT prompting"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights a specific prompting technique relevant to reasoning models, facilitating focused discussions.",
        "novelty_score": 0.72,
        "connectivity_score": 0.79,
        "specificity_score": 0.82,
        "link_intent_score": 0.81
      },
      {
        "surface": "Confidence-guided Compression",
        "canonical": "Confidence-guided Compression",
        "aliases": [
          "ConCISE"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework for improving reasoning efficiency, relevant for new research directions.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Confidence Injection",
        "canonical": "Confidence Injection",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a novel technique within the proposed framework, offering potential for new methodological insights.",
        "novelty_score": 0.78,
        "connectivity_score": 0.68,
        "specificity_score": 0.76,
        "link_intent_score": 0.7
      },
      {
        "surface": "Early Stopping",
        "canonical": "Early Stopping",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A known technique in model training, applicable here in reasoning contexts, enhancing cross-domain applicability.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "compression methods",
      "task performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Reasoning Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain-of-Thought prompting",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.79,
        "specificity": 0.82,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Confidence-guided Compression",
      "resolved_canonical": "Confidence-guided Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Confidence Injection",
      "resolved_canonical": "Confidence Injection",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.68,
        "specificity": 0.76,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Early Stopping",
      "resolved_canonical": "Early Stopping",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning

**Korean Title:** ConCISE: ë‹¨ê³„ë³„ íš¨ìœ¨ì  ì¶”ë¡ ì—ì„œ ì‹ ë¢°ë„ ê¸°ë°˜ ì••ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.04881.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.04881](https://arxiv.org/abs/2505.04881)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (87.0% similar)
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (86.9% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory]] (85.3% similar)
- [[2025-09-22/FLARE_ Faithful Logic-Aided Reasoning and Exploration_20250922|FLARE: Faithful Logic-Aided Reasoning and Exploration]] (84.2% similar)
- [[2025-09-17/Slim-SC_ Thought Pruning for Efficient Scaling with Self-Consistency_20250917|Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/Early Stopping|Early Stopping]]
**âš¡ Unique Technical**: [[keywords/Confidence-guided Compression|Confidence-guided Compression]], [[keywords/Confidence Injection|Confidence Injection]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.04881v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs, increasing computational overhead. Existing fine-tuning-based compression methods either operate post-hoc pruning, risking disruption to reasoning coherence, or rely on sampling-based selection, which fails to remove redundant content thoroughly. To address these limitations, this work begins by framing two key patterns of redundant reflection in LRMs--Confidence Deficit, wherein the model reflects on correct intermediate steps, and Termination Delay, where reflection continues after a verified, confident answer--through a confidence-guided perspective. Based on this, we introduce ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), a framework designed to generate concise reasoning chains, integrating Confidence Injection to boost reasoning confidence, and Early Stopping to terminate reasoning when confidence is sufficient. Extensive experiments demonstrate that compared to baseline methods, fine-tuning LRMs on ConCISE-generated data yields a better balance between compression and task performance, reducing length by up to approximately 50% under SimPO, while maintaining high task accuracy.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.04881v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ëŒ€ê·œëª¨ ì¶”ë¡  ëª¨ë¸(LRMs)ì€ ì—°ì‡„ì  ì‚¬ê³ (Chain-of-Thought, CoT) ìœ ë„ë¥¼ í†µí•´ ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ë§Œ, ì¢…ì¢… ì¥í™©í•œ ì¶œë ¥ìœ¼ë¡œ ì¸í•´ ê³„ì‚°ì  ë¶€ë‹´ì´ ì¦ê°€í•˜ëŠ” ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¯¸ì„¸ ì¡°ì • ê¸°ë°˜ ì••ì¶• ë°©ë²•ì€ ì‚¬í›„ ê°€ì§€ì¹˜ê¸°ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì¶”ë¡ ì˜ ì¼ê´€ì„±ì„ í•´ì¹  ìœ„í—˜ì´ ìˆê±°ë‚˜, ìƒ˜í”Œë§ ê¸°ë°˜ ì„ íƒì— ì˜ì¡´í•˜ì—¬ ì¤‘ë³µëœ ë‚´ìš©ì„ ì² ì €íˆ ì œê±°í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ì—°êµ¬ëŠ” LRMsì—ì„œì˜ ì¤‘ë³µ ë°˜ì˜ì˜ ë‘ ê°€ì§€ ì£¼ìš” íŒ¨í„´ì„ ìì‹ ê° ê²°í•(Confidence Deficit)ê³¼ ì¢…ë£Œ ì§€ì—°(Termination Delay)ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ìì‹ ê° ê²°í•ì€ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì¤‘ê°„ ë‹¨ê³„ë¥¼ ë°˜ì˜í•˜ëŠ” ê²½ìš°ë¥¼ ë§í•˜ë©°, ì¢…ë£Œ ì§€ì—°ì€ í™•ì¸ëœ í™•ì‹  ìˆëŠ” ë‹µë³€ ì´í›„ì—ë„ ë°˜ì˜ì´ ê³„ì†ë˜ëŠ” ê²½ìš°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ì ì„ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë‹¨ê³„ë³„ íš¨ìœ¨ì  ì¶”ë¡ ì—ì„œ ìì‹ ê° ìœ ë„ ì••ì¶•(ConCISE, Confidence-guided Compression In Step-by-step Efficient Reasoning)ì´ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ì¶”ë¡  ìì‹ ê°ì„ ë†’ì´ê¸° ìœ„í•œ ìì‹ ê° ì£¼ì…(Confidence Injection)ê³¼ ì¶©ë¶„í•œ ìì‹ ê°ì´ ìˆì„ ë•Œ ì¶”ë¡ ì„ ì¢…ë£Œí•˜ëŠ” ì¡°ê¸° ì¢…ë£Œ(Early Stopping)ë¥¼ í†µí•©í•˜ì—¬ ê°„ê²°í•œ ì¶”ë¡  ì²´ì¸ì„ ìƒì„±í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼, ConCISEë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LRMsì„ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ ì••ì¶•ê³¼ ì‘ì—… ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì´ ë” ì˜ ë§ì¶°ì§€ë©°, SimPO í•˜ì—ì„œ ê¸¸ì´ê°€ ì•½ 50%ê¹Œì§€ ì¤„ì–´ë“¤ë©´ì„œë„ ë†’ì€ ì‘ì—… ì •í™•ë„ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì¶”ë¡  ëª¨ë¸(LRMs)ì˜ ì²´ì¸ ì˜¤ë¸Œ ì˜íŠ¸(CoT) í”„ë¡¬í”„íŠ¸ì—ì„œ ë°œìƒí•˜ëŠ” ë¶ˆí•„ìš”í•œ ë°˜ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ConCISEë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ConCISEëŠ” Confidence Injectionê³¼ Early Stopping ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì¶”ë¡ ì˜ ìì‹ ê°ì„ ë†’ì´ê³ , ë¶ˆí•„ìš”í•œ ë°˜ì˜ì„ ì¤„ì—¬ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ConCISEë¥¼ ì ìš©í•œ ëª¨ë¸ì€ ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ ì•½ 50%ì˜ ì¶œë ¥ ê¸¸ì´ ê°ì†Œì™€ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, ì••ì¶•ê³¼ ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì¶”ë¡  ëª¨ë¸(LRMs)ì€ ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì¥í™©í•œ ì¶œë ¥ìœ¼ë¡œ ì¸í•´ ê³„ì‚° ì˜¤ë²„í—¤ë“œê°€ ì¦ê°€í•˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì••ì¶• ë°©ë²•ì€ ì‚¬í›„ ê°€ì§€ì¹˜ê¸° ë˜ëŠ” ìƒ˜í”Œë§ ê¸°ë°˜ ì„ íƒì— ì˜ì¡´í•˜ì—¬ ì¶”ë¡ ì˜ ì¼ê´€ì„±ì„ í•´ì¹˜ê±°ë‚˜ ì¤‘ë³µ ì½˜í…ì¸ ë¥¼ ì™„ì „íˆ ì œê±°í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ìˆë‹¤.
- 3. ì´ ì—°êµ¬ëŠ” Confidence Deficitê³¼ Termination Delayë¼ëŠ” ë‘ ê°€ì§€ ì¤‘ë³µ ë°˜ì˜ íŒ¨í„´ì„ ì‹ë³„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ConCISEë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. ConCISEëŠ” Confidence Injectionê³¼ Early Stoppingì„ í†µí•´ ì¶”ë¡ ì˜ ìì‹ ê°ì„ ë†’ì´ê³ , ë¶ˆí•„ìš”í•œ ì¶”ë¡ ì„ ì¡°ê¸°ì— ì¢…ë£Œí•˜ì—¬ ê°„ê²°í•œ ì¶”ë¡  ì²´ì¸ì„ ìƒì„±í•œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ConCISEë¥¼ ì‚¬ìš©í•œ ë¯¸ì„¸ ì¡°ì •ì€ ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ ì••ì¶•ê³¼ ì‘ì—… ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì„ ë” ì˜ ìœ ì§€í•˜ë©°, SimPO ê¸°ì¤€ìœ¼ë¡œ ê¸¸ì´ë¥¼ ìµœëŒ€ ì•½ 50% ì¤„ì´ë©´ì„œë„ ë†’ì€ ì‘ì—… ì •í™•ë„ë¥¼ ìœ ì§€í•œë‹¤.


---

*Generated on 2025-09-23 09:55:17*