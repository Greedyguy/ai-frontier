---
keywords:
  - Inverse Reinforcement Learning
  - Active Inverse Reinforcement Learning
  - PAC-EIG
  - Reward-EIG
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2508.03693
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:10:34.476168",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Inverse Reinforcement Learning",
    "Active Inverse Reinforcement Learning",
    "PAC-EIG",
    "Reward-EIG"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Inverse Reinforcement Learning": 0.85,
    "Active Inverse Reinforcement Learning": 0.8,
    "PAC-EIG": 0.78,
    "Reward-EIG": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Inverse Reinforcement Learning",
        "canonical": "Inverse Reinforcement Learning",
        "aliases": [
          "IRL"
        ],
        "category": "specific_connectable",
        "rationale": "Inverse Reinforcement Learning is central to the paper's approach to aligning AI with human preferences.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Active Inverse Reinforcement Learning",
        "canonical": "Active Inverse Reinforcement Learning",
        "aliases": [
          "Active IRL"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper to improve the efficiency of acquiring demonstrations.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "PAC-EIG",
        "canonical": "PAC-EIG",
        "aliases": [
          "Probably Approximately Correct Expected Information Gain"
        ],
        "category": "unique_technical",
        "rationale": "PAC-EIG is a new acquisition function introduced for ensuring PAC guarantees in active IRL.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reward-EIG",
        "canonical": "Reward-EIG",
        "aliases": [
          "Reward Expected Information Gain"
        ],
        "category": "unique_technical",
        "rationale": "Reward-EIG is proposed as an alternative when learning the reward is the primary objective.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "autonomous",
      "demonstration",
      "policy",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Inverse Reinforcement Learning",
      "resolved_canonical": "Inverse Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Active Inverse Reinforcement Learning",
      "resolved_canonical": "Active Inverse Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "PAC-EIG",
      "resolved_canonical": "PAC-EIG",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reward-EIG",
      "resolved_canonical": "Reward-EIG",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning

**Korean Title:** ë² ì´ì§€ì•ˆ ëŠ¥ë™ ì—­ê°•í™”í•™ìŠµì„ í†µí•œ PAC ê²¬ìŠµ í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2508.03693.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2508.03693](https://arxiv.org/abs/2508.03693)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (81.9% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (81.7% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (81.4% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (81.4% similar)
- [[2025-09-22/Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations_20250922|Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Inverse Reinforcement Learning|Inverse Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Active Inverse Reinforcement Learning|Active Inverse Reinforcement Learning]], [[keywords/PAC-EIG|PAC-EIG]], [[keywords/Reward-EIG|Reward-EIG]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.03693v2 Announce Type: replace 
Abstract: As AI systems become increasingly autonomous, reliably aligning their decision-making with human preferences is essential. Inverse reinforcement learning (IRL) offers a promising approach to infer preferences from demonstrations. These preferences can then be used to produce an apprentice policy that performs well on the demonstrated task. However, in domains like autonomous driving or robotics, where errors can have serious consequences, we need not just good average performance but reliable policies with formal guarantees -- yet obtaining sufficient human demonstrations for reliability guarantees can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration. We introduce PAC-EIG, an information-theoretic acquisition function that directly targets probably-approximately-correct (PAC) guarantees for the learned policy -- providing the first such theoretical guarantee for active IRL with noisy expert demonstrations. Our method maximises information gain about the regret of the apprentice policy, efficiently identifying states requiring further demonstration. We also present Reward-EIG as an alternative when learning the reward itself is the primary objective. Focusing on finite state-action spaces, we prove convergence bounds, illustrate failure modes of prior heuristic methods, and demonstrate our method's advantages experimentally.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.03693v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: AI ì‹œìŠ¤í…œì´ ì ì  ë” ììœ¨ì ìœ¼ë¡œ ë°œì „í•¨ì— ë”°ë¼, ê·¸ë“¤ì˜ ì˜ì‚¬ê²°ì •ì„ ì¸ê°„ì˜ ì„ í˜¸ì™€ ì‹ ë¢°ì„± ìˆê²Œ ë§ì¶”ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì—­ê°•í™”í•™ìŠµ(IRL)ì€ ì‹œì—°ì„ í†µí•´ ì„ í˜¸ë¥¼ ì¶”ë¡ í•˜ëŠ” ìœ ë§í•œ ì ‘ê·¼ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„ í˜¸ëŠ” ì‹œì—°ëœ ì‘ì—…ì—ì„œ ì˜ ìˆ˜í–‰ë˜ëŠ” ê²¬ìŠµ ì •ì±…ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ììœ¨ì£¼í–‰ì´ë‚˜ ë¡œë´‡ê³µí•™ê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œëŠ” ì˜¤ë¥˜ê°€ ì‹¬ê°í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, í‰ê· ì ì¸ ì„±ëŠ¥ë¿ë§Œ ì•„ë‹ˆë¼ í˜•ì‹ì ì¸ ë³´ì¥ì„ ê°€ì§„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ì±…ì´ í•„ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹ ë¢°ì„± ë³´ì¥ì„ ìœ„í•œ ì¶©ë¶„í•œ ì¸ê°„ ì‹œì—°ì„ ì–»ëŠ” ê²ƒì€ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŠ¥ë™ì  IRLì€ ì¸ê°„ ì‹œì—°ì„ ìœ„í•œ ê°€ì¥ ì •ë³´ê°€ ë§ì€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì „ëµì ìœ¼ë¡œ ì„ íƒí•¨ìœ¼ë¡œì¨ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í•™ìŠµëœ ì •ì±…ì— ëŒ€í•´ ì•„ë§ˆë„-ëŒ€ëµì ìœ¼ë¡œ-ì •í™•í•œ(PAC) ë³´ì¥ì„ ì§ì ‘ì ìœ¼ë¡œ ëª©í‘œë¡œ í•˜ëŠ” ì •ë³´ ì´ë¡ ì  íšë“ í•¨ìˆ˜ì¸ PAC-EIGë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì „ë¬¸ê°€ ì‹œì—°ì„ í†µí•œ ëŠ¥ë™ì  IRLì— ëŒ€í•œ ìµœì´ˆì˜ ì´ë¡ ì  ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ê²¬ìŠµ ì •ì±…ì˜ í›„íšŒì— ëŒ€í•œ ì •ë³´ ì´ë“ì„ ìµœëŒ€í™”í•˜ì—¬ ì¶”ê°€ ì‹œì—°ì´ í•„ìš”í•œ ìƒíƒœë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‹ë³„í•©ë‹ˆë‹¤. ë˜í•œ ë³´ìƒ ìì²´ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì£¼ìš” ëª©í‘œì¼ ë•Œ ëŒ€ì•ˆìœ¼ë¡œ Reward-EIGë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìœ í•œí•œ ìƒíƒœ-í–‰ë™ ê³µê°„ì— ì´ˆì ì„ ë§ì¶”ì–´, ìš°ë¦¬ëŠ” ìˆ˜ë ´ ê²½ê³„ë¥¼ ì¦ëª…í•˜ê³ , ì´ì „ì˜ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•ì˜ ì‹¤íŒ¨ ëª¨ë“œë¥¼ ì„¤ëª…í•˜ë©°, ì‹¤í—˜ì ìœ¼ë¡œ ìš°ë¦¬ì˜ ë°©ë²•ì˜ ì¥ì ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ AI ì‹œìŠ¤í…œì˜ ì˜ì‚¬ê²°ì •ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì—­ê°•í™”í•™ìŠµ(IRL)ì„ í™œìš©í•©ë‹ˆë‹¤. íŠ¹íˆ, ììœ¨ì£¼í–‰ì´ë‚˜ ë¡œë´‡ ë¶„ì•¼ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ì±…ì„ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ì €ìëŠ” ëŠ¥ë™ì  IRLì„ í†µí•´ ê°€ì¥ ì •ë³´ê°€ ë§ì€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ íƒí•˜ì—¬ ì¸ê°„ ì‹œì—°ì„ ë°›ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. PAC-EIGë¼ëŠ” ì •ë³´ì´ë¡ ì  íšë“ í•¨ìˆ˜ë¥¼ ë„ì…í•˜ì—¬, ì†ŒìŒì´ ìˆëŠ” ì „ë¬¸ê°€ ì‹œì—°ì—ì„œë„ ì•„ë§ˆë„-ëŒ€ëµ-ì •í™•í•œ(PAC) ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í•™ìŠµëœ ì •ì±…ì˜ í›„íšŒë¥¼ ì¤„ì´ëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ ìµœëŒ€í™”í•˜ë©°, ë³´ìƒì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì£¼ëª©ì ì¸ ê²½ìš°ì—ëŠ” Reward-EIGë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìœ í•œ ìƒíƒœ-í–‰ë™ ê³µê°„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ë ´ ê²½ê³„ë¥¼ ì¦ëª…í•˜ê³ , ê¸°ì¡´ ë°©ë²•ì˜ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ì„¤ëª…í•˜ë©°, ì‹¤í—˜ì ìœ¼ë¡œ ì œì•ˆ ë°©ë²•ì˜ ì¥ì ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AI ì‹œìŠ¤í…œì˜ ì˜ì‚¬ê²°ì •ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©°, ì—­ê°•í™”í•™ìŠµ(IRL)ì€ ì‹œì—°ì„ í†µí•´ ì„ í˜¸ë¥¼ ì¶”ë¡ í•˜ëŠ” ìœ ë§í•œ ì ‘ê·¼ë²•ì„ ì œê³µí•œë‹¤.
- 2. ììœ¨ì£¼í–‰ì´ë‚˜ ë¡œë´‡ê³µí•™ê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œëŠ” í‰ê·  ì„±ëŠ¥ë¿ë§Œ ì•„ë‹ˆë¼ í˜•ì‹ì ì¸ ë³´ì¦ì„ ê°€ì§„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ì±…ì´ í•„ìš”í•˜ë‹¤.
- 3. Active IRLì€ ê°€ì¥ ì •ë³´ê°€ ë§ì€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì „ëµì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ì¸ê°„ ì‹œì—°ì˜ í•„ìš”ì„±ì„ ì¤„ì¸ë‹¤.
- 4. PAC-EIGëŠ” ì •ë³´ ì´ë¡ ì  íšë“ í•¨ìˆ˜ë¡œ, noisy expert demonstrationsì—ì„œë„ PAC ë³´ì¦ì„ ëª©í‘œë¡œ í•˜ì—¬ active IRLì— ëŒ€í•œ ì²« ì´ë¡ ì  ë³´ì¦ì„ ì œê³µí•œë‹¤.
- 5. Reward-EIGëŠ” ë³´ìƒì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì£¼ëœ ëª©í‘œì¼ ë•Œ ëŒ€ì•ˆìœ¼ë¡œ ì œì‹œë˜ë©°, ìœ í•œ ìƒíƒœ-í–‰ë™ ê³µê°„ì— ì´ˆì ì„ ë§ì¶° ìˆ˜ë ´ ê²½ê³„ë¥¼ ì¦ëª…í•˜ê³  ì‹¤í—˜ì ìœ¼ë¡œ ë°©ë²•ì˜ ì¥ì ì„ ì…ì¦í•œë‹¤.


---

*Generated on 2025-09-23 11:10:34*