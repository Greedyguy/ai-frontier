---
keywords:
  - Vision-Language Model
  - Test-Time Adaptation
  - Contrastive Loss
  - Outlier Contrastive Exposure
  - Zero-Shot Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2507.14312
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:38:45.761013",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Test-Time Adaptation",
    "Contrastive Loss",
    "Outlier Contrastive Exposure",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Test-Time Adaptation": 0.78,
    "Contrastive Loss": 0.8,
    "Outlier Contrastive Exposure": 0.77,
    "Zero-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-language models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language models are central to the paper's theme and connect well with multimodal and zero-shot learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Test-time adaptation",
        "canonical": "Test-Time Adaptation",
        "aliases": [
          "TTA"
        ],
        "category": "unique_technical",
        "rationale": "Test-time adaptation is a unique concept introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Contrastive loss",
        "canonical": "Contrastive Loss",
        "aliases": [
          "soft contrastive loss"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive loss is a key component of the proposed method, linking to broader contrastive learning techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Outlier Contrastive Exposure",
        "canonical": "Outlier Contrastive Exposure",
        "aliases": [
          "OCE loss"
        ],
        "category": "unique_technical",
        "rationale": "Outlier Contrastive Exposure is a novel loss function introduced for improving OOD detection, specific to this work.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-shot capabilities",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot capabilities are crucial for understanding the performance of vision-language models under distribution shifts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.68,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "entropy minimization",
      "pseudo-label drift",
      "class collapse"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-language models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Test-time adaptation",
      "resolved_canonical": "Test-Time Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Contrastive loss",
      "resolved_canonical": "Contrastive Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Outlier Contrastive Exposure",
      "resolved_canonical": "Outlier Contrastive Exposure",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-shot capabilities",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.68,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation

**Korean Title:** CLIPTTA: ê°•ê±´í•œ ëŒ€ì¡°ì  ë¹„ì „-ì–¸ì–´ í…ŒìŠ¤íŠ¸ ì‹œê°„ ì ì‘

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2507.14312.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2507.14312](https://arxiv.org/abs/2507.14312)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Singular Value Few-shot Adaptation of Vision-Language Models_20250918|Singular Value Few-shot Adaptation of Vision-Language Models]] (84.7% similar)
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (84.0% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (83.9% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (82.7% similar)
- [[2025-09-22/Global Pre-fixing, Local Adjusting_ A Simple yet Effective Contrastive Strategy for Continual Learning_20250922|Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Contrastive Loss|Contrastive Loss]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Test-Time Adaptation|Test-Time Adaptation]], [[keywords/Outlier Contrastive Exposure|Outlier Contrastive Exposure]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.14312v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities but often fail to generalize under distribution shifts. Test-time adaptation (TTA) allows models to update at inference time without labeled data, typically via entropy minimization. However, this objective is fundamentally misaligned with the contrastive image-text training of VLMs, limiting adaptation performance and introducing failure modes such as pseudo-label drift and class collapse. We propose CLIPTTA, a new gradient-based TTA method for vision-language models that leverages a soft contrastive loss aligned with CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's gradients, showing how its batch-aware design mitigates the risk of collapse. We further extend CLIPTTA to the open-set setting, where both in-distribution (ID) and out-of-distribution (OOD) samples are encountered, using an Outlier Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75 datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms entropy-based objectives and is highly competitive with state-of-the-art TTA methods, outperforming them on a large number of datasets and exhibiting more stable performance across diverse shifts.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.14312v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: CLIPê³¼ ê°™ì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì€ ê°•ë ¥í•œ ì œë¡œìƒ· ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ, ë¶„í¬ ë³€í™”ì— ë”°ë¼ ì¼ë°˜í™”í•˜ëŠ” ë° ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì‹œì  ì ì‘(TTA)ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ì´ ì¶”ë¡  ì‹œ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™”ë¥¼ í†µí•´ ìˆ˜í–‰ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ëª©í‘œëŠ” VLMì˜ ëŒ€ì¡° ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í›ˆë ¨ê³¼ ê·¼ë³¸ì ìœ¼ë¡œ ë§ì§€ ì•Šì•„ ì ì‘ ì„±ëŠ¥ì„ ì œí•œí•˜ê³ , ì˜ì‚¬ ë ˆì´ë¸” ë“œë¦¬í”„íŠ¸ ë° í´ë˜ìŠ¤ ë¶•ê´´ì™€ ê°™ì€ ì‹¤íŒ¨ ëª¨ë“œë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” CLIPì˜ ì‚¬ì „ í›ˆë ¨ ëª©í‘œì™€ ì •ë ¬ëœ ë¶€ë“œëŸ¬ìš´ ëŒ€ì¡° ì†ì‹¤ì„ í™œìš©í•˜ëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ìƒˆë¡œìš´ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ TTA ë°©ë²•ì¸ CLIPTTAë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CLIPTTAì˜ ê·¸ë˜ë””ì–¸íŠ¸ì— ëŒ€í•œ ì´ë¡ ì  ë¶„ì„ì„ ì œê³µí•˜ë©°, ë°°ì¹˜ ì¸ì‹ ì„¤ê³„ê°€ ë¶•ê´´ ìœ„í—˜ì„ ì–´ë–»ê²Œ ì™„í™”í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ CLIPTTAë¥¼ ê°œë°©í˜• ì„¸íŠ¸ ì„¤ì •ìœ¼ë¡œ í™•ì¥í•˜ì—¬, ë¶„í¬ ë‚´(ID) ë° ë¶„í¬ ì™¸(OOD) ìƒ˜í”Œì´ ëª¨ë‘ ë°œìƒí•˜ëŠ” ìƒí™©ì—ì„œ OOD íƒì§€ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì´ìƒì¹˜ ëŒ€ì¡° ë…¸ì¶œ(OCE) ì†ì‹¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¶„í¬ ë³€í™”ë¥¼ ì•„ìš°ë¥´ëŠ” 75ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•œ ê²°ê³¼, CLIPTTAëŠ” ì¼ê´€ë˜ê²Œ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ëª©í‘œë¥¼ ëŠ¥ê°€í•˜ë©°, ìµœì²¨ë‹¨ TTA ë°©ë²•ê³¼ ë§¤ìš° ê²½ìŸë ¥ì´ ìˆìœ¼ë©°, ë§ì€ ë°ì´í„°ì…‹ì—ì„œ ê·¸ë“¤ì„ ëŠ¥ê°€í•˜ê³  ë‹¤ì–‘í•œ ë³€í™”ì— ê±¸ì³ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ CLIPê³¼ ê°™ì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì˜ í…ŒìŠ¤íŠ¸ ì‹œ ì ì‘(TTA) ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì¸ CLIPTTAë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™” ê¸°ë°˜ TTAëŠ” VLMì˜ ëŒ€ì¡°ì  ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í›ˆë ¨ê³¼ ì˜ ë§ì§€ ì•Šì•„ ì ì‘ ì„±ëŠ¥ì´ ì œí•œë˜ê³ , ê°€ì§œ ë ˆì´ë¸” ì´ë™ ë° í´ë˜ìŠ¤ ë¶•ê´´ì™€ ê°™ì€ ë¬¸ì œë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. CLIPTTAëŠ” CLIPì˜ ì‚¬ì „ í›ˆë ¨ ëª©í‘œì™€ ì •ë ¬ëœ ì†Œí”„íŠ¸ ëŒ€ì¡° ì†ì‹¤ì„ í™œìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, ë°°ì¹˜ ì¸ì‹ ì„¤ê³„ë¥¼ í†µí•´ ë¶•ê´´ ìœ„í—˜ì„ ì™„í™”í•©ë‹ˆë‹¤. ë˜í•œ, CLIPTTAëŠ” ê°œë°©í˜• ì„¸íŠ¸ ì„¤ì •ìœ¼ë¡œ í™•ì¥ë˜ì–´, ID ë° OOD ìƒ˜í”Œì„ ëª¨ë‘ ì²˜ë¦¬í•˜ë©°, OOD íƒì§€ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ Outlier Contrastive Exposure(OCE) ì†ì‹¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 75ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•œ ê²°ê³¼, CLIPTTAëŠ” ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ëª©í‘œë¥¼ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, ìµœì‹  TTA ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ì—¬ ë§ì€ ë°ì´í„°ì…‹ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ë‹¤ì–‘í•œ ë¶„í¬ ë³€í™”ì— ëŒ€í•´ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision-language ëª¨ë¸(VLMs)ì¸ CLIPì€ ê°•ë ¥í•œ ì œë¡œìƒ· ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ, ë¶„í¬ ë³€í™”ì— ëŒ€í•œ ì¼ë°˜í™”ì—ëŠ” ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
- 2. í…ŒìŠ¤íŠ¸ ì‹œì  ì ì‘(TTA)ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆê²Œ í•˜ì§€ë§Œ, ê¸°ì¡´ì˜ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™” ë°©ì‹ì€ VLMsì˜ ëŒ€ì¡°ì  ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í›ˆë ¨ê³¼ ê·¼ë³¸ì ìœ¼ë¡œ ë§ì§€ ì•Šì•„ ì ì‘ ì„±ëŠ¥ì„ ì œí•œí•©ë‹ˆë‹¤.
- 3. CLIPTTAëŠ” CLIPì˜ ì‚¬ì „ í›ˆë ¨ ëª©í‘œì™€ ì •ë ¬ëœ ì†Œí”„íŠ¸ ëŒ€ì¡° ì†ì‹¤ì„ í™œìš©í•˜ì—¬ VLMsì— ëŒ€í•œ ìƒˆë¡œìš´ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ TTA ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. CLIPTTAëŠ” ë°°ì¹˜ ì¸ì‹ ì„¤ê³„ë¥¼ í†µí•´ í´ë˜ìŠ¤ ë¶•ê´´ ìœ„í—˜ì„ ì™„í™”í•˜ë©°, IDì™€ OOD ìƒ˜í”Œì„ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê°œë°©í˜• ì„¤ì •ìœ¼ë¡œ í™•ì¥ë©ë‹ˆë‹¤.
- 5. 75ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•œ ê²°ê³¼, CLIPTTAëŠ” ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ëª©í‘œë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ì–‘í•œ ë¶„í¬ ë³€í™”ì—ì„œë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:38:45*