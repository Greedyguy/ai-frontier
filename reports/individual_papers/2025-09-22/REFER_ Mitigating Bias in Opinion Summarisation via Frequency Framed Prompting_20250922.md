---
keywords:
  - Large Language Model
  - Frequency Framed Prompting
  - Fairness in Opinion Summarisation
  - Cognitive Load Reduction
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15723
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:33:24.567629",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Frequency Framed Prompting",
    "Fairness in Opinion Summarisation",
    "Cognitive Load Reduction"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Frequency Framed Prompting": 0.8,
    "Fairness in Opinion Summarisation": 0.78,
    "Cognitive Load Reduction": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Model"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's exploration of fairness in summarization, linking to broader discussions on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Frequency Framed Prompting",
        "canonical": "Frequency Framed Prompting",
        "aliases": [
          "REFER"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel technique specific to the paper, enhancing connectivity to studies on bias mitigation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Fairness in Opinion Summarisation",
        "canonical": "Fairness in Opinion Summarisation",
        "aliases": [
          "Fair Opinion Summarization"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on a specific application of fairness, connecting to ethical discussions in AI.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cognitive Load Reduction",
        "canonical": "Cognitive Load Reduction",
        "aliases": [
          "Cognitive Load"
        ],
        "category": "specific_connectable",
        "rationale": "Links to cognitive science principles applied in AI, relevant for interdisciplinary research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "hyperparameter tuning",
      "ground truth distributional information"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Frequency Framed Prompting",
      "resolved_canonical": "Frequency Framed Prompting",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Fairness in Opinion Summarisation",
      "resolved_canonical": "Fairness in Opinion Summarisation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cognitive Load Reduction",
      "resolved_canonical": "Cognitive Load Reduction",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting

**Korean Title:** ì˜ê²¬ ìš”ì•½ì—ì„œì˜ í¸í–¥ ì™„í™”ë¥¼ ìœ„í•œ ë¹ˆë„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸: REFER

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15723.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15723](https://arxiv.org/abs/2509.15723)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.8% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (85.5% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (85.3% similar)
- [[2025-09-22/Bias Beware_ The Impact of Cognitive Biases on LLM-Driven Product Recommendations_20250922|Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations]] (84.5% similar)
- [[2025-09-22/Re-FRAME the Meeting Summarization SCOPE_ Fact-Based Summarization and Personalization via Questions_20250922|Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Cognitive Load Reduction|Cognitive Load Reduction]]
**âš¡ Unique Technical**: [[keywords/Frequency Framed Prompting|Frequency Framed Prompting]], [[keywords/Fairness in Opinion Summarisation|Fairness in Opinion Summarisation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15723v1 Announce Type: new 
Abstract: Individuals express diverse opinions, a fair summary should represent these viewpoints comprehensively. Previous research on fairness in opinion summarisation using large language models (LLMs) relied on hyperparameter tuning or providing ground truth distributional information in prompts. However, these methods face practical limitations: end-users rarely modify default model parameters, and accurate distributional information is often unavailable. Building upon cognitive science research demonstrating that frequency-based representations reduce systematic biases in human statistical reasoning by making reference classes explicit and reducing cognitive load, this study investigates whether frequency framed prompting (REFER) can similarly enhance fairness in LLM opinion summarisation. Through systematic experimentation with different prompting frameworks, we adapted techniques known to improve human reasoning to elicit more effective information processing in language models compared to abstract probabilistic representations.Our results demonstrate that REFER enhances fairness in language models when summarising opinions. This effect is particularly pronounced in larger language models and using stronger reasoning instructions.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15723v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ê°œì¸ë“¤ì€ ë‹¤ì–‘í•œ ì˜ê²¬ì„ í‘œí˜„í•˜ë©°, ê³µì •í•œ ìš”ì•½ì€ ì´ëŸ¬í•œ ê´€ì ì„ í¬ê´„ì ìœ¼ë¡œ ëŒ€í‘œí•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ ì‚¬ìš©í•œ ì˜ê²¬ ìš”ì•½ì˜ ê³µì •ì„±ì— ê´€í•œ ì´ì „ ì—°êµ¬ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ í”„ë¡¬í”„íŠ¸ì— ì •í™•í•œ ë¶„í¬ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ì€ ì‹¤ì§ˆì ì¸ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤: ìµœì¢… ì‚¬ìš©ìëŠ” ê¸°ë³¸ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ê±°ì˜ ìˆ˜ì •í•˜ì§€ ì•Šìœ¼ë©°, ì •í™•í•œ ë¶„í¬ ì •ë³´ëŠ” ì¢…ì¢… ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ê°„ì˜ í†µê³„ì  ì¶”ë¡ ì—ì„œ ê¸°ì¤€ í´ë˜ìŠ¤ë¥¼ ëª…í™•íˆ í•˜ê³  ì¸ì§€ ë¶€í•˜ë¥¼ ì¤„ì„ìœ¼ë¡œì¨ ë¹ˆë„ ê¸°ë°˜ í‘œí˜„ì´ ì²´ê³„ì ì¸ í¸í–¥ì„ ì¤„ì¸ë‹¤ëŠ” ì¸ì§€ ê³¼í•™ ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³¸ ì—°êµ¬ëŠ” ë¹ˆë„ ê¸°ë°˜ í”„ë ˆì´ë° í”„ë¡¬í”„íŠ¸(REFER)ê°€ LLM ì˜ê²¬ ìš”ì•½ì—ì„œ ê³µì •ì„±ì„ ìœ ì‚¬í•˜ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì‹¤í—˜í•˜ì—¬, ì¸ê°„ì˜ ì¶”ë¡ ì„ ê°œì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ ê¸°ë²•ì„ ì¶”ìƒì ì¸ í™•ë¥ ì  í‘œí˜„ë³´ë‹¤ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë” íš¨ê³¼ì ì¸ ì •ë³´ ì²˜ë¦¬ë¥¼ ì´ëŒì–´ë‚´ë„ë¡ ì¡°ì •í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” REFERê°€ ì˜ê²¬ì„ ìš”ì•½í•  ë•Œ ì–¸ì–´ ëª¨ë¸ì˜ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ íš¨ê³¼ëŠ” íŠ¹íˆ ë” í° ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ì¶”ë¡  ì§€ì¹¨ì„ ì‚¬ìš©í•  ë•Œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì˜ê²¬ ìš”ì•½ì—ì„œ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë¹ˆë„ ê¸°ë°˜ í”„ë ˆì´ë°(REPER)ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ë‚˜ ì •í™•í•œ ë¶„í¬ ì •ë³´ë¥¼ í•„ìš”ë¡œ í–ˆì§€ë§Œ, ì´ëŠ” ì‹¤ìš©ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” ì¸ì§€ ê³¼í•™ì˜ ë¹ˆë„ ê¸°ë°˜ í‘œí˜„ì´ ì¸ê°„ì˜ í†µê³„ì  ì¶”ë¡ ì—ì„œ í¸í–¥ì„ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ë¥¼ LLMì— ì ìš©í•˜ì—¬ ê³µì •ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë” í° ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ì¶”ë¡  ì§€ì¹¨ì„ ì‚¬ìš©í•  ë•Œ íš¨ê³¼ê°€ ë‘ë“œëŸ¬ì¡ŒìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì–‘í•œ ì˜ê²¬ì„ ê³µì •í•˜ê²Œ ìš”ì•½í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ëŸ¬í•œ ê´€ì ì„ í¬ê´„ì ìœ¼ë¡œ ëŒ€í‘œí•´ì•¼ í•œë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê³µì •ì„±ì„ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ ì‹¤ì œ ë¶„í¬ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì´ëŠ” ì‹¤ìš©ì ì¸ í•œê³„ê°€ ìˆë‹¤.
- 3. ë¹ˆë„ ê¸°ë°˜ í‘œí˜„ì´ ì¸ê°„ì˜ í†µê³„ì  ì¶”ë¡ ì—ì„œ ì²´ê³„ì  í¸í–¥ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ëœë‹¤ëŠ” ì¸ì§€ ê³¼í•™ ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³¸ ì—°êµ¬ëŠ” ë¹ˆë„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸(REFFER)ê°€ LLMì˜ ì˜ê²¬ ìš”ì•½ì—ì„œ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í–ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, REFERëŠ” íŠ¹íˆ ë” í° ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ì¶”ë¡  ì§€ì¹¨ì„ ì‚¬ìš©í•  ë•Œ ì˜ê²¬ ìš”ì•½ì˜ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-23 11:33:24*