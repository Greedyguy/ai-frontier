---
keywords:
  - Overfitting
  - Negotiation Paradigm
  - Classification Accuracy
  - CIFAR-10
  - Continual Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2311.11410
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:59:29.084948",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Overfitting",
    "Negotiation Paradigm",
    "Classification Accuracy",
    "CIFAR-10",
    "Continual Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Overfitting": 0.8,
    "Negotiation Paradigm": 0.78,
    "Classification Accuracy": 0.75,
    "CIFAR-10": 0.7,
    "Continual Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "overfitting",
        "canonical": "Overfitting",
        "aliases": [
          "model overfitting",
          "overfit"
        ],
        "category": "broad_technical",
        "rationale": "Overfitting is a fundamental issue in machine learning, relevant across various models and datasets.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "negotiation paradigm",
        "canonical": "Negotiation Paradigm",
        "aliases": [
          "negotiated representations",
          "negotiation approach"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper, offering a unique method to mitigate overfitting.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "classification accuracy",
        "canonical": "Classification Accuracy",
        "aliases": [
          "accuracy improvement",
          "classification performance"
        ],
        "category": "specific_connectable",
        "rationale": "Improving classification accuracy is a key goal in machine learning, linking to performance evaluation.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "CIFAR 10",
        "canonical": "CIFAR-10",
        "aliases": [
          "CIFAR10"
        ],
        "category": "specific_connectable",
        "rationale": "CIFAR-10 is a widely used dataset in machine learning, relevant for benchmarking and experimentation.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "continual learning",
        "canonical": "Continual Learning",
        "aliases": [
          "lifelong learning",
          "incremental learning"
        ],
        "category": "evolved_concepts",
        "rationale": "Continual learning is an emerging field that benefits from the negotiation paradigm to address learning challenges.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "model",
      "data set",
      "experimental results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "overfitting",
      "resolved_canonical": "Overfitting",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "negotiation paradigm",
      "resolved_canonical": "Negotiation Paradigm",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "classification accuracy",
      "resolved_canonical": "Classification Accuracy",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "CIFAR 10",
      "resolved_canonical": "CIFAR-10",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "continual learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Negotiated Representations to Prevent Overfitting in Machine Learning Applications

**Korean Title:** ê¸°ê³„ í•™ìŠµ ì‘ìš©ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ í˜‘ìƒëœ í‘œí˜„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2311.11410.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2311.11410](https://arxiv.org/abs/2311.11410)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (83.8% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.8% similar)
- [[2025-09-19/Superpose Task-specific Features for Model Merging_20250919|Superpose Task-specific Features for Model Merging]] (82.4% similar)
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (81.6% similar)
- [[2025-09-19/Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning_20250919|Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Overfitting|Overfitting]]
**ğŸ”— Specific Connectable**: [[keywords/Classification Accuracy|Classification Accuracy]], [[keywords/CIFAR-10|CIFAR-10]]
**âš¡ Unique Technical**: [[keywords/Negotiation Paradigm|Negotiation Paradigm]]
**ğŸš€ Evolved Concepts**: [[keywords/Continual Learning|Continual Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2311.11410v2 Announce Type: replace 
Abstract: Overfitting is a phenomenon that occurs when a machine learning model is trained for too long and focused too much on the exact fitness of the training samples to the provided training labels and cannot keep track of the predictive rules that would be useful on the test data. This phenomenon is commonly attributed to memorization of particular samples, memorization of the noise, and forced fitness into a data set of limited samples by using a high number of neurons. While it is true that the model encodes various peculiarities as the training process continues, we argue that most of the overfitting occurs in the process of reconciling sharply defined membership ratios. In this study, we present an approach that increases the classification accuracy of machine learning models by allowing the model to negotiate output representations of the samples with previously determined class labels. By setting up a negotiation between the models interpretation of the inputs and the provided labels, we not only increased average classification accuracy but also decreased the rate of overfitting without applying any other regularization tricks. By implementing our negotiation paradigm approach to several low regime machine learning problems by generating overfitting scenarios from publicly available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated that the proposed paradigm has more capacity than its intended purpose. We are sharing the experimental results and inviting the machine learning community to explore the limits of the proposed paradigm. We also aim to incentive the community to exploit the negotiation paradigm to overcome the learning related challenges in other research fields such as continual learning. The Python code of the experimental setup is uploaded to GitHub.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2311.11410v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ê³¼ì í•©ì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì´ ë„ˆë¬´ ì˜¤ë«ë™ì•ˆ í›ˆë ¨ë˜ì–´ ì œê³µëœ í›ˆë ¨ ë ˆì´ë¸”ì— ëŒ€í•œ í›ˆë ¨ ìƒ˜í”Œì˜ ì •í™•í•œ ì í•©ì„±ì— ì§€ë‚˜ì¹˜ê²Œ ì§‘ì¤‘í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ìœ ìš©í•œ ì˜ˆì¸¡ ê·œì¹™ì„ ì¶”ì í•  ìˆ˜ ì—†ì„ ë•Œ ë°œìƒí•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ì´ í˜„ìƒì€ ì¼ë°˜ì ìœ¼ë¡œ íŠ¹ì • ìƒ˜í”Œì˜ ì•”ê¸°, ë…¸ì´ì¦ˆì˜ ì•”ê¸°, ë§ì€ ìˆ˜ì˜ ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ì—¬ ì œí•œëœ ìƒ˜í”Œì˜ ë°ì´í„° ì„¸íŠ¸ì— ê°•ì œë¡œ ì í•©ì‹œí‚¤ëŠ” ê²ƒì— ê¸°ì¸í•œë‹¤ê³  ì—¬ê²¨ì§‘ë‹ˆë‹¤. í›ˆë ¨ ê³¼ì •ì´ ê³„ì†ë¨ì— ë”°ë¼ ëª¨ë¸ì´ ë‹¤ì–‘í•œ íŠ¹ì´ì„±ì„ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì€ ì‚¬ì‹¤ì´ì§€ë§Œ, ìš°ë¦¬ëŠ” ëŒ€ë¶€ë¶„ì˜ ê³¼ì í•©ì´ ëª…í™•í•˜ê²Œ ì •ì˜ëœ ë©¤ë²„ì‹­ ë¹„ìœ¨ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ëª¨ë¸ì´ ì´ì „ì— ê²°ì •ëœ í´ë˜ìŠ¤ ë ˆì´ë¸”ê³¼ ìƒ˜í”Œì˜ ì¶œë ¥ í‘œí˜„ì„ í˜‘ìƒí•  ìˆ˜ ìˆê²Œ í•¨ìœ¼ë¡œì¨ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì…ë ¥ì— ëŒ€í•œ ëª¨ë¸ì˜ í•´ì„ê³¼ ì œê³µëœ ë ˆì´ë¸” ê°„ì˜ í˜‘ìƒì„ ì„¤ì •í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” í‰ê·  ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì˜€ì„ ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì •ê·œí™” ê¸°ë²•ì„ ì ìš©í•˜ì§€ ì•Šê³ ë„ ê³¼ì í•©ë¥ ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. CIFAR 10, CIFAR 100, MNISTì™€ ê°™ì€ ê³µê°œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê³¼ì í•© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ì—¬ ì—¬ëŸ¬ ì €ë ˆì§ ê¸°ê³„ í•™ìŠµ ë¬¸ì œì— ìš°ë¦¬ì˜ í˜‘ìƒ íŒ¨ëŸ¬ë‹¤ì„ ì ‘ê·¼ë²•ì„ êµ¬í˜„í•¨ìœ¼ë¡œì¨, ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì´ ì˜ë„ëœ ëª©ì ë³´ë‹¤ ë” ë§ì€ ì—­ëŸ‰ì„ ê°€ì§€ê³  ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ê³µìœ í•˜ê³  ê¸°ê³„ í•™ìŠµ ì»¤ë®¤ë‹ˆí‹°ê°€ ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì˜ í•œê³„ë¥¼ íƒêµ¬í•˜ë„ë¡ ì´ˆëŒ€í•©ë‹ˆë‹¤. ë˜í•œ, ì§€ì† í•™ìŠµê³¼ ê°™ì€ ë‹¤ë¥¸ ì—°êµ¬ ë¶„ì•¼ì—ì„œ í•™ìŠµ ê´€ë ¨ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ í˜‘ìƒ íŒ¨ëŸ¬ë‹¤ì„ì„ í™œìš©í•˜ë„ë¡ ì»¤ë®¤ë‹ˆí‹°ì— ë™ê¸°ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ì„¤ì •ì˜ Python ì½”ë“œëŠ” GitHubì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê³¼ì í•©ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì§€ë‚˜ì¹˜ê²Œ ë§ì¶°ì ¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ìœ ìš©í•œ ì˜ˆì¸¡ ê·œì¹™ì„ ë†“ì¹˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ì—°êµ¬ì§„ì€ ëª¨ë¸ì´ ì…ë ¥ê³¼ ì£¼ì–´ì§„ ë ˆì´ë¸” ê°„ì˜ 'í˜‘ìƒ'ì„ í†µí•´ ì¶œë ¥ í‘œí˜„ì„ ì¡°ì •í•˜ë„ë¡ í•˜ì—¬, í‰ê·  ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ê³  ê³¼ì í•©ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì¶”ê°€ì ì¸ ì •ê·œí™” ê¸°ë²• ì—†ì´ë„ íš¨ê³¼ì ì´ë©°, CIFAR 10, CIFAR 100, MNIST ë°ì´í„°ì…‹ì„ í†µí•´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” GitHubì— ê³µê°œë˜ì–´ ìˆìœ¼ë©°, ì§€ì† í•™ìŠµ ë“± ë‹¤ë¥¸ ì—°êµ¬ ë¶„ì•¼ì—ë„ ì ìš© ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³¼ì í•©ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì§€ë‚˜ì¹˜ê²Œ ì§‘ì¤‘í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ìœ ìš©í•œ ì˜ˆì¸¡ ê·œì¹™ì„ ìœ ì§€í•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒì´ë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ëª¨ë¸ì´ ì…ë ¥ê³¼ ì œê³µëœ ë ˆì´ë¸” ê°„ì˜ í˜‘ìƒì„ í†µí•´ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤.
- 3. ì œì•ˆëœ í˜‘ìƒ íŒ¨ëŸ¬ë‹¤ì„ì€ ê³¼ì í•©ì„ ì¤„ì´ê³  í‰ê·  ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° íš¨ê³¼ì ì´ì—ˆë‹¤.
- 4. CIFAR 10, CIFAR 100, MNISTì™€ ê°™ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ í†µí•´ ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ì˜€ë‹¤.
- 5. ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ê°€ ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì„ ë‹¤ë¥¸ ì—°êµ¬ ë¶„ì•¼ì˜ í•™ìŠµ ê´€ë ¨ ë¬¸ì œ í•´ê²°ì— í™œìš©í•˜ë„ë¡ ì¥ë ¤í•˜ê³ ì í•œë‹¤.


---

*Generated on 2025-09-23 10:59:29*