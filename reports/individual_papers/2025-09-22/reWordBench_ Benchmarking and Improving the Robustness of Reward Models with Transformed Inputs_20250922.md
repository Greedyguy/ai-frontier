---
keywords:
  - Reward Models
  - reWordBench
  - Paraphrases
  - Alignment
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2503.11751
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:42:51.239469",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reward Models",
    "reWordBench",
    "Paraphrases",
    "Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reward Models": 0.78,
    "reWordBench": 0.8,
    "Paraphrases": 0.72,
    "Alignment": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reward Models",
        "canonical": "Reward Models",
        "aliases": [
          "RM",
          "Reward Function"
        ],
        "category": "unique_technical",
        "rationale": "Reward models are central to the paper's focus on robustness and are not covered in the existing vocabulary.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "reWordBench",
        "canonical": "reWordBench",
        "aliases": [
          "Reward Benchmark"
        ],
        "category": "unique_technical",
        "rationale": "reWordBench is a novel benchmark introduced in the paper, crucial for understanding the robustness of reward models.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Paraphrases",
        "canonical": "Paraphrases",
        "aliases": [
          "Rephrasing",
          "Restatement"
        ],
        "category": "specific_connectable",
        "rationale": "Paraphrases are key to the proposed method for improving reward model robustness.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "Alignment",
        "canonical": "Alignment",
        "aliases": [
          "Model Alignment",
          "Output Alignment"
        ],
        "category": "broad_technical",
        "rationale": "Alignment is a critical application area for reward models, relevant to the paper's improvements.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "Performance",
      "Standard Benchmarks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reward Models",
      "resolved_canonical": "Reward Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "reWordBench",
      "resolved_canonical": "reWordBench",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Paraphrases",
      "resolved_canonical": "Paraphrases",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Alignment",
      "resolved_canonical": "Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs

**Korean Title:** reWordBench: ë³€í™˜ëœ ì…ë ¥ì„ í†µí•œ ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„± ë²¤ì¹˜ë§ˆí‚¹ ë° ê°œì„ 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2503.11751.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2503.11751](https://arxiv.org/abs/2503.11751)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/BaseReward_ A Strong Baseline for Multimodal Reward Model_20250922|BaseReward: A Strong Baseline for Multimodal Reward Model]] (86.4% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (85.7% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (84.0% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (83.9% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Alignment|Alignment]]
**ğŸ”— Specific Connectable**: [[keywords/Paraphrases|Paraphrases]]
**âš¡ Unique Technical**: [[keywords/Reward Models|Reward Models]], [[keywords/reWordBench|reWordBench]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.11751v2 Announce Type: replace 
Abstract: Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2503.11751v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë³´ìƒ ëª¨ë¸ì€ í˜„ëŒ€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ í•„ìˆ˜ì ì¸ ìš”ì†Œë¡œ ìë¦¬ì¡ê³  ìˆìœ¼ë©°, í™•ì¥ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ í‰ê°€ìë¡œì„œë¿ë§Œ ì•„ë‹ˆë¼, ë§ì€ ì •ë ¬ ë°©ë²• ë° ì¶”ë¡  ì‹œê°„ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í•„ìˆ˜ì ì¸ êµ¬ì„± ìš”ì†Œë¡œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœê·¼ì˜ ë³´ìƒ ëª¨ë¸ì´ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìˆì§€ë§Œ, ì´ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ê³¼ì í•© íš¨ê³¼ë¡œ ì¸í•œ ê²ƒì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì´ë“¤ì˜ ì‹¤ì œ ëŠ¥ë ¥ì— ëŒ€í•œ ì´í•´ë¥¼ í˜¼ë€ìŠ¤ëŸ½ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ ê·¸ëŸ¬í•œ ê³¼ì í•©ì˜ ì •ë„ë¥¼ ë©´ë°€íˆ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” **reWordBench**ë¥¼ êµ¬ì¶•í•˜ì—¬ ë³´ìƒ ëª¨ë¸ ì…ë ¥ì„ ì˜ë¯¸ ë˜ëŠ” ìˆœìœ„ë¥¼ ë³´ì¡´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìµœì²¨ë‹¨ ë³´ìƒ ëª¨ë¸ì´ ì‚¬ì†Œí•œ ì…ë ¥ ë³€í™˜ì—ë„ ìƒë‹¹í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìœ¼ë©°, ë•Œë¡œëŠ” ë¬´ì‘ìœ„ ì •í™•ë„ë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ìˆ˜ì¤€ìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì–´ ì·¨ì•½ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê·¸ë“¤ì´ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆì— ìœ ì‚¬í•œ ì ìˆ˜ë¥¼ í• ë‹¹í•˜ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì„ ì œì•ˆí•˜ë©°, ì´ ì ‘ê·¼ ë°©ì‹ì´ ë‹¤ë¥¸ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë³€í™˜ì— ëŒ€í•œ ê²¬ê³ ì„±ë„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ì˜ ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ì€ RewardBenchì˜ Chat Hard í•˜ìœ„ ì§‘í•©ì—ì„œ ê·¸ëŸ¬í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì•½ ì ˆë°˜ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë”ìš±ì´, ì •ë ¬ì— ì‚¬ìš©ë  ë•Œ, ìš°ë¦¬ì˜ ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ì€ ë” ë‚˜ì€ ìœ ìš©ì„±ì„ ë³´ì—¬ì£¼ë©°, í‘œì¤€ì ìœ¼ë¡œ í›ˆë ¨ëœ ë³´ìƒ ëª¨ë¸ì— ë¹„í•´ ìµœëŒ€ 59%ì˜ ì‚¬ë¡€ì—ì„œ ë” ë†’ì€ í’ˆì§ˆì˜ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë³´ìƒ ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í‰ê°€í•˜ê³  ê³¼ì í•© ë¬¸ì œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬íŒ€ì€ ì…ë ¥ ë³€í™˜ì„ í†µí•´ ë³´ìƒ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ í™•ì¸í•˜ê³ , ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë¬¸ì¥ì— ëŒ€í•´ ì¼ê´€ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë„ë¡ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë‹¤ì–‘í•œ ë³€í™˜ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, íŠ¹íˆ Chat Hard ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ê°œì„ ëœ ë³´ìƒ ëª¨ë¸ì€ ì •ë ¬ ì‘ì—…ì—ì„œ ë” ë†’ì€ í’ˆì§ˆì˜ ì¶œë ¥ì„ ìƒì„±í•˜ë©°, ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ìµœëŒ€ 59%ì˜ ê²½ìš°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœì‹  ë³´ìƒ ëª¨ë¸ì€ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ì´ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ê³¼ì í•© íš¨ê³¼ ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 2. **reWordBench**ë¥¼ í†µí•´ ë³´ìƒ ëª¨ë¸ ì…ë ¥ì„ ì˜ë¯¸ë‚˜ ìˆœìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì²´ê³„ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì˜ ì·¨ì•½ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- 3. ìµœì²¨ë‹¨ ë³´ìƒ ëª¨ë¸ì€ ì…ë ¥ ë³€í™˜ì— ëŒ€í•´ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ ì·¨ì•½ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
- 4. ë³´ìƒ ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ìœ ì‚¬í•œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ë³€í™˜ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 5. ê°•ê±´í•œ ë³´ìƒ ëª¨ë¸ì€ ì •ë ¬ ì‹œ ë” ë‚˜ì€ ìœ ìš©ì„±ì„ ë³´ì—¬ì£¼ë©°, í‘œì¤€ í›ˆë ¨ëœ ëª¨ë¸ì— ë¹„í•´ ìµœëŒ€ 59%ì˜ ê²½ìš°ì—ì„œ ë” ë†’ì€ í’ˆì§ˆì˜ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:42:51*