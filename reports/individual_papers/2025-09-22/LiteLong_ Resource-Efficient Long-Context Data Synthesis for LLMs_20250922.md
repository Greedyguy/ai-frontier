---
keywords:
  - Large Language Model
  - BISAC Classification
  - BM25 Retrieval
  - Multi-Agent Debate
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15568
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:04:34.359063",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "BISAC Classification",
    "BM25 Retrieval",
    "Multi-Agent Debate"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "BISAC Classification": 0.65,
    "BM25 Retrieval": 0.7,
    "Multi-Agent Debate": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Connects to a foundational concept in NLP and machine learning, facilitating integration with existing research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "BISAC book classification system",
        "canonical": "BISAC Classification",
        "aliases": [
          "BISAC",
          "Book Industry Standards and Communications"
        ],
        "category": "unique_technical",
        "rationale": "Provides a structured topic organization method, crucial for understanding the paper's approach.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.65
      },
      {
        "surface": "BM25 retrieval",
        "canonical": "BM25 Retrieval",
        "aliases": [
          "BM25",
          "Best Matching 25"
        ],
        "category": "specific_connectable",
        "rationale": "A well-known information retrieval algorithm, connecting to broader IR and NLP topics.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.7
      },
      {
        "surface": "multi-agent debate",
        "canonical": "Multi-Agent Debate",
        "aliases": [
          "Agent Debate",
          "Debate Mechanism"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach for generating diverse topics, enhancing the paper's methodological contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "BISAC book classification system",
      "resolved_canonical": "BISAC Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "BM25 retrieval",
      "resolved_canonical": "BM25 Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "multi-agent debate",
      "resolved_canonical": "Multi-Agent Debate",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs

**Korean Title:** LiteLong: LLMì„ ìœ„í•œ ìì› íš¨ìœ¨ì ì¸ ì¥ë¬¸ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° í•©ì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15568.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15568](https://arxiv.org/abs/2509.15568)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (83.7% similar)
- [[2025-09-22/SyGra_ A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data_20250922|SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data]] (83.1% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (83.1% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (82.7% similar)
- [[2025-09-19/DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (80.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/BM25 Retrieval|BM25 Retrieval]]
**âš¡ Unique Technical**: [[keywords/BISAC Classification|BISAC Classification]], [[keywords/Multi-Agent Debate|Multi-Agent Debate]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15568v1 Announce Type: cross 
Abstract: High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency. We present LiteLong, a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. Our approach leverages the BISAC book classification system to provide a comprehensive hierarchical topic organization, and then employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. For each topic, we use lightweight BM25 retrieval to obtain relevant documents and concatenate them into 128K-token training samples. Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods. LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15568v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê³ í’ˆì§ˆì˜ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ ë°©ëŒ€í•œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í›ˆë ¨í•˜ëŠ” ë° í•„ìˆ˜ì ì´ì§€ë§Œ, ê¸°ì¡´ì˜ ê´€ë ¨ì„± ê¸°ë°˜ ì§‘ê³„ ë°©ë²•ì€ ê³„ì‚° íš¨ìœ¨ì„±ì—ì„œ ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” êµ¬ì¡°í™”ëœ ì£¼ì œ ì¡°ì§í™”ì™€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ í†µí•´ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•©ì„±í•˜ëŠ” ë°©ë²•ì¸ LiteLongì„ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ BISAC ë„ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ í¬ê´„ì ì¸ ê³„ì¸µì  ì£¼ì œ ì¡°ì§í™”ë¥¼ ì œê³µí•˜ê³ , ì´ êµ¬ì¡° ë‚´ì—ì„œ ë‹¤ì–‘í•œ ê³ í’ˆì§ˆ ì£¼ì œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ LLMsì™€ì˜ í† ë¡  ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ì£¼ì œì— ëŒ€í•´, ìš°ë¦¬ëŠ” ê²½ëŸ‰í™”ëœ BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì–»ê³  ì´ë¥¼ 128K-í† í° í›ˆë ¨ ìƒ˜í”Œë¡œ ì—°ê²°í•©ë‹ˆë‹¤. HELMET ë° Ruler ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ì€ LiteLongì´ ê²½ìŸë ¥ ìˆëŠ” ê¸´ ë¬¸ë§¥ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©° ë‹¤ë¥¸ ê¸´ ì˜ì¡´ì„± ê°•í™” ë°©ë²•ê³¼ ì›í™œí•˜ê²Œ í†µí•©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. LiteLongì€ ê³„ì‚° ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë¹„ìš©ì„ ì¤„ì—¬ ê³ í’ˆì§ˆì˜ ê¸´ ë¬¸ë§¥ ë°ì´í„° í•©ì„±ì„ ë³´ë‹¤ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì–¸ì–´ í›ˆë ¨ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

LiteLongì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìœ„í•œ ê³ í’ˆì§ˆ ì¥ë¬¸ ë°ì´í„° ìƒì„±ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. BISAC ë„ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ì£¼ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì§í•˜ê³ , ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ í†µí•´ ë‹¤ì–‘í•œ ê³ í’ˆì§ˆ ì£¼ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê° ì£¼ì œì— ëŒ€í•´ BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³ , ì´ë¥¼ 128K í† í°ì˜ í›ˆë ¨ ìƒ˜í”Œë¡œ ê²°í•©í•©ë‹ˆë‹¤. HELMET ë° Ruler ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ LiteLongì€ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ë¥¸ ì¥ê¸° ì˜ì¡´ì„± ê°•í™” ë°©ë²•ê³¼ë„ ì›í™œí•˜ê²Œ í†µí•©ë  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LiteLongì€ ì¥ë¬¸ ë°ì´í„° í•©ì„±ì˜ ë¹„ìš©ì„ ì¤„ì—¬, ì¥ë¬¸ ì–¸ì–´ í›ˆë ¨ ì—°êµ¬ë¥¼ ë”ìš± ì´‰ì§„í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LiteLongì€ êµ¬ì¡°í™”ëœ ì£¼ì œ ì¡°ì§í™”ì™€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ í†µí•´ ìì› íš¨ìœ¨ì ì¸ ì¥ë¬¸ ë§¥ë½ ë°ì´í„° í•©ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. BISAC ë„ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ í¬ê´„ì ì¸ ê³„ì¸µì  ì£¼ì œ ì¡°ì§í™”ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
- 3. ê²½ëŸ‰ BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê° ì£¼ì œì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì–»ê³  ì´ë¥¼ 128K í† í° í›ˆë ¨ ìƒ˜í”Œë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
- 4. HELMET ë° Ruler ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ LiteLongì€ ê²½ìŸë ¥ ìˆëŠ” ì¥ë¬¸ ë§¥ë½ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. LiteLongì€ ê³„ì‚° ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë¹„ìš©ì„ ì¤„ì—¬ ì¥ë¬¸ ë§¥ë½ ë°ì´í„° í•©ì„±ì„ ë” ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:04:34*