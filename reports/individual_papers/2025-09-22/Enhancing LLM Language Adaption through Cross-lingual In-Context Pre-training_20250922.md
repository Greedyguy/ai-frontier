---
keywords:
  - Large Language Model
  - Cross-lingual In-context Pre-training
  - Semantic Retrieval Framework
  - Multilingual Performance
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2504.20484
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:44:27.234495",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-lingual In-context Pre-training",
    "Semantic Retrieval Framework",
    "Multilingual Performance"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cross-lingual In-context Pre-training": 0.88,
    "Semantic Retrieval Framework": 0.82,
    "Multilingual Performance": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and are a key concept in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-lingual In-context Pre-training",
        "canonical": "Cross-lingual In-context Pre-training",
        "aliases": [
          "CrossIC-PT"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel method introduced in the paper, crucial for understanding the proposed approach.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Semantic Retrieval Framework",
        "canonical": "Semantic Retrieval Framework",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This framework is essential for constructing training samples, linking to retrieval-based methods.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multilingual Performance",
        "canonical": "Multilingual Performance",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The paper focuses on improving performance across multiple languages, a key outcome of the study.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-lingual In-context Pre-training",
      "resolved_canonical": "Cross-lingual In-context Pre-training",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Semantic Retrieval Framework",
      "resolved_canonical": "Semantic Retrieval Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multilingual Performance",
      "resolved_canonical": "Multilingual Performance",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training

**Korean Title:** Cross-lingual In-Context ì‚¬ì „ í›ˆë ¨ì„ í†µí•œ LLM ì–¸ì–´ ì ì‘ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2504.20484.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2504.20484](https://arxiv.org/abs/2504.20484)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Exploring Polyglot Harmony_ On Multilingual Data Allocation for Large Language Models Pretraining_20250922|Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining]] (86.4% similar)
- [[2025-09-22/Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation_20250922|Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation]] (86.0% similar)
- [[2025-09-22/A method for improving multilingual quality and diversity of instruction fine-tuning datasets_20250922|A method for improving multilingual quality and diversity of instruction fine-tuning datasets]] (83.8% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (83.7% similar)
- [[2025-09-19/ReCoVeR the Target Language_ Language Steering without Sacrificing Task Performance_20250919|ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Semantic Retrieval Framework|Semantic Retrieval Framework]], [[keywords/Multilingual Performance|Multilingual Performance]]
**âš¡ Unique Technical**: [[keywords/Cross-lingual In-context Pre-training|Cross-lingual In-context Pre-training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.20484v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2504.20484v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì˜ì–´ ì¤‘ì‹¬ì˜ ì‚¬ì „ í•™ìŠµì—ë„ ë¶ˆêµ¬í•˜ê³  ë†€ë¼ìš´ ë‹¤êµ­ì–´ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì‚¬ì „ í•™ìŠµ ì¤‘ì˜ ì–¸ì–´ ê°„ ë©”ì»¤ë‹ˆì¦˜ì— ê¸°ì¸í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì–¸ì–´ ê°„ ì „ì´ í–¥ìƒ ë°©ë²•ì€ ë³‘ë ¬ ìì›ì— ì˜í•´ ì œí•œë˜ì–´ ìˆìœ¼ë©°, ì–¸ì–´ ë° ë„ë©”ì¸ ë²”ìœ„ê°€ ì œí•œì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Cross-lingual In-context Pre-training (CrossIC-PT)ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ê°„ë‹¨í•œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ í†µí•´ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ì´ì¤‘ ì–¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì–¸ì–´ ê°„ ì „ì´ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê°„ë‹¨í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ì´ì¤‘ ì–¸ì–´ ìœ„í‚¤ë°±ê³¼ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ ì°½ì— êµì°¨ ë°°ì¹˜í•˜ì—¬ CrossIC-PT ìƒ˜í”Œì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì°½ í¬ê¸° ì œí•œì— ì ‘ê·¼í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì²´ê³„ì ì¸ ë¶„í•  ì •ì±…ì„ êµ¬í˜„í•˜ì—¬ ê¸´ ì´ì¤‘ ì–¸ì–´ ë¬¸ì„œ ìŒì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê³ , ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë©”ì»¤ë‹ˆì¦˜ì„ ì¡°ì •í•˜ì—¬ ë¬¸ë§¥ì  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ì›¹ í¬ë¡¤ë§ëœ ì½”í¼ìŠ¤ë¡œë¶€í„° CrossIC-PT ìƒ˜í”Œì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ì˜ë¯¸ì  ê²€ìƒ‰ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë°ì´í„° ê°€ìš©ì„±ì„ í™•ì¥í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” CrossIC-PTê°€ ì„¸ ê°€ì§€ ëª¨ë¸(Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-1.5B)ì—ì„œ ì—¬ì„¯ ê°œì˜ ëŒ€ìƒ ì–¸ì–´ì— ëŒ€í•œ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì£¼ë©°, ê°ê° 3.79%, 3.99%, 1.95%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì™”ê³ , ë°ì´í„° ì¦ê°• í›„ ì¶”ê°€ì ì¸ ê°œì„ ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì˜ì–´ ì¤‘ì‹¬ì˜ ì‚¬ì „ í•™ìŠµì—ë„ ë¶ˆêµ¬í•˜ê³  ë›°ì–´ë‚œ ë‹¤êµ­ì–´ ëŠ¥ë ¥ì„ ë³´ì´ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ êµì°¨ ì–¸ì–´ ì „ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì¸ Cross-lingual In-context Pre-training (CrossIC-PT)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ë³‘ë ¬ ìì›ì— ì˜ì¡´í•˜ì—¬ ì–¸ì–´ ë° ë„ë©”ì¸ ë²”ìœ„ê°€ ì œí•œì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, CrossIC-PTëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ì´ì¤‘ ì–¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ í†µí•´ êµì°¨ ì–¸ì–´ ì „ì´ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CrossIC-PTëŠ” Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-1.5B ëª¨ë¸ì—ì„œ 6ê°œ ì–¸ì–´ì— ê±¸ì³ ê°ê° 3.79%, 3.99%, 1.95%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, ë°ì´í„° ì¦ê°• í›„ ì¶”ê°€ì ì¸ ê°œì„ ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì˜ì–´ ì¤‘ì‹¬ì˜ ì‚¬ì „ í•™ìŠµì—ë„ ë¶ˆêµ¬í•˜ê³  ë›°ì–´ë‚œ ë‹¤êµ­ì–´ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì‚¬ì „ í•™ìŠµ ì¤‘ êµì°¨ ì–¸ì–´ ë©”ì»¤ë‹ˆì¦˜ ë•ë¶„ì´ë‹¤.
- 2. CrossIC-PTëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ì´ì¤‘ ì–¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ í†µí•´ êµì°¨ ì–¸ì–´ ì „ì´ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê°„ë‹¨í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì ‘ê·¼ë²•ì´ë‹¤.
- 3. CrossIC-PT ìƒ˜í”Œì€ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ì´ì¤‘ ì–¸ì–´ ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ ì°½ìœ¼ë¡œ êµì°¨ ë°°ì—´í•˜ì—¬ êµ¬ì„±ëœë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, CrossIC-PTëŠ” ì„¸ ê°€ì§€ ëª¨ë¸(Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-1.5B)ì˜ ë‹¤êµ­ì–´ ì„±ëŠ¥ì„ ì—¬ì„¯ ê°œì˜ ëŒ€ìƒ ì–¸ì–´ì—ì„œ ê°ê° 3.79%, 3.99%, 1.95% í–¥ìƒì‹œì¼°ë‹¤.
- 5. ë°ì´í„° ì¦ê°• í›„ ì¶”ê°€ì ì¸ ì„±ëŠ¥ ê°œì„ ì´ ê´€ì°°ë˜ì—ˆë‹¤.


---

*Generated on 2025-09-23 11:44:27*