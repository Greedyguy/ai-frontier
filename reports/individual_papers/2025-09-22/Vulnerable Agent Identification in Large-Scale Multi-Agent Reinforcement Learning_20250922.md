---
keywords:
  - Multi-Agent Reinforcement Learning
  - Vulnerable Agent Identification
  - Hierarchical Adversarial Decentralized Mean Field Control
  - Mean-Field Bellman Operator
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15103
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:18:24.994056",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multi-Agent Reinforcement Learning",
    "Vulnerable Agent Identification",
    "Hierarchical Adversarial Decentralized Mean Field Control",
    "Mean-Field Bellman Operator"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multi-Agent Reinforcement Learning": 0.9,
    "Vulnerable Agent Identification": 0.7,
    "Hierarchical Adversarial Decentralized Mean Field Control": 0.75,
    "Mean-Field Bellman Operator": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-Agent Reinforcement Learning",
        "canonical": "Multi-Agent Reinforcement Learning",
        "aliases": [
          "MARL"
        ],
        "category": "broad_technical",
        "rationale": "This is a core concept in the paper, linking to broader discussions on reinforcement learning in multi-agent systems.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.9
      },
      {
        "surface": "Vulnerable Agent Identification",
        "canonical": "Vulnerable Agent Identification",
        "aliases": [
          "VAI"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique technical term introduced in the paper, crucial for understanding the specific problem addressed.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Hierarchical Adversarial Decentralized Mean Field Control",
        "canonical": "Hierarchical Adversarial Decentralized Mean Field Control",
        "aliases": [
          "HAD-MFC"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a novel approach proposed in the paper, essential for linking the methodology discussed.",
        "novelty_score": 0.9,
        "connectivity_score": 0.55,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "Mean-Field Bellman Operator",
        "canonical": "Mean-Field Bellman Operator",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper's methodology, allowing connections to reinforcement learning optimization techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "agent",
      "failure",
      "system",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-Agent Reinforcement Learning",
      "resolved_canonical": "Multi-Agent Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Vulnerable Agent Identification",
      "resolved_canonical": "Vulnerable Agent Identification",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Hierarchical Adversarial Decentralized Mean Field Control",
      "resolved_canonical": "Hierarchical Adversarial Decentralized Mean Field Control",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.55,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Mean-Field Bellman Operator",
      "resolved_canonical": "Mean-Field Bellman Operator",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning

**Korean Title:** 대규모 다중 에이전트 강화 학습에서 취약 에이전트 식별

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15103.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15103](https://arxiv.org/abs/2509.15103)

## 🔗 유사한 논문
- [[2025-09-19/LEED_ A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning_20250919|LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning]] (84.8% similar)
- [[2025-09-19/Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control_20250919|Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control]] (84.8% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.9% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (82.0% similar)
- [[2025-09-22/Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context_20250922|Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context]] (81.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Multi-Agent Reinforcement Learning|Multi-Agent Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Mean-Field Bellman Operator|Mean-Field Bellman Operator]]
**⚡ Unique Technical**: [[keywords/Vulnerable Agent Identification|Vulnerable Agent Identification]], [[keywords/Hierarchical Adversarial Decentralized Mean Field Control|Hierarchical Adversarial Decentralized Mean Field Control]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15103v2 Announce Type: replace-cross 
Abstract: Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.

## 🔍 Abstract (한글 번역)

arXiv:2509.15103v2 발표 유형: 교차 교체  
초록: 시스템이 확장됨에 따라 부분적인 에이전트 실패는 불가피해지며, 전체 성능을 가장 심각하게 저하시킬 수 있는 에이전트의 하위 집합을 식별하는 것이 중요해집니다. 본 논문에서는 대규모 다중 에이전트 강화 학습(MARL)에서 이 취약 에이전트 식별(VAI) 문제를 연구합니다. 우리는 VAI를 계층적 적대적 분산 평균장 제어(HAD-MFC)로 구성하며, 상위 수준에서는 가장 취약한 에이전트를 선택하는 NP-난해한 조합 과제가 포함되고, 하위 수준에서는 평균장 MARL을 사용하여 이러한 에이전트에 대한 최악의 적대적 정책을 학습합니다. 이 두 문제는 서로 결합되어 있어 HAD-MFC를 해결하기 어렵게 만듭니다. 이를 해결하기 위해, 우리는 먼저 Fenchel-Rockafellar 변환을 통해 계층적 프로세스를 분리하여 상위 수준에 대한 정규화된 평균장 벨만 연산자를 도출하고, 각 수준에서 독립적인 학습을 가능하게 하여 계산 복잡성을 줄입니다. 그런 다음 상위 수준의 조합 문제를 우리의 정규화된 평균장 벨만 연산자로부터의 밀집 보상을 갖는 MDP로 재구성하여, 탐욕적 및 RL 알고리즘을 통해 가장 취약한 에이전트를 순차적으로 식별할 수 있게 합니다. 이 분해는 원래 HAD-MFC의 최적 솔루션을 보존함을 증명합니다. 실험 결과, 우리의 방법은 대규모 MARL 및 규칙 기반 시스템에서 더 많은 취약 에이전트를 효과적으로 식별하여 시스템을 더 심각한 실패로 유도하고, 각 에이전트의 취약성을 드러내는 가치 함수를 학습함을 보여줍니다.

## 📝 요약

이 논문은 대규모 다중 에이전트 강화 학습(MARL)에서 일부 에이전트의 실패가 시스템 성능에 미치는 영향을 최소화하기 위한 취약 에이전트 식별(VAI) 문제를 다룹니다. VAI 문제를 계층적 적대적 분산 평균장 제어(HAD-MFC)로 설정하고, 상위 단계에서 취약한 에이전트를 선택하는 NP-난해한 조합 문제를 해결하며, 하위 단계에서는 평균장 MARL을 통해 최악의 적대적 정책을 학습합니다. 이를 해결하기 위해 Fenchel-Rockafellar 변환을 사용해 계층적 과정을 분리하고, 상위 단계에서 정규화된 평균장 벨만 연산자를 도입하여 각 단계에서 독립적인 학습이 가능하게 하여 계산 복잡성을 줄였습니다. 상위 단계의 조합 문제를 MDP로 재구성하여 탐욕적 및 RL 알고리즘으로 취약한 에이전트를 순차적으로 식별할 수 있게 했습니다. 실험 결과, 제안된 방법이 대규모 MARL에서 더 많은 취약 에이전트를 효과적으로 식별하고, 시스템을 더 심각한 실패로 유도하며, 각 에이전트의 취약성을 드러내는 가치 함수를 학습함을 보여줍니다.

## 🎯 주요 포인트

- 1. 대규모 시스템에서 부분적인 에이전트 실패는 불가피하며, 가장 취약한 에이전트를 식별하는 것이 중요합니다.
- 2. 취약 에이전트 식별 문제를 계층적 적대적 분산 평균장 제어(HAD-MFC)로 정의하였습니다.
- 3. Fenchel-Rockafellar 변환을 사용하여 계층적 과정을 분리하고, 각 레벨에서 독립적인 학습을 가능하게 하여 계산 복잡성을 줄였습니다.
- 4. 상위 레벨의 조합 문제를 MDP로 재구성하여, 탐욕적 및 강화 학습 알고리즘으로 가장 취약한 에이전트를 식별할 수 있게 하였습니다.
- 5. 실험 결과, 제안된 방법이 대규모 MARL에서 더 많은 취약 에이전트를 효과적으로 식별하고, 시스템을 더 큰 실패로 유도함을 보여주었습니다.


---

*Generated on 2025-09-23 10:18:24*