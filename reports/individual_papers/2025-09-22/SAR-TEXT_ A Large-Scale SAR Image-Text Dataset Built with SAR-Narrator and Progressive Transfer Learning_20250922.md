---
keywords:
  - Vision-Language Model
  - Synthetic Aperture Radar
  - SAR-Narrator
  - Image-Text Retrieval
  - Visual Question Answering
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2507.18743
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:39:08.306430",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Synthetic Aperture Radar",
    "SAR-Narrator",
    "Image-Text Retrieval",
    "Visual Question Answering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Synthetic Aperture Radar": 0.78,
    "SAR-Narrator": 0.77,
    "Image-Text Retrieval": 0.8,
    "Visual Question Answering": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for linking multimodal datasets and tasks, reflecting recent advancements in AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Synthetic Aperture Radar",
        "canonical": "Synthetic Aperture Radar",
        "aliases": [
          "SAR"
        ],
        "category": "unique_technical",
        "rationale": "SAR is a specialized technology in remote sensing, providing unique data for image-text datasets.",
        "novelty_score": 0.67,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "SAR-Narrator",
        "canonical": "SAR-Narrator",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "SAR-Narrator is a novel framework for generating textual descriptions of SAR images, enhancing dataset creation.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Image-Text Retrieval",
        "canonical": "Image-Text Retrieval",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Image-Text Retrieval is a key task in vision-language research, facilitating the linking of visual and textual data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual Question Answering",
        "canonical": "Visual Question Answering",
        "aliases": [
          "VQA"
        ],
        "category": "specific_connectable",
        "rationale": "VQA represents a complex vision-language task that tests semantic understanding and reasoning, crucial for AI development.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "performance",
      "experiment",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Synthetic Aperture Radar",
      "resolved_canonical": "Synthetic Aperture Radar",
      "decision": "linked",
      "scores": {
        "novelty": 0.67,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "SAR-Narrator",
      "resolved_canonical": "SAR-Narrator",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Image-Text Retrieval",
      "resolved_canonical": "Image-Text Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual Question Answering",
      "resolved_canonical": "Visual Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning

**Korean Title:** SAR-TEXT: SAR-Narrator와 점진적 전이 학습을 통해 구축된 대규모 SAR 이미지-텍스트 데이터셋

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2507.18743.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2507.18743](https://arxiv.org/abs/2507.18743)

## 🔗 유사한 논문
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (84.2% similar)
- [[2025-09-22/RSCC_ A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events_20250922|RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events]] (82.8% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.4% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (80.2% similar)
- [[2025-09-22/Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation_20250922|Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation]] (80.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Image-Text Retrieval|Image-Text Retrieval]], [[keywords/Visual Question Answering|Visual Question Answering]]
**⚡ Unique Technical**: [[keywords/Synthetic Aperture Radar|Synthetic Aperture Radar]], [[keywords/SAR-Narrator|SAR-Narrator]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.18743v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the field of remote sensing in recent years. Synthetic Aperture Radar (SAR) imagery, with its all-weather capability, is essential in remote sensing, yet the lack of large-scale, high-quality SAR image-text datasets hinders its semantic understanding. In this paper, we construct SAR-TEXT, a large-scale and high-quality dataset consisting of over 130,000 SAR image-text pairs. To construct the SAR-TEXT dataset, we design the SAR-Narrator framework, which generates textual descriptions for SAR images through a multi-stage strategy. To verify the effectiveness of the SAR-TEXT dataset, we conduct experiments on three typical vision-language tasks: image-text retrieval, image captioning, and visual question answering (VQA). Specifically, we construct three representative models on SAR-TEXT: SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable improvements in retrieval performance, boosting average recall by 12.97% and 10.0% on the OSdataset_512 and HRSID test sets, respectively. In the captioning task, SAR-RS-CoCa achieves significant improvements over the original CoCa models in terms of BLEU-4, SPICE, and CIDEr scores. In the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning ability, as further confirmed by qualitative results. It is worth noting that, as a flexible captioning tool, SAR-Narrator can be readily adopted by the community to construct larger-scale SAR image-text datasets. All code, pretrained models, and the SAR-Text dataset are publicly available at: https://github.com/YiguoHe/SAR-TEXT.

## 🔍 Abstract (한글 번역)

arXiv:2507.18743v2 발표 유형: 교체  
초록: 비전 언어 모델(VLMs)은 최근 원격 탐사 분야에서 놀라운 돌파구를 이루었습니다. 모든 기상 조건에서 사용할 수 있는 합성 개구 레이더(SAR) 이미지는 원격 탐사에서 필수적이지만, 대규모 고품질 SAR 이미지-텍스트 데이터 세트의 부족은 의미론적 이해를 저해합니다. 본 논문에서는 130,000개 이상의 SAR 이미지-텍스트 쌍으로 구성된 대규모 고품질 데이터 세트인 SAR-TEXT를 구축합니다. SAR-TEXT 데이터 세트를 구축하기 위해, 우리는 다단계 전략을 통해 SAR 이미지에 대한 텍스트 설명을 생성하는 SAR-Narrator 프레임워크를 설계합니다. SAR-TEXT 데이터 세트의 효과를 검증하기 위해 이미지-텍스트 검색, 이미지 캡셔닝, 그리고 시각적 질문 응답(VQA)이라는 세 가지 일반적인 비전-언어 작업에 대한 실험을 수행합니다. 구체적으로, 우리는 SAR-TEXT에 대해 SAR-RS-CLIP, SAR-RS-CoCa, 그리고 SAR-GPT라는 세 가지 대표적인 모델을 구축합니다. SAR-RS-CLIP은 검색 성능에서 주목할 만한 향상을 이루어, OSdataset_512 및 HRSID 테스트 세트에서 각각 평균 회수를 12.97% 및 10.0% 증가시켰습니다. 캡셔닝 작업에서 SAR-RS-CoCa는 BLEU-4, SPICE, 그리고 CIDEr 점수 측면에서 원래 CoCa 모델보다 상당한 향상을 이루었습니다. VQA 작업에서는 SAR-GPT가 여러 SAR-VQA 데이터 세트에서 기준 및 단일 단계 모델을 능가하여, 더 강력한 의미론적 이해와 추론 능력을 보여주며, 이는 정성적 결과에 의해 추가로 확인되었습니다. 주목할 점은 유연한 캡셔닝 도구로서 SAR-Narrator는 커뮤니티가 더 큰 규모의 SAR 이미지-텍스트 데이터 세트를 구축하는 데 쉽게 채택할 수 있다는 것입니다. 모든 코드, 사전 학습된 모델, 그리고 SAR-Text 데이터 세트는 다음에서 공개적으로 이용 가능합니다: https://github.com/YiguoHe/SAR-TEXT.

## 📝 요약

이 논문은 원격 감지 분야에서 시각 언어 모델(VLM)의 발전을 다루며, 특히 합성 개구 레이더(SAR) 이미지의 의미적 이해를 돕기 위해 SAR-TEXT라는 대규모 고품질 데이터셋을 구축했습니다. SAR-TEXT는 13만 개 이상의 SAR 이미지-텍스트 쌍으로 구성되며, SAR-Narrator 프레임워크를 통해 생성되었습니다. 연구는 이미지-텍스트 검색, 이미지 캡션 생성, 시각적 질문 응답(VQA) 등 세 가지 주요 과제를 통해 SAR-TEXT의 효과를 검증합니다. SAR-RS-CLIP 모델은 검색 성능을 크게 향상시키고, SAR-RS-CoCa는 캡션 생성에서 BLEU-4, SPICE, CIDEr 점수를 개선했습니다. SAR-GPT는 VQA 과제에서 뛰어난 성능을 보였습니다. SAR-Narrator는 유연한 캡션 도구로, 커뮤니티에서 대규모 SAR 데이터셋 구축에 활용될 수 있습니다. 모든 코드와 모델은 공개되어 있습니다.

## 🎯 주요 포인트

- 1. SAR-TEXT는 130,000개 이상의 SAR 이미지-텍스트 쌍으로 구성된 대규모 고품질 데이터셋입니다.
- 2. SAR-Narrator 프레임워크는 다단계 전략을 통해 SAR 이미지에 대한 텍스트 설명을 생성합니다.
- 3. SAR-RS-CLIP 모델은 이미지-텍스트 검색 성능을 크게 향상시켜 평균 회수를 OSdataset_512와 HRSID 테스트 세트에서 각각 12.97%와 10.0% 증가시켰습니다.
- 4. SAR-RS-CoCa 모델은 캡셔닝 작업에서 BLEU-4, SPICE, CIDEr 점수 측면에서 원래 CoCa 모델보다 성능이 향상되었습니다.
- 5. SAR-GPT 모델은 VQA 작업에서 여러 SAR-VQA 데이터셋에서 강력한 의미 이해와 추론 능력을 보여주며, 정성적 결과로 확인되었습니다.


---

*Generated on 2025-09-23 12:39:08*