---
keywords:
  - Vision-Language Model
  - Synthetic Aperture Radar
  - SAR-Narrator
  - Image-Text Retrieval
  - Visual Question Answering
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2507.18743
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:39:08.306430",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Synthetic Aperture Radar",
    "SAR-Narrator",
    "Image-Text Retrieval",
    "Visual Question Answering"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Synthetic Aperture Radar": 0.78,
    "SAR-Narrator": 0.77,
    "Image-Text Retrieval": 0.8,
    "Visual Question Answering": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are crucial for linking multimodal datasets and tasks, reflecting recent advancements in AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Synthetic Aperture Radar",
        "canonical": "Synthetic Aperture Radar",
        "aliases": [
          "SAR"
        ],
        "category": "unique_technical",
        "rationale": "SAR is a specialized technology in remote sensing, providing unique data for image-text datasets.",
        "novelty_score": 0.67,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "SAR-Narrator",
        "canonical": "SAR-Narrator",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "SAR-Narrator is a novel framework for generating textual descriptions of SAR images, enhancing dataset creation.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      },
      {
        "surface": "Image-Text Retrieval",
        "canonical": "Image-Text Retrieval",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Image-Text Retrieval is a key task in vision-language research, facilitating the linking of visual and textual data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Visual Question Answering",
        "canonical": "Visual Question Answering",
        "aliases": [
          "VQA"
        ],
        "category": "specific_connectable",
        "rationale": "VQA represents a complex vision-language task that tests semantic understanding and reasoning, crucial for AI development.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "performance",
      "experiment",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Synthetic Aperture Radar",
      "resolved_canonical": "Synthetic Aperture Radar",
      "decision": "linked",
      "scores": {
        "novelty": 0.67,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "SAR-Narrator",
      "resolved_canonical": "SAR-Narrator",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Image-Text Retrieval",
      "resolved_canonical": "Image-Text Retrieval",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Visual Question Answering",
      "resolved_canonical": "Visual Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning

**Korean Title:** SAR-TEXT: SAR-Narratorì™€ ì ì§„ì  ì „ì´ í•™ìŠµì„ í†µí•´ êµ¬ì¶•ëœ ëŒ€ê·œëª¨ SAR ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2507.18743.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2507.18743](https://arxiv.org/abs/2507.18743)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (84.2% similar)
- [[2025-09-22/RSCC_ A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events_20250922|RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events]] (82.8% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.4% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (80.2% similar)
- [[2025-09-22/Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation_20250922|Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation]] (80.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Image-Text Retrieval|Image-Text Retrieval]], [[keywords/Visual Question Answering|Visual Question Answering]]
**âš¡ Unique Technical**: [[keywords/Synthetic Aperture Radar|Synthetic Aperture Radar]], [[keywords/SAR-Narrator|SAR-Narrator]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.18743v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the field of remote sensing in recent years. Synthetic Aperture Radar (SAR) imagery, with its all-weather capability, is essential in remote sensing, yet the lack of large-scale, high-quality SAR image-text datasets hinders its semantic understanding. In this paper, we construct SAR-TEXT, a large-scale and high-quality dataset consisting of over 130,000 SAR image-text pairs. To construct the SAR-TEXT dataset, we design the SAR-Narrator framework, which generates textual descriptions for SAR images through a multi-stage strategy. To verify the effectiveness of the SAR-TEXT dataset, we conduct experiments on three typical vision-language tasks: image-text retrieval, image captioning, and visual question answering (VQA). Specifically, we construct three representative models on SAR-TEXT: SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable improvements in retrieval performance, boosting average recall by 12.97% and 10.0% on the OSdataset_512 and HRSID test sets, respectively. In the captioning task, SAR-RS-CoCa achieves significant improvements over the original CoCa models in terms of BLEU-4, SPICE, and CIDEr scores. In the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning ability, as further confirmed by qualitative results. It is worth noting that, as a flexible captioning tool, SAR-Narrator can be readily adopted by the community to construct larger-scale SAR image-text datasets. All code, pretrained models, and the SAR-Text dataset are publicly available at: https://github.com/YiguoHe/SAR-TEXT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.18743v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ìµœê·¼ ì›ê²© íƒì‚¬ ë¶„ì•¼ì—ì„œ ë†€ë¼ìš´ ëŒíŒŒêµ¬ë¥¼ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ê¸°ìƒ ì¡°ê±´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•©ì„± ê°œêµ¬ ë ˆì´ë”(SAR) ì´ë¯¸ì§€ëŠ” ì›ê²© íƒì‚¬ì—ì„œ í•„ìˆ˜ì ì´ì§€ë§Œ, ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ SAR ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì˜ ë¶€ì¡±ì€ ì˜ë¯¸ë¡ ì  ì´í•´ë¥¼ ì €í•´í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 130,000ê°œ ì´ìƒì˜ SAR ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„° ì„¸íŠ¸ì¸ SAR-TEXTë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. SAR-TEXT ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë‹¤ë‹¨ê³„ ì „ëµì„ í†µí•´ SAR ì´ë¯¸ì§€ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” SAR-Narrator í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤. SAR-TEXT ë°ì´í„° ì„¸íŠ¸ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰, ì´ë¯¸ì§€ ìº¡ì…”ë‹, ê·¸ë¦¬ê³  ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ(VQA)ì´ë¼ëŠ” ì„¸ ê°€ì§€ ì¼ë°˜ì ì¸ ë¹„ì „-ì–¸ì–´ ì‘ì—…ì— ëŒ€í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” SAR-TEXTì— ëŒ€í•´ SAR-RS-CLIP, SAR-RS-CoCa, ê·¸ë¦¬ê³  SAR-GPTë¼ëŠ” ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. SAR-RS-CLIPì€ ê²€ìƒ‰ ì„±ëŠ¥ì—ì„œ ì£¼ëª©í•  ë§Œí•œ í–¥ìƒì„ ì´ë£¨ì–´, OSdataset_512 ë° HRSID í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê°ê° í‰ê·  íšŒìˆ˜ë¥¼ 12.97% ë° 10.0% ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤. ìº¡ì…”ë‹ ì‘ì—…ì—ì„œ SAR-RS-CoCaëŠ” BLEU-4, SPICE, ê·¸ë¦¬ê³  CIDEr ì ìˆ˜ ì¸¡ë©´ì—ì„œ ì›ë˜ CoCa ëª¨ë¸ë³´ë‹¤ ìƒë‹¹í•œ í–¥ìƒì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. VQA ì‘ì—…ì—ì„œëŠ” SAR-GPTê°€ ì—¬ëŸ¬ SAR-VQA ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê¸°ì¤€ ë° ë‹¨ì¼ ë‹¨ê³„ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ì—¬, ë” ê°•ë ¥í•œ ì˜ë¯¸ë¡ ì  ì´í•´ì™€ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì •ì„±ì  ê²°ê³¼ì— ì˜í•´ ì¶”ê°€ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ëª©í•  ì ì€ ìœ ì—°í•œ ìº¡ì…”ë‹ ë„êµ¬ë¡œì„œ SAR-NarratorëŠ” ì»¤ë®¤ë‹ˆí‹°ê°€ ë” í° ê·œëª¨ì˜ SAR ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ì‰½ê²Œ ì±„íƒí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë“  ì½”ë“œ, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸, ê·¸ë¦¬ê³  SAR-Text ë°ì´í„° ì„¸íŠ¸ëŠ” ë‹¤ìŒì—ì„œ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤: https://github.com/YiguoHe/SAR-TEXT.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì›ê²© ê°ì§€ ë¶„ì•¼ì—ì„œ ì‹œê° ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ë°œì „ì„ ë‹¤ë£¨ë©°, íŠ¹íˆ í•©ì„± ê°œêµ¬ ë ˆì´ë”(SAR) ì´ë¯¸ì§€ì˜ ì˜ë¯¸ì  ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ SAR-TEXTë¼ëŠ” ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. SAR-TEXTëŠ” 13ë§Œ ê°œ ì´ìƒì˜ SAR ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ë©°, SAR-Narrator í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰, ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ(VQA) ë“± ì„¸ ê°€ì§€ ì£¼ìš” ê³¼ì œë¥¼ í†µí•´ SAR-TEXTì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. SAR-RS-CLIP ëª¨ë¸ì€ ê²€ìƒ‰ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³ , SAR-RS-CoCaëŠ” ìº¡ì…˜ ìƒì„±ì—ì„œ BLEU-4, SPICE, CIDEr ì ìˆ˜ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤. SAR-GPTëŠ” VQA ê³¼ì œì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. SAR-NarratorëŠ” ìœ ì—°í•œ ìº¡ì…˜ ë„êµ¬ë¡œ, ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ëŒ€ê·œëª¨ SAR ë°ì´í„°ì…‹ êµ¬ì¶•ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì½”ë“œì™€ ëª¨ë¸ì€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SAR-TEXTëŠ” 130,000ê°œ ì´ìƒì˜ SAR ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
- 2. SAR-Narrator í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ë‹¨ê³„ ì „ëµì„ í†µí•´ SAR ì´ë¯¸ì§€ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 3. SAR-RS-CLIP ëª¨ë¸ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼œ í‰ê·  íšŒìˆ˜ë¥¼ OSdataset_512ì™€ HRSID í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê°ê° 12.97%ì™€ 10.0% ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤.
- 4. SAR-RS-CoCa ëª¨ë¸ì€ ìº¡ì…”ë‹ ì‘ì—…ì—ì„œ BLEU-4, SPICE, CIDEr ì ìˆ˜ ì¸¡ë©´ì—ì„œ ì›ë˜ CoCa ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. SAR-GPT ëª¨ë¸ì€ VQA ì‘ì—…ì—ì„œ ì—¬ëŸ¬ SAR-VQA ë°ì´í„°ì…‹ì—ì„œ ê°•ë ¥í•œ ì˜ë¯¸ ì´í•´ì™€ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ì •ì„±ì  ê²°ê³¼ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:39:08*