---
keywords:
  - Transformer
  - Self-supervised Learning
  - Mental Rotation
  - Vision-Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15271
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:52:15.243666",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Self-supervised Learning",
    "Mental Rotation",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Self-supervised Learning": 0.8,
    "Mental Rotation": 0.78,
    "Vision-Language Model": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the study and application of large vision models, providing a direct link to the broader field of deep learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-supervised ViTs",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "self-supervised vision transformers"
        ],
        "category": "specific_connectable",
        "rationale": "Self-supervised learning is a key technique in enhancing model performance, particularly in capturing geometric structures.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "Mental Rotation",
        "canonical": "Mental Rotation",
        "aliases": [
          "spatial reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Mental rotation is a unique cognitive task that is central to the paper's evaluation of model capabilities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.67,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language models represent an evolved concept that bridges visual and textual data, relevant to the paper's exploration of model capabilities.",
        "novelty_score": 0.68,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "model representations",
      "layer by layer",
      "task difficulty"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-supervised ViTs",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Mental Rotation",
      "resolved_canonical": "Mental Rotation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.67,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Large Vision Models Can Solve Mental Rotation Problems

**Korean Title:** ëŒ€í˜• ë¹„ì „ ëª¨ë¸ì€ ì •ì‹  íšŒì „ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15271.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15271](https://arxiv.org/abs/2509.15271)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Simulated Cortical Magnification Supports Self-Supervised Object Learning_20250922|Simulated Cortical Magnification Supports Self-Supervised Object Learning]] (83.7% similar)
- [[2025-09-22/Modeling the Human Visual System_ Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms_20250922|Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms]] (82.8% similar)
- [[2025-09-22/Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks_20250922|Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks]] (82.2% similar)
- [[2025-09-19/Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models_20250919|Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models]] (82.2% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (81.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Mental Rotation|Mental Rotation]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15271v1 Announce Type: cross 
Abstract: Mental rotation is a key test of spatial reasoning in humans and has been central to understanding how perception supports cognition. Despite the success of modern vision transformers, it is still unclear how well these models develop similar abilities. In this work, we present a systematic evaluation of ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from simple block structures similar to those used by Shepard and Metzler to study human cognition, to more complex block figures, three types of text, and photo-realistic objects. By probing model representations layer by layer, we examine where and how these networks succeed. We find that i) self-supervised ViTs capture geometric structure better than supervised ViTs; ii) intermediate layers perform better than final layers; iii) task difficulty increases with rotation complexity and occlusion, mirroring human reaction times and suggesting similar constraints in embedding space representations.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15271v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì •ì‹ ì  íšŒì „ì€ ì¸ê°„ì˜ ê³µê°„ ì¶”ë¡ ì˜ í•µì‹¬ ì‹œí—˜ì´ë©°, ì§€ê°ì´ ì¸ì§€ë¥¼ ì–´ë–»ê²Œ ì§€ì›í•˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ì‹¬ì ì¸ ì—­í• ì„ í•´ì™”ìŠµë‹ˆë‹¤. í˜„ëŒ€ì˜ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì„±ê³µì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì´ ìœ ì‚¬í•œ ëŠ¥ë ¥ì„ ì–¼ë§ˆë‚˜ ì˜ ê°œë°œí•˜ëŠ”ì§€ëŠ” ì—¬ì „íˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” Shepardì™€ Metzlerê°€ ì¸ê°„ ì¸ì§€ë¥¼ ì—°êµ¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œ ë‹¨ìˆœí•œ ë¸”ë¡ êµ¬ì¡°ë¶€í„° ë³µì¡í•œ ë¸”ë¡ ë„í˜•, ì„¸ ê°€ì§€ ìœ í˜•ì˜ í…ìŠ¤íŠ¸, ì‚¬ì§„ê³¼ ê°™ì€ í˜„ì‹¤ì ì¸ ê°ì²´ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ì •ì‹ ì  íšŒì „ ê³¼ì œë¥¼ í†µí•´ ViT, CLIP, DINOv2, DINOv3ì— ëŒ€í•œ ì²´ê³„ì ì¸ í‰ê°€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ëª¨ë¸ í‘œí˜„ì„ ì¸µë³„ë¡œ ì¡°ì‚¬í•˜ì—¬ ì´ëŸ¬í•œ ë„¤íŠ¸ì›Œí¬ê°€ ì–´ë””ì—ì„œ ì–´ë–»ê²Œ ì„±ê³µí•˜ëŠ”ì§€ë¥¼ ê²€í† í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” i) ìê°€ ì§€ë„ ViTê°€ ì§€ë„ í•™ìŠµëœ ViTë³´ë‹¤ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë” ì˜ í¬ì°©í•œë‹¤ëŠ” ê²ƒ, ii) ì¤‘ê°„ ì¸µì´ ìµœì¢… ì¸µë³´ë‹¤ ë” ì˜ ìˆ˜í–‰í•œë‹¤ëŠ” ê²ƒ, iii) íšŒì „ ë³µì¡ì„±ê³¼ íìƒ‰ì— ë”°ë¼ ê³¼ì œ ë‚œì´ë„ê°€ ì¦ê°€í•˜ì—¬ ì¸ê°„ì˜ ë°˜ì‘ ì‹œê°„ê³¼ ìœ ì‚¬í•œ ì œì•½ì„ ì„ë² ë”© ê³µê°„ í‘œí˜„ì—ì„œ ì œì•ˆí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì¸ê°„ì˜ ê³µê°„ ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì •ì‹  íšŒì „ ê³¼ì œë¥¼ í†µí•´ ìµœì‹  ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT, CLIP, DINOv2, DINOv3)ì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ìê¸° ì§€ë„ í•™ìŠµëœ ViTê°€ ì§€ë„ í•™ìŠµëœ ëª¨ë¸ë³´ë‹¤ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë” ì˜ í¬ì°©í•˜ë©°, ì¤‘ê°„ ê³„ì¸µì´ ìµœì¢… ê³„ì¸µë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, íšŒì „ ë³µì¡ì„±ê³¼ ê°€ë¦¼ í˜„ìƒì´ ì¦ê°€í• ìˆ˜ë¡ ê³¼ì œ ë‚œì´ë„ê°€ ë†’ì•„ì§€ë©°, ì´ëŠ” ì¸ê°„ì˜ ë°˜ì‘ ì‹œê°„ê³¼ ìœ ì‚¬í•œ ì œì•½ì„ ëª¨ë¸ì˜ ì„ë² ë”© ê³µê°„ì—ì„œë„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìê¸° ì§€ë„ í•™ìŠµëœ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT)ëŠ” ì§€ë„ í•™ìŠµëœ ViTë³´ë‹¤ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë” ì˜ í¬ì°©í•œë‹¤.
- 2. ì¤‘ê°„ ê³„ì¸µì´ ìµœì¢… ê³„ì¸µë³´ë‹¤ ì •ì‹  íšŒì „ ê³¼ì œ ìˆ˜í–‰ì— ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 3. íšŒì „ ë³µì¡ì„±ê³¼ ê°€ë¦¼ í˜„ìƒì´ ì¦ê°€í•¨ì— ë”°ë¼ ê³¼ì œ ë‚œì´ë„ê°€ ì¦ê°€í•˜ë©°, ì´ëŠ” ì¸ê°„ì˜ ë°˜ì‘ ì‹œê°„ê³¼ ìœ ì‚¬í•œ ì œì•½ì„ ì‹œì‚¬í•œë‹¤.
- 4. ë‹¤ì–‘í•œ ì •ì‹  íšŒì „ ê³¼ì œì—ì„œ ViT, CLIP, DINOv2, DINOv3 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ì˜€ë‹¤.
- 5. ëª¨ë¸ í‘œí˜„ì„ ê³„ì¸µë³„ë¡œ ë¶„ì„í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ê°€ ì„±ê³µí•˜ëŠ” ì§€ì ì„ ì¡°ì‚¬í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-23 08:52:15*