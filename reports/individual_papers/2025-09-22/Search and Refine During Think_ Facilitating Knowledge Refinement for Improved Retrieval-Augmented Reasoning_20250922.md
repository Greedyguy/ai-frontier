---
keywords:
  - Large Language Model
  - Retrieval Augmented Generation
  - AutoRefine
  - Reinforcement Learning
  - Group Relative Policy Optimization
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.11277
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:57:07.124988",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Retrieval Augmented Generation",
    "AutoRefine",
    "Reinforcement Learning",
    "Group Relative Policy Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Retrieval Augmented Generation": 0.9,
    "AutoRefine": 0.8,
    "Reinforcement Learning": 0.8,
    "Group Relative Policy Optimization": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental concept in the paper, linking to a broad area of AI research.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Retrieval-Augmented Reasoning",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG",
          "Retrieval-Augmented Generation"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's method, connecting to recent advancements in retrieval-augmented techniques.",
        "novelty_score": 0.75,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "AutoRefine",
        "canonical": "AutoRefine",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel framework introduced in the paper, crucial for understanding the proposed methodology.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A key technique used in the paper's methodology, linking to a major area in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "A specific optimization technique introduced in the paper, important for understanding the implementation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "search-and-refine-during-think",
      "knowledge refinement steps"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Retrieval-Augmented Reasoning",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "AutoRefine",
      "resolved_canonical": "AutoRefine",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning

**Korean Title:** ê²€ìƒ‰ ë° ì‚¬ê³  ì¤‘ ì •ì œ: ê²€ìƒ‰ ì¦ê°• ì¶”ë¡ ì„ ìœ„í•œ ì§€ì‹ ì •ì œ ì´‰ì§„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.11277.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.11277](https://arxiv.org/abs/2505.11277)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models_20250922|Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models]] (86.2% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (84.4% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (84.3% similar)
- [[2025-09-22/FLARE_ Faithful Logic-Aided Reasoning and Exploration_20250922|FLARE: Faithful Logic-Aided Reasoning and Exploration]] (84.3% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility: Process-Supervised Rewrite for RAG]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**âš¡ Unique Technical**: [[keywords/AutoRefine|AutoRefine]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.11277v5 Announce Type: replace-cross 
Abstract: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new "search-and-refine-during-think" paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.11277v5 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì¸ìƒì ì¸ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ë³¸ì§ˆì ìœ¼ë¡œ ì§€ì‹ ì €ì¥ì†Œì˜ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ë³´ê°• ì¶”ë¡ ì€ LLMì´ ì™¸ë¶€ ìì›ì„ ì¡°íšŒí•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì´ëŸ¬í•œ í•œê³„ë¥¼ ì™„í™”í•˜ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ì€ ì¢…ì¢… ê´€ë ¨ì´ ì—†ê±°ë‚˜ ì¡ìŒì´ ë§ì€ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ì¶”ë¡ ì„ ë°©í•´í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” "ìƒê°í•˜ëŠ” ë™ì•ˆ ê²€ìƒ‰ ë° ì •ì œ"ë¼ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì±„íƒí•œ ê°•í™” í•™ìŠµ ì‚¬í›„ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ì¸ AutoRefineì„ ì œì•ˆí•©ë‹ˆë‹¤. AutoRefineì€ ì—°ì†ì ì¸ ê²€ìƒ‰ í˜¸ì¶œ ì‚¬ì´ì— ëª…ì‹œì ì¸ ì§€ì‹ ì •ì œ ë‹¨ê³„ë¥¼ ë„ì…í•˜ì—¬ ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì „ì— ì¦ê±°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•„í„°ë§í•˜ê³ , ì •ì œí•˜ê³ , ì¡°ì§í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì˜ ì •í™•ì„± ë³´ìƒê³¼ í•¨ê»˜ ë§ì¶¤í˜• ê²€ìƒ‰ íŠ¹ì • ë³´ìƒì„ í†µí•©í•©ë‹ˆë‹¤. ë‹¨ì¼ í™‰ ë° ë‹¤ì¤‘ í™‰ QA ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ì€ AutoRefineì´ íŠ¹íˆ ë³µì¡í•œ ë‹¤ì¤‘ í™‰ ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì„ í¬ê²Œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìì„¸í•œ ë¶„ì„ì€ AutoRefineì´ ë¹ˆë²ˆí•˜ê³  ë†’ì€ í’ˆì§ˆì˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ì¦ê±°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¢…í•©í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì œí•œëœ ì§€ì‹ ì €ì¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì„ í†µí•œ ì™¸ë¶€ ì •ë³´ í™œìš© ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¢…ì¢… ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ì¶”ë¡ ì„ ë°©í•´í•˜ì§€ë§Œ, AutoRefineë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ëŠ” ì´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. AutoRefineëŠ” ê°•í™” í•™ìŠµì„ í™œìš©í•˜ì—¬ "ìƒê° ì¤‘ ê²€ìƒ‰ ë° ì •ì œ" íŒ¨ëŸ¬ë‹¤ì„ì„ ë„ì…í•˜ê³ , ê²€ìƒ‰ í›„ ëª…ì‹œì ì¸ ì§€ì‹ ì •ì œ ë‹¨ê³„ë¥¼ ì¶”ê°€í•˜ì—¬ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•„í„°ë§í•˜ê³  ì¡°ì§í™”í•©ë‹ˆë‹¤. ë˜í•œ, ê²€ìƒ‰ íŠ¹í™” ë³´ìƒê³¼ ë‹µë³€ ì •í™•ì„± ë³´ìƒì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, AutoRefineëŠ” íŠ¹íˆ ë³µì¡í•œ ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AutoRefineëŠ” "ê²€ìƒ‰ ë° ì •ì œ ì¤‘ ì‚¬ê³ "ë¼ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ë„ì…í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•©ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ê²€ìƒ‰ í˜¸ì¶œ ì‚¬ì´ì— ëª…ì‹œì ì¸ ì§€ì‹ ì •ì œ ë‹¨ê³„ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë¸ì´ ì¦ê±°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•„í„°ë§, ì •ì œ ë° ì¡°ì§í™”í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- 3. ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ íŠ¹í™” ë³´ìƒê³¼ ë‹µë³€ ì •í™•ì„± ë³´ìƒì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, AutoRefineëŠ” íŠ¹íˆ ë³µì¡í•œ ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. AutoRefineëŠ” ë¹ˆë²ˆí•˜ê³  ë†’ì€ í’ˆì§ˆì˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ë©°, íš¨ê³¼ì ìœ¼ë¡œ ì¦ê±°ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:57:07*