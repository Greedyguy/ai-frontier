---
keywords:
  - Large Language Model
  - Retrieval Augmented Generation
  - AutoRefine
  - Reinforcement Learning
  - Group Relative Policy Optimization
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.11277
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:57:07.124988",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Retrieval Augmented Generation",
    "AutoRefine",
    "Reinforcement Learning",
    "Group Relative Policy Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Retrieval Augmented Generation": 0.9,
    "AutoRefine": 0.8,
    "Reinforcement Learning": 0.8,
    "Group Relative Policy Optimization": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental concept in the paper, linking to a broad area of AI research.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Retrieval-Augmented Reasoning",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG",
          "Retrieval-Augmented Generation"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's method, connecting to recent advancements in retrieval-augmented techniques.",
        "novelty_score": 0.75,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "AutoRefine",
        "canonical": "AutoRefine",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel framework introduced in the paper, crucial for understanding the proposed methodology.",
        "novelty_score": 0.95,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A key technique used in the paper's methodology, linking to a major area in AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "A specific optimization technique introduced in the paper, important for understanding the implementation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "search-and-refine-during-think",
      "knowledge refinement steps"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Retrieval-Augmented Reasoning",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "AutoRefine",
      "resolved_canonical": "AutoRefine",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning

**Korean Title:** 검색 및 사고 중 정제: 검색 증강 추론을 위한 지식 정제 촉진

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.11277.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.11277](https://arxiv.org/abs/2505.11277)

## 🔗 유사한 논문
- [[2025-09-22/Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models_20250922|Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models]] (86.2% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know?_20250922|Do Retrieval Augmented Language Models Know When They Don't Know?]] (84.4% similar)
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (84.3% similar)
- [[2025-09-22/FLARE_ Faithful Logic-Aided Reasoning and Exploration_20250922|FLARE: Faithful Logic-Aided Reasoning and Exploration]] (84.3% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility: Process-Supervised Rewrite for RAG]] (84.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]]
**⚡ Unique Technical**: [[keywords/AutoRefine|AutoRefine]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.11277v5 Announce Type: replace-cross 
Abstract: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new "search-and-refine-during-think" paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.

## 🔍 Abstract (한글 번역)

arXiv:2505.11277v5 발표 유형: 교차 교체  
초록: 대형 언어 모델은 인상적인 추론 능력을 보여주었지만, 본질적으로 지식 저장소의 한계가 있습니다. 검색 보강 추론은 LLM이 외부 자원을 조회할 수 있도록 하여 이러한 한계를 완화하지만, 기존 방법은 종종 관련이 없거나 잡음이 많은 정보를 검색하여 정확한 추론을 방해합니다. 본 논문에서는 "생각하는 동안 검색 및 정제"라는 새로운 패러다임을 채택한 강화 학습 사후 훈련 프레임워크인 AutoRefine을 제안합니다. AutoRefine은 연속적인 검색 호출 사이에 명시적인 지식 정제 단계를 도입하여 모델이 답변을 생성하기 전에 증거를 반복적으로 필터링하고, 정제하고, 조직할 수 있도록 합니다. 또한, 그룹 상대 정책 최적화를 사용하여 답변의 정확성 보상과 함께 맞춤형 검색 특정 보상을 통합합니다. 단일 홉 및 다중 홉 QA 벤치마크에 대한 실험은 AutoRefine이 특히 복잡한 다중 홉 추론 시나리오에서 기존 접근 방식을 크게 능가함을 보여줍니다. 자세한 분석은 AutoRefine이 빈번하고 높은 품질의 검색을 수행하고 증거를 효과적으로 종합함을 보여줍니다.

## 📝 요약

이 논문에서는 대형 언어 모델(LLM)의 제한된 지식 저장 문제를 해결하기 위해 검색을 통한 외부 정보 활용 방법을 제안합니다. 기존 방법들은 종종 관련 없는 정보를 검색하여 정확한 추론을 방해하지만, AutoRefine라는 새로운 프레임워크는 이를 개선합니다. AutoRefine는 강화 학습을 활용하여 "생각 중 검색 및 정제" 패러다임을 도입하고, 검색 후 명시적인 지식 정제 단계를 추가하여 정보를 체계적으로 필터링하고 조직화합니다. 또한, 검색 특화 보상과 답변 정확성 보상을 결합하여 성능을 향상시킵니다. 실험 결과, AutoRefine는 특히 복잡한 다중 단계 추론 시나리오에서 기존 방법보다 뛰어난 성능을 보였습니다.

## 🎯 주요 포인트

- 1. AutoRefine는 "검색 및 정제 중 사고"라는 새로운 패러다임을 도입하여 대형 언어 모델의 추론 능력을 강화합니다.
- 2. 이 프레임워크는 검색 호출 사이에 명시적인 지식 정제 단계를 포함하여 모델이 증거를 반복적으로 필터링, 정제 및 조직화할 수 있도록 합니다.
- 3. 그룹 상대 정책 최적화를 사용하여 검색 특화 보상과 답변 정확성 보상을 결합하여 성능을 향상시킵니다.
- 4. 실험 결과, AutoRefine는 특히 복잡한 다중 단계 추론 시나리오에서 기존 접근 방식보다 우수한 성능을 보입니다.
- 5. AutoRefine는 빈번하고 높은 품질의 검색을 수행하며, 효과적으로 증거를 종합합니다.


---

*Generated on 2025-09-23 09:57:07*