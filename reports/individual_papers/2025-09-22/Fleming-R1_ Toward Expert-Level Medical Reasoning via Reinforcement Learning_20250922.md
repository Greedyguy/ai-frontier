---
keywords:
  - Reinforcement Learning from Verifiable Rewards
  - Chain-of-Thought
  - Reasoning-Oriented Data Strategy
  - Knowledge-Graph-Guided Synthesis
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15279
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:19:50.756192",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning from Verifiable Rewards",
    "Chain-of-Thought",
    "Reasoning-Oriented Data Strategy",
    "Knowledge-Graph-Guided Synthesis"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning from Verifiable Rewards": 0.78,
    "Chain-of-Thought": 0.85,
    "Reasoning-Oriented Data Strategy": 0.75,
    "Knowledge-Graph-Guided Synthesis": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning from Verifiable Rewards",
        "canonical": "Reinforcement Learning from Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach specific to the paper, enhancing reinforcement learning with verifiable outcomes.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT"
        ],
        "category": "specific_connectable",
        "rationale": "Chain-of-Thought is a recognized reasoning method that improves understanding of reasoning processes.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reasoning-Oriented Data Strategy",
        "canonical": "Reasoning-Oriented Data Strategy",
        "aliases": [
          "RODS"
        ],
        "category": "unique_technical",
        "rationale": "This strategy is unique to the paper and enhances data handling for medical reasoning.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Knowledge-Graph-Guided Synthesis",
        "canonical": "Knowledge-Graph-Guided Synthesis",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This method leverages knowledge graphs to enhance data synthesis, relevant for linking data-driven insights.",
        "novelty_score": 0.7,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "medical reasoning",
      "clinical reasoning",
      "large language models",
      "parameter-efficient improvements"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning from Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning from Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reasoning-Oriented Data Strategy",
      "resolved_canonical": "Reasoning-Oriented Data Strategy",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Knowledge-Graph-Guided Synthesis",
      "resolved_canonical": "Knowledge-Graph-Guided Synthesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning

**Korean Title:** Fleming-R1: ê°•í™” í•™ìŠµì„ í†µí•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì˜í•™ì  ì¶”ë¡ ì„ í–¥í•˜ì—¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15279.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15279](https://arxiv.org/abs/2509.15279)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/MedFact-R1_ Towards Factual Medical Reasoning via Pseudo-Label Augmentation_20250919|MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation]] (86.8% similar)
- [[2025-09-22/EyePCR_ A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery_20250922|EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery]] (82.7% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (82.7% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (82.4% similar)
- [[2025-09-22/FLARE_ Faithful Logic-Aided Reasoning and Exploration_20250922|FLARE: Faithful Logic-Aided Reasoning and Exploration]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/Knowledge-Graph-Guided Synthesis|Knowledge-Graph-Guided Synthesis]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning from Verifiable Rewards|Reinforcement Learning from Verifiable Rewards]], [[keywords/Reasoning-Oriented Data Strategy|Reasoning-Oriented Data Strategy]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15279v1 Announce Type: new 
Abstract: While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15279v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì„ìƒ ì¶”ë¡ ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ ì •í™•í•œ ë‹µë³€ê³¼ íˆ¬ëª…í•œ ì¶”ë¡  ê³¼ì •ì„ ëª¨ë‘ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì—¬ì „íˆ ë„ì „ì ì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Fleming-R1ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŠ” ì„¸ ê°€ì§€ ìƒí˜¸ ë³´ì™„ì ì¸ í˜ì‹ ì„ í†µí•´ ê²€ì¦ ê°€ëŠ¥í•œ ì˜ë£Œ ì¶”ë¡ ì„ ëª©í‘œë¡œ ì„¤ê³„ëœ ëª¨ë¸ì…ë‹ˆë‹¤. ì²«ì§¸, ìš°ë¦¬ì˜ ì¶”ë¡  ì§€í–¥ ë°ì´í„° ì „ëµ(Reasoning-Oriented Data Strategy, RODS)ì€ íë ˆì´ì…˜ëœ ì˜ë£Œ QA ë°ì´í„°ì…‹ê³¼ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ í•©ì„±ì„ ê²°í•©í•˜ì—¬ ëŒ€í‘œì„±ì´ ë¶€ì¡±í•œ ì§ˆë³‘, ì•½ë¬¼, ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡  ì²´ì¸ì˜ ë²”ìœ„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ìš°ë¦¬ëŠ” ì‚¬ìŠ¬í˜• ì‚¬ê³ (Chain-of-Thought, CoT) ì½œë“œ ìŠ¤íƒ€íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµì‚¬ ëª¨ë¸ë¡œë¶€í„° ê³ í’ˆì§ˆì˜ ì¶”ë¡  ê²½ë¡œë¥¼ ì¶”ì¶œí•˜ì—¬ ê°•ë ¥í•œ ì¶”ë¡  ì‚¬ì „ ì§€ì‹ì„ í™•ë¦½í•©ë‹ˆë‹¤. ì…‹ì§¸, ìš°ë¦¬ëŠ” ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒìœ¼ë¡œë¶€í„°ì˜ ê°•í™” í•™ìŠµ(Reinforcement Learning from Verifiable Rewards, RLVR) ë‘ ë‹¨ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬í˜„í•˜ì—¬, ì ì‘í˜• ì–´ë ¤ìš´ ìƒ˜í”Œ ì±„êµ´ì„ í†µí•´ ì§€ì†ì ì¸ ì‹¤íŒ¨ ëª¨ë“œë¥¼ ëª©í‘œë¡œ í•˜ë©´ì„œ í•µì‹¬ ì¶”ë¡  ê¸°ìˆ ì„ í†µí•©í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ, Fleming-R1ì€ ìƒë‹¹í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤: 7B ë³€í˜•ì€ í›¨ì”¬ ë” í° ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ë©°, 32B ëª¨ë¸ì€ GPT-4oì™€ ê±°ì˜ ë™ë“±í•œ ìˆ˜ì¤€ì— ë„ë‹¬í•˜ê³  ê°•ë ¥í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ëŒ€ì•ˆì„ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„° ì„¤ê³„, ì¶”ë¡  ì§€í–¥ ì´ˆê¸°í™”, ê²€ì¦ ê°€ëŠ¥í•œ ê°•í™” í•™ìŠµì´ ë‹¨ìˆœí•œ ì •í™•ì„± ìµœì í™”ë¥¼ ë„˜ì–´ ì„ìƒ ì¶”ë¡ ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Fleming-R1ì„ ê³µê°œí•˜ì—¬ ì˜ë£Œ AIì˜ íˆ¬ëª…í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•˜ë©° ê°ì‚¬ ê°€ëŠ¥í•œ ë°œì „ì„ ì´‰ì§„í•˜ê³ , ê³ ìœ„í—˜ ì„ìƒ í™˜ê²½ì—ì„œì˜ ì•ˆì „í•œ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Fleming-R1 ëª¨ë¸ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì„ìƒ ì¶”ë¡ ì„ ëª©í‘œë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” ì²«ì§¸, Reasoning-Oriented Data Strategy (RODS)ë¥¼ í†µí•´ ì˜ë£Œ QA ë°ì´í„°ì…‹ê³¼ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë³‘, ì•½ë¬¼, ë‹¤ì¤‘ ì¶”ë¡  ì²´ì¸ì˜ í¬ê´„ì„±ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, Chain-of-Thought (CoT) ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ê³ í’ˆì§ˆ ì¶”ë¡  ê²½ë¡œë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ì…‹ì§¸, Group Relative Policy Optimizationì„ ì‚¬ìš©í•˜ëŠ” ë‘ ë‹¨ê³„ì˜ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ í•µì‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼, Fleming-R1ì€ ë‹¤ì–‘í•œ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ 7B ëª¨ë¸ì€ ë” í° ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê³ , 32B ëª¨ë¸ì€ GPT-4oì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì„±ê³¼ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„° ì„¤ê³„ì™€ ê²€ì¦ ê°€ëŠ¥í•œ ê°•í™” í•™ìŠµì´ ì„ìƒ ì¶”ë¡ ì„ ë‹¨ìˆœí•œ ì •í™•ë„ ìµœì í™”ë¥¼ ë„˜ì–´ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Fleming-R1ì€ ì˜ë£Œ AIì˜ íˆ¬ëª…í•˜ê³  ì•ˆì „í•œ ë°œì „ì„ ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Fleming-R1 ëª¨ë¸ì€ ê²€ì¦ ê°€ëŠ¥í•œ ì˜ë£Œ ì¶”ë¡ ì„ ìœ„í•´ ì„¸ ê°€ì§€ í˜ì‹ ì„ ê²°í•©í•˜ì—¬ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 2. RODS ì „ëµì€ ì˜ë£Œ QA ë°ì´í„°ì…‹ê³¼ ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ ëœ ëŒ€í‘œë˜ëŠ” ì§ˆë³‘, ì•½ë¬¼, ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡ ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 3. Chain-of-Thought ì´ˆê¸°í™”ë¥¼ í†µí•´ ê³ í’ˆì§ˆ ì¶”ë¡  ê²½ë¡œë¥¼ êµ¬ì¶•í•˜ê³  ê°•ë ¥í•œ ì¶”ë¡  ì‚¬ì „ ì§€ì‹ì„ í™•ë¦½í•©ë‹ˆë‹¤.
- 4. RLVR í”„ë ˆì„ì›Œí¬ëŠ” ê·¸ë£¹ ìƒëŒ€ ì •ì±… ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ì¶”ë¡  ê¸°ìˆ ì„ ê°•í™”í•˜ê³  ì‹¤íŒ¨ ëª¨ë“œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 5. Fleming-R1ì€ ë‹¤ì–‘í•œ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ 7B ë° 32B ëª¨ë¸ì´ ëŒ€í˜• ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:19:50*