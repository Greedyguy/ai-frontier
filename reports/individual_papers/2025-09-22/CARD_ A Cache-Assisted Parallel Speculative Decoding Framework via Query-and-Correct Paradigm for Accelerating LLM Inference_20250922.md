---
keywords:
  - Speculative Decoding
  - Cache-Assisted Parallel Speculative Decoding
  - Large Language Model
  - Query-and-Correct Paradigm
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2508.04462
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:10:52.397771",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Speculative Decoding",
    "Cache-Assisted Parallel Speculative Decoding",
    "Large Language Model",
    "Query-and-Correct Paradigm"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Speculative Decoding": 0.78,
    "Cache-Assisted Parallel Speculative Decoding": 0.8,
    "Large Language Model": 0.7,
    "Query-and-Correct Paradigm": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [
          "SD"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to LLM inference that can be linked to advancements in decoding methods.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cache-Assisted Parallel Speculative Decoding",
        "canonical": "Cache-Assisted Parallel Speculative Decoding",
        "aliases": [
          "CARD"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific framework that enhances speculative decoding, offering a unique technical contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Fundamental to the paper's context, connecting to a wide range of research in language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Query-and-Correct Paradigm",
        "canonical": "Query-and-Correct Paradigm",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a novel paradigm that differentiates the proposed method from existing approaches.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "draft model",
      "target model",
      "vanilla autoregressive decoding"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cache-Assisted Parallel Speculative Decoding",
      "resolved_canonical": "Cache-Assisted Parallel Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Query-and-Correct Paradigm",
      "resolved_canonical": "Query-and-Correct Paradigm",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference

**Korean Title:** CARD: ì¿¼ë¦¬ ë° ìˆ˜ì • íŒ¨ëŸ¬ë‹¤ì„ì„ í†µí•œ ìºì‹œ ì§€ì› ë³‘ë ¬ ì¶”ì¸¡ ë””ì½”ë”© í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ LLM ì¶”ë¡  ê°€ì†í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2508.04462.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2508.04462](https://arxiv.org/abs/2508.04462)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (83.7% similar)
- [[2025-09-22/Cross-Attention Speculative Decoding_20250922|Cross-Attention Speculative Decoding]] (83.2% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (82.8% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (82.7% similar)
- [[2025-09-18/Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning_20250918|Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning]] (82.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Cache-Assisted Parallel Speculative Decoding|Cache-Assisted Parallel Speculative Decoding]], [[keywords/Query-and-Correct Paradigm|Query-and-Correct Paradigm]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.04462v2 Announce Type: replace 
Abstract: Speculative decoding (SD), where a draft model provides multiple candidate tokens for the target model to verify in parallel, has demonstrated significant potential for accelerating LLM inference. Yet, existing SD approaches adhere to a strict draft-then-verify paradigm, enforcing a sequential process that hampers performance and constrains the draft model's capacity. Moreover, rejecting a token in the candidate sequence invalidates all subsequent tokens, leading to wasted computation during drafting. To overcome these limitations, we propose a cache-assisted parallel speculative decoding framework called CARD, which employs a novel query-and-correct paradigm. Our approach decouples drafting from verification: the draft model populates a shared cache with candidate tokens, while the target model concurrently refines the draft's trajectory. This enables inference at near-draft-speed, effectively leveraging the draft model's efficiency without additional fine-tuning. Experimental results show that CARD significantly outperforms existing state-of-the-art methods, achieving up to a 4.83x acceleration over vanilla autoregressive decoding, with no fine-tuning required for either models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.04462v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì¶”ë¡  ëª¨ë¸ì´ ì—¬ëŸ¬ í›„ë³´ í† í°ì„ ì œê³µí•˜ê³  ëŒ€ìƒ ëª¨ë¸ì´ ì´ë¥¼ ë³‘ë ¬ë¡œ ê²€ì¦í•˜ëŠ” ì¶”ì¸¡ ë””ì½”ë”©(Speculative Decoding, SD)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ë° ìƒë‹¹í•œ ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ SD ì ‘ê·¼ ë°©ì‹ì€ ì—„ê²©í•œ ì´ˆì•ˆ ì‘ì„± í›„ ê²€ì¦ íŒ¨ëŸ¬ë‹¤ì„ì„ ë”°ë¥´ë©°, ì´ëŠ” ì„±ëŠ¥ì„ ì €í•´í•˜ê³  ì´ˆì•ˆ ëª¨ë¸ì˜ ì—­ëŸ‰ì„ ì œí•œí•©ë‹ˆë‹¤. ê²Œë‹¤ê°€, í›„ë³´ ì‹œí€€ìŠ¤ì—ì„œ í† í°ì„ ê±°ë¶€í•˜ë©´ ì´í›„ì˜ ëª¨ë“  í† í°ì´ ë¬´íš¨í™”ë˜ì–´ ì´ˆì•ˆ ì‘ì„± ì¤‘ì— ê³„ì‚°ì´ ë‚­ë¹„ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œì„ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ì¿¼ë¦¬ ë° ìˆ˜ì • íŒ¨ëŸ¬ë‹¤ì„ì„ ì‚¬ìš©í•˜ëŠ” ìºì‹œ ì§€ì› ë³‘ë ¬ ì¶”ì¸¡ ë””ì½”ë”© í”„ë ˆì„ì›Œí¬ì¸ CARDë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì´ˆì•ˆ ì‘ì„±ê³¼ ê²€ì¦ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤: ì´ˆì•ˆ ëª¨ë¸ì€ í›„ë³´ í† í°ìœ¼ë¡œ ê³µìœ  ìºì‹œë¥¼ ì±„ìš°ê³ , ëŒ€ìƒ ëª¨ë¸ì€ ë™ì‹œì— ì´ˆì•ˆì˜ ê²½ë¡œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ëŠ” ì´ˆì•ˆ ì†ë„ì— ê°€ê¹Œìš´ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬, ì¶”ê°€ì ì¸ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ì´ˆì•ˆ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CARDëŠ” ê¸°ì¡´ ìµœì²¨ë‹¨ ë°©ë²•ì„ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, ê¸°ë³¸ ìë™íšŒê·€ ë””ì½”ë”©ì— ë¹„í•´ ìµœëŒ€ 4.83ë°°ì˜ ê°€ì†ì„ ë‹¬ì„±í•˜ë©°, ë‘ ëª¨ë¸ ëª¨ë‘ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ ì¹´ë“œ(CARD)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì¶”ë¡  ë°©ì‹ì€ ì´ˆì•ˆ ìƒì„± í›„ ê²€ì¦í•˜ëŠ” ìˆœì°¨ì  ê³¼ì •ìœ¼ë¡œ, ì„±ëŠ¥ ì €í•˜ì™€ ìì› ë‚­ë¹„ê°€ ë°œìƒí•©ë‹ˆë‹¤. ì¹´ë“œ(CARD)ëŠ” ì´ˆì•ˆ ìƒì„±ê³¼ ê²€ì¦ì„ ë¶„ë¦¬í•˜ì—¬, ì´ˆì•ˆ ëª¨ë¸ì´ í›„ë³´ í† í°ì„ ìºì‹œì— ì €ì¥í•˜ê³  ëª©í‘œ ëª¨ë¸ì´ ì´ë¥¼ ë™ì‹œ ê²€ì¦í•˜ëŠ” êµ¬ì¡°ë¥¼ ì±„íƒí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì´ˆì•ˆ ì†ë„ì— ê°€ê¹Œìš´ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë©°, ì¶”ê°€ì ì¸ ëª¨ë¸ íŠœë‹ ì—†ì´ë„ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì¹´ë“œ(CARD)ëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 4.83ë°° ë¹ ë¥¸ ì†ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ì¶”ë¡  ëª¨ë¸ì€ ì—„ê²©í•œ ì´ˆì•ˆ-ê²€ì¦ ì ˆì°¨ë¥¼ ë”°ë¥´ë©° ì„±ëŠ¥ì„ ì €í•´í•˜ê³  ì´ˆì•ˆ ëª¨ë¸ì˜ ì—­ëŸ‰ì„ ì œí•œí•œë‹¤.
- 2. ì œì•ˆëœ CARD í”„ë ˆì„ì›Œí¬ëŠ” ì´ˆì•ˆê³¼ ê²€ì¦ì„ ë¶„ë¦¬í•˜ì—¬ ë³‘ë ¬ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 3. CARDëŠ” ê³µìœ  ìºì‹œë¥¼ í™œìš©í•˜ì—¬ ì´ˆì•ˆ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ê²€ì¦ ëª¨ë¸ì´ ì´ˆì•ˆì˜ ê²½ë¡œë¥¼ ë™ì‹œì— ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, CARDëŠ” ê¸°ì¡´ ìµœì²¨ë‹¨ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 4.83ë°° ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ì¶”ê°€ì ì¸ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš” ì—†ë‹¤.


---

*Generated on 2025-09-23 11:10:52*