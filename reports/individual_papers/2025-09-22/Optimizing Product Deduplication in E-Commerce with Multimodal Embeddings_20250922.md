---
keywords:
  - Multimodal Learning
  - Transformer
  - Self-supervised Learning
  - Vector Database
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15858
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:53:45.423207",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Transformer",
    "Self-supervised Learning",
    "Vector Database"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.82,
    "Transformer": 0.85,
    "Self-supervised Learning": 0.8,
    "Vector Database": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal product deduplication",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal deduplication"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the concept of integrating multiple data types, enhancing understanding of cross-modal applications.",
        "novelty_score": 0.58,
        "connectivity_score": 0.87,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "BERT architecture",
        "canonical": "Transformer",
        "aliases": [
          "BERT",
          "Bidirectional Encoder Representations from Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the widely-used Transformer model, facilitating discussions on NLP advancements.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "MaskedAutoEncoders",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "MAE"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the use of self-supervised techniques in image representation, relevant for linking to similar methodologies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Milvus",
        "canonical": "Vector Database",
        "aliases": [
          "Milvus database"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a specific tool for vector similarity search, valuable for discussions on database technologies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "e-commerce",
      "product listings",
      "system RAM"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal product deduplication",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.87,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "BERT architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "MaskedAutoEncoders",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Milvus",
      "resolved_canonical": "Vector Database",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings

**Korean Title:** ì „ììƒê±°ë˜ì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ì„ë² ë”©ì„ í™œìš©í•œ ì œí’ˆ ì¤‘ë³µ ì œê±° ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15858.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15858](https://arxiv.org/abs/2509.15858)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/RoboEye_ Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching_20250918|RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching]] (82.1% similar)
- [[2025-09-19/PRISM_ Product Retrieval In Shopping Carts using Hybrid Matching_20250919|PRISM: Product Retrieval In Shopping Carts using Hybrid Matching]] (82.0% similar)
- [[2025-09-22/Efficient Multimodal Dataset Distillation via Generative Models_20250922|Efficient Multimodal Dataset Distillation via Generative Models]] (79.8% similar)
- [[2025-09-22/Efficient Extractive Text Summarization for Online News Articles Using Machine Learning_20250922|Efficient Extractive Text Summarization for Online News Articles Using Machine Learning]] (79.3% similar)
- [[2025-09-19/What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques_20250919|What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques]] (79.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Vector Database|Vector Database]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15858v1 Announce Type: cross 
Abstract: In large scale e-commerce marketplaces, duplicate product listings frequently cause consumer confusion and operational inefficiencies, degrading trust on the platform and increasing costs. Traditional keyword-based search methodologies falter in accurately identifying duplicates due to their reliance on exact textual matches, neglecting semantic similarities inherent in product titles. To address these challenges, we introduce a scalable, multimodal product deduplication designed specifically for the e-commerce domain. Our approach employs a domain-specific text model grounded in BERT architecture in conjunction with MaskedAutoEncoders for image representations. Both of these architectures are augmented with dimensionality reduction techniques to produce compact 128-dimensional embeddings without significant information loss. Complementing this, we also developed a novel decider model that leverages both text and image vectors. By integrating these feature extraction mechanisms with Milvus, an optimized vector database, our system can facilitate efficient and high-precision similarity searches across extensive product catalogs exceeding 200 million items with just 100GB of system RAM consumption. Empirical evaluations demonstrate that our matching system achieves a macro-average F1 score of 0.90, outperforming third-party solutions which attain an F1 score of 0.83. Our findings show the potential of combining domain-specific adaptations with state-of-the-art machine learning techniques to mitigate duplicate listings in large-scale e-commerce environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15858v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ìš”ì•½: ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ ë§ˆì¼“í”Œë ˆì´ìŠ¤ì—ì„œëŠ” ì¤‘ë³µëœ ì œí’ˆ ëª©ë¡ì´ ì†Œë¹„ì í˜¼ë€ê³¼ ìš´ì˜ ë¹„íš¨ìœ¨ì„±ì„ ìì£¼ ì´ˆë˜í•˜ì—¬ í”Œë«í¼ì— ëŒ€í•œ ì‹ ë¢°ë¥¼ ì €í•˜ì‹œí‚¤ê³  ë¹„ìš©ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ì „í†µì ì¸ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ë°©ë²•ë¡ ì€ ì •í™•í•œ í…ìŠ¤íŠ¸ ì¼ì¹˜ì— ì˜ì¡´í•˜ì—¬ ì œí’ˆ ì œëª©ì— ë‚´ì¬ëœ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê°„ê³¼í•¨ìœ¼ë¡œì¨ ì¤‘ë³µì„ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ëŠ” ë° ì‹¤íŒ¨í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì „ììƒê±°ë˜ ë„ë©”ì¸ì— íŠ¹í™”ëœ í™•ì¥ ê°€ëŠ¥í•œ ë‹¤ì¤‘ ëª¨ë“œ ì œí’ˆ ì¤‘ë³µ ì œê±° ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ BERT ì•„í‚¤í…ì²˜ì— ê¸°ë°˜í•œ ë„ë©”ì¸ íŠ¹í™” í…ìŠ¤íŠ¸ ëª¨ë¸ì„ MaskedAutoEncodersì™€ ê²°í•©í•˜ì—¬ ì´ë¯¸ì§€ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë‘ ì•„í‚¤í…ì²˜ëŠ” ëª¨ë‘ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ìœ¼ë¡œ ë³´ê°•ë˜ì–´ ì •ë³´ ì†ì‹¤ ì—†ì´ 128ì°¨ì›ì˜ ì••ì¶•ëœ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë²¡í„°ë¥¼ ëª¨ë‘ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ ê²°ì • ëª¨ë¸ë„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì§• ì¶”ì¶œ ë©”ì»¤ë‹ˆì¦˜ì„ ìµœì í™”ëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì¸ Milvusì™€ í†µí•©í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ì˜ ì‹œìŠ¤í…œì€ 2ì–µ ê°œ ì´ìƒì˜ ì œí’ˆ ì¹´íƒˆë¡œê·¸ì— ëŒ€í•´ 100GBì˜ ì‹œìŠ¤í…œ RAMë§Œìœ¼ë¡œ íš¨ìœ¨ì ì´ê³  ë†’ì€ ì •ë°€ë„ì˜ ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤ì¦ì  í‰ê°€ ê²°ê³¼, ìš°ë¦¬ì˜ ë§¤ì¹­ ì‹œìŠ¤í…œì€ F1 ì ìˆ˜ 0.83ì„ ë‹¬ì„±í•œ ì œ3ì ì†”ë£¨ì…˜ì„ ëŠ¥ê°€í•˜ì—¬ ë§¤í¬ë¡œ í‰ê·  F1 ì ìˆ˜ 0.90ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ í™˜ê²½ì—ì„œ ì¤‘ë³µ ëª©ë¡ì„ ì¤„ì´ê¸° ìœ„í•´ ë„ë©”ì¸ íŠ¹í™” ì ì‘ê³¼ ìµœì²¨ë‹¨ ê¸°ê³„ í•™ìŠµ ê¸°ë²•ì„ ê²°í•©í•  ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ í”Œë«í¼ì—ì„œ ì¤‘ë³µ ìƒí’ˆ ëª©ë¡ì€ ì†Œë¹„ì í˜¼ë€ê³¼ ìš´ì˜ ë¹„íš¨ìœ¨ì„±ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì „ììƒê±°ë˜ì— íŠ¹í™”ëœ í™•ì¥ ê°€ëŠ¥í•œ ë©€í‹°ëª¨ë‹¬ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ BERT ê¸°ë°˜ì˜ í…ìŠ¤íŠ¸ ëª¨ë¸ê³¼ MaskedAutoEncodersë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ í‘œí˜„ì„ ê²°í•©í•˜ì—¬ 128ì°¨ì›ì˜ ì••ì¶•ëœ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. ë˜í•œ, í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë²¡í„°ë¥¼ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ ê²°ì • ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ í†µí•©í•˜ì—¬ 2ì–µ ê°œ ì´ìƒì˜ ìƒí’ˆ ì¹´íƒˆë¡œê·¸ì—ì„œ íš¨ìœ¨ì ì´ê³  ì •ë°€í•œ ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì‹œìŠ¤í…œ RAMì€ 100GBë§Œ ì†Œëª¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ìš°ë¦¬ ì‹œìŠ¤í…œì€ F1 ì ìˆ˜ 0.90ì„ ê¸°ë¡í•˜ì—¬ ê¸°ì¡´ ì†”ë£¨ì…˜ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ë„ë©”ì¸ íŠ¹í™” ì ì‘ê³¼ ìµœì‹  ê¸°ê³„ í•™ìŠµ ê¸°ìˆ ì˜ ê²°í•©ì´ ì¤‘ë³µ ëª©ë¡ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ ì‹œì¥ì—ì„œ ì¤‘ë³µ ìƒí’ˆ ë“±ë¡ì€ ì†Œë¹„ì í˜¼ë€ê³¼ ìš´ì˜ ë¹„íš¨ìœ¨ì„±ì„ ì´ˆë˜í•˜ë©°, í”Œë«í¼ ì‹ ë¢°ë„ ì €í•˜ì™€ ë¹„ìš© ì¦ê°€ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ë°©ë²•ì€ ì •í™•í•œ í…ìŠ¤íŠ¸ ì¼ì¹˜ì— ì˜ì¡´í•˜ì—¬ ì œí’ˆ ì œëª©ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê°„ê³¼í•¨ìœ¼ë¡œì¨ ì¤‘ë³µ ì‹ë³„ì— ì‹¤íŒ¨í•©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” BERT ì•„í‚¤í…ì²˜ì— ê¸°ë°˜í•œ ë„ë©”ì¸ íŠ¹í™” í…ìŠ¤íŠ¸ ëª¨ë¸ê³¼ MaskedAutoEncodersë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ í‘œí˜„ì„ ê²°í•©í•œ í™•ì¥ ê°€ëŠ¥í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì¤‘ë³µ ì œê±° ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ì‹œìŠ¤í…œì€ Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ í†µí•©ë˜ì–´ 2ì–µ ê°œ ì´ìƒì˜ ì œí’ˆ ì¹´íƒˆë¡œê·¸ì—ì„œ íš¨ìœ¨ì ì´ê³  ê³ ì •ë°€ ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ì‹œìŠ¤í…œì€ ë§¤í¬ë¡œ í‰ê·  F1 ì ìˆ˜ 0.90ì„ ë‹¬ì„±í•˜ì—¬, F1 ì ìˆ˜ 0.83ì„ ê¸°ë¡í•œ íƒ€ì‚¬ ì†”ë£¨ì…˜ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:53:45*