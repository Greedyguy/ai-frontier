---
keywords:
  - Disentangled Semantic Representation
  - Semantic Subdimensions
  - Word Embeddings
  - Large Language Model
  - Voxel-wise Encoding Models
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2508.21436
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:50:45.454293",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Disentangled Semantic Representation",
    "Semantic Subdimensions",
    "Word Embeddings",
    "Large Language Model",
    "Voxel-wise Encoding Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Disentangled Semantic Representation": 0.78,
    "Semantic Subdimensions": 0.77,
    "Word Embeddings": 0.7,
    "Large Language Model": 0.72,
    "Voxel-wise Encoding Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Disentangled Continuous Semantic Representation Model",
        "canonical": "Disentangled Semantic Representation",
        "aliases": [
          "DCSRM"
        ],
        "category": "unique_technical",
        "rationale": "This model introduces a novel approach to decomposing semantic dimensions, which is central to the paper's contributions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "semantic subdimensions",
        "canonical": "Semantic Subdimensions",
        "aliases": [
          "subdimensions of semantics"
        ],
        "category": "unique_technical",
        "rationale": "The concept of semantic subdimensions is a key focus of the paper, offering a new perspective on semantic analysis.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "word embeddings",
        "canonical": "Word Embeddings",
        "aliases": [
          "word vectors"
        ],
        "category": "broad_technical",
        "rationale": "Word embeddings are a fundamental component in the paper's methodology, linking to broader NLP concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are integral to the paper's framework, connecting to a wide range of NLP research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.72
      },
      {
        "surface": "voxel-wise encoding models",
        "canonical": "Voxel-wise Encoding Models",
        "aliases": [
          "voxel encoding"
        ],
        "category": "specific_connectable",
        "rationale": "These models are used to map semantic subdimensions to brain activation, linking neuroscience with computational models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "semantic dimensions",
      "neural plausibility",
      "brain activation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Disentangled Continuous Semantic Representation Model",
      "resolved_canonical": "Disentangled Semantic Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "semantic subdimensions",
      "resolved_canonical": "Semantic Subdimensions",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "word embeddings",
      "resolved_canonical": "Word Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "voxel-wise encoding models",
      "resolved_canonical": "Voxel-wise Encoding Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Discovering Semantic Subdimensions through Disentangled Conceptual Representations

**Korean Title:** 개념적 표현의 분리를 통한 의미적 하위 차원의 발견

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2508.21436.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2508.21436](https://arxiv.org/abs/2508.21436)

## 🔗 유사한 논문
- [[2025-09-22/Capturing Polysemanticity with PRISM_ A Multi-Concept Feature Description Framework_20250922|Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework]] (81.5% similar)
- [[2025-09-22/Enhancing Interpretability in Deep Reinforcement Learning through Semantic Clustering_20250922|Enhancing Interpretability in Deep Reinforcement Learning through Semantic Clustering]] (81.4% similar)
- [[2025-09-22/Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations_20250922|Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations]] (79.2% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (79.0% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (78.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Word Embeddings|Word Embeddings]], [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Voxel-wise Encoding Models|Voxel-wise Encoding Models]]
**⚡ Unique Technical**: [[keywords/Disentangled Semantic Representation|Disentangled Semantic Representation]], [[keywords/Semantic Subdimensions|Semantic Subdimensions]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2508.21436v2 Announce Type: replace 
Abstract: Understanding the core dimensions of conceptual semantics is fundamental to uncovering how meaning is organized in language and the brain. Existing approaches often rely on predefined semantic dimensions that offer only broad representations, overlooking finer conceptual distinctions. This paper proposes a novel framework to investigate the subdimensions underlying coarse-grained semantic dimensions. Specifically, we introduce a Disentangled Continuous Semantic Representation Model (DCSRM) that decomposes word embeddings from large language models into multiple sub-embeddings, each encoding specific semantic information. Using these sub-embeddings, we identify a set of interpretable semantic subdimensions. To assess their neural plausibility, we apply voxel-wise encoding models to map these subdimensions to brain activation. Our work offers more fine-grained interpretable semantic subdimensions of conceptual meaning. Further analyses reveal that semantic dimensions are structured according to distinct principles, with polarity emerging as a key factor driving their decomposition into subdimensions. The neural correlates of the identified subdimensions support their cognitive and neuroscientific plausibility.

## 🔍 Abstract (한글 번역)

arXiv:2508.21436v2 발표 유형: 교체  
초록: 개념적 의미론의 핵심 차원을 이해하는 것은 언어와 뇌에서 의미가 어떻게 조직되는지를 밝히는 데 필수적입니다. 기존 접근법은 종종 미리 정의된 의미 차원에 의존하여 세부적인 개념적 구분을 간과한 채 넓은 표현만을 제공합니다. 본 논문은 거친 의미 차원 아래에 있는 하위 차원을 조사하기 위한 새로운 프레임워크를 제안합니다. 구체적으로, 우리는 대형 언어 모델의 단어 임베딩을 여러 하위 임베딩으로 분해하여 각기 특정한 의미 정보를 인코딩하는 비유사화 연속 의미 표현 모델(DCSRM)을 소개합니다. 이러한 하위 임베딩을 사용하여 해석 가능한 의미 하위 차원의 집합을 식별합니다. 이들의 신경적 타당성을 평가하기 위해, 우리는 이러한 하위 차원을 뇌 활성화에 매핑하기 위해 복셀 단위 인코딩 모델을 적용합니다. 우리의 연구는 개념적 의미의 더 세분화된 해석 가능한 의미 하위 차원을 제공합니다. 추가 분석을 통해 의미 차원이 뚜렷한 원칙에 따라 구조화되어 있으며, 극성이 하위 차원으로의 분해를 주도하는 주요 요인으로 부각됨을 밝혀냅니다. 식별된 하위 차원의 신경적 상관관계는 그들의 인지적 및 신경과학적 타당성을 뒷받침합니다.

## 📝 요약

이 논문은 개념 의미의 세부 차원을 탐구하기 위한 새로운 프레임워크를 제안합니다. 기존 방법론이 넓은 의미의 차원에 의존하는 반면, 이 연구는 대형 언어 모델의 단어 임베딩을 세분화하여 각각의 임베딩이 특정 의미 정보를 담도록 하는 분리 연속 의미 표현 모델(DCSRM)을 소개합니다. 이를 통해 해석 가능한 의미의 세부 차원을 식별하고, 뇌 활성화와의 연관성을 평가하기 위해 보셀 단위 인코딩 모델을 적용합니다. 연구 결과, 의미 차원이 특정 원칙에 따라 구조화되며, 극성이 세부 차원으로 분해되는 주요 요인임을 발견했습니다. 식별된 세부 차원의 신경적 상관관계는 인지 및 신경과학적 타당성을 뒷받침합니다.

## 🎯 주요 포인트

- 1. 본 논문은 개념적 의미의 핵심 차원을 이해하기 위한 새로운 프레임워크를 제안합니다.
- 2. Disentangled Continuous Semantic Representation Model (DCSRM)을 통해 대형 언어 모델의 단어 임베딩을 여러 하위 임베딩으로 분해합니다.
- 3. 하위 임베딩을 사용하여 해석 가능한 의미의 하위 차원을 식별하고, 이를 뇌 활성화와 연결하여 신경학적 타당성을 평가합니다.
- 4. 의미 차원은 서로 다른 원칙에 따라 구조화되며, 극성이 하위 차원으로의 분해를 주도하는 주요 요인으로 나타납니다.
- 5. 식별된 하위 차원의 신경 상관관계는 이들의 인지적 및 신경과학적 타당성을 뒷받침합니다.


---

*Generated on 2025-09-23 11:50:45*