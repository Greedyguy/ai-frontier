---
keywords:
  - Disentangled Semantic Representation
  - Semantic Subdimensions
  - Word Embeddings
  - Large Language Model
  - Voxel-wise Encoding Models
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2508.21436
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:50:45.454293",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Disentangled Semantic Representation",
    "Semantic Subdimensions",
    "Word Embeddings",
    "Large Language Model",
    "Voxel-wise Encoding Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Disentangled Semantic Representation": 0.78,
    "Semantic Subdimensions": 0.77,
    "Word Embeddings": 0.7,
    "Large Language Model": 0.72,
    "Voxel-wise Encoding Models": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Disentangled Continuous Semantic Representation Model",
        "canonical": "Disentangled Semantic Representation",
        "aliases": [
          "DCSRM"
        ],
        "category": "unique_technical",
        "rationale": "This model introduces a novel approach to decomposing semantic dimensions, which is central to the paper's contributions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "semantic subdimensions",
        "canonical": "Semantic Subdimensions",
        "aliases": [
          "subdimensions of semantics"
        ],
        "category": "unique_technical",
        "rationale": "The concept of semantic subdimensions is a key focus of the paper, offering a new perspective on semantic analysis.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "word embeddings",
        "canonical": "Word Embeddings",
        "aliases": [
          "word vectors"
        ],
        "category": "broad_technical",
        "rationale": "Word embeddings are a fundamental component in the paper's methodology, linking to broader NLP concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are integral to the paper's framework, connecting to a wide range of NLP research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.72
      },
      {
        "surface": "voxel-wise encoding models",
        "canonical": "Voxel-wise Encoding Models",
        "aliases": [
          "voxel encoding"
        ],
        "category": "specific_connectable",
        "rationale": "These models are used to map semantic subdimensions to brain activation, linking neuroscience with computational models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "semantic dimensions",
      "neural plausibility",
      "brain activation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Disentangled Continuous Semantic Representation Model",
      "resolved_canonical": "Disentangled Semantic Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "semantic subdimensions",
      "resolved_canonical": "Semantic Subdimensions",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "word embeddings",
      "resolved_canonical": "Word Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "voxel-wise encoding models",
      "resolved_canonical": "Voxel-wise Encoding Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Discovering Semantic Subdimensions through Disentangled Conceptual Representations

**Korean Title:** ê°œë…ì  í‘œí˜„ì˜ ë¶„ë¦¬ë¥¼ í†µí•œ ì˜ë¯¸ì  í•˜ìœ„ ì°¨ì›ì˜ ë°œê²¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2508.21436.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2508.21436](https://arxiv.org/abs/2508.21436)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Capturing Polysemanticity with PRISM_ A Multi-Concept Feature Description Framework_20250922|Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework]] (81.5% similar)
- [[2025-09-22/Enhancing Interpretability in Deep Reinforcement Learning through Semantic Clustering_20250922|Enhancing Interpretability in Deep Reinforcement Learning through Semantic Clustering]] (81.4% similar)
- [[2025-09-22/Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations_20250922|Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations]] (79.2% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (79.0% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (78.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Word Embeddings|Word Embeddings]], [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Voxel-wise Encoding Models|Voxel-wise Encoding Models]]
**âš¡ Unique Technical**: [[keywords/Disentangled Semantic Representation|Disentangled Semantic Representation]], [[keywords/Semantic Subdimensions|Semantic Subdimensions]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.21436v2 Announce Type: replace 
Abstract: Understanding the core dimensions of conceptual semantics is fundamental to uncovering how meaning is organized in language and the brain. Existing approaches often rely on predefined semantic dimensions that offer only broad representations, overlooking finer conceptual distinctions. This paper proposes a novel framework to investigate the subdimensions underlying coarse-grained semantic dimensions. Specifically, we introduce a Disentangled Continuous Semantic Representation Model (DCSRM) that decomposes word embeddings from large language models into multiple sub-embeddings, each encoding specific semantic information. Using these sub-embeddings, we identify a set of interpretable semantic subdimensions. To assess their neural plausibility, we apply voxel-wise encoding models to map these subdimensions to brain activation. Our work offers more fine-grained interpretable semantic subdimensions of conceptual meaning. Further analyses reveal that semantic dimensions are structured according to distinct principles, with polarity emerging as a key factor driving their decomposition into subdimensions. The neural correlates of the identified subdimensions support their cognitive and neuroscientific plausibility.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.21436v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ê°œë…ì  ì˜ë¯¸ë¡ ì˜ í•µì‹¬ ì°¨ì›ì„ ì´í•´í•˜ëŠ” ê²ƒì€ ì–¸ì–´ì™€ ë‡Œì—ì„œ ì˜ë¯¸ê°€ ì–´ë–»ê²Œ ì¡°ì§ë˜ëŠ”ì§€ë¥¼ ë°íˆëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ë²•ì€ ì¢…ì¢… ë¯¸ë¦¬ ì •ì˜ëœ ì˜ë¯¸ ì°¨ì›ì— ì˜ì¡´í•˜ì—¬ ì„¸ë¶€ì ì¸ ê°œë…ì  êµ¬ë¶„ì„ ê°„ê³¼í•œ ì±„ ë„“ì€ í‘œí˜„ë§Œì„ ì œê³µí•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ê±°ì¹œ ì˜ë¯¸ ì°¨ì› ì•„ë˜ì— ìˆëŠ” í•˜ìœ„ ì°¨ì›ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë‹¨ì–´ ì„ë² ë”©ì„ ì—¬ëŸ¬ í•˜ìœ„ ì„ë² ë”©ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê°ê¸° íŠ¹ì •í•œ ì˜ë¯¸ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ë¹„ìœ ì‚¬í™” ì—°ì† ì˜ë¯¸ í‘œí˜„ ëª¨ë¸(DCSRM)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•˜ìœ„ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ í•´ì„ ê°€ëŠ¥í•œ ì˜ë¯¸ í•˜ìœ„ ì°¨ì›ì˜ ì§‘í•©ì„ ì‹ë³„í•©ë‹ˆë‹¤. ì´ë“¤ì˜ ì‹ ê²½ì  íƒ€ë‹¹ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í•˜ìœ„ ì°¨ì›ì„ ë‡Œ í™œì„±í™”ì— ë§¤í•‘í•˜ê¸° ìœ„í•´ ë³µì…€ ë‹¨ìœ„ ì¸ì½”ë”© ëª¨ë¸ì„ ì ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ê°œë…ì  ì˜ë¯¸ì˜ ë” ì„¸ë¶„í™”ëœ í•´ì„ ê°€ëŠ¥í•œ ì˜ë¯¸ í•˜ìœ„ ì°¨ì›ì„ ì œê³µí•©ë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì„ í†µí•´ ì˜ë¯¸ ì°¨ì›ì´ ëšœë ·í•œ ì›ì¹™ì— ë”°ë¼ êµ¬ì¡°í™”ë˜ì–´ ìˆìœ¼ë©°, ê·¹ì„±ì´ í•˜ìœ„ ì°¨ì›ìœ¼ë¡œì˜ ë¶„í•´ë¥¼ ì£¼ë„í•˜ëŠ” ì£¼ìš” ìš”ì¸ìœ¼ë¡œ ë¶€ê°ë¨ì„ ë°í˜€ëƒ…ë‹ˆë‹¤. ì‹ë³„ëœ í•˜ìœ„ ì°¨ì›ì˜ ì‹ ê²½ì  ìƒê´€ê´€ê³„ëŠ” ê·¸ë“¤ì˜ ì¸ì§€ì  ë° ì‹ ê²½ê³¼í•™ì  íƒ€ë‹¹ì„±ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°œë… ì˜ë¯¸ì˜ ì„¸ë¶€ ì°¨ì›ì„ íƒêµ¬í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë¡ ì´ ë„“ì€ ì˜ë¯¸ì˜ ì°¨ì›ì— ì˜ì¡´í•˜ëŠ” ë°˜ë©´, ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë‹¨ì–´ ì„ë² ë”©ì„ ì„¸ë¶„í™”í•˜ì—¬ ê°ê°ì˜ ì„ë² ë”©ì´ íŠ¹ì • ì˜ë¯¸ ì •ë³´ë¥¼ ë‹´ë„ë¡ í•˜ëŠ” ë¶„ë¦¬ ì—°ì† ì˜ë¯¸ í‘œí˜„ ëª¨ë¸(DCSRM)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•´ì„ ê°€ëŠ¥í•œ ì˜ë¯¸ì˜ ì„¸ë¶€ ì°¨ì›ì„ ì‹ë³„í•˜ê³ , ë‡Œ í™œì„±í™”ì™€ì˜ ì—°ê´€ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë³´ì…€ ë‹¨ìœ„ ì¸ì½”ë”© ëª¨ë¸ì„ ì ìš©í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì˜ë¯¸ ì°¨ì›ì´ íŠ¹ì • ì›ì¹™ì— ë”°ë¼ êµ¬ì¡°í™”ë˜ë©°, ê·¹ì„±ì´ ì„¸ë¶€ ì°¨ì›ìœ¼ë¡œ ë¶„í•´ë˜ëŠ” ì£¼ìš” ìš”ì¸ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì‹ë³„ëœ ì„¸ë¶€ ì°¨ì›ì˜ ì‹ ê²½ì  ìƒê´€ê´€ê³„ëŠ” ì¸ì§€ ë° ì‹ ê²½ê³¼í•™ì  íƒ€ë‹¹ì„±ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ê°œë…ì  ì˜ë¯¸ì˜ í•µì‹¬ ì°¨ì›ì„ ì´í•´í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. Disentangled Continuous Semantic Representation Model (DCSRM)ì„ í†µí•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë‹¨ì–´ ì„ë² ë”©ì„ ì—¬ëŸ¬ í•˜ìœ„ ì„ë² ë”©ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
- 3. í•˜ìœ„ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ í•´ì„ ê°€ëŠ¥í•œ ì˜ë¯¸ì˜ í•˜ìœ„ ì°¨ì›ì„ ì‹ë³„í•˜ê³ , ì´ë¥¼ ë‡Œ í™œì„±í™”ì™€ ì—°ê²°í•˜ì—¬ ì‹ ê²½í•™ì  íƒ€ë‹¹ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 4. ì˜ë¯¸ ì°¨ì›ì€ ì„œë¡œ ë‹¤ë¥¸ ì›ì¹™ì— ë”°ë¼ êµ¬ì¡°í™”ë˜ë©°, ê·¹ì„±ì´ í•˜ìœ„ ì°¨ì›ìœ¼ë¡œì˜ ë¶„í•´ë¥¼ ì£¼ë„í•˜ëŠ” ì£¼ìš” ìš”ì¸ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
- 5. ì‹ë³„ëœ í•˜ìœ„ ì°¨ì›ì˜ ì‹ ê²½ ìƒê´€ê´€ê³„ëŠ” ì´ë“¤ì˜ ì¸ì§€ì  ë° ì‹ ê²½ê³¼í•™ì  íƒ€ë‹¹ì„±ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:50:45*