---
keywords:
  - Reinforcement Learning from Verifiable Rewards
  - Large Language Model
  - Composite Reward Function
  - Reward Hacking
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15557
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:03:47.157788",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning from Verifiable Rewards",
    "Large Language Model",
    "Composite Reward Function",
    "Reward Hacking"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning from Verifiable Rewards": 0.78,
    "Large Language Model": 0.85,
    "Composite Reward Function": 0.77,
    "Reward Hacking": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning from Verifiable Rewards",
        "canonical": "Reinforcement Learning from Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique approach within reinforcement learning that specifically addresses reward hacking, making it a key concept for linking.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are central to the paper's discussion and connect broadly across AI research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "composite reward function",
        "canonical": "Composite Reward Function",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is crucial for understanding the proposed solution to reward hacking, offering a specific technical insight.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "reward hacking",
        "canonical": "Reward Hacking",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Reward hacking is a central issue addressed by the paper, making it a pivotal concept for linking related research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "reasoning phase",
      "significant reward",
      "final answer"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning from Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning from Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "composite reward function",
      "resolved_canonical": "Composite Reward Function",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "reward hacking",
      "resolved_canonical": "Reward Hacking",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Reward Hacking Mitigation using Verifiable Composite Rewards

**Korean Title:** ë³´ìƒ í•´í‚¹ ì™„í™”ë¥¼ ìœ„í•œ ê²€ì¦ ê°€ëŠ¥í•œ ë³µí•© ë³´ìƒ ì‚¬ìš©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15557.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15557](https://arxiv.org/abs/2509.15557)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (85.8% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (85.4% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.3% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (85.2% similar)
- [[2025-09-22/Best-of-L_ Cross-Lingual Reward Modeling for Mathematical Reasoning_20250922|Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning from Verifiable Rewards|Reinforcement Learning from Verifiable Rewards]], [[keywords/Composite Reward Function|Composite Reward Function]], [[keywords/Reward Hacking|Reward Hacking]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15557v1 Announce Type: cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that extending RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15557v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ(Reinforcement Learning from Verifiable Rewards, RLVR)ì—ì„œì˜ ê°•í™” í•™ìŠµì€ ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ì§ì ‘ì ì¸ ê°ë… ì—†ì´ë„ ìì²´ì ì¸ ì¶”ë¡ ì„ ê°œë°œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì˜ë£Œ ë¶„ì•¼, íŠ¹íˆ ì§ˆë¬¸ ì‘ë‹µì— ëŒ€í•œ ì‘ìš©ì—ì„œëŠ” ì¶”ë¡  ë‹¨ê³„ì—ì„œ ìƒë‹¹í•œ ë³´ìƒ í•´í‚¹ì— ì·¨ì•½í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ í–‰ë™ì˜ ë‘ ê°€ì§€ ì£¼ìš” í˜•íƒœë¥¼ ë‹¤ë£¹ë‹ˆë‹¤: i) ì¶”ë¡  ì—†ì´ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒ, ii) ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ì„ ì•…ìš©í•˜ê¸° ìœ„í•´ ë¹„í‘œì¤€ì ì¸ ì¶”ë¡  í˜•ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ. ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í–‰ë™ì— ëŒ€í•œ íŠ¹ì • í˜ë„í‹°ë¥¼ í¬í•¨í•œ ë³µí•© ë³´ìƒ í•¨ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ì œì•ˆëœ ë³´ìƒ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ RLVRì„ í™•ì¥í•˜ë©´ ë³´ìƒ í•´í‚¹ì´ ì ê³  í˜•ì‹ì´ ì˜ ê°–ì¶”ì–´ì§„ ì¶”ë¡ ì„ ì œê³µí•˜ë©°, ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬ ì¢‹ì€ ì •í™•ë„ë¥¼ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë³´ìƒ í•´í‚¹ì„ ì¤„ì´ê³  RLVRì„ í™œìš©í•˜ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í•œ ê±¸ìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ(Reinforcement Learning from Verifiable Rewards, RLVR)ì„ í™œìš©í•œ ê°•í™” í•™ìŠµì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê°ë… ì—†ì´ë„ ìì²´ì ì¸ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°œë°œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì˜ë£Œ ë¶„ì•¼ì˜ ì§ˆë¬¸ ì‘ë‹µ ì‘ìš©ì—ì„œ ë³´ìƒ í•´í‚¹ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œ, ì¦‰ ì¶”ë¡  ì—†ì´ ìµœì¢… ë‹µë³€ë§Œ ì œê³µí•˜ê±°ë‚˜ ë¹„í‘œì¤€ ì¶”ë¡  í˜•ì‹ì„ ì‚¬ìš©í•˜ëŠ” í–‰ìœ„ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ íŠ¹ì • í˜ë„í‹°ê°€ í¬í•¨ëœ ë³µí•© ë³´ìƒ í•¨ìˆ˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë³´ìƒ ëª¨ë¸ì„ ì ìš©í•œ RLVRëŠ” ë³´ìƒ í•´í‚¹ì´ ì¤„ì–´ë“¤ê³  ì •í™•ë„ê°€ í–¥ìƒëœ ì¶”ë¡  í˜•ì‹ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” RLVRì„ í™œìš©í•˜ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RLVR(Verifiable Rewards ê¸°ë°˜ ê°•í™” í•™ìŠµ)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ì§ì ‘ì ì¸ ê°ë… ì—†ì´ ìì²´ì ì¸ ì¶”ë¡ ì„ ê°œë°œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- 2. ì˜ë£Œ ë¶„ì•¼ì˜ ì§ˆë¬¸ ì‘ë‹µ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì¶”ë¡  ë‹¨ê³„ì—ì„œ ë³´ìƒ í•´í‚¹ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
- 3. ë³´ìƒ í•´í‚¹ì„ ì¤„ì´ê¸° ìœ„í•´ ìµœì¢… ë‹µë³€ì„ ì¶”ë¡  ì—†ì´ ì œê³µí•˜ê±°ë‚˜ ë¹„í‘œì¤€ ì¶”ë¡  í˜•ì‹ì„ ì‚¬ìš©í•˜ëŠ” í–‰ë™ì— ëŒ€í•œ í˜ë„í‹°ë¥¼ í¬í•¨í•œ ë³µí•© ë³´ìƒ í•¨ìˆ˜ë¥¼ ë„ì…í•˜ì˜€ë‹¤.
- 4. ì œì•ˆëœ ë³´ìƒ ëª¨ë¸ì„ í™•ì¥í•œ RLVRëŠ” ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ë” ì˜ í˜•ì‹í™”ëœ ì¶”ë¡ ê³¼ ë†’ì€ ì •í™•ì„±ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- 5. ì´ ì ‘ê·¼ë²•ì€ ë³´ìƒ í•´í‚¹ì„ ì¤„ì´ê³  RLVRì„ í™œìš©í•˜ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•œë‹¤.


---

*Generated on 2025-09-23 09:03:47*