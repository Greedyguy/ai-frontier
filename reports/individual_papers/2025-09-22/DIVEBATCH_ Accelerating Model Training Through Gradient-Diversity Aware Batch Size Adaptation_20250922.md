---
keywords:
  - DiveBatch
  - Gradient Diversity
  - Stochastic Gradient Descent
  - Neural Network
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.16173
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:44:40.925382",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "DiveBatch",
    "Gradient Diversity",
    "Stochastic Gradient Descent",
    "Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "DiveBatch": 0.8,
    "Gradient Diversity": 0.82,
    "Stochastic Gradient Descent": 0.75,
    "Neural Network": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "DiveBatch",
        "canonical": "DiveBatch",
        "aliases": [
          "Adaptive Batch Size SGD"
        ],
        "category": "unique_technical",
        "rationale": "DiveBatch represents a novel approach to SGD, which is central to the paper's contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Gradient Diversity",
        "canonical": "Gradient Diversity",
        "aliases": [
          "Gradient Variability"
        ],
        "category": "specific_connectable",
        "rationale": "Gradient Diversity is a key concept for understanding the adaptation mechanism in DiveBatch.",
        "novelty_score": 0.7,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Stochastic Gradient Descent",
        "canonical": "Stochastic Gradient Descent",
        "aliases": [
          "SGD"
        ],
        "category": "broad_technical",
        "rationale": "SGD is a foundational technique in machine learning, relevant to the paper's discussion on training acceleration.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "Deep Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "DNN"
        ],
        "category": "broad_technical",
        "rationale": "Deep Neural Networks are the primary focus of the training acceleration discussed in the paper.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "model training",
      "computational efficiency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "DiveBatch",
      "resolved_canonical": "DiveBatch",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Gradient Diversity",
      "resolved_canonical": "Gradient Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Stochastic Gradient Descent",
      "resolved_canonical": "Stochastic Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Deep Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation

**Korean Title:** DIVEBATCH: ê¸°ìš¸ê¸° ë‹¤ì–‘ì„± ì¸ì‹ ë°°ì¹˜ í¬ê¸° ì¡°ì •ì„ í†µí•œ ëª¨ë¸ í•™ìŠµ ê°€ì†í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16173.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.16173](https://arxiv.org/abs/2509.16173)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (83.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (83.1% similar)
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (83.0% similar)
- [[2025-09-22/Generalization and Optimization of SGD with Lookahead_20250922|Generalization and Optimization of SGD with Lookahead]] (82.1% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Stochastic Gradient Descent|Stochastic Gradient Descent]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Gradient Diversity|Gradient Diversity]]
**âš¡ Unique Technical**: [[keywords/DiveBatch|DiveBatch]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16173v1 Announce Type: new 
Abstract: The goal of this paper is to accelerate the training of machine learning models, a critical challenge since the training of large-scale deep neural models can be computationally expensive. Stochastic gradient descent (SGD) and its variants are widely used to train deep neural networks. In contrast to traditional approaches that focus on tuning the learning rate, we propose a novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts the batch size. Adapting the batch size is challenging: using large batch sizes is more efficient due to parallel computation, but small-batch training often converges in fewer epochs and generalizes better. To address this challenge, we introduce a data-driven adaptation based on gradient diversity, enabling DiveBatch to maintain the generalization performance of small-batch training while improving convergence speed and computational efficiency. Gradient diversity has a strong theoretical justification: it emerges from the convergence analysis of SGD. Evaluations of DiveBatch on synthetic and CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a slight trade-off in performance.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16173v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì´ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ëŒ€ê·œëª¨ ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì˜ í›ˆë ¨ì´ ê³„ì‚°ì ìœ¼ë¡œ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ í›ˆë ¨ì„ ê°€ì†í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD) ë° ê·¸ ë³€í˜•ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ í›ˆë ¨í•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì— ì¤‘ì ì„ ë‘” ì „í†µì ì¸ ì ‘ê·¼ ë°©ì‹ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ìƒˆë¡œìš´ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° SGD ì•Œê³ ë¦¬ì¦˜ì¸ DiveBatchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì€ ë„ì „ì ì…ë‹ˆë‹¤: í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë³‘ë ¬ ê³„ì‚°ìœ¼ë¡œ ì¸í•´ ë” íš¨ìœ¨ì ì´ì§€ë§Œ, ì‘ì€ ë°°ì¹˜ í›ˆë ¨ì€ ì¢…ì¢… ë” ì ì€ ì—í¬í¬ë¡œ ìˆ˜ë ´í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì— ê¸°ë°˜í•œ ë°ì´í„° ê¸°ë°˜ ì ì‘ì„ ë„ì…í•˜ì—¬ DiveBatchê°€ ì‘ì€ ë°°ì¹˜ í›ˆë ¨ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì€ ê°•ë ¥í•œ ì´ë¡ ì  ê·¼ê±°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤: ì´ëŠ” SGDì˜ ìˆ˜ë ´ ë¶„ì„ì—ì„œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. DiveBatchë¥¼ í•©ì„± ë°ì´í„°ì™€ CiFar-10, CiFar-100, Tiny-ImageNetì—ì„œ í‰ê°€í•œ ê²°ê³¼, DiveBatchëŠ” í‘œì¤€ SGD ë° AdaBatchë³´ë‹¤ ìƒë‹¹íˆ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©°(1.06 -- 5.0ë°°), ì„±ëŠ¥ì˜ ì•½ê°„ì˜ ì ˆì¶©ì´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ë¥¼ ê°€ì†í™”í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° SGD ì•Œê³ ë¦¬ì¦˜ì¸ DiveBatchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì „í†µì ìœ¼ë¡œ í•™ìŠµë¥  ì¡°ì •ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, DiveBatchëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. í° ë°°ì¹˜ í¬ê¸°ëŠ” ë³‘ë ¬ ê³„ì‚°ì— ìœ ë¦¬í•˜ì§€ë§Œ, ì‘ì€ ë°°ì¹˜ í¬ê¸°ëŠ” ë” ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. DiveBatchëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì— ê¸°ë°˜í•œ ë°ì´í„° ì¤‘ì‹¬ì˜ ì ì‘ ë°©ì‹ì„ í†µí•´ ì‘ì€ ë°°ì¹˜ í¬ê¸°ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DiveBatchëŠ” í‘œì¤€ SGD ë° AdaBatchë³´ë‹¤ 1.06ë°°ì—ì„œ 5.0ë°° ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©°, ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì˜ í•™ìŠµì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° SGD ì•Œê³ ë¦¬ì¦˜, DiveBatchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. DiveBatchëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì‘ì€ ë°°ì¹˜ í•™ìŠµì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ìˆ˜ë ´ ì†ë„ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 3. DiveBatchëŠ” SGDì˜ ìˆ˜ë ´ ë¶„ì„ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë‹¤ì–‘ì„±ì— ê¸°ë°˜í•œ ë°ì´í„° ì¤‘ì‹¬ì˜ ì ì‘ì„ ë„ì…í•©ë‹ˆë‹¤.
- 4. DiveBatchëŠ” CiFar-10, CiFar-100, Tiny-ImageNet ë“±ì˜ í‰ê°€ì—ì„œ í‘œì¤€ SGD ë° AdaBatchë³´ë‹¤ 1.06ë°°ì—ì„œ 5.0ë°°ê¹Œì§€ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤.
- 5. DiveBatchëŠ” ì„±ëŠ¥ì˜ ì•½ê°„ì˜ ì ˆì¶©ì„ ê°ìˆ˜í•˜ë©´ì„œë„ ìˆ˜ë ´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:44:40*