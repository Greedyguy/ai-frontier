---
keywords:
  - Neural Collapse
  - Biased Classification
  - Shortcut Learning
  - Imbalanced Attributes
  - Generalization Capability
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2405.05587
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:14:17.301162",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Collapse",
    "Biased Classification",
    "Shortcut Learning",
    "Imbalanced Attributes",
    "Generalization Capability"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Collapse": 0.8,
    "Biased Classification": 0.78,
    "Shortcut Learning": 0.77,
    "Imbalanced Attributes": 0.75,
    "Generalization Capability": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Collapse",
        "canonical": "Neural Collapse",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Neural Collapse is a novel concept explored in the paper, providing a unique perspective on feature space behavior in neural networks.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "biased classification",
        "canonical": "Biased Classification",
        "aliases": [
          "classification bias"
        ],
        "category": "specific_connectable",
        "rationale": "Biased classification is central to the paper's focus on overcoming shortcut learning, making it a key concept for linking.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "shortcut learning",
        "canonical": "Shortcut Learning",
        "aliases": [
          "shortcut bias"
        ],
        "category": "specific_connectable",
        "rationale": "Shortcut learning is a critical challenge addressed by the proposed framework, relevant for discussions on model generalization.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "imbalanced attributes",
        "canonical": "Imbalanced Attributes",
        "aliases": [
          "attribute imbalance"
        ],
        "category": "specific_connectable",
        "rationale": "Imbalanced attributes are a significant factor in the formation of biased feature spaces, crucial for understanding dataset challenges.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "generalization capability",
        "canonical": "Generalization Capability",
        "aliases": [
          "model generalization"
        ],
        "category": "broad_technical",
        "rationale": "Generalization capability is a fundamental goal of machine learning models, relevant across various applications.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Collapse",
      "resolved_canonical": "Neural Collapse",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "biased classification",
      "resolved_canonical": "Biased Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "shortcut learning",
      "resolved_canonical": "Shortcut Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "imbalanced attributes",
      "resolved_canonical": "Imbalanced Attributes",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "generalization capability",
      "resolved_canonical": "Generalization Capability",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse

**Korean Title:** 지름길을 넘어 항해하기: 신경 붕괴의 관점에서 본 편향 제거 학습

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2405.05587.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2405.05587](https://arxiv.org/abs/2405.05587)

## 🔗 유사한 논문
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.2% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.3% similar)
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (81.0% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (80.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Generalization Capability|Generalization Capability]]
**🔗 Specific Connectable**: [[keywords/Biased Classification|Biased Classification]], [[keywords/Shortcut Learning|Shortcut Learning]], [[keywords/Imbalanced Attributes|Imbalanced Attributes]]
**⚡ Unique Technical**: [[keywords/Neural Collapse|Neural Collapse]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2405.05587v2 Announce Type: replace-cross 
Abstract: Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure. In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes. We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability. To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity. With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations. Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets. Our code is available at https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/main.

## 🔍 Abstract (한글 번역)

arXiv:2405.05587v2 발표 유형: 교차 교체  
초록: 최근 연구에서는 신경 붕괴(Neural Collapse)라는 흥미로운 현상을 주목했습니다. 이는 신경망이 특징 공간과 학습 목표 간의 올바른 상관관계를 확립할 때, 마지막 층의 특징과 분류기 가중치가 안정적이고 대칭적인 구조로 붕괴되는 현상입니다. 본 논문에서는 불균형 속성을 가진 편향된 데이터셋에 대한 신경 붕괴의 조사를 확장합니다. 우리는 모델이 학습 초기에 쉽게 지름길 학습의 함정에 빠져 편향되고 붕괴되지 않은 특징 공간을 형성하게 되며, 이는 되돌리기 어렵고 일반화 능력을 제한한다는 것을 관찰했습니다. 편향된 분류의 근본 원인을 해결하기 위해, 우리는 최근의 주요 학습 영감을 따르고, 추가적인 학습 복잡성 없이 지름길 회피 학습 프레임워크를 제안합니다. 신경 붕괴 구조에 기반한 잘 설계된 지름길 프라임을 통해, 모델은 단순한 지름길 추구를 피하고 본질적인 상관관계를 자연스럽게 포착하도록 유도됩니다. 실험 결과는 우리의 방법이 학습 중 더 나은 수렴 특성을 유도하며, 합성 및 실제 편향된 데이터셋 모두에서 최첨단 일반화 성능을 달성함을 보여줍니다. 우리의 코드는 https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/main에서 이용 가능합니다.

## 📝 요약

이 논문은 Neural Collapse 현상을 불균형 속성을 가진 편향된 데이터셋으로 확장하여 연구합니다. 초기 학습 단계에서 모델이 편향된 비수렴 특징 공간을 형성하는 문제를 해결하기 위해, 추가적인 복잡성 없이 단축 학습을 피하는 프레임워크를 제안합니다. Neural Collapse 구조에 기반한 잘 설계된 단축 프라임을 통해 모델이 본질적인 상관관계를 자연스럽게 포착하도록 유도합니다. 실험 결과, 제안된 방법은 학습 중 더 나은 수렴 특성을 보이며, 편향된 데이터셋에서 최첨단 일반화 성능을 달성합니다.

## 🎯 주요 포인트

- 1. Neural Collapse는 신경망의 마지막 층 특징과 분류기 가중치가 안정적이고 대칭적인 구조로 수렴하는 현상입니다.
- 2. 편향된 데이터셋에서 Neural Collapse를 조사한 결과, 모델이 초기 학습 단계에서 비대칭적이고 비수렴적인 특징 공간을 형성하는 경향이 있음을 발견했습니다.
- 3. 편향된 분류의 근본 원인을 해결하기 위해, 우리는 추가적인 학습 복잡도 없이 지름길 학습을 피하는 학습 프레임워크를 제안합니다.
- 4. 제안된 방법은 Neural Collapse 구조에 기반한 지름길 프라임을 활용하여 모델이 단순한 지름길을 피하고 내재된 상관관계를 포착하도록 유도합니다.
- 5. 실험 결과, 우리의 방법은 학습 중 더 나은 수렴 특성을 유도하고, 편향된 데이터셋에서 최첨단 일반화 성능을 달성합니다.


---

*Generated on 2025-09-23 11:14:17*