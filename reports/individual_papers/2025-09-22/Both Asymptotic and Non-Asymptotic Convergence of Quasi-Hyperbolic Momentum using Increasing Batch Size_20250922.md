---
keywords:
  - Quasi-Hyperbolic Momentum
  - Increasing Batch Size
  - Neural Network
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2506.23544
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:09:38.408680",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Quasi-Hyperbolic Momentum",
    "Increasing Batch Size",
    "Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Quasi-Hyperbolic Momentum": 0.78,
    "Increasing Batch Size": 0.72,
    "Neural Network": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Quasi-Hyperbolic Momentum",
        "canonical": "Quasi-Hyperbolic Momentum",
        "aliases": [
          "QHM"
        ],
        "category": "unique_technical",
        "rationale": "Quasi-Hyperbolic Momentum is a specific algorithm that generalizes various momentum methods, providing a unique perspective on optimization strategies.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Increasing Batch Size",
        "canonical": "Increasing Batch Size",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Increasing batch size is a key strategy discussed in the paper for achieving convergence without decaying the learning rate.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "NN",
          "Deep Neural Networks"
        ],
        "category": "broad_technical",
        "rationale": "Neural networks are central to the discussion of optimization methods in the paper, linking to broader topics in machine learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "momentum methods",
      "stochastic gradient descent",
      "convex objective functions"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Quasi-Hyperbolic Momentum",
      "resolved_canonical": "Quasi-Hyperbolic Momentum",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Increasing Batch Size",
      "resolved_canonical": "Increasing Batch Size",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size

**Korean Title:** ì ì§„ì  ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€ì˜ ì ê·¼ì  ë° ë¹„ì ê·¼ì  ìˆ˜ë ´

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2506.23544.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2506.23544](https://arxiv.org/abs/2506.23544)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size_20250922|Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size]] (84.1% similar)
- [[2025-09-22/DIVEBATCH_ Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation_20250922|DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation]] (83.0% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (82.2% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.5% similar)
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (80.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**âš¡ Unique Technical**: [[keywords/Quasi-Hyperbolic Momentum|Quasi-Hyperbolic Momentum]], [[keywords/Increasing Batch Size|Increasing Batch Size]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.23544v3 Announce Type: replace 
Abstract: Momentum methods were originally introduced for their superiority to stochastic gradient descent (SGD) in deterministic settings with convex objective functions. However, despite their widespread application to deep neural networks -- a representative case of stochastic nonconvex optimization -- the theoretical justification for their effectiveness in such settings remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that generalizes various momentum methods and has been studied to better understand the class of momentum-based algorithms as a whole. In this paper, we provide both asymptotic and non-asymptotic convergence results for mini-batch QHM with an increasing batch size. We show that achieving asymptotic convergence requires either a decaying learning rate or an increasing batch size. Since a decaying learning rate adversely affects non-asymptotic convergence, we demonstrate that using mini-batch QHM with an increasing batch size -- without decaying the learning rate -- can be a more effective strategy. Our experiments show that even a finite increase in batch size can provide benefits for training neural networks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.23544v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëª¨ë©˜í…€ ë°©ë²•ì€ ì›ë˜ ë³¼ë¡ ëª©ì  í•¨ìˆ˜ê°€ ìˆëŠ” ê²°ì •ë¡ ì  í™˜ê²½ì—ì„œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ì´ìœ ë¡œ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¬ì¸µ ì‹ ê²½ë§ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì ìš©ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŠ” í™•ë¥ ì  ë¹„ë³¼ë¡ ìµœì í™”ì˜ ëŒ€í‘œì ì¸ ì‚¬ë¡€ë¡œ, ì´ëŸ¬í•œ í™˜ê²½ì—ì„œì˜ íš¨ê³¼ì— ëŒ€í•œ ì´ë¡ ì  ì •ë‹¹ì„±ì€ ì—¬ì „íˆ ì œí•œì ì…ë‹ˆë‹¤. ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€(QHM)ì€ ë‹¤ì–‘í•œ ëª¨ë©˜í…€ ë°©ë²•ì„ ì¼ë°˜í™”í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ëª¨ë©˜í…€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ì „ì²´ í´ë˜ìŠ¤ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ì—°êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì— ëŒ€í•œ ì ê·¼ì  ë° ë¹„ì ê·¼ì  ìˆ˜ë ´ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì ê·¼ì  ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ë ¤ë©´ í•™ìŠµë¥  ê°ì†Œ ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•™ìŠµë¥  ê°ì†ŒëŠ” ë¹„ì ê·¼ì  ìˆ˜ë ´ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì—, í•™ìŠµë¥ ì„ ê°ì†Œì‹œí‚¤ì§€ ì•Šê³  ë°°ì¹˜ í¬ê¸°ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¸ ì „ëµì´ ë  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ë°°ì¹˜ í¬ê¸°ì˜ ìœ í•œí•œ ì¦ê°€ë§Œìœ¼ë¡œë„ ì‹ ê²½ë§ í›ˆë ¨ì— ì´ì ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëª¨ë©˜í…€ ë°©ë²•ì˜ ì¼ë°˜í™”ëœ ì•Œê³ ë¦¬ì¦˜ì¸ ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€(QHM)ì„ ì—°êµ¬í•˜ì—¬ ëª¨ë©˜í…€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì´í•´ë¥¼ ë•ê³ ì í•©ë‹ˆë‹¤. ë¯¸ë‹ˆë°°ì¹˜ QHMì˜ ìˆ˜ë ´ ê²°ê³¼ë¥¼ ì œì‹œí•˜ë©°, ì ì§„ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ ë¹„ê°ì†Œ í•™ìŠµë¥ ê³¼ ê²°í•©í•˜ì—¬ ë¹„ëŒ€ì¹­ ìˆ˜ë ´ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµë¥  ê°ì†Œê°€ ë¹„ëŒ€ì¹­ ìˆ˜ë ´ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ ê³ ë ¤í•  ë•Œ, ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ ë” íš¨ê³¼ì ì¸ ì „ëµì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ìœ í•œí•œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ë§Œìœ¼ë¡œë„ ì‹ ê²½ë§ í›ˆë ¨ì— ì´ì ì´ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëª¨ë©˜í…€ ë°©ë²•ì€ ì›ë˜ í™•ì •ì  í™˜ê²½ì—ì„œì˜ ìš°ìˆ˜ì„± ë•Œë¬¸ì— ë„ì…ë˜ì—ˆìœ¼ë‚˜, ë¹„í™•ì •ì  ë¹„ë³¼ë¡ ìµœì í™”ì¸ ì‹¬ì¸µ ì‹ ê²½ë§ì—ì„œì˜ ì´ë¡ ì  ê·¼ê±°ëŠ” ì œí•œì ì´ë‹¤.
- 2. ì¤€-ìŒê³¡ì„  ëª¨ë©˜í…€(QHM)ì€ ë‹¤ì–‘í•œ ëª¨ë©˜í…€ ë°©ë²•ì„ ì¼ë°˜í™”í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ëª¨ë©˜í…€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ì „ì²´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì—°êµ¬ë˜ì—ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜ QHMì˜ ì ê·¼ì  ë° ë¹„ì ê·¼ì  ìˆ˜ë ´ ê²°ê³¼ë¥¼ ì œì‹œí•œë‹¤.
- 4. ì ê·¼ì  ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” í•™ìŠµë¥  ê°ì†Œ ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ í•„ìš”í•˜ë©°, í•™ìŠµë¥  ê°ì†ŒëŠ” ë¹„ì ê·¼ì  ìˆ˜ë ´ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ë°°ì¹˜ í¬ê¸°ì˜ ìœ í•œí•œ ì¦ê°€ë§Œìœ¼ë¡œë„ ì‹ ê²½ë§ í›ˆë ¨ì— ì´ì ì´ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-23 11:09:38*