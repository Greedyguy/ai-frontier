---
keywords:
  - Multi-Agent Reinforcement Learning
  - Contextual Markov Decision Process
  - Dynamics-Aware Context
  - Non-stationarity
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15519
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:28:50.770881",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multi-Agent Reinforcement Learning",
    "Contextual Markov Decision Process",
    "Dynamics-Aware Context",
    "Non-stationarity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multi-Agent Reinforcement Learning": 0.78,
    "Contextual Markov Decision Process": 0.82,
    "Dynamics-Aware Context": 0.85,
    "Non-stationarity": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-Agent Reinforcement Learning",
        "canonical": "Multi-Agent Reinforcement Learning",
        "aliases": [
          "MARL"
        ],
        "category": "broad_technical",
        "rationale": "This is a core concept of the paper, linking it to the broader field of reinforcement learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Contextual Markov Decision Process",
        "canonical": "Contextual Markov Decision Process",
        "aliases": [
          "CMDP"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the proposed method and represents a novel approach within the paper.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Dynamics-Aware Context",
        "canonical": "Dynamics-Aware Context",
        "aliases": [
          "DAC"
        ],
        "category": "unique_technical",
        "rationale": "DAC is the novel method introduced in the paper, crucial for understanding the proposed solution.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Non-stationarity",
        "canonical": "Non-stationarity",
        "aliases": [
          "nonstationary"
        ],
        "category": "specific_connectable",
        "rationale": "Addressing non-stationarity is a key challenge in the paper, linking to broader discussions in reinforcement learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "value function",
      "shared rewards"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-Agent Reinforcement Learning",
      "resolved_canonical": "Multi-Agent Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Contextual Markov Decision Process",
      "resolved_canonical": "Contextual Markov Decision Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Dynamics-Aware Context",
      "resolved_canonical": "Dynamics-Aware Context",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Non-stationarity",
      "resolved_canonical": "Non-stationarity",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem

**Korean Title:** ì™„ì „ íƒˆì¤‘ì•™í™”ëœ í˜‘ë ¥ì  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì€ ë§¥ë½ ëª¨ë¸ë§ ë¬¸ì œì´ë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15519.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15519](https://arxiv.org/abs/2509.15519)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity_20250919|Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity]] (85.7% similar)
- [[2025-09-19/Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention_20250919|Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention]] (82.8% similar)
- [[2025-09-19/CRAFT_ Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks_20250919|CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks]] (82.7% similar)
- [[2025-09-22/Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents_20250922|Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents]] (82.7% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Multi-Agent Reinforcement Learning|Multi-Agent Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Non-stationarity|Non-stationarity]]
**âš¡ Unique Technical**: [[keywords/Contextual Markov Decision Process|Contextual Markov Decision Process]], [[keywords/Dynamics-Aware Context|Dynamics-Aware Context]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15519v1 Announce Type: new 
Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15519v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë³¸ ë…¼ë¬¸ì€ ê° ì—ì´ì „íŠ¸ê°€ ìƒíƒœ, ìì‹ ì˜ ì§€ì—­ì  í–‰ë™, ê·¸ë¦¬ê³  ê³µìœ ëœ ë³´ìƒë§Œì„ ê´€ì°°í•˜ëŠ” ì™„ì „ ë¶„ì‚°í˜• í˜‘ë ¥ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ì—°êµ¬í•œë‹¤. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ê°€ì¹˜ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì‹œ ë¹„ì •ìƒì„± ë° ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì • ì‹œ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ê°€ ë°œìƒí•˜ì—¬ íš¨ê³¼ì ì¸ í˜‘ë ¥ ì •ì±… í•™ìŠµì„ ì €í•´í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì™„ì „ ë¶„ì‚°í˜• ì„¤ì •ì—ì„œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê³µë™ ì •ì±…ì„ ëª¨ë¸ë§í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë‘ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ì§€ ëª»í•œë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Dynamics-Aware Context (DAC)ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ì´ëŠ” ê° ì—ì´ì „íŠ¸ê°€ ë¡œì»¬í•˜ê²Œ ì¸ì‹í•˜ëŠ” ì‘ì—…ì„ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§ˆë¥´ì½”í”„ ê²°ì • í”„ë¡œì„¸ìŠ¤ë¡œ í˜•ì‹í™”í•˜ê³ , ë™ì  ì¸ì‹ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ë§ì„ í†µí•´ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ë¥¼ ëª¨ë‘ í•´ê²°í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, DACëŠ” ê° ì—ì´ì „íŠ¸ì˜ ë¹„ì •ìƒì ì¸ ë¡œì»¬ ì‘ì—… ë™íƒœë¥¼ ê´€ì°°ë˜ì§€ ì•Šì€ ì»¨í…ìŠ¤íŠ¸ ê°„ì˜ ì „í™˜ìœ¼ë¡œ ê·€ì†ì‹œí‚¤ë©°, ê° ì»¨í…ìŠ¤íŠ¸ëŠ” ê°œë³„ì ì¸ ê³µë™ ì •ì±…ì— í•´ë‹¹í•œë‹¤. ê·¸ëŸ° ë‹¤ìŒ DACëŠ” ì ì¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê³„ë³„ ë™íƒœ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³  ì´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¼ê³  í•œë‹¤. ê° ì—ì´ì „íŠ¸ì— ëŒ€í•´, DACëŠ” ê°€ì¹˜ í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì‹œ ë¹„ì •ìƒì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í•œë‹¤. ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì • ì‹œ, í˜‘ë ¥ì  í–‰ë™ ì„ íƒì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ë‚™ê´€ì ì¸ í•œê³„ ê°€ì¹˜ë¥¼ ë„ì¶œí•˜ì—¬ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ í˜‘ë ¥ ì‘ì—…(í–‰ë ¬ ê²Œì„, í¬ì‹ìì™€ ë¨¹ì´, SMAC í¬í•¨)ì—ì„œ DACë¥¼ í‰ê°€í•˜ì˜€ìœ¼ë©°, ì—¬ëŸ¬ ê¸°ì¤€ì„  ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì€ ê·¸ íš¨ê³¼ì„±ì„ ì…ì¦í•œë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì™„ì „ ë¶„ì‚°í˜• í˜‘ë ¥ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ì—°êµ¬í•©ë‹ˆë‹¤. ê° ì—ì´ì „íŠ¸ëŠ” ìƒíƒœ, ë¡œì»¬ í–‰ë™, ê³µìœ  ë³´ìƒë§Œì„ ê´€ì°°í•˜ë©°, ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì— ì ‘ê·¼í•  ìˆ˜ ì—†ì–´ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ì§€ ëª»í–ˆìœ¼ë‚˜, ë³¸ ì—°êµ¬ì—ì„œëŠ” Dynamics-Aware Context (DAC)ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. DACëŠ” ê° ì—ì´ì „íŠ¸ê°€ ì¸ì§€í•˜ëŠ” ê³¼ì œë¥¼ ë¬¸ë§¥ì  ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •ìœ¼ë¡œ ê³µì‹í™”í•˜ê³ , ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™”ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. DACëŠ” ìˆ¨ê²¨ì§„ ë¬¸ë§¥ ì „í™˜ì„ í†µí•´ ë¹„ì •ìƒì„±ì„ í•´ê²°í•˜ê³ , ë¬¸ë§¥ ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í•˜ì—¬ í˜‘ë ¥ì  í–‰ë™ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ í˜‘ë ¥ ê³¼ì œì—ì„œ DACì˜ ìš°ìˆ˜í•œ ì„±ëŠ¥ì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ê° ì—ì´ì „íŠ¸ê°€ ìƒíƒœ, ì§€ì—­ì  í–‰ë™, ê³µìœ  ë³´ìƒë§Œì„ ê´€ì¸¡í•˜ëŠ” ì™„ì „ ë¶„ì‚°í˜• í˜‘ë ¥ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ì—°êµ¬í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì™„ì „ ë¶„ì‚°í˜• í™˜ê²½ì—ì„œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê³µë™ ì •ì±…ì„ ëª¨ë¸ë§í•  ìˆ˜ ì—†ì–´ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ Dynamics-Aware Context (DAC) ë°©ë²•ì€ ë¹„ì •ìƒì„±ê³¼ ìƒëŒ€ì  ê³¼ì¼ë°˜í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë™ì  ì¸ì‹ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 4. DACëŠ” ìˆ¨ê²¨ì§„ ì»¨í…ìŠ¤íŠ¸ ê°„ ì „í™˜ì„ í†µí•´ ë¹„ì •ìƒì ì¸ ì§€ì—­ì  ê³¼ì—… ë™íƒœë¥¼ ì„¤ëª…í•˜ê³ , ê° ì—ì´ì „íŠ¸ì— ëŒ€í•´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í•˜ì—¬ ë¹„ì •ìƒì„± ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 5. DACëŠ” ë‹¤ì–‘í•œ í˜‘ë ¥ ê³¼ì œì—ì„œ ì‹¤í—˜ì ìœ¼ë¡œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì—¬ëŸ¬ ê¸°ì¤€ì„  ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ ê·¸ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:28:50*