---
keywords:
  - Large Language Model
  - Logical Reasoning
  - DivLogicEval
  - Evaluation Metric for Bias Mitigation
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15587
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:06:49.012984",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Logical Reasoning",
    "DivLogicEval",
    "Evaluation Metric for Bias Mitigation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Logical Reasoning": 0.78,
    "DivLogicEval": 0.8,
    "Evaluation Metric for Bias Mitigation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating logical reasoning capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "logical reasoning",
        "canonical": "Logical Reasoning",
        "aliases": [
          "logic reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for evaluating intelligence in language models, central to the paper's thesis.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "DivLogicEval",
        "canonical": "DivLogicEval",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The proposed benchmark is a unique contribution of the paper.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "evaluation metric",
        "canonical": "Evaluation Metric for Bias Mitigation",
        "aliases": [
          "bias mitigation metric"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel metric to improve evaluation reliability, significant for linking.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "popular",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "logical reasoning",
      "resolved_canonical": "Logical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "DivLogicEval",
      "resolved_canonical": "DivLogicEval",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "evaluation metric",
      "resolved_canonical": "Evaluation Metric for Bias Mitigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models

**Korean Title:** DivLogicEval: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë…¼ë¦¬ì  ì¶”ë¡  í‰ê°€ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15587.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15587](https://arxiv.org/abs/2509.15587)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.6% similar)
- [[2025-09-22/REFER_ Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting_20250922|REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting]] (85.5% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.4% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (84.9% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Logical Reasoning|Logical Reasoning]], [[keywords/DivLogicEval|DivLogicEval]], [[keywords/Evaluation Metric for Bias Mitigation|Evaluation Metric for Bias Mitigation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15587v1 Announce Type: cross 
Abstract: Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15587v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ìì—°ì–´ì—ì„œì˜ ë…¼ë¦¬ì  ì¶”ë¡ ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì¸ê°„ ì§€ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì¤‘ìš”í•œ ì²™ë„ë¡œ ì¸ì‹ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ì¸ê¸° ìˆëŠ” ë²¤ì¹˜ë§ˆí¬ëŠ” ì—¬ëŸ¬ ì¶”ë¡  ê¸°ìˆ ì„ ì–½íˆê²Œ í•˜ì—¬ ë…¼ë¦¬ì  ì¶”ë¡  ê¸°ìˆ ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” í‰ê°€ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì‹œì— ê¸°ì¡´ì˜ ë…¼ë¦¬ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ëŠ” ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ì œí•œì ì´ë©°, ì´ìƒì ì¸ ë…¼ë¦¬ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì˜ ë¶„í¬ì™€ëŠ” ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆì–´ í¸í–¥ëœ í‰ê°€ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì§„ìˆ ë¡œ êµ¬ì„±ëœ ìì—° ë¬¸ì¥ì„ ì§ê´€ì— ë°˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ê³ ì „ ë…¼ë¦¬ ë²¤ì¹˜ë§ˆí¬ì¸ DivLogicEvalì„ ì œì•ˆí•©ë‹ˆë‹¤. ë³´ë‹¤ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´, LLMì— ë‚´ì¬ëœ í¸í–¥ì„±ê³¼ ë¬´ì‘ìœ„ì„±ì˜ ì˜í–¥ì„ ì™„í™”í•˜ëŠ” ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œë„ ë„ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´ DivLogicEvalì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ìš”êµ¬ë˜ëŠ” ë…¼ë¦¬ì  ì¶”ë¡ ì˜ ì •ë„ë¥¼ ì…ì¦í•˜ê³ , ë…¼ë¦¬ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ ë‹¤ì–‘í•œ ì¸ê¸° ìˆëŠ” LLMì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë…¼ë¦¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ DivLogicEvalì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë…¼ë¦¬ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ëŠ” ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•˜ê³  ì´ìƒì ì¸ ë¶„í¬ì™€ ì°¨ì´ê°€ ìˆì–´ í‰ê°€ ê²°ê³¼ì— í¸í–¥ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. DivLogicEvalì€ ë‹¤ì–‘í•œ ì§„ìˆ ë¡œ êµ¬ì„±ëœ ìì—°ì–´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•˜ë©°, í¸í–¥ê³¼ ë¬´ì‘ìœ„ì„±ì„ ì¤„ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œë„ ë„ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´ DivLogicEvalì—ì„œ ë…¼ë¦¬ì  ì¶”ë¡ ì´ ì–¼ë§ˆë‚˜ í•„ìš”í•œì§€, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ LLMì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ë…¼ë¦¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìì—°ì–´ì—ì„œì˜ ë…¼ë¦¬ì  ì¶”ë¡ ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¸ê°„ ì§€ëŠ¥ ì¸¡ì •ì— ì¤‘ìš”í•œ ìš”ì†Œë¡œ ì¸ì‹ë˜ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ì˜ ë…¼ë¦¬ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ëŠ” ì–¸ì–´ ë‹¤ì–‘ì„±ì´ ì œí•œì ì´ë©°, ì´ìƒì ì¸ ë…¼ë¦¬ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì˜ ë¶„í¬ì™€ ì°¨ì´ê°€ ìˆì–´ í¸í–¥ëœ í‰ê°€ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
- 3. ìƒˆë¡œìš´ ë…¼ë¦¬ ë²¤ì¹˜ë§ˆí¬ì¸ DivLogicEvalì„ ì œì•ˆí•˜ì—¬ ë‹¤ì–‘í•œ ì§„ìˆ ë¡œ êµ¬ì„±ëœ ìì—° ë¬¸ì¥ì„ í†µí•´ ë³´ë‹¤ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ë¥¼ ëª©í‘œë¡œ í•œë‹¤.
- 4. í¸í–¥ê³¼ ë¬´ì‘ìœ„ì„±ì„ ì™„í™”í•˜ëŠ” ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œë¥¼ ë„ì…í•˜ì—¬ LLMì˜ ë…¼ë¦¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ë‹¤ ì •í™•í•˜ê²Œ í‰ê°€í•œë‹¤.
- 5. ì‹¤í—˜ì„ í†µí•´ DivLogicEvalì—ì„œ ë…¼ë¦¬ì  ì¶”ë¡ ì´ í•„ìš”í•œ ì •ë„ë¥¼ ì…ì¦í•˜ê³ , ë‹¤ì–‘í•œ ì¸ê¸° ìˆëŠ” LLMì˜ ë…¼ë¦¬ì  ì¶”ë¡  ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤.


---

*Generated on 2025-09-23 09:06:49*