---
keywords:
  - Large Language Model
  - Dynamics Modelling
  - Self-Verification Sampling
  - Tool Use in Language Models
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2506.02918
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:39:01.793334",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Dynamics Modelling",
    "Self-Verification Sampling",
    "Tool Use in Language Models"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Dynamics Modelling": 0.78,
    "Self-Verification Sampling": 0.77,
    "Tool Use in Language Models": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on enhancing language model capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "dynamics modelling",
        "canonical": "Dynamics Modelling",
        "aliases": [
          "DyMo"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach specific to the paper's methodology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "self-verification sampling",
        "canonical": "Self-Verification Sampling",
        "aliases": [
          "SVS"
        ],
        "category": "unique_technical",
        "rationale": "Represents a unique technique proposed in the paper for improving model reliability.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "tool use",
        "canonical": "Tool Use in Language Models",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Highlights the application focus of the paper, connecting to broader research on LLM applications.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "stateful environments",
      "test-time compute strategies",
      "future states"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "dynamics modelling",
      "resolved_canonical": "Dynamics Modelling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "self-verification sampling",
      "resolved_canonical": "Self-Verification Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "tool use",
      "resolved_canonical": "Tool Use in Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# World Modelling Improves Language Model Agents

**Korean Title:** ì„¸ê³„ ëª¨ë¸ë§ì´ ì–¸ì–´ ëª¨ë¸ ì—ì´ì „íŠ¸ë¥¼ ê°œì„ í•œë‹¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.02918.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2506.02918](https://arxiv.org/abs/2506.02918)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (85.6% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.4% similar)
- [[2025-09-19/Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture]] (84.9% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (84.8% similar)
- [[2025-09-22/Discrete Diffusion in Large Language and Multimodal Models_ A Survey_20250922|Discrete Diffusion in Large Language and Multimodal Models: A Survey]] (84.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Dynamics Modelling|Dynamics Modelling]], [[keywords/Self-Verification Sampling|Self-Verification Sampling]]
**ğŸš€ Evolved Concepts**: [[keywords/Tool Use in Language Models|Tool Use in Language Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.02918v2 Announce Type: replace 
Abstract: Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.02918v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìƒíƒœ ê¸°ë°˜ í™˜ê²½ì—ì„œì˜ ë„êµ¬ ì‚¬ìš©ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì—ê²Œ ë…íŠ¹í•œ ë„ì „ì„ ì œì‹œí•˜ë©°, í™˜ê²½ì—ì„œ ë°˜ë³µì ì¸ ì‹œí—˜ì— ì˜ì¡´í•˜ëŠ” ê¸°ì¡´ì˜ í…ŒìŠ¤íŠ¸ ì‹œì  ê³„ì‚° ì „ëµì€ ë¹„í˜„ì‹¤ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í›„í›ˆë ¨ ë™ì•ˆ í•¨ìˆ˜ í˜¸ì¶œê³¼ í•¨ê»˜ ìƒíƒœ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ LLMì— ì¶”ê°€í•˜ëŠ” ë°©ë²•ì¸ ë™ì  ëª¨ë¸ë§(DyMo)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMì€ ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ í†µí•´ ìì‹ ì˜ í–‰ë™ì˜ ë¯¸ë˜ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Berkeley Function Calling Leaderboard V2ì—ì„œ DyMoëŠ” ì„±ê³µë¥ ì„ í–¥ìƒì‹œí‚¤ê³  í™˜ê°ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ ìê¸° ê²€ì¦ ìƒ˜í”Œë§(SVS)ì— í†µí•©í•˜ì—¬, ì´ê²ƒì´ ì‹œí—˜ íšŸìˆ˜ kì— ëŒ€í•œ pass^kë¥¼ ìƒë‹¹íˆ í–¥ìƒì‹œí‚¤ê³  ëª¨ë¸ì´ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œë ¥ì„ ê±°ë¶€í•  ìˆ˜ ìˆê²Œ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DyMoì™€ SVSëŠ” ë„êµ¬ ì‚¬ìš©ì„ ìœ„í•œ LLMì˜ íš¨ê³¼ì„±ê³¼ ì‹ ë¢°ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì—°êµ¬ê°€ ì˜¤ë¼í´ í™˜ê²½ì„ ë°˜ë³µì ìœ¼ë¡œ ì¿¼ë¦¬í•˜ì§€ ì•Šê³  LLM ì¶”ë¡ ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ê³„íš ê°•í™” í•™ìŠµ(RL) ë°©ë²•ìœ¼ë¡œ ë‚˜ì•„ê°€ëŠ” ê¸¸ì„ ì œì‹œí•œë‹¤ê³  ë¯¿ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìƒíƒœê°€ ë³€í•˜ëŠ” í™˜ê²½ì—ì„œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DyMoë¼ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. DyMoëŠ” í›ˆë ¨ í›„ ìƒíƒœ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ LLMì´ ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ í†µí•´ ë¯¸ë˜ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Berkeley Function Calling Leaderboard V2ì—ì„œ ì„±ê³µë¥ ì„ ë†’ì´ê³  í™˜ê° í˜„ìƒì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ë‚´ë¶€ í™˜ê²½ ëª¨ë¸ì„ ìì²´ ê²€ì¦ ìƒ˜í”Œë§(SVS)ê³¼ í†µí•©í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œë ¥ì„ ê±°ë¶€í•˜ê³  ì—¬ëŸ¬ ì‹œë„ì—ì„œ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLM ì¶”ë¡ ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ê³„íš ê°•í™” í•™ìŠµ(RL) ë°©ë²•ì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. DyMoëŠ” LLMsì— ìƒíƒœ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ë„êµ¬ ì‚¬ìš© ì‹œ ë¯¸ë˜ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
- 2. Berkeley Function Calling Leaderboard V2ì—ì„œ DyMoëŠ” ì„±ê³µë¥ ì„ ë†’ì´ê³  í™˜ê° í˜„ìƒì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.
- 3. DyMoì™€ SVSì˜ í†µí•©ì€ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œë ¥ì„ ê±°ë¶€í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ LLMsì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. ë³¸ ì—°êµ¬ëŠ” ë°˜ë³µì ì¸ í™˜ê²½ ì¿¼ë¦¬ ì—†ì´ LLM ì¶”ë¡ ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ê³„íš RL ë°©ë²•ì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:39:01*