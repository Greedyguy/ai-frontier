---
keywords:
  - Few-Shot Learning
  - Large Language Model
  - Kernel Method
  - Optimal Design Regularization
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15676
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:10:08.131950",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "Large Language Model",
    "Kernel Method",
    "Optimal Design Regularization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.8,
    "Large Language Model": 0.85,
    "Kernel Method": 0.75,
    "Optimal Design Regularization": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "In-context learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "ICL"
        ],
        "category": "specific_connectable",
        "rationale": "In-context learning is closely related to Few-Shot Learning, which is a trending concept in adapting models with limited data.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect well with other concepts in the field.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Kernel trick",
        "canonical": "Kernel Method",
        "aliases": [
          "Kernelization"
        ],
        "category": "unique_technical",
        "rationale": "The kernel trick is a unique method used in the paper to handle high-dimensional spaces, offering a novel approach.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Optimal design-based regularizer",
        "canonical": "Optimal Design Regularization",
        "aliases": [
          "Design-based regularization"
        ],
        "category": "unique_technical",
        "rationale": "This concept introduces a novel regularization technique to enhance diversity in example selection.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "example selection",
      "query-specific optimization",
      "prediction error"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "In-context learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Kernel trick",
      "resolved_canonical": "Kernel Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Optimal design-based regularizer",
      "resolved_canonical": "Optimal Design Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning

**Korean Title:** KITE: ë¬¸ë§¥ í•™ìŠµì„ ìœ„í•œ ì»¤ë„í™” ë° ì •ë³´ ì´ë¡ ì  ì˜ˆì œë“¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15676.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15676](https://arxiv.org/abs/2509.15676)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Disentangling Latent Shifts of In-Context Learning with Weak Supervision_20250922|Disentangling Latent Shifts of In-Context Learning with Weak Supervision]] (85.6% similar)
- [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (84.4% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (83.4% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (82.5% similar)
- [[2025-09-22/Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training_20250922|Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Kernel Method|Kernel Method]], [[keywords/Optimal Design Regularization|Optimal Design Regularization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15676v1 Announce Type: cross 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective. We first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of exemplars from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15676v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë§¥ë½ ë‚´ í•™ìŠµ(ICL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‘ì—…ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” í”„ë¡¬í”„íŠ¸ì— ì œì‹œëœ ëª‡ ê°€ì§€ ì‹ ì¤‘í•˜ê²Œ ì„ íƒëœ ì‘ì—…ë³„ ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ LLMì˜ ì œí•œëœ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ê³ ë ¤í•  ë•Œ, ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì´ ì œê¸°ë©ë‹ˆë‹¤: ì£¼ì–´ì§„ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì˜ˆì œë¥¼ ì„ íƒí•´ì•¼ í• ê¹Œìš”? KATEì™€ ê°™ì€ ìµœê·¼ì ‘ ì´ì›ƒ ê¸°ë°˜ ë°©ë²•ì´ ì´ ëª©ì ì„ ìœ„í•´ ë„ë¦¬ ì±„íƒë˜ì—ˆì§€ë§Œ, ì´ë“¤ì€ ê³ ì°¨ì› ì„ë² ë”© ê³µê°„ì—ì„œì˜ ì¼ë°˜í™” ë¶€ì¡±ê³¼ ë‹¤ì–‘ì„± ê²°ì—¬ì™€ ê°™ì€ ì˜ ì•Œë ¤ì§„ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ ì›ì¹™ì ì¸ ê´€ì ì—ì„œ ICLì—ì„œì˜ ì˜ˆì œ ì„ íƒ ë¬¸ì œë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¨¼ì € LLMì„ ì…ë ¥ ì„ë² ë”©ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¡œ ëª¨ë¸ë§í•˜ê³ , ì˜ˆì œ ì„ íƒ ì‘ì—…ì„ íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ìµœì í™” ë¬¸ì œë¡œ êµ¬ì„±í•©ë‹ˆë‹¤: íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë” í° ì˜ˆì œ ì€í–‰ì—ì„œ ì˜ˆì œì˜ í•˜ìœ„ ì§‘í•©ì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ê³µì‹ì€ íŠ¹ì • ì¿¼ë¦¬ ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ì •í™•í•œ ì˜ˆì¸¡ì„ ëª©í‘œë¡œ í•¨ìœ¼ë¡œì¨ ì „í†µì ì¸ ì¼ë°˜í™” ì¤‘ì‹¬ì˜ í•™ìŠµ ì´ë¡ ì  ì ‘ê·¼ ë°©ì‹ì—ì„œ ë²—ì–´ë‚©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê·¼ì‚¬ì ìœ¼ë¡œ ë¶€ë¶„ ëª¨ë“ˆëŸ¬ì¸ ì›ì¹™ì ì¸ ëŒ€ë¦¬ ëª©í‘œë¥¼ ë„ì¶œí•˜ì—¬ ê·¼ì‚¬ ë³´ì¥ì´ ìˆëŠ” íƒìš• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ (i) ëª…ì‹œì  ë§¤í•‘ ì—†ì´ ê³ ì°¨ì› íŠ¹ì§• ê³µê°„ì—ì„œ ì‘ë™í•˜ê¸° ìœ„í•´ ì»¤ë„ íŠ¸ë¦­ì„ í†µí•©í•˜ê³ , (ii) ì„ íƒëœ ì˜ˆì œì˜ ë‹¤ì–‘ì„±ì„ ì¥ë ¤í•˜ê¸° ìœ„í•´ ìµœì  ì„¤ê³„ ê¸°ë°˜ ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ë°©ë²•ì„ ë”ìš± í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤ì¦ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í‘œì¤€ ê²€ìƒ‰ ë°©ë²•ì— ë¹„í•´ ìƒë‹¹í•œ ê°œì„ ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œ ì„¸ê³„ì˜ ë ˆì´ë¸”ì´ ë¶€ì¡±í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ICLì„ ìœ„í•œ êµ¬ì¡° ì¸ì‹, ë‹¤ì–‘í•œ ì˜ˆì œ ì„ íƒì˜ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ë°ì´í„° ë¶€ì¡± ê³¼ì œì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ì¸ ë¬¸ë§¥ ë‚´ í•™ìŠµ(ICL)ì—ì„œ ìµœì ì˜ ì˜ˆì œ ì„ íƒ ë¬¸ì œë¥¼ ì •ë³´ ì´ë¡ ì  ê´€ì ì—ì„œ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìµœê·¼ì ‘ ì´ì›ƒ ê¸°ë°˜ ë°©ë²•ë“¤ì´ ê³ ì°¨ì› ì„ë² ë”© ê³µê°„ì—ì„œ ì¼ë°˜í™” ë¶€ì¡±ê³¼ ë‹¤ì–‘ì„± ê²°ì—¬ ë¬¸ì œë¥¼ ê²ªëŠ”ë‹¤ëŠ” ì ì„ ì§€ì í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LLMì„ ì…ë ¥ ì„ë² ë”©ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¡œ ëª¨ë¸ë§í•˜ì—¬ íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì˜ˆì œ ì„ íƒ ìµœì í™” ë¬¸ì œë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ê·¼ì‚¬ì ìœ¼ë¡œ ë¶€ë¶„ ëª¨ë“ˆëŸ¬ì¸ ëŒ€ë¦¬ ëª©ì ì„ ë„ì¶œí•˜ì—¬ íƒìš• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê³ , ì»¤ë„ íŠ¸ë¦­ê³¼ ìµœì  ì„¤ê³„ ê¸°ë°˜ ì •ê·œí™”ë¥¼ ë„ì…í•˜ì—¬ ë‹¤ì–‘ì„±ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë‹¤ì–‘í•œ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë¨ì„ ë³´ì—¬ì£¼ë©°, êµ¬ì¡° ì¸ì‹ê³¼ ë‹¤ì–‘í•œ ì˜ˆì œ ì„ íƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. In-context learning(ICL)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‘ì—…ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ê°•ë ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ë¶€ìƒí–ˆìŠµë‹ˆë‹¤.
- 2. LLMì˜ ì œí•œëœ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ë•Œë¬¸ì—, ì£¼ì–´ì§„ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì˜ˆì œë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•œ ë¬¸ì œë¡œ ëŒ€ë‘ë©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ ì ‘ê·¼ë²•ì„ í†µí•´ ICLì—ì„œì˜ ì˜ˆì œ ì„ íƒ ë¬¸ì œë¥¼ ë‹¤ë£¨ë©°, ì¿¼ë¦¬ë³„ ìµœì í™” ë¬¸ì œë¡œ ì˜ˆì œ ì„ íƒì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤.
- 4. ëŒ€ìš©ëŸ‰ ì˜ˆì œ ì€í–‰ì—ì„œ íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì˜ˆì œë¥¼ ì„ íƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê·¼ì‚¬ì ìœ¼ë¡œ ë¶€ë¶„ ëª¨ë“ˆëŸ¬ì¸ ëŒ€ë¦¬ ëª©í‘œë¥¼ ë„ì¶œí•˜ê³ , íƒìš• ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ì ìœ¼ë¡œ, êµ¬ì¡° ì¸ì‹ ë° ë‹¤ì–‘í•œ ì˜ˆì œ ì„ íƒì´ ì‹¤ì œ ì„¸ê³„ì˜ ë ˆì´ë¸”ì´ ë¶€ì¡±í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ICLì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:10:08*