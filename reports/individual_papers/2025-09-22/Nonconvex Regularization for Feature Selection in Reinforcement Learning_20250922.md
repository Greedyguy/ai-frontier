---
keywords:
  - Machine Learning
  - Feature Selection
  - Nonconvex Regularization
  - Least-Squares Temporal-Difference
  - Forward-Reflected-Backward Splitting
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15652
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:32:07.369664",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Machine Learning",
    "Feature Selection",
    "Nonconvex Regularization",
    "Least-Squares Temporal-Difference",
    "Forward-Reflected-Backward Splitting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Machine Learning": 0.8,
    "Feature Selection": 0.75,
    "Nonconvex Regularization": 0.7,
    "Least-Squares Temporal-Difference": 0.8,
    "Forward-Reflected-Backward Splitting": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a key subfield of Machine Learning, providing strong connectivity to related topics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Feature Selection",
        "canonical": "Feature Selection",
        "aliases": [
          "Variable Selection"
        ],
        "category": "specific_connectable",
        "rationale": "Feature Selection is crucial for improving model performance and is widely applicable across various domains.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Nonconvex Regularization",
        "canonical": "Nonconvex Regularization",
        "aliases": [
          "Non-convex Penalty"
        ],
        "category": "unique_technical",
        "rationale": "Nonconvex Regularization is a specialized technique that offers novel approaches to optimization problems.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Least-Squares Temporal-Difference",
        "canonical": "Least-Squares Temporal-Difference",
        "aliases": [
          "LSTD"
        ],
        "category": "specific_connectable",
        "rationale": "LSTD is a specific method in reinforcement learning for policy evaluation, linking to advanced RL techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Forward-Reflected-Backward Splitting",
        "canonical": "Forward-Reflected-Backward Splitting",
        "aliases": [
          "FRBS"
        ],
        "category": "unique_technical",
        "rationale": "FRBS is a novel algorithmic approach, expanding the toolkit for solving nonmonotone-inclusion problems.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "Batch Algorithm",
      "Estimation Bias",
      "Benchmark Datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Feature Selection",
      "resolved_canonical": "Feature Selection",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Nonconvex Regularization",
      "resolved_canonical": "Nonconvex Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Least-Squares Temporal-Difference",
      "resolved_canonical": "Least-Squares Temporal-Difference",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Forward-Reflected-Backward Splitting",
      "resolved_canonical": "Forward-Reflected-Backward Splitting",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Nonconvex Regularization for Feature Selection in Reinforcement Learning

**Korean Title:** 비볼록 정규화 기법을 활용한 강화 학습에서의 특징 선택

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15652.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15652](https://arxiv.org/abs/2509.15652)

## 🔗 유사한 논문
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (82.9% similar)
- [[2025-09-22/Deep Reinforcement Learning with Gradient Eligibility Traces_20250922|Deep Reinforcement Learning with Gradient Eligibility Traces]] (82.5% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (82.1% similar)
- [[2025-09-22/Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations_20250922|Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations]] (82.0% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Machine Learning|Machine Learning]]
**🔗 Specific Connectable**: [[keywords/Feature Selection|Feature Selection]], [[keywords/Least-Squares Temporal-Difference|Least-Squares Temporal-Difference]]
**⚡ Unique Technical**: [[keywords/Nonconvex Regularization|Nonconvex Regularization]], [[keywords/Forward-Reflected-Backward Splitting|Forward-Reflected-Backward Splitting]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15652v1 Announce Type: new 
Abstract: This work proposes an efficient batch algorithm for feature selection in reinforcement learning (RL) with theoretical convergence guarantees. To mitigate the estimation bias inherent in conventional regularization schemes, the first contribution extends policy evaluation within the classical least-squares temporal-difference (LSTD) framework by formulating a Bellman-residual objective regularized with the sparsity-inducing, nonconvex projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC penalty, this formulation can be interpreted as a special instance of a general nonmonotone-inclusion problem. The second contribution establishes novel convergence conditions for the forward-reflected-backward splitting (FRBS) algorithm to solve this class of problems. Numerical experiments on benchmark datasets demonstrate that the proposed approach substantially outperforms state-of-the-art feature-selection methods, particularly in scenarios with many noisy features.

## 🔍 Abstract (한글 번역)

arXiv:2509.15652v1 발표 유형: 신규  
초록: 이 연구는 강화 학습(RL)에서 이론적 수렴 보장을 갖춘 효율적인 배치 알고리즘을 제안합니다. 기존의 정규화 방식에 내재된 추정 편향을 완화하기 위해, 첫 번째 기여는 고전적인 최소자승 시차(LSTD) 프레임워크 내에서 정책 평가를 확장하여 희소성을 유도하는 비볼록 투영 극소 오목(PMC) 패널티로 정규화된 벨만 잔차 목표를 수립합니다. PMC 패널티의 약한 볼록성 덕분에, 이 수식은 일반적인 비단조 포함 문제의 특별한 사례로 해석될 수 있습니다. 두 번째 기여는 이 클래스의 문제를 해결하기 위한 전진-반사-후진 분할(FRBS) 알고리즘의 새로운 수렴 조건을 확립합니다. 벤치마크 데이터셋에 대한 수치 실험은 제안된 접근 방식이 특히 많은 잡음이 있는 특징이 있는 시나리오에서 최신 특징 선택 방법을 상당히 능가함을 보여줍니다.

## 📝 요약

이 연구는 강화 학습에서 효율적인 특성 선택을 위한 배치 알고리즘을 제안하며, 이론적 수렴 보장을 제공합니다. 첫 번째로, 기존의 정규화 방식에서 발생하는 추정 편향을 줄이기 위해, 고전적인 최소자승 시차(LSTD) 프레임워크 내에서 벨만 잔차 목표를 희소성을 유도하는 비볼록 PMC 페널티로 정규화하는 방법을 제시합니다. 두 번째로, 이러한 문제를 해결하기 위한 FRBS 알고리즘의 새로운 수렴 조건을 확립합니다. 벤치마크 데이터셋에 대한 실험 결과, 제안된 방법이 특히 많은 잡음 특성이 있는 상황에서 기존의 최첨단 특성 선택 방법보다 뛰어난 성능을 보였습니다.

## 🎯 주요 포인트

- 1. 강화 학습에서 효율적인 피처 선택을 위한 배치 알고리즘을 제안하며, 이론적 수렴 보장을 제공합니다.
- 2. 기존 정규화 방식의 추정 편향을 줄이기 위해, 비볼록한 PMC 패널티를 사용하여 벨만 잔차 목표를 정규화하는 방법을 제안합니다.
- 3. PMC 패널티의 약한 볼록성 덕분에, 이 방법은 일반적인 비단조 포함 문제의 특수 사례로 해석될 수 있습니다.
- 4. FRBS 알고리즘의 새로운 수렴 조건을 확립하여 이 클래스의 문제를 해결합니다.
- 5. 벤치마크 데이터셋에 대한 수치 실험 결과, 제안된 접근 방식이 특히 많은 노이즈 피처가 있는 시나리오에서 최신 피처 선택 방법을 크게 능가함을 보여줍니다.


---

*Generated on 2025-09-23 10:32:07*