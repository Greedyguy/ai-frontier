---
keywords:
  - Machine Learning
  - Feature Selection
  - Nonconvex Regularization
  - Least-Squares Temporal-Difference
  - Forward-Reflected-Backward Splitting
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15652
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:32:07.369664",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Machine Learning",
    "Feature Selection",
    "Nonconvex Regularization",
    "Least-Squares Temporal-Difference",
    "Forward-Reflected-Backward Splitting"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Machine Learning": 0.8,
    "Feature Selection": 0.75,
    "Nonconvex Regularization": 0.7,
    "Least-Squares Temporal-Difference": 0.8,
    "Forward-Reflected-Backward Splitting": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a key subfield of Machine Learning, providing strong connectivity to related topics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Feature Selection",
        "canonical": "Feature Selection",
        "aliases": [
          "Variable Selection"
        ],
        "category": "specific_connectable",
        "rationale": "Feature Selection is crucial for improving model performance and is widely applicable across various domains.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Nonconvex Regularization",
        "canonical": "Nonconvex Regularization",
        "aliases": [
          "Non-convex Penalty"
        ],
        "category": "unique_technical",
        "rationale": "Nonconvex Regularization is a specialized technique that offers novel approaches to optimization problems.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Least-Squares Temporal-Difference",
        "canonical": "Least-Squares Temporal-Difference",
        "aliases": [
          "LSTD"
        ],
        "category": "specific_connectable",
        "rationale": "LSTD is a specific method in reinforcement learning for policy evaluation, linking to advanced RL techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Forward-Reflected-Backward Splitting",
        "canonical": "Forward-Reflected-Backward Splitting",
        "aliases": [
          "FRBS"
        ],
        "category": "unique_technical",
        "rationale": "FRBS is a novel algorithmic approach, expanding the toolkit for solving nonmonotone-inclusion problems.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "Batch Algorithm",
      "Estimation Bias",
      "Benchmark Datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Feature Selection",
      "resolved_canonical": "Feature Selection",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Nonconvex Regularization",
      "resolved_canonical": "Nonconvex Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Least-Squares Temporal-Difference",
      "resolved_canonical": "Least-Squares Temporal-Difference",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Forward-Reflected-Backward Splitting",
      "resolved_canonical": "Forward-Reflected-Backward Splitting",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Nonconvex Regularization for Feature Selection in Reinforcement Learning

**Korean Title:** ë¹„ë³¼ë¡ ì •ê·œí™” ê¸°ë²•ì„ í™œìš©í•œ ê°•í™” í•™ìŠµì—ì„œì˜ íŠ¹ì§• ì„ íƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15652.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15652](https://arxiv.org/abs/2509.15652)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (82.9% similar)
- [[2025-09-22/Deep Reinforcement Learning with Gradient Eligibility Traces_20250922|Deep Reinforcement Learning with Gradient Eligibility Traces]] (82.5% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (82.1% similar)
- [[2025-09-22/Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations_20250922|Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations]] (82.0% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Machine Learning|Machine Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Feature Selection|Feature Selection]], [[keywords/Least-Squares Temporal-Difference|Least-Squares Temporal-Difference]]
**âš¡ Unique Technical**: [[keywords/Nonconvex Regularization|Nonconvex Regularization]], [[keywords/Forward-Reflected-Backward Splitting|Forward-Reflected-Backward Splitting]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15652v1 Announce Type: new 
Abstract: This work proposes an efficient batch algorithm for feature selection in reinforcement learning (RL) with theoretical convergence guarantees. To mitigate the estimation bias inherent in conventional regularization schemes, the first contribution extends policy evaluation within the classical least-squares temporal-difference (LSTD) framework by formulating a Bellman-residual objective regularized with the sparsity-inducing, nonconvex projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC penalty, this formulation can be interpreted as a special instance of a general nonmonotone-inclusion problem. The second contribution establishes novel convergence conditions for the forward-reflected-backward splitting (FRBS) algorithm to solve this class of problems. Numerical experiments on benchmark datasets demonstrate that the proposed approach substantially outperforms state-of-the-art feature-selection methods, particularly in scenarios with many noisy features.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15652v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì´ ì—°êµ¬ëŠ” ê°•í™” í•™ìŠµ(RL)ì—ì„œ ì´ë¡ ì  ìˆ˜ë ´ ë³´ì¥ì„ ê°–ì¶˜ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì •ê·œí™” ë°©ì‹ì— ë‚´ì¬ëœ ì¶”ì • í¸í–¥ì„ ì™„í™”í•˜ê¸° ìœ„í•´, ì²« ë²ˆì§¸ ê¸°ì—¬ëŠ” ê³ ì „ì ì¸ ìµœì†ŒììŠ¹ ì‹œì°¨(LSTD) í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ ì •ì±… í‰ê°€ë¥¼ í™•ì¥í•˜ì—¬ í¬ì†Œì„±ì„ ìœ ë„í•˜ëŠ” ë¹„ë³¼ë¡ íˆ¬ì˜ ê·¹ì†Œ ì˜¤ëª©(PMC) íŒ¨ë„í‹°ë¡œ ì •ê·œí™”ëœ ë²¨ë§Œ ì”ì°¨ ëª©í‘œë¥¼ ìˆ˜ë¦½í•©ë‹ˆë‹¤. PMC íŒ¨ë„í‹°ì˜ ì•½í•œ ë³¼ë¡ì„± ë•ë¶„ì—, ì´ ìˆ˜ì‹ì€ ì¼ë°˜ì ì¸ ë¹„ë‹¨ì¡° í¬í•¨ ë¬¸ì œì˜ íŠ¹ë³„í•œ ì‚¬ë¡€ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ë²ˆì§¸ ê¸°ì—¬ëŠ” ì´ í´ë˜ìŠ¤ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ì§„-ë°˜ì‚¬-í›„ì§„ ë¶„í• (FRBS) ì•Œê³ ë¦¬ì¦˜ì˜ ìƒˆë¡œìš´ ìˆ˜ë ´ ì¡°ê±´ì„ í™•ë¦½í•©ë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì€ ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì´ íŠ¹íˆ ë§ì€ ì¡ìŒì´ ìˆëŠ” íŠ¹ì§•ì´ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìµœì‹  íŠ¹ì§• ì„ íƒ ë°©ë²•ì„ ìƒë‹¹íˆ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ê°•í™” í•™ìŠµì—ì„œ íš¨ìœ¨ì ì¸ íŠ¹ì„± ì„ íƒì„ ìœ„í•œ ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ì´ë¡ ì  ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ë¡œ, ê¸°ì¡´ì˜ ì •ê·œí™” ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì¶”ì • í¸í–¥ì„ ì¤„ì´ê¸° ìœ„í•´, ê³ ì „ì ì¸ ìµœì†ŒììŠ¹ ì‹œì°¨(LSTD) í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ ë²¨ë§Œ ì”ì°¨ ëª©í‘œë¥¼ í¬ì†Œì„±ì„ ìœ ë„í•˜ëŠ” ë¹„ë³¼ë¡ PMC í˜ë„í‹°ë¡œ ì •ê·œí™”í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ë¡œ, ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ FRBS ì•Œê³ ë¦¬ì¦˜ì˜ ìƒˆë¡œìš´ ìˆ˜ë ´ ì¡°ê±´ì„ í™•ë¦½í•©ë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ íŠ¹íˆ ë§ì€ ì¡ìŒ íŠ¹ì„±ì´ ìˆëŠ” ìƒí™©ì—ì„œ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ íŠ¹ì„± ì„ íƒ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµì—ì„œ íš¨ìœ¨ì ì¸ í”¼ì²˜ ì„ íƒì„ ìœ„í•œ ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ì´ë¡ ì  ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ì •ê·œí™” ë°©ì‹ì˜ ì¶”ì • í¸í–¥ì„ ì¤„ì´ê¸° ìœ„í•´, ë¹„ë³¼ë¡í•œ PMC íŒ¨ë„í‹°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¨ë§Œ ì”ì°¨ ëª©í‘œë¥¼ ì •ê·œí™”í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. PMC íŒ¨ë„í‹°ì˜ ì•½í•œ ë³¼ë¡ì„± ë•ë¶„ì—, ì´ ë°©ë²•ì€ ì¼ë°˜ì ì¸ ë¹„ë‹¨ì¡° í¬í•¨ ë¬¸ì œì˜ íŠ¹ìˆ˜ ì‚¬ë¡€ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. FRBS ì•Œê³ ë¦¬ì¦˜ì˜ ìƒˆë¡œìš´ ìˆ˜ë ´ ì¡°ê±´ì„ í™•ë¦½í•˜ì—¬ ì´ í´ë˜ìŠ¤ì˜ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 5. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì´ íŠ¹íˆ ë§ì€ ë…¸ì´ì¦ˆ í”¼ì²˜ê°€ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìµœì‹  í”¼ì²˜ ì„ íƒ ë°©ë²•ì„ í¬ê²Œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:32:07*