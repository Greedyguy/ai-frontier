---
keywords:
  - Neural Network
  - Influence Function
  - Instruction Fine-Tuning
  - Subset Selection
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2502.09969
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:50:55.848536",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Network",
    "Influence Function",
    "Instruction Fine-Tuning",
    "Subset Selection"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Network": 0.85,
    "Influence Function": 0.8,
    "Instruction Fine-Tuning": 0.82,
    "Subset Selection": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "NN",
          "Neural Nets"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are fundamental to the paper's methodology and connect to a wide range of related concepts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Influence Functions",
        "canonical": "Influence Function",
        "aliases": [
          "Influence Estimation"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's contribution, providing a unique perspective on model training insights.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Instruction Fine-Tuning",
        "canonical": "Instruction Fine-Tuning",
        "aliases": [
          "Instruction Tuning"
        ],
        "category": "specific_connectable",
        "rationale": "A specific technique relevant to the paper's application and broader trends in model training.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Subset Selection",
        "canonical": "Subset Selection",
        "aliases": [
          "Data Subset Selection"
        ],
        "category": "unique_technical",
        "rationale": "A key application of the proposed method, relevant for optimizing training data usage.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "model training",
      "large models",
      "hyperparameter analyses"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Influence Functions",
      "resolved_canonical": "Influence Function",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Instruction Fine-Tuning",
      "resolved_canonical": "Instruction Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Subset Selection",
      "resolved_canonical": "Subset Selection",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data

**Korean Title:** ëª…ë ¹ ì„¸ë¶„í™” ë°ì´í„°ì˜ í•™ìŠµ ê°€ëŠ¥í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì˜í–¥ ì¶”ì •ì„ ìœ„í•œ ì‹ ê²½ë§

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.09969.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2502.09969](https://arxiv.org/abs/2502.09969)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Toward Efficient Influence Function_ Dropout as a Compression Tool_20250922|Toward Efficient Influence Function: Dropout as a Compression Tool]] (83.8% similar)
- [[2025-09-22/Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions_20250922|Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions]] (82.5% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (80.6% similar)
- [[2025-09-22/Mind the Gap_ Data Rewriting for Stable Off-Policy Supervised Fine-Tuning_20250922|Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning]] (80.5% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (80.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Instruction Fine-Tuning|Instruction Fine-Tuning]]
**âš¡ Unique Technical**: [[keywords/Influence Function|Influence Function]], [[keywords/Subset Selection|Subset Selection]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.09969v3 Announce Type: replace-cross 
Abstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.09969v3 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ì˜í–¥ í•¨ìˆ˜ëŠ” ëª¨ë¸ í•™ìŠµì— ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ì€ í° ê³„ì‚° ë¹„ìš©ê³¼ ì œí•œëœ ì¼ë°˜í™” ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ìµœê·¼ ì—°êµ¬ë“¤ì€ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì˜í–¥ì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì§€í‘œì™€ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí–ˆì§€ë§Œ, ì´ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì— ì˜ í™•ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ê³„ì‚°ì„ ìœ„í•œ ë¹„ì‹¼ ìˆœë°©í–¥ ë° ì—­ë°©í–¥ íŒ¨ìŠ¤, ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìƒë‹¹í•œ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜í–¥ ì¶”ì •ì˜ ë‚®ì€ ì¼ë°˜í™” ë•Œë¬¸ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì˜í–¥ ê°’ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ ì‘ì€ ì‹ ê²½ë§(InfluenceNetworkë¼ ë¶€ë¦„)ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ì—¬ ìµœëŒ€ 99%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í‰ê°€ì—ì„œëŠ” ì „ì²´ ì–¸ì–´ ëª¨ë¸ í¬ê¸°ì˜ ë‹¨ 0.0027%ì— ë¶ˆê³¼í•œ ëª¨ë¸ë¡œë„ ì˜í–¥ ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤(7B ë° 8B ë²„ì „ì„ ì‚¬ìš©). ìš°ë¦¬ëŠ” ì¼ë°˜ì ì¸ ì§€ì‹œ ì„¸ë¶€ ì¡°ì •ì„ ìœ„í•œ í•˜ìœ„ ì§‘í•© ì„ íƒì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì˜í–¥ ê°’ ì¶”ì • ì•Œê³ ë¦¬ì¦˜(NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning)ì„ ì ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ì—ì„œëŠ” ë„¤ ê°€ì§€ ìµœì‹  ì˜í–¥ í•¨ìˆ˜ë¥¼ í¬í•¨í•˜ì—¬ NN-CIFTì™€ ì›ë˜ ì˜í–¥ í•¨ìˆ˜ ê°„ì˜ ì„±ëŠ¥ ì €í•˜ ì—†ì´ í° ì†ë„ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” NN-CIFTì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì— ëŒ€í•œ ì½”ë“œëŠ” ë‹¤ìŒì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤: https://github.com/agarwalishika/NN-CIFT.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë°ì´í„°ì˜ ì˜í–¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì •í•˜ê¸° ìœ„í•´ ì‘ì€ ì‹ ê²½ë§ì¸ InfluenceNetworkë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ê³„ì‚° ë¹„ìš©ì´ í¬ê³  ì¼ë°˜í™”ê°€ ì œí•œì ì´ì—ˆìœ¼ë‚˜, ì œì•ˆëœ ë°©ë²•ì€ ëª¨ë¸ í¬ê¸°ì˜ 0.0027%ë§Œìœ¼ë¡œë„ ì˜í–¥ì„ ì¶”ì •í•  ìˆ˜ ìˆì–´ ìµœëŒ€ 99%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. NN-CIFT ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì¼ë°˜ì ì¸ ì§€ì‹œ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ í•˜ìœ„ ì§‘í•© ì„ íƒ ì‘ì—…ì— ì ìš©í–ˆìœ¼ë©°, ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ NN-CIFTì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ê´€ë ¨ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ë°ì´í„° ì˜í–¥ë ¥ ê³„ì‚° ë°©ë²•ì€ í° ê³„ì‚° ë¹„ìš©ê³¼ ì¼ë°˜í™”ì˜ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤.
- 2. InfluenceNetworkë¼ëŠ” ì‘ì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì˜í–¥ë ¥ ê°’ì„ ì¶”ì •í•˜ë©´ ìµœëŒ€ 99%ì˜ ë¹„ìš© ì ˆê°ì´ ê°€ëŠ¥í•˜ë‹¤.
- 3. ì „ì²´ ì–¸ì–´ ëª¨ë¸ í¬ê¸°ì˜ 0.0027%ì— ë¶ˆê³¼í•œ ëª¨ë¸ë¡œë„ ì˜í–¥ë ¥ ê°’ì„ ì •í™•í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.
- 4. NN-CIFT ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì¼ë°˜ ì§€ì¹¨ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ í•˜ìœ„ ì§‘í•© ì„ íƒ ì‘ì—…ì—ì„œ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.
- 5. NN-CIFTì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•˜ë©°, ê´€ë ¨ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µëœë‹¤.


---

*Generated on 2025-09-23 09:50:55*