---
keywords:
  - Large Language Model
  - Activation Space Interventions
  - AI Safety
  - Backdoor Removal
  - Corrupted Capabilities
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2503.04429
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:35:54.287381",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Activation Space Interventions",
    "AI Safety",
    "Backdoor Removal",
    "Corrupted Capabilities"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Activation Space Interventions": 0.78,
    "AI Safety": 0.8,
    "Backdoor Removal": 0.77,
    "Corrupted Capabilities": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large-scale language models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's theme of transferring interventions, linking to a well-established concept.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Activation Space Interventions",
        "canonical": "Activation Space Interventions",
        "aliases": [
          "activation interventions",
          "activation space mapping"
        ],
        "category": "unique_technical",
        "rationale": "A novel concept introduced in the paper, crucial for understanding the transfer mechanism.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "AI Safety Tasks",
        "canonical": "AI Safety",
        "aliases": [
          "safety tasks",
          "AI safety interventions"
        ],
        "category": "specific_connectable",
        "rationale": "Links to broader discussions on AI safety, a key application area for the interventions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Backdoor Removal",
        "canonical": "Backdoor Removal",
        "aliases": [
          "backdoor mitigation",
          "backdoor defense"
        ],
        "category": "specific_connectable",
        "rationale": "A specific task that demonstrates the practical application of the proposed interventions.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Corrupted Capabilities",
        "canonical": "Corrupted Capabilities",
        "aliases": [
          "capability corruption",
          "skill corruption"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new task that tests model robustness, relevant for AI safety discussions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "model",
      "experiment",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Activation Space Interventions",
      "resolved_canonical": "Activation Space Interventions",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "AI Safety Tasks",
      "resolved_canonical": "AI Safety",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Backdoor Removal",
      "resolved_canonical": "Backdoor Removal",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Corrupted Capabilities",
      "resolved_canonical": "Corrupted Capabilities",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Activation Space Interventions Can Be Transferred Between Large Language Models

**Korean Title:** 활성화 공간 개입은 대형 언어 모델 간에 전이될 수 있다.

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.04429.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2503.04429](https://arxiv.org/abs/2503.04429)

## 🔗 유사한 논문
- [[2025-09-22/SABER_ Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection_20250922|SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection]] (84.2% similar)
- [[2025-09-22/Backdoor Mitigation via Invertible Pruning Masks_20250922|Backdoor Mitigation via Invertible Pruning Masks]] (83.9% similar)
- [[2025-09-22/AdaSteer_ Your Aligned LLM is Inherently an Adaptive Jailbreak Defender_20250922|AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender]] (83.4% similar)
- [[2025-09-19/Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (83.4% similar)
- [[2025-09-19/Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment]] (83.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/AI Safety|AI Safety]], [[keywords/Backdoor Removal|Backdoor Removal]]
**⚡ Unique Technical**: [[keywords/Activation Space Interventions|Activation Space Interventions]], [[keywords/Corrupted Capabilities|Corrupted Capabilities]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2503.04429v4 Announce Type: replace 
Abstract: The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches", allowing dynamic toggling between model behaviors.

## 🔍 Abstract (한글 번역)

arXiv:2503.04429v4 발표 유형: 교체  
초록: AI 모델에서의 표현 보편성 연구는 도메인, 모달리티, 아키텍처 전반에 걸쳐 점점 더 많은 수렴을 드러내고 있습니다. 그러나 표현 보편성의 실제 응용은 아직 대부분 탐구되지 않았습니다. 우리는 모델의 공유 활성화 공간을 학습된 매핑을 통해 안전 개입을 모델 간에 전이할 수 있음을 보여줌으로써 이 격차를 해소합니다. 우리는 이 접근법을 두 가지 잘 확립된 AI 안전 작업, 즉 백도어 제거와 유해한 프롬프트 거부에 적용하여 모델의 출력을 예측 가능한 방식으로 변경하는 조정 벡터의 성공적인 전이를 보여줍니다. 또한, 우리는 \textit{손상된 능력}이라는 새로운 작업을 제안하며, 여기서 모델은 백도어와 관련된 지식을 내재화하도록 미세 조정됩니다. 이는 유용한 기술을 백도어와 분리하는 능력을 테스트하여 현실 세계의 도전을 반영합니다. Llama, Qwen 및 Gemma 모델 계열에 걸친 광범위한 실험은 우리의 방법이 더 작은 모델을 사용하여 더 큰 모델을 효율적으로 정렬할 수 있음을 보여줍니다. 또한, 기본 모델과 미세 조정된 모델 간의 오토인코더 매핑이 신뢰할 수 있는 "경량 안전 스위치"로 작용할 수 있음을 보여주며, 모델 행동 간의 동적 전환을 가능하게 합니다.

## 📝 요약

이 논문은 AI 모델의 표현 보편성을 활용하여 안전성 개입을 모델 간에 전이할 수 있음을 보여줍니다. 두 가지 AI 안전성 과제인 백도어 제거와 유해한 프롬프트 거부에서, 모델의 출력 변화를 예측 가능한 방식으로 유도하는 조정 벡터의 성공적인 전이를 입증했습니다. 또한, 백도어와 관련된 지식을 내재화하도록 모델을 미세 조정하는 새로운 과제인 '손상된 능력'을 제안하여, 유용한 기술과 백도어를 분리하는 능력을 테스트합니다. Llama, Qwen, Gemma 모델을 대상으로 한 실험 결과, 작은 모델을 사용해 더 큰 모델을 효율적으로 정렬할 수 있음을 확인했습니다. 또한, 기본 모델과 미세 조정된 모델 간의 오토인코더 매핑이 신뢰할 수 있는 '경량 안전 스위치'로 작용하여 모델 행동을 동적으로 전환할 수 있음을 보여줍니다.

## 🎯 주요 포인트

- 1. AI 모델의 표현 보편성 연구는 도메인, 모달리티, 아키텍처 간의 수렴을 보여주지만, 실제 응용은 아직 탐구되지 않았다.
- 2. 본 연구는 모델 간의 공유 활성화 공간을 통한 학습 매핑을 통해 안전 개입을 전이할 수 있음을 입증하였다.
- 3. 백도어 제거와 유해한 프롬프트 거부라는 AI 안전 과제에서 모델 출력의 예측 가능한 변화를 유도하는 조정 벡터의 성공적인 전이를 보여주었다.
- 4. 새로운 과제인 '손상된 역량'을 제안하여, 백도어와 연결된 지식을 내재화하도록 모델을 미세 조정하고, 유용한 기술과 백도어를 분리하는 능력을 테스트하였다.
- 5. Llama, Qwen, Gemma 모델 패밀리 전반에 걸친 실험을 통해 작은 모델을 사용하여 더 큰 모델을 효율적으로 정렬할 수 있음을 보여주었다.


---

*Generated on 2025-09-23 09:35:54*