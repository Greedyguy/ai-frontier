---
keywords:
  - Transformer
  - Attention Mechanism
  - Hierarchical Attention
  - Multimodal Learning
  - Zero-Shot Learning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15448
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:59:03.571889",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Attention Mechanism",
    "Hierarchical Attention",
    "Multimodal Learning",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Attention Mechanism": 0.9,
    "Hierarchical Attention": 0.78,
    "Multimodal Learning": 0.82,
    "Zero-Shot Learning": 0.83
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformers",
        "canonical": "Transformer",
        "aliases": [
          "Transformers"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the paper's proposed methodology and connect to a wide range of related works.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "attention mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "attention mechanisms"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are a core component of the paper's focus and link to numerous related concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.9
      },
      {
        "surface": "hierarchical attention",
        "canonical": "Hierarchical Attention",
        "aliases": [
          "multi-scale attention"
        ],
        "category": "unique_technical",
        "rationale": "Hierarchical attention is a novel concept introduced in the paper, extending existing attention mechanisms.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "multi-modal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multi-modal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is key to the paper's approach, linking various data modalities in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "zero-shot manner",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot learning is a significant application of the proposed method, enhancing model efficiency.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.83
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "algorithm"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "attention mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "hierarchical attention",
      "resolved_canonical": "Hierarchical Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multi-modal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "zero-shot manner",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.83
      }
    }
  ]
}
-->

# Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems

**Korean Title:** ê³„ì¸µì  ìê¸°-ì–´í…ì…˜: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¬¸ì œì— ëŒ€í•œ ì‹ ê²½ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì¼ë°˜í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15448.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15448](https://arxiv.org/abs/2509.15448)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers]] (82.6% similar)
- [[2025-09-19/Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images]] (82.2% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (81.1% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop: A Novel Regularization Method for Transformer Models]] (81.0% similar)
- [[2025-09-18/Stochastic Clock Attention for Aligning Continuous and Ordered Sequences_20250918|Stochastic Clock Attention for Aligning Continuous and Ordered Sequences]] (80.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Hierarchical Attention|Hierarchical Attention]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15448v1 Announce Type: cross 
Abstract: Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15448v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ê·¸ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì´ì—ˆìŠµë‹ˆë‹¤. ì›ë˜ ì–¸ì–´ ë°ì´í„°ë¥¼ ìœ„í•´ ì œì•ˆë˜ì—ˆì§€ë§Œ, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ê·¸ë˜í”„ ë“± ë‹¤ì–‘í•œ ì‹ í˜¸ ê¸°í•˜í•™ì„ ê°€ì§„ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ë¡œ ë¹ ë¥´ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¤ì¬ë‹¤ëŠ¥í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ì ì¬ì ìœ¼ë¡œ ë‹¤ë¥¸ ê·œëª¨ë¡œ ë°ì´í„°ê°€ ì œê³µë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì¼ë°˜í™”í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ë‚´ì—ì„œ ê³„ì¸µ êµ¬ì¡°ì™€ ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ë ¤ëŠ” ì‹œë„ëŠ” ì£¼ë¡œ ì„ì‹œë°©í¸ì˜ íœ´ë¦¬ìŠ¤í‹±ì— ê¸°ë°˜ì„ ë‘ê³  ìˆìœ¼ë©°, ì´ëŠ” ì ì¬ì ìœ¼ë¡œ ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§„ ìœ ì‚¬í•œ ë¬¸ì œì— ì›í™œí•˜ê²Œ ì¼ë°˜í™”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•©ë‹ˆë‹¤: ìš°ë¦¬ëŠ” ë¨¼ì € ë‹¤ì¤‘ ëª¨ë‹¬, ë‹¤ì¤‘ ê·œëª¨ ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•œ ìˆ˜í•™ì  êµ¬ì„±ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì œì•ˆëœ êµ¬ì„±ì— ëŒ€í•œ ì‹ ê²½ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™”ì˜ ì œ1ì›ë¦¬ì—ì„œ ìˆ˜í•™ì ìœ¼ë¡œ ë„ì¶œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë„ì¶œëœ ê³µì‹ì´ ê³„ì¸µì /ê¸°í•˜í•™ì  ì •ë³´ì—ì„œ ìœ ë˜í•œ ê·€ë‚©ì  í¸í–¥ì„ í†µí•©í•˜ë©´ì„œë„ í‘œì¤€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì£¼ì˜ì— ê°€ì¥ ê°€ê¹Œìš´ ìµœì ì˜ í˜•íƒœì„ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë™ì  í”„ë¡œê·¸ë˜ë°ì— ê¸°ë°˜í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬ ë„ì¶œëœ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©í•¨ìœ¼ë¡œì¨, ì œì•ˆëœ ê³„ì¸µì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ê³„ì¸µì /ë‹¤ì¤‘ ëª¨ë‹¬ ì„¤ì •ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼, í›ˆë ¨ í›„ ê¸°ì¡´ì˜ ì‚¬ì „ í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ê³„ì¸µì  ì •ë³´ë¥¼ ì£¼ì…í•˜ì—¬ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ë” íš¨ìœ¨ì ì¸ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì–‘í•œ ì‹ í˜¸ ê¸°í•˜í•™ì„ ê°€ì§„ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ì— ì ìš©ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê³„ì¸µì  ë° ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹° ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë° í•œê³„ê°€ ìˆì—ˆê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ìë“¤ì€ ë‹¤ì¤‘ ëª¨ë‹¬, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë°ì´í„°ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™” ì›ì¹™ì— ê¸°ë°˜í•˜ì—¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ìˆ˜í•™ì ìœ¼ë¡œ ë„ì¶œí•˜ë©°, ê¸°ì¡´ì˜ Softmax ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ê°€ì¥ ìœ ì‚¬í•˜ë©´ì„œë„ ë¬¸ì œì˜ ê³„ì¸µì /ê¸°í•˜í•™ì  ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ë˜í•œ, ë™ì  í”„ë¡œê·¸ë˜ë°ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬, ì´ ë©”ì»¤ë‹ˆì¦˜ì„ íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©í•¨ìœ¼ë¡œì¨ ê³„ì¸µì /ë‹¤ì¤‘ ëª¨ë‹¬ í™˜ê²½ì—ì„œì˜ ëª¨ë¸ í•™ìŠµ ë° ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ê³„ì¸µì  ì •ë³´ë¥¼ ì£¼ì…í•˜ì—¬ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ì–‘í•œ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ì— ì ìš©ë˜ì—ˆì§€ë§Œ, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë° ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹° ë°ì´í„°ì— ì¼ë°˜í™”í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤.
- 2. ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê³„ì¸µ êµ¬ì¡° ë° ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ëŠ” ë° ìˆì–´ ì„ì‹œ ë°©í¸ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì¤‘ ëª¨ë‹¬, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë°ì´í„°ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ê³ , ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™” ì›ì¹™ì— ê¸°ë°˜í•˜ì—¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ìˆ˜í•™ì ìœ¼ë¡œ ë„ì¶œí•˜ì˜€ë‹¤.
- 4. ë„ì¶œëœ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ í‘œì¤€ Softmax ì£¼ì˜ì™€ ê°€ì¥ ê°€ê¹Œìš°ë©´ì„œë„ ë¬¸ì œì˜ ê³„ì¸µì /ê¸°í•˜í•™ì  ì •ë³´ë¥¼ ë°˜ì˜í•˜ëŠ” ìµœì ì˜ í˜•íƒœì´ë‹¤.
- 5. ì œì•ˆëœ ê³„ì¸µì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê³„ì¸µì /ë‹¤ì¤‘ ëª¨ë‹¬ ì„¤ì •ì—ì„œ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ê±°ë‚˜, ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì— ê³„ì¸µ ì •ë³´ë¥¼ ì£¼ì…í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-23 08:59:03*