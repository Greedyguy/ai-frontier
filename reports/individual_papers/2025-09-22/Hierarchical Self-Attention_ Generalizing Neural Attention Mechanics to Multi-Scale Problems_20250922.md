# Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems

**Korean Title:** ê³„ì¸µì  ìê¸°-ì–´í…ì…˜: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¬¸ì œë¡œì˜ ì‹ ê²½ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì¼ë°˜í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multi-Scale Attention

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Attention Schema-based Attention Control (ASAC)_ A Cognitive-Inspired Approach for Attention Management in Transformers_20250922|Attention Schema-based Attention Control (ASAC) A Cognitive-Inspired Approach for Attention Management in Transformers]] (82.6% similar)
- [[2025-09-19/Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention A Scalable Multilevel Attention Mechanism for Text and Images]] (82.2% similar)
- [[2025-09-18/Stochastic Clock Attention for Aligning Continuous and Ordered Sequences_20250918|Stochastic Clock Attention for Aligning Continuous and Ordered Sequences]] (80.9% similar)
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods Reviving Transformer for Graph Clustering]] (79.5% similar)
- [[2025-09-18/Beyond Marginals_ Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection_20250918|Beyond Marginals Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection]] (78.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15448v1 Announce Type: cross 
Abstract: Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15448v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ê·¸ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë¨¸ì‹  ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì´ì—ˆìŠµë‹ˆë‹¤. ì›ë˜ ì–¸ì–´ ë°ì´í„°ë¥¼ ìœ„í•´ ì œì•ˆë˜ì—ˆì§€ë§Œ, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ê·¸ë˜í”„ ë“± ë‹¤ì–‘í•œ ì‹ í˜¸ ê¸°í•˜í•™ì„ ê°€ì§„ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ë¡œ ë¹ ë¥´ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¤ì¬ë‹¤ëŠ¥í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì ì¬ì ìœ¼ë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ë‹¤ì–‘í•œ ê·œëª¨ë¡œ ë°ì´í„°ê°€ ì œê³µë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì¼ë°˜í™”í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ë‚´ì—ì„œ ê³„ì¸µ êµ¬ì¡°ì™€ ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ë ¤ëŠ” ì‹œë„ëŠ” ì£¼ë¡œ ì„ì‹œë°©í¸ì ì¸ íœ´ë¦¬ìŠ¤í‹±ì— ê¸°ë°˜í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ì ì¬ì ìœ¼ë¡œ ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§„ ìœ ì‚¬í•œ ë¬¸ì œì— ì›í™œí•˜ê²Œ ì¼ë°˜í™”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•©ë‹ˆë‹¤: ìš°ë¦¬ëŠ” ë¨¼ì € ë‹¤ì¤‘ ëª¨ë‹¬, ë‹¤ì¤‘ ê·œëª¨ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ ìˆ˜í•™ì  êµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì œì•ˆëœ êµ¬ì¡°ì— ëŒ€í•œ ì‹ ê²½ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™”ì˜ ì²« ë²ˆì§¸ ì›ë¦¬ì—ì„œ ìˆ˜í•™ì ìœ¼ë¡œ ë„ì¶œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë„ì¶œëœ ê³µì‹ì´ ë¬¸ì œì˜ ê³„ì¸µì /ê¸°í•˜í•™ì  ì •ë³´ì—ì„œ ìœ ë˜í•œ ê·€ë‚©ì  í¸í–¥ì„ í†µí•©í•˜ë©´ì„œ í‘œì¤€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì£¼ì˜ì™€ ê°€ì¥ ê°€ê¹Œìš´ ìµœì ì˜ í˜•íƒœì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë™ì  í”„ë¡œê·¸ë˜ë°ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬ ë„ì¶œëœ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©í•¨ìœ¼ë¡œì¨, ì œì•ˆëœ ê³„ì¸µì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ê³„ì¸µì /ë‹¤ì¤‘ ëª¨ë‹¬ ì„¤ì •ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼, í›ˆë ¨ í›„ ê¸°ì¡´ì˜ ì‚¬ì „ í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ê³„ì¸µì  ì •ë³´ë¥¼ ì£¼ì…í•˜ì—¬ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ë” íš¨ìœ¨ì ì¸ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¤ì–‘í•œ ì‹ í˜¸ ê¸°í•˜í•™ì„ ê°€ì§„ ë‹¤ì¤‘ ëª¨ë‹¬ ë° ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë°ì´í„°ì— ì¼ë°˜í™”í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì´ ì„ì‹œë°©í¸ì ì¸ íœ´ë¦¬ìŠ¤í‹±ì— ì˜ì¡´í•˜ëŠ” ë°˜ë©´, ë³¸ ì—°êµ¬ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë°ì´í„°ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ê³ , ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™” ì›ì¹™ì—ì„œ ì¶œë°œí•˜ì—¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ìˆ˜í•™ì ìœ¼ë¡œ ë„ì¶œí•©ë‹ˆë‹¤. ì´ ë„ì¶œëœ ë©”ì»¤ë‹ˆì¦˜ì€ í‘œì¤€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì£¼ì˜ì™€ ê°€ì¥ ê°€ê¹Œìš°ë©´ì„œë„ ê³„ì¸µì /ê¸°í•˜í•™ì  ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ìµœì ì„ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, ë™ì  í”„ë¡œê·¸ë˜ë°ì„ ê¸°ë°˜ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬, ê³„ì¸µì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©í•¨ìœ¼ë¡œì¨ ê³„ì¸µì /ë‹¤ì¤‘ ëª¨ë‹¬ í™˜ê²½ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ê±°ë‚˜, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì— ê³„ì¸µ ì •ë³´ë¥¼ ì£¼ì…í•˜ì—¬ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ë” íš¨ìœ¨ì ì¸ ëª¨ë¸ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ì–‘í•œ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ì— ì ìš©ë˜ë©°, íŠ¹íˆ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë° ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹° ì‹œë‚˜ë¦¬ì˜¤ì— ì¼ë°˜í™”í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤.

- 2. ê¸°ì¡´ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê³„ì¸µ êµ¬ì¡°ì™€ ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ëŠ” ë° ìˆì–´ ì„ì‹œë°©í¸ì ì¸ ë°©ë²•ì— ì˜ì¡´í•˜ê³  ìˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì¤‘ ëª¨ë‹¬, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ ìˆ˜í•™ì  êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—”íŠ¸ë¡œí”¼ ìµœì†Œí™” ì›ì¹™ì—ì„œ ì¶œë°œí•˜ì—¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì¶œí•˜ì˜€ë‹¤.

- 4. ì œì•ˆëœ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ê³„ì¸µì /ê¸°í•˜í•™ì  ì •ë³´ë¡œë¶€í„° ìœ ë„ëœ ê·€ë‚©ì  í¸í–¥ì„ í¬í•¨í•˜ë©´ì„œë„ í‘œì¤€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— ê°€ì¥ ê·¼ì ‘í•œ ìµœì ì˜ í˜•íƒœë¥¼ ê°€ì§„ë‹¤.

- 5. ë™ì  í”„ë¡œê·¸ë˜ë°ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì œì•ˆëœ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê³„ì‚°í•˜ê³ , ì´ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©í•˜ì—¬ ê³„ì¸µì /ë‹¤ì¤‘ ëª¨ë‹¬ ì„¤ì •ì—ì„œì˜ í•™ìŠµ ë° ê¸°ì¡´ ëª¨ë¸ì— ê³„ì¸µì  ì •ë³´ë¥¼ ì£¼ì…í•˜ëŠ” ë° í™œìš©í•  ìˆ˜ ìˆë‹¤.

---

*Generated on 2025-09-22 13:57:47*