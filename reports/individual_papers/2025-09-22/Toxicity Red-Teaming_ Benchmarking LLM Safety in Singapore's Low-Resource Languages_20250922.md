---
keywords:
  - Large Language Model
  - SGToxicGuard
  - Multilingual Learning
  - Red-Teaming
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15260
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:26:06.022200",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "SGToxicGuard",
    "Multilingual Learning",
    "Red-Teaming"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "SGToxicGuard": 0.85,
    "Multilingual Learning": 0.8,
    "Red-Teaming": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Connects to a broad range of discussions on language model advancements.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.9
      },
      {
        "surface": "SGToxicGuard",
        "canonical": "SGToxicGuard",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a new dataset and framework specific to the paper's focus on safety in multilingual settings.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "multilingual settings",
        "canonical": "Multilingual Learning",
        "aliases": [
          "multilingual environments"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the focus on language diversity, crucial for linking to multilingual AI research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "red-teaming approach",
        "canonical": "Red-Teaming",
        "aliases": [
          "red teaming"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific method for probing vulnerabilities, relevant for safety and security discussions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "sensitive content",
      "real-world scenarios"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "SGToxicGuard",
      "resolved_canonical": "SGToxicGuard",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "multilingual settings",
      "resolved_canonical": "Multilingual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "red-teaming approach",
      "resolved_canonical": "Red-Teaming",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages

**Korean Title:** ë…ì„± ë ˆë“œíŒ€: ì‹±ê°€í¬ë¥´ì˜ ì €ìì› ì–¸ì–´ì—ì„œ LLM ì•ˆì „ì„± ë²¤ì¹˜ë§ˆí‚¹

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15260.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15260](https://arxiv.org/abs/2509.15260)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/SABER_ Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection_20250922|SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection]] (85.2% similar)
- [[2025-09-22/From Judgment to Interference_ Early Stopping LLM Harmful Outputs via Streaming Content Monitoring_20250922|From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring]] (85.1% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (84.7% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (84.7% similar)
- [[2025-09-22/Red Teaming Multimodal Language Models_ Evaluating Harm Across Prompt Modalities and Models_20250922|Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multilingual Learning|Multilingual Learning]]
**âš¡ Unique Technical**: [[keywords/SGToxicGuard|SGToxicGuard]], [[keywords/Red-Teaming|Red-Teaming]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15260v1 Announce Type: new 
Abstract: The advancement of Large Language Models (LLMs) has transformed natural language processing; however, their safety mechanisms remain under-explored in low-resource, multilingual settings. Here, we aim to bridge this gap. In particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation framework for benchmarking LLM safety in Singapore's diverse linguistic context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a red-teaming approach to systematically probe LLM vulnerabilities in three real-world scenarios: \textit{conversation}, \textit{question-answering}, and \textit{content composition}. We conduct extensive experiments with state-of-the-art multilingual LLMs, and the results uncover critical gaps in their safety guardrails. By offering actionable insights into cultural sensitivity and toxicity mitigation, we lay the foundation for safer and more inclusive AI systems in linguistically diverse environments.\footnote{Link to the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.} \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15260v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ë°œì „ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì— í˜ì‹ ì„ ê°€ì ¸ì™”ìœ¼ë‚˜, ì €ìì›, ë‹¤ì–¸ì–´ í™˜ê²½ì—ì„œì˜ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì€ ì—¬ì „íˆ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê³ ì í•©ë‹ˆë‹¤. íŠ¹íˆ, ì‹±ê°€í¬ë¥´ì˜ ë‹¤ì–‘í•œ ì–¸ì–´ì  ë§¥ë½(ì‹±ê¸€ë¦¬ì‹œ, ì¤‘êµ­ì–´, ë§ë ˆì´ì–´, íƒ€ë°€ì–´ í¬í•¨)ì—ì„œ LLM ì•ˆì „ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë° í‰ê°€ í”„ë ˆì„ì›Œí¬ì¸ \textsf{SGToxicGuard}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. SGToxicGuardëŠ” ë ˆë“œ íŒ€ ì ‘ê·¼ ë°©ì‹ì„ ì±„íƒí•˜ì—¬ ì„¸ ê°€ì§€ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì¸ \textit{ëŒ€í™”}, \textit{ì§ˆë¬¸-ì‘ë‹µ}, \textit{ì½˜í…ì¸  êµ¬ì„±}ì—ì„œ LLMì˜ ì·¨ì•½ì ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤. ìµœì²¨ë‹¨ ë‹¤ì–¸ì–´ LLMì„ ì‚¬ìš©í•˜ì—¬ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•œ ê²°ê³¼, ì´ë“¤ì˜ ì•ˆì „ ì¥ì¹˜ì— ì¤‘ìš”í•œ ê²©ì°¨ê°€ ìˆìŒì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. ë¬¸í™”ì  ë¯¼ê°ì„±ê³¼ ë…ì„± ì™„í™”ì— ëŒ€í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ í†µì°°ë ¥ì„ ì œê³µí•¨ìœ¼ë¡œì¨, ì–¸ì–´ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë” ì•ˆì „í•˜ê³  í¬ìš©ì ì¸ AI ì‹œìŠ¤í…œì„ ìœ„í•œ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•˜ê³ ì í•©ë‹ˆë‹¤.\footnote{ë°ì´í„°ì…‹ ë§í¬: https://github.com/Social-AI-Studio/SGToxicGuard.} \textcolor{red}{ë©´ì±… ì¡°í•­: ì´ ë…¼ë¬¸ì—ëŠ” ì¼ë¶€ ë…ìì—ê²Œ ë¶ˆì¾Œê°ì„ ì¤„ ìˆ˜ ìˆëŠ” ë¯¼ê°í•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.}

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì´ ì €ìì› ë‹¤ì–¸ì–´ í™˜ê²½ì—ì„œ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŒì„ ì§€ì í•˜ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‹±ê°€í¬ë¥´ì˜ ë‹¤ì–‘í•œ ì–¸ì–´ì  ë§¥ë½ì„ ë°˜ì˜í•œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ê³¼ í‰ê°€ í”„ë ˆì„ì›Œí¬ì¸ \textsf{SGToxicGuard}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ëŒ€í™”, ì§ˆë¬¸-ì‘ë‹µ, ì½˜í…ì¸  ì‘ì„±ì˜ ì„¸ ê°€ì§€ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ LLMì˜ ì·¨ì•½ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìµœì‹  ë‹¤ì–¸ì–´ LLMì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ ê²°ê³¼, ì´ë“¤ì˜ ì•ˆì „ ì¥ì¹˜ì— ì¤‘ìš”í•œ ê²°í•¨ì´ ìˆìŒì„ ë°œê²¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë¬¸í™”ì  ë¯¼ê°ì„±ê³¼ ë…ì„± ì™„í™”ì— ëŒ€í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ í†µì°°ì„ ì œê³µí•˜ì—¬, ì–¸ì–´ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë” ì•ˆì „í•˜ê³  í¬ìš©ì ì¸ AI ì‹œìŠ¤í…œ ê°œë°œì˜ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì´ ì €ìì›, ë‹¤ì–¸ì–´ í™˜ê²½ì—ì„œ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŒì„ ì§€ì í•˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- 2. ì‹±ê°€í¬ë¥´ì˜ ë‹¤ì–‘í•œ ì–¸ì–´ì  ë§¥ë½ì„ ë°˜ì˜í•œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë° í‰ê°€ í”„ë ˆì„ì›Œí¬ì¸ SGToxicGuardë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.
- 3. SGToxicGuardëŠ” ëŒ€í™”, ì§ˆë¬¸-ì‘ë‹µ, ì½˜í…ì¸  êµ¬ì„±ì˜ ì„¸ ê°€ì§€ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ LLMì˜ ì·¨ì•½ì ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.
- 4. ìµœì‹  ë‹¤ì–¸ì–´ LLMì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ ê²°ê³¼, ì•ˆì „ì„±ì˜ ì¤‘ìš”í•œ ê²°í•¨ì´ ë“œëŸ¬ë‚¬ìœ¼ë©°, ë¬¸í™”ì  ë¯¼ê°ì„±ê³¼ ë…ì„± ì™„í™”ì— ëŒ€í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ì–¸ì–´ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë” ì•ˆì „í•˜ê³  í¬ìš©ì ì¸ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:26:06*