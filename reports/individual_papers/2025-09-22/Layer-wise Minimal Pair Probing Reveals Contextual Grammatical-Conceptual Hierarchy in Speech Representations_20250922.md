---
keywords:
  - Transformer
  - Self-supervised Learning
  - Automatic Speech Recognition
  - Audio Large Language Models
  - Speech Compression
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15655
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:32:20.792880",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Self-supervised Learning",
    "Automatic Speech Recognition",
    "Audio Large Language Models",
    "Speech Compression"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Self-supervised Learning": 0.88,
    "Automatic Speech Recognition": 0.8,
    "Audio Large Language Models": 0.78,
    "Speech Compression": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer-based speech language models",
        "canonical": "Transformer",
        "aliases": [
          "SLMs"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational technology in speech and language processing, linking to various related concepts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "self-supervised learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "S3M"
        ],
        "category": "specific_connectable",
        "rationale": "Self-supervised learning is a key technique in modern speech model training, connecting to other learning paradigms.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "automatic speech recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "specific_connectable",
        "rationale": "ASR is a primary application of speech models, facilitating connections to related speech technologies.",
        "novelty_score": 0.45,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "auditory large language models",
        "canonical": "Audio Large Language Models",
        "aliases": [
          "AudioLLMs"
        ],
        "category": "unique_technical",
        "rationale": "This represents a novel integration of auditory processing with language models, offering unique linking opportunities.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "speech compression",
        "canonical": "Speech Compression",
        "aliases": [
          "codec"
        ],
        "category": "specific_connectable",
        "rationale": "Speech compression is crucial for efficient storage and transmission, linking to codec technologies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "minimal pair designs",
      "diagnostic feature analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer-based speech language models",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "self-supervised learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "automatic speech recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "auditory large language models",
      "resolved_canonical": "Audio Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "speech compression",
      "resolved_canonical": "Speech Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations

**Korean Title:** ì¸µë³„ ìµœì†Œ ìŒ íƒì‚¬ë¥¼ í†µí•´ ìŒì„± í‘œí˜„ì—ì„œ ë§¥ë½ì  ë¬¸ë²•-ê°œë…ì  ê³„ì¸µ êµ¬ì¡°ê°€ ë“œëŸ¬ë‚˜ë‹¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15655.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15655](https://arxiv.org/abs/2509.15655)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (82.3% similar)
- [[2025-09-22/Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment_20250922|Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment]] (82.0% similar)
- [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (81.9% similar)
- [[2025-09-17/Do Large Language Models Understand Word Senses?_20250917|Do Large Language Models Understand Word Senses?]] (81.7% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]], [[keywords/Speech Compression|Speech Compression]]
**âš¡ Unique Technical**: [[keywords/Audio Large Language Models|Audio Large Language Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15655v1 Announce Type: new 
Abstract: Transformer-based speech language models (SLMs) have significantly improved neural speech recognition and understanding. While existing research has examined how well SLMs encode shallow acoustic and phonetic features, the extent to which SLMs encode nuanced syntactic and conceptual features remains unclear. By drawing parallels with linguistic competence assessments for large language models, this study is the first to systematically evaluate the presence of contextual syntactic and semantic features across SLMs for self-supervised learning (S3M), automatic speech recognition (ASR), speech compression (codec), and as the encoder for auditory large language models (AudioLLMs). Through minimal pair designs and diagnostic feature analysis across 71 tasks spanning diverse linguistic levels, our layer-wise and time-resolved analysis uncovers that 1) all speech encode grammatical features more robustly than conceptual ones.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15655v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ìŒì„± ì–¸ì–´ ëª¨ë¸(SLM)ì€ ì‹ ê²½ë§ ê¸°ë°˜ ìŒì„± ì¸ì‹ ë° ì´í•´ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” SLMì´ ì–•ì€ ìŒí–¥ ë° ìŒì„±í•™ì  íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ì˜ ì¸ì½”ë”©í•˜ëŠ”ì§€ ì¡°ì‚¬í–ˆì§€ë§Œ, SLMì´ ë¯¸ë¬˜í•œ êµ¬ë¬¸ì  ë° ê°œë…ì  íŠ¹ì§•ì„ ì–´ëŠ ì •ë„ ì¸ì½”ë”©í•˜ëŠ”ì§€ëŠ” ì—¬ì „íˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì–¸ì–´ì  ëŠ¥ë ¥ í‰ê°€ì™€ì˜ ìœ ì‚¬ì„±ì„ í†µí•´, ë³¸ ì—°êµ¬ëŠ” ìê¸° ì§€ë„ í•™ìŠµ(S3M), ìë™ ìŒì„± ì¸ì‹(ASR), ìŒì„± ì••ì¶•(ì½”ë±), ê·¸ë¦¬ê³  ì²­ê° ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(AudioLLMs)ì˜ ì¸ì½”ë”ë¡œì„œ SLM ì „ë°˜ì— ê±¸ì³ ë¬¸ë§¥ì  êµ¬ë¬¸ ë° ì˜ë¯¸ì  íŠ¹ì§•ì˜ ì¡´ì¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì–¸ì–´ì  ìˆ˜ì¤€ì— ê±¸ì¹œ 71ê°œì˜ ê³¼ì œë¥¼ í†µí•´ ìµœì†Œ ìŒ ì„¤ê³„ ë° ì§„ë‹¨ì  íŠ¹ì§• ë¶„ì„ì„ í†µí•´, ê³„ì¸µë³„ ë° ì‹œê°„ë³„ ë¶„ì„ ê²°ê³¼ 1) ëª¨ë“  ìŒì„±ì€ ê°œë…ì  íŠ¹ì§•ë³´ë‹¤ ë¬¸ë²•ì  íŠ¹ì§•ì„ ë” ê°•ë ¥í•˜ê²Œ ì¸ì½”ë”©í•œë‹¤ëŠ” ê²ƒì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” Transformer ê¸°ë°˜ì˜ ìŒì„± ì–¸ì–´ ëª¨ë¸(SLMs)ì´ ë¬¸ë²•ì  íŠ¹ì§•ì„ ê°œë…ì  íŠ¹ì§•ë³´ë‹¤ ë” ì˜ ì¸ì½”ë”©í•œë‹¤ëŠ” ê²ƒì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” SLMsê°€ ì–•ì€ ìŒí–¥ ë° ìŒì„± íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ì˜ ì¸ì½”ë”©í•˜ëŠ”ì§€ì— ì§‘ì¤‘í–ˆìœ¼ë‚˜, ì´ ì—°êµ¬ëŠ” SLMsê°€ ë¬¸ë§¥ì  êµ¬ë¬¸ ë° ì˜ë¯¸ì  íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ì˜ ì¸ì½”ë”©í•˜ëŠ”ì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤. ìê¸° ì§€ë„ í•™ìŠµ(S3M), ìë™ ìŒì„± ì¸ì‹(ASR), ìŒì„± ì••ì¶•(codec), ì²­ê° ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(AudioLLMs)ì˜ ì¸ì½”ë”ë¡œì„œì˜ ì—­í• ì„ ë¶„ì„í•˜ë©°, 71ê°œì˜ ë‹¤ì–‘í•œ ì–¸ì–´ ìˆ˜ì¤€ì˜ ê³¼ì œë¥¼ í†µí•´ ìµœì†Œ ìŒ ì„¤ê³„ì™€ ì§„ë‹¨ì  íŠ¹ì§• ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ SLMsê°€ ë¬¸ë²•ì  íŠ¹ì§•ì„ ë³´ë‹¤ ê°•ë ¥í•˜ê²Œ ì¸ì½”ë”©í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transformer ê¸°ë°˜ ìŒì„± ì–¸ì–´ ëª¨ë¸(SLMs)ì€ ì‹ ê²½ë§ ê¸°ë°˜ ìŒì„± ì¸ì‹ ë° ì´í•´ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 2. SLMsê°€ ì–•ì€ ìŒí–¥ ë° ìŒì„± íŠ¹ì§•ì„ ì¸ì½”ë”©í•˜ëŠ” ëŠ¥ë ¥ì€ ì—°êµ¬ë˜ì—ˆìœ¼ë‚˜, êµ¬ë¬¸ì  ë° ê°œë…ì  íŠ¹ì§•ì„ ì¸ì½”ë”©í•˜ëŠ” ì •ë„ëŠ” ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 3. ì´ ì—°êµ¬ëŠ” SLMsê°€ ë¬¸ë§¥ì  êµ¬ë¬¸ ë° ì˜ë¯¸ì  íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ì˜ ì¸ì½”ë”©í•˜ëŠ”ì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤.
- 4. 71ê°œì˜ ë‹¤ì–‘í•œ ì–¸ì–´ ìˆ˜ì¤€ì˜ ê³¼ì œë¥¼ í†µí•´ ìµœì†Œ ìŒ ì„¤ê³„ ë° ì§„ë‹¨ íŠ¹ì§• ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
- 5. ë¶„ì„ ê²°ê³¼, ëª¨ë“  ìŒì„± ëª¨ë¸ì€ ê°œë…ì  íŠ¹ì§•ë³´ë‹¤ ë¬¸ë²•ì  íŠ¹ì§•ì„ ë” ê°•ë ¥í•˜ê²Œ ì¸ì½”ë”©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:32:20*