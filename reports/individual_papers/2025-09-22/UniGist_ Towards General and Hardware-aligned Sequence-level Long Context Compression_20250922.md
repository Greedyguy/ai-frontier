---
keywords:
  - Large Language Model
  - Sequence-level Compression
  - Long-Range Dependency Modeling
  - Memory Overhead
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15763
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:34:08.118382",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Sequence-level Compression",
    "Long-Range Dependency Modeling",
    "Memory Overhead"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Sequence-level Compression": 0.7,
    "Long-Range Dependency Modeling": 0.8,
    "Memory Overhead": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's context and link to many related concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sequence-level Compression",
        "canonical": "Sequence-level Compression",
        "aliases": [
          "Sequence Compression",
          "Token Compression"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique technique introduced in the paper, critical for understanding the proposed framework.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Long-Range Dependency Modeling",
        "canonical": "Long-Range Dependency Modeling",
        "aliases": [
          "Dependency Modeling",
          "Long-Range Modeling"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is crucial for understanding the impact of the compression method on model performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Memory Overhead",
        "canonical": "Memory Overhead",
        "aliases": [
          "Memory Usage",
          "Memory Constraints"
        ],
        "category": "unique_technical",
        "rationale": "Memory overhead is a key challenge addressed by the paper's proposed solution.",
        "novelty_score": 0.65,
        "connectivity_score": 0.65,
        "specificity_score": 0.68,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sequence-level Compression",
      "resolved_canonical": "Sequence-level Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Long-Range Dependency Modeling",
      "resolved_canonical": "Long-Range Dependency Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Memory Overhead",
      "resolved_canonical": "Memory Overhead",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.65,
        "specificity": 0.68,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression

**Korean Title:** UniGist: ì¼ë°˜ì ì´ê³  í•˜ë“œì›¨ì–´ì— ë§ì¶˜ ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ê¸´ ë¬¸ë§¥ ì••ì¶•ì„ í–¥í•˜ì—¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15763.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15763](https://arxiv.org/abs/2509.15763)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (84.9% similar)
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (83.6% similar)
- [[2025-09-17/Dense Video Understanding with Gated Residual Tokenization_20250917|Dense Video Understanding with Gated Residual Tokenization]] (80.4% similar)
- [[2025-09-22/CORE-RAG_ Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning_20250922|CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning]] (80.1% similar)
- [[2025-09-22/LiMuon_ Light and Fast Muon Optimizer for Large Models_20250922|LiMuon: Light and Fast Muon Optimizer for Large Models]] (79.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Long-Range Dependency Modeling|Long-Range Dependency Modeling]]
**âš¡ Unique Technical**: [[keywords/Sequence-level Compression|Sequence-level Compression]], [[keywords/Memory Overhead|Memory Overhead]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15763v1 Announce Type: new 
Abstract: Large language models are increasingly capable of handling long-context inputs, but the memory overhead of key-value (KV) cache remains a major bottleneck for general-purpose deployment. While various compression strategies have been explored, sequence-level compression, which drops the full KV caches for certain tokens, is particularly challenging as it can lead to the loss of important contextual information. To address this, we introduce UniGist, a sequence-level long-context compression framework that efficiently preserves context information by replacing raw tokens with special compression tokens (gists) in a fine-grained manner. We adopt a chunk-free training strategy and design an efficient kernel with a gist shift trick, enabling optimized GPU training. Our scheme also supports flexible inference by allowing the actual removal of compressed tokens, resulting in real-time memory savings. Experiments across multiple long-context tasks demonstrate that UniGist significantly improves compression quality, with especially strong performance in detail-recalling tasks and long-range dependency modeling.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15763v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì ì  ë” ê¸´ ë¬¸ë§¥ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆì§€ë§Œ, í‚¤-ê°’(KV) ìºì‹œì˜ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œëŠ” ë²”ìš© ë°°í¬ì˜ ì£¼ìš” ë³‘ëª© í˜„ìƒìœ¼ë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì••ì¶• ì „ëµì´ íƒêµ¬ë˜ì–´ ì™”ì§€ë§Œ, íŠ¹ì • í† í°ì— ëŒ€í•´ ì „ì²´ KV ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì••ì¶•ì€ ì¤‘ìš”í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ìƒì„ ìˆ˜ ìˆì–´ íŠ¹íˆ ë„ì „ì ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ì›ì‹œ í† í°ì„ íŠ¹ë³„í•œ ì••ì¶• í† í°(ìš”ì•½)ìœ¼ë¡œ ì„¸ë°€í•˜ê²Œ ëŒ€ì²´í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ì¡´í•˜ëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì¥ë¬¸ë§¥ ì••ì¶• í”„ë ˆì„ì›Œí¬ì¸ UniGistë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì²­í¬ ì—†ëŠ” í›ˆë ¨ ì „ëµì„ ì±„íƒí•˜ê³  ìš”ì•½ ì´ë™ íŠ¸ë¦­ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì»¤ë„ì„ ì„¤ê³„í•˜ì—¬ GPU í›ˆë ¨ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ìŠ¤í‚´ì€ ì••ì¶•ëœ í† í°ì˜ ì‹¤ì œ ì œê±°ë¥¼ í—ˆìš©í•¨ìœ¼ë¡œì¨ ìœ ì—°í•œ ì¶”ë¡ ì„ ì§€ì›í•˜ì—¬ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì ˆê°ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì¥ë¬¸ë§¥ ì‘ì—…ì— ëŒ€í•œ ì‹¤í—˜ì€ UniGistê°€ ì••ì¶• í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, íŠ¹íˆ ì„¸ë¶€ ì‚¬í•­ íšŒìƒ ì‘ì—…ê³¼ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ê¸´ ë¬¸ë§¥ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ UniGistë¼ëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. UniGistëŠ” ì¤‘ìš”í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì›ë˜ì˜ í† í°ì„ íŠ¹ìˆ˜ ì••ì¶• í† í°ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì²­í¬ ì—†ëŠ” í•™ìŠµ ì „ëµê³¼ íš¨ìœ¨ì ì¸ GPU í•™ìŠµì„ ìœ„í•œ ì»¤ë„ ì„¤ê³„ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, UniGistëŠ” ê¸´ ë¬¸ë§¥ ì‘ì—…ì—ì„œ íŠ¹íˆ ì„¸ë¶€ ì‚¬í•­ íšŒìƒê³¼ ì¥ê¸° ì˜ì¡´ì„± ëª¨ë¸ë§ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í‚¤-ê°’(KV) ìºì‹œ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œëŠ” ì¼ë°˜ì ì¸ ë°°í¬ì˜ ì£¼ìš” ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤.
- 2. UniGistëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ê¸´ ë§¥ë½ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¡œ, ì›ì‹œ í† í°ì„ íŠ¹ìˆ˜ ì••ì¶• í† í°(ìš”ì•½)ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ë§¥ë½ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ì¡´í•©ë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” ì²­í¬ ì—†ëŠ” í›ˆë ¨ ì „ëµê³¼ ìš”ì•½ ì´ë™ íŠ¸ë¦­ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì»¤ë„ì„ ì„¤ê³„í•˜ì—¬ GPU í›ˆë ¨ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.
- 4. UniGistëŠ” ì••ì¶•ëœ í† í°ì˜ ì‹¤ì œ ì œê±°ë¥¼ í—ˆìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 5. ë‹¤ì–‘í•œ ê¸´ ë§¥ë½ ì‘ì—… ì‹¤í—˜ì—ì„œ UniGistëŠ” íŠ¹íˆ ì„¸ë¶€ ì‚¬í•­ íšŒìƒ ì‘ì—…ê³¼ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§ì—ì„œ ì••ì¶• í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:34:08*