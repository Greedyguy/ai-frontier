---
keywords:
  - Influence Function
  - Dropout
  - Gradient Compression
  - Machine Learning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15651
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:08:12.567616",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Influence Function",
    "Dropout",
    "Gradient Compression",
    "Machine Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Influence Function": 0.8,
    "Dropout": 0.75,
    "Gradient Compression": 0.7,
    "Machine Learning": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Influence Function",
        "canonical": "Influence Function",
        "aliases": [
          "Influence Functions"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's contribution, offering a novel approach to compute it efficiently.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dropout",
        "canonical": "Dropout",
        "aliases": [
          "Dropout Technique"
        ],
        "category": "specific_connectable",
        "rationale": "Used as a compression tool in the paper, linking it to its broader applications in neural networks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Gradient Compression",
        "canonical": "Gradient Compression",
        "aliases": [
          "Compressed Gradients"
        ],
        "category": "unique_technical",
        "rationale": "A novel application in the paper, crucial for reducing computational overhead.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Machine Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "ML"
        ],
        "category": "broad_technical",
        "rationale": "The foundational field for the paper's context, connecting various technical aspects.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      }
    ],
    "ban_list_suggestions": [
      "training data",
      "model's performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Influence Function",
      "resolved_canonical": "Influence Function",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dropout",
      "resolved_canonical": "Dropout",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Gradient Compression",
      "resolved_canonical": "Gradient Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Machine Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    }
  ]
}
-->

# Toward Efficient Influence Function: Dropout as a Compression Tool

**Korean Title:** íš¨ìœ¨ì ì¸ ì˜í–¥ í•¨ìˆ˜ë¡œ: ì••ì¶• ë„êµ¬ë¡œì„œì˜ ë“œë¡­ì•„ì›ƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15651.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15651](https://arxiv.org/abs/2509.15651)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data_20250922|Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data]] (83.8% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (81.9% similar)
- [[2025-09-22/Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions_20250922|Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions]] (80.8% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (79.7% similar)
- [[2025-09-22/Estimating Model Performance Under Covariate Shift Without Labels_20250922|Estimating Model Performance Under Covariate Shift Without Labels]] (79.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Machine Learning|Machine Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Dropout|Dropout]]
**âš¡ Unique Technical**: [[keywords/Influence Function|Influence Function]], [[keywords/Gradient Compression|Gradient Compression]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15651v1 Announce Type: cross 
Abstract: Assessing the impact the training data on machine learning models is crucial for understanding the behavior of the model, enhancing the transparency, and selecting training data. Influence function provides a theoretical framework for quantifying the effect of training data points on model's performance given a specific test data. However, the computational and memory costs of influence function presents significant challenges, especially for large-scale models, even when using approximation methods, since the gradients involved in computation are as large as the model itself. In this work, we introduce a novel approach that leverages dropout as a gradient compression mechanism to compute the influence function more efficiently. Our method significantly reduces computational and memory overhead, not only during the influence function computation but also in gradient compression process. Through theoretical analysis and empirical validation, we demonstrate that our method could preserves critical components of the data influence and enables its application to modern large-scale models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15651v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°ì˜ ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•˜ê³ , íˆ¬ëª…ì„±ì„ í–¥ìƒì‹œí‚¤ë©°, í›ˆë ¨ ë°ì´í„°ë¥¼ ì„ íƒí•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜í–¥ í•¨ìˆ˜ëŠ” íŠ¹ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í›ˆë ¨ ë°ì´í„° í¬ì¸íŠ¸ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•œ ì´ë¡ ì  í‹€ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì˜í–¥ í•¨ìˆ˜ì˜ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ì€ íŠ¹íˆ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ê²½ìš° ìƒë‹¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê·¼ì‚¬ ë°©ë²•ì„ ì‚¬ìš©í•˜ë”ë¼ë„ ê³„ì‚°ì— ê´€ë ¨ëœ ê¸°ìš¸ê¸°ê°€ ëª¨ë¸ ìì²´ë§Œí¼ í¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒì„ ê¸°ìš¸ê¸° ì••ì¶• ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì˜í–¥ í•¨ìˆ˜ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ì˜í–¥ í•¨ìˆ˜ ê³„ì‚°ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ìš¸ê¸° ì••ì¶• ê³¼ì •ì—ì„œë„ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤ì¦ì  ê²€ì¦ì„ í†µí•´, ìš°ë¦¬ì˜ ë°©ë²•ì´ ë°ì´í„° ì˜í–¥ì˜ ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ë³´ì¡´í•˜ê³  í˜„ëŒ€ì˜ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì˜í–¥ í•¨ìˆ˜ëŠ” ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ì´ ë†’ì•„ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš©í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ë“œë¡­ì•„ì›ƒì„ í™œìš©í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ì—¬ ì˜í–¥ í•¨ìˆ˜ ê³„ì‚°ì˜ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì´ê³ , ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œë„ ë°ì´í„° ì˜í–¥ì˜ í•µì‹¬ ìš”ì†Œë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆìŒì„ ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤í—˜ì„ í†µí•´ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ í–‰ë™ ì´í•´, íˆ¬ëª…ì„± í–¥ìƒ, í›ˆë ¨ ë°ì´í„° ì„ íƒì— ì¤‘ìš”í•˜ë‹¤.
- 2. ì˜í–¥ í•¨ìˆ˜ëŠ” íŠ¹ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ í›ˆë ¨ ë°ì´í„°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”í•˜ëŠ” ì´ë¡ ì  í‹€ì„ ì œê³µí•œë‹¤.
- 3. ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œ ì˜í–¥ í•¨ìˆ˜ì˜ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ì€ ìƒë‹¹í•œ ë„ì „ ê³¼ì œê°€ ë˜ë©°, ì´ëŠ” ê·¼ì‚¬ ë°©ë²•ì„ ì‚¬ìš©í•˜ë”ë¼ë„ ë§ˆì°¬ê°€ì§€ì´ë‹¤.
- 4. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒì„ í™œìš©í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•˜ì—¬ ì˜í–¥ í•¨ìˆ˜ë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê³ , ê³„ì‚° ë° ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ í¬ê²Œ ì¤„ì˜€ë‹¤.
- 5. ì´ ë°©ë²•ì€ ë°ì´í„° ì˜í–¥ì˜ ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ë³´ì¡´í•˜ë©´ì„œ í˜„ëŒ€ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•¨ì„ ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤ì¦ì  ê²€ì¦ì„ í†µí•´ ì…ì¦í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-23 09:08:12*