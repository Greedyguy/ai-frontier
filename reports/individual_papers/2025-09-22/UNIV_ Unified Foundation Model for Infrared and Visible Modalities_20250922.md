---
keywords:
  - Cross-modality Contrastive Learning
  - Transformer
  - Knowledge Preservation
  - Multimodal Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15642
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:04:06.791192",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Cross-modality Contrastive Learning",
    "Transformer",
    "Knowledge Preservation",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Cross-modality Contrastive Learning": 0.78,
    "Transformer": 0.8,
    "Knowledge Preservation": 0.72,
    "Multimodal Learning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Patch-wise Cross-modality Contrastive Learning",
        "canonical": "Cross-modality Contrastive Learning",
        "aliases": [
          "PCCL"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a novel approach for aligning features across modalities, enhancing multimodal learning capabilities.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer-based architecture",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational architecture in deep learning, relevant to the proposed model's compatibility.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "dual-knowledge preservation mechanism",
        "canonical": "Knowledge Preservation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This mechanism is crucial for maintaining performance across modalities, reflecting advanced model design.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Multimodal Learning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "The paper's focus on integrating infrared and visible data highlights the importance of multimodal approaches.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "autonomous vehicles",
      "weather conditions",
      "baseline performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Patch-wise Cross-modality Contrastive Learning",
      "resolved_canonical": "Cross-modality Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer-based architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "dual-knowledge preservation mechanism",
      "resolved_canonical": "Knowledge Preservation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Multimodal Learning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# UNIV: Unified Foundation Model for Infrared and Visible Modalities

**Korean Title:** UNIV: ì ì™¸ì„  ë° ê°€ì‹œê´‘ì„  ëª¨ë‹¬ë¦¬í‹°ë¥¼ ìœ„í•œ í†µí•© ê¸°ì´ˆ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15642.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15642](https://arxiv.org/abs/2509.15642)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (84.2% similar)
- [[2025-09-22/UniMRSeg_ Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation_20250922|UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation]] (83.2% similar)
- [[2025-09-18/UniPLV_ Towards Label-Efficient Open-World 3D Scene Understanding by Regional Visual Language Supervision_20250918|UniPLV: Towards Label-Efficient Open-World 3D Scene Understanding by Regional Visual Language Supervision]] (82.5% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (81.5% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (81.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Cross-modality Contrastive Learning|Cross-modality Contrastive Learning]], [[keywords/Knowledge Preservation|Knowledge Preservation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15642v1 Announce Type: new 
Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly, particularly to achieve robust performance under diverse weather conditions. Although pre-trained models for RGB-visible and infrared data excel in their respective domains, they often underperform in multimodal scenarios, such as autonomous vehicles equipped with both sensors. To address this challenge, we propose a biologically inspired UNified foundation model for Infrared and Visible modalities (UNIV), featuring two key innovations. First, we introduce Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided distillation framework that mimics retinal horizontal cells' lateral inhibition, which enables effective cross-modal feature alignment while remaining compatible with any transformer-based architecture. Second, our dual-knowledge preservation mechanism emulates the retina's bipolar cell signal routing - combining LoRA adapters (2% added parameters) with synchronous distillation to prevent catastrophic forgetting, thereby replicating the retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To support cross-modal learning, we introduce the MVIP dataset, the most comprehensive visible-infrared benchmark to date. It contains 98,992 precisely aligned image pairs spanning diverse scenarios. Extensive experiments demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+ of the baseline performance on visible RGB tasks. Our code is available at https://github.com/fangyuanmao/UNIV.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15642v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: RGB-ê°€ì‹œê´‘ì„ ê³¼ ì ì™¸ì„ ì˜ ê²°í•© ì¸ì‹ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ ë‹¤ì–‘í•œ ê¸°ìƒ ì¡°ê±´ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•©ë‹ˆë‹¤. RGB-ê°€ì‹œê´‘ì„ ê³¼ ì ì™¸ì„  ë°ì´í„°ë¥¼ ìœ„í•œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ ê°ìì˜ ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë‘ ì„¼ì„œë¥¼ ì¥ì°©í•œ ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ê³¼ ê°™ì€ ë‹¤ì¤‘ ëª¨ë‹¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ì¢…ì¢… ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì ì™¸ì„  ë° ê°€ì‹œê´‘ì„  ëª¨ë‹¬ë¦¬í‹°ë¥¼ ìœ„í•œ ìƒë¬¼í•™ì ìœ¼ë¡œ ì˜ê°ì„ ë°›ì€ í†µí•© ê¸°ì´ˆ ëª¨ë¸(UNIV)ì„ ì œì•ˆí•˜ë©°, ë‘ ê°€ì§€ ì£¼ìš” í˜ì‹ ì„ íŠ¹ì§•ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì²«ì§¸, ìš°ë¦¬ëŠ” íŒ¨ì¹˜ ê¸°ë°˜ êµì°¨ ëª¨ë‹¬ ëŒ€ì¡° í•™ìŠµ(PCCL)ì„ ë„ì…í•©ë‹ˆë‹¤. ì´ëŠ” ë§ë§‰ì˜ ìˆ˜í‰ ì„¸í¬ì˜ ì¸¡ë©´ ì–µì œë¥¼ ëª¨ë°©í•œ ì£¼ì˜ ê¸°ë°˜ ì¦ë¥˜ í”„ë ˆì„ì›Œí¬ë¡œ, ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ì™€ í˜¸í™˜ë˜ë©´ì„œ íš¨ê³¼ì ì¸ êµì°¨ ëª¨ë‹¬ íŠ¹ì§• ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ìš°ë¦¬ì˜ ì´ì¤‘ ì§€ì‹ ë³´ì¡´ ë©”ì»¤ë‹ˆì¦˜ì€ ë§ë§‰ì˜ ì–‘ê·¹ ì„¸í¬ ì‹ í˜¸ ê²½ë¡œë¥¼ ëª¨ë°©í•˜ì—¬ LoRA ì–´ëŒ‘í„°(2% ì¶”ê°€ ë§¤ê°œë³€ìˆ˜)ì™€ ë™ê¸°í™” ì¦ë¥˜ë¥¼ ê²°í•©í•˜ì—¬ íŒŒêµ­ì  ë§ê°ì„ ë°©ì§€í•˜ë©°, ë§ë§‰ì˜ ëª…ì‹œì (ì›ì¶”ì„¸í¬ ì£¼ë„) ë° ì•”ì‹œì (ë§‰ëŒ€ì„¸í¬ ì£¼ë„) ê¸°ëŠ¥ì„ ì¬í˜„í•©ë‹ˆë‹¤. êµì°¨ ëª¨ë‹¬ í•™ìŠµì„ ì§€ì›í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í˜„ì¬ê¹Œì§€ ê°€ì¥ í¬ê´„ì ì¸ ê°€ì‹œê´‘ì„ -ì ì™¸ì„  ë²¤ì¹˜ë§ˆí¬ì¸ MVIP ë°ì´í„°ì…‹ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ê±¸ì³ ì •í™•íˆ ì •ë ¬ëœ 98,992ê°œì˜ ì´ë¯¸ì§€ ìŒì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ UNIVê°€ ì ì™¸ì„  ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤(ì˜ë¯¸ë¡ ì  ë¶„í• ì—ì„œ +1.7 mIoU, ê°ì²´ íƒì§€ì—ì„œ +0.7 mAP)í•˜ë©°, ê°€ì‹œ RGB ì‘ì—…ì—ì„œ ê¸°ì¤€ ì„±ëŠ¥ì˜ 99% ì´ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/fangyuanmao/UNIVì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì–‘í•œ ê¸°ìƒ ì¡°ê±´ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ê¸° ìœ„í•´ RGB-ê°€ì‹œê´‘ì„ ê³¼ ì ì™¸ì„  ë°ì´í„°ë¥¼ ê²°í•©í•œ ì¸ì‹ì˜ í•„ìš”ì„±ì´ ì¦ê°€í•˜ëŠ” ìƒí™©ì—ì„œ, ë‘ ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” UNIV ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” ë‘ ê°€ì§€ í˜ì‹ ì ì¸ ë°©ë²•ë¡ ì´ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, Patch-wise Cross-modality Contrastive Learning(PCCL)ì€ ë§ë§‰ì˜ ìˆ˜í‰ ì„¸í¬ ì–µì œ ë©”ì»¤ë‹ˆì¦˜ì„ ëª¨ë°©í•˜ì—¬ êµì°¨ ëª¨ë‹¬ í”¼ì²˜ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ë“€ì–¼ ì§€ì‹ ë³´ì¡´ ë©”ì»¤ë‹ˆì¦˜ì€ ë§ë§‰ì˜ ì‹ í˜¸ ê²½ë¡œë¥¼ ëª¨ë°©í•˜ì—¬ LoRA ì–´ëŒ‘í„°ì™€ ë™ê¸° ì¦ë¥˜ë¥¼ ê²°í•©, ë§ê°ì„ ë°©ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬í•¨í•œ 98,992ê°œì˜ ì´ë¯¸ì§€ ìŒì„ ì œê³µí•˜ëŠ” MVIP ë°ì´í„°ì…‹ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, UNIVëŠ” ì ì™¸ì„  ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, RGB ì‘ì—…ì—ì„œë„ 99% ì´ìƒì˜ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RGB-ê°€ì‹œê´‘ì„ ê³¼ ì ì™¸ì„  ë°ì´í„°ë¥¼ ê²°í•©í•œ ì¸ì‹ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ ë‹¤ì–‘í•œ ë‚ ì”¨ ì¡°ê±´ì—ì„œì˜ ê²¬ê³ í•œ ì„±ëŠ¥ì„ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. UNIV ëª¨ë¸ì€ Patch-wise Cross-modality Contrastive Learning (PCCL)ì„ ë„ì…í•˜ì—¬ íš¨ê³¼ì ì¸ êµì°¨ ëª¨ë‹¬ íŠ¹ì„± ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. ì´ ëª¨ë¸ì€ LoRA ì–´ëŒ‘í„°ì™€ ë™ê¸° ì¦ë¥˜ë¥¼ ê²°í•©í•œ ì´ì¤‘ ì§€ì‹ ë³´ì¡´ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë§ê°ì„ ë°©ì§€í•˜ê³ , ë§ë§‰ì˜ ê¸°ëŠ¥ì„ ëª¨ë°©í•©ë‹ˆë‹¤.
- 4. MVIP ë°ì´í„°ì…‹ì€ ê°€ì¥ í¬ê´„ì ì¸ ê°€ì‹œê´‘ì„ -ì ì™¸ì„  ë²¤ì¹˜ë§ˆí¬ë¡œ, ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì•„ìš°ë¥´ëŠ” 98,992ê°œì˜ ì •ë°€í•˜ê²Œ ì •ë ¬ëœ ì´ë¯¸ì§€ ìŒì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 5. UNIV ëª¨ë¸ì€ ì ì™¸ì„  ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ê°€ì‹œê´‘ì„  RGB ì‘ì—…ì—ì„œë„ 99% ì´ìƒì˜ ê¸°ì¤€ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:04:06*