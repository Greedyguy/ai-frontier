---
keywords:
  - Vision-Language Model
  - Object Recognition in Incongruous Context Benchmark
  - LLM-guided sampling
  - CLIP-guided sampling
  - contextual incongruity
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15695
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:51:44.628683",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Object Recognition in Incongruous Context Benchmark",
    "LLM-guided sampling",
    "CLIP-guided sampling",
    "contextual incongruity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Object Recognition in Incongruous Context Benchmark": 0.78,
    "LLM-guided sampling": 0.72,
    "CLIP-guided sampling": 0.74,
    "contextual incongruity": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on integrating visual and textual information, making it a key concept for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Object Recognition in Incongruous Context Benchmark",
        "canonical": "Object Recognition in Incongruous Context Benchmark",
        "aliases": [
          "ORIC"
        ],
        "category": "unique_technical",
        "rationale": "ORIC is a novel benchmark introduced in the paper, crucial for understanding the evaluation of LVLMs in unexpected contexts.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "LLM-guided sampling",
        "canonical": "LLM-guided sampling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This method is specific to the paper's approach to identifying contextually incongruous objects, offering unique insights.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "CLIP-guided sampling",
        "canonical": "CLIP-guided sampling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "CLIP-guided sampling is a distinct technique used in the paper to detect hallucinated objects, highlighting its innovative approach.",
        "novelty_score": 0.8,
        "connectivity_score": 0.62,
        "specificity_score": 0.8,
        "link_intent_score": 0.74
      },
      {
        "surface": "contextual incongruity",
        "canonical": "contextual incongruity",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Understanding contextual incongruity is essential for linking the paper's discussion on recognition failures in LVLMs.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "errors",
      "recognition failures",
      "context"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Object Recognition in Incongruous Context Benchmark",
      "resolved_canonical": "Object Recognition in Incongruous Context Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LLM-guided sampling",
      "resolved_canonical": "LLM-guided sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "CLIP-guided sampling",
      "resolved_canonical": "CLIP-guided sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.62,
        "specificity": 0.8,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "contextual incongruity",
      "resolved_canonical": "contextual incongruity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models

**Korean Title:** ORIC: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ë¶€ì¡°í™” ë§¥ë½ì—ì„œì˜ ê°ì²´ ì¸ì‹ ë²¤ì¹˜ë§ˆí‚¹

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15695.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15695](https://arxiv.org/abs/2509.15695)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (88.7% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (82.5% similar)
- [[2025-09-18/The Art of Saying "Maybe"_ A Conformal Lens for Uncertainty Benchmarking in VLMs_20250918|The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs]] (81.5% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (81.5% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/contextual incongruity|contextual incongruity]]
**âš¡ Unique Technical**: [[keywords/Object Recognition in Incongruous Context Benchmark|Object Recognition in Incongruous Context Benchmark]], [[keywords/LLM-guided sampling|LLM-guided sampling]], [[keywords/CLIP-guided sampling|CLIP-guided sampling]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15695v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image caption, visual question answering, and robotics by integrating visual and textual information. However, they remain prone to errors in incongruous contexts, where objects appear unexpectedly or are absent when contextually expected. This leads to two key recognition failures: object misidentification and hallucination. To systematically examine this issue, we introduce the Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark that evaluates LVLMs in scenarios where object-context relationships deviate from expectations. ORIC employs two key strategies: (1) LLM-guided sampling, which identifies objects that are present but contextually incongruous, and (2) CLIP-guided sampling, which detects plausible yet nonexistent objects that are likely to be hallucinated, thereby creating an incongruous context. Evaluating 18 LVLMs and two open-vocabulary detection models, our results reveal significant recognition gaps, underscoring the challenges posed by contextual incongruity. This work provides critical insights into LVLMs' limitations and encourages further research on context-aware object recognition.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15695v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì´ë¯¸ì§€ ìº¡ì…˜, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ ë° ë¡œë´‡ ê³µí•™ ë¶„ì•¼ì—ì„œ ìƒë‹¹í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ëª¨ë¸ì€ ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì—ì„œ ê°ì²´ê°€ ë‚˜íƒ€ë‚˜ê±°ë‚˜ ë§¥ë½ìƒ ì˜ˆìƒë˜ëŠ” ê°ì²´ê°€ ì—†ëŠ” ê²½ìš°ì™€ ê°™ì€ ë¶ˆì¼ì¹˜í•œ ë§¥ë½ì—ì„œ ì˜¤ë¥˜ì— ì·¨ì•½í•©ë‹ˆë‹¤. ì´ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì¸ì‹ ì‹¤íŒ¨ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤: ê°ì²´ ì˜¤ì¸ì‹ê³¼ í™˜ê°. ì´ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê°ì²´-ë§¥ë½ ê´€ê³„ê°€ ê¸°ëŒ€ì™€ ë²—ì–´ë‚˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ LVLMsë¥¼ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ ë¶ˆì¼ì¹˜í•œ ë§¥ë½ì—ì„œì˜ ê°ì²´ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬(ORIC)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ORICëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤: (1) LLM-ìœ ë„ ìƒ˜í”Œë§, ì´ëŠ” ì¡´ì¬í•˜ì§€ë§Œ ë§¥ë½ìƒ ë¶ˆì¼ì¹˜í•œ ê°ì²´ë¥¼ ì‹ë³„í•˜ë©°, (2) CLIP-ìœ ë„ ìƒ˜í”Œë§, ì´ëŠ” í™˜ê°ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°ì²´ë¥¼ ê°ì§€í•˜ì—¬ ë¶ˆì¼ì¹˜í•œ ë§¥ë½ì„ ìƒì„±í•©ë‹ˆë‹¤. 18ê°œì˜ LVLMsì™€ ë‘ ê°œì˜ ê°œë°©í˜• ì–´íœ˜ íƒì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë§¥ë½ì  ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ì¸ì‹ ê²©ì°¨ê°€ ìƒë‹¹í•¨ì„ ë°í˜€ë‚´ì–´, ë§¥ë½ì  ë¶ˆì¼ì¹˜ê°€ ì œê¸°í•˜ëŠ” ë„ì „ ê³¼ì œë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LVLMsì˜ í•œê³„ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ë©°, ë§¥ë½ ì¸ì‹ ê°ì²´ ì¸ì‹ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì¥ë ¤í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì´ë¯¸ì§€ ìº¡ì…˜, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ, ë¡œë´‡ê³µí•™ ë“±ì—ì„œ ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ì˜ˆìƒì¹˜ ëª»í•œ ë§¥ë½ì—ì„œ ì˜¤ë¥˜ë¥¼ ë²”í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì˜¤ë¥˜ëŠ” ê°ì²´ ì˜¤ì¸ì‹ê³¼ í™˜ê°ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ ê°ì²´-ë§¥ë½ ê´€ê³„ê°€ ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ ORICë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ORICëŠ” LLMê³¼ CLIP ê°€ì´ë“œ ìƒ˜í”Œë§ì„ í†µí•´ ë§¥ë½ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ ê°ì²´ì™€ í™˜ê° ê°€ëŠ¥ì„±ì´ ìˆëŠ” ê°ì²´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤. 18ê°œì˜ LVLMê³¼ ë‘ ê°œì˜ ê°œë°©í˜• ì–´íœ˜ íƒì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë§¥ë½ì  ë¶€ì¡°í™”ê°€ ì¸ì‹ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì‚¬ì‹¤ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LVLMì˜ í•œê³„ë¥¼ ì´í•´í•˜ê³  ë§¥ë½ ì¸ì‹ ê°ì²´ ì¸ì‹ ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì´ë¯¸ì§€ ìº¡ì…˜, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ, ë¡œë´‡ ê³µí•™ì—ì„œ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ í° ë°œì „ì„ ì´ë£¨ì—ˆìœ¼ë‚˜, ë¶€ì ì ˆí•œ ë§¥ë½ì—ì„œëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê¸° ì‰½ë‹¤.
- 2. LVLMsì˜ ì£¼ìš” ì¸ì‹ ì‹¤íŒ¨ëŠ” ê°ì²´ ì˜¤ì¸ì‹ê³¼ í™˜ê° í˜„ìƒìœ¼ë¡œ, ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê°ì²´-ë§¥ë½ ê´€ê³„ì—ì„œ ë°œìƒí•œë‹¤.
- 3. ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ ORICëŠ” ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê°ì²´-ë§¥ë½ ê´€ê³„ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ LVLMsë¥¼ í‰ê°€í•˜ë©°, LLM ë° CLIP ê°€ì´ë“œ ìƒ˜í”Œë§ì„ í†µí•´ ë¶€ì ì ˆí•œ ë§¥ë½ì„ ìƒì„±í•œë‹¤.
- 4. 18ê°œì˜ LVLMsì™€ ë‘ ê°œì˜ ì˜¤í”ˆ ë³´ìºë·¸ëŸ¬ë¦¬ ê°ì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë§¥ë½ì  ë¶€ì ì ˆì„±ìœ¼ë¡œ ì¸í•œ ì¸ì‹ ê²©ì°¨ê°€ í¬ê²Œ ë‚˜íƒ€ë‚¬ë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” LVLMsì˜ í•œê³„ë¥¼ ë°íˆê³ , ë§¥ë½ ì¸ì‹ ê°ì²´ ì¸ì‹ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì´‰ì§„í•œë‹¤.


---

*Generated on 2025-09-23 10:51:44*