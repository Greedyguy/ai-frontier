---
keywords:
  - Zero-Shot Learning
  - Large Language Model
  - Biomedical Relation Extraction
  - OpenAI GPT-4
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2504.04083
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:43:28.950915",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Zero-Shot Learning",
    "Large Language Model",
    "Biomedical Relation Extraction",
    "OpenAI GPT-4"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Zero-Shot Learning": 0.85,
    "Large Language Model": 0.8,
    "Biomedical Relation Extraction": 0.78,
    "OpenAI GPT-4": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Zero-shot methodology",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "zero-shot",
          "zero-shot approach"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept that connects to various NLP and ML tasks, enhancing cross-disciplinary insights.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs",
          "large-scale language models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are foundational to current NLP research, providing a broad technical base for linking.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Biomedical relation extraction",
        "canonical": "Biomedical Relation Extraction",
        "aliases": [
          "biomedical RE",
          "bio-relation extraction"
        ],
        "category": "unique_technical",
        "rationale": "This is a specialized task within NLP, crucial for linking biomedical data and enhancing domain-specific research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "OpenAI GPT-4-turbo",
        "canonical": "OpenAI GPT-4",
        "aliases": [
          "GPT-4-turbo",
          "OpenAI turbo"
        ],
        "category": "unique_technical",
        "rationale": "This specific model variant is significant for performance benchmarking in NLP tasks, offering unique insights.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Zero-shot methodology",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Biomedical relation extraction",
      "resolved_canonical": "Biomedical Relation Extraction",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "OpenAI GPT-4-turbo",
      "resolved_canonical": "OpenAI GPT-4",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models

**Korean Title:** ì—”ë“œ-íˆ¬-ì—”ë“œ ì œë¡œ-ìƒ· ìƒë¬¼ì˜í•™ ê´€ê³„ ì¶”ì¶œì„ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬: OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‹¤í—˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2504.04083.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2504.04083](https://arxiv.org/abs/2504.04083)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/LLM-OREF_ An Open Relation Extraction Framework Based on Large Language Models_20250919|LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models]] (82.9% similar)
- [[2025-09-22/Data-Efficient Learning for Generalizable Surgical Video Understanding_20250922|Data-Efficient Learning for Generalizable Surgical Video Understanding]] (81.2% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (81.0% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (80.8% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (80.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Biomedical Relation Extraction|Biomedical Relation Extraction]], [[keywords/OpenAI GPT-4|OpenAI GPT-4]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.04083v2 Announce Type: replace 
Abstract: Objective: Zero-shot methodology promises to cut down on costs of dataset annotation and domain expertise needed to make use of NLP. Generative large language models trained to align with human goals have achieved high zero-shot performance across a wide variety of tasks. As of yet, it is unclear how well these models perform on biomedical relation extraction (RE). To address this knowledge gap, we explore patterns in the performance of OpenAI LLMs across a diverse sampling of RE tasks.
  Methods: We use OpenAI GPT-4-turbo and OpenAI's reasoning models o1 and GPT-OSS to conduct end-to-end RE experiments on seven datasets. We use the JSON generation capabilities of GPT models to generate structured output in two ways: (1) by defining an explicit schema describing the structure of relations, and (2) using a setting that infers the structure from the prompt language.
  Results: Our work is the first to study and compare the performance of the GPT-4, o1 and GPT-OSS for the end-to-end zero-shot biomedical RE task across a broad array of datasets. We found the zero-shot performances to be proximal to that of fine-tuned methods. The limitations of this approach are that it performs poorly on instances containing many relations and errs on the boundaries of textual mentions.
  Conclusion: LLMs exhibit promising zero-shot capabilities in complex biomedical RE tasks, offering competitive performance with reduced dataset curation costs and NLP modeling needs but with increased perpetual compute costs. Addressing the limitations we identify could further boost reliability. The code, data, and prompts for all our experiments are publicly available for additional benchmarking by the community: https://github.com/bionlproc/ZeroShotRE

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2504.04083v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëª©í‘œ: ì œë¡œìƒ· ë°©ë²•ë¡ ì€ NLPë¥¼ í™œìš©í•˜ëŠ” ë° í•„ìš”í•œ ë°ì´í„°ì…‹ ì£¼ì„ ë° ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ì˜ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. ì¸ê°„ì˜ ëª©í‘œì— ë§ì¶° í›ˆë ¨ëœ ìƒì„±ì  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë†’ì€ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ëª¨ë¸ì´ ìƒë¬¼ì˜í•™ì  ê´€ê³„ ì¶”ì¶œ(RE)ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰ë˜ëŠ”ì§€ëŠ” ì•„ì§ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì§€ì‹ ê²©ì°¨ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ RE ì‘ì—… ìƒ˜í”Œë§ì—ì„œ OpenAI LLMì˜ ì„±ëŠ¥ íŒ¨í„´ì„ íƒìƒ‰í•©ë‹ˆë‹¤.  
ë°©ë²•: ìš°ë¦¬ëŠ” OpenAI GPT-4-turboì™€ OpenAIì˜ ì¶”ë¡  ëª¨ë¸ o1 ë° GPT-OSSë¥¼ ì‚¬ìš©í•˜ì—¬ 7ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ì—”ë“œ íˆ¬ ì—”ë“œ RE ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. GPT ëª¨ë¸ì˜ JSON ìƒì„± ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤: (1) ê´€ê³„ì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ëŠ” ëª…ì‹œì  ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•, (2) í”„ë¡¬í”„íŠ¸ ì–¸ì–´ì—ì„œ êµ¬ì¡°ë¥¼ ì¶”ë¡ í•˜ëŠ” ì„¤ì •ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•.  
ê²°ê³¼: ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ì—”ë“œ íˆ¬ ì—”ë“œ ì œë¡œìƒ· ìƒë¬¼ì˜í•™ RE ì‘ì—…ì„ ìœ„í•´ GPT-4, o1 ë° GPT-OSSì˜ ì„±ëŠ¥ì„ ì—°êµ¬í•˜ê³  ë¹„êµí•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì œë¡œìƒ· ì„±ëŠ¥ì´ ë¯¸ì„¸ ì¡°ì •ëœ ë°©ë²•ê³¼ ê·¼ì ‘í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì˜ í•œê³„ëŠ” ë§ì€ ê´€ê³„ë¥¼ í¬í•¨í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì„±ëŠ¥ì´ ì €ì¡°í•˜ê³  í…ìŠ¤íŠ¸ ì–¸ê¸‰ì˜ ê²½ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.  
ê²°ë¡ : LLMì€ ë³µì¡í•œ ìƒë¬¼ì˜í•™ RE ì‘ì—…ì—ì„œ ìœ ë§í•œ ì œë¡œìƒ· ê¸°ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ë°ì´í„°ì…‹ íë ˆì´ì…˜ ë¹„ìš©ê³¼ NLP ëª¨ë¸ë§ ìš”êµ¬ë¥¼ ì¤„ì´ë©´ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì§€ì†ì ì¸ ì»´í“¨íŒ… ë¹„ìš©ì€ ì¦ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‹ë³„í•œ í•œê³„ë¥¼ í•´ê²°í•˜ë©´ ì‹ ë¢°ì„±ì„ ë”ìš± ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì‹¤í—˜ì— ëŒ€í•œ ì½”ë“œ, ë°ì´í„° ë° í”„ë¡¬í”„íŠ¸ëŠ” ì»¤ë®¤ë‹ˆí‹°ì˜ ì¶”ê°€ ë²¤ì¹˜ë§ˆí‚¹ì„ ìœ„í•´ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤: https://github.com/bionlproc/ZeroShotRE

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì œë¡œìƒ· ë°©ë²•ë¡ ì„ í™œìš©í•˜ì—¬ ë°”ì´ì˜¤ë©”ë””ì»¬ ê´€ê³„ ì¶”ì¶œ(RE)ì—ì„œì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. OpenAIì˜ GPT-4-turbo, o1, GPT-OSS ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 7ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ìœ¼ë©°, JSON ìƒì„± ê¸°ëŠ¥ì„ í†µí•´ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì œë¡œìƒ· ì„±ëŠ¥ì´ ë¯¸ì„¸ ì¡°ì •ëœ ë°©ë²•ê³¼ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë‚˜, ë§ì€ ê´€ê³„ë¥¼ í¬í•¨í•œ ê²½ìš°ì™€ í…ìŠ¤íŠ¸ ê²½ê³„ì—ì„œì˜ ì˜¤ë¥˜ê°€ í•œê³„ë¡œ ì§€ì ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì œë¡œìƒ· ëŠ¥ë ¥ì€ ë°ì´í„°ì…‹ ì œì‘ ë¹„ìš©ê³¼ NLP ëª¨ë¸ë§ í•„ìš”ì„±ì„ ì¤„ì´ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ì˜ ì½”ë“œì™€ ë°ì´í„°ëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” OpenAIì˜ GPT-4-turbo, o1, GPT-OSS ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìƒì˜í•™ì  ê´€ê³„ ì¶”ì¶œ(RE) ì‘ì—…ì—ì„œì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤.
- 2. ì œë¡œìƒ· ì„±ëŠ¥ì€ ë¯¸ì„¸ ì¡°ì •ëœ ë°©ë²•ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìœ¼ë‚˜, ë§ì€ ê´€ê³„ë¥¼ í¬í•¨í•œ ê²½ìš°ì™€ í…ìŠ¤íŠ¸ ì–¸ê¸‰ì˜ ê²½ê³„ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. LLMsëŠ” ë³µì¡í•œ ìƒì˜í•™ì  RE ì‘ì—…ì—ì„œ ìœ ë§í•œ ì œë¡œìƒ· ëŠ¥ë ¥ì„ ë³´ì´ë©°, ë°ì´í„°ì…‹ íë ˆì´ì…˜ ë¹„ìš©ê³¼ NLP ëª¨ë¸ë§ í•„ìš”ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. ì—°êµ¬ì˜ í•œê³„ì ì„ í•´ê²°í•˜ë©´ ì‹ ë¢°ì„±ì„ ë”ìš± ë†’ì¼ ìˆ˜ ìˆìœ¼ë©°, ëª¨ë“  ì‹¤í—˜ì˜ ì½”ë“œ, ë°ì´í„° ë° í”„ë¡¬í”„íŠ¸ëŠ” ê³µê°œë˜ì–´ ì¶”ê°€ ë²¤ì¹˜ë§ˆí‚¹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:43:28*