---
keywords:
  - Gradient Descent
  - Neural Network
  - Implicit Bias in Machine Learning
  - Margin in Machine Learning
  - Adaptive Optimization Algorithms
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2410.22069
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:02:44.840723",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Gradient Descent",
    "Neural Network",
    "Implicit Bias in Machine Learning",
    "Margin in Machine Learning",
    "Adaptive Optimization Algorithms"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Gradient Descent": 0.72,
    "Neural Network": 0.78,
    "Implicit Bias in Machine Learning": 0.82,
    "Margin in Machine Learning": 0.8,
    "Adaptive Optimization Algorithms": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "steepest descent algorithms",
        "canonical": "Gradient Descent",
        "aliases": [
          "steepest descent",
          "gradient descent"
        ],
        "category": "broad_technical",
        "rationale": "Gradient Descent is a fundamental optimization method in machine learning, linking various algorithmic discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "homogeneous neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "homogeneous networks"
        ],
        "category": "broad_technical",
        "rationale": "Homogeneous neural networks are a specific type of neural network, connecting to broader deep learning discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "implicit bias",
        "canonical": "Implicit Bias in Machine Learning",
        "aliases": [
          "bias in learning",
          "algorithmic bias"
        ],
        "category": "unique_technical",
        "rationale": "Implicit bias is crucial for understanding how learning algorithms behave beyond their explicit objectives.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "geometric margin",
        "canonical": "Margin in Machine Learning",
        "aliases": [
          "geometric margin",
          "margin"
        ],
        "category": "unique_technical",
        "rationale": "Geometric margin is a key concept in understanding the decision boundaries of classifiers.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "adaptive methods",
        "canonical": "Adaptive Optimization Algorithms",
        "aliases": [
          "adaptive methods",
          "adaptive algorithms"
        ],
        "category": "specific_connectable",
        "rationale": "Adaptive optimization methods like Adam are widely used and connect to various optimization discussions.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "KKT point",
      "perfect training accuracy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "steepest descent algorithms",
      "resolved_canonical": "Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "homogeneous neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "implicit bias",
      "resolved_canonical": "Implicit Bias in Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "geometric margin",
      "resolved_canonical": "Margin in Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "adaptive methods",
      "resolved_canonical": "Adaptive Optimization Algorithms",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks

**Korean Title:** 경계의 맛: 동차 신경망에서의 최급강하법의 암묵적 편향

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2410.22069.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2410.22069](https://arxiv.org/abs/2410.22069)

## 🔗 유사한 논문
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (82.2% similar)
- [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (82.0% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (81.7% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.4% similar)
- [[2025-09-22/Navigate Beyond Shortcuts_ Debiased Learning through the Lens of Neural Collapse_20250922|Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse]] (81.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Gradient Descent|Gradient Descent]], [[keywords/Neural Network|Neural Network]]
**🔗 Specific Connectable**: [[keywords/Adaptive Optimization Algorithms|Adaptive Optimization Algorithms]]
**⚡ Unique Technical**: [[keywords/Implicit Bias in Machine Learning|Implicit Bias in Machine Learning]], [[keywords/Margin in Machine Learning|Margin in Machine Learning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2410.22069v3 Announce Type: replace 
Abstract: We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).

## 🔍 Abstract (한글 번역)

arXiv:2410.22069v3 발표 유형: 교체  
초록: 우리는 깊은 동차 신경망에서 무한소 학습률을 갖는 가장 가파른 하강 알고리즘의 일반적인 계열의 암묵적 편향을 연구합니다. 우리는 다음을 보여줍니다: (a) 네트워크가 완벽한 훈련 정확도에 도달하면 알고리즘에 의존하는 기하학적 여유가 증가하기 시작하며, (b) 훈련 경로의 모든 극한점은 해당 여유 극대화 문제의 KKT 점에 해당합니다. 우리는 다양한 가장 가파른 하강 알고리즘으로 최적화된 신경망의 경로를 실험적으로 확대하여 인기 있는 적응 방법(Adam 및 Shampoo)의 암묵적 편향과의 연결을 강조합니다.

## 📝 요약

이 논문은 깊은 동질 신경망에서 무한소 학습률을 가진 가장 가파른 하강 알고리즘의 암묵적 편향을 연구합니다. 주요 기여는 (a) 네트워크가 완벽한 훈련 정확도에 도달하면 알고리즘에 따라 결정되는 기하학적 마진이 증가하기 시작하며, (b) 훈련 경로의 모든 극한점이 해당 마진 최대화 문제의 KKT 점에 해당한다는 점입니다. 다양한 가파른 하강 알고리즘으로 최적화된 신경망의 경로를 실험적으로 분석하여 인기 있는 적응적 방법(Adam과 Shampoo)의 암묵적 편향과의 연결성을 강조합니다.

## 🎯 주요 포인트

- 1. 무한소 학습률을 사용하는 급강하 알고리즘의 암묵적 편향을 심층 동차 신경망에서 연구합니다.
- 2. 네트워크가 완벽한 훈련 정확도에 도달하면 알고리즘에 따라 결정되는 기하학적 마진이 증가하기 시작합니다.
- 3. 훈련 궤적의 한계점은 해당 마진 최대화 문제의 KKT 점에 해당합니다.
- 4. 다양한 급강하 알고리즘으로 최적화된 신경망의 궤적을 실험적으로 분석하여 인기 있는 적응 방법(Adam 및 Shampoo)의 암묵적 편향과의 연결을 강조합니다.


---

*Generated on 2025-09-23 11:02:44*