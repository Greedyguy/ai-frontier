---
keywords:
  - Gradient Descent
  - Neural Network
  - Implicit Bias in Machine Learning
  - Margin in Machine Learning
  - Adaptive Optimization Algorithms
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2410.22069
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:02:44.840723",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Gradient Descent",
    "Neural Network",
    "Implicit Bias in Machine Learning",
    "Margin in Machine Learning",
    "Adaptive Optimization Algorithms"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Gradient Descent": 0.72,
    "Neural Network": 0.78,
    "Implicit Bias in Machine Learning": 0.82,
    "Margin in Machine Learning": 0.8,
    "Adaptive Optimization Algorithms": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "steepest descent algorithms",
        "canonical": "Gradient Descent",
        "aliases": [
          "steepest descent",
          "gradient descent"
        ],
        "category": "broad_technical",
        "rationale": "Gradient Descent is a fundamental optimization method in machine learning, linking various algorithmic discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.72
      },
      {
        "surface": "homogeneous neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "homogeneous networks"
        ],
        "category": "broad_technical",
        "rationale": "Homogeneous neural networks are a specific type of neural network, connecting to broader deep learning discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "implicit bias",
        "canonical": "Implicit Bias in Machine Learning",
        "aliases": [
          "bias in learning",
          "algorithmic bias"
        ],
        "category": "unique_technical",
        "rationale": "Implicit bias is crucial for understanding how learning algorithms behave beyond their explicit objectives.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "geometric margin",
        "canonical": "Margin in Machine Learning",
        "aliases": [
          "geometric margin",
          "margin"
        ],
        "category": "unique_technical",
        "rationale": "Geometric margin is a key concept in understanding the decision boundaries of classifiers.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "adaptive methods",
        "canonical": "Adaptive Optimization Algorithms",
        "aliases": [
          "adaptive methods",
          "adaptive algorithms"
        ],
        "category": "specific_connectable",
        "rationale": "Adaptive optimization methods like Adam are widely used and connect to various optimization discussions.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "KKT point",
      "perfect training accuracy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "steepest descent algorithms",
      "resolved_canonical": "Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "homogeneous neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "implicit bias",
      "resolved_canonical": "Implicit Bias in Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "geometric margin",
      "resolved_canonical": "Margin in Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "adaptive methods",
      "resolved_canonical": "Adaptive Optimization Algorithms",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks

**Korean Title:** ê²½ê³„ì˜ ë§›: ë™ì°¨ ì‹ ê²½ë§ì—ì„œì˜ ìµœê¸‰ê°•í•˜ë²•ì˜ ì•”ë¬µì  í¸í–¥

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2410.22069.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2410.22069](https://arxiv.org/abs/2410.22069)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (82.2% similar)
- [[2025-09-22/On Optimal Steering to Achieve Exact Fairness_20250922|On Optimal Steering to Achieve Exact Fairness]] (82.0% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (81.7% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (81.4% similar)
- [[2025-09-22/Navigate Beyond Shortcuts_ Debiased Learning through the Lens of Neural Collapse_20250922|Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse]] (81.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Gradient Descent|Gradient Descent]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Adaptive Optimization Algorithms|Adaptive Optimization Algorithms]]
**âš¡ Unique Technical**: [[keywords/Implicit Bias in Machine Learning|Implicit Bias in Machine Learning]], [[keywords/Margin in Machine Learning|Margin in Machine Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2410.22069v3 Announce Type: replace 
Abstract: We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2410.22069v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ìš°ë¦¬ëŠ” ê¹Šì€ ë™ì°¨ ì‹ ê²½ë§ì—ì„œ ë¬´í•œì†Œ í•™ìŠµë¥ ì„ ê°–ëŠ” ê°€ì¥ ê°€íŒŒë¥¸ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜ì ì¸ ê³„ì—´ì˜ ì•”ë¬µì  í¸í–¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤: (a) ë„¤íŠ¸ì›Œí¬ê°€ ì™„ë²½í•œ í›ˆë ¨ ì •í™•ë„ì— ë„ë‹¬í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì— ì˜ì¡´í•˜ëŠ” ê¸°í•˜í•™ì  ì—¬ìœ ê°€ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©°, (b) í›ˆë ¨ ê²½ë¡œì˜ ëª¨ë“  ê·¹í•œì ì€ í•´ë‹¹ ì—¬ìœ  ê·¹ëŒ€í™” ë¬¸ì œì˜ KKT ì ì— í•´ë‹¹í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ê°€ì¥ ê°€íŒŒë¥¸ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”ëœ ì‹ ê²½ë§ì˜ ê²½ë¡œë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ í™•ëŒ€í•˜ì—¬ ì¸ê¸° ìˆëŠ” ì ì‘ ë°©ë²•(Adam ë° Shampoo)ì˜ ì•”ë¬µì  í¸í–¥ê³¼ì˜ ì—°ê²°ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¹Šì€ ë™ì§ˆ ì‹ ê²½ë§ì—ì„œ ë¬´í•œì†Œ í•™ìŠµë¥ ì„ ê°€ì§„ ê°€ì¥ ê°€íŒŒë¥¸ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì˜ ì•”ë¬µì  í¸í–¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” (a) ë„¤íŠ¸ì›Œí¬ê°€ ì™„ë²½í•œ í›ˆë ¨ ì •í™•ë„ì— ë„ë‹¬í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ê²°ì •ë˜ëŠ” ê¸°í•˜í•™ì  ë§ˆì§„ì´ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©°, (b) í›ˆë ¨ ê²½ë¡œì˜ ëª¨ë“  ê·¹í•œì ì´ í•´ë‹¹ ë§ˆì§„ ìµœëŒ€í™” ë¬¸ì œì˜ KKT ì ì— í•´ë‹¹í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê°€íŒŒë¥¸ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”ëœ ì‹ ê²½ë§ì˜ ê²½ë¡œë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì¸ê¸° ìˆëŠ” ì ì‘ì  ë°©ë²•(Adamê³¼ Shampoo)ì˜ ì•”ë¬µì  í¸í–¥ê³¼ì˜ ì—°ê²°ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¬´í•œì†Œ í•™ìŠµë¥ ì„ ì‚¬ìš©í•˜ëŠ” ê¸‰ê°•í•˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì•”ë¬µì  í¸í–¥ì„ ì‹¬ì¸µ ë™ì°¨ ì‹ ê²½ë§ì—ì„œ ì—°êµ¬í•©ë‹ˆë‹¤.
- 2. ë„¤íŠ¸ì›Œí¬ê°€ ì™„ë²½í•œ í›ˆë ¨ ì •í™•ë„ì— ë„ë‹¬í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ê²°ì •ë˜ëŠ” ê¸°í•˜í•™ì  ë§ˆì§„ì´ ì¦ê°€í•˜ê¸° ì‹œì‘í•©ë‹ˆë‹¤.
- 3. í›ˆë ¨ ê¶¤ì ì˜ í•œê³„ì ì€ í•´ë‹¹ ë§ˆì§„ ìµœëŒ€í™” ë¬¸ì œì˜ KKT ì ì— í•´ë‹¹í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ ê¸‰ê°•í•˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”ëœ ì‹ ê²½ë§ì˜ ê¶¤ì ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì¸ê¸° ìˆëŠ” ì ì‘ ë°©ë²•(Adam ë° Shampoo)ì˜ ì•”ë¬µì  í¸í–¥ê³¼ì˜ ì—°ê²°ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:02:44*