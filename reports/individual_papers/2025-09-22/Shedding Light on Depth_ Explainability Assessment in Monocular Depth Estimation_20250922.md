---
keywords:
  - Monocular Depth Estimation
  - Explainable Artificial Intelligence
  - Saliency Maps
  - Integrated Gradients
  - Attribution Fidelity
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15980
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:24:28.846438",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Monocular Depth Estimation",
    "Explainable Artificial Intelligence",
    "Saliency Maps",
    "Integrated Gradients",
    "Attribution Fidelity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Monocular Depth Estimation": 0.8,
    "Explainable Artificial Intelligence": 0.78,
    "Saliency Maps": 0.85,
    "Integrated Gradients": 0.83,
    "Attribution Fidelity": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Monocular Depth Estimation",
        "canonical": "Monocular Depth Estimation",
        "aliases": [
          "MDE"
        ],
        "category": "unique_technical",
        "rationale": "Monocular Depth Estimation is a specific task in computer vision that is central to the paper's focus on explainability.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Explainable Artificial Intelligence",
        "canonical": "Explainable Artificial Intelligence",
        "aliases": [
          "XAI"
        ],
        "category": "broad_technical",
        "rationale": "Explainable AI is a broad technical concept that underpins the study of model interpretability in the paper.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Saliency Maps",
        "canonical": "Saliency Maps",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Saliency Maps are a specific method for visualizing important features, directly relevant to the paper's analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Integrated Gradients",
        "canonical": "Integrated Gradients",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Integrated Gradients is a key technique evaluated in the paper for feature attribution.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.83
      },
      {
        "surface": "Attribution Fidelity",
        "canonical": "Attribution Fidelity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Attribution Fidelity is a novel metric introduced in the paper to evaluate the reliability of feature attributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Monocular Depth Estimation",
      "resolved_canonical": "Monocular Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Explainable Artificial Intelligence",
      "resolved_canonical": "Explainable Artificial Intelligence",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Saliency Maps",
      "resolved_canonical": "Saliency Maps",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Integrated Gradients",
      "resolved_canonical": "Integrated Gradients",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Attribution Fidelity",
      "resolved_canonical": "Attribution Fidelity",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation

**Korean Title:** 단일 카메라 깊이 추정에서 설명 가능성 평가: 깊이에 대한 이해 증진

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15980.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15980](https://arxiv.org/abs/2509.15980)

## 🔗 유사한 논문
- [[2025-09-22/Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models_20250922|Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models]] (84.5% similar)
- [[2025-09-18/Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients_20250918|Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients]] (83.1% similar)
- [[2025-09-22/Generating Part-Based Global Explanations Via Correspondence_20250922|Generating Part-Based Global Explanations Via Correspondence]] (81.5% similar)
- [[2025-09-22/GIN-Graph_ A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks_20250922|GIN-Graph: A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks]] (80.5% similar)
- [[2025-09-22/Towards Sharper Object Boundaries in Self-Supervised Depth Estimation_20250922|Towards Sharper Object Boundaries in Self-Supervised Depth Estimation]] (80.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Explainable Artificial Intelligence|Explainable Artificial Intelligence]]
**🔗 Specific Connectable**: [[keywords/Saliency Maps|Saliency Maps]], [[keywords/Integrated Gradients|Integrated Gradients]]
**⚡ Unique Technical**: [[keywords/Monocular Depth Estimation|Monocular Depth Estimation]], [[keywords/Attribution Fidelity|Attribution Fidelity]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15980v1 Announce Type: cross 
Abstract: Explainable artificial intelligence is increasingly employed to understand the decision-making process of deep learning models and create trustworthiness in their adoption. However, the explainability of Monocular Depth Estimation (MDE) remains largely unexplored despite its wide deployment in real-world applications. In this work, we study how to analyze MDE networks to map the input image to the predicted depth map. More in detail, we investigate well-established feature attribution methods, Saliency Maps, Integrated Gradients, and Attention Rollout on different computationally complex models for MDE: METER, a lightweight network, and PixelFormer, a deep network. We assess the quality of the generated visual explanations by selectively perturbing the most relevant and irrelevant pixels, as identified by the explainability methods, and analyzing the impact of these perturbations on the model's output. Moreover, since existing evaluation metrics can have some limitations in measuring the validity of visual explanations for MDE, we additionally introduce the Attribution Fidelity. This metric evaluates the reliability of the feature attribution by assessing their consistency with the predicted depth map. Experimental results demonstrate that Saliency Maps and Integrated Gradients have good performance in highlighting the most important input features for MDE lightweight and deep models, respectively. Furthermore, we show that Attribution Fidelity effectively identifies whether an explainability method fails to produce reliable visual maps, even in scenarios where conventional metrics might suggest satisfactory results.

## 🔍 Abstract (한글 번역)

arXiv:2509.15980v1 발표 유형: 교차  
초록: 설명 가능한 인공지능은 딥러닝 모델의 의사결정 과정을 이해하고 그 채택에 대한 신뢰성을 구축하기 위해 점점 더 많이 사용되고 있습니다. 그러나 단안 깊이 추정(Monocular Depth Estimation, MDE)의 설명 가능성은 실제 응용에서 널리 사용됨에도 불구하고 여전히 크게 탐구되지 않았습니다. 이 연구에서는 입력 이미지를 예측된 깊이 맵으로 매핑하기 위해 MDE 네트워크를 분석하는 방법을 연구합니다. 더 구체적으로, 우리는 MDE를 위한 다양한 계산 복잡한 모델들인 경량 네트워크인 METER와 심층 네트워크인 PixelFormer에 대해 잘 확립된 특징 귀속 방법인 살리언시 맵(Saliency Maps), 통합 기울기(Integrated Gradients), 주의 롤아웃(Attention Rollout)을 조사합니다. 설명 가능성 방법에 의해 식별된 가장 관련 있고 관련 없는 픽셀을 선택적으로 변형하고, 이러한 변형이 모델의 출력에 미치는 영향을 분석함으로써 생성된 시각적 설명의 품질을 평가합니다. 또한, 기존 평가 지표가 MDE에 대한 시각적 설명의 타당성을 측정하는 데 한계가 있을 수 있기 때문에, 우리는 추가로 귀속 충실도(Attribution Fidelity)를 도입합니다. 이 지표는 예측된 깊이 맵과의 일관성을 평가하여 특징 귀속의 신뢰성을 평가합니다. 실험 결과는 살리언시 맵과 통합 기울기가 각각 MDE 경량 및 심층 모델에 대해 가장 중요한 입력 특징을 강조하는 데 있어 좋은 성능을 보임을 보여줍니다. 더욱이, 귀속 충실도는 기존 지표가 만족스러운 결과를 제시할 수 있는 시나리오에서도 설명 가능성 방법이 신뢰할 수 있는 시각적 맵을 생성하지 못하는지를 효과적으로 식별함을 보여줍니다.

## 📝 요약

이 논문은 단안 깊이 추정(MDE) 모델의 설명 가능성을 연구하여, 입력 이미지와 예측된 깊이 맵 간의 관계를 분석합니다. 저자들은 Saliency Maps, Integrated Gradients, Attention Rollout 등 다양한 특징 귀속 방법을 사용하여 MDE의 경량 네트워크(METER)와 심층 네트워크(PixelFormer)를 평가했습니다. 이들은 설명 가능성 방법이 식별한 중요한 픽셀을 선택적으로 변형하여 모델 출력에 미치는 영향을 분석했습니다. 또한, 기존 평가 지표의 한계를 보완하기 위해 Attribution Fidelity라는 새로운 평가 지표를 제안하여, 특징 귀속의 신뢰성을 예측된 깊이 맵과의 일관성으로 평가했습니다. 실험 결과, Saliency Maps와 Integrated Gradients는 각각 경량 및 심층 모델에서 중요한 입력 특징을 잘 강조하는 것으로 나타났으며, Attribution Fidelity는 기존 지표가 만족스러운 결과를 제시할 때도 설명 가능성 방법의 신뢰성을 효과적으로 평가할 수 있음을 보여주었습니다.

## 🎯 주요 포인트

- 1. 설명 가능한 인공지능은 딥러닝 모델의 의사결정 과정을 이해하고 신뢰성을 확보하는 데 점점 더 많이 사용되고 있다.
- 2. 단안 깊이 추정(MDE)의 설명 가능성은 실제 응용에서 널리 사용되고 있음에도 불구하고 아직 많이 탐구되지 않았다.
- 3. MDE 네트워크를 분석하여 입력 이미지를 예측된 깊이 맵으로 매핑하는 방법을 연구하였다.
- 4. Saliency Maps와 Integrated Gradients는 각각 경량 및 심층 MDE 모델에서 중요한 입력 특징을 잘 강조하는 성능을 보였다.
- 5. Attribution Fidelity는 설명 가능성 방법이 신뢰할 수 있는 시각적 지도를 생성하지 못하는지를 효과적으로 식별한다.


---

*Generated on 2025-09-23 09:24:28*