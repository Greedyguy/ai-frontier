---
keywords:
  - Monocular Depth Estimation
  - Explainable Artificial Intelligence
  - Saliency Maps
  - Integrated Gradients
  - Attribution Fidelity
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15980
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:24:28.846438",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Monocular Depth Estimation",
    "Explainable Artificial Intelligence",
    "Saliency Maps",
    "Integrated Gradients",
    "Attribution Fidelity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Monocular Depth Estimation": 0.8,
    "Explainable Artificial Intelligence": 0.78,
    "Saliency Maps": 0.85,
    "Integrated Gradients": 0.83,
    "Attribution Fidelity": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Monocular Depth Estimation",
        "canonical": "Monocular Depth Estimation",
        "aliases": [
          "MDE"
        ],
        "category": "unique_technical",
        "rationale": "Monocular Depth Estimation is a specific task in computer vision that is central to the paper's focus on explainability.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Explainable Artificial Intelligence",
        "canonical": "Explainable Artificial Intelligence",
        "aliases": [
          "XAI"
        ],
        "category": "broad_technical",
        "rationale": "Explainable AI is a broad technical concept that underpins the study of model interpretability in the paper.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Saliency Maps",
        "canonical": "Saliency Maps",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Saliency Maps are a specific method for visualizing important features, directly relevant to the paper's analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Integrated Gradients",
        "canonical": "Integrated Gradients",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Integrated Gradients is a key technique evaluated in the paper for feature attribution.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.83
      },
      {
        "surface": "Attribution Fidelity",
        "canonical": "Attribution Fidelity",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Attribution Fidelity is a novel metric introduced in the paper to evaluate the reliability of feature attributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Monocular Depth Estimation",
      "resolved_canonical": "Monocular Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Explainable Artificial Intelligence",
      "resolved_canonical": "Explainable Artificial Intelligence",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Saliency Maps",
      "resolved_canonical": "Saliency Maps",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Integrated Gradients",
      "resolved_canonical": "Integrated Gradients",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "Attribution Fidelity",
      "resolved_canonical": "Attribution Fidelity",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation

**Korean Title:** ë‹¨ì¼ ì¹´ë©”ë¼ ê¹Šì´ ì¶”ì •ì—ì„œ ì„¤ëª… ê°€ëŠ¥ì„± í‰ê°€: ê¹Šì´ì— ëŒ€í•œ ì´í•´ ì¦ì§„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15980.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15980](https://arxiv.org/abs/2509.15980)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models_20250922|Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models]] (84.5% similar)
- [[2025-09-18/Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients_20250918|Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients]] (83.1% similar)
- [[2025-09-22/Generating Part-Based Global Explanations Via Correspondence_20250922|Generating Part-Based Global Explanations Via Correspondence]] (81.5% similar)
- [[2025-09-22/GIN-Graph_ A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks_20250922|GIN-Graph: A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks]] (80.5% similar)
- [[2025-09-22/Towards Sharper Object Boundaries in Self-Supervised Depth Estimation_20250922|Towards Sharper Object Boundaries in Self-Supervised Depth Estimation]] (80.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Explainable Artificial Intelligence|Explainable Artificial Intelligence]]
**ğŸ”— Specific Connectable**: [[keywords/Saliency Maps|Saliency Maps]], [[keywords/Integrated Gradients|Integrated Gradients]]
**âš¡ Unique Technical**: [[keywords/Monocular Depth Estimation|Monocular Depth Estimation]], [[keywords/Attribution Fidelity|Attribution Fidelity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15980v1 Announce Type: cross 
Abstract: Explainable artificial intelligence is increasingly employed to understand the decision-making process of deep learning models and create trustworthiness in their adoption. However, the explainability of Monocular Depth Estimation (MDE) remains largely unexplored despite its wide deployment in real-world applications. In this work, we study how to analyze MDE networks to map the input image to the predicted depth map. More in detail, we investigate well-established feature attribution methods, Saliency Maps, Integrated Gradients, and Attention Rollout on different computationally complex models for MDE: METER, a lightweight network, and PixelFormer, a deep network. We assess the quality of the generated visual explanations by selectively perturbing the most relevant and irrelevant pixels, as identified by the explainability methods, and analyzing the impact of these perturbations on the model's output. Moreover, since existing evaluation metrics can have some limitations in measuring the validity of visual explanations for MDE, we additionally introduce the Attribution Fidelity. This metric evaluates the reliability of the feature attribution by assessing their consistency with the predicted depth map. Experimental results demonstrate that Saliency Maps and Integrated Gradients have good performance in highlighting the most important input features for MDE lightweight and deep models, respectively. Furthermore, we show that Attribution Fidelity effectively identifies whether an explainability method fails to produce reliable visual maps, even in scenarios where conventional metrics might suggest satisfactory results.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15980v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì„¤ëª… ê°€ëŠ¥í•œ ì¸ê³µì§€ëŠ¥ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ì´í•´í•˜ê³  ê·¸ ì±„íƒì— ëŒ€í•œ ì‹ ë¢°ì„±ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •(Monocular Depth Estimation, MDE)ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì€ ì‹¤ì œ ì‘ìš©ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ í¬ê²Œ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì˜ˆì¸¡ëœ ê¹Šì´ ë§µìœ¼ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ MDE ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” MDEë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ê³„ì‚° ë³µì¡í•œ ëª¨ë¸ë“¤ì¸ ê²½ëŸ‰ ë„¤íŠ¸ì›Œí¬ì¸ METERì™€ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì¸ PixelFormerì— ëŒ€í•´ ì˜ í™•ë¦½ëœ íŠ¹ì§• ê·€ì† ë°©ë²•ì¸ ì‚´ë¦¬ì–¸ì‹œ ë§µ(Saliency Maps), í†µí•© ê¸°ìš¸ê¸°(Integrated Gradients), ì£¼ì˜ ë¡¤ì•„ì›ƒ(Attention Rollout)ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì„¤ëª… ê°€ëŠ¥ì„± ë°©ë²•ì— ì˜í•´ ì‹ë³„ëœ ê°€ì¥ ê´€ë ¨ ìˆê³  ê´€ë ¨ ì—†ëŠ” í”½ì…€ì„ ì„ íƒì ìœ¼ë¡œ ë³€í˜•í•˜ê³ , ì´ëŸ¬í•œ ë³€í˜•ì´ ëª¨ë¸ì˜ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•¨ìœ¼ë¡œì¨ ìƒì„±ëœ ì‹œê°ì  ì„¤ëª…ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ, ê¸°ì¡´ í‰ê°€ ì§€í‘œê°€ MDEì— ëŒ€í•œ ì‹œê°ì  ì„¤ëª…ì˜ íƒ€ë‹¹ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ì¶”ê°€ë¡œ ê·€ì† ì¶©ì‹¤ë„(Attribution Fidelity)ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ì´ ì§€í‘œëŠ” ì˜ˆì¸¡ëœ ê¹Šì´ ë§µê³¼ì˜ ì¼ê´€ì„±ì„ í‰ê°€í•˜ì—¬ íŠ¹ì§• ê·€ì†ì˜ ì‹ ë¢°ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ì‚´ë¦¬ì–¸ì‹œ ë§µê³¼ í†µí•© ê¸°ìš¸ê¸°ê°€ ê°ê° MDE ê²½ëŸ‰ ë° ì‹¬ì¸µ ëª¨ë¸ì— ëŒ€í•´ ê°€ì¥ ì¤‘ìš”í•œ ì…ë ¥ íŠ¹ì§•ì„ ê°•ì¡°í•˜ëŠ” ë° ìˆì–´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ìš±ì´, ê·€ì† ì¶©ì‹¤ë„ëŠ” ê¸°ì¡´ ì§€í‘œê°€ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ì œì‹œí•  ìˆ˜ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œë„ ì„¤ëª… ê°€ëŠ¥ì„± ë°©ë²•ì´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‹œê°ì  ë§µì„ ìƒì„±í•˜ì§€ ëª»í•˜ëŠ”ì§€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‹ë³„í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •(MDE) ëª¨ë¸ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì„ ì—°êµ¬í•˜ì—¬, ì…ë ¥ ì´ë¯¸ì§€ì™€ ì˜ˆì¸¡ëœ ê¹Šì´ ë§µ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì €ìë“¤ì€ Saliency Maps, Integrated Gradients, Attention Rollout ë“± ë‹¤ì–‘í•œ íŠ¹ì§• ê·€ì† ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ MDEì˜ ê²½ëŸ‰ ë„¤íŠ¸ì›Œí¬(METER)ì™€ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬(PixelFormer)ë¥¼ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ì„¤ëª… ê°€ëŠ¥ì„± ë°©ë²•ì´ ì‹ë³„í•œ ì¤‘ìš”í•œ í”½ì…€ì„ ì„ íƒì ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ ëª¨ë¸ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ê¸°ì¡´ í‰ê°€ ì§€í‘œì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Attribution Fidelityë¼ëŠ” ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œë¥¼ ì œì•ˆí•˜ì—¬, íŠ¹ì§• ê·€ì†ì˜ ì‹ ë¢°ì„±ì„ ì˜ˆì¸¡ëœ ê¹Šì´ ë§µê³¼ì˜ ì¼ê´€ì„±ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, Saliency Mapsì™€ Integrated GradientsëŠ” ê°ê° ê²½ëŸ‰ ë° ì‹¬ì¸µ ëª¨ë¸ì—ì„œ ì¤‘ìš”í•œ ì…ë ¥ íŠ¹ì§•ì„ ì˜ ê°•ì¡°í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, Attribution FidelityëŠ” ê¸°ì¡´ ì§€í‘œê°€ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ì œì‹œí•  ë•Œë„ ì„¤ëª… ê°€ëŠ¥ì„± ë°©ë²•ì˜ ì‹ ë¢°ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì„¤ëª… ê°€ëŠ¥í•œ ì¸ê³µì§€ëŠ¥ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ì´í•´í•˜ê³  ì‹ ë¢°ì„±ì„ í™•ë³´í•˜ëŠ” ë° ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤.
- 2. ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •(MDE)ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì€ ì‹¤ì œ ì‘ìš©ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì•„ì§ ë§ì´ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.
- 3. MDE ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•˜ì—¬ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì˜ˆì¸¡ëœ ê¹Šì´ ë§µìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•˜ì˜€ë‹¤.
- 4. Saliency Mapsì™€ Integrated GradientsëŠ” ê°ê° ê²½ëŸ‰ ë° ì‹¬ì¸µ MDE ëª¨ë¸ì—ì„œ ì¤‘ìš”í•œ ì…ë ¥ íŠ¹ì§•ì„ ì˜ ê°•ì¡°í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
- 5. Attribution FidelityëŠ” ì„¤ëª… ê°€ëŠ¥ì„± ë°©ë²•ì´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‹œê°ì  ì§€ë„ë¥¼ ìƒì„±í•˜ì§€ ëª»í•˜ëŠ”ì§€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‹ë³„í•œë‹¤.


---

*Generated on 2025-09-23 09:24:28*