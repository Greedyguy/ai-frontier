---
keywords:
  - Large Language Model
  - Multilingual Generation
  - Low-resource Languages
  - Conversational Tasks
  - Benchmark Transformation
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.14395
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:57:31.197195",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multilingual Generation",
    "Low-resource Languages",
    "Conversational Tasks",
    "Benchmark Transformation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multilingual Generation": 0.8,
    "Low-resource Languages": 0.78,
    "Conversational Tasks": 0.77,
    "Benchmark Transformation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating multilingual capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multilingual Generation",
        "canonical": "Multilingual Generation",
        "aliases": [
          "Multilingual Text Generation"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for linking multilingual evaluation frameworks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Low-resource Languages",
        "canonical": "Low-resource Languages",
        "aliases": [
          "Under-resourced Languages"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the challenge addressed by the framework.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Conversational Tasks",
        "canonical": "Conversational Tasks",
        "aliases": [
          "Dialogue Tasks"
        ],
        "category": "unique_technical",
        "rationale": "Essential for understanding the evaluation method proposed.",
        "novelty_score": 0.65,
        "connectivity_score": 0.67,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Benchmark Transformation",
        "canonical": "Benchmark Transformation",
        "aliases": [
          "Benchmark Adaptation"
        ],
        "category": "unique_technical",
        "rationale": "Describes the innovative approach of adapting benchmarks for evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Evaluation",
      "Framework",
      "Capabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multilingual Generation",
      "resolved_canonical": "Multilingual Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Low-resource Languages",
      "resolved_canonical": "Low-resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Conversational Tasks",
      "resolved_canonical": "Conversational Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.67,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Benchmark Transformation",
      "resolved_canonical": "Benchmark Transformation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language

**Korean Title:** MUG-Eval: ëª¨ë“  ì–¸ì–´ì—ì„œ ë‹¤êµ­ì–´ ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ëŒ€ë¦¬ í‰ê°€ í”„ë ˆì„ì›Œí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.14395.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.14395](https://arxiv.org/abs/2505.14395)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (87.9% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (87.3% similar)
- [[2025-09-19/Ticket-Bench_ A Kickoff for Multilingual and Regionalized Agent Evaluation_20250919|Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation]] (86.4% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (84.7% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (84.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Low-resource Languages|Low-resource Languages]]
**âš¡ Unique Technical**: [[keywords/Multilingual Generation|Multilingual Generation]], [[keywords/Conversational Tasks|Conversational Tasks]], [[keywords/Benchmark Transformation|Benchmark Transformation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.14395v2 Announce Type: replace-cross 
Abstract: Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy for successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.14395v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ íŠ¹íˆ ì§ì ‘ í‰ê°€ ë°©ë²•ì´ ë“œë¬¸ ì €ìì› ì–¸ì–´ì˜ ê²½ìš° ì–´ë ¤ìš´ ê³¼ì œì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ëŒ€í™”í˜• ê³¼ì œë¡œ ë³€í™˜í•˜ê³  í•´ë‹¹ ê³¼ì œì—ì„œ LLMì˜ ì •í™•ì„±ì„ ì¸¡ì •í•˜ì—¬ LLMì˜ ë‹¤êµ­ì–´ ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ MUG-Evalì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ëŒ€í™”í˜• ê³¼ì œë¥¼ ì„¤ê³„í•˜ì—¬ ëª©í‘œ ì–¸ì–´ì—ì„œ íš¨ê³¼ì ì¸ ì˜ì‚¬ì†Œí†µì´ í•„ìš”í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ê³¼ì œ ì„±ê³µë¥ ì„ ë‹¨ìˆœíˆ ì„±ê³µì ì¸ ëŒ€í™” ìƒì„±ì˜ ëŒ€ë¦¬ ì§€í‘œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ë‘ ê°€ì§€ ì£¼ìš” ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤: ëŒ€ë¶€ë¶„ì˜ ì–¸ì–´ì— ì œí•œì ì¸ ì–¸ì–´ë³„ NLP ë„êµ¬ë‚˜ ì£¼ì„ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë©°, í‰ê°€ í’ˆì§ˆì´ ëª‡ëª‡ ê³ ìì› ì–¸ì–´ ì™¸ì—ì„œëŠ” ì €í•˜ë˜ëŠ” LLM-íŒì‚¬ì— ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê³ ìì›, ì¤‘ìì›, ì €ìì› ë²”ì£¼ì— ê±¸ì¹œ 30ê°œ ì–¸ì–´ì—ì„œ 8ê°œì˜ LLMì„ í‰ê°€í•˜ì˜€ìœ¼ë©°, MUG-Evalì´ í™•ë¦½ëœ ë²¤ì¹˜ë§ˆí¬ì™€ ê°•í•œ ìƒê´€ê´€ê³„($r$ > 0.75)ë¥¼ ê°€ì§€ë©´ì„œë„ ì–¸ì–´ì™€ ëª¨ë¸ ê°„ì˜ í‘œì¤€í™”ëœ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤êµ­ì–´ ìƒì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ê²¬ê³ í•˜ê³  ìì› íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ë©°, ìˆ˜ì²œ ê°œì˜ ì–¸ì–´ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

MUG-Evalì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‹¤êµ­ì–´ ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ëŒ€í™”í˜• ê³¼ì œë¡œ ë³€í™˜í•˜ì—¬ LLMì˜ ì •í™•ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì–¸ì–´ë³„ NLP ë„êµ¬ë‚˜ ì£¼ì„ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë©°, í‰ê°€ í’ˆì§ˆì´ ì €í•˜ë˜ëŠ” LLM ìì²´ í‰ê°€ë¥¼ í”¼í•©ë‹ˆë‹¤. 30ê°œ ì–¸ì–´ì˜ 8ê°œ LLMì„ í‰ê°€í•œ ê²°ê³¼, MUG-Evalì€ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ì™€ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ë©°, í‘œì¤€í™”ëœ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ìˆ˜ì²œ ê°œì˜ ì–¸ì–´ë¡œ í™•ì¥ ê°€ëŠ¥í•œ ë‹¤êµ­ì–´ ìƒì„± í‰ê°€ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MUG-Evalì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ë‹¤êµ­ì–´ ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ëŒ€í™”í˜• ê³¼ì œë¡œ ë³€í™˜í•˜ì—¬ LLMsì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì–¸ì–´ë³„ NLP ë„êµ¬ë‚˜ ì£¼ì„ ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë©°, LLMsë¥¼ í‰ê°€ìë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.
- 3. 30ê°œì˜ ê³ ìì›, ì¤‘ìì›, ì €ìì› ì–¸ì–´ë¥¼ ëŒ€ìƒìœ¼ë¡œ 8ê°œì˜ LLMsë¥¼ í‰ê°€í•œ ê²°ê³¼, MUG-Evalì€ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ì™€ ê°•í•œ ìƒê´€ê´€ê³„($r$ > 0.75)ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. MUG-Evalì€ ì–¸ì–´ì™€ ëª¨ë¸ ê°„ì˜ í‘œì¤€í™”ëœ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ìˆ˜ì²œ ê°œì˜ ì–¸ì–´ë¡œ í™•ì¥ ê°€ëŠ¥í•œ ë‹¤êµ­ì–´ ìƒì„± í‰ê°€ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:57:31*