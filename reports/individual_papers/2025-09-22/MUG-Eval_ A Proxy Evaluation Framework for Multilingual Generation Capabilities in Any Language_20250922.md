---
keywords:
  - Large Language Model
  - Multilingual Generation
  - Low-resource Languages
  - Conversational Tasks
  - Benchmark Transformation
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2505.14395
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:57:31.197195",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multilingual Generation",
    "Low-resource Languages",
    "Conversational Tasks",
    "Benchmark Transformation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multilingual Generation": 0.8,
    "Low-resource Languages": 0.78,
    "Conversational Tasks": 0.77,
    "Benchmark Transformation": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating multilingual capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multilingual Generation",
        "canonical": "Multilingual Generation",
        "aliases": [
          "Multilingual Text Generation"
        ],
        "category": "unique_technical",
        "rationale": "Key concept for linking multilingual evaluation frameworks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Low-resource Languages",
        "canonical": "Low-resource Languages",
        "aliases": [
          "Under-resourced Languages"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the challenge addressed by the framework.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Conversational Tasks",
        "canonical": "Conversational Tasks",
        "aliases": [
          "Dialogue Tasks"
        ],
        "category": "unique_technical",
        "rationale": "Essential for understanding the evaluation method proposed.",
        "novelty_score": 0.65,
        "connectivity_score": 0.67,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "Benchmark Transformation",
        "canonical": "Benchmark Transformation",
        "aliases": [
          "Benchmark Adaptation"
        ],
        "category": "unique_technical",
        "rationale": "Describes the innovative approach of adapting benchmarks for evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Evaluation",
      "Framework",
      "Capabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multilingual Generation",
      "resolved_canonical": "Multilingual Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Low-resource Languages",
      "resolved_canonical": "Low-resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Conversational Tasks",
      "resolved_canonical": "Conversational Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.67,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Benchmark Transformation",
      "resolved_canonical": "Benchmark Transformation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language

**Korean Title:** MUG-Eval: 모든 언어에서 다국어 생성 능력을 평가하기 위한 대리 평가 프레임워크

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.14395.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2505.14395](https://arxiv.org/abs/2505.14395)

## 🔗 유사한 논문
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (87.9% similar)
- [[2025-09-22/MEDAL_ A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators_20250922|MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators]] (87.3% similar)
- [[2025-09-19/Ticket-Bench_ A Kickoff for Multilingual and Regionalized Agent Evaluation_20250919|Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation]] (86.4% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (84.7% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (84.7% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Low-resource Languages|Low-resource Languages]]
**⚡ Unique Technical**: [[keywords/Multilingual Generation|Multilingual Generation]], [[keywords/Conversational Tasks|Conversational Tasks]], [[keywords/Benchmark Transformation|Benchmark Transformation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.14395v2 Announce Type: replace-cross 
Abstract: Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy for successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.

## 🔍 Abstract (한글 번역)

arXiv:2505.14395v2 발표 유형: 교차 대체  
초록: 대형 언어 모델(LLM)의 텍스트 생성 능력을 평가하는 것은 특히 직접 평가 방법이 드문 저자원 언어의 경우 어려운 과제입니다. 우리는 기존 벤치마크를 대화형 과제로 변환하고 해당 과제에서 LLM의 정확성을 측정하여 LLM의 다국어 생성 능력을 평가하는 새로운 프레임워크인 MUG-Eval을 제안합니다. 우리는 이러한 대화형 과제를 설계하여 목표 언어에서 효과적인 의사소통이 필요하도록 했습니다. 그런 다음, 과제 성공률을 단순히 성공적인 대화 생성의 대리 지표로 사용합니다. 우리의 접근 방식은 두 가지 주요 이점을 제공합니다: 대부분의 언어에 제한적인 언어별 NLP 도구나 주석 데이터셋에 의존하지 않으며, 평가 품질이 몇몇 고자원 언어 외에서는 저하되는 LLM-판사에 의존하지 않습니다. 우리는 고자원, 중자원, 저자원 범주에 걸친 30개 언어에서 8개의 LLM을 평가하였으며, MUG-Eval이 확립된 벤치마크와 강한 상관관계($r$ > 0.75)를 가지면서도 언어와 모델 간의 표준화된 비교를 가능하게 함을 발견했습니다. 우리의 프레임워크는 다국어 생성을 평가하기 위한 견고하고 자원 효율적인 솔루션을 제공하며, 수천 개의 언어로 확장할 수 있습니다.

## 📝 요약

MUG-Eval은 대형 언어 모델(LLM)의 다국어 생성 능력을 평가하기 위한 새로운 프레임워크로, 기존 벤치마크를 대화형 과제로 변환하여 LLM의 정확성을 측정합니다. 이 방법은 언어별 NLP 도구나 주석 데이터셋에 의존하지 않으며, 평가 품질이 저하되는 LLM 자체 평가를 피합니다. 30개 언어의 8개 LLM을 평가한 결과, MUG-Eval은 기존 벤치마크와 강한 상관관계를 보이며, 표준화된 비교를 가능하게 합니다. 이 프레임워크는 수천 개의 언어로 확장 가능한 다국어 생성 평가 솔루션을 제공합니다.

## 🎯 주요 포인트

- 1. MUG-Eval은 대형 언어 모델(LLMs)의 다국어 생성 능력을 평가하기 위한 새로운 프레임워크로, 기존 벤치마크를 대화형 과제로 변환하여 LLMs의 정확도를 측정합니다.
- 2. 이 프레임워크는 언어별 NLP 도구나 주석 데이터셋에 의존하지 않으며, LLMs를 평가자로 사용하는 방법의 한계를 극복합니다.
- 3. 30개의 고자원, 중자원, 저자원 언어를 대상으로 8개의 LLMs를 평가한 결과, MUG-Eval은 기존 벤치마크와 강한 상관관계($r$ > 0.75)를 보였습니다.
- 4. MUG-Eval은 언어와 모델 간의 표준화된 비교를 가능하게 하며, 수천 개의 언어로 확장 가능한 다국어 생성 평가 솔루션을 제공합니다.


---

*Generated on 2025-09-23 09:57:31*