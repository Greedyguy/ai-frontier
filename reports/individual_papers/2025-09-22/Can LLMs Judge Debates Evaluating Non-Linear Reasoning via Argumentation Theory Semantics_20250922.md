---
keywords:
  - Large Language Model
  - Computational Argumentation Theory
  - Quantitative Argumentation Debate
  - Chain-of-Thought
  - In-Context Learning
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15739
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:33:48.205895",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Computational Argumentation Theory",
    "Quantitative Argumentation Debate",
    "Chain-of-Thought",
    "In-Context Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "Computational Argumentation Theory": 0.78,
    "Quantitative Argumentation Debate": 0.82,
    "Chain-of-Thought": 0.77,
    "In-Context Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's exploration of reasoning capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Computational Argumentation Theory",
        "canonical": "Computational Argumentation Theory",
        "aliases": [
          "CAT"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific framework used to evaluate LLMs in the context of argumentation.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Quantitative Argumentation Debate semantics",
        "canonical": "Quantitative Argumentation Debate",
        "aliases": [
          "QuAD semantics"
        ],
        "category": "unique_technical",
        "rationale": "QuAD semantics is a novel method for evaluating argument acceptability, central to the paper's methodology.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Chain-of-Thought is a prompting strategy that enhances LLM reasoning, relevant for linking to advanced instruction strategies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.77
      },
      {
        "surface": "In-Context Learning",
        "canonical": "In-Context Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "In-Context Learning is a key strategy for improving LLM performance, relevant for linking to learning methodologies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "debate",
      "dialogue",
      "discourse flow"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Computational Argumentation Theory",
      "resolved_canonical": "Computational Argumentation Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Quantitative Argumentation Debate semantics",
      "resolved_canonical": "Quantitative Argumentation Debate",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics

**Korean Title:** LLM이 토론을 평가할 수 있는가? 논증 이론 의미론을 통한 비선형 추론 평가

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15739.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15739](https://arxiv.org/abs/2509.15739)

## 🔗 유사한 논문
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (86.7% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (86.1% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (85.4% similar)
- [[2025-09-19/CLEAR_ A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models_20250919|CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models]] (85.4% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/In-Context Learning|In-Context Learning]]
**⚡ Unique Technical**: [[keywords/Computational Argumentation Theory|Computational Argumentation Theory]], [[keywords/Quantitative Argumentation Debate|Quantitative Argumentation Debate]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15739v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at linear reasoning tasks but remain underexplored on non-linear structures such as those found in natural debates, which are best expressed as argument graphs. We evaluate whether LLMs can approximate structured reasoning from Computational Argumentation Theory (CAT). Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which assigns acceptability scores to arguments based on their attack and support relations. Given only dialogue-formatted debates from two NoDE datasets, models are prompted to rank arguments without access to the underlying graph. We test several LLMs under advanced instruction strategies, including Chain-of-Thought and In-Context Learning. While models show moderate alignment with QuAD rankings, performance degrades with longer inputs or disrupted discourse flow. Advanced prompting helps mitigate these effects by reducing biases related to argument length and position. Our findings highlight both the promise and limitations of LLMs in modeling formal argumentation semantics and motivate future work on graph-aware reasoning.

## 🔍 Abstract (한글 번역)

arXiv:2509.15739v1 발표 유형: 신규  
초록: 대형 언어 모델(LLMs)은 선형 추론 작업에서 뛰어난 성능을 보이지만, 자연 토론에서 발견되는 비선형 구조에 대해서는 아직 충분히 탐구되지 않았습니다. 이러한 비선형 구조는 주로 논증 그래프로 표현됩니다. 우리는 LLMs가 계산 논증 이론(CAT)에서의 구조적 추론을 근사할 수 있는지를 평가합니다. 구체적으로, 공격 및 지원 관계에 기반하여 논증의 수용 가능성 점수를 할당하는 정량적 논증 토론(QuAD) 의미론을 사용합니다. 두 개의 NoDE 데이터셋에서 대화 형식의 토론만을 제공받은 상태에서, 모델들은 기본 그래프에 접근하지 않고 논증을 순위화하도록 요청받습니다. 우리는 연쇄 사고(Chain-of-Thought) 및 맥락 내 학습(In-Context Learning)과 같은 고급 지시 전략 하에 여러 LLMs를 테스트합니다. 모델들은 QuAD 순위와 중간 정도의 일치를 보이지만, 입력이 길어지거나 담화 흐름이 방해받을 경우 성능이 저하됩니다. 고급 프롬프트는 논증 길이와 위치에 관련된 편향을 줄임으로써 이러한 효과를 완화하는 데 도움이 됩니다. 우리의 연구 결과는 LLMs가 형식적 논증 의미론을 모델링하는 데 있어 가능성과 한계를 모두 강조하며, 그래프 인식 추론에 대한 향후 연구를 촉진합니다.

## 📝 요약

대형 언어 모델(LLM)은 선형 추론 작업에 뛰어나지만, 자연 토론에서 발견되는 비선형 구조, 특히 논증 그래프에 대한 탐구는 부족합니다. 본 연구는 LLM이 계산 논증 이론(CAT)의 구조적 추론을 근사할 수 있는지를 평가합니다. 우리는 QuAD 의미론을 사용하여 논증의 공격 및 지원 관계에 따라 수용 가능성 점수를 할당합니다. 두 개의 NoDE 데이터셋에서 대화 형식의 토론만을 제공받아, 모델이 기저 그래프 없이 논증을 순위화하도록 합니다. 여러 LLM을 체인 오브 쏘트(Chain-of-Thought) 및 인컨텍스트 러닝(In-Context Learning)과 같은 고급 지시 전략 하에 테스트했습니다. 모델은 QuAD 순위와 중간 정도의 일치를 보였으나, 입력이 길어지거나 담론 흐름이 방해받을 때 성능이 저하되었습니다. 고급 프롬프트는 논증 길이와 위치에 관련된 편향을 줄여 이러한 효과를 완화했습니다. 연구 결과는 LLM이 형식 논증 의미론을 모델링하는 데 있어 가능성과 한계를 모두 보여주며, 그래프 인식 추론에 대한 향후 연구를 촉진합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 자연 토론과 같은 비선형 구조에서의 성능이 충분히 탐구되지 않았다.
- 2. 연구는 LLM이 Computational Argumentation Theory(CAT)의 구조적 추론을 근사할 수 있는지를 평가한다.
- 3. Quantitative Argumentation Debate(QuAD) 의미론을 사용하여 모델이 그래프 없이 논쟁을 순위 매기는 능력을 테스트했다.
- 4. 고급 프롬프트 전략을 사용하여 LLM의 성능을 개선하려 했으나 긴 입력이나 흐름이 끊긴 경우 성능이 저하되었다.
- 5. 연구 결과는 LLM이 형식적 논증 의미론을 모델링하는 데 있어 가능성과 한계를 모두 보여준다.


---

*Generated on 2025-09-23 11:33:48*