---
keywords:
  - Analog In-memory Computing
  - Non-ideal Response Functions
  - Residual Learning
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2502.06309
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:04:18.572800",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Analog In-memory Computing",
    "Non-ideal Response Functions",
    "Residual Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Analog In-memory Computing": 0.78,
    "Non-ideal Response Functions": 0.72,
    "Residual Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Analog In-memory Computing",
        "canonical": "Analog In-memory Computing",
        "aliases": [
          "AIMC"
        ],
        "category": "unique_technical",
        "rationale": "Analog In-memory Computing is central to the paper's focus on energy-efficient solutions and training dynamics, offering a novel perspective on hardware-based learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Non-ideal Response Functions",
        "canonical": "Non-ideal Response Functions",
        "aliases": [
          "asymmetric response functions"
        ],
        "category": "unique_technical",
        "rationale": "The paper's investigation of non-ideal response functions is crucial for understanding the limitations and potential of AIMC hardware.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "Residual Learning Algorithm",
        "canonical": "Residual Learning",
        "aliases": [
          "Residual Learning Algorithm"
        ],
        "category": "specific_connectable",
        "rationale": "Residual Learning is proposed as a solution to overcome the challenges posed by non-ideal response functions, linking to broader machine learning optimization techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "training dynamic",
      "objective",
      "simulations"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Analog In-memory Computing",
      "resolved_canonical": "Analog In-memory Computing",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Non-ideal Response Functions",
      "resolved_canonical": "Non-ideal Response Functions",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Residual Learning Algorithm",
      "resolved_canonical": "Residual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions

**Korean Title:** ë¹„ì´ìƒì  ì €í•­ ì†Œìë¥¼ ì´ìš©í•œ ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ í•™ìŠµ: ì‘ë‹µ í•¨ìˆ˜ì˜ ì˜í–¥

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.06309.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2502.06309](https://arxiv.org/abs/2502.06309)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Training thermodynamic computers by gradient descent_20250922|Training thermodynamic computers by gradient descent]] (81.9% similar)
- [[2025-09-18/Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (81.1% similar)
- [[2025-09-22/Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size_20250922|Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size]] (80.7% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (80.0% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (79.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Residual Learning|Residual Learning]]
**âš¡ Unique Technical**: [[keywords/Analog In-memory Computing|Analog In-memory Computing]], [[keywords/Non-ideal Response Functions|Non-ideal Response Functions]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.06309v3 Announce Type: replace 
Abstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear \textit{response functions}, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.06309v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€ê·œëª¨ ë¹„ì „ ë˜ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ë° ë°°í¬ì™€ ê´€ë ¨ëœ ê²½ì œì  ë° í™˜ê²½ì  ë¹„ìš©ì´ ê¸‰ê²©íˆ ì¦ê°€í•¨ì— ë”°ë¼, ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ ì»´í“¨íŒ…(AIMC)ì€ ìœ ë§í•œ ì—ë„ˆì§€ íš¨ìœ¨ì  ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í›ˆë ¨ ê´€ì , íŠ¹íˆ í›ˆë ¨ ì—­í•™ì€ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AIMC í•˜ë“œì›¨ì–´ì—ì„œëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ê°€ ì €í•­ì„± ìš”ì†Œì˜ ì»¨ë•í„´ìŠ¤ë¡œ í‘œí˜„ë˜ë©°, ì—°ì†ì ì¸ ì „ê¸° í„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ê° í„ìŠ¤ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì»¨ë•í„´ìŠ¤ê°€ ì¼ì •í•˜ê²Œ ë³€í™”í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì„ í˜•ì ì¸ \textit{ì‘ë‹µ í•¨ìˆ˜}ì— ì˜í•´ ë³€í™”ê°€ ì¡°ì •ë˜ì–´ ë¹„ì´ìƒì ì¸ í›ˆë ¨ ì—­í•™ì´ ë°œìƒí•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ë¥¼ ê°€ì§„ AIMC í•˜ë“œì›¨ì–´ì—ì„œì˜ ê¸°ìš¸ê¸° ê¸°ë°˜ í›ˆë ¨ì„ ìœ„í•œ ì´ë¡ ì  ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¹„ëŒ€ì¹­ ì‘ë‹µ í•¨ìˆ˜ê°€ ëª©í‘œì— ì•”ë¬µì ì¸ í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ì—¬ ì•„ë‚ ë¡œê·¸ SGDì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì´ì¤‘ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ì„ê³„ì ì— ì •í™•íˆ ìˆ˜ë ´í•˜ëŠ” ê²ƒìœ¼ë¡œ ì…ì¦ëœ ì”ì°¨ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì œí•œëœ ì‘ë‹µ ì„¸ë¶„í™”ì™€ ê°™ì€ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ ê²°í•¨ì„ í•´ê²°í•˜ê¸° ìœ„í•´ í™•ì¥ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ì´ëŠ” ì¼ë°˜ì ì¸ ë¹„ì´ìƒì  ì‘ë‹µ í•¨ìˆ˜ì˜ ì˜í–¥ì„ ì¡°ì‚¬í•œ ì²« ë²ˆì§¸ ë…¼ë¬¸ì…ë‹ˆë‹¤. ê²°ë¡ ì€ ìš°ë¦¬ì˜ ì´ë¡ ì  í†µì°°ì„ ê²€ì¦í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ì— ì˜í•´ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ ì»´í“¨íŒ…(AIMC) í•˜ë“œì›¨ì–´ì—ì„œì˜ ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ê°€ í›ˆë ¨ ë™ì—­í•™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. AIMC í•˜ë“œì›¨ì–´ì—ì„œëŠ” ì €í•­ì„± ìš”ì†Œì˜ ì „ë„ë„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ í‘œí˜„í•˜ê³ , ì „ê¸° í„ìŠ¤ë¥¼ í†µí•´ ì´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì„ í˜•ì ì¸ ì‘ë‹µ í•¨ìˆ˜ë¡œ ì¸í•´ í›ˆë ¨ ë™ì—­í•™ì´ ì´ìƒì ì´ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìëŠ” ì”ì—¬ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬ ì´ì¤‘ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•¨ìœ¼ë¡œì¨ ì •í™•í•œ ìˆ˜ë ´ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‘ë‹µì˜ ì„¸ë¶„í™” ì œí•œê³¼ ê°™ì€ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ ê²°í•¨ì—ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ëŠ” ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ê°€ AIMC í›ˆë ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²˜ìŒìœ¼ë¡œ ì¡°ì‚¬í•œ ì—°êµ¬ë¡œ, ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ë¡ ì  í†µì°°ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì•„ë‚ ë¡œê·¸ ì¸ë©”ëª¨ë¦¬ ì»´í“¨íŒ…(AIMC)ì€ ëŒ€ê·œëª¨ ë¹„ì „ ë° ì–¸ì–´ ëª¨ë¸ì˜ ê²½ì œì , í™˜ê²½ì  ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•œ ì—ë„ˆì§€ íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.
- 2. AIMC í•˜ë“œì›¨ì–´ì—ì„œ ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì„ í˜•ì ì¸ ì‘ë‹µ í•¨ìˆ˜ëŠ” ì•„ë‚ ë¡œê·¸ SGDì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ì´ëŠ” ëª©í‘œì— ì•”ë¬µì ì¸ í˜ë„í‹°ë¥¼ ë¶€ê³¼í•©ë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì€ ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ê°€ ìˆëŠ” AIMC í•˜ë“œì›¨ì–´ì—ì„œì˜ ê²½ì‚¬ ê¸°ë°˜ í•™ìŠµì„ ìœ„í•œ ì´ë¡ ì  ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ Residual Learning ì•Œê³ ë¦¬ì¦˜ì€ ì´ì¤‘ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ì„ê³„ì ì— ì •í™•íˆ ìˆ˜ë ´í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤.
- 5. ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì´ ì´ë¡ ì  í†µì°°ì„ ë’·ë°›ì¹¨í•˜ë©°, ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ ê²°í•¨ì—ë„ í™•ì¥ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:04:18*