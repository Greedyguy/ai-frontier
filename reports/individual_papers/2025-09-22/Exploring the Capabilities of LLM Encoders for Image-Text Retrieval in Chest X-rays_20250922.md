---
keywords:
  - Large Language Model
  - Vision-Language Model
  - LLM2VEC4CXR
  - LLM2CLIP4CXR
  - Multimodal Learning
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15234
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:53:57.126791",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Vision-Language Model",
    "LLM2VEC4CXR",
    "LLM2CLIP4CXR",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Vision-Language Model": 0.78,
    "LLM2VEC4CXR": 0.77,
    "LLM2CLIP4CXR": 0.79,
    "Multimodal Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LLM Encoders",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Model"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's exploration of image-text retrieval, offering a strong link to existing NLP research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision-language pretraining",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-language",
          "VL Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language models are pivotal in aligning image and text data, directly relevant to the paper's focus on radiology.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "LLM2VEC4CXR",
        "canonical": "LLM2VEC4CXR",
        "aliases": [
          "LLM2VEC"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel model introduced in the paper, crucial for understanding its unique contributions to clinical text understanding.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.89,
        "link_intent_score": 0.77
      },
      {
        "surface": "LLM2CLIP4CXR",
        "canonical": "LLM2CLIP4CXR",
        "aliases": [
          "LLM2CLIP"
        ],
        "category": "unique_technical",
        "rationale": "Another novel model from the paper, enhancing retrieval accuracy and generalization, key for linking to multimodal learning.",
        "novelty_score": 0.9,
        "connectivity_score": 0.68,
        "specificity_score": 0.87,
        "link_intent_score": 0.79
      },
      {
        "surface": "Multimodal learning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "The paper's focus on integrating text and image data aligns with multimodal learning, a trending topic in AI research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "heterogeneity",
      "scaling",
      "robustness"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LLM Encoders",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision-language pretraining",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LLM2VEC4CXR",
      "resolved_canonical": "LLM2VEC4CXR",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.89,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "LLM2CLIP4CXR",
      "resolved_canonical": "LLM2CLIP4CXR",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.68,
        "specificity": 0.87,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Multimodal learning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays

**Korean Title:** í‰ë¶€ X-ì„ ì—ì„œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ìœ„í•œ LLM ì¸ì½”ë”ì˜ ê¸°ëŠ¥ íƒêµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15234.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15234](https://arxiv.org/abs/2509.15234)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/RegionMed-CLIP_ A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding_20250922|RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding]] (86.1% similar)
- [[2025-09-22/Data-Efficient Learning for Generalizable Surgical Video Understanding_20250922|Data-Efficient Learning for Generalizable Surgical Video Understanding]] (84.4% similar)
- [[2025-09-19/Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model_20250919|Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model]] (84.3% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (84.0% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/LLM2VEC4CXR|LLM2VEC4CXR]], [[keywords/LLM2CLIP4CXR|LLM2CLIP4CXR]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15234v1 Announce Type: new 
Abstract: Vision-language pretraining has advanced image-text alignment, yet progress in radiology remains constrained by the heterogeneity of clinical reports, including abbreviations, impression-only notes, and stylistic variability. Unlike general-domain settings where more data often leads to better performance, naively scaling to large collections of noisy reports can plateau or even degrade model learning. We ask whether large language model (LLM) encoders can provide robust clinical representations that transfer across diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR, a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a dual-tower framework that couples this encoder with a vision backbone. LLM2VEC4CXR improves clinical text understanding over BERT-based baselines, handles abbreviations and style variation, and achieves strong clinical alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to boost retrieval accuracy and clinically oriented scores, with stronger cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M CXR studies from public and private sources with heterogeneous and noisy reports, our models demonstrate that robustness -- not scale alone -- is the key to effective multimodal learning. We release models to support further research in medical image-text representation learning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15234v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë¹„ì „-ì–¸ì–´ ì‚¬ì „ í•™ìŠµì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì„ ë°œì „ì‹œì¼°ì§€ë§Œ, ë°©ì‚¬ì„ í•™ ë¶„ì•¼ì—ì„œëŠ” ì„ìƒ ë³´ê³ ì„œì˜ ì´ì§ˆì„±, ì•½ì–´, ì¸ìƒë§Œì„ ê¸°ë¡í•œ ë…¸íŠ¸, ìŠ¤íƒ€ì¼ì˜ ë‹¤ì–‘ì„± ë“±ìœ¼ë¡œ ì¸í•´ ì§„ì „ì´ ì œí•œì ì…ë‹ˆë‹¤. ì¼ë°˜ ë„ë©”ì¸ ì„¤ì •ì—ì„œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ ì¢…ì¢… ë” ë‚˜ì€ ì„±ëŠ¥ìœ¼ë¡œ ì´ì–´ì§€ì§€ë§Œ, ì¡ìŒì´ ë§ì€ ë³´ê³ ì„œì˜ ëŒ€ê·œëª¨ ì»¬ë ‰ì…˜ì„ ë‹¨ìˆœíˆ í™•ì¥í•˜ëŠ” ê²ƒì€ ëª¨ë¸ í•™ìŠµì„ ì •ì²´ì‹œí‚¤ê±°ë‚˜ ì˜¤íˆë ¤ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ì¸ì½”ë”ê°€ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì— ê±¸ì³ ì „ì´ ê°€ëŠ¥í•œ ê°•ë ¥í•œ ì„ìƒ í‘œí˜„ì„ ì œê³µí•˜ê³  ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì„ ë” ì˜ ì•ˆë‚´í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë¬»ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í‰ë¶€ Xì„  ë³´ê³ ì„œë¥¼ ìœ„í•œ ë„ë©”ì¸ ì ì‘í˜• LLM ì¸ì½”ë”ì¸ LLM2VEC4CXRê³¼ ì´ ì¸ì½”ë”ë¥¼ ë¹„ì „ ë°±ë³¸ê³¼ ê²°í•©í•œ ì´ì¤‘ íƒ€ì›Œ í”„ë ˆì„ì›Œí¬ì¸ LLM2CLIP4CXRì„ ì†Œê°œí•©ë‹ˆë‹¤. LLM2VEC4CXRì€ BERT ê¸°ë°˜ì˜ ê¸°ì¤€ ëª¨ë¸ë“¤ë³´ë‹¤ ì„ìƒ í…ìŠ¤íŠ¸ ì´í•´ë¥¼ ê°œì„ í•˜ê³ , ì•½ì–´ì™€ ìŠ¤íƒ€ì¼ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ë©°, ë³´ê³ ì„œ ìˆ˜ì¤€ì˜ ì§€í‘œì—ì„œ ê°•ë ¥í•œ ì„ìƒ ì •ë ¬ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. LLM2CLIP4CXRì€ ì´ëŸ¬í•œ ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ì™€ ì„ìƒ ì§€í–¥ ì ìˆ˜ë¥¼ í–¥ìƒì‹œí‚¤ë©°, ì´ì „ì˜ ì˜ë£Œ CLIP ë³€í˜•ë³´ë‹¤ ë” ê°•ë ¥í•œ êµì°¨ ë°ì´í„°ì…‹ ì¼ë°˜í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê³µê³µ ë° ë¯¼ê°„ ì¶œì²˜ì—ì„œ ì´ì§ˆì ì´ê³  ì¡ìŒì´ ë§ì€ ë³´ê³ ì„œë¥¼ í¬í•¨í•œ 160ë§Œ ê±´ì˜ CXR ì—°êµ¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨ëœ ìš°ë¦¬ì˜ ëª¨ë¸ì€ íš¨ê³¼ì ì¸ ë‹¤ì¤‘ ëª¨ë“œ í•™ìŠµì˜ í•µì‹¬ì´ ê·œëª¨ê°€ ì•„ë‹Œ ê²¬ê³ ì„±ì„ì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì˜ë£Œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í‘œí˜„ í•™ìŠµì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ê³µê°œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë°©ì‚¬ì„ í•™ ë¶„ì•¼ì—ì„œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì¸ì½”ë”ë¥¼ í™œìš©í•œ ì—°êµ¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì¼ë°˜ ë„ë©”ì¸ê³¼ ë‹¬ë¦¬, ë°©ì‚¬ì„ í•™ì—ì„œëŠ” ì„ìƒ ë³´ê³ ì„œì˜ ì´ì§ˆì„± ë•Œë¬¸ì— ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ í™•ì¥í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ í‰ë¶€ X-ray ë³´ê³ ì„œì— íŠ¹í™”ëœ LLM ì¸ì½”ë”ì¸ LLM2VEC4CXRì™€ ì´ë¥¼ ë¹„ì „ ë°±ë³¸ê³¼ ê²°í•©í•œ ì´ì¤‘ íƒ€ì›Œ í”„ë ˆì„ì›Œí¬ì¸ LLM2CLIP4CXRë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. LLM2VEC4CXRëŠ” BERT ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ì„ìƒ í…ìŠ¤íŠ¸ ì´í•´ë¥¼ ê°œì„ í•˜ê³ , ì•½ì–´ ë° ìŠ¤íƒ€ì¼ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ë©°, ë³´ê³ ì„œ ìˆ˜ì¤€ì˜ ì„ìƒ ì •ë ¬ì„ ê°•í™”í•©ë‹ˆë‹¤. LLM2CLIP4CXRëŠ” ì´ëŸ¬í•œ ì„ë² ë”©ì„ í™œìš©í•´ ê²€ìƒ‰ ì •í™•ë„ì™€ ì„ìƒ ì§€í–¥ ì ìˆ˜ë¥¼ í–¥ìƒì‹œí‚¤ë©°, ì´ì „ì˜ ì˜ë£Œ CLIP ë³€í˜•ë³´ë‹¤ ê°•ë ¥í•œ ë°ì´í„°ì…‹ ê°„ ì¼ë°˜í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. 160ë§Œ ê±´ì˜ ë‹¤ì–‘í•œ ì¶œì²˜ì˜ X-ray ì—°êµ¬ë¥¼ í†µí•´ í›ˆë ¨ëœ ì´ ëª¨ë¸ë“¤ì€ ê·œëª¨ë³´ë‹¤ëŠ” ê°•ê±´í•¨ì´ íš¨ê³¼ì ì¸ ë©€í‹°ëª¨ë‹¬ í•™ìŠµì˜ í•µì‹¬ì„ì„ ì…ì¦í•˜ë©°, ì˜ë£Œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í‘œí˜„ í•™ìŠµ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê³µê°œë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë°©ì‚¬ì„ í•™ ë¶„ì•¼ì—ì„œ ì„ìƒ ë³´ê³ ì„œì˜ ì´ì§ˆì„± ë•Œë¬¸ì— ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì˜ ë°œì „ì´ ì œí•œë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. LLM2VEC4CXRëŠ” í‰ë¶€ Xì„  ë³´ê³ ì„œë¥¼ ìœ„í•œ ë„ë©”ì¸ ì ì‘í˜• LLM ì¸ì½”ë”ë¡œ, BERT ê¸°ë°˜ì˜ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ì„ìƒ í…ìŠ¤íŠ¸ ì´í•´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 3. LLM2CLIP4CXRëŠ” LLM2VEC4CXR ì¸ì½”ë”ì™€ ë¹„ì „ ë°±ë³¸ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ì™€ ì„ìƒ ì§€í–¥ ì ìˆ˜ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ì´ í•­ìƒ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§€ì§€ ì•Šìœ¼ë©°, ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì— ê±¸ì³ ì „ì´ ê°€ëŠ¥í•œ ê°•ë ¥í•œ ì„ìƒ í‘œí˜„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
- 5. ëª¨ë¸ì€ 160ë§Œ ê°œì˜ CXR ì—°êµ¬ì—ì„œ í›ˆë ¨ë˜ì–´, ê°•ë ¥í•œ í¬ë¡œìŠ¤-ë°ì´í„°ì…‹ ì¼ë°˜í™”ë¥¼ ë³´ì—¬ì£¼ë©°, ì˜ë£Œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í‘œí˜„ í•™ìŠµ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê³µê°œë©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 11:53:57*