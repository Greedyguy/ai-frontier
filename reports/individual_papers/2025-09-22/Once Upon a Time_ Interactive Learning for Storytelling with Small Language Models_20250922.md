---
keywords:
  - Interactive Learning
  - Storytelling
  - Cognitively Inspired Feedback
  - Large Language Model
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15714
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:11:23.898204",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Interactive Learning",
    "Storytelling",
    "Cognitively Inspired Feedback",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Interactive Learning": 0.82,
    "Storytelling": 0.75,
    "Cognitively Inspired Feedback": 0.78,
    "Large Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Interactive Learning",
        "canonical": "Interactive Learning",
        "aliases": [
          "Interactive Training"
        ],
        "category": "specific_connectable",
        "rationale": "Interactive learning is a key aspect of the paper's approach, offering a novel perspective on language model training.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Storytelling",
        "canonical": "Storytelling",
        "aliases": [
          "Narrative Generation"
        ],
        "category": "unique_technical",
        "rationale": "Storytelling is the primary application explored, providing a unique context for language model evaluation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Cognitively Inspired Feedback",
        "canonical": "Cognitively Inspired Feedback",
        "aliases": [
          "Cognitive Feedback"
        ],
        "category": "unique_technical",
        "rationale": "This feedback mechanism is a novel approach to improving language models, distinct from traditional methods.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Model"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are central to the paper's investigation, providing a basis for comparison.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "next-word prediction",
      "teacher model",
      "student model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Interactive Learning",
      "resolved_canonical": "Interactive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Storytelling",
      "resolved_canonical": "Storytelling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Cognitively Inspired Feedback",
      "resolved_canonical": "Cognitively Inspired Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Once Upon a Time: Interactive Learning for Storytelling with Small Language Models

**Korean Title:** ì˜›ë‚  ì˜›ì ì—: ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ìŠ¤í† ë¦¬í…”ë§ì„ ìœ„í•œ ìƒí˜¸ì‘ìš© í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15714.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15714](https://arxiv.org/abs/2509.15714)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (82.0% similar)
- [[2025-09-19/From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery: Reinforced Distillation of Large Language Model Agents]] (81.7% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (81.4% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (81.2% similar)
- [[2025-09-22/Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation_20250922|Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Interactive Learning|Interactive Learning]]
**âš¡ Unique Technical**: [[keywords/Storytelling|Storytelling]], [[keywords/Cognitively Inspired Feedback|Cognitively Inspired Feedback]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15714v1 Announce Type: cross 
Abstract: Children efficiently acquire language not just by listening, but by interacting with others in their social environment. Conversely, large language models are typically trained with next-word prediction on massive amounts of text. Motivated by this contrast, we investigate whether language models can be trained with less data by learning not only from next-word prediction but also from high-level, cognitively inspired feedback. We train a student model to generate stories, which a teacher model rates on readability, narrative coherence, and creativity. By varying the amount of pretraining before the feedback loop, we assess the impact of this interactive learning on formal and functional linguistic competence. We find that the high-level feedback is highly data efficient: With just 1 M words of input in interactive learning, storytelling skills can improve as much as with 410 M words of next-word prediction.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15714v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì–´ë¦°ì´ëŠ” ë‹¨ìˆœíˆ ë“£ëŠ” ê²ƒë§Œìœ¼ë¡œ ì–¸ì–´ë¥¼ ìŠµë“í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬íšŒì  í™˜ê²½ì—ì„œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ ìƒí˜¸ì‘ìš©í•¨ìœ¼ë¡œì¨ íš¨ìœ¨ì ìœ¼ë¡œ ì–¸ì–´ë¥¼ ìŠµë“í•©ë‹ˆë‹¤. ë°˜ë©´ì—, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ í†µí•´ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ ëŒ€ì¡°ì—ì„œ ì˜ê°ì„ ë°›ì•„, ìš°ë¦¬ëŠ” ì–¸ì–´ ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ë¿ë§Œ ì•„ë‹ˆë¼ ê³ ì°¨ì›ì ì´ê³  ì¸ì§€ì ìœ¼ë¡œ ì˜ê°ì„ ë°›ì€ í”¼ë“œë°±ì„ í†µí•´ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë” ì ì€ ë°ì´í„°ë¡œ í›ˆë ¨ë  ìˆ˜ ìˆëŠ”ì§€ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í•™ìƒ ëª¨ë¸ì´ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ë„ë¡ í›ˆë ¨í•˜ê³ , êµì‚¬ ëª¨ë¸ì´ ê°€ë…ì„±, ì„œì‚¬ì  ì¼ê´€ì„± ë° ì°½ì˜ì„±ì— ëŒ€í•´ ì´ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. í”¼ë“œë°± ë£¨í”„ ì´ì „ì˜ ì‚¬ì „ í›ˆë ¨ ì–‘ì„ ì¡°ì •í•˜ì—¬, ì´ëŸ¬í•œ ìƒí˜¸ì‘ìš© í•™ìŠµì´ í˜•ì‹ì  ë° ê¸°ëŠ¥ì  ì–¸ì–´ ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê³ ì°¨ì› í”¼ë“œë°±ì´ ë§¤ìš° ë°ì´í„° íš¨ìœ¨ì ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: ìƒí˜¸ì‘ìš© í•™ìŠµì—ì„œ ë‹¨ 100ë§Œ ë‹¨ì–´ì˜ ì…ë ¥ë§Œìœ¼ë¡œë„ ì´ì•¼ê¸° ê¸°ìˆ ì´ 4ì–µ 1ì²œë§Œ ë‹¨ì–´ì˜ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ë§Œí¼ í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì•„ë™ì´ ì‚¬íšŒì  ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì–¸ì–´ë¥¼ ìŠµë“í•˜ëŠ” ë°©ì‹ê³¼ ëŒ€ì¡°ì ìœ¼ë¡œ, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¥¼ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì ì— ì£¼ëª©í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” ì–¸ì–´ ëª¨ë¸ì´ ê³ ì°¨ì›ì ì¸ ì¸ì§€ í”¼ë“œë°±ì„ í†µí•´ ë” ì ì€ ë°ì´í„°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. í•™ìƒ ëª¨ë¸ì´ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ë©´, êµì‚¬ ëª¨ë¸ì´ ê°€ë…ì„±, ì„œì‚¬ì  ì¼ê´€ì„±, ì°½ì˜ì„± ì¸¡ë©´ì—ì„œ ì´ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëŸ‰ì„ ì¡°ì ˆí•˜ì—¬ ìƒí˜¸ì‘ìš© í•™ìŠµì´ ì–¸ì–´ì  ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•œ ê²°ê³¼, 100ë§Œ ë‹¨ì–´ì˜ ìƒí˜¸ì‘ìš© í•™ìŠµìœ¼ë¡œë„ 4ì–µ 1ì²œë§Œ ë‹¨ì–´ì˜ ì˜ˆì¸¡ í•™ìŠµê³¼ ë™ì¼í•œ ìˆ˜ì¤€ì˜ ì´ì•¼ê¸° ìƒì„± ëŠ¥ë ¥ í–¥ìƒì„ ì´ë£° ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–´ë¦°ì´ëŠ” ì‚¬íšŒì  í™˜ê²½ì—ì„œ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì–¸ì–´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìŠµë“í•œë‹¤ëŠ” ì ì„ ë°”íƒ•ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆë‹¤.
- 2. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì£¼ë¡œ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¥¼ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ìœ¼ë¡œ í›ˆë ¨ë˜ì§€ë§Œ, ë³¸ ì—°êµ¬ëŠ” ì¸ì§€ì ìœ¼ë¡œ ì˜ê°ì„ ë°›ì€ í”¼ë“œë°±ì„ í†µí•´ ë” ì ì€ ë°ì´í„°ë¡œ í›ˆë ¨í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•˜ì˜€ë‹¤.
- 3. í•™ìƒ ëª¨ë¸ì´ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ê³ , êµì‚¬ ëª¨ë¸ì´ ê°€ë…ì„±, ì„œì‚¬ì  ì¼ê´€ì„±, ì°½ì˜ì„± ì¸¡ë©´ì—ì„œ ì´ë¥¼ í‰ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì´ ì§„í–‰ë˜ì—ˆë‹¤.
- 4. ìƒí˜¸ì‘ìš© í•™ìŠµì—ì„œ ê³ ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì€ ë§¤ìš° ë°ì´í„° íš¨ìœ¨ì ì´ë©°, 100ë§Œ ë‹¨ì–´ì˜ ì…ë ¥ë§Œìœ¼ë¡œë„ 4ì–µ 1ì²œë§Œ ë‹¨ì–´ì˜ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ê³¼ ë™ì¼í•œ ìˆ˜ì¤€ì˜ ì´ì•¼ê¸° ìƒì„± ëŠ¥ë ¥ í–¥ìƒì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆì—ˆë‹¤.
- 5. í”¼ë“œë°± ë£¨í”„ ì´ì „ì˜ ì‚¬ì „ í›ˆë ¨ ì–‘ì„ ì¡°ì ˆí•˜ì—¬ ìƒí˜¸ì‘ìš© í•™ìŠµì´ ì–¸ì–´ì  ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-23 09:11:23*