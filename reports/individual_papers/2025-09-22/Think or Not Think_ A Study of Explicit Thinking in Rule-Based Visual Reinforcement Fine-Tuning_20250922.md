---
keywords:
  - Rule-Based Reinforcement Fine-Tuning
  - Explicit Thinking Process
  - Multimodal Large Language Models
  - Adaptive-Thinking Method
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2503.16188
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:32:33.069511",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Rule-Based Reinforcement Fine-Tuning",
    "Explicit Thinking Process",
    "Multimodal Large Language Models",
    "Adaptive-Thinking Method"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Rule-Based Reinforcement Fine-Tuning": 0.78,
    "Explicit Thinking Process": 0.77,
    "Multimodal Large Language Models": 0.8,
    "Adaptive-Thinking Method": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "rule-based reinforcement fine-tuning",
        "canonical": "Rule-Based Reinforcement Fine-Tuning",
        "aliases": [
          "RFT"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's exploration of explicit thinking in reinforcement learning, offering a unique perspective on model training.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "explicit thinking process",
        "canonical": "Explicit Thinking Process",
        "aliases": [
          "Explicit Thinking"
        ],
        "category": "unique_technical",
        "rationale": "The paper challenges the necessity of explicit thinking in reinforcement learning, making it a key concept for understanding the study's findings.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Large Language Models",
        "aliases": [
          "MLLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "The study focuses on MLLMs, which are a significant evolution in AI, linking to broader discussions on multimodal learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adaptive-Thinking method",
        "canonical": "Adaptive-Thinking Method",
        "aliases": [
          "Adaptive Thinking"
        ],
        "category": "unique_technical",
        "rationale": "This method represents an innovative approach to dynamically deciding when models should engage in explicit thinking.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "CLS-RL",
      "No-Thinking-RL",
      "Think-After-Answer"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "rule-based reinforcement fine-tuning",
      "resolved_canonical": "Rule-Based Reinforcement Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "explicit thinking process",
      "resolved_canonical": "Explicit Thinking Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adaptive-Thinking method",
      "resolved_canonical": "Adaptive-Thinking Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning

**Korean Title:** ìƒê°í•  ê²ƒì¸ê°€, ìƒê°í•˜ì§€ ì•Šì„ ê²ƒì¸ê°€: ê·œì¹™ ê¸°ë°˜ ì‹œê°ì  ê°•í™” í•™ìŠµ ë¯¸ì„¸ ì¡°ì •ì—ì„œì˜ ëª…ì‹œì  ì‚¬ê³ ì— ëŒ€í•œ ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2503.16188.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2503.16188](https://arxiv.org/abs/2503.16188)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (84.7% similar)
- [[2025-09-22/Pointing to a Llama and Call it a Camel_ On the Sycophancy of Multimodal Large Language Models_20250922|Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models]] (83.3% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (82.6% similar)
- [[2025-09-22/RePIC_ Reinforced Post-Training for Personalizing Multi-Modal Language Models_20250922|RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models]] (82.5% similar)
- [[2025-09-19/ThinkAct_ Vision-Language-Action Reasoning via Reinforced Visual Latent Planning_20250919|ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/Rule-Based Reinforcement Fine-Tuning|Rule-Based Reinforcement Fine-Tuning]], [[keywords/Explicit Thinking Process|Explicit Thinking Process]], [[keywords/Adaptive-Thinking Method|Adaptive-Thinking Method]]
**ğŸš€ Evolved Concepts**: [[keywords/Multimodal Large Language Models|Multimodal Large Language Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.16188v5 Announce Type: replace 
Abstract: This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM image classification, using verifiable rewards for fine-tuning. Experiments show CLS-RL significantly outperforms SFT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RL on 6 diverse tasks across different model sizes and types. Experimental results reveal three key findings: 1). Visual perception tasks do not require thinking during RFT, as No-Thinking-RL consistently outperforms or matches Thinking-based RFT across model sizes. 2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-based RFT less effective than No-Thinking-RL. 3). There are inconsistencies between the answers in the thinking and answer tags for some responses of thinking-based RFT, which show lower accuracy than the overall accuracy. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance. To test this hypothesis, we propose Think-After-Answer, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an Adaptive-Thinking method. Experiments show that it converges to a specific prompt depending on model capability and task complexity, achieving comparable or better performance than both Thinking and No-Thinking-RL. This suggests MLLMs can adaptively decide to think or not based on their capabilities and task complexity.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2503.16188v5 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì´ ë…¼ë¬¸ì€ MLLMì˜ ê·œì¹™ ê¸°ë°˜ ê°•í™” ë¯¸ì„¸ ì¡°ì •(RFT)ì—ì„œ ëª…ì‹œì  ì‚¬ê³  ê³¼ì •ì˜ ì—­í• ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¨¼ì € ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì„ ì‚¬ìš©í•˜ì—¬ MLLM ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ CLS-RLì„ ì œì•ˆí•˜ê³ , ì´ë¥¼ í†µí•´ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ CLS-RLì€ SFTë³´ë‹¤ ìƒë‹¹íˆ ìš°ìˆ˜í•˜ë©°, ë°ì´í„°ì…‹ ê°„ ì¼ë°˜í™” íš¨ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ RFTì—ì„œ ëª…ì‹œì  ì‚¬ê³ ê°€ í•­ìƒ í•„ìš”í•œì§€ì— ëŒ€í•´ ì¬ê³ í•˜ê³  ì˜ë¬¸ì„ ì œê¸°í•©ë‹ˆë‹¤. RFTì˜ ì„±ê³µì— ëª…ì‹œì  ì‚¬ê³ ê°€ í•„ìˆ˜ì ì´ë¼ëŠ” ê¸°ì¡´ ê´€ë…ì— ë„ì „í•˜ë©°, ë‹¨ìˆœí•œ í‰ë“± ì •í™•ë„ ë³´ìƒì„ ë„ì…í•˜ì—¬ ì‚¬ê³  ì—†ì´ RFTë¥¼ íƒêµ¬í•˜ëŠ” No-Thinking-RLì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì™€ ìœ í˜•ì— ê±¸ì³ 6ê°œì˜ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ No-Thinking-RLì„ í‰ê°€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë°œê²¬ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤: 1). ì‹œê°ì  ì¸ì‹ ì‘ì—…ì€ RFT ë™ì•ˆ ì‚¬ê³ ë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•Šìœ¼ë©°, No-Thinking-RLì´ ëª¨ë¸ í¬ê¸°ì— ê´€ê³„ì—†ì´ ì‚¬ê³  ê¸°ë°˜ RFTë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•˜ê±°ë‚˜ ë™ë“±í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. 2). ì œí•œëœ ëŠ¥ë ¥ì„ ê°€ì§„ ëª¨ë¸ì€ RFTë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ CoTë¥¼ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì–´, ì‚¬ê³  ê¸°ë°˜ RFTê°€ No-Thinking-RLë³´ë‹¤ ëœ íš¨ê³¼ì ì…ë‹ˆë‹¤. 3). ì‚¬ê³  ê¸°ë°˜ RFTì˜ ì¼ë¶€ ì‘ë‹µì— ëŒ€í•´ ì‚¬ê³ ì™€ ë‹µë³€ íƒœê·¸ ê°„ì˜ ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©°, ì´ëŠ” ì „ì²´ ì •í™•ë„ë³´ë‹¤ ë‚®ì€ ì •í™•ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê²€ì¦ ê°€ëŠ¥í•œ ë‹µë³€ ì´ì „ì˜ ëª…ì‹œì  ì‚¬ê³ ê°€ ë³´ìƒ ìˆ˜ë ´ì„ ë°©í•´í•˜ê³  ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤ê³  ê°€ì„¤í•©ë‹ˆë‹¤. ì´ ê°€ì„¤ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì‹¤í—˜ì  ê²€ì¦ì„ ìœ„í•´ ë‹µë³€ í›„ ì‚¬ê³ ë¥¼ ë°°ì¹˜í•˜ì—¬ ì´ íš¨ê³¼ë¥¼ ì™„í™”í•˜ëŠ” Think-After-Answerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, MLLMì´ RFT ë™ì•ˆ ì–¸ì œ ì‚¬ê³ í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€ íƒêµ¬í•˜ê¸° ìœ„í•´ Adaptive-Thinking ë°©ë²•ì„ ë„ì…í•˜ëŠ” íŒŒì¼ëŸ¿ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ëª¨ë¸ì˜ ëŠ¥ë ¥ê³¼ ì‘ì—… ë³µì¡ì„±ì— ë”°ë¼ íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¡œ ìˆ˜ë ´í•˜ë©°, ì‚¬ê³  ë° No-Thinking-RLë³´ë‹¤ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” MLLMì´ ìì‹ ì˜ ëŠ¥ë ¥ê³¼ ì‘ì—… ë³µì¡ì„±ì— ë”°ë¼ ì‚¬ê³  ì—¬ë¶€ë¥¼ ì ì‘ì ìœ¼ë¡œ ê²°ì •í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê·œì¹™ ê¸°ë°˜ ê°•í™” í•™ìŠµ(RFT)ì—ì„œ ëª…ì‹œì  ì‚¬ê³  ê³¼ì •ì˜ ì—­í• ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. MLLM ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ CLS-RLì„ ì œì•ˆí•˜ì—¬ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì„ ì‚¬ìš©í•´ ë¯¸ì„¸ ì¡°ì •í•˜ë©°, ì‹¤í—˜ ê²°ê³¼ CLS-RLì´ SFTë³´ë‹¤ ë›°ì–´ë‚˜ê³  ë°ì´í„°ì…‹ ê°„ ì¼ë°˜í™” íš¨ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, RFTì—ì„œ ëª…ì‹œì  ì‚¬ê³ ê°€ í•­ìƒ í•„ìš”í•œì§€ì— ëŒ€í•´ ì˜ë¬¸ì„ ì œê¸°í•˜ë©°, No-Thinking-RLì„ ë„ì…í•´ ì‚¬ê³  ì—†ì´ RFTë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì‹œê°ì  ì¸ì‹ ì‘ì—…ì—ì„œëŠ” ì‚¬ê³ ê°€ í•„ìš” ì—†ìœ¼ë©°, ì œí•œëœ ëª¨ë¸ì€ ê³ í’ˆì§ˆì˜ ì‚¬ê³ ë¥¼ ìƒì„±í•˜ê¸° ì–´ë ¤ì›Œ No-Thinking-RLì´ ë” íš¨ê³¼ì ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ê³  ê¸°ë°˜ RFTì˜ ì‘ë‹µì—ì„œ ì¼ê´€ì„±ì´ ë–¨ì–´ì§€ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ì´ëŠ” ë³´ìƒ ìˆ˜ë ´ì„ ë°©í•´í•  ìˆ˜ ìˆìŒì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Think-After-Answer ë°©ë²•ì„ ì œì•ˆí•˜ë©°, MLLMì´ RFT ì¤‘ ì–¸ì œ ì‚¬ê³ í• ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” Adaptive-Thinking ë°©ë²•ì„ ë„ì…í•˜ì—¬, ëª¨ë¸ì˜ ëŠ¥ë ¥ê³¼ ì‘ì—… ë³µì¡ì„±ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ì‚¬ê³  ì—¬ë¶€ë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CLS-RLì„ ì‚¬ìš©í•œ MLLM ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œ ëª…ì‹œì  ì‚¬ê³  ê³¼ì •ì˜ í•„ìš”ì„±ì„ ì¬ê³ í•˜ë©°, CLS-RLì´ SFTë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ë°ì´í„°ì…‹ ê°„ ì¼ë°˜í™” íš¨ê³¼ë¥¼ ë‚˜íƒ€ëƒ„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- 2. No-Thinking-RLì„ ë„ì…í•˜ì—¬, ëª…ì‹œì  ì‚¬ê³  ì—†ì´ RFTë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì™€ ìœ í˜•ì˜ ì‹œê°ì  ì¸ì‹ ì‘ì—…ì—ì„œ Thinking ê¸°ë°˜ RFTë¥¼ ëŠ¥ê°€í•˜ê±°ë‚˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 3. ì œí•œëœ ëŠ¥ë ¥ì„ ê°€ì§„ ëª¨ë¸ì€ RFTë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ CoT ìƒì„±ì— ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ì´ëŠ” Thinking ê¸°ë°˜ RFTë³´ë‹¤ No-Thinking-RLì´ ë” íš¨ê³¼ì ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
- 4. Thinking ê¸°ë°˜ RFTì˜ ì‘ë‹µì—ì„œ ì‚¬ê³ ì™€ ë‹µë³€ íƒœê·¸ ê°„ì˜ ë¶ˆì¼ì¹˜ê°€ ë°œìƒí•˜ë©°, ì´ëŠ” ì „ì²´ ì •í™•ë„ë³´ë‹¤ ë‚®ì€ ì •í™•ë„ë¥¼ ë³´ì…ë‹ˆë‹¤.
- 5. Adaptive-Thinking ë°©ë²•ì„ í†µí•´ MLLMì´ RFT ì¤‘ ì–¸ì œ ì‚¬ê³ í• ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ íƒêµ¬í•˜ì˜€ìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ê³¼ ì‘ì—… ë³µì¡ì„±ì— ë”°ë¼ íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¡œ ìˆ˜ë ´í•˜ì—¬ Thinking ë° No-Thinking-RLë³´ë‹¤ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:32:33*