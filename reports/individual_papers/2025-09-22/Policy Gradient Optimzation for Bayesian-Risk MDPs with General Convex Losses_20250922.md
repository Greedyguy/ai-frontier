---
keywords:
  - Markov Decision Process
  - Bayesian Inference
  - Coherent Risk Measure
  - Policy Gradient Method
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15509
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:27:53.666476",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Markov Decision Process",
    "Bayesian Inference",
    "Coherent Risk Measure",
    "Policy Gradient Method"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Markov Decision Process": 0.8,
    "Bayesian Inference": 0.78,
    "Coherent Risk Measure": 0.75,
    "Policy Gradient Method": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Markov decision processes",
        "canonical": "Markov Decision Process",
        "aliases": [
          "MDP",
          "Markov process"
        ],
        "category": "broad_technical",
        "rationale": "Markov Decision Processes are foundational in decision-making models and connect well with reinforcement learning frameworks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Bayesian approach",
        "canonical": "Bayesian Inference",
        "aliases": [
          "Bayesian method",
          "Bayesian estimation"
        ],
        "category": "specific_connectable",
        "rationale": "Bayesian Inference is crucial for handling uncertainty in model parameters, linking well with probabilistic models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "coherent risk functional",
        "canonical": "Coherent Risk Measure",
        "aliases": [
          "risk measure",
          "risk functional"
        ],
        "category": "unique_technical",
        "rationale": "Coherent Risk Measures are important in financial and decision-theoretic contexts, offering unique insights into risk management.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "policy gradient optimization",
        "canonical": "Policy Gradient Method",
        "aliases": [
          "policy optimization",
          "gradient-based policy"
        ],
        "category": "specific_connectable",
        "rationale": "Policy Gradient Methods are central to reinforcement learning, facilitating connections with optimization techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "general loss function",
      "unknown parameters",
      "dynamic programming"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Markov decision processes",
      "resolved_canonical": "Markov Decision Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Bayesian approach",
      "resolved_canonical": "Bayesian Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "coherent risk functional",
      "resolved_canonical": "Coherent Risk Measure",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "policy gradient optimization",
      "resolved_canonical": "Policy Gradient Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses

**Korean Title:** ë² ì´ì§€ì•ˆ ìœ„í—˜ MDPì˜ ì •ì±… ê²½ì‚¬ ìµœì í™”: ì¼ë°˜ì ì¸ ë³¼ë¡ ì†ì‹¤ì„ ê³ ë ¤í•˜ì—¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15509.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15509](https://arxiv.org/abs/2509.15509)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Online Bayesian Risk-Averse Reinforcement Learning_20250917|Online Bayesian Risk-Averse Reinforcement Learning]] (85.7% similar)
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (83.0% similar)
- [[2025-09-22/Gaussian process policy iteration with additive Schwarz acceleration for forward and inverse HJB and mean field game problems_20250922|Gaussian process policy iteration with additive Schwarz acceleration for forward and inverse HJB and mean field game problems]] (82.1% similar)
- [[2025-09-19/Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks_20250919|Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks]] (81.6% similar)
- [[2025-09-18/Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain_20250918|Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Markov Decision Process|Markov Decision Process]]
**ğŸ”— Specific Connectable**: [[keywords/Bayesian Inference|Bayesian Inference]], [[keywords/Policy Gradient Method|Policy Gradient Method]]
**âš¡ Unique Technical**: [[keywords/Coherent Risk Measure|Coherent Risk Measure]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15509v1 Announce Type: new 
Abstract: Motivated by many application problems, we consider Markov decision processes (MDPs) with a general loss function and unknown parameters. To mitigate the epistemic uncertainty associated with unknown parameters, we take a Bayesian approach to estimate the parameters from data and impose a coherent risk functional (with respect to the Bayesian posterior distribution) on the loss. Since this formulation usually does not satisfy the interchangeability principle, it does not admit Bellman equations and cannot be solved by approaches based on dynamic programming. Therefore, We propose a policy gradient optimization method, leveraging the dual representation of coherent risk measures and extending the envelope theorem to continuous cases. We then show the stationary analysis of the algorithm with a convergence rate of $O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations and $r$ is the sample size of the gradient estimator. We further extend our algorithm to an episodic setting, and establish the global convergence of the extended algorithm and provide bounds on the number of iterations needed to achieve an error bound $O(\epsilon)$ in each episode.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15509v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì–‘í•œ ì‘ìš© ë¬¸ì œì— ë™ê¸° ë¶€ì—¬ë˜ì–´, ìš°ë¦¬ëŠ” ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ ê³ ë ¤í•©ë‹ˆë‹¤. ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ì™€ ê´€ë ¨ëœ ì¸ì‹ë¡ ì  ë¶ˆí™•ì‹¤ì„±ì„ ì™„í™”í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë°ì´í„°ë¡œë¶€í„° ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³  ì†ì‹¤ì— ëŒ€í•´ ë² ì´ì¦ˆ í›„ë°© ë¶„í¬ì— ëŒ€í•œ ì¼ê´€ëœ ìœ„í—˜ í•¨ìˆ˜ì  ì ‘ê·¼ì„ ì·¨í•©ë‹ˆë‹¤. ì´ í˜•ì‹ì€ ëŒ€ê°œ êµí™˜ ê°€ëŠ¥ì„± ì›ì¹™ì„ ì¶©ì¡±í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë²¨ë§Œ ë°©ì •ì‹ì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë©° ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì¼ê´€ëœ ìœ„í—˜ ì¸¡ì •ì˜ ì´ì¤‘ í‘œí˜„ì„ í™œìš©í•˜ê³  ì—°ì†ì ì¸ ê²½ìš°ì— ëŒ€í•´ ë´‰íˆ¬ ì •ë¦¬ë¥¼ í™•ì¥í•˜ì—¬ ì •ì±… ê²½ì‚¬ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ìš°ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ì •ì  ë¶„ì„ì„ ë³´ì—¬ì£¼ë©°, ì •ì±… ê²½ì‚¬ ë°˜ë³µ íšŸìˆ˜ $T$ì™€ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ê¸°ì˜ ìƒ˜í”Œ í¬ê¸° $r$ì— ëŒ€í•´ $O(T^{-1/2}+r^{-1/2})$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì—í”¼ì†Œë“œ ì„¤ì •ìœ¼ë¡œ í™•ì¥í•˜ê³ , í™•ì¥ëœ ì•Œê³ ë¦¬ì¦˜ì˜ ì „ì—­ ìˆ˜ë ´ì„ í™•ë¦½í•˜ë©° ê° ì—í”¼ì†Œë“œì—ì„œ $O(\epsilon)$ì˜ ì˜¤ë¥˜ ê²½ê³„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë°˜ë³µ íšŸìˆ˜ì— ëŒ€í•œ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ ë‹¤ë£¹ë‹ˆë‹¤. ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³ , ì†ì‹¤ì— ëŒ€í•´ ë² ì´ì§€ì•ˆ ì‚¬í›„ ë¶„í¬ì— ê¸°ë°˜í•œ ì¼ê´€ëœ ìœ„í—˜ í•¨ìˆ˜ì ì„ ì ìš©í•©ë‹ˆë‹¤. ì´ ë¬¸ì œëŠ” êµí™˜ ì›ë¦¬ë¥¼ ë§Œì¡±í•˜ì§€ ì•Šì•„ ë²¨ë§Œ ë°©ì •ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ì˜ ì ‘ê·¼ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹ , ì €ìë“¤ì€ ì •ì±… ê²½ì‚¬ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì¼ê´€ëœ ìœ„í—˜ ì¸¡ì •ì˜ ì´ì¤‘ í‘œí˜„ì„ í™œìš©í•˜ê³  ì—°ì†ì ì¸ ê²½ìš°ì— ëŒ€í•´ ì—”ë²¨ë¡œí”„ ì •ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ì†ë„ëŠ” $O(T^{-1/2}+r^{-1/2})$ì´ë©°, ì—¬ê¸°ì„œ $T$ëŠ” ì •ì±… ê²½ì‚¬ ë°˜ë³µ íšŸìˆ˜, $r$ì€ ê²½ì‚¬ ì¶”ì •ê¸°ì˜ ìƒ˜í”Œ í¬ê¸°ì…ë‹ˆë‹¤. ë˜í•œ, ì•Œê³ ë¦¬ì¦˜ì„ ì—í”¼ì†Œë“œ ì„¤ì •ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì „ì—­ ìˆ˜ë ´ì„±ì„ ì…ì¦í•˜ê³ , ê° ì—í”¼ì†Œë“œì—ì„œ $O(\epsilon)$ì˜ ì˜¤ë¥˜ í•œê³„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë°˜ë³µ íšŸìˆ˜ì˜ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ë¯¸ì§€ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ ë‹¤ë£¨ë©°, ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì„ í†µí•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ê³  ì†ì‹¤ì— ëŒ€í•œ ì¼ê´€ëœ ìœ„í—˜ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
- 2. ì´ ë¬¸ì œëŠ” êµí™˜ ê°€ëŠ¥ì„± ì›ë¦¬ë¥¼ ë§Œì¡±í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë²¨ë§Œ ë°©ì •ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©°, ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
- 3. ì •ì±… ê²½ì‚¬ ìµœì í™” ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì¼ê´€ëœ ìœ„í—˜ ì¸¡ì •ì˜ ì´ì¤‘ í‘œí˜„ì„ í™œìš©í•˜ê³  ì—°ì†ì ì¸ ê²½ìš°ì— ëŒ€í•´ ë´‰íˆ¬ ì •ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ì†ë„ëŠ” $O(T^{-1/2}+r^{-1/2})$ë¡œ, ì—¬ê¸°ì„œ $T$ëŠ” ì •ì±… ê²½ì‚¬ ë°˜ë³µ íšŸìˆ˜, $r$ì€ ê²½ì‚¬ ì¶”ì •ê¸°ì˜ ìƒ˜í”Œ í¬ê¸°ì…ë‹ˆë‹¤.
- 5. ì•Œê³ ë¦¬ì¦˜ì„ ì—í”¼ì†Œë“œ ì„¤ì •ìœ¼ë¡œ í™•ì¥í•˜ì—¬, í™•ì¥ëœ ì•Œê³ ë¦¬ì¦˜ì˜ ì „ì—­ ìˆ˜ë ´ì„±ì„ í™•ë¦½í•˜ê³  ê° ì—í”¼ì†Œë“œì—ì„œ $O(\epsilon)$ì˜ ì˜¤ë¥˜ ë²”ìœ„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë°˜ë³µ íšŸìˆ˜ì— ëŒ€í•œ ê²½ê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:27:53*