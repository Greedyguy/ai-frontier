---
keywords:
  - Adaptive Neural Network
  - Computer Vision
  - Reinforcement Learning
  - Attention Mechanism
  - Visual Cognition
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15333
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:54:02.866138",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Adaptive Neural Network",
    "Computer Vision",
    "Reinforcement Learning",
    "Attention Mechanism",
    "Visual Cognition"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Adaptive Neural Network": 0.88,
    "Computer Vision": 0.7,
    "Reinforcement Learning": 0.8,
    "Attention Mechanism": 0.85,
    "Visual Cognition": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "AdaptiveNN",
        "canonical": "Adaptive Neural Network",
        "aliases": [
          "AdaptiveNN"
        ],
        "category": "unique_technical",
        "rationale": "Represents the core innovation of the paper, linking to adaptive vision models.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "visual perception",
        "canonical": "Computer Vision",
        "aliases": [
          "visual perception"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader field of computer vision, which is central to the study.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "reinforcement learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "reinforcement learning"
        ],
        "category": "broad_technical",
        "rationale": "Key method used in the framework, connecting to machine learning techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "fixation patterns",
        "canonical": "Attention Mechanism",
        "aliases": [
          "fixation patterns"
        ],
        "category": "specific_connectable",
        "rationale": "Relates to how the model selectively attends to parts of the visual input.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "visual cognition",
        "canonical": "Visual Cognition",
        "aliases": [
          "visual cognition"
        ],
        "category": "evolved_concepts",
        "rationale": "Highlights the model's alignment with human-like perception, linking to cognitive science.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "task-relevant regions",
      "efficient sampling",
      "real-world application"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "AdaptiveNN",
      "resolved_canonical": "Adaptive Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "visual perception",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "reinforcement learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "fixation patterns",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "visual cognition",
      "resolved_canonical": "Visual Cognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception

**Korean Title:** ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì ì‘ì  ì‹œê°ì„ ëª¨ë°©í•˜ì—¬ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•œ ê¸°ê³„ ì‹œê° ì¸ì‹ì„ êµ¬í˜„í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15333.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15333](https://arxiv.org/abs/2509.15333)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Modeling the Human Visual System_ Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms_20250922|Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms]] (85.3% similar)
- [[2025-09-18/Embodied Navigation Foundation Model_20250918|Embodied Navigation Foundation Model]] (82.4% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (82.1% similar)
- [[2025-09-19/Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models_20250919|Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models]] (82.0% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Computer Vision|Computer Vision]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Adaptive Neural Network|Adaptive Neural Network]]
**ğŸš€ Evolved Concepts**: [[keywords/Visual Cognition|Visual Cognition]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15333v1 Announce Type: cross 
Abstract: Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15333v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¸ê°„ì˜ ì‹œê°ì€ ë§¤ìš° ì ì‘ì ì´ë©°, ë³µì¡í•œ í™˜ê²½ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì— ì£¼ëª©í•©ë‹ˆë‹¤. ë°˜ë©´, ê¸°ì¡´ì˜ ê¸°ê³„ ì‹œê° ëª¨ë¸ì€ ì „ì²´ ì¥ë©´ì„ í•œ ë²ˆì— ìˆ˜ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê³µê°„-ì‹œê°„ ì…ë ¥ í•´ìƒë„ì™€ ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ê³¼ë„í•œ ìì› ìš”êµ¬ë¥¼ ì´ˆë˜í•˜ë©°, ì´ëŠ” ë¯¸ë˜ ë°œì „ê³¼ ì‹¤ì„¸ê³„ ì‘ìš©ì„ ì €í•´í•˜ëŠ” ì¤‘ìš”í•œ í•œê³„ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” 'ìˆ˜ë™ì 'ì—ì„œ 'ëŠ¥ë™ì , ì ì‘ì ' ì‹œê° ëª¨ë¸ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ì¶”ì§„í•˜ê¸° ìœ„í•œ ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ì¸ AdaptiveNNì„ ì†Œê°œí•©ë‹ˆë‹¤. AdaptiveNNì€ ì‹œê°ì  ì¸ì‹ì„ ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì„ ì ì§„ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ì£¼ëª©í•˜ë©°, ê³ ì •ëœ ìœ„ì¹˜ë¥¼ í†µí•´ ì •ë³´ë¥¼ ì ì§„ì ìœ¼ë¡œ ê²°í•©í•˜ê³  ì¶©ë¶„í•  ë•Œ ê´€ì°°ì„ ëŠ¥ë™ì ìœ¼ë¡œ ì¢…ë£Œí•˜ëŠ” ìˆœì°¨ì  ì˜ì‚¬ê²°ì • ê³¼ì •ìœ¼ë¡œ ê³µì‹í™”í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í‘œí˜„ í•™ìŠµê³¼ ìê¸° ë³´ìƒ ê°•í™” í•™ìŠµì„ í†µí•©í•˜ëŠ” ì´ë¡ ì„ ìˆ˜ë¦½í•˜ì—¬, ê³ ì • ìœ„ì¹˜ì— ëŒ€í•œ ì¶”ê°€ ê°ë… ì—†ì´ ë¹„ë¶„í™” ê°€ëŠ¥í•œ AdaptiveNNì˜ ì¢…ë‹¨ ê°„ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ ì‹œê° ì¸ì‹, ì„¸ë°€í•œ ì°¨ë³„í™”, ì‹œê°ì  ê²€ìƒ‰, ì‹¤ì œ ìš´ì „ ë° ì˜ë£Œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì´ë¯¸ì§€ ì²˜ë¦¬, ì–¸ì–´ ê¸°ë°˜ êµ¬í˜„ AI, ì¸ê°„ê³¼ì˜ ë‚˜ë€íˆ ë¹„êµë¥¼ í¬í•¨í•œ 9ê°œì˜ ê³¼ì œë¥¼ ì•„ìš°ë¥´ëŠ” 17ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ AdaptiveNNì„ í‰ê°€í•©ë‹ˆë‹¤. AdaptiveNNì€ ì •í™•ì„±ì„ í¬ìƒí•˜ì§€ ì•Šê³  ìµœëŒ€ 28ë°°ì˜ ì¶”ë¡  ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ë©°, ì¬í›ˆë ¨ ì—†ì´ ë‹¤ì–‘í•œ ê³¼ì œ ìš”êµ¬ì™€ ìì› ì˜ˆì‚°ì— ìœ ì—°í•˜ê²Œ ì ì‘í•˜ê³ , ê³ ì • íŒ¨í„´ì„ í†µí•´ í–¥ìƒëœ í•´ì„ ê°€ëŠ¥ì„±ì„ ì œê³µí•˜ì—¬ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•˜ë©° í•´ì„ ê°€ëŠ¥í•œ ì»´í“¨í„° ë¹„ì „ìœ¼ë¡œì˜ ìœ ë§í•œ ê²½ë¡œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ìš±ì´, AdaptiveNNì€ ë§ì€ ê²½ìš°ì— ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê° í–‰ë™ì„ ë³´ì—¬ì£¼ë©°, ì‹œê° ì¸ì§€ë¥¼ ì¡°ì‚¬í•˜ëŠ” ê°€ì¹˜ ìˆëŠ” ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/LeapLabTHU/AdaptiveNNì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

AdaptiveNNì€ ì¸ê°„ì˜ ì‹œê°ì  ì¸ì§€ ë°©ì‹ì„ ëª¨ë°©í•˜ì—¬ 'ìˆ˜ë™ì 'ì¸ ê¸°ì¡´ ê¸°ê³„ ì‹œê° ëª¨ë¸ì„ 'ëŠ¥ë™ì , ì ì‘ì 'ìœ¼ë¡œ ì „í™˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì‹œê°ì  ì¸ì‹ì„ ìˆœì°¨ì  ì˜ì‚¬ê²°ì • ê³¼ì •ìœ¼ë¡œ ì •ì˜í•˜ì—¬, ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì„ ì ì§„ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ì£¼ëª©í•˜ë©°, ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ë©´ ê´€ì°°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ë¹„ë¶„í™” ê°€ëŠ¥í•œ AdaptiveNNì˜ ì¢…ë‹¨ ê°„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì´ë¡ ì„ ì œì‹œí•˜ë©°, 17ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 28ë°°ì˜ ì¶”ë¡  ë¹„ìš© ì ˆê°ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ê³¼ì œ ìš”êµ¬ì™€ ìì› ì˜ˆì‚°ì— ìœ ì—°í•˜ê²Œ ì ì‘í•˜ë©°, ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê° í–‰ë™ì„ ë³´ì—¬ ì‹œê° ì¸ì§€ ì—°êµ¬ì— ìœ ìš©í•œ ë„êµ¬ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AdaptiveNNì€ 'ìˆ˜ë™ì 'ì¸ ë¹„ì „ ëª¨ë¸ì—ì„œ 'ëŠ¥ë™ì , ì ì‘ì 'ì¸ ë¹„ì „ ëª¨ë¸ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ëª©í‘œë¡œ í•˜ëŠ” ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. AdaptiveNNì€ ì‹œê°ì  ì¸ì‹ì„ ì ì§„ì ì¸ ì˜ì‚¬ ê²°ì • ê³¼ì •ìœ¼ë¡œ ê³µì‹í™”í•˜ì—¬, ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ì£¼ëª©í•˜ë©°, ì¶©ë¶„í•  ë•Œ ê´€ì°°ì„ ëŠ¥ë™ì ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.
- 3. ì´ë¡ ì ìœ¼ë¡œ í‘œí˜„ í•™ìŠµê³¼ ìê¸° ë³´ìƒ ê°•í™” í•™ìŠµì„ í†µí•©í•˜ì—¬, ê³ ì • ìœ„ì¹˜ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ê°ë… ì—†ì´ ë¹„ë¶„í™” ê°€ëŠ¥í•œ AdaptiveNNì˜ ì¢…ë‹¨ ê°„ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. AdaptiveNNì€ ìµœëŒ€ 28ë°°ì˜ ì¶”ë¡  ë¹„ìš© ì ˆê°ì„ ì´ë£¨ë©´ì„œë„ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, ë‹¤ì–‘í•œ ê³¼ì œ ìš”êµ¬ì™€ ìì› ì˜ˆì‚°ì— ìœ ì—°í•˜ê²Œ ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. AdaptiveNNì€ ë§ì€ ê²½ìš°ì—ì„œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê° í–‰ë™ì„ ë³´ì—¬ì£¼ë©°, ì‹œê° ì¸ì§€ë¥¼ ì—°êµ¬í•˜ëŠ” ë° ìœ ìš©í•œ ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 08:54:02*