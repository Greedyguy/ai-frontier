# Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception

**Korean Title:** ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì ì‘ì  ì‹œê°ì„ ëª¨ë°©í•˜ì—¬ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•œ ê¸°ê³„ ì‹œê° ì¸ì‹ì„ êµ¬í˜„í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Adaptive Vision

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Embodied Navigation Foundation Model_20250918|Embodied Navigation Foundation Model]] (82.4% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot MEEG Visual Decoding_20250919|UMind A Unified Multitask Network for Zero-Shot MEEG Visual Decoding]] (82.1% similar)
- [[2025-09-19/Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models_20250919|Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models]] (82.0% similar)
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual A Framework for Constructing Unified Vision-Language Datasets]] (81.8% similar)
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM Hierarchical Adapter Merging for Scalable Continual Learning]] (81.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15333v1 Announce Type: cross 
Abstract: Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15333v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¸ê°„ì˜ ì‹œê°ì€ ë§¤ìš° ì ì‘ë ¥ì´ ë›°ì–´ë‚˜ë©°, ë³µì¡í•œ í™˜ê²½ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì— ì£¼ëª©í•©ë‹ˆë‹¤. ë°˜ë©´ì—, ê¸°ì¡´ì˜ ê¸°ê³„ ì‹œê° ëª¨ë¸ì€ ì „ì²´ ì¥ë©´ì„ í•œ ë²ˆì— ìˆ˜ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬, ê³µê°„-ì‹œê°„ì  ì…ë ¥ í•´ìƒë„ì™€ ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ê³¼ë„í•œ ìì› ìš”êµ¬ë¥¼ ì´ˆë˜í•˜ê³ , ì´ëŠ” ë¯¸ë˜ ë°œì „ê³¼ ì‹¤ì œ ì‘ìš©ì„ ì €í•´í•˜ëŠ” ì¤‘ìš”í•œ í•œê³„ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” 'ìˆ˜ë™ì 'ì—ì„œ 'ëŠ¥ë™ì ì´ê³  ì ì‘ì ì¸' ì‹œê° ëª¨ë¸ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ëª©í‘œë¡œ í•˜ëŠ” ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ì¸ AdaptiveNNì„ ì†Œê°œí•©ë‹ˆë‹¤. AdaptiveNNì€ ì‹œê°ì  ì¸ì‹ì„ ê±°ì¹ ê²Œë¶€í„° ì„¸ë°€í•˜ê²Œ ìˆœì°¨ì ì¸ ì˜ì‚¬ê²°ì • ê³¼ì •ìœ¼ë¡œ ê³µì‹í™”í•˜ì—¬, ì ì°¨ì ìœ¼ë¡œ ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì„ ì‹ë³„í•˜ê³  ì£¼ëª©í•˜ë©°, ê³ ì •ì ì„ í†µí•´ ì •ë³´ë¥¼ ì ì§„ì ìœ¼ë¡œ ê²°í•©í•˜ê³ , ì¶©ë¶„í•  ë•Œ ëŠ¥ë™ì ìœ¼ë¡œ ê´€ì°°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í‘œí˜„ í•™ìŠµê³¼ ìê¸° ë³´ìƒ ê°•í™” í•™ìŠµì„ í†µí•©í•˜ëŠ” ì´ë¡ ì„ ìˆ˜ë¦½í•˜ì—¬, ê³ ì •ì  ìœ„ì¹˜ì— ëŒ€í•œ ì¶”ê°€ ê°ë… ì—†ì´ ë¹„ë¶„í™” ê°€ëŠ¥í•œ AdaptiveNNì˜ ì¢…ë‹¨ ê°„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ ì‹œê° ì¸ì‹, ì„¸ë°€í•œ ì°¨ë³„, ì‹œê°ì  ê²€ìƒ‰, ì‹¤ì œ ìš´ì „ ë° ì˜ë£Œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ì´ë¯¸ì§€ ì²˜ë¦¬, ì–¸ì–´ ê¸°ë°˜ì˜ êµ¬í˜„ëœ AI, ì¸ê°„ê³¼ì˜ ë‚˜ë€í•œ ë¹„êµë¥¼ í¬í•¨í•œ 9ê°œì˜ ê³¼ì œì— ê±¸ì¹œ 17ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ AdaptiveNNì„ í‰ê°€í•©ë‹ˆë‹¤. AdaptiveNNì€ ì •í™•ì„±ì„ í¬ìƒí•˜ì§€ ì•Šê³  ìµœëŒ€ 28ë°°ì˜ ì¶”ë¡  ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ë©°, ì¬í›ˆë ¨ ì—†ì´ ë‹¤ì–‘í•œ ê³¼ì œ ìš”êµ¬ì™€ ìì› ì˜ˆì‚°ì— ìœ ì—°í•˜ê²Œ ì ì‘í•˜ê³ , ê³ ì • íŒ¨í„´ì„ í†µí•´ í–¥ìƒëœ í•´ì„ ê°€ëŠ¥ì„±ì„ ì œê³µí•˜ì—¬ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•˜ë©° í•´ì„ ê°€ëŠ¥í•œ ì»´í“¨í„° ë¹„ì „ìœ¼ë¡œì˜ ìœ ë§í•œ ê²½ë¡œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë”ìš±ì´, AdaptiveNNì€ ë§ì€ ê²½ìš°ì—ì„œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê° í–‰ë™ì„ ë³´ì—¬ì£¼ì–´, ì‹œê° ì¸ì§€ë¥¼ ì¡°ì‚¬í•˜ëŠ” ê·€ì¤‘í•œ ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/LeapLabTHU/AdaptiveNNì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

AdaptiveNNì€ ì¸ê°„ì˜ ì‹œê°ì  ì¸ì§€ ë°©ì‹ì„ ëª¨ë°©í•˜ì—¬ 'ìˆ˜ë™ì 'ì—ì„œ 'ëŠ¥ë™ì , ì ì‘ì 'ì¸ ë¹„ì „ ëª¨ë¸ë¡œì˜ ì „í™˜ì„ ëª©í‘œë¡œ í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì‹œê°ì  ì¸ì‹ì„ ì ì§„ì ì¸ ì˜ì‚¬ ê²°ì • ê³¼ì •ìœ¼ë¡œ ì •ì˜í•˜ì—¬, ê³¼ì œì™€ ê´€ë ¨ëœ ì˜ì—­ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ì£¼ëª©í•˜ë©°, ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ë©´ ê´€ì°°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ í‘œí˜„ í•™ìŠµê³¼ ìê¸° ë³´ìƒ ê°•í™” í•™ìŠµì„ í†µí•©í•œ ì´ë¡ ì„ ì œì‹œí•˜ì—¬, ê³ ì • ìœ„ì¹˜ì— ëŒ€í•œ ì¶”ê°€ ê°ë… ì—†ì´ ë¹„ë¶„í™” ê°€ëŠ¥í•œ AdaptiveNNì˜ ì¢…ë‹¨ ê°„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. 17ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ AdaptiveNNì€ ìµœëŒ€ 28ë°°ì˜ ì¶”ë¡  ë¹„ìš© ì ˆê°ì„ ì´ë£¨ë©´ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©°, ë‹¤ì–‘í•œ ê³¼ì œ ìš”êµ¬ì™€ ìì› ì˜ˆì‚°ì— ìœ ì—°í•˜ê²Œ ì ì‘í•©ë‹ˆë‹¤. ë˜í•œ, ê³ ì • íŒ¨í„´ì„ í†µí•´ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì´ë©°, ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê° í–‰ë™ì„ ë³´ì—¬ ì‹œê° ì¸ì§€ ì—°êµ¬ì— ìœ ìš©í•œ ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. AdaptiveNNì€ 'ìˆ˜ë™ì 'ì—ì„œ 'ëŠ¥ë™ì , ì ì‘ì ' ë¹„ì „ ëª¨ë¸ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ëª©í‘œë¡œ í•˜ëŠ” ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 2. AdaptiveNNì€ ì‹œê°ì  ì¸ì‹ì„ ì ì§„ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì˜ì—­ì„ ì‹ë³„í•˜ê³  ì£¼ëª©í•˜ëŠ” ìˆœì°¨ì  ì˜ì‚¬ê²°ì • ê³¼ì •ìœ¼ë¡œ ê³µì‹í™”í•©ë‹ˆë‹¤.

- 3. AdaptiveNNì€ 17ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœëŒ€ 28ë°°ì˜ ì¶”ë¡  ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

- 4. AdaptiveNNì€ ë‹¤ì–‘í•œ ì‘ì—… ìš”êµ¬ì™€ ìì› ì˜ˆì‚°ì— ìœ ì—°í•˜ê²Œ ì ì‘í•˜ë©°, ì¬í›ˆë ¨ ì—†ì´ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

- 5. AdaptiveNNì€ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê° í–‰ë™ì„ ë³´ì—¬ì£¼ë©°, ì‹œê° ì¸ì§€ ì—°êµ¬ì— ìœ ìš©í•œ ë„êµ¬ë¡œì„œì˜ ì ì¬ë ¥ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.

---

*Generated on 2025-09-22 13:53:35*