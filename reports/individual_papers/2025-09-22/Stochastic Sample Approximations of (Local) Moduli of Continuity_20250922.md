---
keywords:
  - Moduli of Local Continuity
  - Neural Network
  - Stochastic Sample Approximation
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15368
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:21:59.535276",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Moduli of Local Continuity",
    "Neural Network",
    "Stochastic Sample Approximation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Moduli of Local Continuity": 0.78,
    "Neural Network": 0.85,
    "Stochastic Sample Approximation": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "moduli of local continuity",
        "canonical": "Moduli of Local Continuity",
        "aliases": [
          "local continuity modulus",
          "local moduli"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and offers a unique perspective on evaluating neural network robustness.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "neural nets",
          "NN"
        ],
        "category": "broad_technical",
        "rationale": "Neural networks are a foundational element in the study, providing a strong link to existing literature.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "stochastic sample approximation",
        "canonical": "Stochastic Sample Approximation",
        "aliases": [
          "stochastic approximation",
          "sample approximation"
        ],
        "category": "unique_technical",
        "rationale": "This method is a novel approach within the paper, relevant for linking to stochastic processes in machine learning.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "robustness",
      "fairness",
      "repeated uses"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "moduli of local continuity",
      "resolved_canonical": "Moduli of Local Continuity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "stochastic sample approximation",
      "resolved_canonical": "Stochastic Sample Approximation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Stochastic Sample Approximations of (Local) Moduli of Continuity

**Korean Title:** 연속성의 (국소) 모듈라이의 확률적 샘플 근사

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15368.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15368](https://arxiv.org/abs/2509.15368)

## 🔗 유사한 논문
- [[2025-09-22/Geometric Integration for Neural Control Variates_20250922|Geometric Integration for Neural Control Variates]] (79.2% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (79.2% similar)
- [[2025-09-22/Computing Linear Regions in Neural Networks with Skip Connections_20250922|Computing Linear Regions in Neural Networks with Skip Connections]] (78.6% similar)
- [[2025-09-22/Accelerated Gradient Methods with Biased Gradient Estimates_ Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds_20250922|Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds]] (78.0% similar)
- [[2025-09-18/Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (77.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Neural Network|Neural Network]]
**⚡ Unique Technical**: [[keywords/Moduli of Local Continuity|Moduli of Local Continuity]], [[keywords/Stochastic Sample Approximation|Stochastic Sample Approximation]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15368v1 Announce Type: new 
Abstract: Modulus of local continuity is used to evaluate the robustness of neural networks and fairness of their repeated uses in closed-loop models. Here, we revisit a connection between generalized derivatives and moduli of local continuity, and present a non-uniform stochastic sample approximation for moduli of local continuity. This is of importance in studying robustness of neural networks and fairness of their repeated uses.

## 🔍 Abstract (한글 번역)

arXiv:2509.15368v1 발표 유형: 신규  
초록: 국소 연속성의 모듈러스는 신경망의 견고성과 폐루프 모델에서의 반복 사용의 공정성을 평가하는 데 사용됩니다. 여기서 우리는 일반화된 도함수와 국소 연속성의 모듈러스 간의 연결을 재검토하고, 국소 연속성의 모듈러스에 대한 비균일 확률 샘플 근사를 제시합니다. 이는 신경망의 견고성과 반복 사용의 공정성을 연구하는 데 중요합니다.

## 📝 요약

이 논문은 신경망의 강건성과 반복 사용의 공정성을 평가하기 위해 지역 연속성의 모듈러스를 사용하는 방법을 제안합니다. 일반화된 도함수와 지역 연속성 모듈러스 간의 연결을 재검토하고, 비균일 확률적 샘플 근사를 제시합니다. 이는 신경망의 강건성과 공정성 연구에 중요한 기여를 합니다.

## 🎯 주요 포인트

- 1. 지역 연속성의 모듈러스를 사용하여 신경망의 견고성과 반복 사용의 공정성을 평가합니다.
- 2. 일반화된 도함수와 지역 연속성의 모듈러스 간의 연결을 재검토합니다.
- 3. 지역 연속성의 모듈러스에 대한 비균일 확률 샘플 근사를 제시합니다.
- 4. 이는 신경망의 견고성과 반복 사용의 공정성을 연구하는 데 중요합니다.


---

*Generated on 2025-09-23 10:21:59*