---
keywords:
  - Deep Learning
  - Gradient Eligibility Traces
  - Projected Bellman Error
  - MuJoCo
  - Experience Replay
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2507.09087
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:05:26.966381",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Gradient Eligibility Traces",
    "Projected Bellman Error",
    "MuJoCo",
    "Experience Replay"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.78,
    "Gradient Eligibility Traces": 0.69,
    "Projected Bellman Error": 0.71,
    "MuJoCo": 0.75,
    "Experience Replay": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Reinforcement Learning",
        "canonical": "Deep Learning",
        "aliases": [
          "Deep RL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Reinforcement Learning is a subfield of Deep Learning, providing a strong connection to existing knowledge in the field.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Gradient Eligibility Traces",
        "canonical": "Gradient Eligibility Traces",
        "aliases": [
          "Eligibility Traces"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific technique used in reinforcement learning, offering unique insights into credit assignment.",
        "novelty_score": 0.72,
        "connectivity_score": 0.64,
        "specificity_score": 0.82,
        "link_intent_score": 0.69
      },
      {
        "surface": "Projected Bellman Error",
        "canonical": "Projected Bellman Error",
        "aliases": [
          "PBE"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's methodology, providing a basis for linking to related optimization techniques.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.71
      },
      {
        "surface": "MuJoCo",
        "canonical": "MuJoCo",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "MuJoCo is a widely used environment for testing reinforcement learning algorithms, facilitating connections to other works using it.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Experience Replay",
        "canonical": "Experience Replay",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Experience Replay is a key technique in reinforcement learning, allowing for connections to various algorithmic improvements.",
        "novelty_score": 0.6,
        "connectivity_score": 0.77,
        "specificity_score": 0.69,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "off-policy learning",
      "temporal-difference methods",
      "credit assignment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Reinforcement Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Gradient Eligibility Traces",
      "resolved_canonical": "Gradient Eligibility Traces",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.64,
        "specificity": 0.82,
        "link_intent": 0.69
      }
    },
    {
      "candidate_surface": "Projected Bellman Error",
      "resolved_canonical": "Projected Bellman Error",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.71
      }
    },
    {
      "candidate_surface": "MuJoCo",
      "resolved_canonical": "MuJoCo",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Experience Replay",
      "resolved_canonical": "Experience Replay",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.77,
        "specificity": 0.69,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Deep Reinforcement Learning with Gradient Eligibility Traces

**Korean Title:** ê¹Šì€ ê°•í™” í•™ìŠµê³¼ ê·¸ë˜ë””ì–¸íŠ¸ ì ê²©ì„± ì¶”ì 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.09087.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2507.09087](https://arxiv.org/abs/2507.09087)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (85.5% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (83.5% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (83.3% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (82.9% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (82.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/MuJoCo|MuJoCo]], [[keywords/Experience Replay|Experience Replay]]
**âš¡ Unique Technical**: [[keywords/Gradient Eligibility Traces|Gradient Eligibility Traces]], [[keywords/Projected Bellman Error|Projected Bellman Error]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.09087v2 Announce Type: replace-cross 
Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the generalized Projected Bellman Error ($\overline{\text{PBE}}$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the generalized $\overline{\text{PBE}}$ objective to support multistep credit assignment based on the $\lambda$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at https://github.com/esraaelelimy/gtd\_algos

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.09087v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì—ì„œ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì˜¤í”„ í´ë¦¬ì‹œ í•™ìŠµì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ ë„ì „ì ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ë°©ë²•ì€ ë‹¨ìˆœì„±ê³¼ íš¨ìœ¨ì„± ë•Œë¬¸ì— ì¤€-ê¸°ìš¸ê¸° ì‹œê°„ì°¨(TD) ë°©ë²•ì— ì˜ì¡´í•˜ì§€ë§Œ, ê·¸ ê²°ê³¼ ë°œì‚°ì— ì·¨ì•½í•©ë‹ˆë‹¤. Gradient TD (GTD) ë°©ë²•ê³¼ ê°™ì€ ë³´ë‹¤ ì›ì¹™ì ì¸ ì ‘ê·¼ ë°©ì‹ì€ ê°•ë ¥í•œ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•˜ì§€ë§Œ, ì‹¬ì¸µ RLì—ì„œëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ê·¼ì‚¬ì™€ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ GTD ë°©ë²•ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì¼ë°˜í™”ëœ íˆ¬ì˜ ë²¨ë§Œ ì˜¤ë¥˜($\overline{\text{PBE}}$)ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ì—°êµ¬ëŠ” ì‹ ìš© í• ë‹¹ì´ ëŠë¦¬ê³  ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•œ ë‹¨ì¼ ë‹¨ê³„ ë°©ë²•ì— êµ­í•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $\lambda$-ë¦¬í„´ì— ê¸°ë°˜í•œ ë‹¤ë‹¨ê³„ ì‹ ìš© í• ë‹¹ì„ ì§€ì›í•˜ë„ë¡ ì¼ë°˜í™”ëœ $\overline{\text{PBE}}$ ëª©í‘œë¥¼ í™•ì¥í•˜ê³ , ì´ ìƒˆë¡œìš´ ëª©í‘œë¥¼ ìµœì í™”í•˜ëŠ” ì„¸ ê°€ì§€ ê¸°ìš¸ê¸° ê¸°ë°˜ ë°©ë²•ì„ ë„ì¶œí•©ë‹ˆë‹¤. ê²½í—˜ ì¬ìƒê³¼ í˜¸í™˜ë˜ëŠ” ì „ë°© ë³´ê¸° ê³µì‹ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì•Œê³ ë¦¬ì¦˜ê³¼ í˜¸í™˜ë˜ëŠ” í›„ë°© ë³´ê¸° ê³µì‹ì„ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì„ í‰ê°€í•˜ê³  MuJoCo ë° MinAtar í™˜ê²½ì—ì„œ ê°ê° PPOì™€ StreamQë¥¼ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œ: https://github.com/esraaelelimy/gtd_algos

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ê°•í™” í•™ìŠµì—ì„œ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì˜¤í”„ í´ë¦¬ì‹œ í•™ìŠµì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°˜-ê²½ì‚¬ TD ë°©ë²•ì€ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì´ì§€ë§Œ ë°œì‚°ì— ì·¨ì•½í•œ ë°˜ë©´, GTD ë°©ë²•ì€ ê°•ë ¥í•œ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•˜ì§€ë§Œ ì‹¬ì¸µ ê°•í™” í•™ìŠµì—ì„œëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ê·¼ì‚¬ì™€ í•¨ê»˜ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆëŠ” ì¼ë°˜í™”ëœ íˆ¬ì˜ ë²¨ë§Œ ì˜¤ë¥˜($\overline{\text{PBE}}$)ë¥¼ ë„ì…í–ˆìœ¼ë‚˜, ì´ëŠ” ëŠë¦° í¬ë ˆë”§ í• ë‹¹ê³¼ ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•œ 1ë‹¨ê³„ ë°©ë²•ì— êµ­í•œë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $\lambda$-ë¦¬í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë‹¨ê³„ í¬ë ˆë”§ í• ë‹¹ì„ ì§€ì›í•˜ë„ë¡ ì¼ë°˜í™”ëœ $\overline{\text{PBE}}$ ëª©í‘œë¥¼ í™•ì¥í•˜ê³ , ì´ë¥¼ ìµœì í™”í•˜ëŠ” ì„¸ ê°€ì§€ ê²½ì‚¬ ê¸°ë°˜ ë°©ë²•ì„ ë„ì¶œí•©ë‹ˆë‹¤. ê²½í—˜ ì¬ìƒê³¼ í˜¸í™˜ë˜ëŠ” ì „ë°©í–¥ ë·°ì™€ ìŠ¤íŠ¸ë¦¬ë° ì•Œê³ ë¦¬ì¦˜ê³¼ í˜¸í™˜ë˜ëŠ” í›„ë°©í–¥ ë·°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ MuJoCoì™€ MinAtar í™˜ê²½ì—ì„œ PPOì™€ StreamQë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ì‹¬ì¸µ ê°•í™” í•™ìŠµì—ì„œëŠ” ë°˜ì •ê·œí™”ëœ TD ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì´ì§€ë§Œ ë°œì‚°ì— ì·¨ì•½í•˜ë‹¤.
- 2. GTD ë°©ë²•ì€ ìˆ˜ë ´ ë³´ì¥ì´ ê°•í•˜ì§€ë§Œ ì‹¬ì¸µ ê°•í™” í•™ìŠµì—ì„œëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë‹¤.
- 3. ì¼ë°˜í™”ëœ Projected Bellman Error($\overline{\text{PBE}}$)ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ê·¼ì‚¬ì™€ í•¨ê»˜ GTD ë°©ë²•ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•œë‹¤.
- 4. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $\lambda$-returnì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë‹¨ê³„ í¬ë ˆë”§ í• ë‹¹ì„ ì§€ì›í•˜ë„ë¡ ì¼ë°˜í™”ëœ $\overline{\text{PBE}}$ ëª©í‘œë¥¼ í™•ì¥í•˜ì˜€ë‹¤.
- 5. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ MuJoCoì™€ MinAtar í™˜ê²½ì—ì„œ PPOì™€ StreamQë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-23 10:05:26*