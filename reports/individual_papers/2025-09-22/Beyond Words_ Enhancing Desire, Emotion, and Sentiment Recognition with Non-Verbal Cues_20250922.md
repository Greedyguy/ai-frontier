---
keywords:
  - Multimodal Learning
  - Desire Understanding
  - Cross-modal Alignment
  - Masked Image Modeling
  - Vision-Language Model
category: cs.CL
publish_date: 2025-09-22
arxiv_id: 2509.15540
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:38:36.499218",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Desire Understanding",
    "Cross-modal Alignment",
    "Masked Image Modeling",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Desire Understanding": 0.78,
    "Cross-modal Alignment": 0.77,
    "Masked Image Modeling": 0.74,
    "Vision-Language Model": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Learning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal",
          "Multimodal Approach"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is central to the paper's methodology and aligns with trending concepts in AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Desire Understanding",
        "canonical": "Desire Understanding",
        "aliases": [
          "Desire Recognition",
          "Desire Analysis"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique focus of the paper, extending beyond traditional sentiment analysis.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cross-modal Alignment",
        "canonical": "Cross-modal Alignment",
        "aliases": [
          "Cross-modal Interaction",
          "Cross-modal Integration"
        ],
        "category": "unique_technical",
        "rationale": "Cross-modal Alignment is a specific technique used in the proposed framework, enhancing multimodal interactions.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "Masked Image Modeling",
        "canonical": "Masked Image Modeling",
        "aliases": [
          "Masked Modeling",
          "Image Masking"
        ],
        "category": "unique_technical",
        "rationale": "This technique is crucial for capturing fine-grained local features in images.",
        "novelty_score": 0.7,
        "connectivity_score": 0.67,
        "specificity_score": 0.75,
        "link_intent_score": 0.74
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language",
          "V-L Model"
        ],
        "category": "evolved_concepts",
        "rationale": "The paper's framework involves integrating vision and language, aligning with evolving concepts in AI.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "Symmetrical Bidirectional Multimodal Learning Framework",
      "MSED"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Learning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Desire Understanding",
      "resolved_canonical": "Desire Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cross-modal Alignment",
      "resolved_canonical": "Cross-modal Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Masked Image Modeling",
      "resolved_canonical": "Masked Image Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.67,
        "specificity": 0.75,
        "link_intent": 0.74
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues

**Korean Title:** ë‹¨ì–´ë¥¼ ë„˜ì–´: ë¹„ì–¸ì–´ì  ì‹ í˜¸ë¥¼ í†µí•œ ìš•ë§, ê°ì •, ê·¸ë¦¬ê³  ê°ì • ì¸ì‹ì˜ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15540.pdf)
**Category**: cs.CL
**Published**: 2025-09-22
**ArXiv ID**: [2509.15540](https://arxiv.org/abs/2509.15540)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (82.5% similar)
- [[2025-09-22/Pointing to a Llama and Call it a Camel_ On the Sycophancy of Multimodal Large Language Models_20250922|Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models]] (81.5% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (81.4% similar)
- [[2025-09-22/Experimenting with Affective Computing Models in Video Interviews with Spanish-speaking Older Adults_20250922|Experimenting with Affective Computing Models in Video Interviews with Spanish-speaking Older Adults]] (81.2% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (81.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Desire Understanding|Desire Understanding]], [[keywords/Cross-modal Alignment|Cross-modal Alignment]], [[keywords/Masked Image Modeling|Masked Image Modeling]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15540v1 Announce Type: cross 
Abstract: Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15540v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¸ê°„ í–‰ë™ì„ ì´ë„ëŠ” ì˜ë„ë¡œì„œì˜ ìš•ë§ì€ ê°ì • ë° ì •ì„œì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµì€ ê°ì • ë° ì •ì„œ ì¸ì‹ ë¶„ì•¼ì—ì„œ ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ì¸ê°„ì˜ ìš•ë§ ì´í•´ë¥¼ íŠ¹ë³„íˆ ëª©í‘œë¡œ í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì ‘ê·¼ë²•ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë˜í•œ, ê¸°ì¡´ì˜ ê°ì • ë¶„ì„ ë°©ë²•ì€ ì£¼ë¡œ ì–¸ì–´ì  ë‹¨ì„œì— ì¤‘ì ì„ ë‘ê³  ìˆìœ¼ë©°, ì´ë¯¸ì§€ì™€ ê°™ì€ ë¹„ì–¸ì–´ì  ë‹¨ì„œë¥¼ ë³´ì™„ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë° ì†Œí™€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìš•ë§, ê°ì •, ì •ì„œ ì¸ì‹ì„ ìœ„í•œ ëŒ€ì¹­ì  ì–‘ë°©í–¥ ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒí˜¸ ì§€ë„ë¥¼ í†µí•´ ì´ë¯¸ì§€ ë‚´ ì˜ë„ ê´€ë ¨ í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ êµì°¨ ëª¨ë‹¬ ì •ë ¬ì„ ìœ„í•œ ì „ì—­ ì‹œê°ì  í‘œí˜„ì„ ì–»ê³ , ê³ í•´ìƒë„ ì´ë¯¸ì§€ëŠ” í•˜ìœ„ ì´ë¯¸ì§€ë¡œ ë¶„í• í•˜ì—¬ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ëª¨ë¸ë§ì„ í†µí•´ ì„¸ë°€í•œ ì§€ì—­ì  íŠ¹ì§•ì„ í¬ì°©í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. í…ìŠ¤íŠ¸ ìœ ë„ ì´ë¯¸ì§€ ë””ì½”ë”ì™€ ì´ë¯¸ì§€ ìœ ë„ í…ìŠ¤íŠ¸ ë””ì½”ë”ë¥¼ ë„ì…í•˜ì—¬ ì´ë¯¸ì§€ ì •ë³´ì˜ ì§€ì—­ ë° ì „ì—­ í‘œí˜„ì—ì„œ ê¹Šì€ êµì°¨ ëª¨ë‹¬ ìƒí˜¸ì‘ìš©ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ë˜í•œ, ì§€ê°ì  ì´ë“ê³¼ ê³„ì‚° ë¹„ìš©ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´, ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ í•˜ìœ„ ì´ë¯¸ì§€ë¡œ ì˜ë¼ ë§ˆìŠ¤í¬ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ëŠ” í˜¼í•© ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ ì „ëµì„ ì±„íƒí•©ë‹ˆë‹¤. ì œì•ˆëœ ì ‘ê·¼ë²•ì€ ìš•ë§ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì™€ ê°ì • ë° ì •ì„œ ì¸ì‹ì„ í¬í•¨í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ì…‹ì¸ MSEDì—ì„œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ë‹¤ë¥¸ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ì— ë¹„í•´ ì¼ê´€ëœ ê°œì„ ì„ ë³´ì—¬ì£¼ë©°, ì œì•ˆëœ ë°©ë²•ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ê¸°ì¡´ ì ‘ê·¼ë²•ì„ ëŠ¥ê°€í•˜ì—¬ ìš•ë§ ì´í•´ì—ì„œ 1.1%, ê°ì • ì¸ì‹ì—ì„œ 0.6%, ì •ì„œ ë¶„ì„ì—ì„œ 0.9%ì˜ F1-score ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” ë‹¤ìŒì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: https://github.com/especiallyW/SyDES.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ì˜ ìš•ë§, ê°ì •, ê°ì •ì„ ì¸ì‹í•˜ê¸° ìœ„í•œ ëŒ€ì¹­ì  ì–‘ë°©í–¥ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°ì • ë¶„ì„ ë°©ë²•ì´ ì£¼ë¡œ ì–¸ì–´ì  ë‹¨ì„œì— ì˜ì¡´í•˜ê³  ì´ë¯¸ì§€ì˜ ë¹„ì–¸ì–´ì  ë‹¨ì„œë¥¼ ê°„ê³¼í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ ìƒí˜¸ ì§€ë„ë¥¼ í†µí•´ ì´ë¯¸ì§€ì—ì„œ ì˜ë„ ê´€ë ¨ í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ë©°, ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ í†µí•´ ì „ë°˜ì ì¸ ì‹œê° í‘œí˜„ì„ ì–»ê³ , ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì„¸ë¶„í™”í•˜ì—¬ ì„¸ë°€í•œ ì§€ì—­ì  íŠ¹ì§•ì„ í¬ì°©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ìš•ë§ ì´í•´, ê°ì • ì¸ì‹, ê°ì • ë¶„ì„ì—ì„œ ê°ê° 1.1%, 0.6%, 0.9%ì˜ F1-score ê°œì„ ì„ ë³´ì´ë©°, ìµœì²¨ë‹¨ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê°„ì˜ ìš•ë§ ì´í•´ë¥¼ ëª©í‘œë¡œ í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµ ì ‘ê·¼ë²•ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.
- 2. ê¸°ì¡´ ê°ì • ë¶„ì„ ë°©ë²•ì€ ì£¼ë¡œ ì–¸ì–´ì  ë‹¨ì„œì— ì§‘ì¤‘í•˜ë©° ì´ë¯¸ì§€ì™€ ê°™ì€ ë¹„ì–¸ì–´ì  ë‹¨ì„œë¥¼ ê°„ê³¼í•œë‹¤.
- 3. ì œì•ˆëœ ëŒ€ì¹­ì  ì–‘ë°©í–¥ ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒí˜¸ ì§€ë„ë¥¼ í†µí•´ ì˜ë„ ê´€ë ¨ í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•œë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ MSED ë°ì´í„°ì…‹ì—ì„œ ìš•ë§ ì´í•´, ê°ì • ì¸ì‹, ê°ì • ë¶„ì„ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ìš•ë§ ì´í•´ì—ì„œ 1.1%, ê°ì • ì¸ì‹ì—ì„œ 0.6%, ê°ì • ë¶„ì„ì—ì„œ 0.9%ì˜ F1-score í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤.


---

*Generated on 2025-09-23 11:38:36*