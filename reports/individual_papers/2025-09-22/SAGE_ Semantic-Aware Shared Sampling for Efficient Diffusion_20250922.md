---
keywords:
  - Diffusion Models
  - Semantic-Aware Shared Sampling
  - FrÃ©chet Inception Distance
  - Contrastive Languageâ€“Image Pretraining
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2509.15865
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:37:41.665105",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Models",
    "Semantic-Aware Shared Sampling",
    "FrÃ©chet Inception Distance",
    "Contrastive Languageâ€“Image Pretraining"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Models": 0.78,
    "Semantic-Aware Shared Sampling": 0.82,
    "FrÃ©chet Inception Distance": 0.8,
    "Contrastive Languageâ€“Image Pretraining": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion models",
        "canonical": "Diffusion Models",
        "aliases": [
          "Diffusion",
          "Diffusion Process"
        ],
        "category": "broad_technical",
        "rationale": "Diffusion models are central to the paper's methodology and have broad applicability across domains.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Semantic-aware shared sampling",
        "canonical": "Semantic-Aware Shared Sampling",
        "aliases": [
          "SAGE",
          "Shared Sampling"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper, essential for understanding the proposed efficiency improvements.",
        "novelty_score": 0.92,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "FID",
        "canonical": "FrÃ©chet Inception Distance",
        "aliases": [
          "FID Score"
        ],
        "category": "specific_connectable",
        "rationale": "FID is a standard metric for evaluating the quality of generated images, relevant for linking to evaluation methods.",
        "novelty_score": 0.3,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "CLIP",
        "canonical": "Contrastive Languageâ€“Image Pretraining",
        "aliases": [
          "CLIP Model"
        ],
        "category": "specific_connectable",
        "rationale": "CLIP is a well-known model for vision-language tasks, relevant for linking to multimodal learning discussions.",
        "novelty_score": 0.35,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      }
    ],
    "ban_list_suggestions": [
      "sampling cost",
      "generation quality",
      "efficiency gains"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion models",
      "resolved_canonical": "Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Semantic-aware shared sampling",
      "resolved_canonical": "Semantic-Aware Shared Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "FID",
      "resolved_canonical": "FrÃ©chet Inception Distance",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "CLIP",
      "resolved_canonical": "Contrastive Languageâ€“Image Pretraining",
      "decision": "linked",
      "scores": {
        "novelty": 0.35,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    }
  ]
}
-->

# SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion

**Korean Title:** SAGE: íš¨ìœ¨ì ì¸ í™•ì‚°ì„ ìœ„í•œ ì˜ë¯¸ ì¸ì‹ ê³µìœ  ìƒ˜í”Œë§

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15865.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2509.15865](https://arxiv.org/abs/2509.15865)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/LowDiff_ Efficient Diffusion Sampling with Low-Resolution Condition_20250922|LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition]] (83.5% similar)
- [[2025-09-17/SpecDiff_ Accelerating Diffusion Model Inference with Self-Speculation_20250917|SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation]] (81.9% similar)
- [[2025-09-18/Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model_20250918|Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model]] (80.5% similar)
- [[2025-09-22/SCoT_ Straight Consistent Trajectory for Pre-Trained Diffusion Model Distillations_20250922|SCoT: Straight Consistent Trajectory for Pre-Trained Diffusion Model Distillations]] (80.5% similar)
- [[2025-09-22/Autoguided Online Data Curation for Diffusion Model Training_20250922|Autoguided Online Data Curation for Diffusion Model Training]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Diffusion Models|Diffusion Models]]
**ğŸ”— Specific Connectable**: [[keywords/FrÃ©chet Inception Distance|FrÃ©chet Inception Distance]], [[keywords/Contrastive Languageâ€“Image Pretraining|Contrastive Languageâ€“Image Pretraining]]
**âš¡ Unique Technical**: [[keywords/Semantic-Aware Shared Sampling|Semantic-Aware Shared Sampling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15865v1 Announce Type: new 
Abstract: Diffusion models manifest evident benefits across diverse domains, yet their high sampling cost, requiring dozens of sequential model evaluations, remains a major limitation. Prior efforts mainly accelerate sampling via optimized solvers or distillation, which treat each query independently. In contrast, we reduce total number of steps by sharing early-stage sampling across semantically similar queries. To enable such efficiency gains without sacrificing quality, we propose SAGE, a semantic-aware shared sampling framework that integrates a shared sampling scheme for efficiency and a tailored training strategy for quality preservation. Extensive experiments show that SAGE reduces sampling cost by 25.5%, while improving generation quality with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over baselines.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15865v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: í™•ì‚° ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ëª…ë°±í•œ ì´ì ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ìˆ˜ì‹­ ë²ˆì˜ ìˆœì°¨ì ì¸ ëª¨ë¸ í‰ê°€ê°€ í•„ìš”í•œ ë†’ì€ ìƒ˜í”Œë§ ë¹„ìš©ì€ ì—¬ì „íˆ ì£¼ìš” ì œí•œ ì‚¬í•­ìœ¼ë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ì´ì „ì˜ ë…¸ë ¥ë“¤ì€ ì£¼ë¡œ ìµœì í™”ëœ í•´ë²•ì´ë‚˜ ì¦ë¥˜ë¥¼ í†µí•´ ìƒ˜í”Œë§ì„ ê°€ì†í™”í–ˆìœ¼ë©°, ì´ëŠ” ê° ì¿¼ë¦¬ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë°˜ë©´ì—, ìš°ë¦¬ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì¿¼ë¦¬ ê°„ì— ì´ˆê¸° ë‹¨ê³„ ìƒ˜í”Œë§ì„ ê³µìœ í•¨ìœ¼ë¡œì¨ ì´ ë‹¨ê³„ ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íš¨ìœ¨ì„± í–¥ìƒì„ í’ˆì§ˆì„ í¬ìƒí•˜ì§€ ì•Šê³  ë‹¬ì„±í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•œ ê³µìœ  ìƒ˜í”Œë§ ì²´ê³„ì™€ í’ˆì§ˆ ë³´ì¡´ì„ ìœ„í•œ ë§ì¶¤í˜• í›ˆë ¨ ì „ëµì„ í†µí•©í•œ ì˜ë¯¸ ì¸ì‹ ê³µìœ  ìƒ˜í”Œë§ í”„ë ˆì„ì›Œí¬ì¸ SAGEë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼, SAGEëŠ” ìƒ˜í”Œë§ ë¹„ìš©ì„ 25.5% ì¤„ì´ë©´ì„œ, ê¸°ì¤€ì„  ëŒ€ë¹„ 5.0% ë‚®ì€ FID, 5.4% ë†’ì€ CLIP, 160% ë†’ì€ ë‹¤ì–‘ì„±ìœ¼ë¡œ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í™•ì‚° ëª¨ë¸ì˜ ë†’ì€ ìƒ˜í”Œë§ ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ SAGEë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ê° ì¿¼ë¦¬ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, SAGEëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì¿¼ë¦¬ ê°„ì— ì´ˆê¸° ìƒ˜í”Œë§ì„ ê³µìœ í•˜ì—¬ ì „ì²´ ë‹¨ê³„ ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íš¨ìœ¨ì„±ì„ ë†’ì´ë©´ì„œë„ í’ˆì§ˆì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë§ì¶¤í˜• í•™ìŠµ ì „ëµì„ í†µí•©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SAGEëŠ” ìƒ˜í”Œë§ ë¹„ìš©ì„ 25.5% ì ˆê°í•˜ê³ , ìƒì„± í’ˆì§ˆì„ ê°œì„ í•˜ì—¬ FIDëŠ” 5.0% ë‚®ì¶”ê³ , CLIP ì ìˆ˜ëŠ” 5.4% ë†’ì´ë©°, ë‹¤ì–‘ì„±ì€ 160% ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í™•ì‚° ëª¨ë¸ì˜ ë†’ì€ ìƒ˜í”Œë§ ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ SAGEë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. SAGEëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì¿¼ë¦¬ ê°„ì˜ ì´ˆê¸° ìƒ˜í”Œë§ì„ ê³µìœ í•˜ì—¬ ì´ ìƒ˜í”Œë§ ë‹¨ê³„ë¥¼ ì¤„ì…ë‹ˆë‹¤.
- 3. SAGEëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•œ ê³µìœ  ìƒ˜í”Œë§ ìŠ¤í‚´ê³¼ í’ˆì§ˆ ìœ ì§€ë¥¼ ìœ„í•œ ë§ì¶¤í˜• í›ˆë ¨ ì „ëµì„ í†µí•©í•©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, SAGEëŠ” ìƒ˜í”Œë§ ë¹„ìš©ì„ 25.5% ì ˆê°í•˜ë©´ì„œ, FID 5.0% ê°ì†Œ, CLIP 5.4% ì¦ê°€, ë‹¤ì–‘ì„± 160% ì¦ê°€ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:37:41*