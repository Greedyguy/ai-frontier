---
keywords:
  - Scene Graph Representation
  - Graph Neural Network
  - Vision-Language Model
  - Diffusion-based Imitation Learning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.16053
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:27:31.822965",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Scene Graph Representation",
    "Graph Neural Network",
    "Vision-Language Model",
    "Diffusion-based Imitation Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Scene Graph Representation": 0.78,
    "Graph Neural Network": 0.8,
    "Vision-Language Model": 0.79,
    "Diffusion-based Imitation Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "scene graph-based representation",
        "canonical": "Scene Graph Representation",
        "aliases": [
          "scene graph",
          "graph-based scene"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's approach and offers a unique method for task-relevant object and relation focus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Graph Neural Networks",
        "canonical": "Graph Neural Network",
        "aliases": [
          "GNN"
        ],
        "category": "specific_connectable",
        "rationale": "GNNs are integral to the proposed skill learning framework, enhancing connectivity with existing research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "vision-language model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "The integration of VLMs with scene-graph skills highlights a novel approach to task planning.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "diffusion-based imitation learning",
        "canonical": "Diffusion-based Imitation Learning",
        "aliases": [
          "diffusion imitation"
        ],
        "category": "unique_technical",
        "rationale": "This represents a novel learning approach within the framework, offering new avenues for exploration.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "planner",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "scene graph-based representation",
      "resolved_canonical": "Scene Graph Representation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Graph Neural Networks",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "vision-language model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "diffusion-based imitation learning",
      "resolved_canonical": "Diffusion-based Imitation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Compose by Focus: Scene Graph-based Atomic Skills

**Korean Title:** í¬ì»¤ìŠ¤ë¥¼ í†µí•œ êµ¬ì„±: ì¥ë©´ ê·¸ë˜í”„ ê¸°ë°˜ì˜ ì›ìì  ê¸°ìˆ 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.16053.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.16053](https://arxiv.org/abs/2509.16053)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring_20250919|Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring]] (82.5% similar)
- [[2025-09-19/CRAFT_ Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks_20250919|CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks]] (82.2% similar)
- [[2025-09-19/Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control_20250919|Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control]] (81.9% similar)
- [[2025-09-22/SCENEFORGE_ Enhancing 3D-text alignment with Structured Scene Compositions_20250922|SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions]] (81.6% similar)
- [[2025-09-18/\textsc{Gen2Real}_ Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video_20250918|\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]]
**âš¡ Unique Technical**: [[keywords/Scene Graph Representation|Scene Graph Representation]], [[keywords/Diffusion-based Imitation Learning|Diffusion-based Imitation Learning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16053v1 Announce Type: cross 
Abstract: A key requirement for generalist robots is compositional generalization - the ability to combine atomic skills to solve complex, long-horizon tasks. While prior work has primarily focused on synthesizing a planner that sequences pre-learned skills, robust execution of the individual skills themselves remains challenging, as visuomotor policies often fail under distribution shifts induced by scene composition. To address this, we introduce a scene graph-based representation that focuses on task-relevant objects and relations, thereby mitigating sensitivity to irrelevant variation. Building on this idea, we develop a scene-graph skill learning framework that integrates graph neural networks with diffusion-based imitation learning, and further combine "focused" scene-graph skills with a vision-language model (VLM) based task planner. Experiments in both simulation and real-world manipulation tasks demonstrate substantially higher success rates than state-of-the-art baselines, highlighting improved robustness and compositional generalization in long-horizon tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16053v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¼ë°˜ì ì¸ ë¡œë´‡ì—ê²Œ ì¤‘ìš”í•œ ìš”êµ¬ ì‚¬í•­ ì¤‘ í•˜ë‚˜ëŠ” ì¡°í•©ì  ì¼ë°˜í™”ì…ë‹ˆë‹¤. ì´ëŠ” ì›ìì  ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬ ë³µì¡í•˜ê³  ì¥ê¸°ì ì¸ ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ì „ ì—°êµ¬ëŠ” ì£¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê¸°ìˆ ì„ ìˆœì„œëŒ€ë¡œ ë°°ì—´í•˜ëŠ” ê³„íšìë¥¼ í•©ì„±í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆì§€ë§Œ, ê°œë³„ ê¸°ìˆ ì˜ ê²¬ê³ í•œ ì‹¤í–‰ ìì²´ëŠ” ì—¬ì „íˆ ë„ì „ ê³¼ì œë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¥ë©´ êµ¬ì„±ì— ì˜í•´ ìœ ë„ëœ ë¶„í¬ ë³€í™”ë¡œ ì¸í•´ ì‹œê°-ìš´ë™ ì •ì±…ì´ ì¢…ì¢… ì‹¤íŒ¨í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê³¼ì œì™€ ê´€ë ¨ëœ ê°ì²´ì™€ ê´€ê³„ì— ì¤‘ì ì„ ë‘” ì¥ë©´ ê·¸ë˜í”„ ê¸°ë°˜ í‘œí˜„ì„ ë„ì…í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³€ë™ì— ëŒ€í•œ ë¯¼ê°ì„±ì„ ì™„í™”í•©ë‹ˆë‹¤. ì´ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê·¸ë˜í”„ ì‹ ê²½ë§ê³¼ í™•ì‚° ê¸°ë°˜ ëª¨ë°© í•™ìŠµì„ í†µí•©í•œ ì¥ë©´ ê·¸ë˜í”„ ê¸°ìˆ  í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí•˜ê³ , "ì§‘ì¤‘ëœ" ì¥ë©´ ê·¸ë˜í”„ ê¸°ìˆ ì„ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM) ê¸°ë°˜ ê³¼ì œ ê³„íšìì™€ ê²°í•©í•©ë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ ì¡°ì‘ ì‘ì—… ëª¨ë‘ì—ì„œì˜ ì‹¤í—˜ì€ ìµœì²¨ë‹¨ ê¸°ì¤€ì„ ë³´ë‹¤ ìƒë‹¹íˆ ë†’ì€ ì„±ê³µë¥ ì„ ë³´ì—¬ì£¼ë©°, ì¥ê¸° ê³¼ì œì—ì„œì˜ í–¥ìƒëœ ê²¬ê³ ì„±ê³¼ ì¡°í•©ì  ì¼ë°˜í™”ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¼ë°˜ì ì¸ ë¡œë´‡ì´ ë³µì¡í•œ ì‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë³„ ê¸°ìˆ ì„ ì¡°í•©í•˜ëŠ” ëŠ¥ë ¥ì¸ êµ¬ì„±ì  ì¼ë°˜í™”ë¥¼ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ì‚¬ì „ì— í•™ìŠµëœ ê¸°ìˆ ì„ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜í•˜ëŠ” ê³„íšì— ì´ˆì ì„ ë§ì·„ìœ¼ë‚˜, ê°œë³„ ê¸°ìˆ ì˜ ì‹¤í–‰ì´ ì¥ë©´ êµ¬ì„± ë³€í™”ì— ë¯¼ê°í•˜ì—¬ ì–´ë ¤ì›€ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ ì—°êµ¬ëŠ” ì‘ì—… ê´€ë ¨ ê°ì²´ì™€ ê´€ê³„ì— ì§‘ì¤‘í•˜ëŠ” ì¥ë©´ ê·¸ë˜í”„ ê¸°ë°˜ í‘œí˜„ì„ ë„ì…í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³€ë™ì— ëŒ€í•œ ë¯¼ê°ì„±ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ì´ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·¸ë˜í”„ ì‹ ê²½ë§ê³¼ í™•ì‚° ê¸°ë°˜ ëª¨ë°© í•™ìŠµì„ í†µí•©í•œ ì¥ë©´ ê·¸ë˜í”„ ê¸°ìˆ  í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí•˜ê³ , ì´ë¥¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ì˜ ì‘ì—… ê³„íšìì™€ ê²°í•©í–ˆìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ ì¡°ì‘ ì‘ì—… ì‹¤í—˜ì—ì„œ, ì œì•ˆëœ ë°©ë²•ì€ ìµœì‹  ê¸°ë²•ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ì„ ë³´ì—¬ì£¼ë©°, ì¥ê¸° ì‘ì—…ì—ì„œì˜ í–¥ìƒëœ ê°•ê±´ì„±ê³¼ êµ¬ì„±ì  ì¼ë°˜í™”ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¼ë°˜ ë¡œë´‡ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì€ ì›ìì  ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬ ë³µì¡í•œ ì¥ê¸° ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” ì¡°í•©ì  ì¼ë°˜í™” ëŠ¥ë ¥ì´ë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê¸°ìˆ ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê³„íšì í•©ì„±ì— ì§‘ì¤‘í–ˆìœ¼ë‚˜, ê°œë³„ ê¸°ìˆ ì˜ ì‹¤í–‰ì€ ì—¬ì „íˆ ë„ì „ ê³¼ì œì´ë‹¤.
- 3. ì¥ë©´ ê·¸ë˜í”„ ê¸°ë°˜ í‘œí˜„ì„ ë„ì…í•˜ì—¬ ê³¼ì œ ê´€ë ¨ ê°ì²´ì™€ ê´€ê³„ì— ì§‘ì¤‘í•¨ìœ¼ë¡œì¨ ë¶ˆí•„ìš”í•œ ë³€ë™ì— ëŒ€í•œ ë¯¼ê°ì„±ì„ ì¤„ì˜€ë‹¤.
- 4. ê·¸ë˜í”„ ì‹ ê²½ë§ê³¼ í™•ì‚° ê¸°ë°˜ ëª¨ë°© í•™ìŠµì„ í†µí•©í•œ ì¥ë©´ ê·¸ë˜í”„ ê¸°ìˆ  í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí–ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì‹œë®¬ë ˆì´ì…˜ ë° ì‹¤ì œ ì¡°ì‘ ì‘ì—…ì—ì„œ ê¸°ì¡´ ìµœì²¨ë‹¨ ê¸°ì¤€ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ì„ ë³´ì—¬ì£¼ë©°, ì¥ê¸° ê³¼ì œì—ì„œì˜ í–¥ìƒëœ ê°•ê±´ì„±ê³¼ ì¡°í•©ì  ì¼ë°˜í™”ë¥¼ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-23 09:27:31*