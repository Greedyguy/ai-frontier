---
keywords:
  - Reinforcement Learning
  - Sparse Rewards
  - Learning from Demonstrations
  - Q-value Distributions
  - Uncertainty-Based Methods
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15981
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:24:51.928880",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Sparse Rewards",
    "Learning from Demonstrations",
    "Q-value Distributions",
    "Uncertainty-Based Methods"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Sparse Rewards": 0.72,
    "Learning from Demonstrations": 0.8,
    "Q-value Distributions": 0.78,
    "Uncertainty-Based Methods": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational concept that connects to numerous other topics in machine learning and robotics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.92,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sparse Rewards",
        "canonical": "Sparse Rewards",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Sparse Rewards are a specific challenge in reinforcement learning that influences the design of learning algorithms.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Demonstrations",
        "canonical": "Learning from Demonstrations",
        "aliases": [
          "Imitation Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Learning from Demonstrations is a key technique in reinforcement learning that enhances connectivity with imitation learning research.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Q-value Distributions",
        "canonical": "Q-value Distributions",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Q-value Distributions provide a nuanced approach to decision-making in reinforcement learning, linking to advanced policy evaluation techniques.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Uncertainty-Based Methods",
        "canonical": "Uncertainty-Based Methods",
        "aliases": [
          "Uncertainty Quantification"
        ],
        "category": "specific_connectable",
        "rationale": "Uncertainty-Based Methods are pivotal in reinforcement learning for balancing exploration and exploitation, connecting to broader AI safety topics.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.92,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sparse Rewards",
      "resolved_canonical": "Sparse Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Demonstrations",
      "resolved_canonical": "Learning from Demonstrations",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Q-value Distributions",
      "resolved_canonical": "Q-value Distributions",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Uncertainty-Based Methods",
      "resolved_canonical": "Uncertainty-Based Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations

**Korean Title:** ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ì˜ ë§¤ë„ëŸ¬ìš´ ì •ì±… ì •ê·œí™”: ì†Œìˆ˜ì˜ ì‹œì—°ì„ í†µí•œ ê°•í™” í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15981.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15981](https://arxiv.org/abs/2509.15981)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (82.7% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (82.2% similar)
- [[2025-09-22/Nonconvex Regularization for Feature Selection in Reinforcement Learning_20250922|Nonconvex Regularization for Feature Selection in Reinforcement Learning]] (82.0% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.0% similar)
- [[2025-09-19/Sample Efficient Experience Replay in Non-stationary Environments_20250919|Sample Efficient Experience Replay in Non-stationary Environments]] (81.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Learning from Demonstrations|Learning from Demonstrations]], [[keywords/Uncertainty-Based Methods|Uncertainty-Based Methods]]
**âš¡ Unique Technical**: [[keywords/Sparse Rewards|Sparse Rewards]], [[keywords/Q-value Distributions|Q-value Distributions]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15981v1 Announce Type: cross 
Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15981v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: í¬ì†Œí•œ ë³´ìƒì„ ê°€ì§„ ê°•í™” í•™ìŠµì—ì„œ ì‹œë²”ì€ í•™ìŠµì„ ê°€ì†í™”í•  ìˆ˜ ìˆì§€ë§Œ, ì–¸ì œ ì´ë¥¼ ëª¨ë°©í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ì–´ë ¤ìš´ ë¬¸ì œì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—ì´ì „íŠ¸ê°€ ì‹œë²”ì„ ëª¨ë°©í•´ì•¼ í•  ë•Œì™€ ìì‹ ì˜ ì •ì±…ì„ ë”°ë¼ì•¼ í•  ë•Œë¥¼ ê²°ì •í•˜ëŠ” ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì„ í•´ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì¸ ì‹œë²”ìœ¼ë¡œë¶€í„°ì˜ ë¶€ë“œëŸ¬ìš´ ì •ì±… ì •ê·œí™”(Smooth Policy Regularisation from Demonstrations, SPReD)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. SPReDëŠ” ì•™ìƒë¸” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë²”ê³¼ ì •ì±… í–‰ë™ ëª¨ë‘ì— ëŒ€í•œ Q-ê°’ ë¶„í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ë¹„êµë¥¼ ìœ„í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ìƒí˜¸ ë³´ì™„ì ì¸ ë¶ˆí™•ì‹¤ì„± ì¸ì‹ ë°©ë²•ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤: ì‹œë²”ì˜ ìš°ì›”ì„± ê°€ëŠ¥ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì ‘ê·¼ë²•ê³¼ í†µê³„ì  ìœ ì˜ì„±ì— ë”°ë¼ ëª¨ë°©ì„ ì¡°ì •í•˜ëŠ” ì´ì  ê¸°ë°˜ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤. ì´ì§„ ëª¨ë°© ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê¸°ì¡´ ë°©ë²•ë“¤(ì˜ˆ: Q-filter)ê³¼ ë‹¬ë¦¬, SPReDëŠ” ì—°ì†ì ì´ê³  ë¶ˆí™•ì‹¤ì„±ì— ë¹„ë¡€í•˜ëŠ” ì •ê·œí™” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ ê¸°ìš¸ê¸° ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤. ê³„ì‚°ì ìœ¼ë¡œ ê°„ë‹¨í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , SPReDëŠ” 8ê°œì˜ ë¡œë´‡ ê³¼ì œì—ì„œ ì‹¤í—˜ì ìœ¼ë¡œ ë†€ë¼ìš´ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ë³µì¡í•œ ê³¼ì œì—ì„œ ê¸°ì¡´ ì ‘ê·¼ë²•ì„ ìµœëŒ€ 14ë°°ê¹Œì§€ ëŠ¥ê°€í•˜ë©´ì„œ ì‹œë²”ì˜ í’ˆì§ˆê³¼ ì–‘ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/YujieZhu7/SPReDì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í¬ì†Œí•œ ë³´ìƒì„ ê°€ì§„ ê°•í™” í•™ìŠµì—ì„œ ì‹œì—°ì„ í†µí•œ í•™ìŠµ ê°€ì†í™”ë¥¼ ëª©í‘œë¡œ í•˜ëŠ” SPReD(Smooth Policy Regularisation from Demonstrations) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. SPReDëŠ” ì—ì´ì „íŠ¸ê°€ ì‹œì—°ì„ ëª¨ë°©í•  ì‹œì ê³¼ ìì²´ ì •ì±…ì„ ë”°ë¥¼ ì‹œì ì„ ê²°ì •í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì•™ìƒë¸” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹œì—° ë° ì •ì±… í–‰ë™ì— ëŒ€í•œ Q-ê°’ ë¶„í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê³ , ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤. ë‘ ê°€ì§€ ë¶ˆí™•ì‹¤ì„± ì¸ì‹ ë°©ë²•ì„ ê°œë°œí–ˆìœ¼ë©°, í•˜ë‚˜ëŠ” ì‹œì—° ìš°ì›”ì„±ì˜ ê°€ëŠ¥ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì ‘ê·¼ë²•ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” í†µê³„ì  ìœ ì˜ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë°©ì„ ì¡°ì •í•˜ëŠ” ì´ì  ê¸°ë°˜ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤. SPReDëŠ” ê¸°ì¡´ì˜ ì´ì§„ ëª¨ë°© ê²°ì • ë°©ì‹ê³¼ ë‹¬ë¦¬ ì—°ì†ì ì´ê³  ë¶ˆí™•ì‹¤ì„±ì— ë¹„ë¡€í•˜ëŠ” ì •ê·œí™” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SPReDëŠ” 8ê°œì˜ ë¡œë´‡ ê³¼ì œì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 14ë°° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì‹œì—°ì˜ ì§ˆê³¼ ì–‘ì— ëŒ€í•œ ê°•ì¸ì„±ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SPReDëŠ” ê°•í™” í•™ìŠµì—ì„œ ì—ì´ì „íŠ¸ê°€ ì–¸ì œ ì‹œë²”ì„ ëª¨ë°©í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. SPReDëŠ” ì‹œë²”ê³¼ ì •ì±… í–‰ë™ì˜ Q-ê°’ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ë¹„êµë¥¼ ìœ„í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.
- 3. ë‘ ê°€ì§€ ë¶ˆí™•ì‹¤ì„± ì¸ì‹ ë°©ë²•ì„ ê°œë°œí–ˆìœ¼ë©°, ì´ëŠ” ì‹œë²”ì˜ ìš°ì›”ì„±ì„ í™•ë¥ ì ìœ¼ë¡œ ì¶”ì •í•˜ê±°ë‚˜ í†µê³„ì  ìœ ì˜ì„±ì— ë”°ë¼ ëª¨ë°©ì„ ì¡°ì •í•©ë‹ˆë‹¤.
- 4. SPReDëŠ” ì´ì‚°ì  ëª¨ë°© ê²°ì • ëŒ€ì‹  ì—°ì†ì ì´ê³  ë¶ˆí™•ì‹¤ì„±ì— ë¹„ë¡€í•˜ëŠ” ì •ê·œí™” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµ ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤.
- 5. SPReDëŠ” 8ê°œì˜ ë¡œë´‡ ì‘ì—… ì‹¤í—˜ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 14ë°° ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‹œë²”ì˜ í’ˆì§ˆê³¼ ì–‘ì— ëŒ€í•œ ê°•ì¸ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:24:51*