---
keywords:
  - Deep Learning
  - Mixture-of-Experts
  - Channel Estimation
  - Zero-Shot Learning
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15964
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T09:23:24.419304",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Mixture-of-Experts",
    "Channel Estimation",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.85,
    "Mixture-of-Experts": 0.78,
    "Channel Estimation": 0.77,
    "Zero-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Deep Learning",
        "canonical": "Deep Learning",
        "aliases": [
          "DL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Learning is a fundamental concept that connects various advanced techniques in the paper.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "The Mixture-of-Experts framework is central to the paper's proposed method, offering a unique approach to enhance generalization.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Channel Estimation",
        "canonical": "Channel Estimation",
        "aliases": [
          "CE"
        ],
        "category": "unique_technical",
        "rationale": "Channel Estimation is a specific technical focus of the paper, crucial for understanding the application domain.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a trending concept relevant to the paper's evaluation scenarios.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Deep Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Channel Estimation",
      "resolved_canonical": "Channel Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework

**Korean Title:** MoE-CE: ì „ë¬¸ê°€ í˜¼í•© í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì±„ë„ ì¶”ì •ì˜ ì¼ë°˜í™” í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15964.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15964](https://arxiv.org/abs/2509.15964)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (85.0% similar)
- [[2025-09-22/TrueMoE_ Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection_20250922|TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection]] (84.2% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (84.1% similar)
- [[2025-09-19/Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation_20250919|Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation]] (84.1% similar)
- [[2025-09-17/Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection_20250917|Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Channel Estimation|Channel Estimation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15964v1 Announce Type: cross 
Abstract: Reliable channel estimation (CE) is fundamental for robust communication in dynamic wireless environments, where models must generalize across varying conditions such as signal-to-noise ratios (SNRs), the number of resource blocks (RBs), and channel profiles. Traditional deep learning (DL)-based methods struggle to generalize effectively across such diverse settings, particularly under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a flexible mixture-of-experts (MoE) framework designed to enhance the generalization capability of DL-based CE methods. MoE-CE provides an appropriate inductive bias by leveraging multiple expert subnetworks, each specialized in distinct channel characteristics, and a learned router that dynamically selects the most relevant experts per input. This architecture enhances model capacity and adaptability without a proportional rise in computational cost while being agnostic to the choice of the backbone model and the learning algorithm. Through extensive experiments on synthetic datasets generated under diverse SNRs, RB numbers, and channel profiles, including multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently outperforms conventional DL approaches, achieving significant performance gains while maintaining efficiency.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15964v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì±„ë„ ì¶”ì •(CE)ì€ ë™ì  ë¬´ì„  í™˜ê²½ì—ì„œ ê°•ë ¥í•œ í†µì‹ ì„ ìœ„í•´ í•„ìˆ˜ì ì´ë©°, ëª¨ë¸ì€ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„(SNR), ìì› ë¸”ë¡(RB)ì˜ ìˆ˜, ì±„ë„ í”„ë¡œí•„ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì¡°ê±´ì— ê±¸ì³ ì¼ë°˜í™”í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì „í†µì ì¸ ë”¥ëŸ¬ë‹(DL) ê¸°ë°˜ ë°©ë²•ì€ íŠ¹íˆ ë©€í‹°íƒœìŠ¤í¬ ë° ì œë¡œìƒ· ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì„¤ì •ì— íš¨ê³¼ì ìœ¼ë¡œ ì¼ë°˜í™”í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” DL ê¸°ë°˜ CE ë°©ë²•ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì„¤ê³„ëœ ìœ ì—°í•œ ì „ë¬¸ê°€ í˜¼í•©(MoE) í”„ë ˆì„ì›Œí¬ì¸ MoE-CEë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MoE-CEëŠ” ì—¬ëŸ¬ ì „ë¬¸ê°€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ê°ê¸° ë‹¤ë¥¸ ì±„ë„ íŠ¹ì„±ì— íŠ¹í™”ëœ ì ì ˆí•œ ê·€ë‚©ì  í¸í–¥ì„ ì œê³µí•˜ë©°, í•™ìŠµëœ ë¼ìš°í„°ê°€ ì…ë ¥ì— ë”°ë¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì „ë¬¸ê°€ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤. ì´ ì•„í‚¤í…ì²˜ëŠ” ë°±ë³¸ ëª¨ë¸ê³¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì„ íƒì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©´ì„œë„ ê³„ì‚° ë¹„ìš©ì˜ ë¹„ë¡€ì ì¸ ì¦ê°€ ì—†ì´ ëª¨ë¸ì˜ ìš©ëŸ‰ê³¼ ì ì‘ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë‹¤ì–‘í•œ SNR, RB ìˆ˜, ì±„ë„ í”„ë¡œí•„ í•˜ì—ì„œ ìƒì„±ëœ í•©ì„± ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´, ë©€í‹°íƒœìŠ¤í¬ ë° ì œë¡œìƒ· í‰ê°€ë¥¼ í¬í•¨í•˜ì—¬ MoE-CEê°€ ê¸°ì¡´ DL ì ‘ê·¼ë²•ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë™ì  ë¬´ì„  í™˜ê²½ì—ì„œì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì±„ë„ ì¶”ì •ì„ ìœ„í•´ MoE-CEë¼ëŠ” í˜¼í•© ì „ë¬¸ê°€(MoE) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MoE-CEëŠ” ë‹¤ì–‘í•œ ì±„ë„ íŠ¹ì„±ì— íŠ¹í™”ëœ ì—¬ëŸ¬ ì „ë¬¸ê°€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ì™€ ì…ë ¥ì— ë”°ë¼ ì ì ˆí•œ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•˜ëŠ” ë¼ìš°í„°ë¥¼ í†µí•´ DL ê¸°ë°˜ ì±„ë„ ì¶”ì • ë°©ë²•ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ëª¨ë¸ì˜ ìš©ëŸ‰ê³¼ ì ì‘ì„±ì„ ë†’ì´ë©´ì„œë„ ê³„ì‚° ë¹„ìš© ì¦ê°€ë¥¼ ìµœì†Œí™”í•˜ë©°, ë°±ë³¸ ëª¨ë¸ì´ë‚˜ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— êµ¬ì• ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ SNR, ë¦¬ì†ŒìŠ¤ ë¸”ë¡ ìˆ˜, ì±„ë„ í”„ë¡œí•„ì„ í¬í•¨í•œ ì‹¤í—˜ì—ì„œ MoE-CEëŠ” ê¸°ì¡´ DL ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MoE-CEëŠ” ë‹¤ì–‘í•œ ì±„ë„ íŠ¹ì„±ì— íŠ¹í™”ëœ ì—¬ëŸ¬ ì „ë¬¸ê°€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ DL ê¸°ë°˜ ì±„ë„ ì¶”ì • ë°©ë²•ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. MoE-CEëŠ” ì…ë ¥ì— ë”°ë¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì „ë¬¸ê°€ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” í•™ìŠµëœ ë¼ìš°í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì ì‘ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 3. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë°±ë³¸ ëª¨ë¸ê³¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ë¬´ê´€í•˜ê²Œ ëª¨ë¸ ìš©ëŸ‰ê³¼ ì ì‘ì„±ì„ ì¦ê°€ì‹œí‚¤ë©´ì„œë„ ê³„ì‚° ë¹„ìš©ì˜ ë¹„ë¡€ì  ì¦ê°€ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ SNR, RB ìˆ˜, ì±„ë„ í”„ë¡œí•„ì„ í¬í•¨í•œ í•©ì„± ë°ì´í„°ì…‹ ì‹¤í—˜ì—ì„œ MoE-CEëŠ” ê¸°ì¡´ DL ì ‘ê·¼ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. MoE-CEëŠ” ë©€í‹°íƒœìŠ¤í¬ ë° ì œë¡œìƒ· í‰ê°€ì—ì„œë„ íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-23 09:23:24*