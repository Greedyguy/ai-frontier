---
keywords:
  - Large Language Model
  - Entropy-Regularized Process Reward Model
  - Reinforcement Learning
  - Markov Decision Process
  - Process Reward
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2412.11006
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:03:01.035688",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Entropy-Regularized Process Reward Model",
    "Reinforcement Learning",
    "Markov Decision Process",
    "Process Reward"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Entropy-Regularized Process Reward Model": 0.8,
    "Reinforcement Learning": 0.78,
    "Markov Decision Process": 0.7,
    "Process Reward": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on enhancing reasoning capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Entropy-Regularized Process Reward Model",
        "canonical": "Entropy-Regularized Process Reward Model",
        "aliases": [
          "ER-PRM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to reward modeling in reinforcement learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A foundational method discussed in the context of reward models.",
        "novelty_score": 0.25,
        "connectivity_score": 0.88,
        "specificity_score": 0.55,
        "link_intent_score": 0.78
      },
      {
        "surface": "Markov Decision Processes",
        "canonical": "Markov Decision Process",
        "aliases": [
          "MDP"
        ],
        "category": "specific_connectable",
        "rationale": "Integral to the proposed model's theoretical framework.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Process Rewards",
        "canonical": "Process Reward",
        "aliases": [
          "Intermediate Rewards"
        ],
        "category": "unique_technical",
        "rationale": "Key concept in the paper's approach to improving reasoning trajectories.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Entropy-Regularized Process Reward Model",
      "resolved_canonical": "Entropy-Regularized Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.88,
        "specificity": 0.55,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Markov Decision Processes",
      "resolved_canonical": "Markov Decision Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Process Rewards",
      "resolved_canonical": "Process Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Entropy-Regularized Process Reward Model

**Korean Title:** 엔트로피 정규화된 프로세스 보상 모델

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2412.11006.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2412.11006](https://arxiv.org/abs/2412.11006)

## 🔗 유사한 논문
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (87.9% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (87.9% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (87.0% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.0% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (85.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Markov Decision Process|Markov Decision Process]]
**⚡ Unique Technical**: [[keywords/Entropy-Regularized Process Reward Model|Entropy-Regularized Process Reward Model]], [[keywords/Process Reward|Process Reward]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2412.11006v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise in performing complex multi-step reasoning, yet they continue to struggle with mathematical reasoning, often making systematic errors. A promising solution is reinforcement learning (RL) guided by reward models, particularly those focusing on process rewards, which score each intermediate step rather than solely evaluating the final outcome. This approach is more effective at guiding policy models towards correct reasoning trajectories. In this work, we propose an entropy-regularized process reward model (ER-PRM) that integrates KL-regularized Markov Decision Processes (MDP) to balance policy optimization with the need to prevent the policy from shifting too far from its initial distribution. We derive a novel reward construction method based on the theoretical results. Our theoretical analysis shows that we could derive the optimal reward model from the initial policy sampling. Our empirical experiments on the MATH and GSM8K benchmarks demonstrate that ER-PRM consistently outperforms existing process reward models, achieving 1% improvement on GSM8K and 2-3% improvement on MATH under best-of-N evaluation, and more than 1% improvement under RLHF. These results highlight the efficacy of entropy-regularization in enhancing LLMs' reasoning capabilities.

## 🔍 Abstract (한글 번역)

arXiv:2412.11006v2 발표 유형: 교체  
초록: 대형 언어 모델(LLMs)은 복잡한 다단계 추론을 수행하는 데 있어 유망한 성과를 보였지만, 여전히 수학적 추론에서는 체계적인 오류를 자주 범하고 있습니다. 유망한 해결책으로는 보상 모델에 의해 안내되는 강화 학습(RL)이 있으며, 특히 최종 결과만 평가하는 것이 아니라 각 중간 단계를 평가하는 과정 보상에 중점을 둔 모델이 주목받고 있습니다. 이 접근 방식은 정책 모델이 올바른 추론 경로로 나아가도록 안내하는 데 더 효과적입니다. 본 연구에서는 KL-정규화 마르코프 결정 과정(MDP)을 통합하여 정책 최적화와 초기 분포에서 정책이 너무 멀리 이동하지 않도록 하는 필요성을 균형 있게 조절하는 엔트로피 정규화 과정 보상 모델(ER-PRM)을 제안합니다. 우리는 이론적 결과에 기반하여 새로운 보상 구성 방법을 도출합니다. 우리의 이론적 분석은 초기 정책 샘플링에서 최적의 보상 모델을 도출할 수 있음을 보여줍니다. MATH와 GSM8K 벤치마크에 대한 실험 결과, ER-PRM은 기존의 과정 보상 모델을 지속적으로 능가하며, GSM8K에서 1%, MATH에서 2-3%의 향상을 보였으며, RLHF 하에서는 1% 이상의 향상을 이루었습니다. 이러한 결과는 엔트로피 정규화가 LLM의 추론 능력을 향상시키는 데 효과적임을 강조합니다.

## 📝 요약

이 논문은 대형 언어 모델(LLM)이 수학적 추론에서 발생하는 체계적 오류를 해결하기 위해 강화 학습(RL)과 보상 모델을 활용하는 방법을 제안합니다. 특히, 중간 단계마다 점수를 부여하는 프로세스 보상 모델이 효과적임을 강조합니다. 저자들은 KL-정규화 마르코프 결정 과정(MDP)을 통합한 엔트로피 정규화 프로세스 보상 모델(ER-PRM)을 제안하여 정책 최적화와 초기 분포에서의 과도한 이탈을 방지합니다. 이론적 분석을 통해 최적의 보상 모델을 도출할 수 있음을 보였으며, MATH와 GSM8K 벤치마크 실험에서 ER-PRM이 기존 모델보다 우수한 성능을 보였습니다. 이는 LLM의 추론 능력을 향상시키는 데 있어 엔트로피 정규화의 효과를 입증합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 복잡한 다단계 추론에서 유망한 성과를 보였지만, 수학적 추론에서는 여전히 체계적인 오류를 범하고 있다.
- 2. 보상 모델에 의해 안내되는 강화 학습(RL)은 특히 중간 단계마다 점수를 매기는 프로세스 보상에 중점을 두어 올바른 추론 경로로 정책 모델을 안내하는 데 효과적이다.
- 3. 본 연구에서는 KL-정규화 마르코프 결정 과정(MDP)을 통합하여 초기 분포에서 정책이 너무 멀리 이동하지 않도록 하는 엔트로피 정규화 프로세스 보상 모델(ER-PRM)을 제안한다.
- 4. MATH와 GSM8K 벤치마크에 대한 실험 결과, ER-PRM은 기존 프로세스 보상 모델을 일관되게 능가하며, GSM8K에서 1%, MATH에서 2-3%의 성능 향상을 보였다.
- 5. ER-PRM의 결과는 엔트로피 정규화가 LLM의 추론 능력을 향상시키는 데 효과적임을 강조한다.


---

*Generated on 2025-09-23 11:03:01*