---
keywords:
  - Large Language Model
  - Entropy-Regularized Process Reward Model
  - Reinforcement Learning
  - Markov Decision Process
  - Process Reward
category: cs.LG
publish_date: 2025-09-22
arxiv_id: 2412.11006
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T11:03:01.035688",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Entropy-Regularized Process Reward Model",
    "Reinforcement Learning",
    "Markov Decision Process",
    "Process Reward"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Entropy-Regularized Process Reward Model": 0.8,
    "Reinforcement Learning": 0.78,
    "Markov Decision Process": 0.7,
    "Process Reward": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on enhancing reasoning capabilities.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Entropy-Regularized Process Reward Model",
        "canonical": "Entropy-Regularized Process Reward Model",
        "aliases": [
          "ER-PRM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to reward modeling in reinforcement learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A foundational method discussed in the context of reward models.",
        "novelty_score": 0.25,
        "connectivity_score": 0.88,
        "specificity_score": 0.55,
        "link_intent_score": 0.78
      },
      {
        "surface": "Markov Decision Processes",
        "canonical": "Markov Decision Process",
        "aliases": [
          "MDP"
        ],
        "category": "specific_connectable",
        "rationale": "Integral to the proposed model's theoretical framework.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Process Rewards",
        "canonical": "Process Reward",
        "aliases": [
          "Intermediate Rewards"
        ],
        "category": "unique_technical",
        "rationale": "Key concept in the paper's approach to improving reasoning trajectories.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Entropy-Regularized Process Reward Model",
      "resolved_canonical": "Entropy-Regularized Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.88,
        "specificity": 0.55,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Markov Decision Processes",
      "resolved_canonical": "Markov Decision Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Process Rewards",
      "resolved_canonical": "Process Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Entropy-Regularized Process Reward Model

**Korean Title:** ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ëœ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2412.11006.pdf)
**Category**: cs.LG
**Published**: 2025-09-22
**ArXiv ID**: [2412.11006](https://arxiv.org/abs/2412.11006)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (87.9% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (87.9% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (87.0% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.0% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (85.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Markov Decision Process|Markov Decision Process]]
**âš¡ Unique Technical**: [[keywords/Entropy-Regularized Process Reward Model|Entropy-Regularized Process Reward Model]], [[keywords/Process Reward|Process Reward]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2412.11006v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise in performing complex multi-step reasoning, yet they continue to struggle with mathematical reasoning, often making systematic errors. A promising solution is reinforcement learning (RL) guided by reward models, particularly those focusing on process rewards, which score each intermediate step rather than solely evaluating the final outcome. This approach is more effective at guiding policy models towards correct reasoning trajectories. In this work, we propose an entropy-regularized process reward model (ER-PRM) that integrates KL-regularized Markov Decision Processes (MDP) to balance policy optimization with the need to prevent the policy from shifting too far from its initial distribution. We derive a novel reward construction method based on the theoretical results. Our theoretical analysis shows that we could derive the optimal reward model from the initial policy sampling. Our empirical experiments on the MATH and GSM8K benchmarks demonstrate that ER-PRM consistently outperforms existing process reward models, achieving 1% improvement on GSM8K and 2-3% improvement on MATH under best-of-N evaluation, and more than 1% improvement under RLHF. These results highlight the efficacy of entropy-regularization in enhancing LLMs' reasoning capabilities.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2412.11006v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ì§€ë§Œ, ì—¬ì „íˆ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œëŠ” ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¥¼ ìì£¼ ë²”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ ë§í•œ í•´ê²°ì±…ìœ¼ë¡œëŠ” ë³´ìƒ ëª¨ë¸ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ê°•í™” í•™ìŠµ(RL)ì´ ìˆìœ¼ë©°, íŠ¹íˆ ìµœì¢… ê²°ê³¼ë§Œ í‰ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê° ì¤‘ê°„ ë‹¨ê³„ë¥¼ í‰ê°€í•˜ëŠ” ê³¼ì • ë³´ìƒì— ì¤‘ì ì„ ë‘” ëª¨ë¸ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì •ì±… ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì¶”ë¡  ê²½ë¡œë¡œ ë‚˜ì•„ê°€ë„ë¡ ì•ˆë‚´í•˜ëŠ” ë° ë” íš¨ê³¼ì ì…ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” KL-ì •ê·œí™” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ í†µí•©í•˜ì—¬ ì •ì±… ìµœì í™”ì™€ ì´ˆê¸° ë¶„í¬ì—ì„œ ì •ì±…ì´ ë„ˆë¬´ ë©€ë¦¬ ì´ë™í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” í•„ìš”ì„±ì„ ê· í˜• ìˆê²Œ ì¡°ì ˆí•˜ëŠ” ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” ê³¼ì • ë³´ìƒ ëª¨ë¸(ER-PRM)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¡ ì  ê²°ê³¼ì— ê¸°ë°˜í•˜ì—¬ ìƒˆë¡œìš´ ë³´ìƒ êµ¬ì„± ë°©ë²•ì„ ë„ì¶œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì´ë¡ ì  ë¶„ì„ì€ ì´ˆê¸° ì •ì±… ìƒ˜í”Œë§ì—ì„œ ìµœì ì˜ ë³´ìƒ ëª¨ë¸ì„ ë„ì¶œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ER-PRMì€ ê¸°ì¡´ì˜ ê³¼ì • ë³´ìƒ ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•˜ë©°, GSM8Kì—ì„œ 1%, MATHì—ì„œ 2-3%ì˜ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, RLHF í•˜ì—ì„œëŠ” 1% ì´ìƒì˜ í–¥ìƒì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ê°€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œ ë°œìƒí•˜ëŠ” ì²´ê³„ì  ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°•í™” í•™ìŠµ(RL)ê³¼ ë³´ìƒ ëª¨ë¸ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì´ íš¨ê³¼ì ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì €ìë“¤ì€ KL-ì •ê·œí™” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ í†µí•©í•œ ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(ER-PRM)ì„ ì œì•ˆí•˜ì—¬ ì •ì±… ìµœì í™”ì™€ ì´ˆê¸° ë¶„í¬ì—ì„œì˜ ê³¼ë„í•œ ì´íƒˆì„ ë°©ì§€í•©ë‹ˆë‹¤. ì´ë¡ ì  ë¶„ì„ì„ í†µí•´ ìµœì ì˜ ë³´ìƒ ëª¨ë¸ì„ ë„ì¶œí•  ìˆ˜ ìˆìŒì„ ë³´ì˜€ìœ¼ë©°, MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ER-PRMì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ìˆì–´ ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì—ì„œ ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ì§€ë§Œ, ìˆ˜í•™ì  ì¶”ë¡ ì—ì„œëŠ” ì—¬ì „íˆ ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¥¼ ë²”í•˜ê³  ìˆë‹¤.
- 2. ë³´ìƒ ëª¨ë¸ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ê°•í™” í•™ìŠµ(RL)ì€ íŠ¹íˆ ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒì— ì¤‘ì ì„ ë‘ì–´ ì˜¬ë°”ë¥¸ ì¶”ë¡  ê²½ë¡œë¡œ ì •ì±… ëª¨ë¸ì„ ì•ˆë‚´í•˜ëŠ” ë° íš¨ê³¼ì ì´ë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” KL-ì •ê·œí™” ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ í†µí•©í•˜ì—¬ ì´ˆê¸° ë¶„í¬ì—ì„œ ì •ì±…ì´ ë„ˆë¬´ ë©€ë¦¬ ì´ë™í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(ER-PRM)ì„ ì œì•ˆí•œë‹¤.
- 4. MATHì™€ GSM8K ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ER-PRMì€ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, GSM8Kì—ì„œ 1%, MATHì—ì„œ 2-3%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ë‹¤.
- 5. ER-PRMì˜ ê²°ê³¼ëŠ” ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ê°€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-23 11:03:01*