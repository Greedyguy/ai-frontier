---
keywords:
  - Referential Video Object Segmentation
  - Large Language Model
  - Video-Language Checker
  - Key-Frame Sampler
  - Vision-Language Model
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15546
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:00:05.769898",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Referential Video Object Segmentation",
    "Large Language Model",
    "Video-Language Checker",
    "Key-Frame Sampler",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Referential Video Object Segmentation": 0.78,
    "Large Language Model": 0.82,
    "Video-Language Checker": 0.79,
    "Key-Frame Sampler": 0.77,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Referential Video Object Segmentation",
        "canonical": "Referential Video Object Segmentation",
        "aliases": [
          "RVOS"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task that integrates vision and language, making it a unique technical concept relevant to the paper.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's methodology, linking language understanding with video segmentation.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.82
      },
      {
        "surface": "Video-Language Checker",
        "canonical": "Video-Language Checker",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel component introduced in the paper, enhancing video object segmentation by verifying language descriptions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.79
      },
      {
        "surface": "Key-Frame Sampler",
        "canonical": "Key-Frame Sampler",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This component is crucial for improving temporal context capture, a unique contribution of the paper.",
        "novelty_score": 0.78,
        "connectivity_score": 0.58,
        "specificity_score": 0.86,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "The integration of vision and language is a key theme of the paper, aligning with trending concepts.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Referential Video Object Segmentation",
      "resolved_canonical": "Referential Video Object Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Video-Language Checker",
      "resolved_canonical": "Video-Language Checker",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Key-Frame Sampler",
      "resolved_canonical": "Key-Frame Sampler",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.58,
        "specificity": 0.86,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track

**Korean Title:** 참조 비디오 객체 분할을 위한 Sa2VA 향상: 제7회 LSVOS RVOS 트랙 2위 솔루션

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15546.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15546](https://arxiv.org/abs/2509.15546)

## 🔗 유사한 논문
- [[2025-09-22/Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge_ 3rd Place Solution_20250922|Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution]] (86.6% similar)
- [[2025-09-18/VSE-MOT_ Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement_20250918|VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement]] (85.4% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (82.8% similar)
- [[2025-09-18/SAIL-VL2 Technical Report_20250918|SAIL-VL2 Technical Report]] (82.7% similar)
- [[2025-09-18/Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing_20250918|Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing]] (82.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Referential Video Object Segmentation|Referential Video Object Segmentation]], [[keywords/Video-Language Checker|Video-Language Checker]], [[keywords/Key-Frame Sampler|Key-Frame Sampler]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15546v1 Announce Type: new 
Abstract: Referential Video Object Segmentation (RVOS) aims to segment all objects in a video that match a given natural language description, bridging the gap between vision and language understanding. Recent work, such as Sa2VA, combines Large Language Models (LLMs) with SAM~2, leveraging the strong video reasoning capability of LLMs to guide video segmentation. In this work, we present a training-free framework that substantially improves Sa2VA's performance on the RVOS task. Our method introduces two key components: (1) a Video-Language Checker that explicitly verifies whether the subject and action described in the query actually appear in the video, thereby reducing false positives; and (2) a Key-Frame Sampler that adaptively selects informative frames to better capture both early object appearances and long-range temporal context. Without any additional training, our approach achieves a J&amp;F score of 64.14% on the MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge at ICCV 2025.

## 🔍 Abstract (한글 번역)

arXiv:2509.15546v1 발표 유형: 신규  
초록: 참조 비디오 객체 분할(RVOS)은 주어진 자연어 설명과 일치하는 비디오 내 모든 객체를 분할하는 것을 목표로 하며, 이는 시각적 이해와 언어적 이해 간의 격차를 줄이는 역할을 합니다. Sa2VA와 같은 최근 연구는 대형 언어 모델(LLMs)을 SAM~2와 결합하여 LLM의 강력한 비디오 추론 능력을 활용해 비디오 분할을 안내합니다. 본 연구에서는 Sa2VA의 RVOS 작업 성능을 크게 향상시키는 훈련이 필요 없는 프레임워크를 제시합니다. 우리의 방법은 두 가지 주요 구성 요소를 도입합니다: (1) 쿼리에 설명된 주제와 행동이 실제로 비디오에 나타나는지를 명시적으로 확인하여 거짓 양성을 줄이는 비디오-언어 검사기, (2) 초기 객체 출현과 장기적인 시간적 맥락을 더 잘 포착하기 위해 정보가 풍부한 프레임을 적응적으로 선택하는 키-프레임 샘플러. 추가적인 훈련 없이, 우리의 접근 방식은 MeViS 테스트 세트에서 64.14%의 J&F 점수를 달성하여 ICCV 2025의 제7회 LSVOS 챌린지 RVOS 트랙에서 2위를 기록했습니다.

## 📝 요약

이 논문은 자연어 설명에 맞는 비디오 객체를 분할하는 RVOS 문제에서 Sa2VA의 성능을 개선하는 훈련이 필요 없는 프레임워크를 제안합니다. 주요 기여는 두 가지 구성 요소로, 첫째, 비디오-언어 검사기를 통해 쿼리의 주제와 행동이 실제 비디오에 나타나는지를 검증하여 오탐을 줄이고, 둘째, 핵심 프레임 샘플러를 통해 정보가 풍부한 프레임을 선택하여 초기 객체 등장과 장기적 시간적 맥락을 잘 포착하는 것입니다. 이 방법은 추가적인 훈련 없이 MeViS 테스트 세트에서 64.14%의 J&F 점수를 기록하며, ICCV 2025의 LSVOS 챌린지 RVOS 트랙에서 2위를 차지했습니다.

## 🎯 주요 포인트

- 1. RVOS는 자연어 설명과 일치하는 객체를 비디오에서 분할하는 작업으로, 시각과 언어 이해의 격차를 줄이는 것을 목표로 한다.
- 2. Sa2VA는 대형 언어 모델(LLMs)과 SAM~2를 결합하여, LLMs의 강력한 비디오 추론 능력을 이용해 비디오 분할을 안내한다.
- 3. 본 연구에서는 Sa2VA의 성능을 크게 향상시키는 훈련이 필요 없는 프레임워크를 제안한다.
- 4. 제안된 방법은 쿼리에서 설명된 주제와 행동이 실제로 비디오에 나타나는지를 명시적으로 검증하는 비디오-언어 체커와 정보가 풍부한 프레임을 적응적으로 선택하는 키-프레임 샘플러를 도입한다.
- 5. 추가적인 훈련 없이 제안된 방법은 MeViS 테스트 세트에서 J&F 점수 64.14%를 기록하며, ICCV 2025의 7th LSVOS 챌린지 RVOS 트랙에서 2위를 차지하였다.


---

*Generated on 2025-09-23 12:00:05*