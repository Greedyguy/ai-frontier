---
keywords:
  - Cooperative Trajectory Prediction
  - Anchor-oriented Decoder
  - Multi-source Trajectory Fusion
  - Attention Mechanism
category: cs.CV
publish_date: 2025-09-22
arxiv_id: 2509.15984
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T12:16:22.535447",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Cooperative Trajectory Prediction",
    "Anchor-oriented Decoder",
    "Multi-source Trajectory Fusion",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Cooperative Trajectory Prediction": 0.78,
    "Anchor-oriented Decoder": 0.77,
    "Multi-source Trajectory Fusion": 0.75,
    "Attention Mechanism": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Cooperative Trajectory Prediction",
        "canonical": "Cooperative Trajectory Prediction",
        "aliases": [
          "Cooperative Prediction",
          "Trajectory Cooperation"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a novel approach to improving trajectory prediction in V2X scenarios.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Anchor-oriented Decoder",
        "canonical": "Anchor-oriented Decoder",
        "aliases": [
          "AoD"
        ],
        "category": "unique_technical",
        "rationale": "The anchor-oriented decoder is a specific technique introduced in the paper, crucial for generating complete trajectories.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Multi-source Trajectory Fusion",
        "canonical": "Multi-source Trajectory Fusion",
        "aliases": [
          "Trajectory Fusion"
        ],
        "category": "unique_technical",
        "rationale": "This method is a key innovation in the paper, enhancing trajectory data integration from multiple sources.",
        "novelty_score": 0.68,
        "connectivity_score": 0.67,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Past Time Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "PTA"
        ],
        "category": "specific_connectable",
        "rationale": "The Past Time Attention module is a specialized application of attention mechanisms, relevant for capturing historical interactions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Cooperative Trajectory Prediction",
      "resolved_canonical": "Cooperative Trajectory Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Anchor-oriented Decoder",
      "resolved_canonical": "Anchor-oriented Decoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Multi-source Trajectory Fusion",
      "resolved_canonical": "Multi-source Trajectory Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.67,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Past Time Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios

**Korean Title:** CoPAD : V2X ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì•µì»¤ ì§€í–¥ ë””ì½”ë”ë¥¼ í™œìš©í•œ ë‹¤ì¤‘ ì†ŒìŠ¤ ê¶¤ì  ìœµí•© ë° í˜‘ë ¥ì  ê¶¤ì  ì˜ˆì¸¡

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15984.pdf)
**Category**: cs.CV
**Published**: 2025-09-22
**ArXiv ID**: [2509.15984](https://arxiv.org/abs/2509.15984)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/MAP_ End-to-End Autonomous Driving with Map-Assisted Planning_20250917|MAP: End-to-End Autonomous Driving with Map-Assisted Planning]] (81.7% similar)
- [[2025-09-22/DAOcc_ 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction_20250922|DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction]] (81.6% similar)
- [[2025-09-19/STEP_ Structured Training and Evaluation Platform for benchmarking trajectory prediction models_20250919|STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models]] (81.6% similar)
- [[2025-09-22/CoReVLA_ A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine_20250922|CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine]] (81.4% similar)
- [[2025-09-17/Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction_20250917|Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction]] (81.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Cooperative Trajectory Prediction|Cooperative Trajectory Prediction]], [[keywords/Anchor-oriented Decoder|Anchor-oriented Decoder]], [[keywords/Multi-source Trajectory Fusion|Multi-source Trajectory Fusion]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15984v1 Announce Type: new 
Abstract: Recently, data-driven trajectory prediction methods have achieved remarkable results, significantly advancing the development of autonomous driving. However, the instability of single-vehicle perception introduces certain limitations to trajectory prediction. In this paper, a novel lightweight framework for cooperative trajectory prediction, CoPAD, is proposed. This framework incorporates a fusion module based on the Hungarian algorithm and Kalman filtering, along with the Past Time Attention (PTA) module, mode attention module and anchor-oriented decoder (AoD). It effectively performs early fusion on multi-source trajectory data from vehicles and road infrastructure, enabling the trajectories with high completeness and accuracy. The PTA module can efficiently capture potential interaction information among historical trajectories, and the mode attention module is proposed to enrich the diversity of predictions. Additionally, the decoder based on sparse anchors is designed to generate the final complete trajectories. Extensive experiments show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq dataset, validating the effectiveness of the model in cooperative trajectory prediction in V2X scenarios.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15984v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ìµœê·¼ ë°ì´í„° ê¸°ë°˜ ê¶¤ì  ì˜ˆì¸¡ ë°©ë²•ì€ ììœ¨ ì£¼í–‰ì˜ ë°œì „ì„ í¬ê²Œ ì§„ì „ì‹œí‚¤ë©° ì£¼ëª©í•  ë§Œí•œ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¨ì¼ ì°¨ëŸ‰ ì¸ì‹ì˜ ë¶ˆì•ˆì •ì„±ì€ ê¶¤ì  ì˜ˆì¸¡ì— ì¼ì •í•œ í•œê³„ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í˜‘ë ¥ì  ê¶¤ì  ì˜ˆì¸¡ì„ ìœ„í•œ ìƒˆë¡œìš´ ê²½ëŸ‰ í”„ë ˆì„ì›Œí¬ì¸ CoPADë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ê³¼ ì¹¼ë§Œ í•„í„°ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìœµí•© ëª¨ë“ˆ, ê³¼ê±° ì‹œê°„ ì£¼ì˜(PTA) ëª¨ë“ˆ, ëª¨ë“œ ì£¼ì˜ ëª¨ë“ˆ, ì•µì»¤ ì§€í–¥ ë””ì½”ë”(AoD)ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì°¨ëŸ‰ ë° ë„ë¡œ ì¸í”„ë¼ë¡œë¶€í„°ì˜ ë‹¤ì¤‘ ì†ŒìŠ¤ ê¶¤ì  ë°ì´í„°ë¥¼ ì¡°ê¸°ì— ìœµí•©í•˜ì—¬ ë†’ì€ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì„ ê°€ì§„ ê¶¤ì ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. PTA ëª¨ë“ˆì€ ì—­ì‚¬ì  ê¶¤ì  ê°„ì˜ ì ì¬ì  ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í¬ì°©í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë“œ ì£¼ì˜ ëª¨ë“ˆì€ ì˜ˆì¸¡ì˜ ë‹¤ì–‘ì„±ì„ í’ë¶€í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, í¬ì†Œ ì•µì»¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë””ì½”ë”ëŠ” ìµœì¢… ì™„ì „í•œ ê¶¤ì ì„ ìƒì„±í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼, CoPADëŠ” DAIR-V2X-Seq ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, V2X ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í˜‘ë ¥ì  ê¶¤ì  ì˜ˆì¸¡ì— ìˆì–´ ëª¨ë¸ì˜ íš¨ê³¼ì„±ì„ ê²€ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ìµœê·¼ ë°ì´í„° ê¸°ë°˜ ê¶¤ì  ì˜ˆì¸¡ ë°©ë²•ì´ ììœ¨ì£¼í–‰ ë°œì „ì— í¬ê²Œ ê¸°ì—¬í–ˆìœ¼ë‚˜, ë‹¨ì¼ ì°¨ëŸ‰ ì¸ì‹ì˜ ë¶ˆì•ˆì •ì„±ì€ í•œê³„ë¡œ ì‘ìš©í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í˜‘ë ¥ì  ê¶¤ì  ì˜ˆì¸¡ì„ ìœ„í•œ ê²½ëŸ‰ í”„ë ˆì„ì›Œí¬ CoPADë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ê³¼ ì¹¼ë§Œ í•„í„°ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìœµí•© ëª¨ë“ˆ, ê³¼ê±° ì‹œê°„ ì£¼ì˜(PTA) ëª¨ë“ˆ, ëª¨ë“œ ì£¼ì˜ ëª¨ë“ˆ, ì•µì»¤ ì§€í–¥ ë””ì½”ë”(AoD)ë¥¼ í¬í•¨í•˜ì—¬ ë‹¤ì¤‘ ì†ŒìŠ¤ ê¶¤ì  ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìœµí•©í•˜ì—¬ ë†’ì€ ì •í™•ë„ì˜ ê¶¤ì ì„ ìƒì„±í•©ë‹ˆë‹¤. PTA ëª¨ë“ˆì€ ê³¼ê±° ê¶¤ì  ê°„ ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í¬ì°©í•˜ë©°, ëª¨ë“œ ì£¼ì˜ ëª¨ë“ˆì€ ì˜ˆì¸¡ì˜ ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ë˜í•œ, í¬ì†Œ ì•µì»¤ ê¸°ë°˜ ë””ì½”ë”ëŠ” ìµœì¢… ê¶¤ì ì„ ìƒì„±í•©ë‹ˆë‹¤. DAIR-V2X-Seq ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, CoPADëŠ” V2X ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í˜‘ë ¥ì  ê¶¤ì  ì˜ˆì¸¡ì˜ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CoPADëŠ” í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ê³¼ ì¹¼ë§Œ í•„í„°ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìœµí•© ëª¨ë“ˆì„ í¬í•¨í•˜ì—¬ ê²½ëŸ‰í™”ëœ í˜‘ë ¥ ê¶¤ì  ì˜ˆì¸¡ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. Past Time Attention (PTA) ëª¨ë“ˆì€ ê³¼ê±° ê¶¤ì  ê°„ì˜ ì ì¬ì  ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤.
- 3. ëª¨ë“œ ì£¼ì˜ ëª¨ë“ˆì€ ì˜ˆì¸¡ì˜ ë‹¤ì–‘ì„±ì„ í’ë¶€í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. í¬ì†Œ ì•µì»¤ ê¸°ë°˜ ë””ì½”ë”ëŠ” ìµœì¢… ì™„ì „í•œ ê¶¤ì ì„ ìƒì„±í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. CoPADëŠ” DAIR-V2X-Seq ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì—¬ V2X ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í˜‘ë ¥ ê¶¤ì  ì˜ˆì¸¡ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 12:16:22*