---
keywords:
  - Vision-Language Model
  - Agentic Reasoning
  - Adversarial Robustness
  - Hallucination in Models
  - Structured Inference
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2509.15435
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T08:56:27.929430",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Agentic Reasoning",
    "Adversarial Robustness",
    "Hallucination in Models",
    "Structured Inference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Agentic Reasoning": 0.8,
    "Adversarial Robustness": 0.82,
    "Hallucination in Models": 0.78,
    "Structured Inference": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLMs",
          "Vision-Language Systems"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on improving multimodal capabilities and robustness.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Agentic Reasoning",
        "canonical": "Agentic Reasoning",
        "aliases": [
          "ORCA Framework"
        ],
        "category": "unique_technical",
        "rationale": "Agentic Reasoning is a novel framework proposed in the paper for enhancing model robustness.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adversarial Robustness",
        "canonical": "Adversarial Robustness",
        "aliases": [
          "Robustness to Adversarial Attacks"
        ],
        "category": "specific_connectable",
        "rationale": "Adversarial Robustness is a key outcome of the proposed framework, relevant to many existing models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Hallucination",
        "canonical": "Hallucination in Models",
        "aliases": [
          "Model Hallucination"
        ],
        "category": "specific_connectable",
        "rationale": "Addressing hallucination is a critical challenge in LVLMs, directly tackled by the ORCA framework.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "Structured Inference Reasoning",
        "canonical": "Structured Inference",
        "aliases": [
          "Inference Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Structured Inference Reasoning is a specific method used in ORCA to improve model accuracy.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "Large Vision-Language Models",
      "test-time structured inference"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Agentic Reasoning",
      "resolved_canonical": "Agentic Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adversarial Robustness",
      "resolved_canonical": "Adversarial Robustness",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Hallucination",
      "resolved_canonical": "Hallucination in Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Structured Inference Reasoning",
      "resolved_canonical": "Structured Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models

**Korean Title:** ORCA: 시각-언어 모델에서 환각 및 적대적 견고성을 위한 에이전트적 추론

## 📋 메타데이터

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.15435.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2509.15435](https://arxiv.org/abs/2509.15435)

## 🔗 유사한 논문
- [[2025-09-22/ORIC_ Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models_20250922|ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models]] (88.7% similar)
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (84.6% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (84.1% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (83.4% similar)
- [[2025-09-22/EyePCR_ A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery_20250922|EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Adversarial Robustness|Adversarial Robustness]], [[keywords/Hallucination in Models|Hallucination in Models]]
**⚡ Unique Technical**: [[keywords/Agentic Reasoning|Agentic Reasoning]], [[keywords/Structured Inference|Structured Inference]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15435v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities but remain vulnerable to hallucinations from intrinsic errors and adversarial attacks from external exploitations, limiting their reliability in real-world applications. We present ORCA, an agentic reasoning framework that improves the factual accuracy and adversarial robustness of pretrained LVLMs through test-time structured inference reasoning with a suite of small vision models (less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act loop, querying multiple visual tools with evidential questions, validating cross-model inconsistencies, and refining predictions iteratively without access to model internals or retraining. ORCA also stores intermediate reasoning traces, which supports auditable decision-making. Though designed primarily to mitigate object-level hallucinations, ORCA also exhibits emergent adversarial robustness without requiring adversarial training or defense mechanisms. We evaluate ORCA across three settings: (1) clean images on hallucination benchmarks, (2) adversarially perturbed images without defense, and (3) adversarially perturbed images with defense applied. On the POPE hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\% to +40.67\% across different subsets. Under adversarial perturbations on POPE, ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined with defense techniques on adversarially perturbed AMBER images, ORCA further improves standalone LVLM performance, with gains ranging from +1.20\% to +48.00\% across evaluation metrics. These results demonstrate that ORCA offers a promising path toward building more reliable and robust multimodal systems.

## 🔍 Abstract (한글 번역)

arXiv:2509.15435v1 발표 유형: 교차  
초록: 대형 비전-언어 모델(LVLMs)은 강력한 다중 모달 능력을 보이지만, 내재적 오류로 인한 환각과 외부 악용으로 인한 적대적 공격에 취약하여 실제 응용에서의 신뢰성을 제한합니다. 우리는 사전 학습된 LVLMs의 사실적 정확성과 적대적 강건성을 향상시키기 위해 테스트 시간 구조적 추론을 통해 소형 비전 모델(3B 매개변수 미만)의 집합을 사용하는 ORCA라는 에이전트 추론 프레임워크를 제시합니다. ORCA는 관찰-추론-비판-행동 루프를 통해 여러 시각 도구에 증거 기반 질문을 하고, 모델 간 불일치를 검증하며, 모델 내부 접근이나 재훈련 없이 예측을 반복적으로 수정합니다. ORCA는 또한 감사 가능한 의사 결정을 지원하는 중간 추론 흔적을 저장합니다. 주로 객체 수준의 환각을 완화하기 위해 설계되었지만, ORCA는 적대적 훈련이나 방어 메커니즘 없이도 발생하는 적대적 강건성을 보입니다. 우리는 ORCA를 세 가지 설정에서 평가합니다: (1) 환각 벤치마크에서의 깨끗한 이미지, (2) 방어 없이 적대적으로 변형된 이미지, (3) 방어가 적용된 적대적으로 변형된 이미지. POPE 환각 벤치마크에서 ORCA는 독립형 LVLM 성능을 다양한 하위 집합에서 +3.64%에서 +40.67%까지 향상시킵니다. POPE에서의 적대적 변형 하에서 ORCA는 LVLMs 전반에 걸쳐 평균 정확도 증가 +20.11%를 달성합니다. 적대적으로 변형된 AMBER 이미지에 방어 기법을 결합할 때, ORCA는 독립형 LVLM 성능을 더욱 향상시켜 평가 지표 전반에 걸쳐 +1.20%에서 +48.00%까지의 향상을 보여줍니다. 이러한 결과는 ORCA가 보다 신뢰할 수 있고 강력한 다중 모달 시스템을 구축하는 데 유망한 경로를 제공함을 보여줍니다.

## 📝 요약

대형 비전-언어 모델(LVLM)은 강력한 다중 모달 기능을 가지고 있지만, 내재적 오류와 외부 공격으로 인한 환각에 취약하여 실제 응용에서 신뢰성이 제한됩니다. 본 논문에서는 ORCA라는 에이전트적 추론 프레임워크를 제안하여, 사전 학습된 LVLM의 사실적 정확성과 적대적 강인성을 향상시킵니다. ORCA는 작은 비전 모델을 활용하여 테스트 시 구조화된 추론을 수행하며, 모델 내부 접근이나 재학습 없이 예측을 개선합니다. ORCA는 중간 추론 과정을 저장하여 검증 가능한 의사결정을 지원하며, 객체 수준 환각을 줄이기 위해 설계되었지만, 적대적 강인성도 발현합니다. POPE 환각 벤치마크에서 ORCA는 LVLM 성능을 최대 40.67%까지 향상시켰으며, 적대적 이미지에서도 평균 20.11%의 정확도 향상을 보였습니다. 이러한 결과는 ORCA가 더 신뢰할 수 있고 강력한 다중 모달 시스템 구축에 유망한 경로를 제공함을 시사합니다.

## 🎯 주요 포인트

- 1. ORCA는 대형 비전-언어 모델(LVLM)의 사실적 정확성과 적대적 견고성을 향상시키기 위해 설계된 에이전트적 추론 프레임워크입니다.
- 2. ORCA는 관찰-추론-비판-행동 루프를 통해 여러 시각 도구를 사용하여 증거 기반 질문을 하고, 교차 모델 불일치를 검증하며, 예측을 반복적으로 수정합니다.
- 3. ORCA는 주로 객체 수준의 환각을 완화하도록 설계되었지만, 적대적 훈련이나 방어 메커니즘 없이도 적대적 견고성을 보여줍니다.
- 4. POPE 환각 벤치마크에서 ORCA는 독립형 LVLM 성능을 +3.64%에서 +40.67%까지 개선했습니다.
- 5. ORCA는 적대적으로 변형된 이미지에 방어 기술을 결합할 때, 평가 지표 전반에 걸쳐 독립형 LVLM 성능을 +1.20%에서 +48.00%까지 추가로 향상시켰습니다.


---

*Generated on 2025-09-23 08:56:27*