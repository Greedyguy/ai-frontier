---
keywords:
  - Large Language Model
  - Low-Resource Languages
  - Document-Level Translation
  - Health Domain
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2508.16048
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:08:40.426084",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Low-Resource Languages",
    "Document-Level Translation",
    "Health Domain"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Low-Resource Languages": 0.78,
    "Document-Level Translation": 0.77,
    "Health Domain": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's evaluation of translation performance, linking to broader AI advancements.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "low-resource languages",
        "canonical": "Low-Resource Languages",
        "aliases": [
          "under-resourced languages"
        ],
        "category": "unique_technical",
        "rationale": "The focus on low-resource languages is unique to this research domain and crucial for understanding the paper's contribution.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "document-level translation",
        "canonical": "Document-Level Translation",
        "aliases": [
          "document translation"
        ],
        "category": "unique_technical",
        "rationale": "Document-level translation is a specific approach that enhances translation accuracy in specialized domains.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "health domain",
        "canonical": "Health Domain",
        "aliases": [
          "medical domain",
          "healthcare domain"
        ],
        "category": "unique_technical",
        "rationale": "The health domain is a specialized area where translation accuracy is critical, linking to domain-specific research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "low-resource languages",
      "resolved_canonical": "Low-Resource Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "document-level translation",
      "resolved_canonical": "Document-Level Translation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "health domain",
      "resolved_canonical": "Health Domain",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages

**Korean Title:** OpenWHO: ì €ìì› ì–¸ì–´ì˜ ê±´ê°• ë²ˆì—­ì„ ìœ„í•œ ë¬¸ì„œ ìˆ˜ì¤€ ë³‘ë ¬ ì½”í¼ìŠ¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.16048.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2508.16048](https://arxiv.org/abs/2508.16048)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MedCOD_ Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework_20250922|MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework]] (82.1% similar)
- [[2025-09-19/OpenLens AI_ Fully Autonomous Research Agent for Health Infomatics_20250919|OpenLens AI: Fully Autonomous Research Agent for Health Infomatics]] (80.6% similar)
- [[2025-09-22/UPRPRC_ Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations_20250922|UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations]] (79.9% similar)
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (79.6% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL: Toward Expert-Level Medical Text Validation with Language Models]] (79.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Low-Resource Languages|Low-Resource Languages]], [[keywords/Document-Level Translation|Document-Level Translation]], [[keywords/Health Domain|Health Domain]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.16048v3 Announce Type: replace-cross 
Abstract: In machine translation (MT), health is a high-stakes domain characterised by widespread deployment and domain-specific vocabulary. However, there is a lack of MT evaluation datasets for low-resource languages in this domain. To address this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978 documents and 26,824 sentences from the World Health Organization's e-learning platform. Sourced from expert-authored, professionally translated materials shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages, of which nine are low-resource. Leveraging this new resource, we evaluate modern large language models (LLMs) against traditional MT models. Our findings reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5 Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our low-resource test set. Further, we investigate how LLM context utilisation affects accuracy, finding that the benefits of document-level translation are most pronounced in specialised domains like health. We release the OpenWHO corpus to encourage further research into low-resource MT in the health domain.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.16048v3 ë°œí‘œ ìœ í˜•: êµì²´-í¬ë¡œìŠ¤  
ì´ˆë¡: ê¸°ê³„ ë²ˆì—­(MT)ì—ì„œ ê±´ê°• ë¶„ì•¼ëŠ” ê´‘ë²”ìœ„í•œ ë°°í¬ì™€ ë¶„ì•¼ë³„ ì–´íœ˜ë¡œ íŠ¹ì§•ì§€ì–´ì§€ëŠ” ê³ ìœ„í—˜ ë¶„ì•¼ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë¶„ì•¼ì—ì„œ ì €ìì› ì–¸ì–´ì— ëŒ€í•œ MT í‰ê°€ ë°ì´í„°ì…‹ì€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì„¸ê³„ë³´ê±´ê¸°êµ¬ì˜ e-ëŸ¬ë‹ í”Œë«í¼ì—ì„œ ì œê³µë˜ëŠ” 2,978ê°œì˜ ë¬¸ì„œì™€ 26,824ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ë¬¸ì„œ ìˆ˜ì¤€ì˜ ë³‘ë ¬ ì½”í¼ìŠ¤ì¸ OpenWHOë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì „ë¬¸ê°€ê°€ ì‘ì„±í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë²ˆì—­ëœ ìë£Œë¡œ, ì›¹ í¬ë¡¤ë§ìœ¼ë¡œë¶€í„° ë³´í˜¸ëœ OpenWHOëŠ” 20ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì–¸ì–´ë¥¼ í¬í•¨í•˜ë©°, ê·¸ ì¤‘ 9ê°œëŠ” ì €ìì› ì–¸ì–´ì…ë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ ìì›ì„ í™œìš©í•˜ì—¬, ìš°ë¦¬ëŠ” í˜„ëŒ€ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì „í†µì ì¸ MT ëª¨ë¸ê³¼ ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” LLMì´ ì „í†µì ì¸ MT ëª¨ë¸ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, Gemini 2.5 FlashëŠ” ì €ìì› í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ NLLB-54Bë³´ë‹¤ ChrF ì ìˆ˜ê°€ +4.79 í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, LLMì˜ ë¬¸ë§¥ í™œìš©ì´ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•˜ì—¬, ë¬¸ì„œ ìˆ˜ì¤€ ë²ˆì—­ì˜ ì´ì ì´ ê±´ê°•ê³¼ ê°™ì€ ì „ë¬¸ ë¶„ì•¼ì—ì„œ ê°€ì¥ ë‘ë“œëŸ¬ì§„ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê±´ê°• ë¶„ì•¼ì˜ ì €ìì› MT ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ OpenWHO ì½”í¼ìŠ¤ë¥¼ ê³µê°œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì €ìë“¤ì´ ì„¸ê³„ë³´ê±´ê¸°êµ¬ì˜ e-learning í”Œë«í¼ì—ì„œ ìˆ˜ì§‘í•œ 2,978ê°œì˜ ë¬¸ì„œì™€ 26,824ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ OpenWHOë¼ëŠ” ë³‘ë ¬ ì½”í¼ìŠ¤ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì½”í¼ìŠ¤ëŠ” 20ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ í¬í•¨í•˜ë©°, ê·¸ì¤‘ 9ê°œëŠ” ìì›ì´ ë¶€ì¡±í•œ ì–¸ì–´ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ì „í†µì ì¸ ê¸°ê³„ ë²ˆì—­(MT) ëª¨ë¸ì„ ë¹„êµ í‰ê°€í•œ ê²°ê³¼, LLMì´ ì „í†µì ì¸ MT ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ Gemini 2.5 Flash ëª¨ë¸ì´ NLLB-54B ëª¨ë¸ë³´ë‹¤ ChrF ì ìˆ˜ê°€ 4.79ì  ë” ë†’ì•˜ìŠµë‹ˆë‹¤. ë˜í•œ, ë¬¸ì„œ ìˆ˜ì¤€ ë²ˆì—­ì˜ ë§¥ë½ í™œìš©ì´ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•˜ì—¬, ê±´ê°•ê³¼ ê°™ì€ ì „ë¬¸ ë¶„ì•¼ì—ì„œ ê·¸ íš¨ê³¼ê°€ ë‘ë“œëŸ¬ì§ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. OpenWHO ì½”í¼ìŠ¤ëŠ” ê±´ê°• ë¶„ì•¼ì˜ ì €ìì› ì–¸ì–´ MT ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. OpenWHOëŠ” WHOì˜ e-learning í”Œë«í¼ì—ì„œ ìˆ˜ì§‘ëœ 2,978ê°œì˜ ë¬¸ì„œì™€ 26,824ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ë³‘ë ¬ ì½”í¼ìŠ¤ë¡œ, 20ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ í¬í•¨í•˜ë©° ê·¸ ì¤‘ 9ê°œëŠ” ì €ìì› ì–¸ì–´ì…ë‹ˆë‹¤.
- 2. OpenWHOë¥¼ í™œìš©í•œ ì—°êµ¬ì—ì„œ í˜„ëŒ€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì „í†µì ì¸ ê¸°ê³„ ë²ˆì—­(MT) ëª¨ë¸ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 3. Gemini 2.5 Flash ëª¨ë¸ì€ ì €ìì› í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ NLLB-54B ëª¨ë¸ì— ë¹„í•´ ChrF ì ìˆ˜ê°€ +4.79 í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ë¬¸ì„œ ìˆ˜ì¤€ ë²ˆì—­ì˜ ì´ì ì€ ê±´ê°•ê³¼ ê°™ì€ ì „ë¬¸í™”ëœ ë„ë©”ì¸ì—ì„œ ê°€ì¥ ë‘ë“œëŸ¬ì§€ë©°, LLMì˜ ë¬¸ë§¥ í™œìš©ì´ ë²ˆì—­ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.
- 5. OpenWHO ì½”í¼ìŠ¤ë¥¼ ê³µê°œí•˜ì—¬ ê±´ê°• ë¶„ì•¼ì˜ ì €ìì› ê¸°ê³„ ë²ˆì—­ ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê³ ì í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:08:40*