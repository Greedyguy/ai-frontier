---
keywords:
  - Large Language Model
  - Causal2Vec
  - Contextual Token
  - Attention Mechanism
  - Massive Text Embeddings Benchmark
category: cs.AI
publish_date: 2025-09-22
arxiv_id: 2507.23386
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-23T10:07:10.615438",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Causal2Vec",
    "Contextual Token",
    "Attention Mechanism",
    "Massive Text Embeddings Benchmark"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Causal2Vec": 0.88,
    "Contextual Token": 0.78,
    "Attention Mechanism": 0.82,
    "Massive Text Embeddings Benchmark": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Decoder-only large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Decoder-only LLM"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the broader concept of language models, which is central to the paper.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Causal2Vec",
        "canonical": "Causal2Vec",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel embedding model that is the focus of the paper.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Contextual token",
        "canonical": "Contextual Token",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a key component of the proposed method, highlighting its unique approach.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Causal attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Causal Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the broader concept of attention mechanisms, crucial for understanding model architecture.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Massive Text Embeddings Benchmark",
        "canonical": "Massive Text Embeddings Benchmark",
        "aliases": [
          "MTEB"
        ],
        "category": "unique_technical",
        "rationale": "Provides a benchmark context for evaluating the model's performance.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "embedding models",
      "semantic information",
      "pretraining"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Decoder-only large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Causal2Vec",
      "resolved_canonical": "Causal2Vec",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Contextual token",
      "resolved_canonical": "Contextual Token",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Causal attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Massive Text Embeddings Benchmark",
      "resolved_canonical": "Massive Text Embeddings Benchmark",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models

**Korean Title:** Causal2Vec: ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë‹¤ì¬ë‹¤ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ë¡œ ê°œì„ í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250922|20250922]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.23386.pdf)
**Category**: cs.AI
**Published**: 2025-09-22
**ArXiv ID**: [2507.23386](https://arxiv.org/abs/2507.23386)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (83.4% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.7% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (82.0% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (81.8% similar)
- [[2025-09-19/VLM-E2E_ Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion_20250919|VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion]] (81.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Causal2Vec|Causal2Vec]], [[keywords/Contextual Token|Contextual Token]], [[keywords/Massive Text Embeddings Benchmark|Massive Text Embeddings Benchmark]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.23386v2 Announce Type: replace-cross 
Abstract: Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.23386v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ìì—°ì–´ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ ì •ë³´ë¥¼ ë‹¤ì–‘í•œ ì„ë² ë”© ì‘ì—…ì„ ìœ„í•´ ë°€ì§‘ ë²¡í„° í‘œí˜„ìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ê¸°ì¡´ ë°©ë²•ì€ LLMì—ì„œ ì¸ê³¼ì  ì£¼ì˜ ë§ˆìŠ¤í¬ë¥¼ ì œê±°í•˜ì—¬ ì–‘ë°©í–¥ ì£¼ì˜ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, ì´ëŠ” ì‚¬ì „ í•™ìŠµ ì¤‘ì— íšë“í•œ ì˜ë¯¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ ì €í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì£¼ìš” ë‹¨ë°©í–¥ ì ‘ê·¼ë²•ì€ ì¸ê³¼ì  ì£¼ì˜ì˜ ê³ ìœ í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¶”ê°€ ì…ë ¥ í…ìŠ¤íŠ¸ì— ì˜ì¡´í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ í•„ì—°ì ìœ¼ë¡œ ê³„ì‚° ë¹„ìš©ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë””ì½”ë” ì „ìš© LLMì˜ ì›ë˜ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ìƒë‹¹í•œ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ ë„ì…í•˜ì§€ ì•Šê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë§ì¶¤í™”ëœ ë²”ìš© ì„ë² ë”© ëª¨ë¸ì¸ Causal2Vecì„ ì œì•ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë¨¼ì € ê²½ëŸ‰ì˜ BERT ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì»¨í…ìŠ¤ì¶”ì–¼ í† í°ìœ¼ë¡œ ì‚¬ì „ ì¸ì½”ë”©í•œ í›„, ì´ë¥¼ LLMì˜ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì•ì— ì¶”ê°€í•˜ì—¬ ê° í† í°ì´ ë¯¸ë˜ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì§€ ì•Šê³ ë„ ë§¥ë½í™”ëœ ì •ë³´ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, ë§ˆì§€ë§‰ í† í° í’€ë§ì— ì˜í•´ ë„ì…ëœ ìµœì‹  í¸í–¥ì„ ì™„í™”í•˜ê³  LLMì´ ì»¨í…ìŠ¤ì¶”ì–¼ í† í°ì— ì¸ì½”ë”©ëœ ì˜ë¯¸ ì •ë³´ë¥¼ ë” ì˜ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë•ê¸° ìœ„í•´, ì»¨í…ìŠ¤ì¶”ì–¼ ë° EOS í† í°ì˜ ë§ˆì§€ë§‰ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ìµœì¢… í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ, Causal2Vecì€ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ë°ì´í„°ì…‹ë§Œìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì¤‘ì—ì„œ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬(MTEB)ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë°©ë²•ì— ë¹„í•´ í•„ìš”í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ 85%ê¹Œì§€, ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 82%ê¹Œì§€ ì¤„ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Causal2VecëŠ” ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë²”ìš© ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ì–‘ë°©í–¥ ì£¼ì˜(attention)ë¥¼ ìœ„í•´ ì¸ê³¼ì  ì£¼ì˜ ë§ˆìŠ¤í¬ë¥¼ ì œê±°í•˜ì—¬ ëª¨ë¸ì˜ ì˜ë¯¸ ì •ë³´ ì¶”ì¶œ ëŠ¥ë ¥ì„ ì €í•´í•  ìˆ˜ ìˆëŠ” ë°˜ë©´, Causal2VecëŠ” LLMì˜ ì›ë˜ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ ë„ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤. ê²½ëŸ‰ BERT ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ì „ ì¸ì½”ë”©í•˜ì—¬ ë¬¸ë§¥í™”ëœ í† í°ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ LLM ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ê° í† í°ì´ ë¯¸ë˜ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì§€ ì•Šê³ ë„ ë¬¸ë§¥ ì •ë³´ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, ë§ˆì§€ë§‰ í† í° í’€ë§ì˜ ìµœì‹ ì„± í¸í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ ë¬¸ë§¥í™”ëœ í† í°ê³¼ EOS í† í°ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. Causal2VecëŠ” ê³µê°œëœ ê²€ìƒ‰ ë°ì´í„°ì…‹ë§Œìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ 85%, ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 82% ì¤„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Causal2VecëŠ” ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì„¤ê³„ëœ ë²”ìš© ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤.
- 2. Causal2VecëŠ” LLMì˜ ì›ë˜ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ìƒë‹¹í•œ ê³„ì‚° ë¹„ìš©ì„ ì¶”ê°€í•˜ì§€ ì•Šê³ ë„ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 3. ì´ ëª¨ë¸ì€ ê²½ëŸ‰ì˜ BERT ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ì „ ì¸ì½”ë”©í•˜ê³ , ì´ë¥¼ LLMì˜ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ê° í† í°ì´ ë¬¸ë§¥í™”ëœ ì •ë³´ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- 4. Causal2VecëŠ” ë§ˆì§€ë§‰ í† í° í’€ë§ì— ì˜í•´ ë„ì…ëœ ìµœì‹  í¸í–¥ì„ ì™„í™”í•˜ê³ , ë¬¸ë§¥ í† í°ì— ì¸ì½”ë”©ëœ ì˜ë¯¸ ì •ë³´ë¥¼ ë” ì˜ í™œìš©í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
- 5. Causal2VecëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ë°ì´í„°ì…‹ë§Œìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì¤‘ì—ì„œ Massive Text Embeddings Benchmark (MTEB)ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ 85%, ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 82%ê¹Œì§€ ì¤„ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-23 10:07:10*