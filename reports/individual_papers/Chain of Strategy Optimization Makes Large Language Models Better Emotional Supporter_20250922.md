# Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter

**Korean Title:** 전략 최적화 체인은 대형 언어 모델을 더 나은 감정 지원자로 만든다.

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Turn-level Preference Modeling|Turn-level Preference Modeling]] [[keywords/specific/Monte Carlo Tree Search|Monte Carlo Tree Search]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/unique/Chain of Strategy Optimization|Chain of Strategy Optimization]] [[categories/cs.CL|cs.CL]] [[2025-09-22/Empathy-R1_ A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support_20250922|Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support]] (81.0% similar) [[2025-09-22/Beyond Linear Steering_ Unified Multi-Attribute Control for Language Models_20250922|Beyond Linear Steering: Unified Multi-Attribute Control for Language Models]] (80.5% similar) [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (80.3% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Turn-level Preference Modeling
**🔗 Specific Connectable**: Monte Carlo Tree Search
**🔬 Broad Technical**: Large Language Models, Natural Language Processing
**⭐ Unique Technical**: Chain of Strategy Optimization
## 🔗 유사한 논문
- [[2025-09-22/Empathy-R1_ A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support_20250922|Empathy-R1 A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support]] (81.0% similar)
- [[2025-09-22/Beyond Linear Steering_ Unified Multi-Attribute Control for Language Models_20250922|Beyond Linear Steering Unified Multi-Attribute Control for Language Models]] (80.5% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (80.3% similar)
- [[2025-09-17/DSCC-HS_ A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models_20250917|DSCC-HS A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (80.1% similar)
- [[2025-09-18/Empathy-R1_ A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support_20250918|Empathy-R1 A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support]] (79.9% similar)


**ArXiv ID**: [2503.05362](https://arxiv.org/abs/2503.05362)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2503.05362.pdf)


**ArXiv ID**: [2503.05362](https://arxiv.org/abs/2503.05362)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2503.05362.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Turn-level Preference Modeling
**🔗 Specific Connectable**: Monte Carlo Tree Search
**⭐ Unique Technical**: Chain of Strategy Optimization
**🔬 Broad Technical**: Large Language Models, Natural Language Processing

## 🏷️ 추출된 키워드



`Large Language Models` • 

`Monte Carlo Tree Search` • 

`Chain of Strategy Optimization` • 

`Turn-level Preference Modeling`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2503.05362v3 Announce Type: replace 
Abstract: The growing emotional stress in modern society has increased the demand for Emotional Support Conversations (ESC). While Large Language Models (LLMs) show promise for ESC, they face two key challenges: (1) low strategy selection accuracy, and (2) preference bias, limiting their adaptability to emotional needs of users. Existing supervised fine-tuning (SFT) struggles to address these issues, as it rigidly trains models on single gold-standard responses without modeling nuanced strategy trade-offs. To overcome these limitations, we propose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes strategy selection preferences at each dialogue turn. We first leverage Monte Carlo Tree Search to construct ESC-Pro, a high-quality preference dataset with turn-level strategy-response pairs. Training on ESC-Pro with CSO improves both strategy accuracy and bias mitigation, enabling LLMs to generate more empathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B, Gemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT, highlighting the efficacy of fine-grained, turn-level preference modeling in ESC.

## 🔍 Abstract (한글 번역)

arXiv:2503.05362v3 발표 유형: 교체  
초록: 현대 사회에서 증가하는 감정적 스트레스는 감정 지원 대화(ESC)에 대한 수요를 증가시켰습니다. 대형 언어 모델(LLM)은 ESC에 대한 가능성을 보여주지만 두 가지 주요 과제에 직면해 있습니다: (1) 낮은 전략 선택 정확도, (2) 사용자 감정적 요구에 대한 적응성을 제한하는 선호 편향. 기존의 지도 학습 기반 미세 조정(SFT)은 단일 표준 응답에 모델을 엄격하게 훈련시키며, 미세한 전략 간의 균형을 모델링하지 못해 이러한 문제를 해결하는 데 어려움을 겪고 있습니다. 이러한 한계를 극복하기 위해, 우리는 대화의 각 턴에서 전략 선택 선호도를 최적화하는 새로운 접근 방식인 전략 체인 최적화(CSO)를 제안합니다. 먼저 몬테 카를로 트리 탐색을 활용하여 턴 수준의 전략-응답 쌍을 포함한 고품질 선호 데이터셋인 ESC-Pro를 구축합니다. CSO를 사용하여 ESC-Pro에서 훈련하면 전략 정확도와 편향 완화가 모두 개선되어 LLM이 보다 공감적이고 상황에 적합한 응답을 생성할 수 있게 됩니다. LLaMA-3.1-8B, Gemma-2-9B, Qwen2.5-7B에 대한 실험은 CSO가 표준 SFT를 능가함을 보여주며, ESC에서 세밀한 턴 수준의 선호 모델링의 효과를 강조합니다.

## 📝 요약

현대 사회의 감정적 스트레스 증가로 감정 지원 대화(ESC)에 대한 수요가 높아지고 있습니다. 대형 언어 모델(LLM)은 ESC에 유망하지만, 전략 선택 정확도가 낮고 사용자 감정 요구에 대한 적응성이 부족한 편향 문제가 있습니다. 기존의 지도 학습은 이러한 문제를 해결하기 어려운데, 이는 단일 정답에만 집중해 전략의 미세한 차이를 반영하지 못하기 때문입니다. 이를 해결하기 위해 우리는 대화 턴마다 전략 선택을 최적화하는 '전략 체인 최적화(CSO)'를 제안합니다. 몬테카를로 트리 탐색을 활용해 턴 수준 전략-응답 쌍을 포함한 ESC-Pro 데이터셋을 구축하고, 이를 통해 CSO를 훈련함으로써 전략 정확도와 편향 완화가 개선됩니다. 실험 결과, CSO는 기존 지도 학습을 능가하며, 감정 지원 대화에서 세밀한 턴 수준 선호 모델링의 효과를 입증했습니다.

## 🎯 주요 포인트


- 1. 현대 사회의 감정적 스트레스 증가로 인해 감정 지원 대화(ESC)에 대한 수요가 증가하고 있다.

- 2. 대형 언어 모델(LLM)은 ESC에 유망하지만 전략 선택 정확도 저하와 선호 편향이라는 두 가지 주요 문제에 직면해 있다.

- 3. 기존의 지도 학습 튜닝(SFT)은 단일 정답에 기반한 훈련으로 인해 전략 선택의 미세한 균형을 모델링하지 못한다.

- 4. 체인-오브-전략 최적화(CSO)는 각 대화 턴에서 전략 선택 선호도를 최적화하여 LLM의 전략 정확도와 편향 완화를 개선한다.

- 5. 실험 결과, CSO는 기존 SFT보다 우수한 성능을 보여 감정 지원 대화에서의 세밀한 턴 수준 선호도 모델링의 효과를 입증한다.


---

*Generated on 2025-09-22 16:35:42*