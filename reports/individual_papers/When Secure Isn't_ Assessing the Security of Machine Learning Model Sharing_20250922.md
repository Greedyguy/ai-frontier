# When Secure Isn't: Assessing the Security of Machine Learning Model Sharing

**Korean Title:** 보안이 보장되지 않을 때: 기계 학습 모델 공유의 보안성 평가

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/specific/Model Sharing|Model Sharing]] [[keywords/broad/Machine Learning|Machine Learning]] [[keywords/broad/Security|Security]] [[keywords/unique/0-day Vulnerabilities|0-day Vulnerabilities]] [[categories/cs.LG|cs.LG]] [[2025-09-22/PickleBall_ Secure Deserialization of Pickle-based Machine Learning Models (Extended Report)_20250922|PickleBall: Secure Deserialization of Pickle-based Machine Learning Models (Extended Report)]] (82.7% similar) [[2025-09-19/The Sum Leaks More Than Its Parts_ Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration_20250919|The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration]] (81.7% similar) [[2025-09-19/Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (80.8% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: Model Sharing
**🔬 Broad Technical**: Machine Learning, Security
**⭐ Unique Technical**: 0-day Vulnerabilities
## 🔗 유사한 논문
- [[2025-09-22/PickleBall_ Secure Deserialization of Pickle-based Machine Learning Models (Extended Report)_20250922|PickleBall Secure Deserialization of Pickle-based Machine Learning Models (Extended Report)]] (82.7% similar)
- [[2025-09-19/The Sum Leaks More Than Its Parts_ Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration_20250919|The Sum Leaks More Than Its Parts Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration]] (81.7% similar)
- [[2025-09-19/Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (80.8% similar)
- [[2025-09-18/Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection_20250918|Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection]] (80.6% similar)
- [[2025-09-18/The Cybersecurity of a Humanoid Robot_20250918|The Cybersecurity of a Humanoid Robot]] (80.5% similar)


**ArXiv ID**: [2509.06703](https://arxiv.org/abs/2509.06703)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.06703.pdf)


**ArXiv ID**: [2509.06703](https://arxiv.org/abs/2509.06703)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.06703.pdf)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: Model Sharing
**⭐ Unique Technical**: 0-day Vulnerabilities
**🔬 Broad Technical**: Machine Learning, Security

## 🏷️ 추출된 키워드



`Machine Learning` • 

`Security` • 

`Model Sharing` • 

`0-day Vulnerabilities`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.06703v2 Announce Type: replace-cross 
Abstract: The rise of model-sharing through frameworks and dedicated hubs makes Machine Learning significantly more accessible. Despite their benefits, these tools expose users to underexplored security risks, while security awareness remains limited among both practitioners and developers. To enable a more security-conscious culture in Machine Learning model sharing, in this paper we evaluate the security posture of frameworks and hubs, assess whether security-oriented mechanisms offer real protection, and survey how users perceive the security narratives surrounding model sharing. Our evaluation shows that most frameworks and hubs address security risks partially at best, often by shifting responsibility to the user. More concerningly, our analysis of frameworks advertising security-oriented settings and complete model sharing uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through this analysis, we debunk the misconceptions that the model-sharing problem is largely solved and that its security can be guaranteed by the file format used for sharing. As expected, our survey shows that the surrounding security narrative leads users to consider security-oriented settings as trustworthy, despite the weaknesses shown in this work. From this, we derive takeaways and suggestions to strengthen the security of model-sharing ecosystems.

## 🔍 Abstract (한글 번역)

arXiv:2509.06703v2 발표 유형: 교체-교차  
초록: 프레임워크와 전용 허브를 통한 모델 공유의 증가는 머신러닝의 접근성을 크게 향상시켰습니다. 이러한 도구들은 장점에도 불구하고, 사용자들을 충분히 탐구되지 않은 보안 위험에 노출시키며, 실무자와 개발자 모두의 보안 인식은 여전히 제한적입니다. 머신러닝 모델 공유에서 보다 보안 의식이 높은 문화를 조성하기 위해, 본 논문에서는 프레임워크와 허브의 보안 태세를 평가하고, 보안 지향적 메커니즘이 실제로 보호를 제공하는지 여부를 평가하며, 모델 공유를 둘러싼 보안 서사를 사용자가 어떻게 인식하는지를 조사합니다. 우리의 평가는 대부분의 프레임워크와 허브가 보안 위험을 최대한 부분적으로만 해결하며, 종종 책임을 사용자에게 전가한다는 것을 보여줍니다. 더욱 우려되는 점은, 보안 지향적 설정과 완전한 모델 공유를 광고하는 프레임워크에 대한 우리의 분석에서 임의 코드 실행을 가능하게 하는 6개의 제로데이 취약점이 발견되었습니다. 이 분석을 통해, 모델 공유 문제가 대체로 해결되었고, 파일 형식에 의해 보안이 보장될 수 있다는 오해를 불식시킵니다. 예상대로, 우리의 설문조사는 주변의 보안 서사가 사용자로 하여금 보안 지향적 설정을 신뢰할 수 있다고 여기게 만든다는 것을 보여주며, 본 연구에서 드러난 약점에도 불구하고 그러한 경향이 있음을 확인합니다. 이를 바탕으로, 모델 공유 생태계의 보안을 강화하기 위한 교훈과 제안을 도출합니다.

## 📝 요약

이 논문은 머신러닝 모델 공유의 보안 문제를 평가하고, 보안 인식 제고를 위한 방안을 제시합니다. 연구 결과, 대부분의 프레임워크와 허브는 보안 위험을 부분적으로만 다루며, 사용자에게 책임을 전가하는 경향이 있습니다. 특히, 보안 설정을 광고하는 프레임워크에서 임의 코드 실행이 가능한 6개의 제로데이 취약점이 발견되었습니다. 이러한 분석을 통해 모델 공유 문제는 여전히 해결되지 않았으며, 파일 형식만으로 보안을 보장할 수 없음을 입증합니다. 사용자 설문조사 결과, 보안 관련 설정이 신뢰할 수 있다고 인식되지만, 실제로는 취약점이 존재함을 보여줍니다. 이를 바탕으로 모델 공유 생태계의 보안을 강화하기 위한 제언을 제공합니다.

## 🎯 주요 포인트


- 1. 모델 공유 프레임워크와 허브는 보안 위험을 부분적으로만 해결하며, 종종 사용자에게 책임을 전가합니다.

- 2. 보안 지향 설정을 광고하는 프레임워크에서 임의 코드 실행을 가능하게 하는 6개의 제로데이 취약점을 발견했습니다.

- 3. 모델 공유 문제는 대부분 해결되었다는 오해와 파일 형식으로 보안을 보장할 수 있다는 오해를 불식시켰습니다.

- 4. 사용자들은 보안 지향 설정을 신뢰할 수 있다고 여기지만, 실제로는 취약점이 존재합니다.

- 5. 모델 공유 생태계의 보안을 강화하기 위한 제안과 교훈을 도출했습니다.


---

*Generated on 2025-09-22 16:15:29*