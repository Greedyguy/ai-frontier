# Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents

**Korean Title:** AI가 실패할 때 누가 책임을 지는가? AI 프라이버시 및 윤리적 사건의 원인, 주체, 결과 분석

## 📋 메타데이터

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Responsible AI Governance

## 🔗 유사한 논문
- [[2025-09-22/The Great AI Witch Hunt_ Reviewers Perception and (Mis)Conception of Generative AI in Research Writing_20250922|The Great AI Witch Hunt Reviewers Perception and (Mis)Conception of Generative AI in Research Writing]] (82.2% similar)
- [[2025-09-18/Designing AI-Agents with Personalities_ A Psychometric Approach_20250918|Designing AI-Agents with Personalities A Psychometric Approach]] (80.6% similar)
- [[2025-09-18/Can I Trust This Chatbot Assessing User Privacy in AI-Healthcare Chatbot Applications_20250918|Can I Trust This Chatbot Assessing User Privacy in AI-Healthcare Chatbot Applications]] (80.4% similar)
- [[2025-09-18/Position_ AI Safety Must Embrace an Antifragile Perspective_20250918|Position AI Safety Must Embrace an Antifragile Perspective]] (80.2% similar)
- [[2025-09-22/Where Do I 'Add the Egg'_ Exploring Agency and Ownership in AI Creative Co-Writing Systems_20250922|Where Do I 'Add the Egg' Exploring Agency and Ownership in AI Creative Co-Writing Systems]] (79.9% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2504.01029v2 Announce Type: replace-cross 
Abstract: The rapid growth of artificial intelligence (AI) technologies has raised major privacy and ethical concerns. However, existing AI incident taxonomies and guidelines lack grounding in real-world cases, limiting their effectiveness for prevention and mitigation. We analyzed 202 real-world AI privacy and ethical incidents to develop a taxonomy that classifies them across AI lifecycle stages and captures contributing factors, including causes, responsible entities, sources of disclosure, and impacts. Our findings reveal widespread harms from poor organizational decisions and legal non-compliance, limited corrective interventions, and rare reporting from AI developers and adopting entities. Our taxonomy offers a structured approach for systematic incident reporting and emphasizes the weaknesses of current AI governance frameworks. Our findings provide actionable guidance for policymakers and practitioners to strengthen user protections, develop targeted AI policies, enhance reporting practices, and foster responsible AI governance and innovation, especially in contexts such as social media and child protection.

## 🔍 Abstract (한글 번역)

arXiv:2504.01029v2 발표 유형: 교차 대체  
초록: 인공지능(AI) 기술의 급속한 발전은 주요한 개인정보 보호 및 윤리적 문제를 제기하고 있습니다. 그러나 기존의 AI 사건 분류 체계와 지침은 실제 사례에 기반하지 않아 예방 및 완화에 효과적이지 못합니다. 우리는 202건의 실제 AI 개인정보 및 윤리적 사건을 분석하여 AI 생애 주기 단계에 따라 분류하고, 원인, 책임 기관, 공개 출처 및 영향 등 기여 요인을 포착하는 분류 체계를 개발했습니다. 우리의 연구 결과는 조직의 잘못된 결정과 법적 비준수로 인한 광범위한 피해, 제한된 교정 조치, 그리고 AI 개발자 및 채택 기관의 드문 보고를 드러냅니다. 우리의 분류 체계는 체계적인 사건 보고를 위한 구조화된 접근 방식을 제공하며, 현재 AI 거버넌스 프레임워크의 약점을 강조합니다. 우리의 연구 결과는 정책 입안자와 실무자에게 사용자 보호를 강화하고, 목표 지향적인 AI 정책을 개발하며, 보고 관행을 개선하고, 특히 소셜 미디어 및 아동 보호와 같은 맥락에서 책임 있는 AI 거버넌스와 혁신을 촉진하기 위한 실행 가능한 지침을 제공합니다.

## 📝 요약

이 논문은 인공지능(AI) 기술의 급속한 발전으로 인한 프라이버시 및 윤리적 문제를 다루고 있습니다. 연구진은 202건의 실제 AI 프라이버시 및 윤리적 사건을 분석하여 AI 생애 주기 단계에 따른 분류 체계를 개발했습니다. 이 체계는 사건의 원인, 책임 주체, 공개 출처, 영향 등을 포함합니다. 연구 결과, 조직의 잘못된 결정과 법적 비준수로 인한 피해가 광범위하며, AI 개발자와 채택 기관의 보고가 드물다는 점이 드러났습니다. 이 분류 체계는 체계적인 사건 보고를 위한 구조적 접근을 제공하며, 현재 AI 거버넌스의 약점을 강조합니다. 정책 입안자와 실무자에게 사용자 보호 강화, AI 정책 개발, 보고 관행 개선, 책임 있는 AI 거버넌스 및 혁신 촉진을 위한 실질적인 지침을 제공합니다. 특히 소셜 미디어와 아동 보호와 같은 맥락에서 중요합니다.

## 🎯 주요 포인트

- 1. 인공지능(AI) 기술의 급속한 발전은 주요한 프라이버시 및 윤리적 문제를 야기하고 있다.

- 2. 기존 AI 사건 분류 체계와 가이드라인은 실제 사례에 기반하지 않아 예방 및 완화에 한계가 있다.

- 3. 202건의 실제 AI 프라이버시 및 윤리 사건 분석을 통해 AI 생애 주기 단계별로 분류하는 분류 체계를 개발하였다.

- 4. 분석 결과, 조직의 부실한 결정과 법적 비준수로 인한 피해가 광범위하게 발생하고 있으며, AI 개발자와 채택 기관의 보고는 드물다.

- 5. 연구 결과는 정책 입안자와 실무자에게 사용자 보호 강화, AI 정책 개발, 보고 관행 개선, 책임 있는 AI 거버넌스 및 혁신 촉진에 대한 실질적인 지침을 제공한다.

---

*Generated on 2025-09-22 14:45:40*