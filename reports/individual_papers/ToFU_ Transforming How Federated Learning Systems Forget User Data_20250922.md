# ToFU: Transforming How Federated Learning Systems Forget User Data

**Korean Title:** ToFU: ì—°í•© í•™ìŠµ ì‹œìŠ¤í…œì´ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ìŠëŠ” ë°©ì‹ì„ ë³€í˜í•˜ë‹¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Learning-to-unlearn

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/CUFG_ Curriculum Unlearning Guided by the Forgetting Gradient_20250918|CUFG Curriculum Unlearning Guided by the Forgetting Gradient]] (85.5% similar)
- [[2025-09-22/Pre-Forgettable Models_ Prompt Learning as a Native Mechanism for Unlearning_20250922|Pre-Forgettable Models Prompt Learning as a Native Mechanism for Unlearning]] (82.5% similar)
- [[2025-09-17/Differential Privacy in Federated Learning_ Mitigating Inference Attacks with Randomized Response_20250917|Differential Privacy in Federated Learning Mitigating Inference Attacks with Randomized Response]] (79.7% similar)
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (79.7% similar)
- [[2025-09-19/Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning_20250919|Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning]] (79.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15861v1 Announce Type: new 
Abstract: Neural networks unintentionally memorize training data, creating privacy risks in federated learning (FL) systems, such as inference and reconstruction attacks on sensitive data. To mitigate these risks and to comply with privacy regulations, Federated Unlearning (FU) has been introduced to enable participants in FL systems to remove their data's influence from the global model. However, current FU methods primarily act post-hoc, struggling to efficiently erase information deeply memorized by neural networks. We argue that effective unlearning necessitates a paradigm shift: designing FL systems inherently amenable to forgetting. To this end, we propose a learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework that incorporates transformations during the learning process to reduce memorization of specific instances. Our theoretical analysis reveals how transformation composition provably bounds instance-specific information, directly simplifying subsequent unlearning. Crucially, ToFU can work as a plug-and-play framework that improves the performance of existing FU methods. Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU outperforms existing FU baselines, enhances performance when integrated with current methods, and reduces unlearning time.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15861v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì‹ ê²½ë§ì€ ì˜ë„ì¹˜ ì•Šê²Œ í›ˆë ¨ ë°ì´í„°ë¥¼ ì•”ê¸°í•˜ì—¬, ë¯¼ê°í•œ ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡  ë° ì¬êµ¬ì„± ê³µê²©ê³¼ ê°™ì€ ì—°í•© í•™ìŠµ(FL) ì‹œìŠ¤í…œì—ì„œ í”„ë¼ì´ë²„ì‹œ ìœ„í—˜ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ìœ„í—˜ì„ ì™„í™”í•˜ê³  í”„ë¼ì´ë²„ì‹œ ê·œì •ì„ ì¤€ìˆ˜í•˜ê¸° ìœ„í•´, ì—°í•© í•™ìŠµ ì‹œìŠ¤í…œì˜ ì°¸ê°€ìë“¤ì´ ê¸€ë¡œë²Œ ëª¨ë¸ì—ì„œ ìì‹ ì˜ ë°ì´í„° ì˜í–¥ì„ ì œê±°í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì—°í•© ìŠê¸°(FU)ê°€ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì¬ì˜ FU ë°©ë²•ì€ ì£¼ë¡œ ì‚¬í›„ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ì‹ ê²½ë§ì— ê¹Šì´ ì•”ê¸°ëœ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ìš°ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” íš¨ê³¼ì ì¸ ìŠê¸°ê°€ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ í•„ìš”ë¡œ í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤: ìŠê¸°ì— ë³¸ì§ˆì ìœ¼ë¡œ ì í•©í•œ FL ì‹œìŠ¤í…œ ì„¤ê³„. ì´ë¥¼ ìœ„í•´, íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ì˜ ì•”ê¸°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•™ìŠµ ê³¼ì •ì—ì„œ ë³€í™˜ì„ í†µí•©í•˜ëŠ” í•™ìŠµ-ìŠê¸° ë³€í™˜ ì•ˆë‚´ ì—°í•© ìŠê¸°(ToFU) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì´ë¡ ì  ë¶„ì„ì€ ë³€í™˜ êµ¬ì„±ì´ ì¸ìŠ¤í„´ìŠ¤ë³„ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì¦ëª… ê°€ëŠ¥í•˜ê²Œ ì œí•œí•˜ëŠ”ì§€ë¥¼ ë°í˜€ë‚´ë©°, ì´ëŠ” í›„ì† ìŠê¸°ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€, ToFUëŠ” ê¸°ì¡´ FU ë°©ë²•ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ í”„ë ˆì„ì›Œí¬ë¡œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CIFAR-10, CIFAR-100, MUFAC ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ì€ ToFUê°€ ê¸°ì¡´ FU ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ê³ , í˜„ì¬ ë°©ë²•ê³¼ í†µí•©ë  ë•Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ìŠê¸° ì‹œê°„ì„ ì¤„ì´ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì—°í•© í•™ìŠµ(FL) ì‹œìŠ¤í…œì—ì„œ ë°œìƒí•˜ëŠ” í”„ë¼ì´ë²„ì‹œ ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´ ì œì•ˆëœ ì—°í•© ìŠê¸°(FU) ë°©ë²•ë¡ ì„ ê°œì„ í•©ë‹ˆë‹¤. ê¸°ì¡´ FU ë°©ë²•ì€ ì‚¬í›„ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ì‹ ê²½ë§ì— ê¹Šì´ ê°ì¸ëœ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì • ë°ì´í„°ì˜ ì•”ê¸°ë¥¼ ì¤„ì´ëŠ” ë³€í™˜ì„ ë„ì…í•œ ToFU(Transformation-guided Federated Unlearning) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì´ë¡ ì ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ë³„ ì •ë³´ë¥¼ ì œí•œí•˜ì—¬ ìŠê¸° ê³¼ì •ì„ ë‹¨ìˆœí™”í•˜ë©°, ê¸°ì¡´ FU ë°©ë²•ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. CIFAR-10, CIFAR-100 ë° MUFAC ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ToFUëŠ” ê¸°ì¡´ FU ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ê³ , ìŠê¸° ì‹œê°„ì„ ë‹¨ì¶•ì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹ ê²½ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ ë¬´ì˜ì‹ì ìœ¼ë¡œ ê¸°ì–µí•˜ì—¬ ì—°í•© í•™ìŠµ ì‹œìŠ¤í…œì—ì„œ ê°œì¸ì •ë³´ ì¹¨í•´ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 2. ì—°í•© í•™ìŠµ ì‹œìŠ¤í…œì—ì„œ ì°¸ê°€ìê°€ ìì‹ ì˜ ë°ì´í„° ì˜í–¥ì„ ì œê±°í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Federated Unlearning(FU)ì´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.

- 3. ê¸°ì¡´ FU ë°©ë²•ì€ ì£¼ë¡œ ì‚¬í›„ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ì‹ ê²½ë§ì— ê¹Šì´ ê¸°ì–µëœ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ìš°ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

- 4. ToFU(Transformation-guided Federated Unlearning) í”„ë ˆì„ì›Œí¬ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ë³€í™˜ì„ í†µí•©í•˜ì—¬ íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ì˜ ê¸°ì–µì„ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 5. ToFUëŠ” ê¸°ì¡´ FU ë°©ë²•ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ì‹¤í—˜ ê²°ê³¼ CIFAR-10, CIFAR-100, MUFAC ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:26:49*