# Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization

**Korean Title:** ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì˜ ì˜ˆë¦¬í•œ ìˆ˜ë ´ ì†ë„ë¥¼ ê°–ì¶˜ í™•ë¥ ì  ê³„ì¸µ ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Minimax Optimization, Bilevel Optimization

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (86.4% similar)
- [[2025-09-18/Stochastic Adaptive Gradient Descent Without Descent_20250918|Stochastic Adaptive Gradient Descent Without Descent]] (84.5% similar)
- [[2025-09-18/Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (83.4% similar)
- [[2025-09-17/Decentralized Optimization with Topology-Independent Communication_20250917|Decentralized Optimization with Topology-Independent Communication]] (81.9% similar)
- [[2025-09-17/A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning_20250917|A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning]] (81.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15399v1 Announce Type: new 
Abstract: Hierarchical optimization refers to problems with interdependent decision variables and objectives, such as minimax and bilevel formulations. While various algorithms have been proposed, existing methods and analyses lack adaptivity in stochastic optimization settings: they cannot achieve optimal convergence rates across a wide spectrum of gradient noise levels without prior knowledge of the noise magnitude. In this paper, we propose novel adaptive algorithms for two important classes of stochastic hierarchical optimization problems: nonconvex-strongly-concave minimax optimization and nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound on the stochastic gradient noise. Notably, these rates are obtained without prior knowledge of the noise level, thereby enabling automatic adaptivity in both low and high-noise regimes. To our knowledge, this work provides the first adaptive and sharp convergence guarantees for stochastic hierarchical optimization. Our algorithm design combines the momentum normalization technique with novel adaptive parameter choices. Extensive experiments on synthetic and deep learning tasks demonstrate the effectiveness of our proposed algorithms.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15399v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ê³„ì¸µì  ìµœì í™”ëŠ” ìµœì†Œ-ìµœëŒ€ ë° ì´ì¤‘ ìˆ˜ì¤€ í˜•ì‹ê³¼ ê°™ì´ ìƒí˜¸ ì˜ì¡´ì ì¸ ê²°ì • ë³€ìˆ˜ì™€ ëª©í‘œë¥¼ ê°€ì§„ ë¬¸ì œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì œì•ˆë˜ì—ˆì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ê³¼ ë¶„ì„ì€ í™•ë¥ ì  ìµœì í™” í™˜ê²½ì—ì„œ ì ì‘ì„±ì„ ê²°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¦‰, ì¡ìŒì˜ í¬ê¸°ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ë„ ë‹¤ì–‘í•œ ë²”ìœ„ì˜ ê¸°ìš¸ê¸° ì¡ìŒ ìˆ˜ì¤€ì—ì„œ ìµœì ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ í´ë˜ìŠ¤ì˜ í™•ë¥ ì  ê³„ì¸µì  ìµœì í™” ë¬¸ì œì— ëŒ€í•œ ìƒˆë¡œìš´ ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤: ë¹„ë³¼ë¡-ê°•í•œ ì˜¤ëª© ìµœì†Œ-ìµœëŒ€ ìµœì í™”ì™€ ë¹„ë³¼ë¡-ê°•í•œ ë³¼ë¡ ì´ì¤‘ ìˆ˜ì¤€ ìµœì í™”. ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ê¸°ìš¸ê¸° ë…¸ë¦„ì— ëŒ€í•´ $T$ ë°˜ë³µì—ì„œ $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ì˜ ëª…í™•í•œ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ì—¬ê¸°ì„œ $\bar{\sigma}$ëŠ” í™•ë¥ ì  ê¸°ìš¸ê¸° ì¡ìŒì˜ ìƒí•œì…ë‹ˆë‹¤. íŠ¹íˆ, ì´ëŸ¬í•œ ì†ë„ëŠ” ì¡ìŒ ìˆ˜ì¤€ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ ì–»ì–´ì§€ë©°, ì €ì¡ìŒ ë° ê³ ì¡ìŒ í™˜ê²½ ëª¨ë‘ì—ì„œ ìë™ ì ì‘ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì§€ì‹ì— ë”°ë¥´ë©´, ì´ ì—°êµ¬ëŠ” í™•ë¥ ì  ê³„ì¸µì  ìµœì í™”ì— ëŒ€í•œ ìµœì´ˆì˜ ì ì‘í˜• ë° ëª…í™•í•œ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ëŠ” ëª¨ë©˜í…€ ì •ê·œí™” ê¸°ë²•ê³¼ ìƒˆë¡œìš´ ì ì‘í˜• ë§¤ê°œë³€ìˆ˜ ì„ íƒì„ ê²°í•©í•©ë‹ˆë‹¤. í•©ì„± ë° ì‹¬ì¸µ í•™ìŠµ ê³¼ì œì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³„ì¸µì  ìµœì í™” ë¬¸ì œì—ì„œì˜ ìƒˆë¡œìš´ ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì¡ìŒ ìˆ˜ì¤€ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ ìµœì ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¹„ë³¼ë¡-ê°•í•œ ì˜¤ëª© ìµœì†ŒìµœëŒ€ ìµœì í™”ì™€ ë¹„ë³¼ë¡-ê°•í•œ ë³¼ë¡ ì´ìˆ˜ì¤€ ìµœì í™” ë¬¸ì œì— ëŒ€í•´ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬, ì¡ìŒ ìˆ˜ì¤€ì— ìƒê´€ì—†ì´ ìë™ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆëŠ” ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë©˜í…€ ì •ê·œí™” ê¸°ë²•ê³¼ ìƒˆë¡œìš´ ì ì‘í˜• ë§¤ê°œë³€ìˆ˜ ì„ íƒì„ ê²°í•©í•˜ì—¬, $T$ ë°˜ë³µì—ì„œ ê¸°ìš¸ê¸° ë…¸ë¦„ì— ëŒ€í•´ $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. ì´ëŠ” ê³„ì¸µì  ìµœì í™” ë¬¸ì œì—ì„œ ìµœì´ˆë¡œ ì ì‘í˜• ë° ë‚ ì¹´ë¡œìš´ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•˜ëŠ” ì—°êµ¬ë¡œ, ì‹¤í—˜ì„ í†µí•´ ê·¸ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê³„ì¸µì  ìµœì í™” ë¬¸ì œì—ì„œ ê¸°ì¡´ ë°©ë²•ë“¤ì€ í™•ë¥ ì  ìµœì í™” í™˜ê²½ì—ì„œ ì ì‘ì„±ì„ ê²°ì—¬í•˜ê³  ìˆë‹¤.

- 2. ë³¸ ë…¼ë¬¸ì€ ë¹„ë³¼ë¡-ê°•í•œ ì˜¤ëª© ìµœì†Œê·¹ëŒ€í™”ì™€ ë¹„ë³¼ë¡-ê°•í•œ ë³¼ë¡ ì´ìˆ˜ì¤€ ìµœì í™” ë¬¸ì œë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•œë‹¤.

- 3. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ ì‚¬ì „ ì†ŒìŒ ìˆ˜ì¤€ì— ëŒ€í•œ ì •ë³´ ì—†ì´ë„ ìë™ ì ì‘ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ë‚ ì¹´ë¡œìš´ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•œë‹¤.

- 4. ëª¨ë©˜í…€ ì •ê·œí™” ê¸°ë²•ê³¼ ìƒˆë¡œìš´ ì ì‘í˜• ë§¤ê°œë³€ìˆ˜ ì„ íƒì„ ê²°í•©í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ê³„í•˜ì˜€ë‹¤.

- 5. í•©ì„± ë° ë”¥ëŸ¬ë‹ ê³¼ì œì—ì„œì˜ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆë‹¤.

---

*Generated on 2025-09-22 15:13:09*