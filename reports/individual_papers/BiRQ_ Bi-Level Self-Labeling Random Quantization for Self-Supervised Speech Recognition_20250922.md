# BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition

**Korean Title:** BiRQ: ìê°€ ì§€ë„ ìŒì„± ì¸ì‹ì„ ìœ„í•œ ì´ì¤‘ ìˆ˜ì¤€ ìê¸° ë ˆì´ë¸”ë§ ëœë¤ ì–‘ìí™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Random Quantization|Random Quantization]] [[keywords/specific/Bilevel Optimization|Bilevel Optimization]] [[keywords/broad/Self-supervised Learning|Self-supervised Learning]] [[keywords/unique/BiRQ|BiRQ]] [[categories/cs.CL|cs.CL]] [[2025-09-17/Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures_20250917|Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures]] (80.6% similar) [[2025-09-18/Evolving Language Models without Labels_ Majority Drives Selection, Novelty Promotes Variation_20250918|Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation]] (80.4% similar) [[2025-09-18/BabyHuBERT_ Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings_20250918|BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings]] (80.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Self-labeling Random Quantization
**ğŸ”— Specific Connectable**: First-order Bilevel Optimization
**ğŸ”¬ Broad Technical**: Self-supervised Learning
**â­ Unique Technical**: BiRQ
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures_20250917|Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures]] (80.6% similar)
- [[2025-09-18/Evolving Language Models without Labels_ Majority Drives Selection, Novelty Promotes Variation_20250918|Evolving Language Models without Labels Majority Drives Selection, Novelty Promotes Variation]] (80.4% similar)
- [[2025-09-18/BabyHuBERT_ Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings_20250918|BabyHuBERT Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings]] (80.2% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (80.1% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility Process-Supervised Rewrite for RAG]] (79.6% similar)


**ArXiv ID**: [2509.15430](https://arxiv.org/abs/2509.15430)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15430.pdf)


**ArXiv ID**: [2509.15430](https://arxiv.org/abs/2509.15430)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15430.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Self-labeling Random Quantization
**ğŸ”— Specific Connectable**: Bilevel Optimization
**â­ Unique Technical**: BiRQ
**ğŸ”¬ Broad Technical**: Self-supervised Learning

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Self-supervised Learning` â€¢ 

`Bilevel Optimization` â€¢ 

`BiRQ` â€¢ 

`Random Quantization`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15430v1 Announce Type: new 
Abstract: Speech is a rich signal, and labeled audio-text pairs are costly, making self-supervised learning essential for scalable representation learning. A core challenge in speech SSL is generating pseudo-labels that are both informative and efficient: strong labels, such as those used in HuBERT, improve downstream performance but rely on external encoders and multi-stage pipelines, while efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels. We propose BiRQ, a bilevel SSL framework that combines the efficiency of BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key idea is to reuse part of the model itself as a pseudo-label generator: intermediate representations are discretized by a random-projection quantizer to produce enhanced labels, while anchoring labels derived directly from the raw input stabilize training and prevent collapse. Training is formulated as an efficient first-order bilevel optimization problem, solved end-to-end with differentiable Gumbel-softmax selection. This design eliminates the need for external label encoders, reduces memory cost, and enables iterative label refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ while maintaining low complexity and computational efficiency. We validate our method on various datasets, including 960-hour LibriSpeech, 150-hour AMI meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15430v1 ë°œí‘œ ìœ í˜•: ìƒˆë¡œìš´ ê²ƒ  
ì´ˆë¡: ìŒì„±ì€ í’ë¶€í•œ ì‹ í˜¸ì´ë©°, ë¼ë²¨ì´ ìˆëŠ” ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ìŒì€ ë¹„ìš©ì´ ë§ì´ ë“¤ê¸° ë•Œë¬¸ì— í™•ì¥ ê°€ëŠ¥í•œ í‘œí˜„ í•™ìŠµì„ ìœ„í•´ ìê°€ ì§€ë„ í•™ìŠµ(self-supervised learning)ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ìŒì„± SSLì˜ í•µì‹¬ ê³¼ì œëŠ” ì •ë³´ê°€ í’ë¶€í•˜ê³  íš¨ìœ¨ì ì¸ ì˜ì‚¬ ë¼ë²¨(pseudo-label)ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. HuBERTì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°•ë ¥í•œ ë¼ë²¨ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ ì™¸ë¶€ ì¸ì½”ë”ì™€ ë‹¤ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì— ì˜ì¡´í•˜ëŠ” ë°˜ë©´, BEST-RQì™€ ê°™ì€ íš¨ìœ¨ì ì¸ ë°©ë²•ì€ ë‹¨ìˆœì„±ì„ ì–»ëŠ” ëŒ€ì‹  ì•½í•œ ë¼ë²¨ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” BiRQë¼ëŠ” ì´ì¤‘ ìˆ˜ì¤€ SSL í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ BEST-RQì˜ íš¨ìœ¨ì„±ê³¼ HuBERT ìŠ¤íƒ€ì¼ ë¼ë²¨ ê°œì„ ì˜ ì •ì œ ì´ì ì„ ê²°í•©í•©ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ì˜ì‚¬ ë¼ë²¨ ìƒì„±ê¸°ë¡œ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤: ì¤‘ê°„ í‘œí˜„ì€ ëœë¤ í”„ë¡œì ì…˜ ì–‘ìí™”ê¸°ë¥¼ í†µí•´ ì´ì‚°í™”ë˜ì–´ ê°œì„ ëœ ë¼ë²¨ì„ ìƒì„±í•˜ë©°, ì›ì‹œ ì…ë ¥ì—ì„œ ì§ì ‘ íŒŒìƒëœ ì•µì»¤ ë¼ë²¨ì€ í›ˆë ¨ì„ ì•ˆì •í™”í•˜ê³  ë¶•ê´´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤. í›ˆë ¨ì€ íš¨ìœ¨ì ì¸ 1ì°¨ ì´ì¤‘ ìˆ˜ì¤€ ìµœì í™” ë¬¸ì œë¡œ ê³µì‹í™”ë˜ë©°, ì°¨ë³„ ê°€ëŠ¥í•œ Gumbel-softmax ì„ íƒì„ í†µí•´ ì¢…ë‹¨ ê°„ìœ¼ë¡œ í•´ê²°ë©ë‹ˆë‹¤. ì´ ì„¤ê³„ëŠ” ì™¸ë¶€ ë¼ë²¨ ì¸ì½”ë”ì˜ í•„ìš”ì„±ì„ ì œê±°í•˜ê³  ë©”ëª¨ë¦¬ ë¹„ìš©ì„ ì¤„ì´ë©°, ì¢…ë‹¨ ê°„ ë°©ì‹ìœ¼ë¡œ ë°˜ë³µì ì¸ ë¼ë²¨ ê°œì„ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. BiRQëŠ” BEST-RQì— ë¹„í•´ ë‚®ì€ ë³µì¡ì„±ê³¼ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” 960ì‹œê°„ì˜ LibriSpeech, 150ì‹œê°„ì˜ AMI íšŒì˜, 5,000ì‹œê°„ì˜ YODASë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ìš°ë¦¬ì˜ ë°©ë²•ì„ ê²€ì¦í•˜ì—¬ BEST-RQì— ë¹„í•´ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

BiRQëŠ” ìŒì„± ìê¸°ì§€ë„ í•™ìŠµ(SSL)ì—ì„œ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— ê°œì„ í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ HuBERTëŠ” ê°•ë ¥í•œ ë ˆì´ë¸”ì„ ì‚¬ìš©í•´ ì„±ëŠ¥ì„ ë†’ì´ì§€ë§Œ ë³µì¡í•œ ì™¸ë¶€ ì¸ì½”ë”ì™€ ë‹¤ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë°˜ë©´ BEST-RQëŠ” ë‹¨ìˆœí•˜ì§€ë§Œ ì•½í•œ ë ˆì´ë¸”ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. BiRQëŠ” ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ì˜ì‚¬ ë ˆì´ë¸” ìƒì„±ê¸°ë¡œ í™œìš©í•˜ì—¬ ì¤‘ê°„ í‘œí˜„ì„ ë¬´ì‘ìœ„ íˆ¬ì˜ ì–‘ìí™”ê¸°ë¡œ ë³€í™˜í•˜ê³ , ì›ì‹œ ì…ë ¥ì—ì„œ ì§ì ‘ íŒŒìƒëœ ì•µì»¤ ë ˆì´ë¸”ë¡œ í›ˆë ¨ì„ ì•ˆì •í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì™¸ë¶€ ë ˆì´ë¸” ì¸ì½”ë” ì—†ì´ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ ì¤„ì´ê³ , ì°¨ë³„ ê°€ëŠ¥í•œ Gumbel-softmax ì„ íƒì„ í†µí•´ íš¨ìœ¨ì ì¸ ì¼ì°¨ ìµœì í™” ë¬¸ì œë¡œ í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ BiRQëŠ” BEST-RQë³´ë‹¤ ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ë‚®ì€ ë³µì¡ì„±ê³¼ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. BiRQëŠ” BEST-RQì˜ íš¨ìœ¨ì„±ê³¼ HuBERT ìŠ¤íƒ€ì¼ì˜ ë¼ë²¨ í–¥ìƒ ì´ì ì„ ê²°í•©í•œ ì´ì¤‘ ë ˆë²¨ ìê°€ ì§€ë„ í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 2. ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ì˜ì‚¬ ë¼ë²¨ ìƒì„±ê¸°ë¡œ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ í‘œí˜„ì„ ë¬´ì‘ìœ„ íˆ¬ì˜ ì–‘ìí™”ê¸°ë¡œ ì´ì‚°í™”í•˜ì—¬ í–¥ìƒëœ ë¼ë²¨ì„ ìƒì„±í•©ë‹ˆë‹¤.

- 3. Gumbel-softmax ì„ íƒì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ 1ì°¨ ì´ì¤‘ ë ˆë²¨ ìµœì í™” ë¬¸ì œë¡œ í›ˆë ¨ì´ ê³µì‹í™”ë˜ì–´ ì™¸ë¶€ ë¼ë²¨ ì¸ì½”ë”ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

- 4. BiRQëŠ” BEST-RQì— ë¹„í•´ ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ë‚®ì€ ë³µì¡ì„±ê³¼ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

- 5. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ BiRQì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ì—¬ BEST-RQ ëŒ€ë¹„ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:21:13*