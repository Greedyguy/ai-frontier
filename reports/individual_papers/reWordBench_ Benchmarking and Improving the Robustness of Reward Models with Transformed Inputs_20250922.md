# reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs

**Korean Title:** reWordBench: ë³€í™˜ëœ ì…ë ¥ì„ í†µí•´ ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„±ì„ ë²¤ì¹˜ë§ˆí‚¹í•˜ê³  ê°œì„ í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Robust Reward Models|Robust Reward Models]] [[keywords/specific/Input Transformation|Input Transformation]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/unique/reWordBench|reWordBench]] [[categories/cs.CL|cs.CL]] [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (85.7% similar) [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (84.0% similar) [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Robust Reward Models
**ğŸ”— Specific Connectable**: Input Transformation
**ğŸ”¬ Broad Technical**: Natural Language Processing
**â­ Unique Technical**: reWordBench
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (85.7% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (84.0% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (83.9% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (83.2% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (82.5% similar)


**ArXiv ID**: [2503.11751](https://arxiv.org/abs/2503.11751)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2503.11751.pdf)


**ArXiv ID**: [2503.11751](https://arxiv.org/abs/2503.11751)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2503.11751.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Robust Reward Models
**ğŸ”— Specific Connectable**: Input Transformation
**â­ Unique Technical**: reWordBench
**ğŸ”¬ Broad Technical**: Natural Language Processing

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Natural Language Processing` â€¢ 

`Input Transformation` â€¢ 

`reWordBench` â€¢ 

`Robust Reward Models`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.11751v2 Announce Type: replace 
Abstract: Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2503.11751v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë³´ìƒ ëª¨ë¸ì€ í˜„ëŒ€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ìë¦¬ ì¡ì•˜ìœ¼ë©°, í™•ì¥ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ í‰ê°€ ë„êµ¬ì¼ ë¿ë§Œ ì•„ë‹ˆë¼ ë§ì€ ì •ë ¬ ê¸°ë²• ë° ì¶”ë¡  ì•Œê³ ë¦¬ì¦˜ì˜ í•„ìˆ˜ êµ¬ì„± ìš”ì†Œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìµœê·¼ì˜ ë³´ìƒ ëª¨ë¸ì´ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì€ ë¶€ë¶„ì ìœ¼ë¡œ ê³¼ì í•© íš¨ê³¼ ë•Œë¬¸ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì´ ëª¨ë¸ì˜ ì§„ì •í•œ ëŠ¥ë ¥ì„ ì´í•´í•˜ëŠ” ë° í˜¼ë€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ ì´ëŸ¬í•œ ê³¼ì í•©ì˜ ì •ë„ë¥¼ ë©´ë°€íˆ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë³´ìƒ ëª¨ë¸ ì…ë ¥ì„ ì˜ë¯¸ ë˜ëŠ” ìˆœìœ„ë¥¼ ë³´ì¡´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” **reWordBench**ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ìµœì²¨ë‹¨ ë³´ìƒ ëª¨ë¸ì€ ì‚¬ì†Œí•œ ì…ë ¥ ë³€í™˜ì—ë„ ìƒë‹¹í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìœ¼ë©°, ë•Œë¡œëŠ” ë¬´ì‘ìœ„ ì •í™•ë„ë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ìˆ˜ì¤€ìœ¼ë¡œ ë–¨ì–´ì§€ê¸°ë„ í•˜ì—¬ ì·¨ì•½ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆì— ìœ ì‚¬í•œ ì ìˆ˜ë¥¼ í• ë‹¹í•˜ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì´ ì ‘ê·¼ë²•ì´ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ë³€í™˜ì— ëŒ€í•œ ê²¬ê³ ì„±ë„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ì˜ ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ì€ RewardBenchì˜ Chat Hard í•˜ìœ„ ì§‘í•©ì—ì„œ ì´ëŸ¬í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì•½ ì ˆë°˜ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë”ìš±ì´, ì •ë ¬ì— ì‚¬ìš©ë  ë•Œ, ìš°ë¦¬ì˜ ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ì€ ë” ë‚˜ì€ ìœ ìš©ì„±ì„ ë³´ì—¬ì£¼ë©°, í‘œì¤€ì ìœ¼ë¡œ í›ˆë ¨ëœ ë³´ìƒ ëª¨ë¸ì— ë¹„í•´ ìµœëŒ€ 59%ì˜ ì‚¬ë¡€ì—ì„œ ë” ë†’ì€ í’ˆì§ˆì˜ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë³´ìƒ ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ ê³¼ì í•© ë¬¸ì œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì…ë ¥ì„ ì˜ë¯¸ë‚˜ ìˆœìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë³€í˜•í•˜ëŠ” **reWordBench**ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ìµœì‹  ë³´ìƒ ëª¨ë¸ì€ ì…ë ¥ ë³€í˜•ì— ì·¨ì•½í•˜ì—¬ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆì— ìœ ì‚¬í•œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë„ë¡ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼°ìœ¼ë©°, ì´ ë°©ë²•ì´ ë‹¤ì–‘í•œ ë³€í˜•ì— ëŒ€í•œ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ, Chat Hard ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì˜€ê³ , ì •ë ¬ ì‘ì—…ì—ì„œë„ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ìµœì‹  ë³´ìƒ ëª¨ë¸ì€ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ì´ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ê³¼ì í•© íš¨ê³¼ ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 2. **reWordBench**ë¥¼ êµ¬ì¶•í•˜ì—¬ ë³´ìƒ ëª¨ë¸ ì…ë ¥ì„ ì˜ë¯¸ë‚˜ ìˆœìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì²´ê³„ì ìœ¼ë¡œ ë³€í˜•í•˜ê³ , ë³´ìƒ ëª¨ë¸ì˜ ì·¨ì•½ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

- 3. ìµœì‹  ë³´ìƒ ëª¨ë¸ì€ ì…ë ¥ì˜ ì‚¬ì†Œí•œ ë³€í˜•ì—ë„ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìœ¼ë©°, ë•Œë¡œëŠ” ë¬´ì‘ìœ„ ì •í™•ë„ë³´ë‹¤ ë‚®ì€ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.

- 4. ë³´ìƒ ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ìœ ì‚¬í•œ ë¬¸ì¥ì— ìœ ì‚¬í•œ ì ìˆ˜ë¥¼ í• ë‹¹í•˜ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 5. ê°•ê±´í•œ ë³´ìƒ ëª¨ë¸ì€ ì •ë ¬ ì‹œ ë” ë‚˜ì€ íš¨ìš©ì„±ì„ ë³´ì´ë©°, ìµœëŒ€ 59%ì˜ ê²½ìš°ì—ì„œ í‘œì¤€ í›ˆë ¨ëœ ëª¨ë¸ë³´ë‹¤ ë†’ì€ í’ˆì§ˆì˜ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:36:05*