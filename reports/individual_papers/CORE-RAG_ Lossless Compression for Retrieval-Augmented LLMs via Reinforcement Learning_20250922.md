# CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning

**Korean Title:** CORE-RAG: ê°•í™” í•™ìŠµì„ í†µí•œ ê²€ìƒ‰ ì¦ê°• ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë¬´ì†ì‹¤ ì••ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Lossless Context Compression

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Enhancing Retrieval Augmentation via Adversarial Collaboration_20250919|Enhancing Retrieval Augmentation via Adversarial Collaboration]] (86.5% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility Process-Supervised Rewrite for RAG]] (85.8% similar)
- [[2025-09-19/GRADA_ Graph-based Reranking against Adversarial Documents Attack_20250919|GRADA Graph-based Reranking against Adversarial Documents Attack]] (85.7% similar)
- [[2025-09-19/Causal-Counterfactual RAG_ The Integration of Causal-Counterfactual Reasoning into RAG_20250919|Causal-Counterfactual RAG The Integration of Causal-Counterfactual Reasoning into RAG]] (85.5% similar)
- [[2025-09-19/Engineering RAG Systems for Real-World Applications_ Design, Development, and Evaluation_20250919|Engineering RAG Systems for Real-World Applications Design, Development, and Evaluation]] (85.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.19282v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to enhance the timeliness of knowledge and the factual accuracy of responses in Large Language Models (LLMs). However, the inclusion of excessive retrieved documents substantially increases the input length, leading to higher computational costs. Previous studies have attempted to compress retrieved documents into shorter texts before in-context integration, but such methods often compromise end-task performance. The lack of well-defined compression targets forces many approaches to rely on fixed heuristics, which cannot guarantee that the compressed content will effectively support the end task. To address these limitations, we propose CORE, a novel method designed to achieve lossless context compression for RAG. CORE employs reinforcement learning to optimize the compression process without relying on predefined compression labels, which enables the compressor to generate summaries that maximize the accuracy of answers generated by the LLM. Extensive experiments on four datasets demonstrate the superiority of our approach. With a high compression ratio of 3\%, our method not only avoids performance degradation compared to prepending full documents across all datasets but also improves the average Exact Match (EM) score by 3.3 points. The code will be released soon.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.19282v2 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval-Augmented Generation, RAG)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì§€ì‹ì˜ ì ì‹œì„±ê³¼ ì‘ë‹µì˜ ì‚¬ì‹¤ì  ì •í™•ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìœ ë§í•œ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³¼ë„í•œ ê²€ìƒ‰ ë¬¸ì„œì˜ í¬í•¨ì€ ì…ë ¥ ê¸¸ì´ë¥¼ ìƒë‹¹íˆ ì¦ê°€ì‹œì¼œ ê³„ì‚° ë¹„ìš©ì„ ë†’ì…ë‹ˆë‹¤. ì´ì „ ì—°êµ¬ì—ì„œëŠ” ë§¥ë½ ë‚´ í†µí•© ì „ì— ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë” ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ì••ì¶•í•˜ë ¤ê³  ì‹œë„í–ˆì§€ë§Œ, ì´ëŸ¬í•œ ë°©ë²•ì€ ì¢…ì¢… ìµœì¢… ì‘ì—… ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚µë‹ˆë‹¤. ëª…í™•í•˜ê²Œ ì •ì˜ëœ ì••ì¶• ëª©í‘œì˜ ë¶€ì¬ë¡œ ì¸í•´ ë§ì€ ì ‘ê·¼ ë°©ì‹ì´ ê³ ì •ëœ íœ´ë¦¬ìŠ¤í‹±ì— ì˜ì¡´í•˜ê²Œ ë˜ë©°, ì´ëŠ” ì••ì¶•ëœ ì½˜í…ì¸ ê°€ ìµœì¢… ì‘ì—…ì„ íš¨ê³¼ì ìœ¼ë¡œ ì§€ì›í•  ê²ƒì´ë¼ëŠ” ë³´ì¥ì„ ì œê³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” RAGë¥¼ ìœ„í•œ ë¬´ì†ì‹¤ ë§¥ë½ ì••ì¶•ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ìƒˆë¡œìš´ ë°©ë²•ì¸ COREë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. COREëŠ” ì‚¬ì „ ì •ì˜ëœ ì••ì¶• ë ˆì´ë¸”ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ê°•í™” í•™ìŠµì„ í†µí•´ ì••ì¶• ê³¼ì •ì„ ìµœì í™”í•˜ì—¬, LLMì´ ìƒì„±í•œ ë‹µë³€ì˜ ì •í™•ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë„¤ ê°œì˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì˜ ìš°ìˆ˜ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. 3%ì˜ ë†’ì€ ì••ì¶• ë¹„ìœ¨ì„ ê°€ì§„ ìš°ë¦¬ì˜ ë°©ë²•ì€ ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì „ì²´ ë¬¸ì„œë¥¼ ë¯¸ë¦¬ ì²¨ë¶€í•˜ëŠ” ê²ƒê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ í”¼í•  ë¿ë§Œ ì•„ë‹ˆë¼ í‰ê·  ì •í™•í•œ ì¼ì¹˜(Exact Match, EM) ì ìˆ˜ë¥¼ 3.3ì  í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì½”ë“œëŠ” ê³§ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‘ë‹µ ì •í™•ì„±ê³¼ ì§€ì‹ì˜ ìµœì‹ ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¬¸ì„œ ì••ì¶• ë°©ë²•ì€ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” ë°˜ë©´, ì €ìë“¤ì€ ìƒˆë¡œìš´ ë°©ë²•ì¸ COREë¥¼ ì œì•ˆí•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. COREëŠ” ê°•í™” í•™ìŠµì„ í†µí•´ ì‚¬ì „ ì •ì˜ëœ ì••ì¶• ë ˆì´ë¸” ì—†ì´ë„ ë¬¸ì„œ ì••ì¶•ì„ ìµœì í™”í•˜ì—¬ LLMì˜ ì‘ë‹µ ì •í™•ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. ë„¤ ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, COREëŠ” ì„±ëŠ¥ ì €í•˜ ì—†ì´ í‰ê·  ì •í™•ë„ ì ìˆ˜ë¥¼ 3.3ì  í–¥ìƒì‹œí‚¤ë©°, ë†’ì€ ì••ì¶•ë¥ (3%)ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì˜ ì½”ë“œë„ ê³§ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Retrieval-Augmented Generation (RAG)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì§€ì‹ ìµœì‹ ì„±ê³¼ ì‚¬ì‹¤ì  ì •í™•ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ìœ ë§í•œ ì ‘ê·¼ë²•ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.

- 2. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ë¬¸ì„œ ì••ì¶•ì„ ì‹œë„í–ˆìœ¼ë‚˜, ê³ ì •ëœ íœ´ë¦¬ìŠ¤í‹±ì— ì˜ì¡´í•˜ì—¬ ìµœì¢… ì‘ì—… ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìœ„í—˜ì´ ìˆì—ˆë‹¤.

- 3. COREëŠ” ê°•í™” í•™ìŠµì„ í™œìš©í•˜ì—¬ ì‚¬ì „ ì •ì˜ëœ ì••ì¶• ë ˆì´ë¸” ì—†ì´ ì••ì¶• ê³¼ì •ì„ ìµœì í™”í•˜ì—¬ LLMì˜ ë‹µë³€ ì •í™•ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ìš”ì•½ì„ ìƒì„±í•œë‹¤.

- 4. COREëŠ” 3%ì˜ ë†’ì€ ì••ì¶• ë¹„ìœ¨ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ ì—†ì´ í‰ê·  ì •í™• ì¼ì¹˜(EM) ì ìˆ˜ë¥¼ 3.3ì  í–¥ìƒì‹œì¼°ë‹¤.

- 5. COREì˜ ìš°ìˆ˜ì„±ì€ ë„¤ ê°€ì§€ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ì…ì¦ë˜ì—ˆìœ¼ë©°, ì½”ë“œê°€ ê³§ ê³µê°œë  ì˜ˆì •ì´ë‹¤.

---

*Generated on 2025-09-22 14:59:17*