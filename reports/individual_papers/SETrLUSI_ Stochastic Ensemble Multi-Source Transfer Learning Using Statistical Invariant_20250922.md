# SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant

**Korean Title:** SETrLUSI: 통계적 불변량을 사용하는 확률적 앙상블 다중 소스 전이 학습

## 📋 메타데이터

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.LG|cs.LG]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Multi-source Transfer Learning

## 🔗 유사한 논문
- [[2025-09-19/Select to Know_ An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering_20250919|Select to Know An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (80.6% similar)
- [[2025-09-22/TISDiSS_ A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation_20250922|TISDiSS A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation]] (80.5% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (79.7% similar)
- [[2025-09-19/CodeLSI_ Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning_20250919|CodeLSI Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning]] (79.6% similar)
- [[2025-09-19/Superpose Task-specific Features for Model Merging_20250919|Superpose Task-specific Features for Model Merging]] (79.0% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15593v1 Announce Type: cross 
Abstract: In transfer learning, a source domain often carries diverse knowledge, and different domains usually emphasize different types of knowledge. Different from handling only a single type of knowledge from all domains in traditional transfer learning methods, we introduce an ensemble learning framework with a weak mode of convergence in the form of Statistical Invariant (SI) for multi-source transfer learning, formulated as Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI extracts and integrates various types of knowledge from both source and target domains, which not only effectively utilizes diverse knowledge but also accelerates the convergence process. Further, SETrLUSI incorporates stochastic SI selection, proportional source domain sampling, and target domain bootstrapping, which improves training efficiency while enhancing model stability. Experiments show that SETrLUSI has good convergence and outperforms related methods with a lower time cost.

## 🔍 Abstract (한글 번역)

arXiv:2509.15593v1 발표 유형: 교차  
초록: 전이 학습에서 소스 도메인은 종종 다양한 지식을 포함하고 있으며, 서로 다른 도메인은 보통 서로 다른 유형의 지식을 강조합니다. 전통적인 전이 학습 방법에서 모든 도메인으로부터 단일 유형의 지식만을 다루는 것과 달리, 우리는 다중 소스 전이 학습을 위한 통계적 불변성(SI)의 형태로 약한 수렴 모드를 가진 앙상블 학습 프레임워크를 도입합니다. 이는 통계적 불변성을 사용하는 확률적 앙상블 다중 소스 전이 학습(SETrLUSI)으로 공식화됩니다. 제안된 SI는 소스 및 타겟 도메인 모두에서 다양한 유형의 지식을 추출하고 통합하여, 다양한 지식을 효과적으로 활용할 뿐만 아니라 수렴 과정을 가속화합니다. 또한, SETrLUSI는 확률적 SI 선택, 비례 소스 도메인 샘플링, 타겟 도메인 부트스트래핑을 포함하여, 모델의 안정성을 향상시키면서 훈련 효율성을 개선합니다. 실험 결과, SETrLUSI는 좋은 수렴성을 보이며, 관련 방법들에 비해 낮은 시간 비용으로 우수한 성능을 발휘합니다.

## 📝 요약

이 논문은 전이 학습에서 다양한 지식을 효과적으로 활용하기 위해 다중 소스 전이 학습을 위한 통계적 불변(SI) 기반의 앙상블 학습 프레임워크(SETrLUSI)를 제안합니다. 제안된 방법은 소스와 타겟 도메인에서 다양한 지식을 추출하고 통합하여 수렴 과정을 가속화합니다. 또한, 확률적 SI 선택, 비례적 소스 도메인 샘플링, 타겟 도메인 부트스트래핑을 통해 모델의 안정성을 높이고 훈련 효율성을 향상시킵니다. 실험 결과, SETrLUSI는 우수한 수렴성을 보이며 관련 방법들보다 낮은 시간 비용으로 뛰어난 성능을 보였습니다.

## 🎯 주요 포인트

- 1. SETrLUSI는 통계적 불변성(SI)을 활용하여 다양한 출처와 목표 도메인에서 지식을 추출하고 통합합니다.

- 2. 제안된 프레임워크는 다양한 지식을 효과적으로 활용하면서 수렴 과정을 가속화합니다.

- 3. SETrLUSI는 확률적 SI 선택, 비례적 출처 도메인 샘플링, 목표 도메인 부트스트래핑을 포함하여 훈련 효율성을 개선하고 모델 안정성을 강화합니다.

- 4. 실험 결과, SETrLUSI는 좋은 수렴성을 보이며, 관련 방법들보다 낮은 시간 비용으로 우수한 성능을 발휘합니다.

---

*Generated on 2025-09-22 15:39:58*