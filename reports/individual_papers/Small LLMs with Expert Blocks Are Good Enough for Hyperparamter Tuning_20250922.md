# Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning

**Korean Title:** ì „ë¬¸ ë¸”ë¡ì„ í¬í•¨í•œ ì‘ì€ LLMì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì— ì¶©ë¶„í•˜ë‹¤

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Trajectory Context Summarizer

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM Language of Browsing]] (85.4% similar)
- [[2025-09-18/CARGO_ A Framework for Confidence-Aware Routing of Large Language Models_20250918|CARGO A Framework for Confidence-Aware Routing of Large Language Models]] (83.5% similar)
- [[2025-09-22/Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges_20250922|Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges]] (83.2% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (83.1% similar)
- [[2025-09-22/Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs_20250922|Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs]] (83.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15561v1 Announce Type: new 
Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML) pipelines but becomes computationally expensive and opaque with larger models. Recently, Large Language Models (LLMs) have been explored for HPT, yet most rely on models exceeding 100 billion parameters. We propose an Expert Block Framework for HPT using Small LLMs. At its core is the Trajectory Context Summarizer (TCS), a deterministic block that transforms raw training trajectories into structured context, enabling small LLMs to analyze optimization progress with reliability comparable to larger models. Using two locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9 percentage points of GPT-4 across six diverse tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15561v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(HPT)ì€ ë¨¸ì‹ ëŸ¬ë‹(ML) íŒŒì´í”„ë¼ì¸ì—ì„œ í•„ìˆ˜ì ì¸ ë‹¨ê³„ì´ì§€ë§Œ, ëª¨ë¸ì´ ì»¤ì§ˆìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ë¶ˆíˆ¬ëª…í•´ì§‘ë‹ˆë‹¤. ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ HPTì— íƒêµ¬ë˜ê³  ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ 1,000ì–µ ê°œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ëª¨ë¸ì— ì˜ì¡´í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì†Œí˜• LLMì„ ì‚¬ìš©í•œ HPTë¥¼ ìœ„í•œ ì „ë¬¸ê°€ ë¸”ë¡ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê·¸ í•µì‹¬ì€ Trajectory Context Summarizer(TCS)ë¡œ, ì›ì‹œ í›ˆë ¨ ê²½ë¡œë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê²°ì •ë¡ ì  ë¸”ë¡ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì†Œí˜• LLMì´ ë” í° ëª¨ë¸ê³¼ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì‹ ë¢°ì„±ìœ¼ë¡œ ìµœì í™” ì§„í–‰ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ê°œì˜ ë¡œì»¬ ì‹¤í–‰ LLM(phi4:reasoning14B ë° qwen2.5-coder:32B)ê³¼ 10íšŒ ì‹¤í—˜ ì˜ˆì‚°ì„ ì‚¬ìš©í•˜ì—¬, TCSë¥¼ í™œìš©í•œ HPT íŒŒì´í”„ë¼ì¸ì€ ì—¬ì„¯ ê°€ì§€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ GPT-4ì™€ í‰ê·  ì„±ëŠ¥ ì°¨ì´ê°€ ì•½ 0.9 í¼ì„¼íŠ¸ í¬ì¸íŠ¸ ì´ë‚´ë¡œ ë‹¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ íš¨ê³¼ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(HPT)ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ 'ì „ë¬¸ê°€ ë¸”ë¡ í”„ë ˆì„ì›Œí¬'ë¡œ, í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ 'Trajectory Context Summarizer(TCS)'ë¥¼ í†µí•´ ì‘ì€ LLMì´ ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì‹ ë¢°ë„ë¡œ ìµœì í™” ì§„í–‰ ìƒí™©ì„ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ë‘ ê°œì˜ ì†Œê·œëª¨ LLM(phi4:reasoning14B ë° qwen2.5-coder:32B)ì„ ì‚¬ìš©í•˜ì—¬ 10ë²ˆì˜ ì‹¤í—˜ ì˜ˆì‚° ë‚´ì—ì„œ GPT-4ì™€ ë¹„êµí•´ í‰ê·  ì„±ëŠ¥ ì°¨ì´ê°€ ì•½ 0.9% í¬ì¸íŠ¸ì— ë¶ˆê³¼í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(HPT)ì€ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì—ì„œ í•„ìˆ˜ì ì´ì§€ë§Œ, ëŒ€í˜• ëª¨ë¸ì—ì„œëŠ” ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ë¶ˆíˆ¬ëª…í•´ì§„ë‹¤.

- 2. ì†Œí˜• ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í™œìš©í•œ ì „ë¬¸ê°€ ë¸”ë¡ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ HPTë¥¼ ìˆ˜í–‰í•œë‹¤.

- 3. Trajectory Context Summarizer(TCS)ëŠ” í›ˆë ¨ ê²½ë¡œë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì†Œí˜• LLMì´ ìµœì í™” ì§„í–‰ì„ ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

- 4. ë‘ ê°œì˜ ë¡œì»¬ LLM(phi4:reasoning14B ë° qwen2.5-coder:32B)ì„ ì‚¬ìš©í•˜ì—¬, 10ë²ˆì˜ ì‹œë„ ì˜ˆì‚° ë‚´ì—ì„œ GPT-4ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.

- 5. ì œì•ˆëœ HPT íŒŒì´í”„ë¼ì¸ì€ ì—¬ì„¯ ê°€ì§€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ í‰ê·  ì„±ëŠ¥ì´ GPT-4ì™€ ì•½ 0.9 í¼ì„¼íŠ¸ í¬ì¸íŠ¸ ì°¨ì´ë¥¼ ë³´ì¸ë‹¤.

---

*Generated on 2025-09-22 15:20:02*