# It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge

**Korean Title:** ì˜ì¡´ì„±: ìƒì‹ ì§€ì‹ì„ í™œìš©í•œ ìµœì†Œí•œì˜ ë¬¸ë§¥ì—ì„œì˜ ì§€ì‹œì  ëª¨í˜¸ì„± í•´ê²°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Direct Preference Optimization|Direct Preference Optimization]] [[keywords/specific/Multilingual Evaluation|Multilingual Evaluation]] [[keywords/broad/Commonsense Knowledge|Commonsense Knowledge]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/unique/DeepSeek v3|DeepSeek v3]] [[categories/cs.CL|cs.CL]] [[2025-09-17/Correct-Detect_ Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs_20250917|Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs]] (88.3% similar) [[2025-09-17/Do Large Language Models Understand Word Senses_20250917|Do Large Language Models Understand Word Senses?]] (87.5% similar) [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (87.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Direct Preference Optimization
**ğŸ”— Specific Connectable**: Multilingual Evaluation
**ğŸ”¬ Broad Technical**: Large Language Models, Commonsense Knowledge
**â­ Unique Technical**: DeepSeek v3
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Correct-Detect_ Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs_20250917|Correct-Detect Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs]] (88.3% similar)
- [[2025-09-17/Do Large Language Models Understand Word Senses_20250917|Do Large Language Models Understand Word Senses]] (87.5% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM Language of Browsing]] (87.4% similar)
- [[2025-09-22/Do Retrieval Augmented Language Models Know When They Don't Know_20250922|Do Retrieval Augmented Language Models Know When They Don't Know]] (86.5% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (86.4% similar)


**ArXiv ID**: [2509.16107](https://arxiv.org/abs/2509.16107)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.16107.pdf)


**ArXiv ID**: [2509.16107](https://arxiv.org/abs/2509.16107)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.16107.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Direct Preference Optimization
**ğŸ”— Specific Connectable**: Multilingual Evaluation
**â­ Unique Technical**: DeepSeek v3
**ğŸ”¬ Broad Technical**: Large Language Models, Commonsense Knowledge

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Commonsense Knowledge` â€¢ 

`Multilingual Evaluation` â€¢ 

`DeepSeek v3` â€¢ 

`Direct Preference Optimization`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16107v1 Announce Type: new 
Abstract: Ambiguous words or underspecified references require interlocutors to resolve them, often by relying on shared context and commonsense knowledge. Therefore, we systematically investigate whether Large Language Models (LLMs) can leverage commonsense to resolve referential ambiguity in multi-turn conversations and analyze their behavior when ambiguity persists. Further, we study how requests for simplified language affect this capacity. Using a novel multilingual evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that current LLMs struggle to resolve ambiguity effectively: they tend to commit to a single interpretation or cover all possible references, rather than hedging or seeking clarification. This limitation becomes more pronounced under simplification prompts, which drastically reduce the use of commonsense reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct Preference Optimization substantially improves ambiguity resolution across all request types. These results underscore the need for advanced fine-tuning to improve LLMs' handling of ambiguity and to ensure robust performance across diverse communication styles.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16107v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëª¨í˜¸í•œ ë‹¨ì–´ë‚˜ ëª…ì‹œë˜ì§€ ì•Šì€ ì°¸ì¡°ëŠ” ëŒ€í™”ìë“¤ì´ ì´ë¥¼ í•´ê²°í•´ì•¼ í•˜ë©°, ì¢…ì¢… ê³µìœ ëœ ë§¥ë½ê³¼ ìƒì‹ì  ì§€ì‹ì— ì˜ì¡´í•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìƒì‹ì„ í™œìš©í•˜ì—¬ ë‹¤ì¤‘ íšŒì „ ëŒ€í™”ì—ì„œ ì°¸ì¡°ì  ëª¨í˜¸ì„±ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ê³ , ëª¨í˜¸ì„±ì´ ì§€ì†ë  ë•Œ ê·¸ë“¤ì˜ í–‰ë™ì„ ë¶„ì„í•©ë‹ˆë‹¤. ë˜í•œ, ê°„ë‹¨í•œ ì–¸ì–´ ìš”ì²­ì´ ì´ ëŠ¥ë ¥ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì—°êµ¬í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ë‹¤êµ­ì–´ í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, Llama-3.1-8Bë¥¼ LLM-as-Judge ë° ì¸ê°„ ì£¼ì„ì„ í†µí•´ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” í˜„ì¬ì˜ LLMì´ ëª¨í˜¸ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤: ê·¸ë“¤ì€ ë‹¨ì¼ í•´ì„ì— ì „ë…í•˜ê±°ë‚˜ ëª¨ë“  ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ í¬ê´„í•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ëª¨í˜¸ì„±ì„ í”¼í•˜ê±°ë‚˜ ëª…í™•í™”ë¥¼ ì¶”êµ¬í•˜ê¸°ë³´ë‹¤ëŠ” ê·¸ë ‡ìŠµë‹ˆë‹¤. ì´ ì œí•œì€ ê°„ì†Œí™” ìš”ì²­ í•˜ì—ì„œ ë”ìš± ë‘ë“œëŸ¬ì§€ë©°, ì´ëŠ” ìƒì‹ì  ì¶”ë¡ ê³¼ ë‹¤ì–‘í•œ ì‘ë‹µ ì „ëµì˜ ì‚¬ìš©ì„ í¬ê²Œ ê°ì†Œì‹œí‚µë‹ˆë‹¤. Llama-3.1-8Bë¥¼ ì§ì ‘ ì„ í˜¸ ìµœì í™”ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ëª¨ë“  ìš”ì²­ ìœ í˜•ì— ê±¸ì³ ëª¨í˜¸ì„± í•´ê²°ì´ í¬ê²Œ ê°œì„ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLMì˜ ëª¨í˜¸ì„± ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ë‹¤ì–‘í•œ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ê³ ê¸‰ ë¯¸ì„¸ ì¡°ì •ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì¤‘ íšŒì°¨ ëŒ€í™”ì—ì„œ ëª¨í˜¸í•œ ì°¸ì¡°ë¥¼ í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. íŠ¹íˆ, ê³µí†µì˜ ë§¥ë½ê³¼ ìƒì‹ì  ì§€ì‹ì„ í™œìš©í•´ ì°¸ì¡° ëª¨í˜¸ì„±ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•˜ë©°, ëª¨í˜¸ì„±ì´ ì§€ì†ë  ë•Œì˜ ëª¨ë¸ í–‰ë™ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ìƒˆë¡œìš´ ë‹¤êµ­ì–´ í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, Llama-3.1-8Bë¥¼ í…ŒìŠ¤íŠ¸í•˜ì˜€ìœ¼ë©°, LLM-as-Judgeì™€ ì¸ê°„ ì£¼ì„ì„ í†µí•´ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, í˜„ì¬ LLMì€ ëª¨í˜¸ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ë‹¨ì¼ í•´ì„ì— ì§‘ì°©í•˜ê±°ë‚˜ ëª¨ë“  ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ í¬ê´„í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¨ìˆœí™” ìš”ì²­ ì‹œ ìƒì‹ì  ì¶”ë¡ ê³¼ ë‹¤ì–‘í•œ ì‘ë‹µ ì „ëµ ì‚¬ìš©ì´ í¬ê²Œ ê°ì†Œí•©ë‹ˆë‹¤. Llama-3.1-8Bì˜ ì„¸ë°€í•œ íŠœë‹ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê°œì„ í•˜ì—¬ ëª¨í˜¸ì„± í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” LLMì˜ ëª¨í˜¸ì„± ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ê³ ê¸‰ íŠœë‹ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì¤‘ íšŒì°¨ ëŒ€í™”ì—ì„œ ì°¸ì¡°ì  ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ë‹¨ì¼ í•´ì„ì— ì§‘ì°©í•˜ê±°ë‚˜ ëª¨ë“  ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ í¬ê´„í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.

- 2. ë‹¨ìˆœí™”ëœ ì–¸ì–´ ìš”ì²­ì€ ìƒì‹ì  ì¶”ë¡ ê³¼ ë‹¤ì–‘í•œ ì‘ë‹µ ì „ëµ ì‚¬ìš©ì„ í¬ê²Œ ê°ì†Œì‹œì¼œ ëª¨í˜¸ì„± í•´ê²° ëŠ¥ë ¥ì„ ì €í•˜ì‹œí‚¨ë‹¤.

- 3. Llama-3.1-8B ëª¨ë¸ì„ Direct Preference Optimizationìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ëª¨ë“  ìš”ì²­ ìœ í˜•ì—ì„œ ëª¨í˜¸ì„± í•´ê²° ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒëœë‹¤.

- 4. ì—°êµ¬ ê²°ê³¼ëŠ” LLMì˜ ëª¨í˜¸ì„± ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê°œì„ í•˜ê³  ë‹¤ì–‘í•œ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ì—ì„œì˜ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ê³ ê¸‰ ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•¨ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-22 16:29:23*