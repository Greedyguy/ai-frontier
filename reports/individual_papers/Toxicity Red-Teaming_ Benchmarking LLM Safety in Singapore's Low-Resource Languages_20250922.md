# Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages

**Korean Title:** 독성 레드팀: 싱가포르의 저자원 언어에서 LLM 안전성 벤치마킹

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Red-Teaming Approach|Red-Teaming Approach]] [[keywords/specific/Multilingual LLMs|Multilingual LLMs]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/unique/SGToxicGuard|SGToxicGuard]] [[categories/cs.CL|cs.CL]] [[2025-09-22/SABER_ Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection_20250922|SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection]] (85.2% similar) [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (84.7% similar) [[2025-09-19/Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (84.5% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Red-Teaming Approach
**🔗 Specific Connectable**: Multilingual LLMs
**🔬 Broad Technical**: Large Language Models, Natural Language Processing
**⭐ Unique Technical**: SGToxicGuard
## 🔗 유사한 논문
- [[2025-09-22/SABER_ Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection_20250922|SABER Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection]] (85.2% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (84.7% similar)
- [[2025-09-19/Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (84.5% similar)
- [[2025-09-18/Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs_20250918|Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs]] (84.5% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (84.0% similar)


**ArXiv ID**: [2509.15260](https://arxiv.org/abs/2509.15260)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15260.pdf)


**ArXiv ID**: [2509.15260](https://arxiv.org/abs/2509.15260)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15260.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Red-Teaming Approach
**🔗 Specific Connectable**: Multilingual LLMs
**⭐ Unique Technical**: SGToxicGuard
**🔬 Broad Technical**: Large Language Models, Natural Language Processing

## 🏷️ 추출된 키워드



`Large Language Models` • 

`Natural Language Processing` • 

`Multilingual LLMs` • 

`SGToxicGuard` • 

`Red-teaming Approach`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15260v1 Announce Type: new 
Abstract: The advancement of Large Language Models (LLMs) has transformed natural language processing; however, their safety mechanisms remain under-explored in low-resource, multilingual settings. Here, we aim to bridge this gap. In particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation framework for benchmarking LLM safety in Singapore's diverse linguistic context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a red-teaming approach to systematically probe LLM vulnerabilities in three real-world scenarios: \textit{conversation}, \textit{question-answering}, and \textit{content composition}. We conduct extensive experiments with state-of-the-art multilingual LLMs, and the results uncover critical gaps in their safety guardrails. By offering actionable insights into cultural sensitivity and toxicity mitigation, we lay the foundation for safer and more inclusive AI systems in linguistically diverse environments.\footnote{Link to the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.} \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

## 🔍 Abstract (한글 번역)

arXiv:2509.15260v1 발표 유형: 신규  
초록: 대형 언어 모델(LLMs)의 발전은 자연어 처리 분야에 혁신을 가져왔지만, 저자원 다국어 환경에서의 안전 메커니즘은 여전히 충분히 탐구되지 않았습니다. 본 연구에서는 이 격차를 해소하고자 합니다. 특히, 싱가포르의 다양한 언어적 맥락(싱글리시, 중국어, 말레이어, 타밀어 포함)에서 LLM의 안전성을 평가하기 위한 새로운 데이터셋 및 평가 프레임워크인 \textsf{SGToxicGuard}를 소개합니다. SGToxicGuard는 레드팀 접근법을 채택하여 \textit{대화}, \textit{질문-응답}, \textit{콘텐츠 작성}의 세 가지 실제 시나리오에서 LLM의 취약점을 체계적으로 탐색합니다. 최첨단 다국어 LLM을 사용하여 광범위한 실험을 수행한 결과, 이들의 안전 장치에 중요한 격차가 있음을 밝혀냈습니다. 문화적 민감성과 독성 완화에 대한 실행 가능한 통찰력을 제공함으로써, 언어적으로 다양한 환경에서 더 안전하고 포용적인 AI 시스템을 위한 기초를 마련합니다.\footnote{데이터셋 링크: https://github.com/Social-AI-Studio/SGToxicGuard.} \textcolor{red}{면책 조항: 이 논문에는 일부 독자에게 불쾌감을 줄 수 있는 민감한 내용이 포함되어 있습니다.}

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 안전 메커니즘이 저자원, 다언어 환경에서 충분히 탐구되지 않았음을 지적하며, 이를 해결하기 위해 \textsf{SGToxicGuard}라는 새로운 데이터셋과 평가 프레임워크를 소개합니다. 싱가포르의 다양한 언어적 맥락을 반영한 이 프레임워크는 Singlish, 중국어, 말레이어, 타밀어를 포함하여 LLM의 안전성을 평가합니다. \textsf{SGToxicGuard}는 대화, 질문-응답, 콘텐츠 작성의 세 가지 실제 시나리오에서 LLM의 취약점을 체계적으로 탐색하는 레드팀 접근 방식을 채택합니다. 최신 다언어 LLM을 대상으로 한 실험 결과, 안전성에 중요한 격차가 있음을 발견했습니다. 이 연구는 문화적 민감성과 독성 완화에 대한 실질적인 통찰을 제공하여 언어적으로 다양한 환경에서 더 안전하고 포용적인 AI 시스템의 기초를 마련합니다.

## 🎯 주요 포인트


- 1. 대형 언어 모델(LLM)의 안전 메커니즘은 저자원, 다언어 환경에서 충분히 탐구되지 않았다.

- 2. SGToxicGuard는 싱가포르의 다양한 언어적 맥락에서 LLM의 안전성을 평가하기 위한 새로운 데이터셋 및 평가 프레임워크이다.

- 3. SGToxicGuard는 대화, 질문-응답, 콘텐츠 구성의 세 가지 실제 시나리오에서 LLM의 취약성을 체계적으로 탐색한다.

- 4. 최첨단 다언어 LLM을 대상으로 한 실험 결과, 안전성의 중요한 격차가 드러났다.

- 5. 문화적 민감성과 독성 완화에 대한 실행 가능한 통찰을 제공하여, 언어적으로 다양한 환경에서 더 안전하고 포용적인 AI 시스템의 기초를 마련한다.


---

*Generated on 2025-09-22 16:19:04*