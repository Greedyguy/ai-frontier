
# SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation

**Korean Title:** SSL-SSAW: 질문 기반 수어 번역을 위한 시그모이드 자기 주의 가중치를 사용한 자기 지도 학습

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Question-based Sign Language Translation|Question-based Sign Language Translation]] [[keywords/broad/Self-supervised Learning|Self-supervised Learning]] [[keywords/broad/Attention Mechanism|Attention Mechanism]] [[keywords/specific/Multimodal Fusion|Multimodal Fusion]] [[keywords/unique/SSL-SSAW|SSL-SSAW]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Question-based Sign Language Translation
**🔬 Broad Technical**: Self-supervised Learning, Attention Mechanism
**🔗 Specific Connectable**: Multimodal Fusion
**⭐ Unique Technical**: SSL-SSAW

**ArXiv ID**: [2509.14036](https://arxiv.org/abs/2509.14036)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.14036.pdf)


## 🏷️ 추출된 키워드



`Self-supervised Learning` • 

`Attention Mechanism` • 

`Multimodal Fusion` • 

`SSL-SSAW` • 

`Question-based Sign Language Translation`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14036v1 Announce Type: cross 
Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf people and hearing people, where dialogue provides crucial contextual cues to aid in translation. Building on this foundational concept, this paper proposes Question-based Sign Language Translation (QB-SLT), a novel task that explores the efficient integration of dialogue. Unlike gloss (sign language transcription) annotations, dialogue naturally occurs in communication and is easier to annotate. The key challenge lies in aligning multimodality features while leveraging the context of the question to improve translation. To address this issue, we propose a cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) fusion method for sign language translation. Specifically, we employ contrastive learning to align multimodality features in QB-SLT, then introduce a Sigmoid Self-attention Weighting (SSAW) module for adaptive feature extraction from question and sign language sequences. Additionally, we leverage available question text through self-supervised learning to enhance representation and translation capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably, easily accessible question assistance can achieve or even surpass the performance of gloss assistance. Furthermore, visualization results demonstrate the effectiveness of incorporating dialogue in improving translation quality.

## 🔍 Abstract (한글 번역)

arXiv:2509.14036v1 발표 유형: 교차
요약: 수화 번역(SLT)은 청각 장애인과 청각 장애가 없는 사람들 간의 의사 소통 간극을 줄여주는데, 여기서 대화는 번역을 돕는 중요한 맥락적 단서를 제공합니다. 이 기본 개념을 기반으로, 본 논문은 질문 중심 수화 번역(QB-SLT)이라는 혁신적인 작업을 제안합니다. 이 작업은 대화의 효율적 통합을 탐구합니다. 번역을 위해 질문의 맥락을 활용하는 것이 핵심적인 도전입니다. 이 문제를 해결하기 위해, 우리는 수화 번역을 위한 교차 모달리티 자기 지도 학습과 시그모이드 자기 주의 가중치(SSAW) 퓨전 방법을 제안합니다. 구체적으로, 우리는 QB-SLT에서 다중 모달리티 특징을 정렬하기 위해 대조적 학습을 채택하고, 질문과 수화 순서에서 적응적 특징 추출을 위한 SSAW 모듈을 도입합니다. 또한, 자기 지도 학습을 통해 사용 가능한 질문 텍스트를 활용하여 표현 및 번역 능력을 향상시킵니다. 우리는 새롭게 구성된 CSL-Daily-QA 및 PHOENIX-2014T-QA 데이터셋에서 접근한 접근 방법을 평가했으며, SSL-SSAW가 SOTA 성능을 달성했습니다. 특히, 쉽게 접근할 수 있는 질문 지원은 번역 지원을 달성하거나 심지어 초월할 수 있습니다. 또한, 시각화 결과는 대화를 통합하여 번역 품질을 향상시키는 효과를 보여줍니다.

## 📝 요약

이 연구는 수어 번역을 위한 질문 기반 수어 번역(QB-SLT) 방법을 제안하였다. 이를 위해 대화를 효율적으로 통합하는 방법을 탐구하였으며, 대화는 번역을 돕는 중요한 맥락적 단서를 제공한다. QB-SLT에서는 대화를 자연스럽게 활용하고자 하였으며, 이를 위해 Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) 융합 방법을 제안하였다. 이 방법은 대화를 통해 번역 품질을 향상시키는 효과를 시각화 결과를 통해 입증하였으며, CSL-Daily-QA 및 PHOENIX-2014T-QA 데이터셋에서 SOTA 성능을 달성하였다. 이를 통해 질문 보조가 번역 성능을 향상시킬 수 있음을 입증하였다.

## 🎯 주요 포인트


- 수화 번역은 청각 장애인과 청각 장애가 없는 사람 사이의 의사 소통 간격을 줄여준다.

- 질문 기반 수화 번역(QB-SLT)은 다이얼로그를 효율적으로 통합하는 새로운 작업이다.

- SSL-SSAW 방법은 수화 번역에서 SOTA 성능을 달성하며 대화를 통해 번역 품질을 향상시킬 수 있다.


---

*Generated on 2025-09-18 16:25:00*