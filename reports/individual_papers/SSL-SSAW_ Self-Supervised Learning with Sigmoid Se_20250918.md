
# SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation

**Korean Title:** SSL-SSAW: ì§ˆë¬¸ ê¸°ë°˜ ìˆ˜ì–´ ë²ˆì—­ì„ ìœ„í•œ ì‹œê·¸ëª¨ì´ë“œ ìê¸° ì£¼ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•œ ìê¸° ì§€ë„ í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Question-based Sign Language Translation|Question-based Sign Language Translation]] [[keywords/broad/Self-supervised Learning|Self-supervised Learning]] [[keywords/broad/Attention Mechanism|Attention Mechanism]] [[keywords/specific/Multimodal Fusion|Multimodal Fusion]] [[keywords/unique/SSL-SSAW|SSL-SSAW]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Question-based Sign Language Translation
**ğŸ”¬ Broad Technical**: Self-supervised Learning, Attention Mechanism
**ğŸ”— Specific Connectable**: Multimodal Fusion
**â­ Unique Technical**: SSL-SSAW

**ArXiv ID**: [2509.14036](https://arxiv.org/abs/2509.14036)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.14036.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Self-supervised Learning` â€¢ 

`Attention Mechanism` â€¢ 

`Multimodal Fusion` â€¢ 

`SSL-SSAW` â€¢ 

`Question-based Sign Language Translation`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14036v1 Announce Type: cross 
Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf people and hearing people, where dialogue provides crucial contextual cues to aid in translation. Building on this foundational concept, this paper proposes Question-based Sign Language Translation (QB-SLT), a novel task that explores the efficient integration of dialogue. Unlike gloss (sign language transcription) annotations, dialogue naturally occurs in communication and is easier to annotate. The key challenge lies in aligning multimodality features while leveraging the context of the question to improve translation. To address this issue, we propose a cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) fusion method for sign language translation. Specifically, we employ contrastive learning to align multimodality features in QB-SLT, then introduce a Sigmoid Self-attention Weighting (SSAW) module for adaptive feature extraction from question and sign language sequences. Additionally, we leverage available question text through self-supervised learning to enhance representation and translation capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably, easily accessible question assistance can achieve or even surpass the performance of gloss assistance. Furthermore, visualization results demonstrate the effectiveness of incorporating dialogue in improving translation quality.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14036v1 ë°œí‘œ ìœ í˜•: êµì°¨
ìš”ì•½: ìˆ˜í™” ë²ˆì—­(SLT)ì€ ì²­ê° ì¥ì• ì¸ê³¼ ì²­ê° ì¥ì• ê°€ ì—†ëŠ” ì‚¬ëŒë“¤ ê°„ì˜ ì˜ì‚¬ ì†Œí†µ ê°„ê·¹ì„ ì¤„ì—¬ì£¼ëŠ”ë°, ì—¬ê¸°ì„œ ëŒ€í™”ëŠ” ë²ˆì—­ì„ ë•ëŠ” ì¤‘ìš”í•œ ë§¥ë½ì  ë‹¨ì„œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ê¸°ë³¸ ê°œë…ì„ ê¸°ë°˜ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì€ ì§ˆë¬¸ ì¤‘ì‹¬ ìˆ˜í™” ë²ˆì—­(QB-SLT)ì´ë¼ëŠ” í˜ì‹ ì ì¸ ì‘ì—…ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ëŒ€í™”ì˜ íš¨ìœ¨ì  í†µí•©ì„ íƒêµ¬í•©ë‹ˆë‹¤. ë²ˆì—­ì„ ìœ„í•´ ì§ˆë¬¸ì˜ ë§¥ë½ì„ í™œìš©í•˜ëŠ” ê²ƒì´ í•µì‹¬ì ì¸ ë„ì „ì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìˆ˜í™” ë²ˆì—­ì„ ìœ„í•œ êµì°¨ ëª¨ë‹¬ë¦¬í‹° ìê¸° ì§€ë„ í•™ìŠµê³¼ ì‹œê·¸ëª¨ì´ë“œ ìê¸° ì£¼ì˜ ê°€ì¤‘ì¹˜(SSAW) í“¨ì „ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” QB-SLTì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹° íŠ¹ì§•ì„ ì •ë ¬í•˜ê¸° ìœ„í•´ ëŒ€ì¡°ì  í•™ìŠµì„ ì±„íƒí•˜ê³ , ì§ˆë¬¸ê³¼ ìˆ˜í™” ìˆœì„œì—ì„œ ì ì‘ì  íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ SSAW ëª¨ë“ˆì„ ë„ì…í•©ë‹ˆë‹¤. ë˜í•œ, ìê¸° ì§€ë„ í•™ìŠµì„ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ í‘œí˜„ ë° ë²ˆì—­ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìƒˆë¡­ê²Œ êµ¬ì„±ëœ CSL-Daily-QA ë° PHOENIX-2014T-QA ë°ì´í„°ì…‹ì—ì„œ ì ‘ê·¼í•œ ì ‘ê·¼ ë°©ë²•ì„ í‰ê°€í–ˆìœ¼ë©°, SSL-SSAWê°€ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ ì§€ì›ì€ ë²ˆì—­ ì§€ì›ì„ ë‹¬ì„±í•˜ê±°ë‚˜ ì‹¬ì§€ì–´ ì´ˆì›”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‹œê°í™” ê²°ê³¼ëŠ” ëŒ€í™”ë¥¼ í†µí•©í•˜ì—¬ ë²ˆì—­ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ìˆ˜ì–´ ë²ˆì—­ì„ ìœ„í•œ ì§ˆë¬¸ ê¸°ë°˜ ìˆ˜ì–´ ë²ˆì—­(QB-SLT) ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤. ì´ë¥¼ ìœ„í•´ ëŒ€í™”ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ì˜€ìœ¼ë©°, ëŒ€í™”ëŠ” ë²ˆì—­ì„ ë•ëŠ” ì¤‘ìš”í•œ ë§¥ë½ì  ë‹¨ì„œë¥¼ ì œê³µí•œë‹¤. QB-SLTì—ì„œëŠ” ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•˜ê³ ì í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ ìœ„í•´ Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) ìœµí•© ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤. ì´ ë°©ë²•ì€ ëŒ€í™”ë¥¼ í†µí•´ ë²ˆì—­ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ì‹œê°í™” ê²°ê³¼ë¥¼ í†µí•´ ì…ì¦í•˜ì˜€ìœ¼ë©°, CSL-Daily-QA ë° PHOENIX-2014T-QA ë°ì´í„°ì…‹ì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤. ì´ë¥¼ í†µí•´ ì§ˆë¬¸ ë³´ì¡°ê°€ ë²ˆì—­ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì…ì¦í•˜ì˜€ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- ìˆ˜í™” ë²ˆì—­ì€ ì²­ê° ì¥ì• ì¸ê³¼ ì²­ê° ì¥ì• ê°€ ì—†ëŠ” ì‚¬ëŒ ì‚¬ì´ì˜ ì˜ì‚¬ ì†Œí†µ ê°„ê²©ì„ ì¤„ì—¬ì¤€ë‹¤.

- ì§ˆë¬¸ ê¸°ë°˜ ìˆ˜í™” ë²ˆì—­(QB-SLT)ì€ ë‹¤ì´ì–¼ë¡œê·¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ì‘ì—…ì´ë‹¤.

- SSL-SSAW ë°©ë²•ì€ ìˆ˜í™” ë²ˆì—­ì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©° ëŒ€í™”ë¥¼ í†µí•´ ë²ˆì—­ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-18 16:25:00*