# Dynamic Policy Fusion for User Alignment Without Re-Interaction

**Korean Title:** ì‚¬ìš©ì ì¬ìƒí˜¸ì‘ìš© ì—†ì´ ì‚¬ìš©ì ì •ë ¬ì„ ìœ„í•œ ë™ì  ì •ì±… ìœµí•©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Zero-shot Alignment

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (83.4% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (82.5% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (81.9% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (81.7% similar)
- [[2025-09-19/Emergent Alignment via Competition_20250919|Emergent Alignment via Competition]] (81.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2409.20016v4 Announce Type: replace 
Abstract: Deep reinforcement learning (RL) policies, although optimal in terms of task rewards, may not align with the personal preferences of human users. To ensure this alignment, a naive solution would be to retrain the agent using a reward function that encodes the user's specific preferences. However, such a reward function is typically not readily available, and as such, retraining the agent from scratch can be prohibitively expensive. We propose a more practical approach - to adapt the already trained policy to user-specific needs with the help of human feedback. To this end, we infer the user's intent through trajectory-level feedback and combine it with the trained task policy via a theoretically grounded dynamic policy fusion approach. As our approach collects human feedback on the very same trajectories used to learn the task policy, it does not require any additional interactions with the environment, making it a zero-shot approach. We empirically demonstrate in a number of environments that our proposed dynamic policy fusion approach consistently achieves the intended task while simultaneously adhering to user-specific needs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2409.20016v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL) ì •ì±…ì€ ê³¼ì œ ë³´ìƒ ì¸¡ë©´ì—ì„œ ìµœì ì¼ì§€ë¼ë„ ì¸ê°„ ì‚¬ìš©ìì˜ ê°œì¸ì  ì„ í˜¸ì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¼ì¹˜ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•œ ë‹¨ìˆœí•œ í•´ê²°ì±…ì€ ì‚¬ìš©ìì˜ íŠ¹ì • ì„ í˜¸ë¥¼ ì•”í˜¸í™”í•œ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ì¬í›ˆë ¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë³´ìƒ í•¨ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ì—†ìœ¼ë©°, ë”°ë¼ì„œ ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì¬í›ˆë ¨í•˜ëŠ” ê²ƒì€ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ í›ˆë ¨ëœ ì •ì±…ì„ ì¸ê°„ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ì íŠ¹ì • ìš”êµ¬ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ë³´ë‹¤ ì‹¤ìš©ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ê¶¤ì  ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì¶”ë¡ í•˜ê³ , ì´ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ê¸°ë°˜ì„ ë‘” ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ í›ˆë ¨ëœ ê³¼ì œ ì •ì±…ê³¼ ê²°í•©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ê³¼ì œ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ëœ ë™ì¼í•œ ê¶¤ì ì— ëŒ€í•œ ì¸ê°„ì˜ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ë¯€ë¡œ í™˜ê²½ê³¼ì˜ ì¶”ê°€ì ì¸ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” ì œë¡œìƒ· ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ì œì•ˆëœ ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ ë°©ì‹ì´ ì‚¬ìš©ì íŠ¹ì • ìš”êµ¬ë¥¼ ì¶©ì¡±í•˜ë©´ì„œ ì˜ë„ëœ ê³¼ì œë¥¼ ì¼ê´€ë˜ê²Œ ë‹¬ì„±í•¨ì„ ì‹¤ì¦ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL) ì •ì±…ì´ ì‚¬ìš©ì ê°œì¸ì˜ ì„ í˜¸ì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì •ì±…ì„ ì‚¬ìš©ì ìš”êµ¬ì— ë§ê²Œ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ì í”¼ë“œë°±ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‚¬ìš©ì ì˜ë„ë¥¼ ê²½ë¡œ ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì„ í†µí•´ ì¶”ë¡ í•˜ê³ , ì´ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ê¸°ë°˜í•œ ë™ì  ì •ì±… ìœµí•© ë°©ë²•ìœ¼ë¡œ ê¸°ì¡´ í•™ìŠµëœ ì •ì±…ê³¼ ê²°í•©í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ í™˜ê²½ê³¼ì˜ ì¶”ê°€ ìƒí˜¸ì‘ìš© ì—†ì´ ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬, ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”êµ¬ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤. ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ì‚¬ìš©ì ìš”êµ¬ë¥¼ ì¶©ì¡±í•˜ë©´ì„œë„ ê³¼ì œë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¬ì¸µ ê°•í™” í•™ìŠµ ì •ì±…ì€ ì‚¬ìš©ì ê°œì¸ì˜ ì„ í˜¸ë„ì™€ í•­ìƒ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

- 2. ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì •ì±…ì„ ì‚¬ìš©ì ìš”êµ¬ì— ë§ê²Œ ì¸ê°„ì˜ í”¼ë“œë°±ì„ í™œìš©í•˜ì—¬ ì ì‘ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.

- 3. ì œì•ˆëœ ë°©ë²•ì€ ê²½ë¡œ ìˆ˜ì¤€ì˜ í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì¶”ë¡ í•˜ê³ , ì´ë¥¼ í•™ìŠµëœ ì •ì±…ê³¼ ì´ë¡ ì ìœ¼ë¡œ ê²°í•©í•œë‹¤.

- 4. ì´ ì ‘ê·¼ë²•ì€ í™˜ê²½ê³¼ì˜ ì¶”ê°€ ìƒí˜¸ì‘ìš© ì—†ì´ ë™ì¼í•œ ê²½ë¡œì—ì„œ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ì‘ë™í•œë‹¤.

- 5. ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì œì•ˆëœ ë™ì  ì •ì±… ìœµí•© ì ‘ê·¼ë²•ì´ ì‚¬ìš©ì ìš”êµ¬ë¥¼ ì¶©ì¡±í•˜ë©´ì„œë„ ì˜ë„ëœ ì‘ì—…ì„ ì¼ê´€ë˜ê²Œ ìˆ˜í–‰í•¨ì„ ì‹¤ì¦ì ìœ¼ë¡œ ì…ì¦í•˜ì˜€ë‹¤.

---

*Generated on 2025-09-22 14:28:29*