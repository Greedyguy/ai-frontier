# MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework

**Korean Title:** MoE-CE: ì „ë¬¸ê°€ í˜¼í•© í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì±„ë„ ì¶”ì •ì˜ ì¼ë°˜í™” í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Mixture of Experts, Zero-shot Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (84.1% similar)
- [[2025-09-19/Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation_20250919|Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation]] (84.1% similar)
- [[2025-09-17/Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection_20250917|Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection]] (83.8% similar)
- [[2025-09-18/CSMoE_ An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts_20250918|CSMoE An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts]] (82.9% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (82.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15964v1 Announce Type: cross 
Abstract: Reliable channel estimation (CE) is fundamental for robust communication in dynamic wireless environments, where models must generalize across varying conditions such as signal-to-noise ratios (SNRs), the number of resource blocks (RBs), and channel profiles. Traditional deep learning (DL)-based methods struggle to generalize effectively across such diverse settings, particularly under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a flexible mixture-of-experts (MoE) framework designed to enhance the generalization capability of DL-based CE methods. MoE-CE provides an appropriate inductive bias by leveraging multiple expert subnetworks, each specialized in distinct channel characteristics, and a learned router that dynamically selects the most relevant experts per input. This architecture enhances model capacity and adaptability without a proportional rise in computational cost while being agnostic to the choice of the backbone model and the learning algorithm. Through extensive experiments on synthetic datasets generated under diverse SNRs, RB numbers, and channel profiles, including multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently outperforms conventional DL approaches, achieving significant performance gains while maintaining efficiency.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15964v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì±„ë„ ì¶”ì •(CE)ì€ ë™ì  ë¬´ì„  í™˜ê²½ì—ì„œ ê²¬ê³ í•œ í†µì‹ ì„ ìœ„í•´ í•„ìˆ˜ì ì´ë©°, ì—¬ê¸°ì„œ ëª¨ë¸ì€ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„(SNR), ìì› ë¸”ë¡(RB)ì˜ ìˆ˜, ì±„ë„ í”„ë¡œí•„ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì¡°ê±´ì— ê±¸ì³ ì¼ë°˜í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì „í†µì ì¸ ë”¥ëŸ¬ë‹(DL) ê¸°ë°˜ ë°©ë²•ì€ ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì„¤ì •, íŠ¹íˆ ë©€í‹°íƒœìŠ¤í¬ ë° ì œë¡œìƒ· ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì¼ë°˜í™”í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” DL ê¸°ë°˜ CE ë°©ë²•ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ìœ ì—°í•œ ì „ë¬¸ê°€ í˜¼í•©(MoE) í”„ë ˆì„ì›Œí¬ì¸ MoE-CEë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MoE-CEëŠ” ì—¬ëŸ¬ ì „ë¬¸ê°€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ê°ê¸° ë‹¤ë¥¸ ì±„ë„ íŠ¹ì„±ì— íŠ¹í™”ëœ ì ì ˆí•œ ê·€ë‚©ì  í¸í–¥ì„ ì œê³µí•˜ë©°, í•™ìŠµëœ ë¼ìš°í„°ê°€ ì…ë ¥ì— ë”°ë¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì „ë¬¸ê°€ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤. ì´ ì•„í‚¤í…ì²˜ëŠ” ëª¨ë¸ ìš©ëŸ‰ê³¼ ì ì‘ì„±ì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ê³„ì‚° ë¹„ìš©ì˜ ë¹„ë¡€ì  ì¦ê°€ ì—†ì´ ë°±ë³¸ ëª¨ë¸ê³¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì„ íƒì— êµ¬ì• ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ SNR, RB ìˆ˜, ì±„ë„ í”„ë¡œí•„ í•˜ì—ì„œ ìƒì„±ëœ í•©ì„± ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´, ë©€í‹°íƒœìŠ¤í¬ ë° ì œë¡œìƒ· í‰ê°€ë¥¼ í¬í•¨í•˜ì—¬, MoE-CEê°€ ê¸°ì¡´ DL ì ‘ê·¼ ë°©ì‹ì„ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•˜ë©° íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë™ì  ë¬´ì„  í™˜ê²½ì—ì„œ ê°•ë ¥í•œ í†µì‹ ì„ ìœ„í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì±„ë„ ì¶”ì •(CE)ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ì€ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ì¼ë°˜í™”í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ MoE-CEë¼ëŠ” ìœ ì—°í•œ ì „ë¬¸ê°€ í˜¼í•©(MoE) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MoE-CEëŠ” ì—¬ëŸ¬ ì „ë¬¸ê°€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ê°ê¸° ë‹¤ë¥¸ ì±„ë„ íŠ¹ì„±ì— íŠ¹í™”ëœ ëª¨ë¸ì„ êµ¬ì„±í•˜ê³ , ì…ë ¥ì— ë”°ë¼ ì ì ˆí•œ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•˜ëŠ” ë¼ìš°í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ëª¨ë¸ì˜ ìš©ëŸ‰ê³¼ ì ì‘ì„±ì„ ë†’ì´ë©´ì„œë„ ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¦ê°€ì‹œí‚¤ì§€ ì•Šìœ¼ë©°, ë°±ë³¸ ëª¨ë¸ê³¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ë…ë¦½ì ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ SNR, RB ìˆ˜, ì±„ë„ í”„ë¡œíŒŒì¼ì„ í¬í•¨í•œ í•©ì„± ë°ì´í„°ì…‹ ì‹¤í—˜ì„ í†µí•´, MoE-CEê°€ ê¸°ì¡´ ë”¥ëŸ¬ë‹ ë°©ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MoE-CEëŠ” ë‹¤ì–‘í•œ ì±„ë„ íŠ¹ì„±ì— íŠ¹í™”ëœ ì—¬ëŸ¬ ì „ë¬¸ê°€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ DL ê¸°ë°˜ CE ë°©ë²•ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 2. MoE-CEëŠ” ì…ë ¥ì— ë”°ë¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì „ë¬¸ê°€ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” í•™ìŠµëœ ë¼ìš°í„°ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë¸ì˜ ì ì‘ì„±ì„ ë†’ì…ë‹ˆë‹¤.

- 3. ì œì•ˆëœ MoE-CE í”„ë ˆì„ì›Œí¬ëŠ” ë°±ë³¸ ëª¨ë¸ê³¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì„ íƒì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ê³„ì‚° ë¹„ìš©ì˜ ë¹„ë¡€ì  ì¦ê°€ ì—†ì´ ëª¨ë¸ ìš©ëŸ‰ì„ í™•ì¥í•©ë‹ˆë‹¤.

- 4. ë‹¤ì–‘í•œ SNR, RB ìˆ˜, ì±„ë„ í”„ë¡œí•„ì„ í¬í•¨í•œ í•©ì„± ë°ì´í„°ì…‹ ì‹¤í—˜ì—ì„œ MoE-CEëŠ” ê¸°ì¡´ DL ì ‘ê·¼ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

- 5. MoE-CEëŠ” ë©€í‹°íƒœìŠ¤í¬ ë° ì œë¡œìƒ· í‰ê°€ì—ì„œë„ íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:19:31*