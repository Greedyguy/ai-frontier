# Deep Reinforcement Learning with Gradient Eligibility Traces

**Korean Title:** ê¹Šì´ ê°•í™” í•™ìŠµì—ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì ê²©ì„± ì¶”ì 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multistep Credit Assignment

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (85.5% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (83.5% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (82.9% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks A Second-Order Optimization Perspective]] (82.7% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.09087v2 Announce Type: replace-cross 
Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the generalized Projected Bellman Error ($\overline{\text{PBE}}$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the generalized $\overline{\text{PBE}}$ objective to support multistep credit assignment based on the $\lambda$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at https://github.com/esraaelelimy/gtd\_algos

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.09087v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì—ì„œ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì˜¤í”„-ì •ì±… í•™ìŠµì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ ë„ì „ì ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ë°©ë²•ì€ ë‹¨ìˆœì„±ê³¼ íš¨ìœ¨ì„± ë•Œë¬¸ì— ë°˜-êµ¬ë°° ì‹œì°¨-ì°¨ì´(TD) ë°©ë²•ì— ì˜ì¡´í•˜ì§€ë§Œ, ê·¸ ê²°ê³¼ ë°œì‚°ì— ì·¨ì•½í•©ë‹ˆë‹¤. ë°˜ë©´ì— Gradient TD(GTD) ë°©ë²•ê³¼ ê°™ì€ ë” ì›ì¹™ì ì¸ ì ‘ê·¼ë²•ì€ ê°•ë ¥í•œ ìˆ˜ë ´ ë³´ì¥ì„ ì œê³µí•˜ì§€ë§Œ, ì‹¬ì¸µ RLì—ì„œëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ì¼ë°˜í™”ëœ íˆ¬ì˜ ë²¨ë§Œ ì˜¤ë¥˜($\overline{\text{PBE}}$)ë¥¼ ë„ì…í•˜ì—¬ GTD ë°©ë²•ì´ ë¹„ì„ í˜• í•¨ìˆ˜ ê·¼ì‚¬ì™€ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ì—°êµ¬ëŠ” ì‹ ìš© í• ë‹¹ì´ ëŠë¦¬ê³  ë§ì€ ìƒ˜í”Œì„ í•„ìš”ë¡œ í•˜ëŠ” ì¼ë‹¨ê³„ ë°©ë²•ì— ì œí•œë©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¼ë°˜í™”ëœ $\overline{\text{PBE}}$ ëª©í‘œë¥¼ $\lambda$-ë°˜í™˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¤ë‹¨ê³„ ì‹ ìš© í• ë‹¹ì„ ì§€ì›í•˜ë„ë¡ í™•ì¥í•˜ê³ , ì´ ìƒˆë¡œìš´ ëª©í‘œë¥¼ ìµœì í™”í•˜ëŠ” ì„¸ ê°€ì§€ êµ¬ë°° ê¸°ë°˜ ë°©ë²•ì„ ë„ì¶œí•©ë‹ˆë‹¤. ê²½í—˜ ì¬ìƒê³¼ í˜¸í™˜ë˜ëŠ” ì „ë°©-ë³´ê¸° ê³µì‹ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì•Œê³ ë¦¬ì¦˜ê³¼ í˜¸í™˜ë˜ëŠ” í›„ë°©-ë³´ê¸° ê³µì‹ì„ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì„ í‰ê°€í•˜ê³ , MuJoCo ë° MinAtar í™˜ê²½ì—ì„œ ê°ê° PPOì™€ StreamQë¥¼ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œ: https://github.com/esraaelelimy/gtd\_algos

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ê°•í™” í•™ìŠµì—ì„œ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì˜¤í”„-ì •ì±… í•™ìŠµì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ë°˜-ê¸°ìš¸ê¸° TD ë°©ë²•ì— ì˜ì¡´í•˜ì—¬ ìˆ˜ë ´ ë¬¸ì œë¥¼ ê²ªì§€ë§Œ, GTD ë°©ë²•ì€ ìˆ˜ë ´ ë³´ì¥ì´ ê°•ë ¥í•©ë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ê·¼ì‚¬ì™€ í•¨ê»˜ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ì¼ë°˜í™”ëœ íˆ¬ì˜ ë²¨ë§Œ ì˜¤ë¥˜($\overline{\text{PBE}}$)ë¥¼ ë„ì…í–ˆìœ¼ë‚˜, ì´ëŠ” ëŠë¦° ì‹ ìš© í• ë‹¹ì˜ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” $\lambda$-ë¦¬í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë‹¨ê³„ ì‹ ìš© í• ë‹¹ì„ ì§€ì›í•˜ëŠ” $\overline{\text{PBE}}$ ëª©í‘œë¥¼ í™•ì¥í•˜ê³ , ì´ë¥¼ ìµœì í™”í•˜ëŠ” ì„¸ ê°€ì§€ ê¸°ìš¸ê¸° ê¸°ë°˜ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ MuJoCoì™€ MinAtar í™˜ê²½ì—ì„œ PPOì™€ StreamQë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ì‹¬ì¸µ ê°•í™” í•™ìŠµ ë°©ë²•ë“¤ì€ ë°˜ì •ê·œí™” TD ë°©ë²•ì— ì˜ì¡´í•˜ì—¬ ìˆ˜ë ´ì„±ì— ì·¨ì•½í•˜ë‹¤.

- 2. Gradient TD (GTD) ë°©ë²•ì€ ìˆ˜ë ´ ë³´ì¥ì´ ê°•í•˜ì§€ë§Œ ì‹¬ì¸µ ê°•í™” í•™ìŠµì—ì„œëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë‹¤.

- 3. ì¼ë°˜í™”ëœ Projected Bellman Error($\overline{\text{PBE}}$)ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ê·¼ì‚¬ì™€ í•¨ê»˜ GTD ë°©ë²•ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

- 4. ë³¸ ì—°êµ¬ëŠ” $\lambda$-returnì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë‹¨ê³„ ì‹ ìš© í• ë‹¹ì„ ì§€ì›í•˜ëŠ” ì¼ë°˜í™”ëœ $\overline{\text{PBE}}$ ëª©í‘œë¥¼ í™•ì¥í•˜ì˜€ë‹¤.

- 5. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ MuJoCoì™€ MinAtar í™˜ê²½ì—ì„œ PPOì™€ StreamQë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

---

*Generated on 2025-09-22 14:56:41*