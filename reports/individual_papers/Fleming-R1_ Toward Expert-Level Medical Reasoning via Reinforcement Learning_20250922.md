# Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning

**Korean Title:** Fleming-R1: ê°•í™” í•™ìŠµì„ í†µí•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì˜í•™ì  ì¶”ë¡ ì„ í–¥í•˜ì—¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Knowledge Graph, Chain of Thought

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/MedFact-R1_ Towards Factual Medical Reasoning via Pseudo-Label Augmentation_20250919|MedFact-R1 Towards Factual Medical Reasoning via Pseudo-Label Augmentation]] (86.8% similar)
- [[2025-09-22/Reward Hacking Mitigation using Verifiable Composite Rewards_20250922|Reward Hacking Mitigation using Verifiable Composite Rewards]] (82.7% similar)
- [[2025-09-19/MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL Toward Expert-Level Medical Text Validation with Language Models]] (82.4% similar)
- [[2025-09-22/FLARE_ Faithful Logic-Aided Reasoning and Exploration_20250922|FLARE Faithful Logic-Aided Reasoning and Exploration]] (82.3% similar)
- [[2025-09-17/Combining Evidence and Reasoning for Biomedical Fact-Checking_20250917|Combining Evidence and Reasoning for Biomedical Fact-Checking]] (81.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15279v1 Announce Type: new 
Abstract: While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15279v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì˜ë£Œ ì‘ìš© ë¶„ì•¼ì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì„ìƒ ì¶”ë¡ ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ ì •í™•í•œ ë‹µë³€ê³¼ íˆ¬ëª…í•œ ì¶”ë¡  ê³¼ì •ì„ ëª¨ë‘ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì—¬ì „íˆ ë„ì „ ê³¼ì œì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê²€ì¦ ê°€ëŠ¥í•œ ì˜ë£Œ ì¶”ë¡ ì„ ìœ„í•´ ì„¤ê³„ëœ ëª¨ë¸ì¸ Fleming-R1ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì„¸ ê°€ì§€ ìƒí˜¸ ë³´ì™„ì ì¸ í˜ì‹ ì„ í†µí•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì²«ì§¸, ìš°ë¦¬ì˜ Reasoning-Oriented Data Strategy (RODS)ëŠ” íë ˆì´ì…˜ëœ ì˜ë£Œ QA ë°ì´í„°ì…‹ê³¼ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ í•©ì„±ì„ ê²°í•©í•˜ì—¬, ëŒ€í‘œì„±ì´ ë¶€ì¡±í•œ ì§ˆë³‘, ì•½ë¬¼ ë° ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡  ì²´ì¸ì˜ ë²”ìœ„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ìš°ë¦¬ëŠ” Chain-of-Thought (CoT) ì½œë“œ ìŠ¤íƒ€íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµì‚¬ ëª¨ë¸ë¡œë¶€í„° ê³ í’ˆì§ˆì˜ ì¶”ë¡  ê²½ë¡œë¥¼ ì¶”ì¶œí•˜ì—¬ ê²¬ê³ í•œ ì¶”ë¡  ì‚¬ì „ ì§€ì‹ì„ í™•ë¦½í•©ë‹ˆë‹¤. ì…‹ì§¸, ìš°ë¦¬ëŠ” Group Relative Policy Optimizationì„ ì‚¬ìš©í•˜ëŠ” ë‘ ë‹¨ê³„ì˜ Reinforcement Learning from Verifiable Rewards (RLVR) í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬í˜„í•˜ì—¬, ì ì‘í˜• í•˜ë“œ ìƒ˜í”Œ ë§ˆì´ë‹ì„ í†µí•´ ì§€ì†ì ì¸ ì‹¤íŒ¨ ëª¨ë“œë¥¼ ëª©í‘œë¡œ í•˜ë©´ì„œ í•µì‹¬ ì¶”ë¡  ê¸°ìˆ ì„ í†µí•©í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ Fleming-R1ì€ ìƒë‹¹í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„± í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤: 7B ë³€í˜•ì€ í›¨ì”¬ ë” í° ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ë©°, 32B ëª¨ë¸ì€ GPT-4oì™€ ê±°ì˜ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ê°•ë ¥í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ëŒ€ì•ˆë“¤ì„ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„° ì„¤ê³„, ì¶”ë¡  ì§€í–¥ ì´ˆê¸°í™” ë° ê²€ì¦ ê°€ëŠ¥í•œ ê°•í™” í•™ìŠµì´ ë‹¨ìˆœí•œ ì •í™•ë„ ìµœì í™”ë¥¼ ë„˜ì–´ ì„ìƒ ì¶”ë¡ ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Fleming-R1ì„ ê³µê°œí•˜ì—¬ íˆ¬ëª…í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•˜ë©° ê°ì‚¬ ê°€ëŠ¥í•œ ì˜ë£Œ AIì˜ ë°œì „ì„ ì´‰ì§„í•˜ê³ , ê³ ìœ„í—˜ ì„ìƒ í™˜ê²½ì—ì„œì˜ ì•ˆì „í•œ ë°°ì¹˜ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Fleming-R1ì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì„ìƒ ì¶”ë¡ ì„ ëª©í‘œë¡œ í•˜ëŠ” ëª¨ë¸ë¡œ, ì •í™•í•œ ë‹µë³€ê³¼ íˆ¬ëª…í•œ ì¶”ë¡  ê³¼ì •ì„ ë™ì‹œì— ë‹¬ì„±í•˜ê¸° ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì„¸ ê°€ì§€ í˜ì‹ ì„ í†µí•´ ì´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì²«ì§¸, Reasoning-Oriented Data Strategy(RODS)ë¥¼ í†µí•´ ì˜ë£Œ QA ë°ì´í„°ì…‹ê³¼ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ í•©ì„±ì„ ê²°í•©í•˜ì—¬ ëœ ëŒ€í‘œë˜ëŠ” ì§ˆë³‘, ì•½ë¬¼, ë‹¤ì¤‘ ì¶”ë¡  ì²´ì¸ì˜ ë²”ìœ„ë¥¼ í™•ì¥í•©ë‹ˆë‹¤. ë‘˜ì§¸, Chain-of-Thought(CoT) ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì¶”ë¡  ê²½ë¡œë¥¼ êµ¬ì¶•í•˜ê³ , ì…‹ì§¸, Verifiable Rewardsë¥¼ í†µí•œ ê°•í™” í•™ìŠµ(RLVR) í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ í•µì‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•©ë‹ˆë‹¤. Fleming-R1ì€ ë‹¤ì–‘í•œ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ 7B ëª¨ë¸ì€ ë” í° ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ê³ , 32B ëª¨ë¸ì€ GPT-4oì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„° ì„¤ê³„ì™€ ê²€ì¦ ê°€ëŠ¥í•œ ê°•í™” í•™ìŠµì´ ì„ìƒ ì¶”ë¡ ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Fleming-R1ì€ ì˜ë£Œ AIì˜ íˆ¬ëª…í•˜ê³  ì•ˆì „í•œ ë°œì „ì„ ìœ„í•´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Fleming-R1 ëª¨ë¸ì€ ì •í™•í•œ ë‹µë³€ê³¼ íˆ¬ëª…í•œ ì¶”ë¡  ê³¼ì •ì„ í†µí•´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì„ìƒ ì¶”ë¡ ì„ ëª©í‘œë¡œ í•œë‹¤.

- 2. Reasoning-Oriented Data Strategy(RODS)ëŠ” ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ì—¬ ë¶€ì¡±í•œ ì§ˆë³‘, ì•½ë¬¼, ë‹¤ì¤‘ ë‹¨ê³„ ì¶”ë¡ ì„ ë³´ì™„í•œë‹¤.

- 3. Chain-of-Thought(CoT) ê¸°ë²•ì„ í†µí•´ ê³ í’ˆì§ˆì˜ ì¶”ë¡  ê²½ë¡œë¥¼ êµ¬ì¶•í•˜ì—¬ ê°•ë ¥í•œ ì¶”ë¡  ê¸°ë°˜ì„ ë§ˆë ¨í•œë‹¤.

- 4. Verifiable Rewardsë¥¼ í†µí•œ ê°•í™” í•™ìŠµ(RLVR) í”„ë ˆì„ì›Œí¬ëŠ” ì ì‘í˜• ìƒ˜í”Œ ì±„êµ´ì„ í†µí•´ ì§€ì†ì ì¸ ì‹¤íŒ¨ ëª¨ë“œë¥¼ í•´ê²°í•œë‹¤.

- 5. Fleming-R1ì€ ë‹¤ì–‘í•œ ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ 7B ëª¨ë¸ì€ ë” í° ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê³ , 32B ëª¨ë¸ì€ GPT-4oì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

---

*Generated on 2025-09-22 15:09:25*