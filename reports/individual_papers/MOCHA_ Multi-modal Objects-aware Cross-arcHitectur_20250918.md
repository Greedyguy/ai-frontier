
# MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment

**Korean Title:** 모카: 다중 모달 객체 인식 교차 아키텍처 정렬

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Few-shot Regimes|Few-shot Regimes]] [[keywords/broad/Knowledge Distillation|Knowledge Distillation]] [[keywords/broad/Vision-Language|Vision-Language]] [[keywords/specific/Region-level Semantics|Region-level Semantics]] [[keywords/unique/MOCHA|MOCHA]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Region-level Semantics Transfer
**🔬 Broad Technical**: Knowledge Distillation, Vision-Language Fusion
**🔗 Specific Connectable**: Few-shot Learning
**⭐ Unique Technical**: MOCHA

**ArXiv ID**: [2509.14001](https://arxiv.org/abs/2509.14001)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.14001.pdf)


## 🏷️ 추출된 키워드



`Knowledge Distillation` • 

`Vision-Language Fusion` • 

`Few-shot Learning` • 

`MOCHA` • 

`Region-level Semantics Transfer`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14001v1 Announce Type: cross 
Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a knowledge distillation approach that transfers region-level multimodal semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight vision-only object detector student (e.g., YOLO). A translation module maps student features into a joint space, where the training of the student and translator is guided by a dual-objective loss that enforces both local alignment and global relational consistency. Unlike prior approaches focused on dense or global alignment, MOCHA operates at the object level, enabling efficient transfer of semantics without modifying the teacher or requiring textual input at inference. We validate our method across four personalized detection benchmarks under few-shot regimes. Results show consistent gains over baselines, with a +10.1 average score improvement. Despite its compact architecture, MOCHA reaches performance on par with larger multimodal models, proving its suitability for real-world deployment.

## 🔍 Abstract (한글 번역)

arXiv:2509.14001v1 발표 유형: 교차
요약: 본 연구에서는 MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment)라는 지식 증류 방법을 소개합니다. 이 방법은 대규모 시각-언어 교사 (예: LLaVa)로부터 지역 수준의 다중 모달 의미론을 가벼운 시각 전용 객체 탐지기 학생 (예: YOLO)로 전이시키는 방법입니다. 번역 모듈은 학생 특징을 공동 공간으로 매핑하며, 학생 및 번역기의 교육은 지역 정렬과 전역 관계 일관성을 강제하는 이중 목적 손실에 의해 안내됩니다. 이전의 밀집 또는 전역 정렬에 중점을 둔 방법과 달리, MOCHA는 객체 수준에서 작동하여 교사를 수정하지 않고 추론 시 텍스트 입력을 필요로하지 않고 의미론을 효율적으로 전달할 수 있습니다. 우리는 몇 가지 샷 조건 하에서 네 가지 개인화된 탐지 벤치마크를 통해 우리의 방법을 검증합니다. 결과는 기준선 대비 일관된 향상을 보여주며, 평균 점수가 +10.1 증가합니다. 그 조밀한 아키텍처에도 불구하고, MOCHA는 더 큰 다중 모달 모델과 성능이 비슷하게 도달하여 실제 세계 배치에 적합함을 입증합니다.

## 📝 요약

MOCHA(Multi-modal Objects-aware Cross-arcHitecture Alignment)는 대규모 비전-언어 교사로부터 지역 수준의 다중 모달 의미론을 가벼운 비전 전용 객체 탐지기 학생에게 전이하는 지식 증류 접근법을 소개한다. MOCHA는 학생 및 번역기의 교육을 지역 정렬과 전역 관계 일관성을 강제하는 이중 목적 손실에 의해 안내된다. MOCHA는 개체 수준에서 작동하여 밀도 또는 전역 정렬에 초점을 맞춘 이전 접근법과는 달리 교사를 수정하지 않고 추론 시 텍스트 입력을 필요로하지 않고 의미론을 효율적으로 전달한다. MOCHA는 몇 가지 적은 샷 규모에서 네 가지 개인화된 감지 벤치마크를 통해 방법을 검증한다. 결과는 기준선 대비 일관된 이득을 보여주며 평균 점수 향상이 +10.1이다. MOCHA는 조밀한 아키텍처임에도 불구하고 더 큰 다중 모달 모델과 성능이 비슷하여 실제 세계 배치에 적합함을 입증한다.

## 🎯 주요 포인트


- 1. MOCHA는 대규모 비전-언어 교사로부터 지식을 전달하여 가벼운 비전 전용 객체 탐지기를 개선하는 지식 증류 접근 방식이다.

- 2. MOCHA는 객체 수준에서 작동하여 밀집 또는 전역 정렬에 집중하는 이전 방법과는 달리, 세맥스를 효율적으로 전달할 수 있도록 한다.

- 3. MOCHA는 소수의 샷 규모에서 네 가지 맞춤형 탐지 벤치마크에서 일관된 이득을 보여주며 평균 점수가 +10.1 향상된다.

- 4. MOCHA는 조밀한 아키텍처임에도 불구하고, 큰 다중 모달 모델과 성능이 비슷하여 현실 세계 배치에 적합함을 입증한다.


---

*Generated on 2025-09-18 16:24:32*