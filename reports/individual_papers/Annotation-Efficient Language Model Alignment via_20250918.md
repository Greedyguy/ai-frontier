
# Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts

**Korean Title:** 다양하고 대표적인 응답 텍스트를 통해 주석 효율적인 언어 모델 정렬

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Limited Annotation Budget|Limited Annotation Budget]] [[keywords/broad/Language Model Alignment|Language Model Alignment]] [[keywords/broad/Preference Optimization|Preference Optimization]] [[keywords/specific/Annotation-Efficient Preference Optimization|Annotation-Efficient Preference Optimization]] [[keywords/unique/AEPO|AEPO]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Limited Annotation Budget
**🔬 Broad Technical**: Language Model Alignment, Preference Optimization
**🔗 Specific Connectable**: Annotation-Efficient Preference Optimization
**⭐ Unique Technical**: AEPO

**ArXiv ID**: [2405.13541](https://arxiv.org/abs/2405.13541)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2405.13541.pdf)


## 🏷️ 추출된 키워드



`Language Model Alignment` • 

`Preference Optimization` • 

`Annotation-Efficient Preference Optimization` • 

`AEPO` • 

`Limited Annotation Budget`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2405.13541v2 Announce Type: replace-cross 
Abstract: Preference optimization is a standard approach to fine-tuning large language models to align with human preferences. The quantity, diversity, and representativeness of the preference dataset are critical to the effectiveness of preference optimization. However, obtaining a large amount of preference annotations is difficult in many applications. This raises the question of how to use the limited annotation budget to create an effective preference dataset. To this end, we propose Annotation-Efficient Preference Optimization (AEPO). Instead of exhaustively annotating preference over all available response texts, AEPO selects a subset of responses that maximizes diversity and representativeness from the available responses and then annotates preference over the selected ones. In this way, AEPO focuses the annotation budget on labeling preferences over a smaller but informative subset of responses. We evaluate the performance of preference learning using AEPO on three datasets and show that it outperforms the baselines with the same annotation budget. Our code is available at https://github.com/CyberAgentAILab/annotation-efficient-po

## 🔍 Abstract (한글 번역)

arXiv:2405.13541v2 발표 유형: replace-cross
초록: 선호도 최적화는 대규모 언어 모델을 인간의 선호와 일치시키기 위한 표준 접근 방식입니다. 선호도 최적화의 효과에 있어서 선호 데이터셋의 양, 다양성 및 대표성은 중요합니다. 그러나 많은 응용 프로그램에서 많은 양의 선호 주석을 얻는 것은 어렵습니다. 이는 제한된 주석 예산을 사용하여 효과적인 선호 데이터셋을 만드는 방법에 대한 문제를 제기합니다. 이를 위해 우리는 주석 효율적인 선호도 최적화 (AEPO)를 제안합니다. AEPO는 모든 사용 가능한 응답 텍스트에 대한 선호를 철저하게 주석 처리하는 대신, 사용 가능한 응답 중에서 다양성과 대표성을 최대화하는 응답의 하위 집합을 선택한 다음 선택된 응답에 대해 선호를 주석 처리합니다. 이러한 방식으로 AEPO는 주석 예산을 더 작지만 정보가 풍부한 응답 하위 집합에 대한 선호도 라벨링에 집중합니다. 우리는 AEPO를 사용하여 선호 학습의 성능을 세 가지 데이터셋에서 평가하고, 동일한 주석 예산으로 베이스라인을 능가하는 것을 보여줍니다. 우리의 코드는 https://github.com/CyberAgentAILab/annotation-efficient-po에서 사용할 수 있습니다.

## 📝 요약

한국어 요약:
이 논문은 대형 언어 모델을 인간의 선호에 맞게 세밀하게 조정하는 선호 최적화에 대해 다룬다. 선호 데이터셋의 양, 다양성, 대표성은 선호 최적화의 효과에 중요하다. 그러나 많은 응용 프로그램에서 선호 주석을 얻는 것은 어렵다. 이에 본 논문에서는 한정된 주석 예산을 활용하여 효과적인 선호 데이터셋을 만드는 Annotation-Efficient Preference Optimization (AEPO)를 제안한다. AEPO는 가능한 응답 텍스트 중 다양성과 대표성을 최대화하는 하위 집합을 선택하고 해당 응답에 대한 선호를 주석으로 달아 선호 학습의 성능을 향상시킨다. AEPO를 통해 세 가지 데이터셋에서 선호 학습 성능을 평가하고 동일한 주석 예산으로 기준선을 능가하는 것을 보여준다.

## 🎯 주요 포인트


- 1. 언어 모델을 인간의 선호도에 맞추기 위한 선호도 최적화의 효과를 높이기 위해 한정된 주석 예산을 효율적으로 활용하는 AEPO 방법을 제안하였다.

- 2. AEPO는 다양성과 대표성을 최대화하는 응답 텍스트 하위 집합을 선택하고 해당 응답에 대한 선호도를 주석으로 달아 선호도 최적화의 성능을 향상시킨다.

- 3. AEPO를 사용하여 선호 학습의 성능을 세 가지 데이터셋에서 평가한 결과, 동일한 주석 예산을 가진 베이스라인을 능가함을 보여주었다.


---

*Generated on 2025-09-18 16:28:21*