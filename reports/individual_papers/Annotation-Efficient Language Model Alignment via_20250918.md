
# Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts

**Korean Title:** ë‹¤ì–‘í•˜ê³  ëŒ€í‘œì ì¸ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ í†µí•´ ì£¼ì„ íš¨ìœ¨ì ì¸ ì–¸ì–´ ëª¨ë¸ ì •ë ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Limited Annotation Budget|Limited Annotation Budget]] [[keywords/broad/Language Model Alignment|Language Model Alignment]] [[keywords/broad/Preference Optimization|Preference Optimization]] [[keywords/specific/Annotation-Efficient Preference Optimization|Annotation-Efficient Preference Optimization]] [[keywords/unique/AEPO|AEPO]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Limited Annotation Budget
**ğŸ”¬ Broad Technical**: Language Model Alignment, Preference Optimization
**ğŸ”— Specific Connectable**: Annotation-Efficient Preference Optimization
**â­ Unique Technical**: AEPO

**ArXiv ID**: [2405.13541](https://arxiv.org/abs/2405.13541)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2405.13541.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Language Model Alignment` â€¢ 

`Preference Optimization` â€¢ 

`Annotation-Efficient Preference Optimization` â€¢ 

`AEPO` â€¢ 

`Limited Annotation Budget`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2405.13541v2 Announce Type: replace-cross 
Abstract: Preference optimization is a standard approach to fine-tuning large language models to align with human preferences. The quantity, diversity, and representativeness of the preference dataset are critical to the effectiveness of preference optimization. However, obtaining a large amount of preference annotations is difficult in many applications. This raises the question of how to use the limited annotation budget to create an effective preference dataset. To this end, we propose Annotation-Efficient Preference Optimization (AEPO). Instead of exhaustively annotating preference over all available response texts, AEPO selects a subset of responses that maximizes diversity and representativeness from the available responses and then annotates preference over the selected ones. In this way, AEPO focuses the annotation budget on labeling preferences over a smaller but informative subset of responses. We evaluate the performance of preference learning using AEPO on three datasets and show that it outperforms the baselines with the same annotation budget. Our code is available at https://github.com/CyberAgentAILab/annotation-efficient-po

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2405.13541v2 ë°œí‘œ ìœ í˜•: replace-cross
ì´ˆë¡: ì„ í˜¸ë„ ìµœì í™”ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì¸ê°„ì˜ ì„ í˜¸ì™€ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•œ í‘œì¤€ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ì„ í˜¸ë„ ìµœì í™”ì˜ íš¨ê³¼ì— ìˆì–´ì„œ ì„ í˜¸ ë°ì´í„°ì…‹ì˜ ì–‘, ë‹¤ì–‘ì„± ë° ëŒ€í‘œì„±ì€ ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ë§ì€ ì–‘ì˜ ì„ í˜¸ ì£¼ì„ì„ ì–»ëŠ” ê²ƒì€ ì–´ë µìŠµë‹ˆë‹¤. ì´ëŠ” ì œí•œëœ ì£¼ì„ ì˜ˆì‚°ì„ ì‚¬ìš©í•˜ì—¬ íš¨ê³¼ì ì¸ ì„ í˜¸ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ ë¬¸ì œë¥¼ ì œê¸°í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ì£¼ì„ íš¨ìœ¨ì ì¸ ì„ í˜¸ë„ ìµœì í™” (AEPO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. AEPOëŠ” ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ë‹µ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ í˜¸ë¥¼ ì² ì €í•˜ê²Œ ì£¼ì„ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹ , ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ë‹µ ì¤‘ì—ì„œ ë‹¤ì–‘ì„±ê³¼ ëŒ€í‘œì„±ì„ ìµœëŒ€í™”í•˜ëŠ” ì‘ë‹µì˜ í•˜ìœ„ ì§‘í•©ì„ ì„ íƒí•œ ë‹¤ìŒ ì„ íƒëœ ì‘ë‹µì— ëŒ€í•´ ì„ í˜¸ë¥¼ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ AEPOëŠ” ì£¼ì„ ì˜ˆì‚°ì„ ë” ì‘ì§€ë§Œ ì •ë³´ê°€ í’ë¶€í•œ ì‘ë‹µ í•˜ìœ„ ì§‘í•©ì— ëŒ€í•œ ì„ í˜¸ë„ ë¼ë²¨ë§ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” AEPOë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜¸ í•™ìŠµì˜ ì„±ëŠ¥ì„ ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•˜ê³ , ë™ì¼í•œ ì£¼ì„ ì˜ˆì‚°ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ì„ ëŠ¥ê°€í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/CyberAgentAILab/annotation-efficient-poì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

í•œêµ­ì–´ ìš”ì•½:
ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ê²Œ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•˜ëŠ” ì„ í˜¸ ìµœì í™”ì— ëŒ€í•´ ë‹¤ë£¬ë‹¤. ì„ í˜¸ ë°ì´í„°ì…‹ì˜ ì–‘, ë‹¤ì–‘ì„±, ëŒ€í‘œì„±ì€ ì„ í˜¸ ìµœì í™”ì˜ íš¨ê³¼ì— ì¤‘ìš”í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì„ í˜¸ ì£¼ì„ì„ ì–»ëŠ” ê²ƒì€ ì–´ë µë‹¤. ì´ì— ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í•œì •ëœ ì£¼ì„ ì˜ˆì‚°ì„ í™œìš©í•˜ì—¬ íš¨ê³¼ì ì¸ ì„ í˜¸ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” Annotation-Efficient Preference Optimization (AEPO)ë¥¼ ì œì•ˆí•œë‹¤. AEPOëŠ” ê°€ëŠ¥í•œ ì‘ë‹µ í…ìŠ¤íŠ¸ ì¤‘ ë‹¤ì–‘ì„±ê³¼ ëŒ€í‘œì„±ì„ ìµœëŒ€í™”í•˜ëŠ” í•˜ìœ„ ì§‘í•©ì„ ì„ íƒí•˜ê³  í•´ë‹¹ ì‘ë‹µì— ëŒ€í•œ ì„ í˜¸ë¥¼ ì£¼ì„ìœ¼ë¡œ ë‹¬ì•„ ì„ í˜¸ í•™ìŠµì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. AEPOë¥¼ í†µí•´ ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œ ì„ í˜¸ í•™ìŠµ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ë™ì¼í•œ ì£¼ì„ ì˜ˆì‚°ìœ¼ë¡œ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ì–¸ì–´ ëª¨ë¸ì„ ì¸ê°„ì˜ ì„ í˜¸ë„ì— ë§ì¶”ê¸° ìœ„í•œ ì„ í˜¸ë„ ìµœì í™”ì˜ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•´ í•œì •ëœ ì£¼ì„ ì˜ˆì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” AEPO ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤.

- 2. AEPOëŠ” ë‹¤ì–‘ì„±ê³¼ ëŒ€í‘œì„±ì„ ìµœëŒ€í™”í•˜ëŠ” ì‘ë‹µ í…ìŠ¤íŠ¸ í•˜ìœ„ ì§‘í•©ì„ ì„ íƒí•˜ê³  í•´ë‹¹ ì‘ë‹µì— ëŒ€í•œ ì„ í˜¸ë„ë¥¼ ì£¼ì„ìœ¼ë¡œ ë‹¬ì•„ ì„ í˜¸ë„ ìµœì í™”ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.

- 3. AEPOë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜¸ í•™ìŠµì˜ ì„±ëŠ¥ì„ ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•œ ê²°ê³¼, ë™ì¼í•œ ì£¼ì„ ì˜ˆì‚°ì„ ê°€ì§„ ë² ì´ìŠ¤ë¼ì¸ì„ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.


---

*Generated on 2025-09-18 16:28:21*