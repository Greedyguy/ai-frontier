# Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models

**Korean Title:** ë‹¤ì¤‘ ëª¨ë“œ ì–¸ì–´ ëª¨ë¸ì˜ ë ˆë“œ íŒ€ ì‘ì—…: í”„ë¡¬í”„íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì™€ ëª¨ë¸ ì „ë°˜ì— ê±¸ì¹œ í•´ì•… í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Multimodal Safety Benchmarks|Multimodal Safety Benchmarks]] [[keywords/specific/Adversarial Prompts|Adversarial Prompts]] [[keywords/broad/Multimodal Large Language Models|Multimodal Large Language Models]] [[keywords/unique/Claude Sonnet 3.5|Claude Sonnet 3.5]] [[categories/cs.CL|cs.CL]] [[2025-09-18/Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection_20250918|Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection]] (86.1% similar) [[2025-09-22/Toxicity Red-Teaming_ Benchmarking LLM Safety in Singapore's Low-Resource Languages_20250922|Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages]] (84.5% similar) [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multimodal Safety Benchmarks
**ğŸ”— Specific Connectable**: Adversarial Prompts
**ğŸ”¬ Broad Technical**: Multimodal Large Language Models
**â­ Unique Technical**: Pixtral 12B
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection_20250918|Is GPT-4o mini Blinded by its Own Safety Filters Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection]] (86.1% similar)
- [[2025-09-22/Toxicity Red-Teaming_ Benchmarking LLM Safety in Singapore's Low-Resource Languages_20250922|Toxicity Red-Teaming Benchmarking LLM Safety in Singapore's Low-Resource Languages]] (84.5% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (83.9% similar)
- [[2025-09-18/MUSE_ MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models_20250918|MUSE MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models]] (83.7% similar)
- [[2025-09-22/Exploring the Impact of Personality Traits on LLM Bias and Toxicity_20250922|Exploring the Impact of Personality Traits on LLM Bias and Toxicity]] (83.5% similar)


**ArXiv ID**: [2509.15478](https://arxiv.org/abs/2509.15478)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15478.pdf)


**ArXiv ID**: [2509.15478](https://arxiv.org/abs/2509.15478)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15478.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multimodal Safety
**ğŸ”— Specific Connectable**: Safety Benchmarks
**â­ Unique Technical**: Claude Sonnet 3.5
**ğŸ”¬ Broad Technical**: Multimodal Large Language Models, Adversarial Conditions

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Multimodal Large Language Models` â€¢ 

`Adversarial Prompts` â€¢ 

`Claude Sonnet 3.5` â€¢ 

`Multimodal Safety Benchmarks`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15478v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly used in real world applications, yet their safety under adversarial conditions remains underexplored. This study evaluates the harmlessness of four leading MLLMs (GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to adversarial prompts across text-only and multimodal formats. A team of 26 red teamers generated 726 prompts targeting three harm categories: illegal activity, disinformation, and unethical behaviour. These prompts were submitted to each model, and 17 annotators rated 2,904 model outputs for harmfulness using a 5-point scale. Results show significant differences in vulnerability across models and modalities. Pixtral 12B exhibited the highest rate of harmful responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%). Contrary to expectations, text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones. Statistical analysis confirmed that both model type and input modality were significant predictors of harmfulness. These findings underscore the urgent need for robust, multimodal safety benchmarks as MLLMs are deployed more widely.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15478v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ì‹¤ì œ ì‘ìš©ì—ì„œ ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì ëŒ€ì  ì¡°ê±´ì—ì„œì˜ ì•ˆì „ì„±ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” í…ìŠ¤íŠ¸ ì „ìš© ë° ë‹¤ì¤‘ ëª¨ë‹¬ í˜•ì‹ì˜ ì ëŒ€ì  í”„ë¡¬í”„íŠ¸ì— ë…¸ì¶œë  ë•Œ ë„¤ ê°€ì§€ ì£¼ìš” MLLM(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, Qwen VL Plus)ì˜ ë¬´í•´ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. 26ëª…ì˜ ë ˆë“œ íŒ€ì›ì´ ë¶ˆë²• í™œë™, í—ˆìœ„ ì •ë³´, ë¹„ìœ¤ë¦¬ì  í–‰ë™ì˜ ì„¸ ê°€ì§€ í•´ì•… ë²”ì£¼ë¥¼ ëª©í‘œë¡œ 726ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í”„ë¡¬í”„íŠ¸ëŠ” ê° ëª¨ë¸ì— ì œì¶œë˜ì—ˆê³ , 17ëª…ì˜ ì£¼ì„ìê°€ 5ì  ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ 2,904ê°œì˜ ëª¨ë¸ ì¶œë ¥ì„ ìœ í•´ì„±ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ëª¨ë¸ê³¼ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ì·¨ì•½ì„±ì— ìˆì–´ ìœ ì˜ë¯¸í•œ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. Pixtral 12BëŠ” ê°€ì¥ ë†’ì€ ë¹„ìœ¨ì˜ ìœ í•´í•œ ì‘ë‹µ(~62%)ì„ ë³´ì˜€ê³ , Claude Sonnet 3.5ëŠ” ê°€ì¥ ì €í•­ë ¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤(~10%). ì˜ˆìƒê³¼ ë‹¬ë¦¬, í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ ë‹¤ì¤‘ ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ëŠ” ë° ì•½ê°„ ë” íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤. í†µê³„ ë¶„ì„ì€ ëª¨ë¸ ìœ í˜•ê³¼ ì…ë ¥ ëª¨ë‹¬ë¦¬í‹°ê°€ ìœ í•´ì„±ì˜ ì¤‘ìš”í•œ ì˜ˆì¸¡ ë³€ìˆ˜ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ MLLMì´ ë” ë„ë¦¬ ë°°í¬ë¨ì— ë”°ë¼ ê°•ë ¥í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì•ˆì „ ë²¤ì¹˜ë§ˆí¬ì˜ ê¸´ê¸‰í•œ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ì•ˆì „ì„±ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. GPT-4o, Claude Sonnet 3.5, Pixtral 12B, Qwen VL Plus ë“± 4ê°œì˜ ì£¼ìš” MLLMì„ ëŒ€ìƒìœ¼ë¡œ, ë¶ˆë²• í™œë™, í—ˆìœ„ ì •ë³´, ë¹„ìœ¤ë¦¬ì  í–‰ë™ì„ ìœ ë„í•˜ëŠ” 726ê°œì˜ ì ëŒ€ì  í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë¬´í•´ì„±ì„ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ Pixtral 12BëŠ” ê°€ì¥ ë†’ì€ ìœ í•´ ë°˜ì‘ë¥ (ì•½ 62%)ì„ ë³´ì˜€ê³ , Claude Sonnet 3.5ëŠ” ê°€ì¥ ì €í•­ë ¥ì´ ë†’ì•˜ìŠµë‹ˆë‹¤(ì•½ 10%). í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ ë‹¤ì¤‘ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” MLLMì˜ ì•ˆì „ì„± ê¸°ì¤€ ë§ˆë ¨ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ë³¸ ì—°êµ¬ëŠ” ë„¤ ê°€ì§€ ì£¼ìš” ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì˜ ì•ˆì „ì„±ì„ í‰ê°€í•˜ë©°, íŠ¹íˆ ì ëŒ€ì  ì¡°ê±´ì—ì„œì˜ ë¬´í•´ì„±ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.

- 2. 26ëª…ì˜ ë ˆë“œ íŒ€ì›ì´ ìƒì„±í•œ 726ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ë¶ˆë²• í™œë™, í—ˆìœ„ ì •ë³´, ë¹„ìœ¤ë¦¬ì  í–‰ë™ì˜ ì„¸ ê°€ì§€ í•´ì•… ë²”ì£¼ë¥¼ ëŒ€ìƒìœ¼ë¡œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.

- 3. Pixtral 12B ëª¨ë¸ì€ ì•½ 62%ì˜ ë†’ì€ í•´ë¡œìš´ ì‘ë‹µë¥ ì„ ë³´ì˜€ìœ¼ë©°, Claude Sonnet 3.5 ëª¨ë¸ì€ ì•½ 10%ë¡œ ê°€ì¥ ì €í•­ë ¥ì´ ë†’ì•˜ìŠµë‹ˆë‹¤.

- 4. í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ ë‹¤ì¤‘ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ìš°íšŒí•˜ëŠ” ë° ë” íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤.

- 5. ì—°êµ¬ ê²°ê³¼ëŠ” MLLMsì˜ ë„ë¦¬ ë³´ê¸‰ë¨ì— ë”°ë¼ ê°•ë ¥í•œ ë‹¤ì¤‘ëª¨ë‹¬ ì•ˆì „ ê¸°ì¤€ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:21:54*