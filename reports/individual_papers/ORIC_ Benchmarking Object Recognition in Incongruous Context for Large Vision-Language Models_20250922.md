# ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models

**Korean Title:** ORIC: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ë¶€ì¡°í™” ë§¥ë½ì—ì„œì˜ ê°ì²´ ì¸ì‹ ë²¤ì¹˜ë§ˆí‚¹

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Contextual Incongruity

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (88.7% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1 Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (82.5% similar)
- [[2025-09-18/The Art of Saying Maybe_ A Conformal Lens for Uncertainty Benchmarking in VLMs_20250918|The Art of Saying Maybe A Conformal Lens for Uncertainty Benchmarking in VLMs]] (81.5% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (81.1% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (80.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15695v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image caption, visual question answering, and robotics by integrating visual and textual information. However, they remain prone to errors in incongruous contexts, where objects appear unexpectedly or are absent when contextually expected. This leads to two key recognition failures: object misidentification and hallucination. To systematically examine this issue, we introduce the Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark that evaluates LVLMs in scenarios where object-context relationships deviate from expectations. ORIC employs two key strategies: (1) LLM-guided sampling, which identifies objects that are present but contextually incongruous, and (2) CLIP-guided sampling, which detects plausible yet nonexistent objects that are likely to be hallucinated, thereby creating an incongruous context. Evaluating 18 LVLMs and two open-vocabulary detection models, our results reveal significant recognition gaps, underscoring the challenges posed by contextual incongruity. This work provides critical insights into LVLMs' limitations and encourages further research on context-aware object recognition.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15695v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì´ë¯¸ì§€ ìº¡ì…˜, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ ë° ë¡œë´‡ ê³µí•™ ë¶„ì•¼ì—ì„œ ìƒë‹¹í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ëª¨ë¸ì€ ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì—ì„œ ê°ì²´ê°€ ë‚˜íƒ€ë‚˜ê±°ë‚˜ ë§¥ë½ìƒ ì˜ˆìƒë˜ëŠ” ê°ì²´ê°€ ì—†ëŠ” ê²½ìš° ì˜¤ë¥˜ë¥¼ ë²”í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì´ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì¸ì‹ ì‹¤íŒ¨ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤: ê°ì²´ ì˜¤ì¸ì‹ê³¼ í™˜ê°. ì´ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê°ì²´-ë§¥ë½ ê´€ê³„ê°€ ê¸°ëŒ€ì—ì„œ ë²—ì–´ë‚˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ LVLMsë¥¼ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ ë¶ˆì¼ì¹˜ ë§¥ë½ì—ì„œì˜ ê°ì²´ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬(ORIC)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ORICëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤: (1) LLM-ìœ ë„ ìƒ˜í”Œë§, ì´ëŠ” ì¡´ì¬í•˜ì§€ë§Œ ë§¥ë½ìƒ ë¶ˆì¼ì¹˜í•œ ê°ì²´ë¥¼ ì‹ë³„í•˜ê³ , (2) CLIP-ìœ ë„ ìƒ˜í”Œë§, ì´ëŠ” í™˜ê°ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°ì²´ë¥¼ ê°ì§€í•˜ì—¬ ë¶ˆì¼ì¹˜í•œ ë§¥ë½ì„ ìƒì„±í•©ë‹ˆë‹¤. 18ê°œì˜ LVLMsì™€ ë‘ ê°œì˜ ê°œë°©í˜• ì–´íœ˜ íƒì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë§¥ë½ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ì¸ì‹ ê²©ì°¨ê°€ ìƒë‹¹í•¨ì„ ë°í˜€ë‚´ë©°, ë§¥ë½ì  ë¶ˆì¼ì¹˜ê°€ ì œê¸°í•˜ëŠ” ë„ì „ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LVLMsì˜ í•œê³„ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ë©°, ë§¥ë½ ì¸ì‹ ê°ì²´ ì¸ì‹ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì¥ë ¤í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì´ë¯¸ì§€ ìº¡ì…˜, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ, ë¡œë´‡ ê³µí•™ì—ì„œ í° ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ì˜ˆìƒì¹˜ ëª»í•œ ë§¥ë½ì—ì„œ ê°ì²´ë¥¼ ì˜ëª» ì¸ì‹í•˜ê±°ë‚˜ í™˜ê°í•˜ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê°ì²´-ë§¥ë½ ê´€ê³„ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í‰ê°€í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ ORIC(Object Recognition in Incongruous Context Benchmark)ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ORICëŠ” LLMê³¼ CLIP ê°€ì´ë“œ ìƒ˜í”Œë§ì„ í†µí•´ ë§¥ë½ì ìœ¼ë¡œ ë¶€ì ì ˆí•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°ì²´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤. 18ê°œì˜ LVLMê³¼ ë‘ ê°œì˜ ê°œë°©í˜• ì–´íœ˜ íƒì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë§¥ë½ì  ë¶€ì¡°í™”ë¡œ ì¸í•œ ì¸ì‹ ê²©ì°¨ê°€ í¬ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LVLMì˜ í•œê³„ë¥¼ ë°íˆê³  ë§¥ë½ ì¸ì‹ ê°ì²´ ì¸ì‹ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ë¹„ì „-ì–¸ì–´ ëª¨ë¸(LVLMs)ì€ ì´ë¯¸ì§€ ìº¡ì…˜, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ, ë¡œë´‡ ê³µí•™ì—ì„œ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ë¶€ì ì ˆí•œ ë¬¸ë§¥ì—ì„œ ì˜¤ë¥˜ë¥¼ ë²”í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.

- 2. LVLMsì˜ ì£¼ìš” ì¸ì‹ ì‹¤íŒ¨ëŠ” ê°ì²´ ì˜¤ì¸ì‹ê³¼ í™˜ê° í˜„ìƒìœ¼ë¡œ, ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ê°ì²´-ë¬¸ë§¥ ê´€ê³„ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤.

- 3. ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ ORICëŠ” ê°ì²´-ë¬¸ë§¥ ê´€ê³„ê°€ ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ LVLMsë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.

- 4. ORICëŠ” LLM-ê°€ì´ë“œ ìƒ˜í”Œë§ê³¼ CLIP-ê°€ì´ë“œ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì ìœ¼ë¡œ ë¶€ì ì ˆí•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°ì²´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.

- 5. 18ê°œì˜ LVLMsì™€ ë‘ ê°œì˜ ì˜¤í”ˆ ë³´ìºë·¸ëŸ¬ë¦¬ ê°ì§€ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ë¬¸ë§¥ì  ë¶€ì ì ˆì„±ìœ¼ë¡œ ì¸í•œ ì¸ì‹ ê²©ì°¨ê°€ í¬ê²Œ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-22 15:41:24*