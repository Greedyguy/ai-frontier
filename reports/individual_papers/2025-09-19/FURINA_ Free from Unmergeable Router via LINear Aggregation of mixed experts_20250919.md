
# FURINA: Free from Unmergeable Router via LINear Aggregation of mixed experts

**Korean Title:** FURINA: í˜¼í•© ì „ë¬¸ê°€ì˜ ì„ í˜• ì§‘ê³„ë¥¼ í†µí•œ ë¹„ë³‘í•© ê°€ëŠ¥ ë¼ìš°í„°ë¡œë¶€í„°ì˜ ììœ 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Router Free MoE

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection]] (81.4% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (80.4% similar)
- [[Don't Forget the Nonlinearity Unlocking Activation Functions in Efficient Fine-Tuning]] (78.6% similar)
- [[FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (76.9% similar)
- [[CSMoE An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts]] (76.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14900v1 Announce Type: new 
Abstract: The Mixture of Experts (MoE) paradigm has been successfully integrated into Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning (PEFT), delivering performance gains with minimal parameter overhead. However, a key limitation of existing MoE-LoRA methods is their reliance on a discrete router, which prevents the integration of the MoE components into the backbone model. To overcome this, we propose FURINA, a novel Free from Unmergeable Router framework based on the LINear Aggregation of experts. FURINA eliminates the router by introducing a Self-Routing mechanism. This is achieved through three core innovations: (1) decoupled learning of the direction and magnitude for LoRA adapters, (2) a shared learnable magnitude vector for consistent activation scaling, and (3) expert selection loss that encourages divergent expert activation. The proposed mechanism leverages the angular similarity between the input and each adapter's directional component to activate experts, which are then scaled by the shared magnitude vector. This design allows the output norm to naturally reflect the importance of each expert, thereby enabling dynamic, router-free routing. The expert selection loss further sharpens this behavior by encouraging sparsity and aligning it with standard MoE activation patterns. We also introduce a shared expert within the MoE-LoRA block that provides stable, foundational knowledge. To the best of our knowledge, FURINA is the first router-free, MoE-enhanced LoRA method that can be fully merged into the backbone model, introducing zero additional inference-time cost or complexity. Extensive experiments demonstrate that FURINA not only significantly outperforms standard LoRA but also matches or surpasses the performance of existing MoE-LoRA methods, while eliminating the extra inference-time overhead of MoE.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14900v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: í˜¼í•© ì „ë¬¸ê°€(Mixture of Experts, MoE) íŒ¨ëŸ¬ë‹¤ì„ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì •(PEFT)ì„ ìœ„í•´ Low-Rank Adaptation (LoRA)ì— ì„±ê³µì ìœ¼ë¡œ í†µí•©ë˜ì–´, ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° ì˜¤ë²„í—¤ë“œë¡œ ì„±ëŠ¥ í–¥ìƒì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ MoE-LoRA ë°©ë²•ì˜ ì£¼ìš” í•œê³„ëŠ” ì´ì‚° ë¼ìš°í„°ì— ì˜ì¡´í•˜ì—¬ MoE êµ¬ì„± ìš”ì†Œë¥¼ ë°±ë³¸ ëª¨ë¸ì— í†µí•©í•  ìˆ˜ ì—†ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì „ë¬¸ê°€ì˜ ì„ í˜• ì§‘ê³„(LINear Aggregation)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ ë¹„ë³‘í•© ë¼ìš°í„° í”„ë ˆì„ì›Œí¬ì¸ FURINAë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. FURINAëŠ” ì…€í”„ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ë¼ìš°í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ì´ëŠ” ì„¸ ê°€ì§€ í•µì‹¬ í˜ì‹ ì„ í†µí•´ ë‹¬ì„±ë©ë‹ˆë‹¤: (1) LoRA ì–´ëŒ‘í„°ì˜ ë°©í–¥ê³¼ í¬ê¸°ë¥¼ ë¶„ë¦¬í•˜ì—¬ í•™ìŠµ, (2) ì¼ê´€ëœ í™œì„±í™” ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ ê³µìœ  ê°€ëŠ¥í•œ í•™ìŠµ í¬ê¸° ë²¡í„°, (3) ì „ë¬¸ê°€ í™œì„±í™”ë¥¼ ë‹¤ì–‘í•˜ê²Œ ìœ ë„í•˜ëŠ” ì „ë¬¸ê°€ ì„ íƒ ì†ì‹¤. ì œì•ˆëœ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ê³¼ ê° ì–´ëŒ‘í„°ì˜ ë°©í–¥ì„± êµ¬ì„± ìš”ì†Œ ê°„ì˜ ê°ë„ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ ì „ë¬¸ê°€ë¥¼ í™œì„±í™”í•˜ê³ , ì´ëŠ” ê³µìœ  í¬ê¸° ë²¡í„°ì— ì˜í•´ ìŠ¤ì¼€ì¼ë§ë©ë‹ˆë‹¤. ì´ ì„¤ê³„ëŠ” ì¶œë ¥ ë…¸ë¦„ì´ ê° ì „ë¬¸ê°€ì˜ ì¤‘ìš”ì„±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ë™ì ì´ê³  ë¼ìš°í„° ì—†ëŠ” ë¼ìš°íŒ…ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì „ë¬¸ê°€ ì„ íƒ ì†ì‹¤ì€ í¬ì†Œì„±ì„ ì¥ë ¤í•˜ê³  í‘œì¤€ MoE í™œì„±í™” íŒ¨í„´ê³¼ì˜ ì •ë ¬ì„ í†µí•´ ì´ í–‰ë™ì„ ë”ìš± ë‚ ì¹´ë¡­ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ì•ˆì •ì ì´ê³  ê¸°ì´ˆì ì¸ ì§€ì‹ì„ ì œê³µí•˜ëŠ” MoE-LoRA ë¸”ë¡ ë‚´ì˜ ê³µìœ  ì „ë¬¸ê°€ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, FURINAëŠ” ë°±ë³¸ ëª¨ë¸ì— ì™„ì „íˆ í†µí•©ë  ìˆ˜ ìˆëŠ” ìµœì´ˆì˜ ë¼ìš°í„° ì—†ëŠ” MoE ê°•í™” LoRA ë°©ë²•ìœ¼ë¡œ, ì¶”ê°€ì ì¸ ì¶”ë¡  ì‹œê°„ ë¹„ìš©ì´ë‚˜ ë³µì¡ì„±ì„ ë„ì…í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ FURINAëŠ” í‘œì¤€ LoRAë¥¼ í¬ê²Œ ëŠ¥ê°€í•  ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ì¡´ MoE-LoRA ë°©ë²•ì˜ ì„±ëŠ¥ì„ ë§ì¶”ê±°ë‚˜ ëŠ¥ê°€í•˜ë©´ì„œ MoEì˜ ì¶”ê°€ ì¶”ë¡  ì‹œê°„ ì˜¤ë²„í—¤ë“œë¥¼ ì œê±°í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

FURINAëŠ” ê¸°ì¡´ì˜ MoE-LoRA ë°©ë²•ì˜ ì œí•œ ì‚¬í•­ì¸ ë¹„ì—°ê²° ë¼ìš°í„° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. FURINAëŠ” ë¼ìš°í„°ë¥¼ ì œê±°í•˜ê³  Self-Routing ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬, LoRA ì–´ëŒ‘í„°ì˜ ë°©í–¥ì„±ê³¼ í¬ê¸°ë¥¼ ë¶„ë¦¬í•˜ì—¬ í•™ìŠµí•˜ê³ , ì¼ê´€ëœ í™œì„±í™” ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ ê³µìœ  ê°€ëŠ¥í•œ í¬ê¸° ë²¡í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ, ì „ë¬¸ê°€ ì„ íƒ ì†ì‹¤ì„ í†µí•´ ì „ë¬¸ê°€ í™œì„±í™”ë¥¼ ë‹¤ì–‘í™”í•©ë‹ˆë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ê³¼ ê° ì–´ëŒ‘í„°ì˜ ë°©í–¥ì„± ê°„ì˜ ê°ë„ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ ì „ë¬¸ê°€ë¥¼ í™œì„±í™”í•˜ê³ , ê³µìœ  í¬ê¸° ë²¡í„°ë¡œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë™ì ì´ê³  ë¼ìš°í„° ì—†ëŠ” ë¼ìš°íŒ…ì´ ê°€ëŠ¥í•´ì§€ë©°, MoE-LoRA ë¸”ë¡ ë‚´ì— ê³µìœ  ì „ë¬¸ê°€ë¥¼ ë„ì…í•˜ì—¬ ì•ˆì •ì ì¸ ê¸°ì´ˆ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. FURINAëŠ” ë°±ë³¸ ëª¨ë¸ì— ì™„ì „íˆ í†µí•©ë  ìˆ˜ ìˆëŠ” ìµœì´ˆì˜ ë¼ìš°í„° ì—†ëŠ” MoE ê°•í™” LoRA ë°©ë²•ìœ¼ë¡œ, ì¶”ê°€ì ì¸ ì¶”ë¡  ì‹œê°„ ë¹„ìš©ì´ë‚˜ ë³µì¡ì„±ì„ ì´ˆë˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, FURINAëŠ” í‘œì¤€ LoRAë¥¼ ëŠ¥ê°€í•˜ë©° ê¸°ì¡´ MoE-LoRA ë°©ë²•ì˜ ì„±ëŠ¥ì„ ë§ì¶”ê±°ë‚˜ ì´ˆê³¼í•˜ë©´ì„œë„ ì¶”ê°€ì ì¸ ì¶”ë¡  ì‹œê°„ ì˜¤ë²„í—¤ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. FURINAëŠ” ê¸°ì¡´ MoE-LoRA ë°©ë²•ì˜ ì œí•œ ì‚¬í•­ì¸ ë¹„í•©ë³‘ ê°€ëŠ¥í•œ ë¼ìš°í„° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 2. FURINAëŠ” Self-Routing ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ ë¼ìš°í„°ë¥¼ ì œê±°í•˜ê³ , ì „ë¬¸ê°€ í™œì„±í™”ë¥¼ ìœ„í•œ ê°ë„ ìœ ì‚¬ì„±ì„ í™œìš©í•©ë‹ˆë‹¤.

- 3. ì„¸ ê°€ì§€ í•µì‹¬ í˜ì‹ ì€ LoRA ì–´ëŒ‘í„°ì˜ ë°©í–¥ê³¼ í¬ê¸°ì˜ ë¶„ë¦¬ í•™ìŠµ, ì¼ê´€ëœ í™œì„±í™” ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ ê³µìœ  ê°€ëŠ¥í•œ í¬ê¸° ë²¡í„°, ì „ë¬¸ê°€ í™œì„±í™”ë¥¼ ìœ ë„í•˜ëŠ” ì „ë¬¸ê°€ ì„ íƒ ì†ì‹¤ì…ë‹ˆë‹¤.

- 4. FURINAëŠ” MoE-LoRA ë¸”ë¡ ë‚´ì— ê³µìœ  ì „ë¬¸ê°€ë¥¼ ë„ì…í•˜ì—¬ ì•ˆì •ì ì¸ ê¸°ì´ˆ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼, FURINAëŠ” ê¸°ì¡´ LoRAë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©°, MoE-LoRA ë°©ë²•ì˜ ì¶”ê°€ ì¶”ë¡  ì‹œê°„ ë¹„ìš© ì—†ì´ë„ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:53:29*