
# UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding

**Korean Title:** UMind: ì œë¡œìƒ· M/EEG ì‹œê° ë””ì½”ë”©ì„ ìœ„í•œ í†µí•© ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multitask Visual Decoding

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Reconstruction Alignment Improves Unified Multimodal Models_20250919|Reconstruction Alignment Improves Unified Multimodal Models]] (82.8% similar)
- [[UnifiedVisual A Framework for Constructing Unified Vision-Language Datasets]] (82.1% similar)
- [[Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation]] (81.9% similar)
- [[Personalization on a Budget Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection]] (81.1% similar)
- [[VocSegMRI Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI]] (80.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14772v1 Announce Type: new 
Abstract: Decoding visual information from time-resolved brain recordings, such as EEG and MEG, plays a pivotal role in real-time brain-computer interfaces. However, existing approaches primarily focus on direct brain-image feature alignment and are limited to single-task frameworks or task-specific models. In this paper, we propose a Unified MultItask Network for zero-shot M/EEG visual Decoding (referred to UMind), including visual stimulus retrieval, classification, and reconstruction, where multiple tasks mutually enhance each other. Our method learns robust neural-visual and semantic representations through multimodal alignment with both image and text modalities. The integration of both coarse and fine-grained texts enhances the extraction of these neural representations, enabling more detailed semantic and visual decoding. These representations then serve as dual conditional inputs to a pre-trained diffusion model, guiding visual reconstruction from both visual and semantic perspectives. Extensive evaluations on MEG and EEG datasets demonstrate the effectiveness, robustness, and biological plausibility of our approach in capturing spatiotemporal neural dynamics. Our approach sets a multitask pipeline for brain visual decoding, highlighting the synergy of semantic information in visual feature extraction.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14772v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: EEG ë° MEGì™€ ê°™ì€ ì‹œê°„ ë¶„í•´ëœ ë‡Œ ê¸°ë¡ìœ¼ë¡œë¶€í„° ì‹œê° ì •ë³´ë¥¼ í•´ë…í•˜ëŠ” ê²ƒì€ ì‹¤ì‹œê°„ ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì€ ì£¼ë¡œ ì§ì ‘ì ì¸ ë‡Œ-ì´ë¯¸ì§€ íŠ¹ì§• ì •ë ¬ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, ë‹¨ì¼ ì‘ì—… í”„ë ˆì„ì›Œí¬ë‚˜ ì‘ì—…ë³„ ëª¨ë¸ì— êµ­í•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹œê° ìê·¹ ê²€ìƒ‰, ë¶„ë¥˜ ë° ì¬êµ¬ì„±ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ ì‘ì—…ì´ ì„œë¡œë¥¼ ìƒí˜¸ ë³´ì™„í•˜ëŠ” ì œë¡œìƒ· M/EEG ì‹œê° í•´ë…ì„ ìœ„í•œ í†µí•© ë‹¤ì¤‘ ì‘ì—… ë„¤íŠ¸ì›Œí¬(UMind)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì™€ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ì •ë ¬ì„ í†µí•´ ê°•ë ¥í•œ ì‹ ê²½-ì‹œê° ë° ì˜ë¯¸ í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê±°ì¹œ í…ìŠ¤íŠ¸ì™€ ì„¸ë°€í•œ í…ìŠ¤íŠ¸ì˜ í†µí•©ì€ ì´ëŸ¬í•œ ì‹ ê²½ í‘œí˜„ì˜ ì¶”ì¶œì„ ê°•í™”í•˜ì—¬ ë³´ë‹¤ ì„¸ë¶€ì ì¸ ì˜ë¯¸ ë° ì‹œê° í•´ë…ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í‘œí˜„ì€ ì‚¬ì „ í›ˆë ¨ëœ í™•ì‚° ëª¨ë¸ì— ëŒ€í•œ ì´ì¤‘ ì¡°ê±´ ì…ë ¥ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ì‹œê° ë° ì˜ë¯¸ì  ê´€ì ì—ì„œ ì‹œê° ì¬êµ¬ì„±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. MEG ë° EEG ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ í‰ê°€ë¥¼ í†µí•´ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì´ ì‹œê³µê°„ ì‹ ê²½ ì—­í•™ì„ í¬ì°©í•˜ëŠ” ë° ìˆì–´ íš¨ê³¼ì ì´ê³ , ê²¬ê³ í•˜ë©°, ìƒë¬¼í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ë‡Œ ì‹œê° í•´ë…ì„ ìœ„í•œ ë‹¤ì¤‘ ì‘ì—… íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ì—¬ ì‹œê° íŠ¹ì§• ì¶”ì¶œì—ì„œ ì˜ë¯¸ ì •ë³´ì˜ ì‹œë„ˆì§€ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì‹¤ì‹œê°„ ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” EEGì™€ MEGì™€ ê°™ì€ ì‹œê°„ ë¶„í•´ ë‡Œ ê¸°ë¡ì—ì„œ ì‹œê° ì •ë³´ë¥¼ í•´ë…í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ë‹¨ì¼ ì‘ì—…ì— êµ­í•œëœ ë°˜ë©´, ë³¸ ì—°êµ¬ëŠ” ì‹œê° ìê·¹ ê²€ìƒ‰, ë¶„ë¥˜, ì¬êµ¬ì„±ì„ í¬í•¨í•˜ëŠ” ë‹¤ì¤‘ ì‘ì—… ë„¤íŠ¸ì›Œí¬ì¸ UMindë¥¼ ì œì•ˆí•˜ì—¬ ê° ì‘ì—…ì´ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‘ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì •ë ¬ì„ í†µí•´ ê°•ë ¥í•œ ì‹ ê²½-ì‹œê° ë° ì˜ë¯¸ í‘œí˜„ì„ í•™ìŠµí•˜ë©°, ì´ë¥¼ í†µí•´ ë” ìƒì„¸í•œ ì˜ë¯¸ ë° ì‹œê° í•´ë…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í‘œí˜„ì€ ì‚¬ì „ í•™ìŠµëœ í™•ì‚° ëª¨ë¸ì˜ ì¡°ê±´ë¶€ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ ì‹œê° ë° ì˜ë¯¸ì  ê´€ì ì—ì„œ ì‹œê° ì¬êµ¬ì„±ì„ ìœ ë„í•©ë‹ˆë‹¤. MEGì™€ EEG ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ í‰ê°€ë¥¼ í†µí•´ ë³¸ ì ‘ê·¼ë²•ì˜ íš¨ê³¼ì„±, ê°•ê±´ì„± ë° ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±ì„ ì…ì¦í•˜ì˜€ìœ¼ë©°, ì‹œê° íŠ¹ì§• ì¶”ì¶œì—ì„œ ì˜ë¯¸ ì •ë³´ì˜ ì‹œë„ˆì§€ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. UMindëŠ” ì‹œê°ì  ìê·¹ ê²€ìƒ‰, ë¶„ë¥˜ ë° ì¬êµ¬ì„±ì„ í¬í•¨í•˜ëŠ” ë‹¤ì¤‘ ì‘ì—… ë„¤íŠ¸ì›Œí¬ë¡œ, ì—¬ëŸ¬ ì‘ì—…ì´ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‘ìš©í•©ë‹ˆë‹¤.

- 2. ì´ ë°©ë²•ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì •ë ¬ì„ í†µí•´ ê°•ë ¥í•œ ì‹ ê²½-ì‹œê° ë° ì˜ë¯¸ í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.

- 3. ì„¸ë°€í•œ í…ìŠ¤íŠ¸ í†µí•©ì€ ì‹ ê²½ í‘œí˜„ì˜ ì¶”ì¶œì„ ê°•í™”í•˜ì—¬ ë” ìƒì„¸í•œ ì˜ë¯¸ ë° ì‹œê°ì  ë””ì½”ë”©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

- 4. ì‚¬ì „ í›ˆë ¨ëœ í™•ì‚° ëª¨ë¸ì— ì´ì¤‘ ì¡°ê±´ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ ì‹œê° ë° ì˜ë¯¸ ê´€ì ì—ì„œ ì‹œê°ì  ì¬êµ¬ì„±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

- 5. MEG ë° EEG ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ í‰ê°€ë¥¼ í†µí•´ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì´ ì‹œê³µê°„ ì‹ ê²½ ì—­í•™ì„ í¬ì°©í•˜ëŠ” ë° íš¨ê³¼ì ì´ê³  ê°•ë ¥í•˜ë©° ìƒë¬¼í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-19 16:47:54*