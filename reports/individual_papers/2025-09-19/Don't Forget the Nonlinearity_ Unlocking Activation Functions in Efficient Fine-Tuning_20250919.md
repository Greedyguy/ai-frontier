---
keywords:
  - Activation Functions
  - Transformer Architecture
  - Vision Transformers
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.13240
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:47:21.902846",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Activation Functions",
    "Transformer Architecture",
    "Vision Transformers"
  ],
  "rejected_keywords": [
    "Parameter-Efficient Fine-Tuning",
    "Low-Rank Adaptation"
  ],
  "similarity_scores": {
    "Activation Functions": 0.85,
    "Transformer Architecture": 0.82,
    "Vision Transformers": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning

**Korean Title:** ë¹„ì„ í˜•ì„±ì„ ìŠì§€ ë§ˆì„¸ìš”: íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì •ì—ì„œ í™œì„±í™” í•¨ìˆ˜ì˜ ì ì¬ë ¥ ë°œíœ˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Transformer Architecture|Transformer Architecture]], [[keywords/Vision Transformers|Vision Transformers]]
**âš¡ Unique Technical**: [[keywords/Activation Functions|activation functions]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[HAM Hierarchical Adapter Merging for Scalable Continual Learning]] (80.9% similar)
- [[NIRVANA Structured pruning reimagined for large language models compression]] (78.1% similar)
- [[Controllable Pareto Trade-off between Fairness and Accuracy]] (77.4% similar)
- [[Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (77.3% similar)
- [[FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (77.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13240v2 Announce Type: replace 
Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving accuracy gains of +0.17\% and +0.27\%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13240v2 ë°œí‘œ ìœ í˜•: êµì²´

ì´ˆë¡: ê¸°ì¡´ì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì€ ì£¼ë¡œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì¡°ì •í•˜ë©´ì„œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê³ ì •ëœ ìƒíƒœë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì§ì ‘ ì¡°ì •í•˜ëŠ” ìµœì´ˆì˜ PEFT í”„ë ˆì„ì›Œí¬ì¸ \textbf{NoRA}ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. NoRAëŠ” ê³ ì •ëœ í™œì„±í™” í•¨ìˆ˜ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ ìœ ë¦¬ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ê³ , ë¶„ìì™€ ë¶„ëª¨ ê³„ìˆ˜ì— êµ¬ì¡°í™”ëœ ì €ë­í¬ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•˜ë©°, ê·¸ë£¹ë³„ ì„¤ê³„ë¥¼ í†µí•´ ì ì‘ì„ êµ­ì§€í™”í•˜ê³  ìµœì†Œí•œì˜ ë¹„ìš©ìœ¼ë¡œ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. CIFAR-10ê³¼ CIFAR-100ì—ì„œ í›ˆë ¨ëœ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ NoRAëŠ” ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ì´ë¥¼ ì´ˆê³¼í•˜ë©°, ë‹¨ 0.4\%ì˜ íŒŒë¼ë¯¸í„°(0.02M)ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì •í™•ë„ í–¥ìƒì„ ê°ê° +0.17\%ì™€ +0.27\% ë‹¬ì„±í•©ë‹ˆë‹¤. LoRAì™€ ê²°í•©ëœ ê²½ìš°(\textbf{NoRA++}), NoRA++ëŠ” ë” ì ì€ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ë™ì¼í•œ í›ˆë ¨ ì˜ˆì‚° í•˜ì—ì„œ LoRAì™€ DoRAë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤. LLaMA3-8B ì§€ì‹œ ì¡°ì •ì—ì„œ NoRA++ëŠ” ìƒì„± í’ˆì§ˆì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ë©°, í‰ê·  MMLU í–¥ìƒì„ +0.3\%--0.8\% ë‹¬ì„±í•˜ê³ , STEM(Alpaca)ì—ì„œ +1.6\% ë° OpenOrcaì—ì„œ +1.3\%ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ NoRAê°€ ì €ì°¨ì› ê¸°ëŠ¥ì  ë¶€ë¶„ ê³µê°„ì— ì ì‘ì„ ì œí•œí•˜ì—¬ ì—…ë°ì´íŠ¸ í¬ê¸°ì™€ ë°©í–¥ì„ ì•”ë¬µì ìœ¼ë¡œ ì •ê·œí™”í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” í™œì„±í™” ê³µê°„ íŠœë‹ì´ ê°€ì¤‘ì¹˜ ê¸°ë°˜ PEFTì— ëŒ€í•œ ë³´ì™„ì ì´ê³  ë§¤ìš° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ëŒ€ì•ˆìœ¼ë¡œì„œ, ëª¨ë¸ ì ì‘ì„ ìœ„í•œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì¼ê¸‰ ê°ì²´ë¡œ ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ê¸°ì¡´ì˜ ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì€ ì£¼ë¡œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì¡°ì •í•˜ì§€ë§Œ, í™œì„±í™” í•¨ìˆ˜ëŠ” ê³ ì •ëœ ìƒíƒœë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì§ì ‘ ì¡°ì •í•˜ëŠ” ìµœì´ˆì˜ PEFT í”„ë ˆì„ì›Œí¬ì¸ NoRAë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. NoRAëŠ” ê³ ì •ëœ í™œì„±í™” í•¨ìˆ˜ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ ìœ ë¦¬ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ê³ , ë¶„ì ë° ë¶„ëª¨ ê³„ìˆ˜ì— êµ¬ì¡°í™”ëœ ì €ë­í¬ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤. CIFAR-10 ë° CIFAR-100ì—ì„œ NoRAëŠ” ì „ì²´ ë¯¸ì„¸ ì¡°ì •ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë§¤ê°œë³€ìˆ˜ì˜ 0.4%ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì •í™•ë„ë¥¼ ê°ê° +0.17% ë° +0.27% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. NoRAì™€ LoRAë¥¼ ê²°í•©í•œ NoRA++ëŠ” ë™ì¼í•œ í•™ìŠµ ì˜ˆì‚° í•˜ì—ì„œ ë” ì ì€ ë§¤ê°œë³€ìˆ˜ë¡œ LoRAì™€ DoRAë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤. LLaMA3-8B ì§€ì¹¨ ì¡°ì •ì—ì„œëŠ” NoRA++ê°€ ì¼ê´€ë˜ê²Œ ìƒì„± í’ˆì§ˆì„ ê°œì„ í•˜ì—¬ í‰ê·  MMLUë¥¼ +0.3%ì—ì„œ +0.8%ê¹Œì§€ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” í™œì„±í™” í•¨ìˆ˜ ì¡°ì •ì´ ê°€ì¤‘ì¹˜ ê¸°ë°˜ PEFTì˜ ë³´ì™„ì  ëŒ€ì•ˆì„ì„ ì…ì¦í•˜ë©°, ëª¨ë¸ ì ì‘ì„ ìœ„í•œ ì¤‘ìš”í•œ ìš”ì†Œë¡œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. NoRAëŠ” ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì§ì ‘ ì¡°ì •í•˜ëŠ” ìµœì´ˆì˜ PEFT í”„ë ˆì„ì›Œí¬ë¡œ, ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ ìœ ë¦¬ í•¨ìˆ˜ë¡œ ê³ ì •ëœ í™œì„±í™”ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.

- 2. NoRAëŠ” CIFAR-10 ë° CIFAR-100ì—ì„œ í›ˆë ¨ëœ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì „ì²´ íŒŒì¸ íŠœë‹ê³¼ ë™ë“±í•˜ê±°ë‚˜ ê·¸ ì´ìƒì˜ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¨ 0.4%ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì •í™•ë„ í–¥ìƒì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 3. NoRA++ëŠ” LoRAì™€ ê²°í•©í•˜ì—¬, ë™ì¼í•œ í›ˆë ¨ ì˜ˆì‚° í•˜ì—ì„œ ë” ì ì€ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ì—¬ LoRA ë° DoRAë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤.

- 4. NoRAëŠ” ì €ì°¨ì› í•¨ìˆ˜ì  ë¶€ë¶„ ê³µê°„ìœ¼ë¡œì˜ ì ì‘ì„ ì œí•œí•˜ì—¬, ì—…ë°ì´íŠ¸ì˜ í¬ê¸°ì™€ ë°©í–¥ì„ ì•”ë¬µì ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.

- 5. í™œì„±í™” ê³µê°„ íŠœë‹ì€ ê°€ì¤‘ì¹˜ ê¸°ë°˜ PEFTì— ëŒ€í•œ ë³´ì™„ì ì´ê³  ë§¤ìš° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ëŒ€ì•ˆìœ¼ë¡œ ìë¦¬ë§¤ê¹€í•˜ë©°, í™œì„±í™” í•¨ìˆ˜ë¥¼ ëª¨ë¸ ì ì‘ì˜ ì£¼ìš” ê°ì²´ë¡œ ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:41:55*