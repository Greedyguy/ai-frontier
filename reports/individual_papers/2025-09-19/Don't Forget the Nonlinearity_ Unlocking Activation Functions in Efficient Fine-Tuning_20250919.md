
# Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning

**Korean Title:** λΉ„μ„ ν•μ„±μ„ μμ§€ λ§μ„Έμ”: ν¨μ¨μ μΈ λ―Έμ„Έ μ΅°μ •μ—μ„ ν™μ„±ν™” ν•¨μμ μ μ¬λ ¥ λ°ν

## π“‹ λ©”νƒ€λ°μ΄ν„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## π·οΈ μΉ΄ν…κ³ λ¦¬ν™”λ ν‚¤μ›λ“
**π€ Evolved Concepts**: Activation Space Tuning

## π”— μ μ‚¬ν• λ…Όλ¬Έ
- [[HAM Hierarchical Adapter Merging for Scalable Continual Learning]] (80.9% similar)
- [[NIRVANA Structured pruning reimagined for large language models compression]] (78.1% similar)
- [[Controllable Pareto Trade-off between Fairness and Accuracy]] (77.4% similar)
- [[Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (77.3% similar)
- [[FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (77.3% similar)

## π“‹ μ €μ μ •λ³΄

**Authors:** 

## π“„ Abstract (μ›λ¬Έ)

arXiv:2509.13240v2 Announce Type: replace 
Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving accuracy gains of +0.17\% and +0.27\%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation.

## π” Abstract (ν•κΈ€ λ²μ—­)

arXiv:2509.13240v2 λ°ν‘ μ ν•: κµμ²΄

μ΄λ΅: κΈ°μ΅΄μ νλΌλ―Έν„° ν¨μ¨μ  λ―Έμ„Έ μ΅°μ •(PEFT) λ°©λ²•μ€ μ£Όλ΅ κ°€μ¤‘μΉ ν–‰λ ¬μ„ μ΅°μ •ν•λ©΄μ„ ν™μ„±ν™” ν•¨μλ¥Ό κ³ μ •λ μƒνƒλ΅ μ μ§€ν•©λ‹λ‹¤. μ°λ¦¬λ” μ‚¬μ „ ν•™μµλ νΈλμ¤ν¬λ¨Έ κΈ°λ° λ¨λΈμ—μ„ λΉ„μ„ ν• ν™μ„±ν™” ν•¨μλ¥Ό μ§μ ‘ μ΅°μ •ν•λ” μµμ΄μ PEFT ν”„λ μ„μ›ν¬μΈ \textbf{NoRA}λ¥Ό μ†κ°ν•©λ‹λ‹¤. NoRAλ” κ³ μ •λ ν™μ„±ν™” ν•¨μλ¥Ό ν•™μµ κ°€λ¥ν• μ λ¦¬ ν•¨μλ΅ λ€μ²΄ν•κ³ , λ¶„μμ™€ λ¶„λ¨ κ³„μμ— κµ¬μ΅°ν™”λ μ €λ­ν¬ μ—…λ°μ΄νΈλ¥Ό μ μ©ν•λ©°, κ·Έλ£Ήλ³„ μ„¤κ³„λ¥Ό ν†µν•΄ μ μ‘μ„ κµ­μ§€ν™”ν•κ³  μµμ†ν•μ λΉ„μ©μΌλ΅ μ•μ •μ„±μ„ ν–¥μƒμ‹ν‚µλ‹λ‹¤. CIFAR-10κ³Ό CIFAR-100μ—μ„ ν›λ ¨λ λΉ„μ „ νΈλμ¤ν¬λ¨Έμ—μ„ NoRAλ” μ „μ²΄ λ―Έμ„Έ μ΅°μ •μ„ μν–‰ν•κ±°λ‚ μ΄λ¥Ό μ΄κ³Όν•λ©°, λ‹¨ 0.4\%μ νλΌλ―Έν„°(0.02M)λ§ μ—…λ°μ΄νΈν•μ—¬ μ •ν™•λ„ ν–¥μƒμ„ κ°κ° +0.17\%μ™€ +0.27\% λ‹¬μ„±ν•©λ‹λ‹¤. LoRAμ™€ κ²°ν•©λ κ²½μ°(\textbf{NoRA++}), NoRA++λ” λ” μ μ€ ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°λ¥Ό μ¶”κ°€ν•μ—¬ λ™μΌν• ν›λ ¨ μμ‚° ν•μ—μ„ LoRAμ™€ DoRAλ¥Ό λ¥κ°€ν•©λ‹λ‹¤. LLaMA3-8B μ§€μ‹ μ΅°μ •μ—μ„ NoRA++λ” μƒμ„± ν’μ§μ„ μ§€μ†μ μΌλ΅ ν–¥μƒμ‹ν‚¤λ©°, ν‰κ·  MMLU ν–¥μƒμ„ +0.3\%--0.8\% λ‹¬μ„±ν•κ³ , STEM(Alpaca)μ—μ„ +1.6\% λ° OpenOrcaμ—μ„ +1.3\%λ¥Ό ν¬ν•¨ν•©λ‹λ‹¤. μ°λ¦¬λ” λν• NoRAκ°€ μ €μ°¨μ› κΈ°λ¥μ  λ¶€λ¶„ κ³µκ°„μ— μ μ‘μ„ μ ν•ν•μ—¬ μ—…λ°μ΄νΈ ν¬κΈ°μ™€ λ°©ν–¥μ„ μ•”λ¬µμ μΌλ΅ μ •κ·ν™”ν•¨μ„ λ³΄μ—¬μ¤λ‹λ‹¤. μ΄λ¬ν• κ²°κ³Όλ” ν™μ„±ν™” κ³µκ°„ νλ‹μ΄ κ°€μ¤‘μΉ κΈ°λ° PEFTμ— λ€ν• λ³΄μ™„μ μ΄κ³  λ§¤μ° νλΌλ―Έν„° ν¨μ¨μ μΈ λ€μ•μΌλ΅μ„, λ¨λΈ μ μ‘μ„ μ„ν• ν™μ„±ν™” ν•¨μλ¥Ό μΌκΈ‰ κ°μ²΄λ΅ μ„μΉμ‹ν‚µλ‹λ‹¤.

## π“ μ”μ•½

κΈ°μ΅΄μ λ§¤κ°λ³€μ ν¨μ¨μ  λ―Έμ„Έ μ΅°μ •(PEFT) λ°©λ²•μ€ μ£Όλ΅ κ°€μ¤‘μΉ ν–‰λ ¬μ„ μ΅°μ •ν•μ§€λ§, ν™μ„±ν™” ν•¨μλ” κ³ μ •λ μƒνƒλ΅ μ μ§€ν•©λ‹λ‹¤. λ³Έ λ…Όλ¬Έμ—μ„λ” μ‚¬μ „ ν•™μµλ νΈλμ¤ν¬λ¨Έ κΈ°λ° λ¨λΈμ—μ„ λΉ„μ„ ν• ν™μ„±ν™” ν•¨μλ¥Ό μ§μ ‘ μ΅°μ •ν•λ” μµμ΄μ PEFT ν”„λ μ„μ›ν¬μΈ NoRAλ¥Ό μ†κ°ν•©λ‹λ‹¤. NoRAλ” κ³ μ •λ ν™μ„±ν™” ν•¨μλ¥Ό ν•™μµ κ°€λ¥ν• μ λ¦¬ ν•¨μλ΅ λ€μ²΄ν•κ³ , λ¶„μ λ° λ¶„λ¨ κ³„μμ— κµ¬μ΅°ν™”λ μ €λ­ν¬ μ—…λ°μ΄νΈλ¥Ό μ μ©ν•μ—¬ μ•μ •μ„±μ„ λ†’μ…λ‹λ‹¤. CIFAR-10 λ° CIFAR-100μ—μ„ NoRAλ” μ „μ²΄ λ―Έμ„Έ μ΅°μ •κ³Ό λ™λ“±ν•κ±°λ‚ λ” λ‚μ€ μ„±λ¥μ„ λ³΄μ΄λ©°, λ§¤κ°λ³€μμ 0.4%λ§ μ—…λ°μ΄νΈν•μ—¬ μ •ν™•λ„λ¥Ό κ°κ° +0.17% λ° +0.27% ν–¥μƒμ‹μΌ°μµλ‹λ‹¤. NoRAμ™€ LoRAλ¥Ό κ²°ν•©ν• NoRA++λ” λ™μΌν• ν•™μµ μμ‚° ν•μ—μ„ λ” μ μ€ λ§¤κ°λ³€μλ΅ LoRAμ™€ DoRAλ¥Ό λ¥κ°€ν•©λ‹λ‹¤. LLaMA3-8B μ§€μΉ¨ μ΅°μ •μ—μ„λ” NoRA++κ°€ μΌκ΄€λκ² μƒμ„± ν’μ§μ„ κ°μ„ ν•μ—¬ ν‰κ·  MMLUλ¥Ό +0.3%μ—μ„ +0.8%κΉμ§€ ν–¥μƒμ‹μΌ°μµλ‹λ‹¤. μ΄ μ—°κµ¬λ” ν™μ„±ν™” ν•¨μ μ΅°μ •μ΄ κ°€μ¤‘μΉ κΈ°λ° PEFTμ λ³΄μ™„μ  λ€μ•μ„μ„ μ…μ¦ν•λ©°, λ¨λΈ μ μ‘μ„ μ„ν• μ¤‘μ”ν• μ”μ†λ΅ ν™μ„±ν™” ν•¨μλ¥Ό μ μ‹ν•©λ‹λ‹¤.

## π― μ£Όμ” ν¬μΈνΈ

- 1. NoRAλ” λΉ„μ„ ν• ν™μ„±ν™” ν•¨μλ¥Ό μ§μ ‘ μ΅°μ •ν•λ” μµμ΄μ PEFT ν”„λ μ„μ›ν¬λ΅, μ‚¬μ „ ν•™μµλ νΈλμ¤ν¬λ¨Έ κΈ°λ° λ¨λΈμ—μ„ ν•™μµ κ°€λ¥ν• μ λ¦¬ ν•¨μλ΅ κ³ μ •λ ν™μ„±ν™”λ¥Ό λ€μ²΄ν•©λ‹λ‹¤.

- 2. NoRAλ” CIFAR-10 λ° CIFAR-100μ—μ„ ν›λ ¨λ λΉ„μ „ νΈλμ¤ν¬λ¨Έμ—μ„ μ „μ²΄ νμΈ νλ‹κ³Ό λ™λ“±ν•κ±°λ‚ κ·Έ μ΄μƒμ μ„±λ¥μ„ λ³΄μ΄λ©°, λ‹¨ 0.4%μ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•μ—¬ μ •ν™•λ„ ν–¥μƒμ„ λ‹¬μ„±ν•©λ‹λ‹¤.

- 3. NoRA++λ” LoRAμ™€ κ²°ν•©ν•μ—¬, λ™μΌν• ν›λ ¨ μμ‚° ν•μ—μ„ λ” μ μ€ ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°λ¥Ό μ¶”κ°€ν•μ—¬ LoRA λ° DoRAλ¥Ό λ¥κ°€ν•©λ‹λ‹¤.

- 4. NoRAλ” μ €μ°¨μ› ν•¨μμ  λ¶€λ¶„ κ³µκ°„μΌλ΅μ μ μ‘μ„ μ ν•ν•μ—¬, μ—…λ°μ΄νΈμ ν¬κΈ°μ™€ λ°©ν–¥μ„ μ•”λ¬µμ μΌλ΅ μ •κ·ν™”ν•©λ‹λ‹¤.

- 5. ν™μ„±ν™” κ³µκ°„ νλ‹μ€ κ°€μ¤‘μΉ κΈ°λ° PEFTμ— λ€ν• λ³΄μ™„μ μ΄κ³  λ§¤μ° νλΌλ―Έν„° ν¨μ¨μ μΈ λ€μ•μΌλ΅ μλ¦¬λ§¤κΉ€ν•λ©°, ν™μ„±ν™” ν•¨μλ¥Ό λ¨λΈ μ μ‘μ μ£Όμ” κ°μ²΄λ΅ μ„μΉμ‹ν‚µλ‹λ‹¤.

---

*Generated on 2025-09-19 15:41:55*