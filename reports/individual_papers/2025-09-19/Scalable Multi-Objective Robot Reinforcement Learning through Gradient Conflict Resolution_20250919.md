---
keywords:
  - Reinforcement Learning
  - Proximal Policy Optimization
  - Multi-Objective Optimization
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14816
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:25:19.387774",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Proximal Policy Optimization",
    "Multi-Objective Optimization"
  ],
  "rejected_keywords": [
    "Gradient Conflict Resolution"
  ],
  "similarity_scores": {
    "Reinforcement Learning": 0.88,
    "Proximal Policy Optimization": 0.82,
    "Multi-Objective Optimization": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution

**Korean Title:** í™•ì¥ ê°€ëŠ¥í•œ ë‹¤ëª©ì  ë¡œë´‡ ê°•í™” í•™ìŠµì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì¶©ëŒ í•´ê²° ë°©ë²•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]]
**âš¡ Unique Technical**: [[keywords/Multi-Objective Optimization|Multi-Objective Optimization]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Controllable Pareto Trade-off between Fairness and Accuracy]] (82.4% similar)
- [[TGPO Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (81.9% similar)
- [[Constructive_Conflict-Driven_Multi-Agent_Reinforcement_Learning_for_Strategic_Diversity_20250919|Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity]] (81.8% similar)
- [[FlowRL Matching Reward Distributions for LLM Reasoning]] (81.8% similar)
- [[Hybrid_Diffusion_Policies_with_Projective_Geometric_Algebra_for_Efficient_Robot_Manipulation_Learning_20250918|Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning]] (81.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14816v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) robot controllers usually aggregate many task objectives into one scalar reward. While large-scale proximal policy optimisation (PPO) has enabled impressive results such as robust robot locomotion in the real world, many tasks still require careful reward tuning and are brittle to local optima. Tuning cost and sub-optimality grow with the number of objectives, limiting scalability. Modelling reward vectors and their trade-offs can address these issues; however, multi-objective methods remain underused in RL for robotics because of computational cost and optimisation difficulty. In this work, we investigate the conflict between gradient contributions for each objective that emerge from scalarising the task objectives. In particular, we explicitly address the conflict between task-based rewards and terms that regularise the policy towards realistic behaviour. We propose GCR-PPO, a modification to actor-critic optimisation that decomposes the actor update into objective-wise gradients using a multi-headed critic and resolves conflicts based on the objective priority. Our methodology, GCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion benchmarks and additional multi-objective modifications on two related tasks. We show superior scalability compared to parallel PPO (p = 0.04), without significant computational overhead. We also show higher performance with more conflicting tasks. GCR-PPO improves on large-scale PPO with an average improvement of 9.5%, with high-conflict tasks observing a greater improvement. The code is available at https://github.com/humphreymunn/GCR-PPO.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14816v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ê°•í™” í•™ìŠµ(RL) ë¡œë´‡ ì œì–´ê¸°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì—¬ëŸ¬ ì‘ì—… ëª©í‘œë¥¼ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ë³´ìƒìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ê·¼ì ‘ ì •ì±… ìµœì í™”(PPO)ëŠ” ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ê°•ë ¥í•œ ë¡œë´‡ ë³´í–‰ê³¼ ê°™ì€ ì¸ìƒì ì¸ ê²°ê³¼ë¥¼ ê°€ëŠ¥í•˜ê²Œ í–ˆì§€ë§Œ, ë§ì€ ì‘ì—…ì€ ì—¬ì „íˆ ì‹ ì¤‘í•œ ë³´ìƒ ì¡°ì •ì´ í•„ìš”í•˜ë©° êµ­ì†Œ ìµœì ì ì— ì·¨ì•½í•©ë‹ˆë‹¤. ëª©í‘œì˜ ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì¡°ì • ë¹„ìš©ê³¼ ë¹„ìµœì ì„±ì´ ì¦ê°€í•˜ì—¬ í™•ì¥ì„±ì´ ì œí•œë©ë‹ˆë‹¤. ë³´ìƒ ë²¡í„°ì™€ ê·¸ ìƒì¶© ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ë©´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ëª©ì  ë°©ë²•ì€ ê³„ì‚° ë¹„ìš©ê³¼ ìµœì í™”ì˜ ì–´ë ¤ì›€ ë•Œë¬¸ì— ë¡œë´‡ ê³µí•™ì˜ RLì—ì„œ ì—¬ì „íˆ ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‘ì—… ëª©í‘œë¥¼ ìŠ¤ì¹¼ë¼í™”í•  ë•Œ ë°œìƒí•˜ëŠ” ê° ëª©í‘œì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ì—¬ ê°„ì˜ ì¶©ëŒì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. íŠ¹íˆ, ìš°ë¦¬ëŠ” í˜„ì‹¤ì ì¸ í–‰ë™ì„ í–¥í•œ ì •ì±…ì„ ì •ê·œí™”í•˜ëŠ” í•­ëª©ê³¼ ì‘ì—… ê¸°ë°˜ ë³´ìƒ ê°„ì˜ ì¶©ëŒì„ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì¤‘ í—¤ë“œ ë¹„í‰ê°€ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª©í‘œë³„ ê·¸ë˜ë””ì–¸íŠ¸ë¡œ ë°°ìš° ì—…ë°ì´íŠ¸ë¥¼ ë¶„í•´í•˜ê³  ëª©í‘œ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì¶©ëŒì„ í•´ê²°í•˜ëŠ” ë°°ìš°-ë¹„í‰ê°€ ìµœì í™”ì˜ ìˆ˜ì • ë²„ì „ì¸ GCR-PPOë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ë¡ ì¸ GCR-PPOëŠ” ì˜ ì•Œë ¤ì§„ IsaacLab ì¡°ì‘ ë° ë³´í–‰ ë²¤ì¹˜ë§ˆí¬ì™€ ë‘ ê°€ì§€ ê´€ë ¨ ì‘ì—…ì— ëŒ€í•œ ì¶”ê°€ ë‹¤ëª©ì  ìˆ˜ì •ì—ì„œ í‰ê°€ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë³‘ë ¬ PPOì™€ ë¹„êµí•˜ì—¬ ìœ ì˜ë¯¸í•œ ê³„ì‚° ì˜¤ë²„í—¤ë“œ ì—†ì´ ìš°ìˆ˜í•œ í™•ì¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤(p = 0.04). ë˜í•œ, ë” ë§ì€ ìƒì¶©ë˜ëŠ” ì‘ì—…ì—ì„œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. GCR-PPOëŠ” ëŒ€ê·œëª¨ PPOì—ì„œ í‰ê·  9.5%ì˜ ê°œì„ ì„ ë³´ì—¬ì£¼ë©°, ë†’ì€ ê°ˆë“± ì‘ì—…ì—ì„œëŠ” ë” í° ê°œì„ ì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/humphreymunn/GCR-PPOì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL) ë¡œë´‡ ì œì–´ì—ì„œ ë‹¤ì¤‘ ëª©í‘œë¥¼ ë‹¨ì¼ ìŠ¤ì¹¼ë¼ ë³´ìƒìœ¼ë¡œ ì§‘ê³„í•˜ëŠ” ê¸°ì¡´ ë°©ë²•ì˜ í•œê³„ë¥¼ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ëª©í‘œë³„ ê¸°ìš¸ê¸° ì¶©ëŒ ë¬¸ì œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ GCR-PPOë¼ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ì¤‘ í—¤ë“œ ë¹„í‰ê°€ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª©í‘œë³„ ê¸°ìš¸ê¸°ë¥¼ ë¶„í•´í•˜ê³ , ëª©í‘œ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì¶©ëŒì„ í•´ê²°í•©ë‹ˆë‹¤. GCR-PPOëŠ” IsaacLab ì¡°ì‘ ë° ì´ë™ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ê¸°ì¡´ì˜ PPO ëŒ€ë¹„ í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ê³ , ì¶©ëŒì´ ë§ì€ ì‘ì—…ì—ì„œ í‰ê·  9.5%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” RLì—ì„œ ë‹¤ì¤‘ ëª©í‘œ ë°©ë²•ë¡ ì˜ í™œìš© ê°€ëŠ¥ì„±ì„ ë†’ì´ë©°, ì½”ë“œë„ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµ ë¡œë´‡ ì œì–´ê¸°ëŠ” ì—¬ëŸ¬ ê³¼ì œ ëª©í‘œë¥¼ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ë³´ìƒìœ¼ë¡œ ì§‘ê³„í•˜ì§€ë§Œ, ë³´ìƒ íŠœë‹ì´ í•„ìš”í•˜ê³  ì§€ì—­ ìµœì ì ì— ë¯¼ê°í•˜ë‹¤.

- 2. ë‹¤ì¤‘ ëª©í‘œ ë°©ë²•ì€ ê³„ì‚° ë¹„ìš©ê³¼ ìµœì í™”ì˜ ì–´ë ¤ì›€ ë•Œë¬¸ì— ë¡œë´‡ ê³µí•™ì—ì„œ ì˜ í™œìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.

- 3. GCR-PPOëŠ” ëª©í‘œë³„ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ìœ„ì ì—…ë°ì´íŠ¸ë¥¼ ë¶„í•´í•˜ê³  ëª©í‘œ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê°ˆë“±ì„ í•´ê²°í•œë‹¤.

- 4. GCR-PPOëŠ” IsaacLab ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ PPOë³´ë‹¤ 9.5%ì˜ í‰ê·  ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ë©°, íŠ¹íˆ ê°ˆë“±ì´ ë§ì€ ì‘ì—…ì—ì„œ ë” í° ê°œì„ ì„ ë³´ì¸ë‹¤.

- 5. GCR-PPOëŠ” ë³‘ë ¬ PPOë³´ë‹¤ ë” ë‚˜ì€ í™•ì¥ì„±ì„ ì œê³µí•˜ë©°, ì¶”ê°€ì ì¸ ê³„ì‚° ë¶€ë‹´ ì—†ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.

---

*Generated on 2025-09-19 15:36:09*