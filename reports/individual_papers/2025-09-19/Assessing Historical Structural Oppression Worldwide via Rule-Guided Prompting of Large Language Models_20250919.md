---
keywords:
  - Large Language Models
  - Rule-Guided Prompting
  - Oppression Measurement
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.15216
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:55:59.156336",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Rule-Guided Prompting",
    "Oppression Measurement"
  ],
  "rejected_keywords": [
    "Systemic Exclusion"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Rule-Guided Prompting": 0.72,
    "Oppression Measurement": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models

**Korean Title:** ì „ ì„¸ê³„ì˜ ì—­ì‚¬ì  êµ¬ì¡°ì  ì–µì•• í‰ê°€: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ê·œì¹™ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Large Language Models|Large Language Models]]
**âš¡ Unique Technical**: [[keywords/Rule-Guided Prompting|rule-guided prompting]], [[keywords/Oppression Measurement|oppression measurement]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Adding LLMs to the psycholinguistic norming toolbox A practical guide to getting the most out of human ratings]] (85.4% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (83.9% similar)
- [[Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (82.7% similar)
- [[Do LLMs Align Human Values Regarding Social Biases_ Judging and Explaining Social Biases with LLMs_20250918|Do LLMs Align Human Values Regarding Social Biases Judging and Explaining Social Biases with LLMs]] (82.5% similar)
- [[A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare]] (82.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15216v1 Announce Type: new 
Abstract: Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/llm-oppression-benchmark).

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15216v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì—­ì‚¬ì  êµ¬ì¡°ì  ì–µì••ì„ ì¸¡ì •í•˜ë ¤ëŠ” ì „í†µì ì¸ ë…¸ë ¥ì€ ê° êµ­ê°€ì˜ ë°°ì œ, ì‹ë¯¼í™”, ì‚¬íšŒì  ì§€ìœ„ì— ëŒ€í•œ ê³ ìœ í•˜ê³  ì§€ì—­ì ìœ¼ë¡œ íŠ¹ì •ëœ ì—­ì‚¬ ë•Œë¬¸ì— êµ­ê°€ ê°„ íƒ€ë‹¹ì„±ì„ í™•ë³´í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ì¢…ì¢… ë¬¼ì§ˆì  ìì›ì„ ìš°ì„ ì‹œí•˜ë©´ì„œ ê²½í—˜ì ì´ê³  ì •ì²´ì„± ê¸°ë°˜ì˜ ë°°ì œë¥¼ ê°„ê³¼í•˜ëŠ” êµ¬ì¡°í™”ëœ ì§€ìˆ˜ì— ì˜ì¡´í•´ ì™”ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ì§€ì •í•™ì  í™˜ê²½ì—ì„œ ê²½í—˜ëœ ì—­ì‚¬ì  ë¶ˆì´ìµì˜ ë§¥ë½ì— ë¯¼ê°í•œ ì ìˆ˜ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ ì–µì•• ì¸¡ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ë‹¤êµ­ì–´ COVID-19 ê¸€ë¡œë²Œ ì—°êµ¬ì—ì„œ ë¹„êµ¬ì¡°í™”ëœ ìê°€ ì‹ë³„ ë¯¼ì¡± ë°œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬, ëª¨ë¸ì´ í•´ì„ ê°€ëŠ¥í•˜ê³  ì´ë¡ ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ì–µì•• ì¶”ì •ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê·œì¹™ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì „ëµì„ ì„¤ê³„í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì „ëµì„ ì—¬ëŸ¬ ìµœì²¨ë‹¨ LLMì— ê±¸ì³ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ëª…ì‹œì ì¸ ê·œì¹™ì— ì˜í•´ ì•ˆë‚´ë  ë•Œ LLMì´ êµ­ê°€ ë‚´ ì •ì²´ì„± ê¸°ë°˜ì˜ ì—­ì‚¬ì  ì–µì••ì˜ ë¯¸ë¬˜í•œ í˜•íƒœë¥¼ í¬ì°©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì²´ê³„ì  ë°°ì œì˜ ì°¨ì›ì„ ê°•ì¡°í•˜ëŠ” ë³´ì™„ì ì¸ ì¸¡ì • ë„êµ¬ë¥¼ ì œê³µí•˜ì—¬ ë°ì´í„° ê¸°ë°˜ ì—°êµ¬ ë° ê³µì¤‘ ë³´ê±´ ë§¥ë½ì—ì„œ ì–µì••ì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ë¬¸í™” ê°„ì˜ ê´€ì ì„ ì œê³µí•©ë‹ˆë‹¤. ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì–µì•• ì¸¡ì •ì— ëŒ€í•œ LLM í‰ê°€ë¥¼ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ê³µê°œí•©ë‹ˆë‹¤ (https://github.com/chattergpt/llm-oppression-benchmark).

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì „í†µì ì¸ ì—­ì‚¬ì  ì–µì•• ì¸¡ì • ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í™œìš©í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê°êµ­ì˜ ê³ ìœ í•œ ë°°ì œì™€ ì‹ë¯¼ì§€í™” ì—­ì‚¬ë¥¼ ë°˜ì˜í•˜ê¸° ì–´ë ¤ìš´ ê¸°ì¡´ ë°©ë²•ë¡  ëŒ€ì‹ , LLMsë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì§€ì •í•™ì  í™˜ê²½ì—ì„œ ë§¥ë½ì— ë¯¼ê°í•œ ì–µì•• ì ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë‹¤êµ­ì–´ COVID-19 ì—°êµ¬ì—ì„œ ìˆ˜ì§‘ëœ ë¹„êµ¬ì¡°í™”ëœ ìê¸° ì‹ë³„ ë¯¼ì¡± ë°œì–¸ì„ ì‚¬ìš©í•˜ì—¬, ëª¨ë¸ì´ í•´ì„ ê°€ëŠ¥í•˜ê³  ì´ë¡ ì ìœ¼ë¡œ ê¸°ë°˜ì„ ë‘” ì–µì•• ì¶”ì •ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê·œì¹™ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì „ëµì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ìµœì‹  LLMsë¥¼ í†µí•´ ì´ëŸ¬í•œ ì „ëµì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼, ëª…ì‹œì ì¸ ê·œì¹™ì— ì˜í•´ ì•ˆë‚´ëœ LLMsê°€ êµ­ê°€ ë‚´ ì •ì²´ì„± ê¸°ë°˜ì˜ ì—­ì‚¬ì  ì–µì••ì„ ì„¸ë°€í•˜ê²Œ í¬ì°©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì²´ê³„ì  ë°°ì œì˜ ì°¨ì›ì„ ê°•ì¡°í•˜ëŠ” ë³´ì™„ì  ì¸¡ì • ë„êµ¬ë¥¼ ì œê³µí•˜ë©°, ë°ì´í„° ê¸°ë°˜ ì—°êµ¬ì™€ ê³µì¤‘ ë³´ê±´ ë§¥ë½ì—ì„œ ì–µì••ì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ë° ìœ ìš©í•œ ë„êµ¬ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ì–µì•• ì¸¡ì •ì— ëŒ€í•œ LLMs í‰ê°€ë¥¼ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ë„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì „í†µì ì¸ ì—­ì‚¬ì  êµ¬ì¡°ì  ì–µì•• ì¸¡ì •ì€ ê°êµ­ì˜ ê³ ìœ í•œ ë°°ì œ, ì‹ë¯¼ì§€í™”, ì‚¬íšŒì  ì§€ìœ„ì˜ ì—­ì‚¬ë¡œ ì¸í•´ êµ­ê°€ ê°„ ìœ íš¨ì„±ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

- 2. ìš°ë¦¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì§€ì •í•™ì  í™˜ê²½ì—ì„œ ê²½í—˜ëœ ì—­ì‚¬ì  ë¶ˆì´ìµì˜ ë§¥ë½ì— ë¯¼ê°í•œ ì ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ì–µì•• ì¸¡ì • í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

- 3. ë‹¤êµ­ì–´ COVID-19 ê¸€ë¡œë²Œ ì—°êµ¬ì—ì„œ ë¹„êµ¬ì¡°í™”ëœ ìê°€ ì‹ë³„ ë¯¼ì¡±ì„±ì„ ì‚¬ìš©í•˜ì—¬, ëª¨ë¸ì´ í•´ì„ ê°€ëŠ¥í•˜ê³  ì´ë¡ ì ìœ¼ë¡œ ê·¼ê±° ìˆëŠ” ì–µì•• ì¶”ì •ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê·œì¹™ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì „ëµì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

- 4. ëª…ì‹œì ì¸ ê·œì¹™ì— ì˜í•´ ì•ˆë‚´ë  ë•Œ LLMsê°€ êµ­ê°€ ë‚´ì—ì„œ ì •ì²´ì„± ê¸°ë°˜ì˜ ì—­ì‚¬ì  ì–µì••ì˜ ë¯¸ë¬˜í•œ í˜•íƒœë¥¼ í¬ì°©í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

- 5. ìš°ë¦¬ëŠ” LLMsì˜ ì–µì•• ì¸¡ì •ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:56:13*