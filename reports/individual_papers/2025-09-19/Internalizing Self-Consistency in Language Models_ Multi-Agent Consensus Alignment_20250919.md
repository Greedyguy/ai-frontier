---
keywords:
  - Large Language Models
  - Multi-Agent Consensus Alignment
  - Reinforcement Learning
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.15172
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:39:48.704673",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Multi-Agent Consensus Alignment",
    "Reinforcement Learning"
  ],
  "rejected_keywords": [
    "Self-Consistency"
  ],
  "similarity_scores": {
    "Large Language Models": 0.88,
    "Multi-Agent Consensus Alignment": 0.8,
    "Reinforcement Learning": 0.85
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment

**Korean Title:** ì–¸ì–´ ëª¨ë¸ì—ì„œ ìê¸° ì¼ê´€ì„± ë‚´ì¬í™”: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í•©ì˜ ì •ë ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Large Language Models|Language Models]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Multi-Agent Consensus Alignment|Multi-Agent Consensus Alignment]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[MAgICoRe Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning]] (84.8% similar)
- [[Enhancing Multi-Agent Debate System Performance via Confidence Expression]] (84.0% similar)
- [[Language Models Identify Ambiguities and Exploit Loopholes]] (83.7% similar)
- [[A_Knowledge-driven_Adaptive_Collaboration_of_LLMs_for_Enhancing_Medical_Decision-making_20250919|A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making]] (83.6% similar)
- [[AgentCompass Towards Reliable Evaluation of Agentic Workflows in Production]] (83.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15172v1 Announce Type: new 
Abstract: Language Models (LMs) are inconsistent reasoners, often generating contradictory responses to identical prompts. While inference-time methods can mitigate these inconsistencies, they fail to address the core problem: LMs struggle to reliably select reasoning pathways leading to consistent outcomes under exploratory sampling. To address this, we formalize self-consistency as an intrinsic property of well-aligned reasoning models and introduce Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with their internal consensus using majority/minority outcomes from multi-agent debate. These trajectories emerge from deliberative exchanges where agents ground reasoning in peer arguments, not just aggregation of independent attempts, creating richer consensus signals than single-round majority voting. MACA enables agents to teach themselves to be more decisive and concise, and better leverage peer insights in multi-agent settings without external supervision, driving substantial improvements across self-consistency (+27.6% on GSM8K), single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4% Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA). These findings, coupled with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more reliably unlocks latent reasoning potential of language models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15172v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì–¸ì–´ ëª¨ë¸(LM)ì€ ì¼ê´€ì„±ì´ ì—†ëŠ” ì¶”ë¡ ì„ í•˜ë©°, ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ëª¨ìˆœëœ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì¶”ë¡  ì‹œê°„ ë°©ë²•ì€ ì´ëŸ¬í•œ ë¶ˆì¼ì¹˜ë¥¼ ì™„í™”í•  ìˆ˜ ìˆì§€ë§Œ, í•µì‹¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ëŠ” ëª»í•©ë‹ˆë‹¤. ì¦‰, LMì€ íƒìƒ‰ì  ìƒ˜í”Œë§ í•˜ì—ì„œ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì´ëŒì–´ë‚´ëŠ” ì¶”ë¡  ê²½ë¡œë¥¼ ì‹ ë¢°ì„± ìˆê²Œ ì„ íƒí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìê¸° ì¼ê´€ì„±ì„ ì˜ ì •ë ¬ëœ ì¶”ë¡  ëª¨ë¸ì˜ ë‚´ì¬ì  íŠ¹ì„±ìœ¼ë¡œ ê³µì‹í™”í•˜ê³ , ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í•©ì˜ ì •ë ¬(Multi-Agent Consensus Alignment, MACA)ì„ ë„ì…í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì—ì„œ ë‹¤ìˆ˜/ì†Œìˆ˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ë¶€ í•©ì˜ì— ë§ëŠ” ì¶”ë¡  ê²½ë¡œë¥¼ ì„ í˜¸í•˜ë„ë¡ ëª¨ë¸ì„ ì‚¬í›„ í•™ìŠµì‹œí‚¤ëŠ” ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ë¡œëŠ” ë…ë¦½ì ì¸ ì‹œë„ì˜ ì§‘ê³„ê°€ ì•„ë‹Œ, ì—ì´ì „íŠ¸ê°€ ë™ë£Œì˜ ì£¼ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ì„ ì •ë¦½í•˜ëŠ” ì‹¬ì˜ì  êµí™˜ì—ì„œ ë°œìƒí•˜ì—¬ ë‹¨ì¼ ë¼ìš´ë“œ ë‹¤ìˆ˜ íˆ¬í‘œë³´ë‹¤ ë” í’ë¶€í•œ í•©ì˜ ì‹ í˜¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. MACAëŠ” ì—ì´ì „íŠ¸ê°€ ì™¸ë¶€ ê°ë… ì—†ì´ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œ ë™ë£Œì˜ í†µì°°ë ¥ì„ ë” ì˜ í™œìš©í•˜ê³ , ë” ê²°ë‹¨ë ¥ ìˆê³  ê°„ê²°í•˜ê²Œ ìŠ¤ìŠ¤ë¡œë¥¼ ê°€ë¥´ì¹  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ìê¸° ì¼ê´€ì„±(+27.6% GSM8K), ë‹¨ì¼ ì—ì´ì „íŠ¸ ì¶”ë¡ (+23.7% MATH), ìƒ˜í”Œë§ ê¸°ë°˜ ì¶”ë¡ (+22.4% Pass@20 MATH), ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì•™ìƒë¸” ì˜ì‚¬ ê²°ì •(+42.7% MathQA) ì „ë°˜ì— ê±¸ì³ ìƒë‹¹í•œ ê°œì„ ì„ ì´ëŒì–´ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ ë¯¸ì§€ì˜ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê°•ë ¥í•œ ì¼ë°˜í™”(+16.3% GPQA, +11.6% CommonsenseQA)ì™€ ê²°í•©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ì ì¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ë” ì‹ ë¢°ì„± ìˆê²Œ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ê²¬ê³ í•œ ìê¸° ì •ë ¬ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸(LM)ì˜ ì¼ê´€ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í•©ì˜ ì •ë ¬(MACA)ì´ë¼ëŠ” ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. MACAëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ í†µí•´ ëª¨ë¸ì´ ë‚´ë¶€ í•©ì˜ì— ë§ëŠ” ì¶”ë¡  ê²½ë¡œë¥¼ ì„ í˜¸í•˜ë„ë¡ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë…ë¦½ì ì¸ ì‹œë„ë“¤ì˜ ë‹¨ìˆœí•œ ì§‘ê³„ê°€ ì•„ë‹Œ, ì—ì´ì „íŠ¸ ê°„ì˜ ì‹¬ì¸µì ì¸ ë…¼ì˜ë¥¼ í†µí•´ í’ë¶€í•œ í•©ì˜ ì‹ í˜¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. MACAëŠ” ì™¸ë¶€ ê°ë… ì—†ì´ ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œ ë” ê²°ì •ì ì´ê³  ê°„ê²°í•˜ê²Œ í•™ìŠµí•˜ë„ë¡ í•˜ë©°, ë™ë£Œì˜ í†µì°°ì„ ë” ì˜ í™œìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼, ìê¸° ì¼ê´€ì„±, ë‹¨ì¼ ì—ì´ì „íŠ¸ ì¶”ë¡ , ìƒ˜í”Œë§ ê¸°ë°˜ ì¶”ë¡ , ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì˜ì‚¬ ê²°ì •ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ì ì¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ë°œíœ˜í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸(LMs)ì€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ëª¨ìˆœëœ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë¹„ì¼ê´€ì ì¸ ì¶”ë¡ ì„ ë³´ì¸ë‹¤.

- 2. Multi-Agent Consensus Alignment(MACA)ëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì˜ ë‹¤ìˆ˜/ì†Œìˆ˜ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ìê¸° ì¼ê´€ì„±ì— ë§ê²Œ í›„í›ˆë ¨ì‹œí‚¤ëŠ” ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì´ë‹¤.

- 3. MACAëŠ” ì—ì´ì „íŠ¸ê°€ ë™ë£Œì˜ ì£¼ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ì„ ì§„í–‰í•˜ì—¬ ë” í’ë¶€í•œ í•©ì˜ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ê³ , ì™¸ë¶€ ê°ë… ì—†ì´ë„ ë” ê²°ì •ì ì´ê³  ê°„ê²°í•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•œë‹¤.

- 4. MACAëŠ” ìê¸° ì¼ê´€ì„±, ë‹¨ì¼ ì—ì´ì „íŠ¸ ì¶”ë¡ , ìƒ˜í”Œë§ ê¸°ë°˜ ì¶”ë¡ , ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì˜ì‚¬ê²°ì •ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ì´ëŒì–´ë‚¸ë‹¤.

- 5. ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì˜ ì ì¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ë” ì‹ ë¢°ì„± ìˆê²Œ ë°œíœ˜í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

---

*Generated on 2025-09-19 14:49:41*