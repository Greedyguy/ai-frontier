
# Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources

**Korean Title:** ë‹¨ì¼ ë° ì´ì¤‘ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¸ì‚¬ ê´€ë¦¬ ì§ë¬´ ë©´ì ‘ ëŒ€í™” ìƒì„± ë¹„êµ ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Synthetic Dialogue Generation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (83.7% similar)
- [[Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech_20250919|Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech]] (83.3% similar)
- [[Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (82.4% similar)
- [[DuetUI A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces]] (81.3% similar)
- [[Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents_20250919|Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents]] (80.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.18650v2 Announce Type: replace 
Abstract: Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains where obtaining authentic human data is challenging. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for producing HR job interviews, and assess which method generates higher-quality dialogues, i.e., those more difficult to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialogue. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We empirically find that, at the expense of a sixfold increase in token count, interviews generated with the dual-prompt method achieve a win rate 2 to 10 times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or quality judging.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.18650v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í™”í˜• ì—ì´ì „íŠ¸ì— ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì–¸ì–´ ëª¨ë¸ì„ ìµœì í™”í•˜ë ¤ë©´ ëŒ€ëŸ‰ì˜ ì˜ˆì‹œ ëŒ€í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëŒ€í™”ëŠ” ì ì  ë” ê°•ë ¥í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„±ì ìœ¼ë¡œ ìƒì„±ë˜ë©°, íŠ¹íˆ ì§„ì •í•œ ì¸ê°„ ë°ì´í„°ë¥¼ ì–»ê¸° ì–´ë ¤ìš´ ë¶„ì•¼ì—ì„œ ê·¸ë ‡ìŠµë‹ˆë‹¤. ê·¸ëŸ¬í•œ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ëŠ” ì¸ì  ìì›(HR)ì…ë‹ˆë‹¤. ì´ ë§¥ë½ì—ì„œ ìš°ë¦¬ëŠ” HR ì§ë¬´ ì¸í„°ë·°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ LLM ê¸°ë°˜ ëŒ€í™” ìƒì„± ë°©ë²•ì„ ë¹„êµí•˜ê³ , ì–´ë–¤ ë°©ë²•ì´ ë” ë†’ì€ í’ˆì§ˆì˜ ëŒ€í™”, ì¦‰ ì§„ì •í•œ ì¸ê°„ ë‹´í™”ì™€ êµ¬ë³„í•˜ê¸° ì–´ë ¤ìš´ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë°©ë²•ì€ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì¸í„°ë·° ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë°©ë²•ì€ ì„œë¡œ ëŒ€í™”í•˜ëŠ” ë‘ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ë°©ë²•ì— ë”°ë¥¸ ëŒ€í™” í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” íŒì‚¬ LLMì—ê²Œ ì¸í„°ë·° ìƒì„±ì— AIê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ë¥¼ ìŒë³„ ì¸í„°ë·° ë¹„êµë¥¼ í†µí•´ íŒë‹¨í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê²½í—˜ì ìœ¼ë¡œ, í† í° ìˆ˜ê°€ ì—¬ì„¯ ë°° ì¦ê°€í•˜ëŠ” ëŒ€ê°€ë¥¼ ì¹˜ë¥´ë”ë¼ë„, ì´ì¤‘ í”„ë¡¬í”„íŠ¸ ë°©ë²•ìœ¼ë¡œ ìƒì„±ëœ ì¸í„°ë·°ê°€ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë°©ë²•ìœ¼ë¡œ ìƒì„±ëœ ì¸í„°ë·°ë³´ë‹¤ 2ë°°ì—ì„œ 10ë°° ë” ë†’ì€ ìŠ¹ë¥ ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ì°¨ì´ëŠ” ì¸í„°ë·° ìƒì„±ì´ë‚˜ í’ˆì§ˆ í‰ê°€ì— GPT-4o ë˜ëŠ” Llama 3.3 70B ì¤‘ ì–´ëŠ ê²ƒì„ ì‚¬ìš©í•˜ë“  ì¼ê´€ë˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í™”í˜• ì—ì´ì „íŠ¸ì— ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ HR ë¶„ì•¼ì—ì„œ ëŒ€í™” ì˜ˆì‹œë¥¼ ìƒì„±í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ë¹„êµí•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë°©ë²•ì€ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸í„°ë·° ëŒ€í™”ë¥¼ ìƒì„±í•˜ê³ , ë‘ ë²ˆì§¸ ë°©ë²•ì€ ë‘ ì—ì´ì „íŠ¸ê°€ ì„œë¡œ ëŒ€í™”í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ëŒ€í™” í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ íŒì • LLMì„ ì‚¬ìš©í•˜ì—¬ AI ì‚¬ìš© ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë‘ ë²ˆì§¸ ë°©ë²•ì´ í† í° ìˆ˜ê°€ ì—¬ì„¯ ë°° ì¦ê°€í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  í’ˆì§ˆ ë©´ì—ì„œ ë” ë†’ì€ ìŠ¹ë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì°¨ì´ëŠ” GPT-4oë‚˜ Llama 3.3 70Bë¥¼ ì‚¬ìš©í•˜ë”ë¼ë„ ì¼ê´€ë˜ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í™”í˜• ì—ì´ì „íŠ¸ì— ì í•©í•œ ì–¸ì–´ ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ ëŒ€ëŸ‰ì˜ ì˜ˆì‹œ ëŒ€í™”ê°€ í•„ìš”í•˜ë©°, íŠ¹íˆ HR ë¶„ì•¼ì—ì„œëŠ” ëŒ€í™”ì˜ í•©ì„± ìƒì„±ì´ ì¤‘ìš”í•˜ë‹¤.

- 2. HR ë©´ì ‘ ëŒ€í™”ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ LLM ê¸°ë°˜ ë°©ë²•ì„ ë¹„êµí–ˆìœ¼ë©°, ë‘ ì—ì´ì „íŠ¸ê°€ ëŒ€í™”í•˜ëŠ” ë°©ë²•ì´ ë” ë†’ì€ í’ˆì§ˆì˜ ëŒ€í™”ë¥¼ ìƒì„±í•œë‹¤.

- 3. ë‘ ë²ˆì§¸ ë°©ë²•ì€ í† í° ìˆ˜ê°€ 6ë°° ì¦ê°€í•˜ëŠ” ëŒ€ê°€ë¡œ, ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë°©ë²•ë³´ë‹¤ 2~10ë°° ë†’ì€ ìŠ¹ë¥ ì„ ê¸°ë¡í–ˆë‹¤.

- 4. ì´ ì°¨ì´ëŠ” GPT-4oë‚˜ Llama 3.3 70Bë¥¼ ì‚¬ìš©í•˜ë”ë¼ë„ ì¼ê´€ë˜ê²Œ ìœ ì§€ëœë‹¤.

---

*Generated on 2025-09-19 15:58:08*