
# Delta Knowledge Distillation for Large Language Models

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ë¸íƒ€ ì§€ì‹ ì¦ë¥˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Distributional Shift Preservation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[KBM Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models]] (83.2% similar)
- [[From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (83.0% similar)
- [[DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (79.4% similar)
- [[DSCC-HS A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models]] (78.9% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (78.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14526v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher's supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher's knowledge.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14526v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì§€ì‹ ì¦ë¥˜(Knowledge Distillation, KD)ëŠ” í° ì‹ ê²½ë§ì„ ì••ì¶•í•˜ê¸° ìœ„í•´ ëŒ€í˜• êµì‚¬ ëª¨ë¸ì˜ ì§€ì‹ì„ ì‘ì€ í•™ìƒ ëª¨ë¸ë¡œ ì „ì´í•˜ëŠ” ë„ë¦¬ ì±„íƒëœ ë°©ë²•ì…ë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë§¥ë½ì—ì„œ, ì¼ë°˜ì ìœ¼ë¡œ í•™ìƒ ì¶œë ¥ ë¶„í¬ì™€ êµì‚¬ ì¶œë ¥ ë¶„í¬ ê°„ì˜ KL ë°œì‚°ì„ ìµœì†Œí™”í•˜ëŠ” í† í° ìˆ˜ì¤€ì˜ KDëŠ” ê°•ë ¥í•œ ê²½í—˜ì  ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ì „ ì—°êµ¬ì—ì„œëŠ” í•™ìƒ ì¶œë ¥ ë¶„í¬ì™€ êµì‚¬ ì¶œë ¥ ë¶„í¬ê°€ ë™ì¼í•œ ìµœì ì˜ í‘œí˜„ ê³µê°„ì„ ê³µìœ í•œë‹¤ê³  ê°€ì •í–ˆëŠ”ë°, ì´ëŠ” ë§ì€ ê²½ìš°ì— ì„±ë¦½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ì „ì œì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Delta Knowledge Distillation (Delta-KD)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” í† í° ìˆ˜ì¤€ KDì˜ ìƒˆë¡œìš´ í™•ì¥ìœ¼ë¡œ, í•™ìƒì´ êµì‚¬ì˜ ê°ë…ëœ ë¯¸ì„¸ ì¡°ì •(SFT) ë™ì•ˆ ë„ì…ëœ ë¶„í¬ì  ë³€í™” Deltaë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´í•¨ìœ¼ë¡œì¨ ìµœì ì˜ í‘œí˜„ ê³µê°„ì„ ê·¼ì‚¬í•˜ë„ë¡ ì¥ë ¤í•©ë‹ˆë‹¤. ROUGE ì§€í‘œì— ëŒ€í•œ ê²½í—˜ì  ê²°ê³¼ëŠ” Delta KDê°€ í•™ìƒì˜ ì„±ëŠ¥ì„ ìƒë‹¹íˆ í–¥ìƒì‹œí‚¤ë©´ì„œ êµì‚¬ì˜ ì§€ì‹ì„ ë” ë§ì´ ë³´ì¡´í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹ ì¦ë¥˜(KD) ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Delta Knowledge Distillation (Delta-KD)ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ í† í° ìˆ˜ì¤€ KDëŠ” í•™ìƒ ëª¨ë¸ê³¼ êµì‚¬ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„í¬ê°€ ë™ì¼í•œ ìµœì ì˜ í‘œí˜„ ê³µê°„ì„ ê³µìœ í•œë‹¤ê³  ê°€ì •í•˜ì§€ë§Œ, ì´ëŠ” í•­ìƒ ì„±ë¦½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Delta-KDëŠ” êµì‚¬ì˜ ì§€ë„ í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¶„í¬ ë³€í™”(Delta)ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´í•˜ì—¬ í•™ìƒ ëª¨ë¸ì´ ìµœì ì˜ í‘œí˜„ ê³µê°„ì„ ë” ì˜ ê·¼ì‚¬í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ROUGE ì§€í‘œì—ì„œ Delta-KDê°€ í•™ìƒ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œ êµì‚¬ì˜ ì§€ì‹ì„ ë” ì˜ ë³´ì¡´í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì§€ì‹ ì¦ë¥˜(KD)ëŠ” í° ì‹ ê²½ë§ì„ ì••ì¶•í•˜ê¸° ìœ„í•´ ëŒ€ê·œëª¨ êµì‚¬ ëª¨ë¸ì˜ ì§€ì‹ì„ ì‘ì€ í•™ìƒ ëª¨ë¸ë¡œ ì „ì´í•˜ëŠ” ë°©ë²•ì´ë‹¤.

- 2. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ì„œ í† í° ë ˆë²¨ KDëŠ” í•™ìƒê³¼ êµì‚¬ ì¶œë ¥ ë¶„í¬ ê°„ì˜ KL ë°œì‚°ì„ ìµœì†Œí™”í•˜ì—¬ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

- 3. ê¸°ì¡´ ì—°êµ¬ëŠ” í•™ìƒê³¼ êµì‚¬ì˜ ì¶œë ¥ ë¶„í¬ê°€ ë™ì¼í•œ ìµœì ì˜ í‘œí˜„ ê³µê°„ì„ ê³µìœ í•œë‹¤ê³  ê°€ì •í•˜ì§€ë§Œ, ì´ëŠ” í•­ìƒ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤.

- 4. Delta Knowledge Distillation(Delta-KD)ì€ êµì‚¬ì˜ ê°ë…ëœ ë¯¸ì„¸ ì¡°ì •(SFT) ë™ì•ˆ ë„ì…ëœ ë¶„í¬ ë³€í™”(Delta)ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ì¡´í•˜ì—¬ í•™ìƒì´ ìµœì ì˜ í‘œí˜„ ê³µê°„ì„ ê·¼ì‚¬í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì´ë‹¤.

- 5. ROUGE ì§€í‘œì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, Delta-KDëŠ” í•™ìƒì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œ êµì‚¬ì˜ ì§€ì‹ì„ ë” ë§ì´ ë³´ì¡´í•œë‹¤.

---

*Generated on 2025-09-19 14:56:43*