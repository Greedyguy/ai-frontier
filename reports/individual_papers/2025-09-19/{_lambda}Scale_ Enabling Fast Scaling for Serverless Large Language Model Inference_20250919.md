---
keywords:
  - Large Language Models
  - Natural Language Processing
  - Distributed Inference
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2502.09922
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:16:00.266501",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Natural Language Processing",
    "Distributed Inference"
  ],
  "rejected_keywords": [
    "Serverless Computing"
  ],
  "similarity_scores": {
    "Large Language Models": 0.9,
    "Natural Language Processing": 0.85,
    "Distributed Inference": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# {\lambda}Scale: Enabling Fast Scaling for Serverless Large Language Model Inference

**Korean Title:** {\lambda}Scale: ì„œë²„ë¦¬ìŠ¤ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ë¹ ë¥¸ í™•ì¥ ì§€ì›

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**âš¡ Unique Technical**: [[keywords/Distributed Inference|Distributed Inference]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Taming Serverless Cold Starts Through OS Co-Design_20250919|Taming Serverless Cold Starts Through OS Co-Design]] (80.5% similar)
- [[ScaleCUA Scaling Open-Source Computer Use Agents with Cross-Platform Data]] (80.3% similar)
- [[Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection_20250919|Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection]] (80.2% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (80.0% similar)
- [[LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (79.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.09922v2 Announce Type: replace 
Abstract: Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services. In this paper, we introduce {\lambda}Scale, an efficient serverless inference system to achieve fast model scaling. The key idea behind {\lambda}Scale is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission -- referred to as "execute-while-load". {\lambda}Scale proposes an efficient model scaling scheme, {\lambda}Pipe, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference. Additionally, {\lambda}Scale supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. Evaluation results show that {\lambda}Scale enables fast model scaling and effectively handles load spikes, achieving up to 5x tail-latency improvement and 31.3% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.09922v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì„œë²„ë¦¬ìŠ¤ ì»´í“¨íŒ…ì€ í´ë¼ìš°ë“œ ê¸°ë°˜ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ë§¤ë ¥ì ì¸ ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ëŒ€ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê³„ì†í•´ì„œ ì»¤ì§ì— ë”°ë¼ ê¸°ì¡´ì˜ ì„œë²„ë¦¬ìŠ¤ í”Œë«í¼ì€ ì¢…ì¢… ìƒë‹¹í•œ ëª¨ë¸ ì‹œì‘ ì˜¤ë²„í—¤ë“œë¥¼ ê²ªìŠµë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ì¶”ë¡  ì„œë¹„ìŠ¤ì—ì„œ í”íˆ ê´€ì°°ë˜ëŠ” ë™ì ì´ê³  ê¸‰ì¦í•˜ëŠ” ì‘ì—… ë¶€í•˜ì— ë§ì¶° ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë° í° ë„ì „ ê³¼ì œê°€ ë©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¹ ë¥¸ ëª¨ë¸ í™•ì¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ íš¨ìœ¨ì ì¸ ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡  ì‹œìŠ¤í…œì¸ {\lambda}Scaleì„ ì†Œê°œí•©ë‹ˆë‹¤. {\lambda}Scaleì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” GPU ë…¸ë“œ ê°„ì˜ ê³ ì† RDMA ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ë¹ ë¥¸ ëª¨ë¸ ë©€í‹°ìºìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë™ì‹œì— ëª¨ë¸ ì „ì†¡ ì¤‘ì— ë¶„ì‚° ì¶”ë¡  ì‹¤í–‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” "ë¡œë“œ ì¤‘ ì‹¤í–‰" ë°©ì‹ì„ ë„ì…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. {\lambda}Scaleì€ ì ì‘í˜• ëª¨ë¸ ë©€í‹°ìºìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ê³  ìˆ˜ì‹  ë…¸ë“œ ê°„ì— ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ í˜‘ë ¥ì ì´ê³  ë¶„ì‚°ëœ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” íš¨ìœ¨ì ì¸ ëª¨ë¸ í™•ì¥ ë°©ì‹ì¸ {\lambda}Pipeë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, {\lambda}Scaleì€ GPUì™€ í˜¸ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ê°„ì˜ íš¨ìœ¨ì ì¸ ëª¨ë¸ ê´€ë¦¬ë¥¼ ì§€ì›í•˜ì—¬ ë‹¤ì–‘í•œ ì €ì¥ ê³„ì¸µì— ê±¸ì³ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í™•ì¥í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. í‰ê°€ ê²°ê³¼, {\lambda}Scaleì€ ë¹ ë¥¸ ëª¨ë¸ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  ë¶€í•˜ ê¸‰ì¦ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬, ì‹¤ì œ LLM ì¶”ë¡  ì¶”ì ì—ì„œ ìµœì²¨ë‹¨ ì†”ë£¨ì…˜ê³¼ ë¹„êµí•˜ì—¬ ìµœëŒ€ 5ë°°ì˜ ê¼¬ë¦¬ ì§€ì—° ì‹œê°„ ê°œì„ ê³¼ 31.3%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì„œë²„ë¦¬ìŠ¤ ì»´í“¨íŒ…ì€ í´ë¼ìš°ë“œ ê¸°ë°˜ ëª¨ë¸ ì¶”ë¡ ì— ë§¤ë ¥ì ì¸ ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí–ˆì§€ë§Œ, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê·œëª¨ê°€ ì»¤ì§ì— ë”°ë¼ ê¸°ì¡´ ì„œë²„ë¦¬ìŠ¤ í”Œë«í¼ì€ ëª¨ë¸ ì‹œì‘ ì§€ì—° ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¹ ë¥¸ ëª¨ë¸ í™•ì¥ì„ ìœ„í•œ ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡  ì‹œìŠ¤í…œì¸ {\lambda}Scaleì„ ì œì•ˆí•©ë‹ˆë‹¤. {\lambda}Scaleì€ GPU ë…¸ë“œ ê°„ ê³ ì† RDMA ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ ë©€í‹°ìºìŠ¤íŠ¸ë¥¼ ê°€ì†í™”í•˜ê³ , ëª¨ë¸ ì „ì†¡ ì¤‘ ë¶„ì‚° ì¶”ë¡  ì‹¤í–‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” "execute-while-load" ë°©ì‹ì„ ë„ì…í•©ë‹ˆë‹¤. ë˜í•œ, {\lambda}Pipeë¼ëŠ” íš¨ìœ¨ì ì¸ ëª¨ë¸ í™•ì¥ ë°©ì‹ì„ í†µí•´ ì ì‘í˜• ëª¨ë¸ ë©€í‹°ìºìŠ¤íŠ¸ì™€ í˜‘ë ¥ì  ë¶„ì‚° ì¶”ë¡ ì„ ìœ„í•œ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. í‰ê°€ ê²°ê³¼, {\lambda}Scaleì€ ìµœëŒ€ 5ë°°ì˜ ì§€ì—° ì‹œê°„ ê°œì„ ê³¼ 31.3%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ë©°, ì‹¤ì œ LLM ì¶”ë¡  ì‘ì—…ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ë¶€í•˜ ê¸‰ì¦ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì„œë²„ë¦¬ìŠ¤ ì»´í“¨íŒ…ì€ í´ë¼ìš°ë“œ ê¸°ë°˜ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ìœ ë§í•œ ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.

- 2. {\lambda}Scaleì€ ë¹ ë¥¸ ëª¨ë¸ í™•ì¥ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡  ì‹œìŠ¤í…œìœ¼ë¡œ, GPU ë…¸ë“œ ê°„ì˜ ê³ ì† RDMA ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ ë©€í‹°ìºìŠ¤íŠ¸ë¥¼ ê°€ì†í™”í•œë‹¤.

- 3. {\lambda}Scaleì€ "execute-while-load" ë°©ì‹ì„ í†µí•´ ëª¨ë¸ ì „ì†¡ ì¤‘ ë¶„ì‚° ì¶”ë¡  ì‹¤í–‰ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

- 4. {\lambda}PipeëŠ” ì ì‘í˜• ëª¨ë¸ ë©€í‹°ìºìŠ¤íŠ¸ì™€ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ í˜‘ë ¥ì ì´ê³  ë¶„ì‚°ëœ ì¶”ë¡ ì„ ì§€ì›í•œë‹¤.

- 5. í‰ê°€ ê²°ê³¼, {\lambda}Scaleì€ ìµœëŒ€ 5ë°°ì˜ ì§€ì—° ì‹œê°„ ê°œì„ ê³¼ 31.3%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ë©°, ì‹¤ì œ LLM ì¶”ë¡  ì‘ì—…ì—ì„œ ë¶€í•˜ ê¸‰ì¦ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.

---

*Generated on 2025-09-19 16:24:00*