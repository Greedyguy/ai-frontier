
# Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility

**Korean Title:** ì´ì§„ ê°ë… í•˜ì—ì„œ í•™ìŠµëœ ì‹ ë¢°ë„ ë³´ì •ì˜ ì‹¤í–‰ ê°€ëŠ¥ì„±ì„ ë°˜ì¦í•˜ê¸°: ì •ë³´ ì´ë¡ ì  ë¶ˆê°€ëŠ¥ì„±

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Adaptive Multi-agent Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Post-Hoc_Split-Point_Self-Consistency_Verification_for_Efficient,_Unified_Quantification_of_Aleatoric_and_Epistemic_Uncertainty_in_Deep_Learning_20250918|Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning]] (80.7% similar)
- [[Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (78.3% similar)
- [[Accuracy Paradox in Large Language Models Regulating Hallucination Risks in Generative AI]] (78.2% similar)
- [[Leveraging_Geometric_Visual_Illusions_as_Perceptual_Inductive_Biases_for_Vision_Models_20250919|Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models]] (77.7% similar)
- [[From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (77.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14386v1 Announce Type: new 
Abstract: We prove a fundamental impossibility theorem: neural networks cannot simultaneously learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision. Through rigorous mathematical analysis and comprehensive empirical evaluation spanning negative reward training, symmetric loss functions, and post-hoc calibration methods, we demonstrate this is an information-theoretic constraint, not a methodological failure. Our experiments reveal universal failure patterns: negative rewards produce extreme underconfidence (ECE greater than 0.8) while destroying confidence diversity (std less than 0.05), symmetric losses fail to escape binary signal averaging, and post-hoc methods achieve calibration (ECE less than 0.02) only by compressing the confidence distribution. We formalize this as an underspecified mapping problem where binary signals cannot distinguish between different confidence levels for correct predictions: a 60 percent confident correct answer receives identical supervision to a 90 percent confident one. Crucially, our real-world validation shows 100 percent failure rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while post-hoc calibration's 33 percent success rate paradoxically confirms our theorem by achieving calibration through transformation rather than learning. This impossibility directly explains neural network hallucinations and establishes why post-hoc calibration is mathematically necessary, not merely convenient. We propose novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning that could overcome these fundamental limitations without requiring human confidence annotations.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14386v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ìš°ë¦¬ëŠ” ê·¼ë³¸ì ì¸ ë¶ˆê°€ëŠ¥ì„± ì •ë¦¬ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤. ì‹ ê²½ë§ì€ ì´ì§„ ì •/ì˜¤ë‹µ ê°ë…ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ë•Œ ì˜ë¯¸ ìˆëŠ” ë‹¤ì–‘ì„±ì„ ê°€ì§„ ì˜ ì¡°ì •ëœ ì‹ ë¢°ë„ ì¶”ì •ì¹˜ë¥¼ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—„ê²©í•œ ìˆ˜í•™ì  ë¶„ì„ê³¼ ë¶€ì •ì  ë³´ìƒ í›ˆë ¨, ëŒ€ì¹­ ì†ì‹¤ í•¨ìˆ˜, ì‚¬í›„ ë³´ì • ë°©ë²•ì„ ì•„ìš°ë¥´ëŠ” í¬ê´„ì ì¸ ì‹¤í—˜ í‰ê°€ë¥¼ í†µí•´, ì´ëŠ” ë°©ë²•ë¡ ì  ì‹¤íŒ¨ê°€ ì•„ë‹Œ ì •ë³´ ì´ë¡ ì  ì œì•½ì„ì„ ì¦ëª…í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ë³´í¸ì ì¸ ì‹¤íŒ¨ íŒ¨í„´ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤: ë¶€ì •ì  ë³´ìƒì€ ê·¹ë‹¨ì ì¸ ê³¼ì†Œ ì‹ ë¢°ë„(ECEê°€ 0.8 ì´ìƒ)ë¥¼ ì´ˆë˜í•˜ë©° ì‹ ë¢°ë„ì˜ ë‹¤ì–‘ì„±ì„ íŒŒê´´í•©ë‹ˆë‹¤(í‘œì¤€í¸ì°¨ê°€ 0.05 ë¯¸ë§Œ). ëŒ€ì¹­ ì†ì‹¤ì€ ì´ì§„ ì‹ í˜¸ í‰ê· í™”ë¥¼ ë²—ì–´ë‚˜ì§€ ëª»í•˜ë©°, ì‚¬í›„ ë°©ë²•ì€ ì‹ ë¢°ë„ ë¶„í¬ë¥¼ ì••ì¶•í•˜ì—¬ ë³´ì •(ECEê°€ 0.02 ë¯¸ë§Œ)ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¥¼ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì‹ ë¢° ìˆ˜ì¤€ì„ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ì´ì§„ ì‹ í˜¸ì˜ ë¶ˆì¶©ë¶„í•œ ë§¤í•‘ ë¬¸ì œë¡œ í˜•ì‹í™”í•©ë‹ˆë‹¤: 60% í™•ì‹ ì˜ ì •ë‹µê³¼ 90% í™•ì‹ ì˜ ì •ë‹µì€ ë™ì¼í•œ ê°ë…ì„ ë°›ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€, ìš°ë¦¬ì˜ ì‹¤ì œ ê²€ì¦ì—ì„œ MNIST, Fashion-MNIST, CIFAR-10ì— ëŒ€í•œ ëª¨ë“  í›ˆë ¨ ë°©ë²•ì˜ 100% ì‹¤íŒ¨ìœ¨ì„ ë³´ì—¬ì£¼ë©°, ì‚¬í›„ ë³´ì •ì˜ 33% ì„±ê³µë¥ ì€ í•™ìŠµì´ ì•„ë‹Œ ë³€í™˜ì„ í†µí•œ ë³´ì •ì„ ë‹¬ì„±í•¨ìœ¼ë¡œì¨ ì—­ì„¤ì ìœ¼ë¡œ ìš°ë¦¬ì˜ ì •ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ ë¶ˆê°€ëŠ¥ì„±ì€ ì‹ ê²½ë§ í™˜ê°ì„ ì§ì ‘ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©°, ì‚¬í›„ ë³´ì •ì´ ìˆ˜í•™ì ìœ¼ë¡œ í•„ìš”í•¨ì„ í™•ë¦½í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¸ê°„ì˜ ì‹ ë¢°ë„ ì£¼ì„ì„ ìš”êµ¬í•˜ì§€ ì•Šê³  ì´ëŸ¬í•œ ê·¼ë³¸ì ì¸ í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆëŠ” ì•™ìƒë¸” ë¶ˆì¼ì¹˜ì™€ ì ì‘í˜• ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í•™ìŠµì„ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ ê°ë… íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹ ê²½ë§ì´ ì´ì§„ ì •ì˜¤ í”¼ë“œë°±ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•  ë•Œ, ì˜ ì¡°ì •ëœ ì‹ ë¢°ë„ ì¶”ì •ê³¼ ì˜ë¯¸ ìˆëŠ” ë‹¤ì–‘ì„±ì„ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ì—†ë‹¤ëŠ” ë¶ˆê°€ëŠ¥ì„± ì •ë¦¬ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤. ìˆ˜í•™ì  ë¶„ì„ê³¼ ì‹¤í—˜ì„ í†µí•´ ì´ëŠ” ë°©ë²•ë¡ ì  ì‹¤íŒ¨ê°€ ì•„ë‹Œ ì •ë³´ ì´ë¡ ì  ì œì•½ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë¶€ì •ì  ë³´ìƒì€ ê·¹ë‹¨ì ì¸ ê³¼ì†Œ ì‹ ë¢°ë¥¼ ì´ˆë˜í•˜ê³ , ëŒ€ì¹­ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì´ì§„ ì‹ í˜¸ í‰ê· í™”ë¥¼ ë²—ì–´ë‚˜ì§€ ëª»í•˜ë©°, ì‚¬í›„ ë³´ì • ë°©ë²•ì€ ì‹ ë¢°ë„ ë¶„í¬ë¥¼ ì••ì¶•í•˜ì—¬ ë³´ì •ë§Œì„ ë‹¬ì„±í•©ë‹ˆë‹¤. MNIST, Fashion-MNIST, CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  í•™ìŠµ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìœ¼ë©°, ì‚¬í›„ ë³´ì •ì˜ 33% ì„±ê³µë¥ ì€ í•™ìŠµì´ ì•„ë‹Œ ë³€í™˜ì„ í†µí•´ ë³´ì •ì´ ì´ë£¨ì–´ì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì‹ ê²½ë§ì˜ í™˜ê° í˜„ìƒì„ ì„¤ëª…í•˜ë©°, ì‚¬í›„ ë³´ì •ì˜ ìˆ˜í•™ì  í•„ìš”ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ê°ë… íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ì•™ìƒë¸” ë¶ˆì¼ì¹˜ì™€ ì ì‘í˜• ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í•™ìŠµì„ ì œì•ˆí•˜ì—¬ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆìŒì„ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹ ê²½ë§ì€ ì´ì§„ ì •ë‹µ/ì˜¤ë‹µ ê°ë… í•˜ì— í•™ìŠµí•  ë•Œ, ì˜ ì¡°ì •ëœ ì‹ ë¢°ë„ ì¶”ì •ê³¼ ì˜ë¯¸ ìˆëŠ” ë‹¤ì–‘ì„±ì„ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ì—†ë‹¤.

- 2. ë¶€ì •ì  ë³´ìƒì€ ê·¹ë‹¨ì ì¸ ê³¼ì†Œì‹ ë¢°ë¥¼ ìœ ë°œí•˜ë©°, ì‹ ë¢°ë„ ë‹¤ì–‘ì„±ì„ íŒŒê´´í•œë‹¤.

- 3. ëŒ€ì¹­ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì´ì§„ ì‹ í˜¸ í‰ê· í™”ë¥¼ ë²—ì–´ë‚˜ì§€ ëª»í•œë‹¤.

- 4. ì‚¬í›„ ë³´ì • ë°©ë²•ì€ ì‹ ë¢°ë„ ë¶„í¬ë¥¼ ì••ì¶•í•¨ìœ¼ë¡œì¨ë§Œ ë³´ì •ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.

- 5. ìƒˆë¡œìš´ ê°ë… íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ ì•™ìƒë¸” ë¶ˆì¼ì¹˜ì™€ ì ì‘í˜• ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í•™ìŠµì„ ì œì•ˆí•œë‹¤.

---

*Generated on 2025-09-19 15:23:46*