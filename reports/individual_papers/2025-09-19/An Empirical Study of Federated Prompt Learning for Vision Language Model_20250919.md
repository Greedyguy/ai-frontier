
# An Empirical Study of Federated Prompt Learning for Vision Language Model

**Korean Title:** ì—°í•© í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ ìœ„í•œ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì‹¤ì¦ì  ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Vision Prompt Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[An Empirical Analysis of VLM-based OOD Detection Mechanisms, Advantages, and Sensitivity]] (82.6% similar)
- [[Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations]] (81.9% similar)
- [[Communication-Efficient and Privacy-Adaptable Mech_20250919|Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning]] (81.7% similar)
- [[Visible Yet Unreadable A Systematic Blind Spot of Vision Language Models Across Writing Systems]] (80.8% similar)
- [[A_Multi-Agent_LLM_Defense_Pipeline_Against_Prompt_Injection_Attacks_20250919|A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks]] (80.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.23024v2 Announce Type: replace 
Abstract: The Vision Language Model (VLM) excels in aligning vision and language representations, and prompt learning has emerged as a key technique for adapting such models to downstream tasks. However, the application of prompt learning with VLM in federated learning (FL) scenarios remains underexplored. This paper systematically investigates the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under data heterogeneity challenges, including label skew and domain shift. We conduct extensive experiments to evaluate the impact of various FL and prompt configurations, such as client scale, aggregation strategies, and prompt length, to assess the robustness of Federated Prompt Learning (FPL). Furthermore, we explore strategies for enhancing prompt learning in complex scenarios where label skew and domain shift coexist, including leveraging both prompt types when computational resources allow. Our findings offer practical insights into optimizing prompt learning in federated settings, contributing to the broader deployment of VLMs in privacy-preserving environments.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2505.23024v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ë¹„ì „ ì–¸ì–´ ëª¨ë¸(Vision Language Model, VLM)ì€ ë¹„ì „ê³¼ ì–¸ì–´ í‘œí˜„ì„ ì •ë ¬í•˜ëŠ” ë° ë›°ì–´ë‚˜ë©°, í”„ë¡¬í”„íŠ¸ í•™ìŠµì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘ì‹œí‚¤ëŠ” ì£¼ìš” ê¸°ìˆ ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—°í•© í•™ìŠµ(FL) ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ VLMê³¼ í•¨ê»˜ í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ ì ìš©í•˜ëŠ” ê²ƒì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ë ˆì´ë¸” í¸í–¥ê³¼ ë„ë©”ì¸ ì´ë™ì„ í¬í•¨í•œ ë°ì´í„° ì´ì§ˆì„± ë¬¸ì œ í•˜ì—ì„œ ì–¸ì–´ í”„ë¡¬í”„íŠ¸ í•™ìŠµ(LPT)ê³¼ ë¹„ì „ í”„ë¡¬í”„íŠ¸ í•™ìŠµ(VPT) ê°„ì˜ í–‰ë™ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í´ë¼ì´ì–¸íŠ¸ ê·œëª¨, ì§‘ê³„ ì „ëµ, í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì™€ ê°™ì€ ë‹¤ì–‘í•œ FL ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì˜ ì˜í–¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì—¬ ì—°í•© í”„ë¡¬í”„íŠ¸ í•™ìŠµ(FPL)ì˜ ê²¬ê³ ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ, ë ˆì´ë¸” í¸í–¥ê³¼ ë„ë©”ì¸ ì´ë™ì´ ê³µì¡´í•˜ëŠ” ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì „ëµì„ íƒêµ¬í•˜ë©°, ê³„ì‚° ìì›ì´ í—ˆìš©ë  ë•Œ ë‘ í”„ë¡¬í”„íŠ¸ ìœ í˜•ì„ ëª¨ë‘ í™œìš©í•˜ëŠ” ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•˜ëŠ” í™˜ê²½ì—ì„œ VLMì˜ ë³´ë‹¤ ê´‘ë²”ìœ„í•œ ë°°í¬ì— ê¸°ì—¬í•˜ë©°, ì—°í•© ì„¤ì •ì—ì„œ í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ì‹¤ì§ˆì ì¸ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì—ì„œ í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ ì—°í•© í•™ìŠµ(FL) ì‹œë‚˜ë¦¬ì˜¤ì— ì ìš©í•˜ëŠ” ê²ƒì— ëŒ€í•œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë°ì´í„°ì˜ ë¶ˆê· í˜• ë¬¸ì œì¸ ë ˆì´ë¸” í¸í–¥ê³¼ ë„ë©”ì¸ ì´ë™ ìƒí™©ì—ì„œ ì–¸ì–´ í”„ë¡¬í”„íŠ¸ í•™ìŠµ(LPT)ê³¼ ë¹„ì „ í”„ë¡¬í”„íŠ¸ í•™ìŠµ(VPT)ì˜ í–‰ë™ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ FL ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •, í´ë¼ì´ì–¸íŠ¸ ê·œëª¨, ì§‘ê³„ ì „ëµ, í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë“±ì„ í†µí•´ ì—°í•© í”„ë¡¬í”„íŠ¸ í•™ìŠµ(FPL)ì˜ ê°•ê±´ì„±ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë ˆì´ë¸” í¸í–¥ê³¼ ë„ë©”ì¸ ì´ë™ì´ ê³µì¡´í•˜ëŠ” ë³µì¡í•œ ìƒí™©ì—ì„œ í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì „ëµì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•˜ëŠ” í™˜ê²½ì—ì„œ VLMì˜ í™œìš©ì„ ìµœì í™”í•˜ëŠ” ë° ì‹¤ì§ˆì ì¸ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision Language Model(VLM)ì€ ì‹œê°ê³¼ ì–¸ì–´ í‘œí˜„ì„ ì •ë ¬í•˜ëŠ” ë° ë›°ì–´ë‚˜ë©°, í”„ë¡¬í”„íŠ¸ í•™ìŠµì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘ì‹œí‚¤ëŠ” í•µì‹¬ ê¸°ìˆ ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.

- 2. í”„ë¡¬í”„íŠ¸ í•™ìŠµê³¼ VLMì˜ ì—°í•© í•™ìŠµ(FL) ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì ìš©ì€ ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì€ ë°ì´í„° ì´ì§ˆì„± ë¬¸ì œ(ë ˆì´ë¸” í¸í–¥ ë° ë„ë©”ì¸ ì´ë™ í¬í•¨) í•˜ì—ì„œ ì–¸ì–´ í”„ë¡¬í”„íŠ¸ í•™ìŠµ(LPT)ê³¼ ì‹œê° í”„ë¡¬í”„íŠ¸ í•™ìŠµ(VPT)ì˜ í–‰ë™ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤.

- 4. ë‹¤ì–‘í•œ FL ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±(í´ë¼ì´ì–¸íŠ¸ ê·œëª¨, ì§‘ê³„ ì „ëµ, í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë“±)ì˜ ì˜í–¥ì„ í‰ê°€í•˜ì—¬ ì—°í•© í”„ë¡¬í”„íŠ¸ í•™ìŠµ(FPL)ì˜ ê°•ê±´ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.

- 5. ë ˆì´ë¸” í¸í–¥ê³¼ ë„ë©”ì¸ ì´ë™ì´ ê³µì¡´í•˜ëŠ” ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í”„ë¡¬í”„íŠ¸ í•™ìŠµì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì „ëµì„ íƒêµ¬í•˜ë©°, ì´ëŠ” í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•˜ëŠ” í™˜ê²½ì—ì„œ VLMì˜ í­ë„“ì€ ë°°í¬ì— ê¸°ì—¬í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:39:41*