---
keywords:
  - Large Language Models
  - SeeAct Framework
  - Web Agents
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14382
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:48:40.932353",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "SeeAct Framework",
    "Web Agents"
  ],
  "rejected_keywords": [
    "Error Analysis",
    "Mind2Web Dataset"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "SeeAct Framework": 0.78,
    "Web Agents": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents

**Korean Title:** 웹 에이전트의 세밀한 분석을 통한 파이프라인 실패 감지

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**⚡ Unique Technical**: [[keywords/SeeAct Framework|SeeAct Framework]], [[keywords/Web Agents|Web Agents]]
**🚀 Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14382v1 Announce Type: new 
Abstract: Web agents powered by large language models (LLMs) can autonomously perform complex, multistep tasks in dynamic web environments. However, current evaluations mostly focus on the overall success while overlooking intermediate errors. This limits insight into failure modes and hinders systematic improvement. This work analyzes existing benchmarks and highlights the lack of fine-grained diagnostic tools. To address this gap, we propose a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis. Using the SeeAct framework and the Mind2Web dataset as a case study, we show how this approach reveals actionable weaknesses missed by standard metrics - paving the way for more robust and generalizable web agents.

## 🔍 Abstract (한글 번역)

arXiv:2509.14382v1 발표 유형: 신규  
초록: 대형 언어 모델(LLMs)로 구동되는 웹 에이전트는 동적 웹 환경에서 복잡하고 다단계의 작업을 자율적으로 수행할 수 있습니다. 그러나 현재의 평가들은 주로 전체적인 성공에 초점을 맞추고 중간 오류를 간과하고 있습니다. 이는 실패 모드에 대한 통찰을 제한하고 체계적인 개선을 방해합니다. 본 연구는 기존 벤치마크를 분석하고 세밀한 진단 도구의 부족을 강조합니다. 이러한 격차를 해결하기 위해, 우리는 에이전트 파이프라인을 해석 가능한 단계로 분해하여 상세한 오류 분석을 가능하게 하는 모듈식 평가 프레임워크를 제안합니다. SeeAct 프레임워크와 Mind2Web 데이터셋을 사례 연구로 사용하여, 이 접근 방식이 표준 지표에 의해 놓친 실행 가능한 약점을 어떻게 드러내는지를 보여주며, 보다 견고하고 일반화 가능한 웹 에이전트를 위한 길을 열어줍니다.

## 📝 요약

이 논문은 대형 언어 모델(LLMs)을 활용한 웹 에이전트의 평가 방식에 대한 문제점을 지적하고, 이를 개선하기 위한 모듈형 평가 프레임워크를 제안합니다. 기존 평가 방식은 전체 성공률에 집중하여 중간 오류를 간과하는 경향이 있으며, 이는 실패 원인 분석과 체계적인 개선을 방해합니다. 제안된 프레임워크는 에이전트의 작업 단계를 해석 가능한 단계로 분해하여 세부적인 오류 분석을 가능하게 합니다. SeeAct 프레임워크와 Mind2Web 데이터셋을 사례로 사용하여, 이 접근 방식이 기존 지표로는 발견하기 어려운 취약점을 드러낼 수 있음을 보여줍니다. 이를 통해 더 견고하고 일반화 가능한 웹 에이전트 개발의 길을 열고자 합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)을 활용한 웹 에이전트는 복잡한 다단계 작업을 자동으로 수행할 수 있다.

- 2. 현재의 평가 방식은 전체적인 성공에 집중하며 중간 오류를 간과하는 경향이 있다.

- 3. 기존 벤치마크의 미세한 진단 도구 부족 문제를 강조하고 있다.

- 4. 에이전트 파이프라인을 해석 가능한 단계로 분해하여 오류 분석을 수행하는 모듈형 평가 프레임워크를 제안한다.

- 5. SeeAct 프레임워크와 Mind2Web 데이터셋을 활용하여 표준 지표로는 발견되지 않는 취약점을 드러낼 수 있음을 보여준다.

---

*Generated on 2025-09-19 18:00:30*