
# Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images

**Korean Title:** íŒ¨ìŠ¤íŠ¸ ë©€í‹°í´ ì–´í…ì…˜: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ë‹¤ë‹¨ê³„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Multilevel Attention

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Superpose_Task-specific_Features_for_Model_Merging_20250919|Superpose Task-specific Features for Model Merging]] (79.2% similar)
- [[MapAnything Universal Feed-Forward Metric 3D Reconstruction]] (78.7% similar)
- [[FAWN A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference]] (78.4% similar)
- [[Masked_Feature_Modeling_Enhances_Adaptive_Segmentation_20250918|Masked Feature Modeling Enhances Adaptive Segmentation]] (78.3% similar)
- [[Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (78.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2310.11960v4 Announce Type: replace-cross 
Abstract: While Transformer networks benefit from a global receptive field, their quadratic cost relative to sequence length restricts their application to long sequences and high-resolution inputs. We introduce Fast Multipole Attention (FMA), a divide-and-conquer mechanism for self-attention inspired by the Fast Multipole Method from n-body physics. FMA reduces the time and memory complexity of self-attention from $\mathcal{O}\left(n^2\right)$ to $\mathcal{O}(n \log n)$ and $\mathcal{O}(n)$ while preserving full-context interactions.
  FMA contains a learned hierarchy with $\mathcal{O}(\log n)$ levels of resolution. In this hierarchy, nearby tokens interact at full resolution, while distant tokens engage through progressively coarser, learned basis functions. We have developed both 1D and 2D implementations of FMA for language and vision tasks, respectively. On autoregressive and bidirectional language modeling benchmarks, the 1D variant either matches or outperforms leading efficient attention baselines with substantially lower memory use. With linear complexity, the 2D variant demonstrates superior performance over strong vision transformer baselines in classification and semantic segmentation tasks.
  Our results confirm that the multilevel attention implemented by FMA allows Transformer-based models to scale to much longer sequences and higher-resolution inputs without loss in accuracy. This provides a principled, physics-inspired approach for developing scalable neural networks suitable for language, vision, and multimodal tasks. Our code will be available at https://github.com/epoch98/FMA.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2310.11960v4 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: Transformer ë„¤íŠ¸ì›Œí¬ëŠ” ì „ì—­ ìˆ˜ìš© ì˜ì—­ì˜ ì´ì ì„ ëˆ„ë¦¬ì§€ë§Œ, ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ ì´ì°¨ ë¹„ìš©ìœ¼ë¡œ ì¸í•´ ê¸´ ì‹œí€€ìŠ¤ì™€ ê³ í•´ìƒë„ ì…ë ¥ì— ëŒ€í•œ ì ìš©ì´ ì œí•œë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” n-ì²´ ë¬¼ë¦¬í•™ì˜ ë¹ ë¥¸ ë‹¤ì¤‘ê·¹ ë°©ë²•(Fast Multipole Method)ì—ì„œ ì˜ê°ì„ ë°›ì€ ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì¸ ë¹ ë¥¸ ë‹¤ì¤‘ê·¹ ì£¼ì˜(Fast Multipole Attention, FMA)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. FMAëŠ” ìê¸° ì£¼ì˜ì˜ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ë³µì¡ì„±ì„ $\mathcal{O}\left(n^2\right)$ì—ì„œ $\mathcal{O}(n \log n)$ ë° $\mathcal{O}(n)$ìœ¼ë¡œ ì¤„ì´ë©´ì„œ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìƒí˜¸ì‘ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤.  
FMAëŠ” $\mathcal{O}(\log n)$ ìˆ˜ì¤€ì˜ í•´ìƒë„ë¥¼ ê°€ì§„ í•™ìŠµëœ ê³„ì¸µ êµ¬ì¡°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ ê³„ì¸µ êµ¬ì¡°ì—ì„œ, ì¸ì ‘í•œ í† í°ì€ ì „ì²´ í•´ìƒë„ë¡œ ìƒí˜¸ì‘ìš©í•˜ë©°, ë¨¼ í† í°ì€ ì ì§„ì ìœ¼ë¡œ ë” ê±°ì¹œ í•™ìŠµëœ ê¸°ì € í•¨ìˆ˜ë“¤ì„ í†µí•´ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê°ê° ì–¸ì–´ ë° ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ 1D ë° 2D FMA êµ¬í˜„ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ìê°€íšŒê·€ ë° ì–‘ë°©í–¥ ì–¸ì–´ ëª¨ë¸ë§ ë²¤ì¹˜ë§ˆí¬ì—ì„œ, 1D ë³€í˜•ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìƒë‹¹íˆ ì ìœ¼ë©´ì„œë„ ì„ ë„ì ì¸ íš¨ìœ¨ì  ì£¼ì˜ ê¸°ì¤€ì„ ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì„ í˜• ë³µì¡ì„±ì„ ê°€ì§„ 2D ë³€í˜•ì€ ë¶„ë¥˜ ë° ì˜ë¯¸ë¡ ì  ë¶„í•  ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ëŠ” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  
ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” FMAì— ì˜í•´ êµ¬í˜„ëœ ë‹¤ì¤‘ ìˆ˜ì¤€ ì£¼ì˜ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì´ ì •í™•ë„ë¥¼ ìƒì§€ ì•Šê³  í›¨ì”¬ ë” ê¸´ ì‹œí€€ìŠ¤ì™€ ê³ í•´ìƒë„ ì…ë ¥ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆê²Œ í•¨ì„ í™•ì¸í•©ë‹ˆë‹¤. ì´ëŠ” ì–¸ì–´, ë¹„ì „ ë° ë‹¤ì¤‘ ëª¨ë‹¬ ì‘ì—…ì— ì í•©í•œ í™•ì¥ ê°€ëŠ¥í•œ ì‹ ê²½ë§ì„ ê°œë°œí•˜ê¸° ìœ„í•œ ì›ì¹™ì ì´ê³  ë¬¼ë¦¬í•™ì— ì˜ê°ì„ ë°›ì€ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/epoch98/FMAì—ì„œ ì œê³µë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸´ ì‹œí€€ìŠ¤ì™€ ê³ í•´ìƒë„ ì…ë ¥ì— ì í•©í•œ íš¨ìœ¨ì ì¸ Transformer ë„¤íŠ¸ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì €ìë“¤ì€ n-body ë¬¼ë¦¬í•™ì˜ Fast Multipole Methodì—ì„œ ì˜ê°ì„ ë°›ì•„ Fast Multipole Attention (FMA)ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. FMAëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ë³µì¡ë„ë¥¼ $\mathcal{O}(n^2)$ì—ì„œ $\mathcal{O}(n \log n)$ ë° $\mathcal{O}(n)$ìœ¼ë¡œ ì¤„ì´ë©´ì„œë„ ì „ì²´ ë¬¸ë§¥ ìƒí˜¸ì‘ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤. FMAëŠ” í•™ìŠµëœ ê³„ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ ì¸ì ‘ í† í°ì€ ë†’ì€ í•´ìƒë„ë¡œ, ë¨¼ í† í°ì€ ì ì§„ì ìœ¼ë¡œ ë‚®ì€ í•´ìƒë„ë¡œ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤. 1D ë° 2D êµ¬í˜„ì„ í†µí•´ ì–¸ì–´ ë° ë¹„ì „ ì‘ì—…ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” Transformer ê¸°ë°˜ ëª¨ë¸ì´ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë” ê¸´ ì‹œí€€ìŠ¤ì™€ ê³ í•´ìƒë„ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/epoch98/FMAì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Fast Multipole Attention (FMA)ëŠ” n-body ë¬¼ë¦¬í•™ì˜ ë¹ ë¥¸ ë‹¤ì¤‘ê·¹ ë°©ë²•ì—ì„œ ì˜ê°ì„ ë°›ì•„, ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ë³µì¡ë„ë¥¼ $\mathcal{O}(n^2)$ì—ì„œ $\mathcal{O}(n \log n)$ ë° $\mathcal{O}(n)$ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

- 2. FMAëŠ” í•™ìŠµëœ ê³„ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ ê°€ê¹Œìš´ í† í°ì€ ë†’ì€ í•´ìƒë„ë¡œ, ë¨¼ í† í°ì€ ì ì°¨ ê±°ì¹œ í•™ìŠµ ê¸°ë°˜ í•¨ìˆ˜ë¡œ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.

- 3. 1D FMAëŠ” ì–¸ì–´ ëª¨ë¸ë§ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ì˜ íš¨ìœ¨ì ì¸ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ ë§ì¶”ê±°ë‚˜ ëŠ¥ê°€í•©ë‹ˆë‹¤.

- 4. 2D FMAëŠ” ì„ í˜• ë³µì¡ì„±ì„ ê°€ì§€ë©°, ë¶„ë¥˜ ë° ì˜ë¯¸ ë¶„í•  ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ì¤€ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

- 5. FMAëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì´ ë” ê¸´ ì‹œí€€ìŠ¤ì™€ ê³ í•´ìƒë„ ì…ë ¥ì„ ì •í™•ë„ ì†ì‹¤ ì—†ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬, ì–¸ì–´, ë¹„ì „ ë° ë‹¤ì¤‘ ëª¨ë‹¬ ì‘ì—…ì— ì í•©í•œ í™•ì¥ ê°€ëŠ¥í•œ ì‹ ê²½ë§ ê°œë°œì„ ìœ„í•œ ë¬¼ë¦¬í•™ì—ì„œ ì˜ê°ì„ ë°›ì€ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:42:29*