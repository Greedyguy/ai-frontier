
# Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection

**Korean Title:** 온라인 악의적 의도 탐지를 위한 적대적 증류 검색 증강 보호 모델

## 📋 메타데이터

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: Adversarial Training, Knowledge Distillation

## 🔗 유사한 논문
- [[Evaluating_and_Improving_the_Robustness_of_Security_Attack_Detectors_Generated_by_LLMs_20250918|Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs]] (84.7% similar)
- [[Enterprise_AI_Must_Enforce_Participant-Aware_Access_Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (84.1% similar)
- [[Enhancing_Retrieval_Augmentation_via_Adversarial_Collaboration_20250919|Enhancing Retrieval Augmentation via Adversarial Collaboration]] (84.0% similar)
- [[DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (83.5% similar)
- [[CyberLLMInstruct A Pseudo-malicious Dataset Revealing Safety-performance Trade-offs in Cyber Security LLM Fine-tuning]] (83.3% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14622v1 Announce Type: cross 
Abstract: With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.

## 🔍 Abstract (한글 번역)

arXiv:2509.14622v1 발표 유형: 교차  
초록: 대형 언어 모델(LLM)이 인터랙티브 애플리케이션에 배포됨에 따라 온라인 악의적 의도 탐지가 점점 더 중요해지고 있습니다. 그러나 기존 접근 방식은 다양한 복잡한 사용자 쿼리를 실시간으로 처리하는 데 한계가 있습니다. 이러한 문제를 해결하기 위해 우리는 강력하고 효율적인 온라인 악의적 의도 탐지를 위한 2단계 프레임워크인 ADRAG(Adversarial Distilled Retrieval-Augmented Guard)를 소개합니다. 훈련 단계에서는 고용량의 교사 모델이 적대적으로 변형된 검색 보강 입력에 대해 훈련되어 다양한 복잡한 사용자 쿼리에 대한 강력한 결정 경계를 학습합니다. 추론 단계에서는 증류 스케줄러가 교사의 지식을 압축된 학생 모델로 전이하며, 온라인으로 수집된 지속적으로 업데이트되는 지식 기반을 사용합니다. 배포 시, 압축된 학생 모델은 온라인으로 업데이트된 지식 기반에서 검색된 상위-K 유사 안전 예제를 활용하여 온라인 및 실시간 악의적 쿼리 탐지를 가능하게 합니다. 열 가지 안전 벤치마크에 대한 평가 결과, ADRAG는 149M-파라미터 모델로 WildGuard-7B의 성능의 98.5%를 달성하고, GPT-4를 3.3% 및 Llama-Guard-3-8B를 9.5% 초과하며, 동시에 실시간 애플리케이션에서 초당 300 쿼리(QPS)에서 최대 5.6배 낮은 지연 시간을 제공합니다.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 상호작용적 응용에서 온라인 악의적 의도 탐지의 중요성을 강조하며, 이를 해결하기 위해 ADRAG라는 이단계 프레임워크를 제안합니다. ADRAG는 훈련 단계에서 고용량 교사 모델을 사용하여 다양한 사용자 쿼리에 대한 강력한 결정 경계를 학습하고, 추론 단계에서 이를 소형 학생 모델로 전이합니다. 이 학생 모델은 온라인으로 업데이트되는 지식 기반에서 유사한 안전 예제를 검색하여 실시간 악의적 쿼리 탐지를 수행합니다. ADRAG는 149M 파라미터 모델로 WildGuard-7B 성능의 98.5%를 달성하며, GPT-4와 Llama-Guard-3-8B를 각각 3.3%와 9.5% 초과하는 성능을 보입니다. 또한, 300 QPS에서 최대 5.6배 낮은 지연 시간을 제공합니다.

## 🎯 주요 포인트

- 1. ADRAG는 온라인 악의적 의도 탐지를 위한 강력하고 효율적인 이단계 프레임워크입니다.

- 2. 훈련 단계에서는 고용량 교사 모델이 다양한 사용자 쿼리에 대한 강력한 결정 경계를 학습합니다.

- 3. 추론 단계에서는 교사의 지식을 압축된 학생 모델로 이전하여 실시간 악의적 쿼리 탐지를 가능하게 합니다.

- 4. ADRAG는 149M-파라미터 모델로 WildGuard-7B의 성능의 98.5%를 달성하며, GPT-4와 Llama-Guard-3-8B를 각각 3.3% 및 9.5% 초과합니다.

- 5. 실시간 애플리케이션에서 300 QPS에서 최대 5.6배 낮은 지연 시간을 제공합니다.

---

*Generated on 2025-09-19 14:58:36*