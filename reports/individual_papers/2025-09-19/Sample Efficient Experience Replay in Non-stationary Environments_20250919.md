
# Sample Efficient Experience Replay in Non-stationary Environments

**Korean Title:** ë¹„ì •ìƒ í™˜ê²½ì—ì„œì˜ ìƒ˜í”Œ íš¨ìœ¨ì  ê²½í—˜ ì¬ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Adaptive Experience Replay

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation]] (78.8% similar)
- [[Self-Guided_Function_Calling_in_Large_Language_Models_via_Stepwise_Experience_Recall_20250918|Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall]] (78.4% similar)
- [[MIMIC-D Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies]] (78.3% similar)
- [[DECAMP Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training]] (77.9% similar)
- [[Video-Language Critic Transferable Reward Functions for Language-Conditioned Robotics]] (77.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15032v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as changing dynamics and rewards quickly make past experiences outdated. Traditional experience replay (ER) methods, especially those using TD-error prioritization, struggle to distinguish between changes caused by the agent's policy and those from the environment, resulting in inefficient learning under dynamic conditions. To address this challenge, we propose the Discrepancy of Environment Dynamics (DoE), a metric that isolates the effects of environment shifts on value functions. Building on this, we introduce Discrepancy of Environment Prioritized Experience Replay (DEER), an adaptive ER framework that prioritizes transitions based on both policy updates and environmental changes. DEER uses a binary classifier to detect environment changes and applies distinct prioritization strategies before and after each shift, enabling more sample-efficient learning. Experiments on four non-stationary benchmarks demonstrate that DEER further improves the performance of off-policy algorithms by 11.54 percent compared to the best-performing state-of-the-art ER methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15032v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë¹„ì •ìƒ í™˜ê²½ì—ì„œì˜ ê°•í™” í•™ìŠµ(RL)ì€ ë„ì „ì ì…ë‹ˆë‹¤. ë³€í™”í•˜ëŠ” ë™íƒœì™€ ë³´ìƒì€ ê³¼ê±°ì˜ ê²½í—˜ì„ ë¹ ë¥´ê²Œ êµ¬ì‹œí™”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì „í†µì ì¸ ê²½í—˜ ì¬ìƒ(ER) ë°©ë²•, íŠ¹íˆ TD-ì˜¤ë¥˜ ìš°ì„ ìˆœìœ„ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ì—ì´ì „íŠ¸ì˜ ì •ì±…ì— ì˜í•´ ë°œìƒí•œ ë³€í™”ì™€ í™˜ê²½ìœ¼ë¡œë¶€í„°ì˜ ë³€í™”ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ì´ëŠ” ë™ì  ì¡°ê±´ í•˜ì—ì„œ ë¹„íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ê°€ì¹˜ í•¨ìˆ˜ì— ëŒ€í•œ í™˜ê²½ ë³€í™”ì˜ ì˜í–¥ì„ ë¶„ë¦¬í•˜ëŠ” ì²™ë„ì¸ í™˜ê²½ ë™íƒœ ë¶ˆì¼ì¹˜(DoE)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì •ì±… ì—…ë°ì´íŠ¸ì™€ í™˜ê²½ ë³€í™”ë¥¼ ëª¨ë‘ ê¸°ë°˜ìœ¼ë¡œ ì „ì´ë¥¼ ìš°ì„ ì‹œí•˜ëŠ” ì ì‘í˜• ER í”„ë ˆì„ì›Œí¬ì¸ í™˜ê²½ ìš°ì„ ìˆœìœ„ ê²½í—˜ ì¬ìƒ(DEER)ì„ ì†Œê°œí•©ë‹ˆë‹¤. DEERì€ í™˜ê²½ ë³€í™”ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•´ ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ê³ , ê° ë³€í™” ì „í›„ì— êµ¬ë³„ëœ ìš°ì„ ìˆœìœ„ ì „ëµì„ ì ìš©í•˜ì—¬ ë” ìƒ˜í”Œ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë„¤ ê°€ì§€ ë¹„ì •ìƒ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ì€ DEERì´ ìµœì‹  ER ë°©ë²• ì¤‘ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ë°©ë²•ì— ë¹„í•´ ì˜¤í”„ í´ë¦¬ì‹œ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ 11.54% í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë¹„ì •ìƒì ì¸ í™˜ê²½ì—ì„œì˜ ê°•í™” í•™ìŠµì€ ë³€í™”í•˜ëŠ” ë™ì  ë° ë³´ìƒ ì²´ê³„ë¡œ ì¸í•´ ê³¼ê±° ê²½í—˜ì´ ë¹ ë¥´ê²Œ ë¬´ìš©ì§€ë¬¼ì´ ë˜ì–´ ë„ì „ì ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê²½í—˜ ì¬ìƒ(ER) ë°©ë²•, íŠ¹íˆ TD-ì˜¤ë¥˜ ìš°ì„ ìˆœìœ„í™” ê¸°ë²•ì€ ì—ì´ì „íŠ¸ì˜ ì •ì±… ë³€í™”ì™€ í™˜ê²½ ë³€í™”ì˜ êµ¬ë¶„ì´ ì–´ë ¤ì›Œ ë¹„íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í™˜ê²½ ë™ì  ë¶ˆì¼ì¹˜(DoE)ë¼ëŠ” ë©”íŠ¸ë¦­ì„ ì œì•ˆí•˜ì—¬ í™˜ê²½ ë³€í™”ê°€ ê°€ì¹˜ í•¨ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì •ì±… ì—…ë°ì´íŠ¸ì™€ í™˜ê²½ ë³€í™”ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ìš°ì„ ìˆœìœ„ ê²½í—˜ ì¬ìƒ í”„ë ˆì„ì›Œí¬ì¸ DEERë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. DEERëŠ” ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•´ í™˜ê²½ ë³€í™”ë¥¼ ê°ì§€í•˜ê³ , ë³€í™” ì „í›„ì— ë‹¤ë¥¸ ìš°ì„ ìˆœìœ„ ì „ëµì„ ì ìš©í•˜ì—¬ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ë„¤ ê°€ì§€ ë¹„ì •ìƒì  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ DEERëŠ” ìµœì‹  ER ë°©ë²• ëŒ€ë¹„ ì„±ëŠ¥ì„ 11.54% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ì •ìƒ í™˜ê²½ì—ì„œì˜ ê°•í™” í•™ìŠµì€ ë³€í™”í•˜ëŠ” ë™ì  ë° ë³´ìƒìœ¼ë¡œ ì¸í•´ ê³¼ê±° ê²½í—˜ì´ ë¹ ë¥´ê²Œ ë¬´ìš©ì§€ë¬¼ì´ ë˜ì–´ ë„ì „ì ì…ë‹ˆë‹¤.

- 2. ì „í†µì ì¸ ê²½í—˜ ì¬ìƒ ë°©ë²•ì€ ì—ì´ì „íŠ¸ì˜ ì •ì±… ë³€í™”ì™€ í™˜ê²½ ë³€í™”ì˜ êµ¬ë¶„ì´ ì–´ë ¤ì›Œ ë¹„íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì´ˆë˜í•©ë‹ˆë‹¤.

- 3. ìš°ë¦¬ëŠ” í™˜ê²½ ë³€í™”ê°€ ê°€ì¹˜ í•¨ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ë¦¬í•˜ëŠ” í™˜ê²½ ë™ì  ë¶ˆì¼ì¹˜(DoE) ë©”íŠ¸ë¦­ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 4. DEERì€ ì •ì±… ì—…ë°ì´íŠ¸ì™€ í™˜ê²½ ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜ì„ ìš°ì„ ì‹œí•˜ëŠ” ì ì‘í˜• ê²½í—˜ ì¬ìƒ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 5. DEERì€ ë¹„ì •ìƒì ì¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì˜¤í”„ í´ë¦¬ì‹œ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ìµœì²¨ë‹¨ ê²½í—˜ ì¬ìƒ ë°©ë²•ì— ë¹„í•´ 11.54% í–¥ìƒì‹œí‚µë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:04:13*