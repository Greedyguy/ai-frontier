
# Value-Guided KV Compression for LLMs via Approximated CUR Decomposition

**Korean Title:** ê°€ì¹˜ ê¸°ë°˜ KV ì••ì¶•ì„ ìœ„í•œ LLMì˜ ê·¼ì‚¬ CUR ë¶„í•´ ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Value Guided KV Compression

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[NIRVANA Structured pruning reimagined for large language models compression]] (81.3% similar)
- [[V-SEAM Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (80.6% similar)
- [[Communication Efficient Split Learning of ViTs with Attention-based Double Compression_20250919|Communication Efficient Split Learning of ViTs with Attention-based Double Compression]] (80.3% similar)
- [[Delta Knowledge Distillation for Large Language Models_20250919|Delta Knowledge Distillation for Large Language Models]] (79.5% similar)
- [[Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (79.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15038v1 Announce Type: new 
Abstract: Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output $softmax(QK^T)V$, ensuring that the retained tokens best preserve the model's predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40% at high compression, offering a practical speed-accuracy tradeoff.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15038v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: í‚¤-ê°’(KV) ìºì‹œ ì••ì¶•ì€ ì¶”ë¡  ì¤‘ ìê°€íšŒê·€ ì–¸ì–´ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë° ì§€ì—° ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ì „ ì ‘ê·¼ë²•ë“¤ì€ ì£¼ë¡œ ì¿¼ë¦¬-í‚¤ ì£¼ì˜ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìºì‹œëœ í† í°ì„ ìˆœìœ„ ë§¤ê¸°ê³  ì œê±°í•˜ëŠ”ë°, ì´ëŠ” ì£¼ì˜ ê°•ë„ê°€ ì˜ë¯¸ì  ì¤‘ìš”ì„±ê³¼ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ íœ´ë¦¬ìŠ¤í‹±ì€ ì£¼ì˜ ì¶œë ¥ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê°’ ë²¡í„°ì˜ ê¸°ì—¬ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” CUR í–‰ë ¬ ë¶„í•´ì—ì„œ ê³„ì‚°ëœ ë ˆë²„ë¦¬ì§€ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì™€ ê°’ì„ ì„ íƒí•˜ëŠ” ìƒˆë¡œìš´ ê°’ ì¤‘ì‹¬ì˜ KV ì••ì¶• ë°©ë²•ì¸ CurDKVë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ì£¼ì˜ ì¶œë ¥ $softmax(QK^T)V$ì˜ ì§€ë°°ì ì¸ ë¶€ë¶„ ê³µê°„ì„ ê·¼ì‚¬í•˜ì—¬, ìœ ì§€ëœ í† í°ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ í–‰ë™ì„ ìµœì ìœ¼ë¡œ ë³´ì¡´í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì£¼ì˜ ì ìˆ˜ ê·¼ì‚¬ê°€ ì¶œë ¥ ë³´ì¡´ì„ ë³´ì¥í•˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì£¼ê³ , CUR ê¸°ë°˜ ì„ íƒì´ ì¢…ë‹¨ ê°„ ì£¼ì˜ ì¬êµ¬ì„± ì†ì‹¤ì„ ìµœì†Œí™”í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ, CurDKVëŠ” LLaMAì™€ Mistralì—ì„œ ê³µê²©ì ì¸ ì••ì¶• ì˜ˆì‚° í•˜ì— SnapKV ë° ChunkKVì™€ ê°™ì€ ìµœì‹  ë°©ë²•ë“¤ë³´ë‹¤ ìµœëŒ€ 9.6% ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, FlashAttention ë° Grouped Query Attentionê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. í–¥ìƒëœ ì •í™•ë„ ì™¸ì—ë„, CurDKVëŠ” ë†’ì€ ì••ì¶•ì—ì„œ ìµœëŒ€ 40%ì˜ ìƒì„± ì§€ì—°ì„ ì¤„ì—¬ ì‹¤ìš©ì ì¸ ì†ë„-ì •í™•ë„ ì ˆì¶©ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ìê°€íšŒê·€ ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ì™€ ì§€ì—° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ í‚¤-ê°’(KV) ìºì‹œ ì••ì¶• ë°©ë²•ì¸ CurDKVë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ ì£¼ë¡œ ì¿¼ë¦¬-í‚¤ ì£¼ì˜ ì ìˆ˜ì— ì˜ì¡´í•˜ì—¬ ìºì‹œëœ í† í°ì„ ê´€ë¦¬í–ˆì§€ë§Œ, ì´ëŠ” ê°’ ë²¡í„°ì˜ ê¸°ì—¬ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤. CurDKVëŠ” CUR í–‰ë ¬ ë¶„í•´ë¥¼ í†µí•´ ê³„ì‚°ëœ ë ˆë²„ë¦¬ì§€ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì™€ ê°’ì„ ì„ íƒí•˜ì—¬ ì£¼ì˜ ì¶œë ¥ì˜ ì§€ë°°ì  ë¶€ë¶„ ê³µê°„ì„ ê·¼ì‚¬í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ìµœì í™”í•˜ë©°, ì´ë¡ ì ìœ¼ë¡œ ì£¼ì˜ ì ìˆ˜ ê·¼ì‚¬ê°€ ì¶œë ¥ ë³´ì¡´ì„ ë³´ì¥í•˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì£¼ê³ , CUR ê¸°ë°˜ ì„ íƒì´ ì£¼ì˜ ì¬êµ¬ì„± ì†ì‹¤ì„ ìµœì†Œí™”í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CurDKVëŠ” LLaMAì™€ Mistral ëª¨ë¸ì—ì„œ SnapKV ë° ChunkKVë³´ë‹¤ ìµœëŒ€ 9.6% ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ê³ , ìƒì„± ì§€ì—° ì‹œê°„ì„ ìµœëŒ€ 40% ì¤„ì—¬ ì‹¤ìš©ì ì¸ ì†ë„-ì •í™•ë„ ê· í˜•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CurDKVëŠ” CUR í–‰ë ¬ ë¶„í•´ë¥¼ í™œìš©í•œ ìƒˆë¡œìš´ ê°’ ì¤‘ì‹¬ì˜ KV ì••ì¶• ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. ì œì•ˆëœ ë°©ë²•ì€ $softmax(QK^T)V$ì˜ ì£¼ë„ì ì¸ ë¶€ë¶„ ê³µê°„ì„ ê·¼ì‚¬í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.

- 3. CUR ê¸°ë°˜ ì„ íƒì€ ì£¼ì˜ ì¶œë ¥ ë³´ì¡´ì„ ë³´ì¥í•˜ì§€ ì•ŠëŠ” ê¸°ì¡´ ì£¼ì˜ ì ìˆ˜ ê·¼ì‚¬ë²•ë³´ë‹¤ ì¢…ë‹¨ ê°„ ì£¼ì˜ ì¬êµ¬ì„± ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.

- 4. CurDKVëŠ” LLaMAì™€ Mistralì—ì„œ SnapKV ë° ChunkKVë³´ë‹¤ ìµœëŒ€ 9.6% ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 5. ë†’ì€ ì••ì¶•ë¥ ì—ì„œ CurDKVëŠ” ìƒì„± ì§€ì—°ì„ ìµœëŒ€ 40% ì¤„ì´ë©°, FlashAttention ë° Grouped Query Attentionê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:53:59*