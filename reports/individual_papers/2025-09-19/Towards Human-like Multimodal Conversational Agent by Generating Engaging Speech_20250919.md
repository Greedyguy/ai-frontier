---
keywords:
  - Large Language Models
  - Paralinguistic Information
  - Multi-Modal Learning
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14627
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:20:58.345050",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Paralinguistic Information",
    "Multi-Modal Learning"
  ],
  "rejected_keywords": [
    "MultiSensory Conversation Dataset"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Paralinguistic Information": 0.78,
    "Multi-Modal Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech

**Korean Title:** ì¸ê°„ê³¼ ìœ ì‚¬í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í™” ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ë§¤ë ¥ì ì¸ ìŒì„± ìƒì„± ì—°êµ¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Paralinguistic Information|paralinguistic information]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|multimodal LLMs]], [[keywords/Multi-Modal Learning|multimodal LLM-based model]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Enhancing Multi-Agent Debate System Performance via Confidence Expression]] (81.4% similar)
- [[CrowdAgent Multi-Agent Managed Multi-Source Annotation System]] (81.3% similar)
- [[JU-NLP at Touch'e Covert Advertisement in Conversational AI-Generation and Detection Strategies]] (79.9% similar)
- [[DuetUI A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces]] (79.9% similar)
- [[Kling-Avatar Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis]] (79.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14627v1 Announce Type: cross 
Abstract: Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14627v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ì¸ê°„ì˜ ëŒ€í™”ëŠ” ì–¸ì–´, ìŒì„±, ì‹œê°ì  ë‹¨ì„œë¥¼ í¬í•¨í•˜ë©°, ê° ë§¤ì²´ëŠ” ìƒí˜¸ ë³´ì™„ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìŒì„±ì€ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œëŠ” ì™„ì „íˆ í¬ì°©í•  ìˆ˜ ì—†ëŠ” ë¶„ìœ„ê¸°ë‚˜ í†¤ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ LLMì€ ë‹¤ì–‘í•œ ì…ë ¥ìœ¼ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì§€ë§Œ, ìì—°ìŠ¤ëŸ½ê³  ë§¤ë ¥ì ì¸ ìŒì„±ì„ ìƒì„±í•˜ëŠ” ë°ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ê´€ì‹¬ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€í™”ì˜ ë¶„ìœ„ê¸°ì™€ ë°˜ì‘ ìŠ¤íƒ€ì¼ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŒì„± ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì—ì´ì „íŠ¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ì—ì´ì „íŠ¸ê°€ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„±ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ìŒì„±ì— ì¤‘ì ì„ ë‘” ìƒˆë¡œìš´ MultiSensory Conversation ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, í…ìŠ¤íŠ¸ ì‘ë‹µê³¼ ìŒì„± ì„¤ëª…ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë‹¤ì¤‘ ëª¨ë‹¬ LLM ê¸°ë°˜ ëª¨ë¸ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ìŒì„±ì„ ìƒì„±í•˜ì—¬ ì¤€ì–¸ì–´ì  ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ëŒ€í™”ì—ì„œ ì‹œê° ë° ì˜¤ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ ë§¤ë ¥ì ì¸ ìŒì„±ì„ ìƒì„±í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œëŠ” https://github.com/kimtaesu24/MSenCì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ ëŒ€í™”ì—ì„œ ì–¸ì–´, ìŒì„±, ì‹œê°ì  ë‹¨ì„œê°€ ìƒí˜¸ ë³´ì™„ì  ì •ë³´ë¥¼ ì œê³µí•œë‹¤ëŠ” ì ì— ì£¼ëª©í•˜ì—¬, ìì—°ìŠ¤ëŸ½ê³  ë§¤ë ¥ì ì¸ ìŒì„±ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ëŒ€í™” ë¶„ìœ„ê¸°ì™€ ë°˜ì‘ ìŠ¤íƒ€ì¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŒì„± ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì—ì´ì „íŠ¸ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ MultiSensory Conversation ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìœ¼ë©°, ë©€í‹°ëª¨ë‹¬ LLM ê¸°ë°˜ ëª¨ë¸ì„ ì œì•ˆí•˜ì—¬ í…ìŠ¤íŠ¸ ì‘ë‹µê³¼ ìŒì„± ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì‹œê° ë° ìŒì„± ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•œ ëŒ€í™”ê°€ ë§¤ë ¥ì ì¸ ìŒì„± ìƒì„±ì— íš¨ê³¼ì ì„ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê°„ì˜ ëŒ€í™”ëŠ” ì–¸ì–´, ìŒì„±, ì‹œê°ì  ë‹¨ì„œë¥¼ í¬í•¨í•˜ë©°, ê° ë§¤ì²´ëŠ” ë³´ì™„ì ì¸ ì •ë³´ë¥¼ ì œê³µí•œë‹¤.

- 2. ìš°ë¦¬ëŠ” ëŒ€í™” ë¶„ìœ„ê¸°ì™€ ë°˜ì‘ ìŠ¤íƒ€ì¼ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŒì„± ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì—ì´ì „íŠ¸ë¥¼ ì œì•ˆí•œë‹¤.

- 3. ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ MultiSensory Conversation ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì˜€ë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, ëŒ€í™”ì—ì„œ ì‹œê° ë° ì˜¤ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•˜ì—¬ ë§¤ë ¥ì ì¸ ìŒì„±ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì„ì„ ì…ì¦í•˜ì˜€ë‹¤.

- 5. ì†ŒìŠ¤ ì½”ë“œëŠ” https://github.com/kimtaesu24/MSenC ì—ì„œ ì œê³µëœë‹¤.

---

*Generated on 2025-09-19 14:59:20*