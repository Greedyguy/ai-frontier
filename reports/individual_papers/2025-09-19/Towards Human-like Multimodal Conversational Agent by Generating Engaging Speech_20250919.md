---
keywords:
  - Large Language Models
  - Paralinguistic Information
  - Multi-Modal Learning
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14627
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:20:58.345050",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Paralinguistic Information",
    "Multi-Modal Learning"
  ],
  "rejected_keywords": [
    "MultiSensory Conversation Dataset"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Paralinguistic Information": 0.78,
    "Multi-Modal Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech

**Korean Title:** 인간과 유사한 다중 모달 대화 에이전트를 위한 매력적인 음성 생성 연구

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Paralinguistic Information|paralinguistic information]]
**🚀 Evolved Concepts**: [[keywords/Large Language Models|multimodal LLMs]], [[keywords/Multi-Modal Learning|multimodal LLM-based model]]

## 🔗 유사한 논문
- [[Enhancing Multi-Agent Debate System Performance via Confidence Expression]] (81.4% similar)
- [[CrowdAgent Multi-Agent Managed Multi-Source Annotation System]] (81.3% similar)
- [[JU-NLP at Touch'e Covert Advertisement in Conversational AI-Generation and Detection Strategies]] (79.9% similar)
- [[DuetUI A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces]] (79.9% similar)
- [[Kling-Avatar Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis]] (79.8% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14627v1 Announce Type: cross 
Abstract: Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC

## 🔍 Abstract (한글 번역)

arXiv:2509.14627v1 발표 유형: 교차  
초록: 인간의 대화는 언어, 음성, 시각적 단서를 포함하며, 각 매체는 상호 보완적인 정보를 제공합니다. 예를 들어, 음성은 텍스트만으로는 완전히 포착할 수 없는 분위기나 톤을 전달합니다. 다중 모달 LLM은 다양한 입력으로부터 텍스트 응답을 생성하는 데 중점을 두지만, 자연스럽고 매력적인 음성을 생성하는 데는 상대적으로 적은 관심이 주어졌습니다. 우리는 대화의 분위기와 반응 스타일 정보를 기반으로 음성 응답을 생성하는 인간과 유사한 에이전트를 제안합니다. 이를 위해 우리는 에이전트가 자연스러운 음성을 생성할 수 있도록 음성에 중점을 둔 새로운 MultiSensory Conversation 데이터셋을 구축합니다. 그런 다음, 텍스트 응답과 음성 설명을 생성하기 위한 다중 모달 LLM 기반 모델을 제안하며, 이는 음성을 생성하여 준언어적 정보를 포함합니다. 실험 결과는 대화에서 시각 및 오디오 모달리티를 모두 활용하여 매력적인 음성을 생성하는 데 효과적임을 보여줍니다. 소스 코드는 https://github.com/kimtaesu24/MSenC에서 이용 가능합니다.

## 📝 요약

이 논문은 인간 대화에서 언어, 음성, 시각적 단서가 상호 보완적 정보를 제공한다는 점에 주목하여, 자연스럽고 매력적인 음성을 생성하는 방법을 제안합니다. 이를 위해 대화 분위기와 반응 스타일 정보를 바탕으로 음성 응답을 생성하는 인간과 유사한 에이전트를 개발했습니다. 새로운 MultiSensory Conversation 데이터셋을 구축하여 자연스러운 음성 생성을 가능하게 했으며, 멀티모달 LLM 기반 모델을 제안하여 텍스트 응답과 음성 설명을 생성합니다. 실험 결과, 시각 및 음성 모달리티를 활용한 대화가 매력적인 음성 생성에 효과적임을 보여주었습니다. 소스 코드는 GitHub에서 확인할 수 있습니다.

## 🎯 주요 포인트

- 1. 인간의 대화는 언어, 음성, 시각적 단서를 포함하며, 각 매체는 보완적인 정보를 제공한다.

- 2. 우리는 대화 분위기와 반응 스타일 정보를 기반으로 음성 응답을 생성하는 인간과 유사한 에이전트를 제안한다.

- 3. 자연스러운 음성 생성을 가능하게 하기 위해 새로운 MultiSensory Conversation 데이터셋을 구축하였다.

- 4. 실험 결과, 대화에서 시각 및 오디오 모달리티를 활용하여 매력적인 음성을 생성하는 것이 효과적임을 입증하였다.

- 5. 소스 코드는 https://github.com/kimtaesu24/MSenC 에서 제공된다.

---

*Generated on 2025-09-19 14:59:20*