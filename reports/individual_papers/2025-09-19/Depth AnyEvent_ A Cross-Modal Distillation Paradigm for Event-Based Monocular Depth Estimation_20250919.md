---
keywords:
  - Foundation Models
  - Neural Networks
  - Cross-Modal Distillation
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.15224
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:48:58.971087",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Foundation Models",
    "Neural Networks",
    "Cross-Modal Distillation"
  ],
  "rejected_keywords": [
    "Event-Based Monocular Depth Estimation"
  ],
  "similarity_scores": {
    "Foundation Models": 0.82,
    "Neural Networks": 0.77,
    "Cross-Modal Distillation": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation

**Korean Title:** AnyEvent ê¹Šì´: ì´ë²¤íŠ¸ ê¸°ë°˜ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì„ ìœ„í•œ êµì°¨ ëª¨ë‹¬ ì¦ë¥˜ íŒ¨ëŸ¬ë‹¤ì„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Neural Networks|Recurrent Architecture]]
**âš¡ Unique Technical**: [[keywords/Cross-Modal Distillation|Cross-Modal Distillation]]
**ğŸš€ Evolved Concepts**: [[keywords/Foundation Models|Vision Foundation Model]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[VSE-MOT Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement]] (80.6% similar)
- [[ColonCrafter A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors]] (80.5% similar)
- [[BEVUDA++ Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection]] (80.3% similar)
- [[FishBEV Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras]] (80.0% similar)
- [[UCorr Wire Detection and Depth Estimation for Autonomous Drones]] (79.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15224v1 Announce Type: new 
Abstract: Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15224v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì´ë²¤íŠ¸ ì¹´ë©”ë¼ëŠ” í¬ì†Œí•˜ê³  ë†’ì€ ì‹œê°„ í•´ìƒë„ì˜ ì‹œê° ì •ë³´ë¥¼ í¬ì°©í•˜ì—¬ ê³ ì† ì›€ì§ì„ê³¼ ê°•í•˜ê²Œ ë³€í•˜ëŠ” ì¡°ëª… ì¡°ê±´ì„ ê°€ì§„ ê¹Œë‹¤ë¡œìš´ í™˜ê²½ì— íŠ¹íˆ ì í•©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¡°ë°€í•œ ì‹¤ì œ ê¹Šì´ ì£¼ì„ì´ í¬í•¨ëœ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì˜ ë¶€ì¡±ì€ ì´ë²¤íŠ¸ ë°ì´í„°ì—ì„œ í•™ìŠµ ê¸°ë°˜ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì„ ë°©í•´í•©ë‹ˆë‹¤. ì´ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¹„ì „ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(VFM)ì„ í™œìš©í•˜ì—¬ ì¡°ë°€í•œ ëŒ€ë¦¬ ë ˆì´ë¸”ì„ ìƒì„±í•˜ëŠ” êµì°¨ ëª¨ë‹¬ ì¦ë¥˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì „ëµì€ RGB í”„ë ˆì„ê³¼ ê³µê°„ì ìœ¼ë¡œ ì •ë ¬ëœ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ í•„ìš”ë¡œ í•˜ë©°, ì´ëŠ” ìƒìš©ìœ¼ë¡œë„ ê°„ë‹¨íˆ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„¤ì •ì´ë©°, ëŒ€ê·œëª¨ VFMì˜ ê²¬ê³ í•¨ì„ í™œìš©í•©ë‹ˆë‹¤. ë˜í•œ, VFMsë¥¼ ì ì‘ì‹œí‚¤ê¸° ìœ„í•´, Depth Anything v2 (DAv2)ì™€ ê°™ì€ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ë‹¨ì•ˆ ì´ë²¤íŠ¸ ì¹´ë©”ë¼ë¡œë¶€í„° ê¹Šì´ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ìˆœí™˜ ì•„í‚¤í…ì²˜ë¥¼ ë„ì¶œí•˜ëŠ” ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í•©ì„± ë° ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì„ í‰ê°€í•˜ë©°, i) ìš°ë¦¬ì˜ êµì°¨ ëª¨ë‹¬ íŒ¨ëŸ¬ë‹¤ì„ì´ ë¹„ì‹¼ ê¹Šì´ ì£¼ì„ ì—†ì´ë„ ì™„ì „ ê°ë… ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³ , ii) ìš°ë¦¬ì˜ VFM ê¸°ë°˜ ëª¨ë¸ì´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì´ë²¤íŠ¸ ì¹´ë©”ë¼ë¥¼ í™œìš©í•œ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¦ë¥˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ Vision Foundation Model(VFM)ì„ í™œìš©í•˜ì—¬ ë°€ë„ ìˆëŠ” ëŒ€ë¦¬ ë¼ë²¨ì„ ìƒì„±í•˜ë©°, ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ê³¼ RGB í”„ë ˆì„ì˜ ê³µê°„ì  ì •ë ¬ì„ ìš”êµ¬í•©ë‹ˆë‹¤. ë˜í•œ, Depth Anything v2(DAv2)ì™€ ê°™ì€ VFMì„ í™œìš©í•˜ê±°ë‚˜ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë„ì…í•˜ì—¬ ê¹Šì´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ í•©ì„± ë° ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ë¹„ì‹¼ ê¹Šì´ ì£¼ì„ ì—†ì´ë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ê³ , VFM ê¸°ë°˜ ëª¨ë¸ì€ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ë²¤íŠ¸ ì¹´ë©”ë¼ëŠ” ê³ ì† ì›€ì§ì„ê³¼ ì¡°ëª…ì´ í¬ê²Œ ë³€í•˜ëŠ” í™˜ê²½ì—ì„œ ì í•©í•œ ê³ í•´ìƒë„ì˜ ì‹œê° ì •ë³´ë¥¼ í¬ì°©í•œë‹¤.

- 2. ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ë¶€ì¡±ì€ ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ í†µí•œ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • í•™ìŠµì— ì¥ì• ê°€ ëœë‹¤.

- 3. ë¹„ì „ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(VFM)ì„ í™œìš©í•œ êµì°¨ ëª¨ë‹¬ ì¦ë¥˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•˜ì—¬ ë°€ì§‘ëœ ëŒ€ë¦¬ ë¼ë²¨ì„ ìƒì„±í•œë‹¤.

- 4. RGB í”„ë ˆì„ê³¼ ê³µê°„ì ìœ¼ë¡œ ì •ë ¬ëœ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ê¹Šì´ ì¶”ì •ì„ ìˆ˜í–‰í•œë‹¤.

- 5. ì œì•ˆëœ VFM ê¸°ë°˜ ëª¨ë¸ì€ ë¹„ì‹¼ ê¹Šì´ ì£¼ì„ ì—†ì´ë„ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.

---

*Generated on 2025-09-19 16:11:26*