
# Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction

**Korean Title:** ì˜´ë‹ˆGEC ì†Œê°œ: ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •ì„ ìœ„í•œ ì€ìƒ‰ ë‹¤êµ­ì–´ ë°ì´í„°ì…‹

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Silver Standard Dataset

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[COMI-LINGUA Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing]] (78.5% similar)
- [[Long-context Reference-based MT Quality Estimation]] (78.1% similar)
- [[Omni-CLST Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (78.0% similar)
- [[Apertus Democratizing Open and Compliant LLMs for Global Language Environments]] (78.0% similar)
- [[MAgICoRe Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning]] (76.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14504v1 Announce Type: cross 
Abstract: In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14504v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •(GEC) ì‘ì—…ì„ ìœ„í•œ ë‹¤êµ­ì–´ ì€í‘œì¤€ ë°ì´í„°ì…‹ ëª¨ìŒì¸ OmniGECë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì²´ì½”ì–´, ì˜ì–´, ì—ìŠ¤í† ë‹ˆì•„ì–´, ë…ì¼ì–´, ê·¸ë¦¬ìŠ¤ì–´, ì•„ì´ìŠ¬ë€ë“œì–´, ì´íƒˆë¦¬ì•„ì–´, ë¼íŠ¸ë¹„ì•„ì–´, ìŠ¬ë¡œë² ë‹ˆì•„ì–´, ìŠ¤ì›¨ë´ì–´, ìš°í¬ë¼ì´ë‚˜ì–´ ë“± 11ê°œ ì–¸ì–´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ì…‹ì€ ë‹¤êµ­ì–´ GEC ì†”ë£¨ì…˜ ê°œë°œì„ ì´‰ì§„í•˜ê³ , ì˜ì–´ GEC ì†”ë£¨ì…˜ì„ ë‹¤êµ­ì–´ GECë¡œ ì ì‘ì‹œí‚¤ëŠ” ê³¼ì •ì—ì„œì˜ ë°ì´í„° ê²©ì°¨ë¥¼ í•´ì†Œí•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ëŠ” 11ê°œ ëŒ€ìƒ ì–¸ì–´ì˜ ìœ„í‚¤ë°±ê³¼ í¸ì§‘, 11ê°œ ëŒ€ìƒ ì–¸ì–´ì˜ Reddit ì„œë¸Œë ˆë”§, ìš°í¬ë¼ì´ë‚˜ì–´ ì „ìš© UberText 2.0 ì†Œì…œ ë¯¸ë””ì–´ ì½”í¼ìŠ¤ì—ì„œ ë¹„ë¡¯ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„í‚¤ë°±ê³¼ í¸ì§‘ì€ ì¸ê°„ì´ ë§Œë“  ìˆ˜ì •ì—ì„œ íŒŒìƒë˜ì—ˆìœ¼ë©°, Reddit ë° UberText 2.0 ë°ì´í„°ëŠ” GPT-4o-mini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ ìˆ˜ì • í’ˆì§ˆì€ ìë™ ë° ìˆ˜ë™ìœ¼ë¡œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë‘ ê°œì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì¸ Aya-Expanse (8B)ì™€ Gemma-3 (12B)ë¥¼ ë‹¤êµ­ì–´ OmniGEC ì½”í¼ìŠ¤ì— ë§ì¶”ì–´ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë‹¨ë½ ìˆ˜ì¤€ì˜ ë‹¤êµ­ì–´ GECì—ì„œ ìµœì‹ (SOTA) ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ ëª¨ìŒê³¼ ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì€ Hugging Faceì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” 11ê°œ ì–¸ì–´ì— ëŒ€í•œ ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •(GEC) ì‘ì—…ì„ ìœ„í•œ ë‹¤êµ­ì–´ ì€í‘œì¤€ ë°ì´í„°ì…‹ ëª¨ìŒì¸ OmniGECë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì²´ì½”ì–´, ì˜ì–´, ì—ìŠ¤í† ë‹ˆì•„ì–´, ë…ì¼ì–´, ê·¸ë¦¬ìŠ¤ì–´, ì•„ì´ìŠ¬ë€ë“œì–´, ì´íƒˆë¦¬ì•„ì–´, ë¼íŠ¸ë¹„ì•„ì–´, ìŠ¬ë¡œë² ë‹ˆì•„ì–´, ìŠ¤ì›¨ë´ì–´, ìš°í¬ë¼ì´ë‚˜ì–´ë¥¼ í¬í•¨í•˜ë©°, ì˜ì–´ GEC ì†”ë£¨ì…˜ì„ ë‹¤êµ­ì–´ë¡œ í™•ì¥í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ë°ì´í„°ëŠ” Wikipedia í¸ì§‘, Redditì˜ ì„œë¸Œë ˆë”§, ìš°í¬ë¼ì´ë‚˜ì–´ ì „ìš© UberText 2.0 ì†Œì…œ ë¯¸ë””ì–´ ì½”í¼ìŠ¤ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. Wikipedia ë°ì´í„°ëŠ” ì¸ê°„ì´ ìˆ˜ì •í•œ ê²ƒì´ë©°, ë‚˜ë¨¸ì§€ëŠ” GPT-4o-mini ëª¨ë¸ë¡œ ìë™ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ í’ˆì§ˆì€ ìë™ ë° ìˆ˜ë™ìœ¼ë¡œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, Aya-Expanseì™€ Gemma-3 ëª¨ë¸ì„ OmniGEC ì½”í¼ìŠ¤ì— ë§ì¶° ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë¬¸ë‹¨ ìˆ˜ì¤€ì˜ ë‹¤êµ­ì–´ GECì—ì„œ ìµœì²¨ë‹¨ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì€ Hugging Faceì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. OmniGECëŠ” 11ê°œ ì–¸ì–´ì— ëŒ€í•œ ë‹¤êµ­ì–´ ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •(GEC)ìš© ì‹¤ë²„ í‘œì¤€ ë°ì´í„°ì…‹ì„ ì œê³µí•©ë‹ˆë‹¤.

- 2. ë°ì´í„°ì…‹ì€ Wikipedia í¸ì§‘, Redditì˜ í•˜ìœ„ ë ˆë”§, ìš°í¬ë¼ì´ë‚˜ì–´ ì „ìš© UberText 2.0 ì†Œì…œ ë¯¸ë””ì–´ ì½”í¼ìŠ¤ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.

- 3. Reddit ë° UberText 2.0 ë°ì´í„°ëŠ” GPT-4o-mini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

- 4. Aya-Expanseì™€ Gemma-3 ëª¨ë¸ì„ OmniGEC ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë‹¤êµ­ì–´ GECì—ì„œ ìµœì²¨ë‹¨ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

- 5. ë°ì´í„°ì…‹ê³¼ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì€ Hugging Faceì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 14:56:05*