
# MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation

**Korean Title:** MedFact-R1: 의사 라벨 증강을 통한 사실적 의료 추론 연구

## 📋 메타데이터

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: Pseudo Label Supervised Fine Tuning, Group Relative Policy Optimization

## 🔗 유사한 논문
- [[MedVAL Toward Expert-Level Medical Text Validation with Language Models]] (82.2% similar)
- [[Select to Know An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering]] (81.4% similar)
- [[RationAnomaly Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning]] (80.9% similar)
- [[A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making_20250919|A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making]] (80.1% similar)
- [[Causal-Counterfactual RAG The Integration of Causal-Counterfactual Reasoning into RAG]] (79.8% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15154v1 Announce Type: new 
Abstract: Ensuring factual consistency and reliable reasoning remains a critical challenge for medical vision-language models. We introduce MEDFACT-R1, a two-stage framework that integrates external knowledge grounding with reinforcement learning to improve the factual medical reasoning. The first stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external factual expertise; while the second stage applies Group Relative Policy Optimization (GRPO) with four tailored factual reward signals to encourage self-consistent reasoning. Across three public medical QA benchmarks, MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over previous state-of-the-art methods. Ablation studies highlight the necessity of pseudo-label SFT cold start and validate the contribution of each GRPO reward, underscoring the synergy between knowledge grounding and RL-driven reasoning for trustworthy medical AI. Codes are released at https://github.com/Garfieldgengliang/MEDFACT-R1.

## 🔍 Abstract (한글 번역)

arXiv:2509.15154v1 발표 유형: 신규  
초록: 사실적 일관성과 신뢰할 수 있는 추론을 보장하는 것은 의료 비전-언어 모델의 중요한 과제로 남아 있습니다. 우리는 외부 지식 기반과 강화 학습을 통합하여 사실적 의료 추론을 개선하는 두 단계의 프레임워크인 MEDFACT-R1을 소개합니다. 첫 번째 단계에서는 외부 사실 전문 지식을 통합하기 위해 의사 라벨 감독 하의 미세 조정(SFT)을 사용하며, 두 번째 단계에서는 네 가지 맞춤형 사실 보상 신호를 사용하여 자기 일관적인 추론을 장려하는 그룹 상대 정책 최적화(GRPO)를 적용합니다. 세 가지 공공 의료 QA 벤치마크에서 MEDFACT-R1은 이전 최첨단 방법에 비해 사실적 정확성에서 최대 22.5%의 절대적인 개선을 제공합니다. 소거 연구는 의사 라벨 SFT의 콜드 스타트 필요성을 강조하고 각 GRPO 보상의 기여를 검증하여 신뢰할 수 있는 의료 AI를 위한 지식 기반과 RL 기반 추론 간의 시너지를 강조합니다. 코드는 https://github.com/Garfieldgengliang/MEDFACT-R1에서 제공됩니다.

## 📝 요약

MEDFACT-R1은 의료 비전-언어 모델의 사실적 일관성과 신뢰할 수 있는 추론을 개선하기 위한 두 단계 프레임워크입니다. 첫 번째 단계에서는 외부 지식을 활용한 의사 라벨 지도 학습을 통해 사실적 전문성을 통합하고, 두 번째 단계에서는 그룹 상대 정책 최적화(GRPO)를 사용하여 네 가지 보상 신호로 자기 일관적 추론을 촉진합니다. 세 가지 의료 QA 벤치마크에서 MEDFACT-R1은 기존 최첨단 방법보다 사실적 정확성을 최대 22.5% 향상시켰습니다. 연구 결과는 의사 라벨 SFT의 필요성과 GRPO 보상의 기여도를 강조하며, 지식 기반과 강화 학습 기반 추론의 시너지를 입증합니다. 코드가 공개되어 있습니다.

## 🎯 주요 포인트

- 1. MEDFACT-R1은 외부 지식 기반과 강화 학습을 통합하여 사실적 의료 추론을 개선하는 두 단계의 프레임워크입니다.

- 2. 첫 번째 단계에서는 외부 사실적 전문 지식을 통합하기 위해 의사 라벨 감독 하에 미세 조정(SFT)을 수행합니다.

- 3. 두 번째 단계에서는 네 가지 맞춤형 사실 보상 신호를 사용하는 그룹 상대 정책 최적화(GRPO)를 적용하여 자기 일관적인 추론을 장려합니다.

- 4. MEDFACT-R1은 세 가지 공공 의료 QA 벤치마크에서 이전 최첨단 방법들에 비해 사실 정확도를 최대 22.5% 절대적으로 개선합니다.

- 5. 코드와 관련 자료는 https://github.com/Garfieldgengliang/MEDFACT-R1에서 제공됩니다.

---

*Generated on 2025-09-19 16:09:34*