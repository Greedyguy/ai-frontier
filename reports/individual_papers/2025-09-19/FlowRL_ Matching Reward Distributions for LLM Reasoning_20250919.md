
# FlowRL: Matching Reward Distributions for LLM Reasoning

**Korean Title:** FlowRL: LLM ì¶”ë¡ ì„ ìœ„í•œ ë³´ìƒ ë¶„í¬ ë§¤ì¹­

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Reward Distribution Matching, Flow Balancing

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[THOR Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning]] (81.0% similar)
- [[MAgICoRe Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning]] (80.9% similar)
- [[Video-Language Critic Transferable Reward Functions for Language-Conditioned Robotics]] (80.3% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (79.7% similar)
- [[Generalizable_Geometric_Image_Caption_Synthesis_20250919|Generalizable Geometric Image Caption Synthesis]] (79.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15207v1 Announce Type: cross 
Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15207v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê°•í™” í•™ìŠµ(RL)ì—ì„œ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ëŒ€ì‹  íë¦„ ê· í˜•ì„ í†µí•´ ì „ì²´ ë³´ìƒ ë¶„í¬ë¥¼ ë§¤ì¹­í•˜ëŠ” FlowRLì„ ì œì•ˆí•©ë‹ˆë‹¤. ìµœê·¼ì˜ ê³ ê¸‰ ì¶”ë¡  ëª¨ë¸ì€ ë³´ìƒ ìµœëŒ€í™” ë°©ë²•(ì˜ˆ: PPO ë° GRPO)ì„ ì±„íƒí•˜ì—¬ ì§€ë°°ì ì¸ ë³´ìƒ ì‹ í˜¸ë¥¼ ê³¼ë„í•˜ê²Œ ìµœì í™”í•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ì´ë¡œ ì¸í•´ ëœ ë¹ˆë²ˆí•˜ì§€ë§Œ ìœ íš¨í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ë¬´ì‹œí•˜ì—¬ ë‹¤ì–‘ì„±ì´ ê°ì†Œí•©ë‹ˆë‹¤. ì´ì— ë°˜í•´, ìš°ë¦¬ëŠ” ìŠ¤ì¹¼ë¼ ë³´ìƒì„ í•™ìŠµ ê°€ëŠ¥í•œ ë¶„í•  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”ëœ ëª©í‘œ ë¶„í¬ë¡œ ë³€í™˜í•œ í›„, ì •ì±…ê³¼ ëª©í‘œ ë¶„í¬ ê°„ì˜ ì—­ KL ë°œì‚°ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì•„ì´ë””ì–´ë¥¼ ë‹¤ì–‘í•œ íƒìƒ‰ê³¼ ì¼ë°˜í™” ê°€ëŠ¥í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ì´‰ì§„í•˜ëŠ” íë¦„ ê· í˜• ìµœì í™” ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. ìˆ˜í•™ ë° ì½”ë“œ ì¶”ë¡  ì‘ì—…ì— ëŒ€í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•œ ê²°ê³¼, FlowRLì€ ìˆ˜í•™ ë²¤ì¹˜ë§ˆí¬ì—ì„œ GRPOë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ 10.0% ê°œì„ , PPOë³´ë‹¤ 5.1% ê°œì„ ì„ ë‹¬ì„±í–ˆìœ¼ë©°, ì½”ë“œ ì¶”ë¡  ì‘ì—…ì—ì„œë„ ì¼ê´€ë˜ê²Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLM ê°•í™” í•™ìŠµì—ì„œ íš¨ìœ¨ì ì¸ íƒìƒ‰ê³¼ ë‹¤ì–‘í•œ ì¶”ë¡ ì„ ìœ„í•œ í•µì‹¬ ë‹¨ê³„ë¡œì„œ ë³´ìƒ ë¶„í¬ ë§¤ì¹­ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

FlowRLì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ê°•í™” í•™ìŠµì—ì„œ ë³´ìƒ ìµœëŒ€í™” ëŒ€ì‹  ë³´ìƒ ë¶„í¬ë¥¼ íë¦„ ê· í˜•ì„ í†µí•´ ë§¤ì¹­í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ì£¼ë¡œ ë³´ìƒ ìµœëŒ€í™” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì§€ë°°ì ì¸ ë³´ìƒ ì‹ í˜¸ì— ê³¼ë„í•˜ê²Œ ìµœì í™”ë˜ì§€ë§Œ, ì´ëŠ” ë‹¤ì–‘í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ê°„ê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. FlowRLì€ ìŠ¤ì¹¼ë¼ ë³´ìƒì„ ì •ê·œí™”ëœ ëª©í‘œ ë¶„í¬ë¡œ ë³€í™˜í•˜ê³ , ì •ì±…ê³¼ ëª©í‘œ ë¶„í¬ ê°„ì˜ ì—­ KL ë°œì‚°ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ íƒìƒ‰ê³¼ ì¼ë°˜í™” ê°€ëŠ¥í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤. ìˆ˜í•™ ë° ì½”ë“œ ì¶”ë¡  ì‘ì—… ì‹¤í—˜ì—ì„œ FlowRLì€ GRPO ëŒ€ë¹„ í‰ê·  10.0%, PPO ëŒ€ë¹„ 5.1% í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, ì½”ë“œ ì¶”ë¡  ì‘ì—…ì—ì„œë„ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤. ì´ëŠ” ë³´ìƒ ë¶„í¬ ë§¤ì¹­ì´ íš¨ìœ¨ì ì¸ íƒìƒ‰ê³¼ ë‹¤ì–‘í•œ ì¶”ë¡ ì„ ìœ„í•œ í•µì‹¬ ë‹¨ê³„ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. FlowRLì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê°•í™” í•™ìŠµì—ì„œ ë³´ìƒ ìµœëŒ€í™” ëŒ€ì‹  íë¦„ ê· í˜•ì„ í†µí•´ ì „ì²´ ë³´ìƒ ë¶„í¬ë¥¼ ë§¤ì¹­í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ ë³´ìƒ ìµœëŒ€í™” ë°©ë²•ë“¤ì€ ì§€ë°°ì ì¸ ë³´ìƒ ì‹ í˜¸ë¥¼ ê³¼ë„í•˜ê²Œ ìµœì í™”í•˜ì—¬ ë‹¤ì–‘í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ê°„ê³¼í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

- 3. FlowRLì€ ìŠ¤ì¹¼ë¼ ë³´ìƒì„ ì •ê·œí™”ëœ ëª©í‘œ ë¶„í¬ë¡œ ë³€í™˜í•˜ê³ , ì •ì±…ê³¼ ëª©í‘œ ë¶„í¬ ê°„ì˜ ì—­ KL ë°œì‚°ì„ ìµœì†Œí™”í•˜ëŠ” íë¦„ ê· í˜• ìµœì í™” ë°©ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

- 4. ìˆ˜í•™ ë° ì½”ë“œ ì¶”ë¡  ì‘ì—…ì—ì„œ FlowRLì€ GRPO ëŒ€ë¹„ í‰ê·  10.0%, PPO ëŒ€ë¹„ 5.1%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, ì½”ë“œ ì¶”ë¡  ì‘ì—…ì—ì„œë„ ì¼ê´€ë˜ê²Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í–ˆìŠµë‹ˆë‹¤.

- 5. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë³´ìƒ ë¶„í¬ ë§¤ì¹­ì´ LLM ê°•í™” í•™ìŠµì—ì„œ íš¨ìœ¨ì ì¸ íƒìƒ‰ê³¼ ë‹¤ì–‘í•œ ì¶”ë¡ ì„ ìœ„í•œ í•µì‹¬ ë‹¨ê³„ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:08:18*