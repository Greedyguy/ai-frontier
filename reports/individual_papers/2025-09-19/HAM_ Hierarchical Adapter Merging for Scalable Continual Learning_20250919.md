
# HAM: Hierarchical Adapter Merging for Scalable Continual Learning

**Korean Title:** HAM: í™•ì¥ ê°€ëŠ¥í•œ ì§€ì† í•™ìŠµì„ ìœ„í•œ ê³„ì¸µì  ì–´ëŒ‘í„° ë³‘í•©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Dynamic Adapter Grouping

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Superpose_Task-specific_Features_for_Model_Merging_20250919|Superpose Task-specific Features for Model Merging]] (81.9% similar)
- [[Zero-Shot LLMs in Human-in-the-Loop RL Replacing Human Feedback for Reward Shaping]] (80.8% similar)
- [[Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (80.6% similar)
- [[FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (80.5% similar)
- [[Personalization on a Budget Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection]] (80.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13211v3 Announce Type: replace 
Abstract: Continual learning is an essential capability of human cognition, yet it poses significant challenges for current deep learning models. The primary issue is that new knowledge can interfere with previously learned information, causing the model to forget earlier knowledge in favor of the new, a phenomenon known as catastrophic forgetting. Although large pre-trained models can partially mitigate forgetting by leveraging their existing knowledge and over-parameterization, they often struggle when confronted with novel data distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, enable efficient adaptation to new knowledge. However, they still face challenges in scaling to dynamic learning scenarios and long sequences of tasks, as maintaining one adapter per task introduces complexity and increases the potential for interference. In this paper, we introduce Hierarchical Adapters Merging (HAM), a novel framework that dynamically combines adapters from different tasks during training. This approach enables HAM to scale effectively, allowing it to manage more tasks than competing baselines with improved efficiency. To achieve this, HAM maintains a fixed set of groups that hierarchically consolidate new adapters. For each task, HAM trains a low-rank adapter along with an importance scalar, then dynamically groups tasks based on adapter similarity. Within each group, adapters are pruned, scaled and merge, facilitating transfer learning between related tasks. Extensive experiments on three vision benchmarks show that HAM significantly outperforms state-of-the-art methods, particularly as the number of tasks increases.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13211v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì§€ì†ì  í•™ìŠµì€ ì¸ê°„ ì¸ì§€ì˜ í•„ìˆ˜ì ì¸ ëŠ¥ë ¥ì´ì§€ë§Œ, í˜„ì¬ì˜ ì‹¬ì¸µ í•™ìŠµ ëª¨ë¸ì—ëŠ” ìƒë‹¹í•œ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì£¼ìš” ë¬¸ì œëŠ” ìƒˆë¡œìš´ ì§€ì‹ì´ ì´ì „ì— í•™ìŠµí•œ ì •ë³´ë¥¼ ë°©í•´í•˜ì—¬, ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ ì„ í˜¸í•˜ë©´ì„œ ì´ì „ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” í˜„ìƒì¸ ì¹˜ëª…ì  ë§ê°(catatastrophic forgetting)ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ ê¸°ì¡´ ì§€ì‹ê³¼ ê³¼ë§¤ê°œë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ë§ê°ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì™„í™”í•  ìˆ˜ ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„° ë¶„í¬ì— ì§ë©´í–ˆì„ ë•Œ ì¢…ì¢… ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. LoRAì™€ ê°™ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì€ ìƒˆë¡œìš´ ì§€ì‹ì— ëŒ€í•œ íš¨ìœ¨ì ì¸ ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ì€ ë™ì  í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì™€ ê¸´ ì‘ì—… ì‹œí€€ìŠ¤ì— í™•ì¥í•˜ëŠ” ë° ì—¬ì „íˆ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì‘ì—…ë‹¹ í•˜ë‚˜ì˜ ì–´ëŒ‘í„°ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì€ ë³µì¡ì„±ì„ ì¦ê°€ì‹œí‚¤ê³  ê°„ì„­ì˜ ê°€ëŠ¥ì„±ì„ ë†’ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í›ˆë ¨ ì¤‘ì— ë‹¤ì–‘í•œ ì‘ì—…ì˜ ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ ê³„ì¸µì  ì–´ëŒ‘í„° ë³‘í•©(HAM)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ HAMì´ ê²½ìŸ ê¸°ë²•ë³´ë‹¤ ë” ë§ì€ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ HAMì€ ìƒˆë¡œìš´ ì–´ëŒ‘í„°ë¥¼ ê³„ì¸µì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ê³ ì •ëœ ê·¸ë£¹ ì„¸íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ê° ì‘ì—…ì— ëŒ€í•´ HAMì€ ì €ë­í¬ ì–´ëŒ‘í„°ì™€ ì¤‘ìš”ë„ ìŠ¤ì¹¼ë¼ë¥¼ í›ˆë ¨í•œ í›„, ì–´ëŒ‘í„° ìœ ì‚¬ì„±ì— ê¸°ë°˜í•˜ì—¬ ì‘ì—…ì„ ë™ì ìœ¼ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤. ê° ê·¸ë£¹ ë‚´ì—ì„œ ì–´ëŒ‘í„°ëŠ” ê°€ì§€ì¹˜ê¸°, ìŠ¤ì¼€ì¼ë§ ë° ë³‘í•©ë˜ì–´ ê´€ë ¨ ì‘ì—… ê°„ì˜ ì „ì´ í•™ìŠµì„ ì´‰ì§„í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ë¹„ì „ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼, HAMì€ íŠ¹íˆ ì‘ì—… ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ìµœì²¨ë‹¨ ë°©ë²•ì„ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ ì¸ì§€ì˜ ì¤‘ìš”í•œ ëŠ¥ë ¥ì¸ ì§€ì†ì  í•™ìŠµì„ ë‹¤ë£¨ë©°, ê¸°ì¡´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì§ë©´í•œ ë¬¸ì œì¸ 'íŒŒêµ­ì  ë§ê°'ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì€ ê¸°ì¡´ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë§ê°ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì™„í™”í•  ìˆ˜ ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„° ë¶„í¬ì— ì ì‘í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ì§€ì‹ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆëŠ” ê³„ì¸µì  ì–´ëŒ‘í„° ë³‘í•©(HAM) í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. HAMì€ ë‹¤ì–‘í•œ ì‘ì—…ì˜ ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì‘ì—… ê°„ ì „ì´ í•™ìŠµì„ ì´‰ì§„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, HAMì€ íŠ¹íˆ ì‘ì—… ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ê¸°ì¡´ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì§€ì†ì  í•™ìŠµì€ ì¸ê°„ ì¸ì§€ì˜ í•„ìˆ˜ ëŠ¥ë ¥ì´ì§€ë§Œ, í˜„ì¬ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œëŠ” ìƒˆë¡œìš´ ì§€ì‹ì´ ì´ì „ì— í•™ìŠµí•œ ì •ë³´ë¥¼ ë°©í•´í•˜ëŠ” 'íŒŒêµ­ì  ë§ê°' í˜„ìƒì´ ë°œìƒí•œë‹¤.

- 2. ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì€ ê¸°ì¡´ ì§€ì‹ê³¼ ê³¼ë§¤ê°œë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ë§ê°ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì™„í™”í•  ìˆ˜ ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„° ë¶„í¬ì— ì§ë©´í–ˆì„ ë•ŒëŠ” ì—¬ì „íˆ ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤.

- 3. LoRAì™€ ê°™ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì€ ìƒˆë¡œìš´ ì§€ì‹ì— ëŒ€í•œ íš¨ìœ¨ì  ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, ë™ì  í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì™€ ê¸´ ì‘ì—… ì‹œí€€ìŠ¤ì— í™•ì¥í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆë‹¤.

- 4. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì˜ ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ HAM(Hierarchical Adapters Merging)ì„ ì†Œê°œí•˜ë©°, ì´ëŠ” ë” ë§ì€ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

- 5. HAMì€ ì„¸ ê°€ì§€ ë¹„ì „ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ ì‘ì—… ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ìµœì²¨ë‹¨ ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.

---

*Generated on 2025-09-19 15:41:26*