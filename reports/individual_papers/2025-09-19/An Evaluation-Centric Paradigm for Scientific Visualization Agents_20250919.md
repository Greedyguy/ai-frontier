---
keywords:
  - Large Language Models
  - Evaluation Benchmarks
  - Agent Self-Improvement
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.15160
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:56:40.099050",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Evaluation Benchmarks",
    "Agent Self-Improvement"
  ],
  "rejected_keywords": [
    "Scientific Visualization"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Evaluation Benchmarks": 0.77,
    "Agent Self-Improvement": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# An Evaluation-Centric Paradigm for Scientific Visualization Agents

**Korean Title:** 과학적 시각화 에이전트를 위한 평가 중심 패러다임

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Large Language Models|multi-modal large language models]]
**⚡ Unique Technical**: [[keywords/Agent Self-Improvement|agent self-improvement]]
**🚀 Evolved Concepts**: [[keywords/Evaluation Benchmarks|evaluation benchmarks]]

## 🔗 유사한 논문
- [[EdiVal-Agent An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing]] (82.5% similar)
- [[Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents_20250919|Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents]] (81.5% similar)
- [[AgentCompass Towards Reliable Evaluation of Agentic Workflows in Production]] (80.8% similar)
- [[Ticket-Bench A Kickoff for Multilingual and Regionalized Agent Evaluation]] (80.4% similar)
- [[Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (80.2% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15160v1 Announce Type: cross 
Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled increasingly sophisticated autonomous visualization agents capable of translating user intentions into data visualizations. However, measuring progress and comparing different agents remains challenging, particularly in scientific visualization (SciVis), due to the absence of comprehensive, large-scale benchmarks for evaluating real-world capabilities. This position paper examines the various types of evaluation required for SciVis agents, outlines the associated challenges, provides a simple proof-of-concept evaluation example, and discusses how evaluation benchmarks can facilitate agent self-improvement. We advocate for a broader collaboration to develop a SciVis agentic evaluation benchmark that would not only assess existing capabilities but also drive innovation and stimulate future development in the field.

## 🔍 Abstract (한글 번역)

arXiv:2509.15160v1 발표 유형: 교차  
초록: 최근 다중 모달 대형 언어 모델(MLLMs)의 발전은 사용자 의도를 데이터 시각화로 변환할 수 있는 점점 더 정교한 자율 시각화 에이전트를 가능하게 했습니다. 그러나 과학 시각화(SciVis)에서 실제 능력을 평가할 수 있는 포괄적이고 대규모의 벤치마크가 부족하기 때문에 진전을 측정하고 다양한 에이전트를 비교하는 것은 여전히 어려운 과제입니다. 이 입장 논문은 SciVis 에이전트에 필요한 다양한 평가 유형을 검토하고, 관련된 도전 과제를 개략적으로 설명하며, 간단한 개념 증명 평가 예시를 제공하고, 평가 벤치마크가 에이전트의 자기 개선을 어떻게 촉진할 수 있는지를 논의합니다. 우리는 기존의 능력을 평가할 뿐만 아니라 혁신을 촉진하고 이 분야의 미래 발전을 자극할 수 있는 SciVis 에이전트 평가 벤치마크 개발을 위한 폭넓은 협력을 권장합니다.

## 📝 요약

최근 다중 모달 대형 언어 모델(MLLM)의 발전으로 사용자의 의도를 데이터 시각화로 변환하는 자율 시각화 에이전트가 발전했습니다. 그러나 과학적 시각화(SciVis) 분야에서 이러한 에이전트의 발전을 측정하고 비교하는 것은 여전히 어렵습니다. 본 논문은 SciVis 에이전트 평가에 필요한 다양한 평가 유형과 관련된 도전 과제를 분석하고, 간단한 평가 사례를 제시하며, 평가 벤치마크가 에이전트의 자기 개선을 어떻게 촉진할 수 있는지를 논의합니다. 또한, 기존 능력을 평가하고 혁신을 촉진할 SciVis 에이전트 평가 벤치마크 개발을 위한 협력을 제안합니다.

## 🎯 주요 포인트

- 1. 최근 다중 모달 대형 언어 모델(MLLMs)의 발전으로 사용자 의도를 데이터 시각화로 변환하는 자율 시각화 에이전트가 개발되고 있다.

- 2. 과학적 시각화(SciVis) 분야에서는 실질적 역량을 평가할 수 있는 대규모 벤치마크가 없어 에이전트의 발전 측정과 비교가 어렵다.

- 3. SciVis 에이전트 평가에 필요한 다양한 평가 유형과 관련된 도전 과제를 분석하고, 간단한 평가 예시를 제공한다.

- 4. 에이전트의 자기 개선을 촉진할 수 있는 평가 벤치마크의 중요성을 논의한다.

- 5. 기존 역량을 평가하고 혁신을 촉진할 수 있는 SciVis 에이전트 평가 벤치마크 개발을 위한 협력의 필요성을 강조한다.

---

*Generated on 2025-09-19 15:57:02*