---
keywords:
  - Retrieval-Augmented Generation
  - Large Language Models
  - Data Exfiltration Attacks
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14608
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:46:04.357249",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval-Augmented Generation",
    "Large Language Models",
    "Data Exfiltration Attacks"
  ],
  "rejected_keywords": [
    "Access Control",
    "Microsoft Copilot Tuning"
  ],
  "similarity_scores": {
    "Retrieval-Augmented Generation": 0.78,
    "Large Language Models": 0.8,
    "Data Exfiltration Attacks": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Enterprise AI Must Enforce Participant-Aware Access Control

**Korean Title:** 기업 인공지능은 참여자 인식 접근 제어를 강화해야 한다.

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**⚡ Unique Technical**: [[keywords/Retrieval-Augmented Generation|Retrieval-Augmented Generation]], [[keywords/Data Exfiltration Attacks|Data Exfiltration Attacks]]
**🚀 Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## 🔗 유사한 논문
- [[The Sum Leaks More Than Its Parts Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration]] (86.8% similar)
- [[CyberLLMInstruct A Pseudo-malicious Dataset Revealing Safety-performance Trade-offs in Cyber Security LLM Fine-tuning]] (84.8% similar)
- [[Language Models Identify Ambiguities and Exploit Loopholes]] (83.8% similar)
- [[Evaluating_and_Improving_the_Robustness_of_Security_Attack_Detectors_Generated_by_LLMs_20250918|Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs]] (83.3% similar)
- [[Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary]] (83.2% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14608v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings where they interact with multiple users and are trained or fine-tuned on sensitive internal data. While fine-tuning enhances performance by internalizing domain knowledge, it also introduces a critical security risk: leakage of confidential training data to unauthorized users. These risks are exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG) pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries can exploit current fine-tuning and RAG architectures to leak sensitive information by leveraging the lack of access control enforcement. We show that existing defenses, including prompt sanitization, output filtering, system isolation, and training-level privacy mechanisms, are fundamentally probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of fine-grained access control during both fine-tuning and RAG-based inference can reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in training, retrieval, or generation by an LLM is explicitly authorized for \emph{all users involved in the interaction}. Our approach offers a simple yet powerful paradigm shift for building secure multi-user LLM systems that are grounded in classical access control but adapted to the unique challenges of modern AI workflows. Our solution has been deployed in Microsoft Copilot Tuning, a product offering that enables organizations to fine-tune models using their own enterprise-specific data.

## 🔍 Abstract (한글 번역)

arXiv:2509.14608v1 발표 유형: 교차  
초록: 대형 언어 모델(LLM)은 여러 사용자가 상호작용하는 기업 환경에서 점점 더 많이 배치되고 있으며, 민감한 내부 데이터로 학습되거나 미세 조정됩니다. 미세 조정은 도메인 지식을 내재화하여 성능을 향상시키지만, 동시에 기밀 학습 데이터가 무단 사용자에게 유출될 수 있는 중요한 보안 위험을 초래합니다. 이러한 위험은 LLM이 추론 시점에 맥락적 문서를 동적으로 가져오는 검색-증강 생성(RAG) 파이프라인과 결합될 때 더욱 악화됩니다.  
우리는 AI 어시스턴트에 대한 데이터 유출 공격을 시연하며, 적대자가 현재의 미세 조정 및 RAG 아키텍처를 악용하여 접근 제어 시행의 부재를 이용해 민감한 정보를 유출할 수 있음을 보여줍니다. 프롬프트 정화, 출력 필터링, 시스템 격리, 학습 수준의 프라이버시 메커니즘을 포함한 기존 방어책이 본질적으로 확률적이며 이러한 공격에 대해 강력한 보호를 제공하지 못함을 증명합니다.  
우리는 미세 조정 및 RAG 기반 추론 중 세분화된 접근 제어의 결정적이고 엄격한 시행만이 민감한 데이터가 무단 수신자에게 유출되는 것을 신뢰성 있게 방지할 수 있다고 주장합니다.  
우리는 LLM이 학습, 검색 또는 생성에 사용하는 모든 콘텐츠가 상호작용에 참여하는 모든 사용자에게 명시적으로 승인되도록 하는 원칙에 중점을 둔 프레임워크를 소개합니다. 우리의 접근 방식은 고전적인 접근 제어에 기반을 두면서도 현대 AI 워크플로우의 독특한 도전에 적응한 강력한 패러다임 전환을 제공합니다. 우리의 솔루션은 조직이 자체 기업 특화 데이터를 사용하여 모델을 미세 조정할 수 있도록 하는 제품인 Microsoft Copilot Tuning에 배포되었습니다.

## 📝 요약

대형 언어 모델(LLM)의 기업 내 활용이 증가하면서 민감한 내부 데이터로 학습된 모델의 보안 위험이 대두되고 있습니다. 특히, 세부 조정 및 검색 증강 생성(RAG) 파이프라인과 결합될 때, 기밀 데이터가 무단으로 유출될 위험이 커집니다. 본 연구는 이러한 환경에서 데이터 유출 공격이 가능함을 보여주며, 기존의 방어 메커니즘이 충분히 견고하지 않음을 지적합니다. 이를 해결하기 위해, 세부적인 접근 제어를 엄격하게 적용하는 프레임워크를 제안합니다. 이 접근법은 모든 사용자에게 명시적으로 승인된 콘텐츠만을 활용하도록 하여, 안전한 다중 사용자 LLM 시스템 구축을 위한 새로운 패러다임을 제공합니다. 이 솔루션은 Microsoft Copilot Tuning에 적용되어 기업의 데이터로 모델을 안전하게 조정할 수 있도록 지원합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)의 미세 조정은 성능을 향상시키지만, 민감한 훈련 데이터가 무단 사용자에게 유출될 위험을 증가시킵니다.

- 2. RAG 파이프라인과 결합된 LLM은 문맥적 문서를 동적으로 가져오며, 이는 보안 위험을 더욱 악화시킵니다.

- 3. 기존 방어 메커니즘은 확률적 접근에 의존하여 민감한 정보 유출을 효과적으로 방지하지 못합니다.

- 4. 세밀한 접근 제어의 결정론적이고 엄격한 시행만이 데이터 유출을 방지할 수 있습니다.

- 5. 모든 사용자에게 명시적으로 승인된 콘텐츠만을 사용하도록 하는 프레임워크가 제안되었으며, 이는 Microsoft Copilot Tuning에 적용되었습니다.

---

*Generated on 2025-09-19 14:58:13*