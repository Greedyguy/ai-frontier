---
keywords:
  - Vision-Language Models
  - Multimodal Driver Attention Fusion
  - Attention Mechanism
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2502.18042
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:18:34.546321",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Models",
    "Multimodal Driver Attention Fusion",
    "Attention Mechanism"
  ],
  "rejected_keywords": [
    "Bird's-Eye-View"
  ],
  "similarity_scores": {
    "Vision-Language Models": 0.88,
    "Multimodal Driver Attention Fusion": 0.85,
    "Attention Mechanism": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion

**Korean Title:** VLM-E2E: ë‹¤ì¤‘ ëª¨ë‹¬ ìš´ì „ì ì£¼ì˜ ìœµí•©ì„ í†µí•œ ì¢…ë‹¨ ê°„ ììœ¨ ì£¼í–‰ í–¥ìƒ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Multimodal Driver Attention Fusion|Multimodal Driver Attention Fusion]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Models|Vision-Language Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[MAP End-to-End Autonomous Driving with Map-Assisted Planning]] (82.1% similar)
- [[BEVUDA++ Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection]] (82.1% similar)
- [[VSE-MOT Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement]] (81.8% similar)
- [[Embodied_Navigation_Foundation_Model_20250918|Embodied Navigation Foundation Model]] (81.3% similar)
- [[FishBEV Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras]] (81.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.18042v2 Announce Type: replace-cross 
Abstract: Human drivers adeptly navigate complex scenarios by utilizing rich attentional semantics, but the current autonomous systems struggle to replicate this ability, as they often lose critical semantic information when converting 2D observations into 3D space. In this sense, it hinders their effective deployment in dynamic and complex environments. Leveraging the superior scene understanding and reasoning abilities of Vision-Language Models (VLMs), we propose VLM-E2E, a novel framework that uses the VLMs to enhance training by providing attentional cues. Our method integrates textual representations into Bird's-Eye-View (BEV) features for semantic supervision, which enables the model to learn richer feature representations that explicitly capture the driver's attentional semantics. By focusing on attentional semantics, VLM-E2E better aligns with human-like driving behavior, which is critical for navigating dynamic and complex environments. Furthermore, we introduce a BEV-Text learnable weighted fusion strategy to address the issue of modality importance imbalance in fusing multimodal information. This approach dynamically balances the contributions of BEV and text features, ensuring that the complementary information from visual and textual modalities is effectively utilized. By explicitly addressing the imbalance in multimodal fusion, our method facilitates a more holistic and robust representation of driving environments. We evaluate VLM-E2E on the nuScenes dataset and achieve significant improvements in perception, prediction, and planning over the baseline end-to-end model, showcasing the effectiveness of our attention-enhanced BEV representation in enabling more accurate and reliable autonomous driving tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.18042v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ì¸ê°„ ìš´ì „ìëŠ” í’ë¶€í•œ ì£¼ì˜ ì˜ë¯¸ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ìƒí™©ì„ ëŠ¥ìˆ™í•˜ê²Œ íƒìƒ‰í•˜ì§€ë§Œ, í˜„ì¬ì˜ ììœ¨ ì‹œìŠ¤í…œì€ 2D ê´€ì°°ì„ 3D ê³µê°„ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì¤‘ìš”í•œ ì˜ë¯¸ ì •ë³´ë¥¼ ì¢…ì¢… ìƒì–´ë²„ë¦¬ê¸° ë•Œë¬¸ì— ì´ ëŠ¥ë ¥ì„ ë³µì œí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì ì—ì„œ ì´ëŠ” ë™ì ì´ê³  ë³µì¡í•œ í™˜ê²½ì—ì„œì˜ íš¨ê³¼ì ì¸ ë°°ì¹˜ë¥¼ ë°©í•´í•©ë‹ˆë‹¤. Vision-Language Models (VLMs)ì˜ ë›°ì–´ë‚œ ì¥ë©´ ì´í•´ ë° ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬, ìš°ë¦¬ëŠ” VLM-E2Eë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” VLMsë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì˜ ë‹¨ì„œë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ í›ˆë ¨ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ í…ìŠ¤íŠ¸ í‘œí˜„ì„ Bird's-Eye-View (BEV) íŠ¹ì§•ì— í†µí•©í•˜ì—¬ ì˜ë¯¸ë¡ ì  ê°ë…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ìš´ì „ìì˜ ì£¼ì˜ ì˜ë¯¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬ì°©í•˜ëŠ” ë” í’ë¶€í•œ íŠ¹ì§• í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì£¼ì˜ ì˜ë¯¸ì— ì§‘ì¤‘í•¨ìœ¼ë¡œì¨, VLM-E2EëŠ” ë™ì ì´ê³  ë³µì¡í•œ í™˜ê²½ì„ íƒìƒ‰í•˜ëŠ” ë° ì¤‘ìš”í•œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìš´ì „ í–‰ë™ê³¼ ë” ì˜ ì¼ì¹˜í•©ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì •ë³´ ìœµí•©ì—ì„œ ëª¨ë‹¬ë¦¬í‹° ì¤‘ìš”ì„± ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ BEV-í…ìŠ¤íŠ¸ í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ìœµí•© ì „ëµì„ ë„ì…í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ BEV ë° í…ìŠ¤íŠ¸ íŠ¹ì§•ì˜ ê¸°ì—¬ë¥¼ ë™ì ìœ¼ë¡œ ê· í˜• ì¡ì•„, ì‹œê° ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì˜ ë³´ì™„ì  ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ ìœµí•©ì˜ ë¶ˆê· í˜•ì„ ëª…ì‹œì ìœ¼ë¡œ í•´ê²°í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ì˜ ë°©ë²•ì€ ìš´ì „ í™˜ê²½ì˜ ë³´ë‹¤ ì „ì²´ì ì´ê³  ê²¬ê³ í•œ í‘œí˜„ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” nuScenes ë°ì´í„°ì…‹ì—ì„œ VLM-E2Eë¥¼ í‰ê°€í•˜ê³ , ì¸ì‹, ì˜ˆì¸¡ ë° ê³„íšì—ì„œ ê¸°ë³¸ì ì¸ ì¢…ë‹¨ ê°„ ëª¨ë¸ì— ë¹„í•´ ìƒë‹¹í•œ ê°œì„ ì„ ë‹¬ì„±í•˜ì—¬, ì£¼ì˜ ê°•í™”ëœ BEV í‘œí˜„ì´ ë³´ë‹¤ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ììœ¨ ì£¼í–‰ ì‘ì—…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì´ ì¸ê°„ ìš´ì „ìì˜ ì£¼ì˜ë ¥ ê¸°ë°˜ ìš´ì „ í–‰ë™ì„ ëª¨ë°©í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ VLM-E2Eë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. Vision-Language Models(VLMs)ì˜ ì¥ì ì„ í™œìš©í•˜ì—¬ ì£¼ì˜ë ¥ ë‹¨ì„œë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ ìš´ì „ìì˜ ì£¼ì˜ë ¥ ì˜ë¯¸ë¥¼ í¬ì°©í•˜ëŠ” í’ë¶€í•œ íŠ¹ì§• í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤. ë˜í•œ, BEV-Text ê°€ì¤‘ì¹˜ ìœµí•© ì „ëµì„ ë„ì…í•˜ì—¬ ì‹œê° ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì˜ ì¤‘ìš”ì„± ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì´ë¥¼ í†µí•´ ìš´ì „ í™˜ê²½ì˜ ë” í¬ê´„ì ì´ê³  ê°•ë ¥í•œ í‘œí˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. nuScenes ë°ì´í„°ì…‹ì„ í†µí•´ í‰ê°€í•œ ê²°ê³¼, ì¸ì‹, ì˜ˆì¸¡ ë° ê³„íšì—ì„œ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VLM-E2EëŠ” Vision-Language Models(VLMs)ë¥¼ í™œìš©í•˜ì—¬ ìš´ì „ìì˜ ì£¼ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ê³ , ë³µì¡í•œ í™˜ê²½ì—ì„œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìš´ì „ í–‰ë™ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

- 2. Bird's-Eye-View(BEV) íŠ¹ì§•ì— í…ìŠ¤íŠ¸ í‘œí˜„ì„ í†µí•©í•˜ì—¬ ì˜ë¯¸ì  ê°ë…ì„ ì œê³µí•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ ìš´ì „ìì˜ ì£¼ì˜ ì˜ë¯¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬ì°©í•˜ëŠ” í’ë¶€í•œ íŠ¹ì§• í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.

- 3. BEV-Text í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ìœµí•© ì „ëµì„ ë„ì…í•˜ì—¬, ì‹œê° ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì˜ ì¤‘ìš”ì„± ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë©€í‹°ëª¨ë‹¬ ì •ë³´ì˜ ìƒí˜¸ë³´ì™„ì  í™œìš©ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

- 4. VLM-E2EëŠ” nuScenes ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ë˜ì–´, ì¸ì‹, ì˜ˆì¸¡, ê³„íš ì¸¡ë©´ì—ì„œ ê¸°ì¡´ì˜ ì—”ë“œíˆ¬ì—”ë“œ ëª¨ë¸ë³´ë‹¤ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:12:19*