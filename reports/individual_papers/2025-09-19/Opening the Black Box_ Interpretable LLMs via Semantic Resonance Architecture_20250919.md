
# Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture

**Korean Title:** ë¸”ë™ ë°•ìŠ¤ ì—´ê¸°: ì˜ë¯¸ ê³µëª… ì•„í‚¤í…ì²˜ë¥¼ í†µí•œ í•´ì„ ê°€ëŠ¥í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Semantic Routing

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (84.7% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (84.2% similar)
- [[CSMoE An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts]] (83.1% similar)
- [[MAgICoRe Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning]] (82.9% similar)
- [[Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary]] (82.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14255v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14255v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ë§Œ í•´ì„í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì „ë¬¸ê°€ í˜¼í•©(MoE) ëª¨ë¸ì€ í¬ì†Œ í™œì„±í™”ë¥¼ í†µí•´ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆíˆ¬ëª…í•œ í•™ìŠµ ê²Œì´íŒ… ê¸°ëŠ¥ì— ì˜ì¡´í•©ë‹ˆë‹¤. ìœ ì‚¬ì„± ê¸°ë°˜ ë¼ìš°íŒ…(ì½”ì‚¬ì¸ ë¼ìš°í„°)ì€ í›ˆë ¨ ì•ˆì •í™”ë¥¼ ìœ„í•´ íƒêµ¬ë˜ì—ˆì§€ë§Œ, ë‚´ì¬ì  í•´ì„ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì ì¬ë ¥ì€ í¬ê²Œ í™œìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¼ìš°íŒ… ê²°ì •ì´ ë³¸ì§ˆì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ëœ MoE ì ‘ê·¼ ë°©ì‹ì¸ ì˜ë¯¸ ê³µëª… ì•„í‚¤í…ì²˜(SRA)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. SRAëŠ” í•™ìŠµëœ ê²Œì´íŒ…ì„ í›ˆë ¨ ê°€ëŠ¥í•œ ì˜ë¯¸ ì•µì»¤ì™€ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì— ê¸°ë°˜í•˜ì—¬ í† í°ì„ ë¼ìš°íŒ…í•˜ëŠ” ì˜ë¯¸ ê³µëª… ì±”ë²„(CSR) ëª¨ë“ˆë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. ë˜í•œ ì•µì»¤ ê°„ì˜ ì§êµì„±ì„ ì¥ë ¤í•˜ì—¬ ë‹¤ì–‘í•œ ì „ë¬¸í™”ë¥¼ ê°•í™”í•˜ëŠ” ìƒˆë¡œìš´ ë¶„ì‚° ì†ì‹¤ì„ ë„ì…í•©ë‹ˆë‹¤. WikiText-103ì— ëŒ€í•œ ì‹¤í—˜ì€ SRAê°€ ê²€ì¦ í˜¼ë€ë„ 13.41ì„ ë‹¬ì„±í•˜ì—¬ ë°€ì§‘ ê¸°ì¤€ì„ (14.13)ê³¼ í‘œì¤€ MoE ê¸°ì¤€ì„ (13.53)ì„ ëŠ¥ë™ ë§¤ê°œë³€ìˆ˜ ì œì•½(29.0M) í•˜ì—ì„œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€, SRAê°€ ì „ë¬¸ê°€ í™œìš©ë„ì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì´ë©°(ë¹„í™œì„± ì „ë¬¸ê°€ 1.0% vs. í‘œì¤€ MoEì˜ 14.8%) í‘œì¤€ MoEì—ì„œ ê´€ì°°ë˜ëŠ” ì†ŒìŒì´ ë§ì€ ì „ë¬¸í™”ì™€ ë‹¬ë¦¬ ëšœë ·í•˜ê³  ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì¼ê´€ëœ ì „ë¬¸í™” íŒ¨í„´ì„ ê°œë°œí•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë³´ë‹¤ íˆ¬ëª…í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ ë°©ë²•ë¡ ìœ¼ë¡œ ì˜ë¯¸ ë¼ìš°íŒ…ì„ í™•ë¦½í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì´ê¸° ìœ„í•´ í˜¼í•© ì „ë¬¸ê°€(MoE) ëª¨ë¸ì˜ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ Semantic Resonance Architecture(SRA)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. SRAëŠ” ê¸°ì¡´ì˜ ë¶ˆíˆ¬ëª…í•œ ê²Œì´íŒ… ê¸°ëŠ¥ì„ ëŒ€ì²´í•˜ì—¬, ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í† í°ì„ ë¼ìš°íŒ…í•˜ëŠ” Chamber of Semantic Resonance(CSR) ëª¨ë“ˆì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ, ì•µì»¤ ê°„ì˜ ì§êµì„±ì„ ì´‰ì§„í•˜ëŠ” Dispersion Lossë¥¼ ë„ì…í•˜ì—¬ ë‹¤ì–‘í•œ ì „ë¬¸í™”ë¥¼ ìœ ë„í•©ë‹ˆë‹¤. WikiText-103 ì‹¤í—˜ ê²°ê³¼, SRAëŠ” ê²€ì¦ í¼í”Œë ‰ì‹œí‹° 13.41ì„ ê¸°ë¡í•˜ë©°, ë°€ì§‘ ë° í‘œì¤€ MoE ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ, SRAëŠ” ì „ë¬¸ê°€ í™œìš©ë„ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì´ë©°, ëª…í™•í•˜ê³  ì¼ê´€ëœ ì „ë¬¸í™” íŒ¨í„´ì„ í˜•ì„±í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ íˆ¬ëª…ì„±ê³¼ ì œì–´ ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ë° ìˆì–´ ì˜ë¯¸ ê¸°ë°˜ ë¼ìš°íŒ…ì˜ ì ì¬ë ¥ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì´ê¸° ìœ„í•´ SRA(Semantic Resonance Architecture)ë¥¼ ë„ì…í•˜ì—¬ í† í° ë¼ìš°íŒ…ì„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- 2. SRAëŠ” í•™ìŠµëœ ê²Œì´íŒ… ëŒ€ì‹  CSR(Chamber of Semantic Resonance) ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ í•´ì„ ê°€ëŠ¥í•œ ë¼ìš°íŒ… ê²°ì •ì„ ë³´ì¥í•©ë‹ˆë‹¤.

- 3. ìƒˆë¡œìš´ ë””ìŠ¤í¼ì…˜ ì†ì‹¤(Dispersion Loss)ì„ ë„ì…í•˜ì—¬ ì•µì»¤ ê°„ì˜ ì§êµì„±ì„ ì´‰ì§„í•˜ê³  ë‹¤ì–‘í•œ ì „ë¬¸í™”ë¥¼ ê°•í™”í•©ë‹ˆë‹¤.

- 4. WikiText-103 ì‹¤í—˜ì—ì„œ SRAëŠ” ê²€ì¦ í¼í”Œë ‰ì‹œí‹° 13.41ì„ ë‹¬ì„±í•˜ì—¬ ë°€ì§‘ ë² ì´ìŠ¤ë¼ì¸(14.13)ê³¼ í‘œì¤€ MoE ë² ì´ìŠ¤ë¼ì¸(13.53)ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

- 5. SRAëŠ” ì „ë¬¸ê°€ í™œìš©ë„ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, í‘œì¤€ MoEì™€ ë‹¬ë¦¬ ëª…í™•í•˜ê³  ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì¼ê´€ëœ ì „ë¬¸í™” íŒ¨í„´ì„ ê°œë°œí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 14:50:43*