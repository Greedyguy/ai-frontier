---
keywords:
  - Reinforcement Learning
  - Large Language Models
  - Reward Shaping
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2503.22723
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:16:53.088019",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Large Language Models",
    "Reward Shaping"
  ],
  "rejected_keywords": [
    "Human-in-the-Loop",
    "Bias Correction"
  ],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Large Language Models": 0.8,
    "Reward Shaping": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping

**Korean Title:** μ λ΅μƒ· λ€ν• μ–Έμ–΄ λ¨λΈ(LLM)μ—μ„μ μΈκ°„ μ°Έμ—¬ κ°•ν™” ν•™μµ(RL): λ³΄μƒ ν•μ„±μ„ μ„ν• μΈκ°„ ν”Όλ“λ°± λ€μ²΄

## π“‹ λ©”νƒ€λ°μ΄ν„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## π·οΈ μΉ΄ν…κ³ λ¦¬ν™”λ ν‚¤μ›λ“
**π”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**β΅ Unique Technical**: [[keywords/Reward Shaping|Reward Shaping]]
**π€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## π”— μ μ‚¬ν• λ…Όλ¬Έ
- [[FlowRL Matching Reward Distributions for LLM Reasoning]] (84.7% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (84.0% similar)
- [[SMARTER A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models]] (82.9% similar)
- [[Do LLMs Align Human Values Regarding Social Biases_ Judging and Explaining Social Biases with LLMs_20250918|Do LLMs Align Human Values Regarding Social Biases Judging and Explaining Social Biases with LLMs]] (82.8% similar)
- [[Judging with Many Minds Do More Perspectives Mean Less Prejudice On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge]] (82.8% similar)

## π“‹ μ €μ μ •λ³΄

**Authors:** 

## π“„ Abstract (μ›λ¬Έ)

arXiv:2503.22723v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) often struggles with reward misalignment, where agents optimize given rewards but fail to exhibit the desired behaviors. This arises when the reward function incentivizes proxy behaviors misaligned with the true objective. While human-in-the-loop (HITL) methods can mitigate this issue, they also introduce biases, leading to inconsistent and subjective feedback that complicates learning. To address these challenges, we propose two key contributions. First, we extend the use of zero-shot, off-the-shelf large language models (LLMs) for reward shaping beyond natural language processing (NLP) to continuous control tasks. Using LLMs as direct feedback providers eliminates the need for surrogate models trained on human feedback, which often inherit biases from training data. Second, we introduce a hybrid framework (LLM-HFBF) that enables LLMs to identify and correct biases in human feedback while incorporating this feedback into the reward shaping process. The LLM-HFBF framework creates a more balanced and reliable system by addressing both the limitations of LLMs (e.g., lack of domain-specific knowledge) and human supervision (e.g., inherent biases). By enabling human feedback bias flagging and correction, our approach improves reinforcement learning performance and reduces reliance on potentially biased human feedback. Empirical experiments show that biased human feedback significantly reduces performance, with Average Episodic Reward dropping by nearly 94% compared to unbiased approaches. In contrast, LLM-based methods sustain performance at a similar level to unbiased feedback, even in challenging edge-case scenarios.

## π” Abstract (ν•κΈ€ λ²μ—­)

arXiv:2503.22723v2 λ°ν‘ μ ν•: κµμ°¨ κµμ²΄  
μ΄λ΅: κ°•ν™” ν•™μµ(RL)μ€ μΆ…μΆ… λ³΄μƒ λ¶μΌμΉλ΅ μ–΄λ ¤μ›€μ„ κ²λ”λ°, μ΄λ” μ—μ΄μ „νΈκ°€ μ£Όμ–΄μ§„ λ³΄μƒμ„ μµμ ν™”ν•μ§€λ§ μ›ν•λ” ν–‰λ™μ„ λ³΄μ΄μ§€ λ»ν•λ” κ²½μ°μ…λ‹λ‹¤. μ΄λ” λ³΄μƒ ν•¨μκ°€ μ‹¤μ  λ©ν‘μ™€ μΌμΉν•μ§€ μ•λ” λ€λ¦¬ ν–‰λ™μ„ μ¥λ ¤ν•  λ• λ°μƒν•©λ‹λ‹¤. μΈκ°„μ΄ κ°μ…ν•λ” λ£¨ν”„(HITL) λ°©λ²•μ€ μ΄ λ¬Έμ λ¥Ό μ™„ν™”ν•  μ μμ§€λ§, νΈν–¥μ„ λ„μ…ν•μ—¬ ν•™μµμ„ λ³µμ΅ν•κ² λ§λ“λ” μΌκ΄€μ„± μ—†λ” μ£Όκ΄€μ  ν”Όλ“λ°±μ„ μ΄λν•©λ‹λ‹¤. μ΄λ¬ν• λ¬Έμ λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄ μ°λ¦¬λ” λ‘ κ°€μ§€ μ£Όμ” κΈ°μ—¬λ¥Ό μ μ•ν•©λ‹λ‹¤. μ²«μ§Έ, μμ—°μ–΄ μ²λ¦¬(NLP)λ¥Ό λ„μ–΄ μ—°μ† μ μ–΄ μ‘μ—…μ— λ€ν• λ³΄μƒ ν•μ„±μ„ μ„ν•΄ μ λ΅μƒ·, κΈ°μ„±ν’ λ€ν• μ–Έμ–΄ λ¨λΈ(LLM)μ μ‚¬μ©μ„ ν™•μ¥ν•©λ‹λ‹¤. LLMμ„ μ§μ ‘ ν”Όλ“λ°± μ κ³µμλ΅ μ‚¬μ©ν•λ©΄ μΆ…μΆ… ν›λ ¨ λ°μ΄ν„°μ—μ„ νΈν–¥μ„ μƒμ†λ°›λ” μΈκ°„ ν”Όλ“λ°±μΌλ΅ ν›λ ¨λ λ€λ¦¬ λ¨λΈμ ν•„μ”μ„±μ΄ μ‚¬λΌμ§‘λ‹λ‹¤. λ‘μ§Έ, LLMμ΄ μΈκ°„ ν”Όλ“λ°±μ νΈν–¥μ„ μ‹λ³„ν•κ³  μμ •ν•λ©΄μ„ μ΄ ν”Όλ“λ°±μ„ λ³΄μƒ ν•μ„± κ³Όμ •μ— ν†µν•©ν•  μ μλ” ν•μ΄λΈλ¦¬λ“ ν”„λ μ„μ›ν¬(LLM-HFBF)λ¥Ό μ†κ°ν•©λ‹λ‹¤. LLM-HFBF ν”„λ μ„μ›ν¬λ” LLMμ ν•κ³„(μ: λ„λ©”μΈλ³„ μ§€μ‹ λ¶€μ΅±)μ™€ μΈκ°„ κ°λ…(μ: κ³ μ ν• νΈν–¥)μ„ λ¨λ‘ ν•΄κ²°ν•μ—¬ λ³΄λ‹¤ κ· ν• μ΅νκ³  μ‹ λΆ°ν•  μ μλ” μ‹μ¤ν…μ„ λ§λ“­λ‹λ‹¤. μΈκ°„ ν”Όλ“λ°±μ νΈν–¥ ν”λκ·Έ μ§€μ • λ° μμ •μ„ κ°€λ¥ν•κ² ν•¨μΌλ΅μ¨, μ°λ¦¬μ μ ‘κ·Ό λ°©μ‹μ€ κ°•ν™” ν•™μµ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¤κ³  μ μ¬μ μΌλ΅ νΈν–¥λ μΈκ°„ ν”Όλ“λ°±μ— λ€ν• μμ΅΄λ„λ¥Ό μ¤„μ…λ‹λ‹¤. μ‹¤ν— κ²°κ³Ό, νΈν–¥λ μΈκ°„ ν”Όλ“λ°±μ€ μ„±λ¥μ„ ν¬κ² μ €ν•μ‹μΌ ν‰κ·  μ—ν”Όμ†λ“ λ³΄μƒμ΄ νΈν–¥λμ§€ μ•μ€ μ ‘κ·Όλ²•μ— λΉ„ν•΄ κ±°μ 94% κ°μ†ν•λ” κ²ƒμΌλ΅ λ‚νƒ€λ‚¬μµλ‹λ‹¤. λ°λ©΄, LLM κΈ°λ° λ°©λ²•μ€ κΉλ‹¤λ΅μ΄ μ—£μ§€ μΌ€μ΄μ¤ μ‹λ‚λ¦¬μ¤μ—μ„λ„ νΈν–¥λμ§€ μ•μ€ ν”Όλ“λ°±κ³Ό μ μ‚¬ν• μμ¤€μ μ„±λ¥μ„ μ μ§€ν•©λ‹λ‹¤.

## π“ μ”μ•½

κ°•ν™” ν•™μµ(RL)μ€ λ³΄μƒ λ¶μΌμΉ λ¬Έμ λ΅ μΈν•΄ μ›ν•λ” ν–‰λ™μ„ μ μν–‰ν•μ§€ λ»ν•λ” κ²½μ°κ°€ λ§μµλ‹λ‹¤. μ΄λ” λ³΄μƒ ν•¨μκ°€ μ‹¤μ  λ©ν‘μ™€ μΌμΉν•μ§€ μ•λ” λ€λ¦¬ ν–‰λ™μ„ μ λ„ν•  λ• λ°μƒν•©λ‹λ‹¤. μΈκ°„ μ°Έμ—¬(HITL) λ°©λ²•μ€ μ΄λ¥Ό μ™„ν™”ν•  μ μμ§€λ§, μ£Όκ΄€μ  ν”Όλ“λ°±μΌλ΅ μΈν•΄ ν•™μµμ΄ λ³µμ΅ν•΄μ§‘λ‹λ‹¤. μ΄λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄ λ‘ κ°€μ§€ μ£Όμ” κΈ°μ—¬λ¥Ό μ μ•ν•©λ‹λ‹¤. μ²«μ§Έ, λ€ν• μ–Έμ–΄ λ¨λΈ(LLM)μ„ μμ—°μ–΄ μ²λ¦¬(NLP) μ™Έμ μ—°μ† μ μ–΄ μ‘μ—…μ—μ„λ„ λ³΄μƒ ν•μ„±μ— ν™μ©ν•©λ‹λ‹¤. λ‘μ§Έ, LLMμ΄ μΈκ°„ ν”Όλ“λ°±μ νΈν–¥μ„ μ‹λ³„ν•κ³  μμ •ν•  μ μλ” ν•μ΄λΈλ¦¬λ“ ν”„λ μ„μ›ν¬(LLM-HFBF)λ¥Ό λ„μ…ν•©λ‹λ‹¤. μ΄ ν”„λ μ„μ›ν¬λ” LLMκ³Ό μΈκ°„ κ°λ…μ ν•κ³„λ¥Ό λ³΄μ™„ν•μ—¬ κ°•ν™” ν•™μµ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¤κ³  νΈν–¥λ μΈκ°„ ν”Όλ“λ°± μμ΅΄λ„λ¥Ό μ¤„μ…λ‹λ‹¤. μ‹¤ν— κ²°κ³Ό, νΈν–¥λ μΈκ°„ ν”Όλ“λ°±μ€ μ„±λ¥μ„ ν¬κ² κ°μ†μ‹ν‚¤μ§€λ§, LLM κΈ°λ° λ°©λ²•μ€ μ„±λ¥μ„ μ μ§€ν•©λ‹λ‹¤.

## π― μ£Όμ” ν¬μΈνΈ

- 1. κ°•ν™” ν•™μµ(RL)μ€ λ³΄μƒ λ¶μΌμΉ λ¬Έμ λ΅ μΈν•΄ μ›ν•λ” ν–‰λ™μ„ λ³΄μ—¬μ£Όμ§€ λ»ν•  λ•κ°€ λ§μµλ‹λ‹¤.

- 2. μΈκ°„ μ°Έμ—¬ λ°©λ²•(HITL)μ€ λ³΄μƒ λ¶μΌμΉ λ¬Έμ λ¥Ό μ™„ν™”ν•  μ μμ§€λ§, μ£Όκ΄€μ  ν”Όλ“λ°±μΌλ΅ μΈν•΄ ν•™μµμ΄ λ³µμ΅ν•΄μ§‘λ‹λ‹¤.

- 3. μ μ•λ λ°©λ²•μ€ λ€ν• μ–Έμ–΄ λ¨λΈ(LLM)μ„ λ³΄μƒ ν•μ„±μ— ν™μ©ν•μ—¬ μΈκ°„ ν”Όλ“λ°±μ νΈν–¥μ„±μ„ μ¤„μ…λ‹λ‹¤.

- 4. LLM-HFBF ν”„λ μ„μ›ν¬λ” μΈκ°„ ν”Όλ“λ°±μ νΈν–¥μ„ μ‹λ³„ν•κ³  μμ •ν•μ—¬ κ°•ν™” ν•™μµ μ„±λ¥μ„ ν–¥μƒμ‹ν‚µλ‹λ‹¤.

- 5. μ‹¤ν— κ²°κ³Ό, νΈν–¥λ μΈκ°„ ν”Όλ“λ°±μ€ μ„±λ¥μ„ ν¬κ² μ €ν•μ‹μΌ°μ§€λ§, LLM κΈ°λ° λ°©λ²•μ€ μ„±λ¥μ„ μ μ§€ν–μµλ‹λ‹¤.

---

*Generated on 2025-09-19 15:13:45*