---
keywords:
  - Model Merging
  - Multi-Task Capabilities
  - Linear Representation Hypothesis
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2502.10698
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:22:40.249632",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Model Merging",
    "Multi-Task Capabilities",
    "Linear Representation Hypothesis"
  ],
  "rejected_keywords": [
    "Neural Networks",
    "Feature Vectors"
  ],
  "similarity_scores": {
    "Model Merging": 0.85,
    "Multi-Task Capabilities": 0.8,
    "Linear Representation Hypothesis": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Superpose Task-specific Features for Model Merging

**Korean Title:** ëª¨ë¸ ë³‘í•©ì„ ìœ„í•œ ì‘ì—…ë³„ íŠ¹ì§• ì¤‘ì²©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multi-Task Capabilities|Multi-Task Capabilities]]
**âš¡ Unique Technical**: [[keywords/Model Merging|Model Merging]], [[keywords/Linear Representation Hypothesis|Linear Representation Hypothesis]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (84.2% similar)
- [[HAM Hierarchical Adapter Merging for Scalable Continual Learning]] (81.9% similar)
- [[LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (80.6% similar)
- [[Masked_Feature_Modeling_Enhances_Adaptive_Segmentation_20250918|Masked Feature Modeling Enhances Adaptive Segmentation]] (80.3% similar)
- [[Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (79.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.10698v2 Announce Type: replace-cross 
Abstract: Model merging enables powerful capabilities in neural networks without requiring additional training. In this paper, we introduce a novel perspective on model merging by leveraging the fundamental mechanisms of neural network representation. Our approach is motivated by the linear representation hypothesis, which states that neural networks encode information through linear combinations of feature vectors. We propose a method that superposes task-specific features from individual models into a merged model. Our approach specifically targets linear transformation matrices, which are crucial for feature activation and extraction in deep networks. By formulating the merging process as a linear system, we can preserve task-specific features from individual models and create merged models that effectively maintain multi-task capabilities compared to existing methods. Extensive experiments across diverse benchmarks and models demonstrate that our method outperforms existing techniques. Code is available at https://github.com/LARS-research/STF.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.10698v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ëª¨ë¸ ë³‘í•©ì€ ì¶”ê°€ì ì¸ í›ˆë ¨ ì—†ì´ ì‹ ê²½ë§ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹ ê²½ë§ í‘œí˜„ì˜ ê¸°ë³¸ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ ë³‘í•©ì— ëŒ€í•œ ìƒˆë¡œìš´ ê´€ì ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ì‹ ê²½ë§ì´ íŠ¹ì§• ë²¡í„°ì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ ì •ë³´ë¥¼ ì¸ì½”ë”©í•œë‹¤ëŠ” ì„ í˜• í‘œí˜„ ê°€ì„¤ì— ì˜í•´ ë™ê¸° ë¶€ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê°œë³„ ëª¨ë¸ì˜ ì‘ì—…ë³„ íŠ¹ì§•ì„ ë³‘í•©ëœ ëª¨ë¸ì— ì¤‘ì²©ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ íŠ¹íˆ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì—ì„œ íŠ¹ì§• í™œì„±í™”ì™€ ì¶”ì¶œì— ì¤‘ìš”í•œ ì„ í˜• ë³€í™˜ í–‰ë ¬ì„ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. ë³‘í•© ê³¼ì •ì„ ì„ í˜• ì‹œìŠ¤í…œìœ¼ë¡œ ê³µì‹í™”í•¨ìœ¼ë¡œì¨, ê°œë³„ ëª¨ë¸ì˜ ì‘ì—…ë³„ íŠ¹ì§•ì„ ë³´ì¡´í•˜ê³  ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ì¤‘ ì‘ì—… ê¸°ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€í•˜ëŠ” ë³‘í•© ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ëª¨ë¸ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ìš°ë¦¬ì˜ ë°©ë²•ì´ ê¸°ì¡´ ê¸°ìˆ ì„ ëŠ¥ê°€í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤. ì½”ë“œëŠ” https://github.com/LARS-research/STFì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¶”ê°€ í•™ìŠµ ì—†ì´ ì‹ ê²½ë§ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ë³‘í•©ì— ëŒ€í•œ ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì‹ ê²½ë§ì´ ì •ë³´ ì¸ì½”ë”©ì„ ì„ í˜• ì¡°í•©ì„ í†µí•´ ìˆ˜í–‰í•œë‹¤ëŠ” ê°€ì„¤ì„ ë°”íƒ•ìœ¼ë¡œ, ê°œë³„ ëª¨ë¸ì˜ ì‘ì—…ë³„ íŠ¹ì§•ì„ ë³‘í•© ëª¨ë¸ì— ì¤‘ì²©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, íŠ¹ì§• í™œì„±í™”ì™€ ì¶”ì¶œì— ì¤‘ìš”í•œ ì„ í˜• ë³€í™˜ í–‰ë ¬ì„ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬ ë³‘í•© ê³¼ì •ì„ ì„ í˜• ì‹œìŠ¤í…œìœ¼ë¡œ ê³µì‹í™”í•¨ìœ¼ë¡œì¨, ê°œë³„ ëª¨ë¸ì˜ ì‘ì—…ë³„ íŠ¹ì§•ì„ ë³´ì¡´í•˜ë©´ì„œë„ ë‹¤ì¤‘ ì‘ì—… ê¸°ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë³‘í•© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ê¸°ìˆ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì½”ë“œë„ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ì¶”ê°€ì ì¸ í•™ìŠµ ì—†ì´ ì‹ ê²½ë§ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ëª¨ë¸ ë³‘í•©ì— ëŒ€í•œ ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

- 2. ëª¨ë¸ ë³‘í•©ì„ ìœ„í•´ ì„ í˜• í‘œí˜„ ê°€ì„¤ì„ í™œìš©í•˜ì—¬ ê°œë³„ ëª¨ë¸ì˜ ì‘ì—…ë³„ íŠ¹ì§•ì„ ë³‘í•© ëª¨ë¸ì— ì¤‘ì²©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 3. ì„ í˜• ë³€í™˜ í–‰ë ¬ì„ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬, ê°œë³„ ëª¨ë¸ì˜ ì‘ì—…ë³„ íŠ¹ì§•ì„ ë³´ì¡´í•˜ë©´ì„œ ë‹¤ì¤‘ ì‘ì—… ê¸°ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€í•˜ëŠ” ë³‘í•© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

- 4. ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ê¸°ìˆ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í•©ë‹ˆë‹¤.

- 5. ì—°êµ¬ì˜ ì½”ë“œëŠ” https://github.com/LARS-research/STFì—ì„œ ì œê³µë©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:11:05*