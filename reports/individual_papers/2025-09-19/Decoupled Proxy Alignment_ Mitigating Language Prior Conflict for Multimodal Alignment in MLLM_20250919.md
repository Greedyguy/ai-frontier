
# Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM

**Korean Title:** ë””ì»¤í”Œë“œ í”„ë¡ì‹œ ì •ë ¬: MLLMì—ì„œ ë‹¤ì¤‘ ëª¨ë‹¬ ì •ë ¬ì„ ìœ„í•œ ì–¸ì–´ ì„ í–‰ ì¶©ëŒ ì™„í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Language Prior Conflict Mitigation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (87.6% similar)
- [[DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (86.9% similar)
- [[Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (86.9% similar)
- [[Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (85.5% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (84.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14735v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at https://github.com/fnlp-vision/DPA.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14735v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Models, MLLMs)ì€ ì‹œê°ê³¼ ì–¸ì–´ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ëŠ” ë›°ì–´ë‚œ ëŠ¥ë ¥ìœ¼ë¡œ ì¸í•´ í° ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ìµœê·¼ MLLMsì˜ ë°œì „ì€ ì£¼ë¡œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹, ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜, ìµœì í™”ëœ í•™ìŠµ ì „ëµì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ì „ì— ê°„ê³¼ëœ ë¬¸ì œì¸ ì–¸ì–´ ì„ í—˜ ì¶©ëŒ(language prior conflict)ì„ ì‹ë³„í•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ê³ ìœ í•œ ì–¸ì–´ ì„ í—˜ê³¼ í•™ìŠµ ë°ì´í„°ì…‹ì˜ ì–¸ì–´ ì„ í—˜ ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ë¡œ, MLLMsì´ í•™ìŠµ ìƒ˜í”Œì˜ ì–¸ì–´ ìŠ¤íƒ€ì¼ì— ì ì‘í•˜ê¸° ì‰¬ì›Œ ìµœì ì´ ì•„ë‹Œ ì‹œê°-ì–¸ì–´ ì •ë ¬ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” Decoupled Proxy Alignment (DPA)ë¼ëŠ” ìƒˆë¡œìš´ í•™ìŠµ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. DPAëŠ” ë‘ ê°€ì§€ ì£¼ìš” í˜ì‹ ì„ ë„ì…í•©ë‹ˆë‹¤: (1) í”„ë¡ì‹œ LLMì„ ì‚¬ì „ í•™ìŠµ ë™ì•ˆ ì‚¬ìš©í•˜ì—¬ ì‹œê°-ì–¸ì–´ ì •ë ¬ ê³¼ì •ì„ ì–¸ì–´ ì„ í—˜ ê°„ì„­ìœ¼ë¡œë¶€í„° ë¶„ë¦¬í•˜ê³ , (2) ì‹œê°ì  ê´€ë ¨ì„±ì— ê¸°ë°˜í•œ ë™ì  ì†ì‹¤ ì¡°ì •ì„ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ê´€ë ¨ëœ í† í°ì— ëŒ€í•œ ìµœì í™” ì‹ í˜¸ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ DPAê°€ ì–¸ì–´ ì„ í—˜ ì¶©ëŒì„ í¬ê²Œ ì™„í™”í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹, ëª¨ë¸ ê³„ì—´, ê·œëª¨ì— ê±¸ì³ ìš°ìˆ˜í•œ ì •ë ¬ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ MLLM í•™ìŠµì˜ íš¨ê³¼ë¥¼ í–¥ìƒì‹œí‚¬ ë¿ë§Œ ì•„ë‹ˆë¼ ë›°ì–´ë‚œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì–´ ì‹œê°-ì–¸ì–´ ì •ë ¬ì„ ìœ„í•œ ê°•ë ¥í•œ ì ‘ê·¼ë²•ì´ ë©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/fnlp-vision/DPAì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ì–¸ì–´ ì„ í–‰ ì¶©ëŒ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ê³¼ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì— ì§‘ì¤‘í–ˆì§€ë§Œ, ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì–¸ì–´ ì„ í–‰ê³¼ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ì–¸ì–´ ì„ í–‰ ê°„ ë¶ˆì¼ì¹˜ê°€ ì‹œê°-ì–¸ì–´ ì •ë ¬ì„ ì €í•´í•œë‹¤ê³  ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ìë“¤ì€ 'ë¶„ë¦¬ í”„ë¡ì‹œ ì •ë ¬(DPA)'ì´ë¼ëŠ” ìƒˆë¡œìš´ í›ˆë ¨ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. DPAëŠ” í”„ë¡ì‹œ LLMì„ ì‚¬ìš©í•´ ì‹œê°-ì–¸ì–´ ì •ë ¬ì„ ì–¸ì–´ ì„ í–‰ ê°„ì„­ì—ì„œ ë¶„ë¦¬í•˜ê³ , ì‹œê°ì  ê´€ë ¨ì„±ì— ê¸°ë°˜í•œ ë™ì  ì†ì‹¤ ì¡°ì •ì„ í†µí•´ ìµœì í™” ì‹ í˜¸ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DPAëŠ” ì–¸ì–´ ì„ í–‰ ì¶©ëŒì„ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•˜ë©°, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì—ì„œ ìš°ìˆ˜í•œ ì •ë ¬ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ì‹œê°ê³¼ ì–¸ì–´ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

- 2. ê¸°ì¡´ ì—°êµ¬ëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ì…‹, ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜, ìµœì í™”ëœ í›ˆë ¨ ì „ëµì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì— ì£¼ë ¥í•´ì™”ìŠµë‹ˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ê³ ìœ  ì–¸ì–´ ì„ í—˜ê³¼ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ì–¸ì–´ ì„ í—˜ ê°„ì˜ ë¶ˆì¼ì¹˜ì¸ ì–¸ì–´ ì„ í—˜ ì¶©ëŒ ë¬¸ì œë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.

- 4. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Decoupled Proxy Alignment (DPA)ë¼ëŠ” ìƒˆë¡œìš´ í›ˆë ¨ ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°, ì´ëŠ” ì‹œê°ì  ê´€ë ¨ì„±ì— ê¸°ë°˜í•œ ë™ì  ì†ì‹¤ ì¡°ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

- 5. DPAëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹, ëª¨ë¸ ê³„ì—´ ë° ê·œëª¨ì—ì„œ ìš°ìˆ˜í•œ ì •ë ¬ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, MLLM í›ˆë ¨ì˜ íš¨ê³¼ì„±ì„ í–¥ìƒì‹œí‚¤ê³  ë›°ì–´ë‚œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:51:16*