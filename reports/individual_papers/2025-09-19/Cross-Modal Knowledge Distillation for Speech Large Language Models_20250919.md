---
keywords:
  - Large Language Models
  - Cross-Modal Knowledge Distillation
  - Natural Language Processing
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14930
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:51:08.513545",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Cross-Modal Knowledge Distillation",
    "Natural Language Processing"
  ],
  "rejected_keywords": [
    "Speech Recognition",
    "Transfer Learning"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Cross-Modal Knowledge Distillation": 0.78,
    "Natural Language Processing": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Cross-Modal Knowledge Distillation for Speech Large Language Models

**Korean Title:** ìŒì„± ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ êµì°¨ ëª¨ë‹¬ ì§€ì‹ ì¦ë¥˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**âš¡ Unique Technical**: [[keywords/Cross-Modal Knowledge Distillation|Cross-Modal Knowledge Distillation]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Delta_Knowledge_Distillation_for_Large_Language_Models_20250919|Delta Knowledge Distillation for Large Language Models]] (84.4% similar)
- [[TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (82.5% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (82.2% similar)
- [[KBM Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models]] (82.0% similar)
- [[Omni-CLST Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (81.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14930v1 Announce Type: cross 
Abstract: In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14930v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë³¸ ì—°êµ¬ì—ì„œëŠ” ìŒì„± ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œì˜ íŒŒêµ­ì  ë§ê°ê³¼ ëª¨ë‹¬ë¦¬í‹° ë¶ˆê· í˜•ì— ëŒ€í•œ ìµœì´ˆì˜ ì²´ê³„ì ì¸ í‰ê°€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìŒì„± ê¸°ëŠ¥ì„ ë„ì…í•  ê²½ìš° ì…ë ¥ì´ ì—¬ì „íˆ í…ìŠ¤íŠ¸ì¸ ê²½ìš°ì—ë„ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë©°, ìŒì„± ì§ˆì˜ì—ì„œëŠ” ì„±ëŠ¥ì´ ë”ìš± ê°ì†Œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ êµì‚¬ ëª¨ë¸ì—ì„œ ìŒì„± LLMìœ¼ë¡œ ì§€ì‹ì„ ì „ì´í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸-í…ìŠ¤íŠ¸ ë° ìŒì„±-í…ìŠ¤íŠ¸ ì±„ë„ì„ ëª¨ë‘ í™œìš©í•˜ëŠ” êµì°¨ ëª¨ë‹¬ ì§€ì‹ ì¦ë¥˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ëŒ€í™” ë° ì˜¤ë””ì˜¤ ì´í•´ ì‘ì—…ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì´ í…ìŠ¤íŠ¸ ì§€ì‹ì„ ë³´ì¡´í•˜ê³ , êµì°¨ ëª¨ë‹¬ ì •ë ¬ì„ ê°œì„ í•˜ë©°, ìŒì„± ê¸°ë°˜ ìƒí˜¸ì‘ìš©ì—ì„œì˜ ì¶”ë¡ ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ê²€ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ìŒì„± ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ë°œìƒí•˜ëŠ” ë§ê° ë¬¸ì œì™€ ëª¨ë‹¬ë¦¬í‹° ë¶ˆí‰ë“±ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤. ìŒì„± ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ë©´ í…ìŠ¤íŠ¸ ì…ë ¥ì—ì„œë„ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë©°, ìŒì„± ì¿¼ë¦¬ì—ì„œëŠ” ì„±ëŠ¥ì´ ë”ìš± ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, í…ìŠ¤íŠ¸ ê¸°ë°˜ êµì‚¬ ëª¨ë¸ì—ì„œ ìŒì„± LLMìœ¼ë¡œ ì§€ì‹ì„ ì „ì´í•˜ëŠ” êµì°¨ ëª¨ë‹¬ ì§€ì‹ ì¦ë¥˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ëŒ€í™” ë° ì˜¤ë””ì˜¤ ì´í•´ ì‘ì—…ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ì´ ì ‘ê·¼ ë°©ì‹ì´ í…ìŠ¤íŠ¸ ì§€ì‹ì„ ë³´ì¡´í•˜ê³ , êµì°¨ ëª¨ë‹¬ ì •ë ¬ì„ ê°œì„ í•˜ë©°, ìŒì„± ê¸°ë°˜ ìƒí˜¸ì‘ìš©ì—ì„œì˜ ì¶”ë¡ ì„ í–¥ìƒì‹œí‚´ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìŒì„± ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œì˜ ë§ê°ê³¼ ëª¨ë‹¬ë¦¬í‹° ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.

- 2. ìŒì„± ê¸°ëŠ¥ ë„ì…ì´ í…ìŠ¤íŠ¸ ì…ë ¥ì—ì„œë„ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

- 3. í…ìŠ¤íŠ¸ ê¸°ë°˜ êµì‚¬ ëª¨ë¸ì—ì„œ ìŒì„± LLMìœ¼ë¡œ ì§€ì‹ì„ ì „ì´í•˜ëŠ” êµì°¨ ëª¨ë‹¬ ì§€ì‹ ì¦ë¥˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

- 4. ì œì•ˆëœ ë°©ë²•ì´ í…ìŠ¤íŠ¸ ì§€ì‹ì„ ë³´ì¡´í•˜ê³ , êµì°¨ ëª¨ë‹¬ ì •ë ¬ì„ ê°œì„ í•˜ë©°, ìŒì„± ê¸°ë°˜ ìƒí˜¸ì‘ìš©ì—ì„œì˜ ì¶”ë¡ ì„ í–¥ìƒì‹œí‚´ì„ ì‹¤í—˜ì„ í†µí•´ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:02:45*