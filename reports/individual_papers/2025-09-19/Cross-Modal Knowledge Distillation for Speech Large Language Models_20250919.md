---
keywords:
  - Large Language Models
  - Cross-Modal Knowledge Distillation
  - Natural Language Processing
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14930
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:51:08.513545",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Cross-Modal Knowledge Distillation",
    "Natural Language Processing"
  ],
  "rejected_keywords": [
    "Speech Recognition",
    "Transfer Learning"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Cross-Modal Knowledge Distillation": 0.78,
    "Natural Language Processing": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# Cross-Modal Knowledge Distillation for Speech Large Language Models

**Korean Title:** 음성 대형 언어 모델을 위한 교차 모달 지식 증류

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🌐 Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**⚡ Unique Technical**: [[keywords/Cross-Modal Knowledge Distillation|Cross-Modal Knowledge Distillation]]
**🚀 Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]]

## 🔗 유사한 논문
- [[Delta_Knowledge_Distillation_for_Large_Language_Models_20250919|Delta Knowledge Distillation for Large Language Models]] (84.4% similar)
- [[TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (82.5% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (82.2% similar)
- [[KBM Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models]] (82.0% similar)
- [[Omni-CLST Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering]] (81.8% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.14930v1 Announce Type: cross 
Abstract: In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.

## 🔍 Abstract (한글 번역)

arXiv:2509.14930v1 발표 유형: 교차  
초록: 본 연구에서는 음성 대형 언어 모델에서의 파국적 망각과 모달리티 불균형에 대한 최초의 체계적인 평가를 제시합니다. 우리는 음성 기능을 도입할 경우 입력이 여전히 텍스트인 경우에도 지식과 추론 능력이 저하될 수 있으며, 음성 질의에서는 성능이 더욱 감소할 수 있음을 보여줍니다. 이러한 문제를 해결하기 위해, 우리는 텍스트 기반 교사 모델에서 음성 LLM으로 지식을 전이하기 위해 텍스트-텍스트 및 음성-텍스트 채널을 모두 활용하는 교차 모달 지식 증류 프레임워크를 제안합니다. 대화 및 오디오 이해 작업에 대한 광범위한 실험을 통해 우리의 접근 방식이 텍스트 지식을 보존하고, 교차 모달 정렬을 개선하며, 음성 기반 상호작용에서의 추론을 향상시키는 데 효과적임을 검증합니다.

## 📝 요약

이 연구는 음성 대형 언어 모델에서 발생하는 망각 문제와 모달리티 불평등을 체계적으로 평가한 최초의 연구입니다. 음성 기능을 추가하면 텍스트 입력에서도 지식과 추론 능력이 저하될 수 있으며, 음성 쿼리에서는 성능이 더욱 감소하는 것을 보여줍니다. 이를 해결하기 위해, 텍스트 기반 교사 모델에서 음성 LLM으로 지식을 전이하는 교차 모달 지식 증류 프레임워크를 제안합니다. 대화 및 오디오 이해 작업에 대한 광범위한 실험을 통해 이 접근 방식이 텍스트 지식을 보존하고, 교차 모달 정렬을 개선하며, 음성 기반 상호작용에서의 추론을 향상시킴을 검증했습니다.

## 🎯 주요 포인트

- 1. 음성 대형 언어 모델에서의 망각과 모달리티 불균형 문제를 체계적으로 평가했습니다.

- 2. 음성 기능 도입이 텍스트 입력에서도 지식과 추론 능력을 저하시킬 수 있음을 발견했습니다.

- 3. 텍스트 기반 교사 모델에서 음성 LLM으로 지식을 전이하는 교차 모달 지식 증류 프레임워크를 제안했습니다.

- 4. 제안된 방법이 텍스트 지식을 보존하고, 교차 모달 정렬을 개선하며, 음성 기반 상호작용에서의 추론을 향상시킴을 실험을 통해 검증했습니다.

---

*Generated on 2025-09-19 15:02:45*