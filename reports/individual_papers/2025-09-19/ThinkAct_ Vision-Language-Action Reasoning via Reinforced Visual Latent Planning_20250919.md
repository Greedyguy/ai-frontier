
# ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning

**Korean Title:** ThinkAct: ê°•í™”ëœ ì‹œê° ì ì¬ ê³„íšì„ í†µí•œ ë¹„ì „-ì–¸ì–´-í–‰ë™ ì¶”ë¡ 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Multimodal Learning, Few-shot Adaptation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[CLAW A Vision-Language-Action Framework for Weight-Aware Robotic Grasping]] (85.2% similar)
- [[Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations]] (83.9% similar)
- [[GeoAware-VLA Implicit Geometry Aware Vision-Language-Action Model]] (83.5% similar)
- [[FSR-VLN Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph]] (83.0% similar)
- [[OpenHA A Series of Open-Source Hierarchical Agentic Models in Minecraft]] (82.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.16815v2 Announce Type: replace-cross 
Abstract: Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.16815v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ì‹œê°-ì–¸ì–´-í–‰ë™(VLA) ì¶”ë¡  ê³¼ì œëŠ” ì—ì´ì „íŠ¸ê°€ ë‹¤ì¤‘ ëª¨ë‹¬ ì§€ì‹œë¥¼ í•´ì„í•˜ê³ , ì¥ê¸° ê³„íšì„ ìˆ˜í–‰í•˜ë©°, ë™ì  í™˜ê²½ì—ì„œ ì ì‘ì ìœ¼ë¡œ í–‰ë™í•  ê²ƒì„ ìš”êµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì€ ì¼ë°˜ì ìœ¼ë¡œ VLA ëª¨ë¸ì„ ì…ë ¥ì„ í–‰ë™ìœ¼ë¡œ ì§ì ‘ ë§¤í•‘í•˜ëŠ” ì¢…ë‹¨ ê°„ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨í•˜ë©°, ëª…ì‹œì ì¸ ì¶”ë¡  ì—†ì´ ì‘ë™í•˜ì—¬ ì—¬ëŸ¬ ë‹¨ê³„ì— ê±¸ì¹œ ê³„íš ìˆ˜ë¦½ì´ë‚˜ ë³µì¡í•œ ê³¼ì œ ë³€í˜•ì— ì ì‘í•˜ëŠ” ëŠ¥ë ¥ì„ ì €í•´í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê°•í™”ëœ ì‹œê° ì ì¬ ê³„íšì„ í†µí•´ ê³ ì°¨ì› ì¶”ë¡ ê³¼ ì €ì°¨ì› í–‰ë™ ì‹¤í–‰ì„ ì—°ê²°í•˜ëŠ” ì´ì¤‘ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ì¸ ThinkActë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ThinkActëŠ” ëª©í‘œ ë‹¬ì„± ë° ê²½ë¡œ ì¼ê´€ì„±ì— ê¸°ë°˜í•œ í–‰ë™ ì •ë ¬ ì‹œê° ë³´ìƒì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” êµ¬í˜„ëœ ì¶”ë¡  ê³„íšì„ ìƒì„±í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ëª¨ë‹¬ LLMì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¶”ë¡  ê³„íšì€ ëª©í‘œ í™˜ê²½ì—ì„œì˜ ê°•ë ¥í•œ í–‰ë™ ì‹¤í–‰ì„ ìœ„í•œ í•˜ìœ„ í–‰ë™ ëª¨ë¸ì„ ì¡°ê±´í™”í•˜ëŠ” ì‹œê° ê³„íš ì ì¬ë¡œ ì••ì¶•ë©ë‹ˆë‹¤. êµ¬í˜„ëœ ì¶”ë¡  ë° ë¡œë´‡ ì¡°ì‘ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ThinkActê°€ ë³µì¡í•œ êµ¬í˜„ AI ê³¼ì œì—ì„œ ì ì€ ìƒ· ì ì‘, ì¥ê¸° ê³„íš ë° ìê¸° ìˆ˜ì • í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë¹„ì „-ì–¸ì–´-í–‰ë™(VLA) ì¶”ë¡  ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ThinkActë¼ëŠ” ì´ì¤‘ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì…ë ¥ì„ í–‰ë™ìœ¼ë¡œ ì§ì ‘ ë§¤í•‘í•˜ì—¬ ëª…ì‹œì ì¸ ì¶”ë¡  ì—†ì´ í•™ìŠµí•˜ì§€ë§Œ, ì´ëŠ” ë³µì¡í•œ ê³¼ì œ ë³€í˜•ì— ì ì‘í•˜ê±°ë‚˜ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê³„íší•˜ëŠ” ë° í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ThinkActëŠ” ê°•í™”ëœ ì‹œê°ì  ì ì¬ ê³„íšì„ í†µí•´ ê³ ìˆ˜ì¤€ì˜ ì¶”ë¡ ê³¼ ì €ìˆ˜ì¤€ì˜ í–‰ë™ ì‹¤í–‰ì„ ì—°ê²°í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í›ˆë ¨ì‹œì¼œ ëª©í‘œ ë‹¬ì„± ë° ê²½ë¡œ ì¼ê´€ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹œê°ì  ë³´ìƒì„ í†µí•´ ì²´í™”ëœ ì¶”ë¡  ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³„íšì€ ì‹œê°ì  ì ì¬ ê³„íšìœ¼ë¡œ ì••ì¶•ë˜ì–´ ëª©í‘œ í™˜ê²½ì—ì„œ ê°•ë ¥í•œ í–‰ë™ ì‹¤í–‰ì„ ìœ„í•œ í–‰ë™ ëª¨ë¸ì„ ì¡°ê±´í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ThinkActëŠ” ë³µì¡í•œ ì²´í™” AI ê³¼ì œì—ì„œ ì†Œìˆ˜ì˜ ìƒ˜í”Œë¡œ ì ì‘, ì¥ê¸° ê³„íš, ìê¸° ìˆ˜ì • ëŠ¥ë ¥ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision-language-action (VLA) ì¶”ë¡  ê³¼ì œëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì§€ì‹œë¥¼ í•´ì„í•˜ê³ , ì¥ê¸° ê³„íšì„ ìˆ˜í–‰í•˜ë©°, ë™ì  í™˜ê²½ì—ì„œ ì ì‘ì ìœ¼ë¡œ í–‰ë™í•´ì•¼ í•©ë‹ˆë‹¤.

- 2. ê¸°ì¡´ ì ‘ê·¼ë²•ì€ ì…ë ¥ì„ í–‰ë™ìœ¼ë¡œ ì§ì ‘ ë§¤í•‘í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ëª…ì‹œì ì¸ ì¶”ë¡  ì—†ì´ VLA ëª¨ë¸ì„ í›ˆë ¨í•˜ì—¬ ë‹¤ë‹¨ê³„ ê³„íšì´ë‚˜ ë³µì¡í•œ ê³¼ì œ ë³€í˜•ì— ì ì‘í•˜ëŠ” ëŠ¥ë ¥ì„ ì €í•´í•©ë‹ˆë‹¤.

- 3. ThinkActëŠ” ê°•í™”ëœ ì‹œê° ì ì¬ ê³„íšì„ í†µí•´ ê³ ìˆ˜ì¤€ ì¶”ë¡ ê³¼ ì €ìˆ˜ì¤€ í–‰ë™ ì‹¤í–‰ì„ ì—°ê²°í•˜ëŠ” ì´ì¤‘ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 4. ThinkActëŠ” ëª©í‘œ ë‹¬ì„±ê³¼ ê²½ë¡œ ì¼ê´€ì„±ì— ê¸°ë°˜í•œ ê°•í™”ëœ í–‰ë™ ì •ë ¬ ì‹œê° ë³´ìƒì— ì˜í•´ ìœ ë„ëœ êµ¬í˜„ëœ ì¶”ë¡  ê³„íšì„ ìƒì„±í•˜ë„ë¡ ë‹¤ì¤‘ ëª¨ë‹¬ LLMì„ í›ˆë ¨í•©ë‹ˆë‹¤.

- 5. ThinkActëŠ” ë³µì¡í•œ êµ¬í˜„ AI ê³¼ì œì—ì„œ ëª‡ ìƒ· ì ì‘, ì¥ê¸° ê³„íš ë° ìê¸° ìˆ˜ì • í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:19:25*