
# Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning

**Korean Title:** ì—°ì† ì‹œê°„ ê°€ì¹˜ ë°˜ë³µì„ í†µí•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Continuous-Time Multi-Agent Reinforcement Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Multi-Fidelity_Hybrid_Reinforcement_Learning_via_Information_Gain_Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (83.0% similar)
- [[Scalable_Multi-Objective_Robot_Reinforcement_Learning_through_Gradient_Conflict_Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (82.5% similar)
- [[Process-Supervised_Reinforcement_Learning_for_Interactive_Multimodal_Tool-Use_Agents_20250919|Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents]] (82.3% similar)
- [[Vulnerable_Agent_Identification_in_Large-Scale_Multi-Agent_Reinforcement_Learning_20250919|Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning]] (82.0% similar)
- [[Resolve_Highway_Conflict_in_Multi-Autonomous_Vehicle_Controls_with_Local_State_Attention_20250919|Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention]] (81.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.09135v2 Announce Type: replace 
Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.09135v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ(RL) ë°©ë²•ì€ ë†’ì€ ë¹ˆë„ ë˜ëŠ” ë¶ˆê·œì¹™í•œ ì‹œê°„ ê°„ê²©ì—ì„œì˜ ìƒí˜¸ì‘ìš©ì„ ìš”êµ¬í•˜ëŠ” ë³µì¡í•œ ë™ì  ì‹œìŠ¤í…œì—ì„œ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì—°ì† ì‹œê°„ ê°•í™” í•™ìŠµ(CTRL)ì€ ì´ì‚° ì‹œê°„ ë²¨ë§Œ ì¬ê·€ë¥¼ í•´ë°€í„´-ìì½”ë¹„-ë²¨ë§Œ(HJB) ë°©ì •ì‹ì˜ ì ì„± í•´ë¡œ ì •ì˜ëœ ë¯¸ë¶„ ê°€ì¹˜ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ì—¬ ìœ ë§í•œ ëŒ€ì•ˆìœ¼ë¡œ ë¶€ìƒí–ˆìŠµë‹ˆë‹¤. CTRLì€ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ê·¸ ì‘ìš©ì€ ì£¼ë¡œ ë‹¨ì¼ ì—ì´ì „íŠ¸ ë¶„ì•¼ì— êµ­í•œë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œì€ ë‘ ê°€ì§€ ì£¼ìš” ë„ì „ ê³¼ì œì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤: (i) HJB ë°©ì •ì‹ì˜ ê¸°ì¡´ í•´ê²° ë°©ë²•ì€ ì°¨ì›ì˜ ì €ì£¼(CoD)ë¡œ ì¸í•´ ê³ ì°¨ì› ì‹œìŠ¤í…œì—ì„œ ë‹¤ë£¨ê¸° ì–´ë µê³ ; (ii) HJB ê¸°ë°˜ í•™ìŠµ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ë”ë¼ë„, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì„¤ì •ì—ì„œ ì¤‘ì•™ ì§‘ì¤‘ì‹ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ê·¼ì‚¬í™”í•˜ëŠ” ê²ƒì´ ì—¬ì „íˆ ì–´ë ¤ì›Œ ì •ì±… í•™ìŠµì„ ë¶ˆì•ˆì •í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¼ë¦¬ ì •ë³´ ì‹ ê²½ë§(PINNs)ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ë¡œ HJB ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í™”í•˜ëŠ” CT-MARL í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê°€ì¹˜ê°€ ê·¸ ë¯¸ë¶„ êµ¬ì¡°ì™€ ì¼ì¹˜í•˜ë„ë¡ í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê°€ì¹˜ í•™ìŠµì„ ê°€ì¹˜-ê¸°ìš¸ê¸° í•™ìŠµê³¼ ì •ë ¬í•˜ì—¬ ê²½ë¡œë¥¼ ë”°ë¼ ê°€ì¹˜ ê¸°ìš¸ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì •ì œí•˜ëŠ” ê°€ì¹˜ ê¸°ìš¸ê¸° ë°˜ë³µ(VGI) ëª¨ë“ˆì„ ë„ì…í•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ìš¸ê¸° ì¶©ì‹¤ë„ë¥¼ í–¥ìƒì‹œì¼œ, ë³´ë‹¤ ì •í™•í•œ ê°€ì¹˜ì™€ ê°•ë ¥í•œ ì •ì±… í•™ìŠµì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì…ì í™˜ê²½(MPE) ë° ë‹¤ì¤‘ ì—ì´ì „íŠ¸ MuJoCoë¥¼ í¬í•¨í•œ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì˜ ì—°ì† ì‹œê°„ ë³€í˜•ì„ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ì˜ ë°©ë²•ì„ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì´ ê¸°ì¡´ì˜ ì—°ì† ì‹œê°„ RL ê¸°ì¤€ì„ ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©° ë³µì¡í•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë™ì  ì‹œìŠ¤í…œì— í™•ì¥ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë³µì¡í•œ ë™ì  ì‹œìŠ¤í…œì—ì„œ ê¸°ì¡´ ê°•í™” í•™ìŠµ(RL) ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì—°ì† ì‹œê°„ ê°•í™” í•™ìŠµ(CTRL)ì„ ì œì•ˆí•©ë‹ˆë‹¤. CTRLì€ ë¶ˆê·œì¹™í•œ ì‹œê°„ ê°„ê²©ì˜ ìƒí˜¸ì‘ìš©ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•´ë°€í„´-ìì½”ë¹„-ë²¨ë§Œ(HJB) ë°©ì •ì‹ì˜ ì ì„± í•´ë¡œ ì •ì˜ëœ ë¯¸ë¶„ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ CTRLì˜ ì‘ìš©ì€ ì£¼ë¡œ ë‹¨ì¼ ì—ì´ì „íŠ¸ì— êµ­í•œë˜ì—ˆìœ¼ë©°, ì´ëŠ” ê³ ì°¨ì› ì‹œìŠ¤í…œì—ì„œ ì°¨ì›ì˜ ì €ì£¼(CoD)ì™€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œì˜ ì¤‘ì•™ ì§‘ì¤‘ì‹ ê°€ì¹˜ í•¨ìˆ˜ ê·¼ì‚¬í™”ì˜ ì–´ë ¤ì›€ ë•Œë¬¸ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¼ë¦¬ ê¸°ë°˜ ì‹ ê²½ë§(PINNs)ì„ í™œìš©í•˜ì—¬ HJB ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ëŒ€ê·œëª¨ë¡œ ê·¼ì‚¬í™”í•˜ëŠ” CT-MARL í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê°€ì¹˜ í•™ìŠµê³¼ ê°€ì¹˜-ê¸°ìš¸ê¸° í•™ìŠµì„ ì •ë ¬í•˜ì—¬ ê°€ì¹˜ ê¸°ìš¸ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” Value Gradient Iteration(VGI) ëª¨ë“ˆì„ ë„ì…í•˜ì—¬ ë” ì •í™•í•œ ê°€ì¹˜ì™€ ê°•ë ¥í•œ ì •ì±… í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ ì—°ì† ì‹œê°„ RL ê¸°ë²•ì„ ëŠ¥ê°€í•˜ë©° ë³µì¡í•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë™ì  ì‹œìŠ¤í…œì— í™•ì¥ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ ê°•í™” í•™ìŠµ ë°©ë²•ì€ ë†’ì€ ë¹ˆë„ë‚˜ ë¶ˆê·œì¹™í•œ ì‹œê°„ ê°„ê²©ì˜ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•œ ë³µì¡í•œ ë™ì  ì‹œìŠ¤í…œì—ì„œ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

- 2. ì—°ì† ì‹œê°„ ê°•í™” í•™ìŠµ(CTRL)ì€ í•´ë°€í„´-ìì½”ë¹„-ë²¨ë§Œ(HJB) ë°©ì •ì‹ì˜ ì ì„± í•´ë¡œ ì •ì˜ëœ ë¯¸ë¶„ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ë§í•œ ëŒ€ì•ˆìœ¼ë¡œ ë¶€ìƒí–ˆìŠµë‹ˆë‹¤.

- 3. CTRLì˜ ì‘ìš©ì€ ë‹¨ì¼ ì—ì´ì „íŠ¸ ë„ë©”ì¸ì— ì£¼ë¡œ ì œí•œë˜ì—ˆìœ¼ë©°, ì´ëŠ” ì°¨ì›ì˜ ì €ì£¼(CoD)ì™€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œì˜ ì¤‘ì•™ ì§‘ì¤‘ì‹ ê°€ì¹˜ í•¨ìˆ˜ ê·¼ì‚¬í™”ì˜ ì–´ë ¤ì›€ ë•Œë¬¸ì…ë‹ˆë‹¤.

- 4. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¼ë¦¬ ì •ë³´ ì‹ ê²½ë§(PINNs)ì„ ì‚¬ìš©í•˜ì—¬ HJB ê¸°ë°˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ëŒ€ê·œëª¨ë¡œ ê·¼ì‚¬í™”í•˜ëŠ” CT-MARL í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ì—°ì† ì‹œê°„ ë³€í˜• í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ì—°ì† ì‹œê°„ ê°•í™” í•™ìŠµ ê¸°ì¤€ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ê³  ë³µì¡í•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì—­í•™ì— í™•ì¥ë©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 15:40:57*