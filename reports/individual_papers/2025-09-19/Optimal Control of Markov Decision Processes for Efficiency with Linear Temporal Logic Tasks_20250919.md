
# Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks

**Korean Title:** 마르코프 결정 프로세스의 최적 제어: 선형 시간 논리 과제를 통한 효율성 향상

## 📋 메타데이터

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Efficiency with LTL Tasks

## 🔗 유사한 논문
- [[Meta-Optimization and Program Search using Language Models for Task and Motion Planning_20250918|Meta-Optimization and Program Search using Language Models for Task and Motion Planning]] (82.1% similar)
- [[Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain_20250918|Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain]] (82.0% similar)
- [[One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields_20250918|One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields]] (80.3% similar)
- [[Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (80.2% similar)
- [[Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (80.2% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2403.18632v2 Announce Type: replace 
Abstract: We investigate the problem of optimal control synthesis for Markov Decision Processes (MDPs), addressing both qualitative and quantitative objectives. Specifically, we require the system to satisfy a qualitative task specified by a Linear Temporal Logic (LTL) formula with probability one. Additionally, to quantify the system's performance, we introduce the concept of efficiency, defined as the ratio between rewards and costs. This measure is more general than the standard long-run average reward metric, as it seeks to maximize the reward obtained per unit cost. Our objective is to synthesize a control policy that not only ensures the LTL task is satisfied but also maximizes efficiency. We present an effective approach for synthesizing a stationary control policy that achieves $\epsilon$-optimality by integrating state classifications of MDPs with perturbation analysis in a novel manner. Our results extend existing work on efficiency-optimal control synthesis for MDPs by incorporating qualitative LTL tasks. Case studies in robot task planning are provided to illustrate the proposed algorithm.

## 🔍 Abstract (한글 번역)

arXiv:2403.18632v2 발표 유형: 교체  
초록: 본 연구에서는 마르코프 결정 프로세스(MDPs)에 대한 최적 제어 합성 문제를 조사하며, 질적 및 양적 목표를 모두 다룹니다. 구체적으로, 시스템이 확률 1로 선형 시간 논리(LTL) 공식으로 지정된 질적 작업을 충족하도록 요구합니다. 또한, 시스템의 성능을 정량화하기 위해 보상과 비용의 비율로 정의된 효율성 개념을 도입합니다. 이 측정은 표준 장기 평균 보상 지표보다 일반적이며, 단위 비용당 얻는 보상을 최대화하려고 합니다. 우리의 목표는 LTL 작업을 충족시키는 동시에 효율성을 극대화하는 제어 정책을 합성하는 것입니다. 우리는 MDP의 상태 분류와 교란 분석을 새로운 방식으로 통합하여 $\epsilon$-최적성을 달성하는 정적 제어 정책을 합성하기 위한 효과적인 접근법을 제시합니다. 우리의 결과는 질적 LTL 작업을 통합하여 MDP에 대한 효율성 최적 제어 합성에 관한 기존 연구를 확장합니다. 제안된 알고리즘을 설명하기 위해 로봇 작업 계획의 사례 연구를 제공합니다.

## 📝 요약

이 논문은 마르코프 결정 프로세스(MDP)를 위한 최적 제어 합성을 연구하며, 질적 및 양적 목표를 모두 다룹니다. 시스템이 선형 시간 논리(LTL) 공식으로 명시된 질적 작업을 확률 1로 만족하도록 요구하며, 성능을 측정하기 위해 보상과 비용의 비율로 정의된 효율성을 도입합니다. 이는 단위 비용당 얻는 보상을 최대화하는 것을 목표로 하며, 기존의 장기 평균 보상 메트릭보다 일반적입니다. 본 연구는 LTL 작업을 만족시키면서 효율성을 극대화하는 제어 정책을 합성하는 효과적인 방법을 제시합니다. MDP의 상태 분류와 섭동 분석을 통합하여 $\epsilon$-최적성을 달성하는 정적 제어 정책을 합성하는 접근법을 제안합니다. 이 결과는 MDP의 효율성 최적 제어 합성에 질적 LTL 작업을 포함하여 기존 연구를 확장합니다. 로봇 작업 계획의 사례 연구를 통해 제안된 알고리즘을 설명합니다.

## 🎯 주요 포인트

- 1. 본 연구는 마르코프 결정 프로세스(MDPs)의 최적 제어 합성을 탐구하며, 질적 및 양적 목표를 모두 다룹니다.

- 2. 시스템은 확률 1로 선형 시간 논리(LTL) 공식에 의해 지정된 질적 작업을 만족해야 합니다.

- 3. 시스템 성능을 정량화하기 위해 보상과 비용의 비율로 정의된 효율성 개념을 도입합니다.

- 4. 제안된 접근법은 MDP의 상태 분류와 섭동 분석을 통합하여 $\epsilon$-최적성을 달성하는 정적 제어 정책을 합성합니다.

- 5. 로봇 작업 계획의 사례 연구를 통해 제안된 알고리즘을 설명합니다.

---

*Generated on 2025-09-19 16:45:57*