---
keywords:
  - Large Language Models
  - Natural Language Processing
  - Adaptive Mechanisms
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2509.14886
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:00:06.618502",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Natural Language Processing",
    "Adaptive Mechanisms"
  ],
  "rejected_keywords": [
    "Adaptive Interview Paradigm"
  ],
  "similarity_scores": {
    "Large Language Models": 0.88,
    "Natural Language Processing": 0.8,
    "Adaptive Mechanisms": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation

**Korean Title:** ν¨μ¨μ μΈ MLLM ν‰κ°€λ¥Ό μ„ν• λ‹¤λ€μΌ μΈν„°λ·° ν¨λ¬λ‹¤μ„

## π“‹ λ©”νƒ€λ°μ΄ν„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## π·οΈ μΉ΄ν…κ³ λ¦¬ν™”λ ν‚¤μ›λ“
**π Broad Technical**: [[keywords/Natural Language Processing|Question-Answering evaluations]]
**π€ Evolved Concepts**: [[keywords/Adaptive Mechanisms|dynamic adjustment of interviewer weights]]

## π”— μ μ‚¬ν• λ…Όλ¬Έ
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (83.9% similar)
- [[LLM-I LLMs are Naturally Interleaved Multimodal Creators]] (82.9% similar)
- [[(P)rior(D)yna(F)low A Priori Dynamic Workflow Construction via Multi-Agent Collaboration]] (82.9% similar)
- [[From Capabilities to Performance Evaluating Key Functional Properties of LLM Architectures in Penetration Testing]] (82.3% similar)
- [[Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (82.1% similar)

## π“‹ μ €μ μ •λ³΄

**Authors:** 

## π“„ Abstract (μ›λ¬Έ)

arXiv:2509.14886v1 Announce Type: cross 
Abstract: The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.

## π” Abstract (ν•κΈ€ λ²μ—­)

arXiv:2509.14886v1 λ°ν‘ μ ν•: κµμ°¨  
μ΄λ΅: λ‹¤μ¤‘ λ¨λ“ λ€ν• μ–Έμ–΄ λ¨λΈ(Multi-Modal Large Language Models, MLLMs)μ κΈ‰μ†ν• λ°μ „μ€ μλ§μ€ λ²¤μΉλ§ν¬μ μƒμ„±μ„ μ΄‰μ§„ν–μµλ‹λ‹¤. κ·Έλ¬λ‚ κΈ°μ΅΄μ μ „λ©΄μ  μ§λ¬Έ-μ‘λ‹µ ν‰κ°€ λ°©μ‹μ€ λ†’μ€ μ¤‘λ³µμ„±κ³Ό λ‚®μ€ ν¨μ¨μ„±μ„ κ²κ³  μμµλ‹λ‹¤. μΈκ°„μ μΈν„°λ·° κ³Όμ •μ„ λ³Έλ– , μ°λ¦¬λ” ν¨μ¨μ μΈ MLLM ν‰κ°€λ¥Ό μ„ν• λ‹¤λ€μΌ μΈν„°λ·° ν¨λ¬λ‹¤μ„μ„ μ μ•ν•©λ‹λ‹¤. μ°λ¦¬μ ν”„λ μ„μ›ν¬λ” (i) μ‚¬μ „ μΈν„°λ·°μ™€ κ³µμ‹ μΈν„°λ·° λ‹¨κ³„λ΅ κµ¬μ„±λ λ‘ λ‹¨κ³„ μΈν„°λ·° μ „λµ, (ii) κ³µμ •μ„±μ„ λ³΄μ¥ν•κΈ° μ„ν• μΈν„°λ·°μ–΄ κ°€μ¤‘μΉμ λ™μ  μ΅°μ •, (iii) μ„ νƒλ μ§λ¬Έ λ‚μ΄λ„ μμ¤€μ— λ€ν• μ μ‘μ  λ©”μ»¤λ‹μ¦μΌλ΅ κµ¬μ„±λ©λ‹λ‹¤. λ‹¤μ–‘ν• λ²¤μΉλ§ν¬μ— λ€ν• μ‹¤ν— κ²°κ³Ό, μ μ•λ ν¨λ¬λ‹¤μ„μ€ λ¬΄μ‘μ„ μƒν”λ§λ³΄λ‹¤ μ „λ©΄μ  κ²°κ³Όμ™€μ μƒκ΄€κ΄€κ³„κ°€ μµλ€ 17.6%μ PLCCμ™€ 16.7%μ SRCC ν–¥μƒμ„ λ³΄μ΄λ©°, ν•„μ”ν• μ§λ¬Έ μλ¥Ό μ¤„μ΄λ” λ° μ„±κ³µν–μµλ‹λ‹¤. μ΄λ¬ν• κ²°κ³Όλ” μ μ•λ ν¨λ¬λ‹¤μ„μ΄ λ€κ·λ¨ MLLM λ²¤μΉλ§ν‚Ήμ— μ‹ λΆ°ν•  μ μκ³  ν¨μ¨μ μΈ λ€μ•μ„ μ κ³µν•¨μ„ μ…μ¦ν•©λ‹λ‹¤.

## π“ μ”μ•½

μ΄ λ…Όλ¬Έμ€ λ‹¤μ¤‘ λ¨λ‹¬ λ€ν• μ–Έμ–΄ λ¨λΈ(MLLM)μ ν‰κ°€ ν¨μ¨μ„±μ„ λ†’μ΄κΈ° μ„ν•΄ μƒλ΅μ΄ μΈν„°λ·° ν¨λ¬λ‹¤μ„μ„ μ μ•ν•©λ‹λ‹¤. κΈ°μ΅΄μ μ „λ©΄μ  μ§λ¬Έ-μ‘λ‹µ λ°©μ‹μ€ μ¤‘λ³µμ„±κ³Ό λΉ„ν¨μ¨μ„±μ΄ λ¬Έμ μ€λ”λ°, μ΄λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄ μΈκ°„μ μΈν„°λ·° κ³Όμ •μ„ λ¨λ°©ν• λ‹¤μ¤‘ λ€ μΌ μΈν„°λ·° λ°©μ‹μ„ λ„μ…ν–μµλ‹λ‹¤. μ μ•λ ν”„λ μ„μ›ν¬λ” μ‚¬μ „ μΈν„°λ·°μ™€ κ³µμ‹ μΈν„°λ·°λ΅ κµ¬μ„±λ λ‘ λ‹¨κ³„ μ „λµ, κ³µμ •μ„±μ„ λ³΄μ¥ν•λ” μΈν„°λ·°μ–΄ κ°€μ¤‘μΉμ λ™μ  μ΅°μ •, μ§λ¬Έ λ‚μ΄λ„ μ„ νƒμ„ μ„ν• μ μ‘μ  λ©”μ»¤λ‹μ¦μ„ ν¬ν•¨ν•©λ‹λ‹¤. λ‹¤μ–‘ν• λ²¤μΉλ§ν¬ μ‹¤ν— κ²°κ³Ό, μ΄ ν¨λ¬λ‹¤μ„μ€ κΈ°μ΅΄μ λ¬΄μ‘μ„ μƒν”λ§λ³΄λ‹¤ μµλ€ 17.6% λ†’μ€ PLCCμ™€ 16.7% λ†’μ€ SRCC μƒκ΄€μ„±μ„ λ³΄μ΄λ©°, ν•„μ”ν• μ§λ¬Έ μλ¥Ό μ¤„μ΄λ©΄μ„λ„ μ‹ λΆ°μ„±κ³Ό ν¨μ¨μ„±μ„ μ…μ¦ν–μµλ‹λ‹¤.

## π― μ£Όμ” ν¬μΈνΈ

- 1. λ‹¤μ¤‘ λ¨λ‹¬ λ€ν• μ–Έμ–΄ λ¨λΈ(MLLM)μ ν¨μ¨μ μΈ ν‰κ°€λ¥Ό μ„ν•΄ λ‹¤λ€μΌ μΈν„°λ·° ν¨λ¬λ‹¤μ„μ„ μ μ•ν•©λ‹λ‹¤.

- 2. μ μ•λ ν”„λ μ„μ›ν¬λ” μ‚¬μ „ μΈν„°λ·°μ™€ κ³µμ‹ μΈν„°λ·° λ‹¨κ³„λ΅ κµ¬μ„±λ λ‘ λ‹¨κ³„ μΈν„°λ·° μ „λµμ„ ν¬ν•¨ν•©λ‹λ‹¤.

- 3. μΈν„°λ·°μ–΄ κ°€μ¤‘μΉμ λ™μ  μ΅°μ •μ„ ν†µν•΄ κ³µμ •μ„±μ„ λ³΄μ¥ν•©λ‹λ‹¤.

- 4. μ§λ¬Έ λ‚μ΄λ„λ¥Ό μ„ νƒν•λ” μ μ‘ λ©”μ»¤λ‹μ¦μ„ λ„μ…ν•μ—¬ ν‰κ°€ ν¨μ¨μ„±μ„ λ†’μ€μµλ‹λ‹¤.

- 5. μ μ•λ ν¨λ¬λ‹¤μ„μ€ κΈ°μ΅΄μ λ¬΄μ‘μ„ μƒν”λ§λ³΄λ‹¤ μµλ€ 17.6%μ PLCCμ™€ 16.7%μ SRCC κ°μ„ μ„ ν†µν•΄ λ†’μ€ μƒκ΄€μ„±μ„ λ‹¬μ„±ν•©λ‹λ‹¤.

---

*Generated on 2025-09-19 15:02:21*