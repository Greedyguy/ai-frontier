---
keywords:
  - MolmoAct
  - Action Reasoning Models
  - Foundation Models
category: cs.AI
publish_date: 2025-09-19
arxiv_id: 2508.07917
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 21:33:46.114586",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "MolmoAct",
    "Action Reasoning Models",
    "Foundation Models"
  ],
  "rejected_keywords": [
    "Trajectory Steering"
  ],
  "similarity_scores": {
    "MolmoAct": 0.85,
    "Action Reasoning Models": 0.8,
    "Foundation Models": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# MolmoAct: Action Reasoning Models that can Reason in Space

**Korean Title:** MolmoAct: ê³µê°„ì—ì„œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” í–‰ë™ ì¶”ë¡  ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250919|2025-09-19]]   [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/MolmoAct|MolmoAct]], [[keywords/Action Reasoning Models|Action Reasoning Models]]
**ğŸš€ Evolved Concepts**: [[keywords/Foundation Models|Robotic Foundation Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[ThinkAct Vision-Language-Action Reasoning via Reinforced Visual Latent Planning]] (83.7% similar)
- [[Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation_20250918|Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation]] (82.2% similar)
- [[PhysicalAgent Towards General Cognitive Robotics with Foundation World Models]] (81.5% similar)
- [[CRAFT Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks]] (81.0% similar)
- [[ForceVLA Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (80.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.07917v4 Announce Type: replace 
Abstract: Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of robotic foundation models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1.5; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2508.07917v4 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ì¶”ë¡ ì€ ëª©ì  ìˆëŠ” í–‰ë™ì˜ ì¤‘ì‹¬ì— ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ë¡œë´‡ ê¸°ë°˜ ëª¨ë¸ì€ ì¸ì‹ê³¼ ì§€ì‹œë¥¼ ì§ì ‘ ì œì–´ë¡œ ë§¤í•‘í•˜ì—¬ ì ì‘ì„±, ì¼ë°˜í™”, ì˜ë¯¸ì  ê¸°ë°˜ì„ ì œí•œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¸ì‹, ê³„íš, ì œì–´ë¥¼ êµ¬ì¡°í™”ëœ 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ í†µí•©í•˜ëŠ” ë¡œë´‡ ê¸°ë°˜ ëª¨ë¸ì˜ í•œ ì¢…ë¥˜ì¸ í–‰ë™ ì¶”ë¡  ëª¨ë¸(ARMs)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª¨ë¸ MolmoActëŠ” ê´€ì°°ê³¼ ì§€ì‹œë¥¼ ê¹Šì´ ì¸ì‹ ê°€ëŠ¥í•œ ì¸ì‹ í† í°ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³ , í¸ì§‘ ê°€ëŠ¥í•œ ê¶¤ì  ì¶”ì ì„ í†µí•´ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê³µê°„ ê³„íšì„ ìƒì„±í•˜ë©°, ì •ë°€í•œ ì €ìˆ˜ì¤€ í–‰ë™ì„ ì˜ˆì¸¡í•˜ì—¬ ì„¤ëª… ê°€ëŠ¥í•˜ê³  ì¡°ì • ê°€ëŠ¥í•œ í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. MolmoAct-7B-DëŠ” ì‹œë®¬ë ˆì´ì…˜ ë° ì‹¤ì œ í™˜ê²½ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤: SimplerEnv ì‹œê°ì  ë§¤ì¹­ ì‘ì—…ì—ì„œ 70.5%ì˜ ì œë¡œìƒ· ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©°, ë¹„ê³µê°œ Pi-0 ë° GR00T N1.5ë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤; LIBEROì—ì„œ í‰ê·  86.6%ì˜ ì„±ê³µë¥ ì„ ê¸°ë¡í•˜ë©°, ì¥ê¸° ê³¼ì œì—ì„œ ThinkActë³´ë‹¤ ì¶”ê°€ë¡œ 6.3%ì˜ í–¥ìƒì„ ë³´ì…ë‹ˆë‹¤; ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ë¯¸ì„¸ ì¡°ì •ì—ì„œëŠ” Pi-0-FASTë³´ë‹¤ ë‹¨ì¼ íŒ” ì‘ì—…ì—ì„œ ì¶”ê°€ë¡œ 10%, ì–‘ì† ì‘ì—…ì—ì„œ ì¶”ê°€ë¡œ 22.7%ì˜ ì‘ì—… ì§„í–‰ì„ ê¸°ë¡í•©ë‹ˆë‹¤. ë˜í•œ, ë¶„í¬ ì™¸ ì¼ë°˜í™”ì—ì„œ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ì¶”ê°€ë¡œ 23.3%ë¥¼ ëŠ¥ê°€í•˜ë©°, ê°œë°©í˜• ì§€ì‹œ ë”°ë¥´ê¸° ë° ê¶¤ì  ì¡°ì •ì—ì„œ ìµœê³  ì¸ê°„ ì„ í˜¸ ì ìˆ˜ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ë” ë‚˜ì•„ê°€, ìš°ë¦¬ëŠ” ì²˜ìŒìœ¼ë¡œ MolmoAct ë°ì´í„°ì…‹ì„ ê³µê°œí•©ë‹ˆë‹¤ -- ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ì‘ì—…ì— ê±¸ì³ 10,000ê°œ ì´ìƒì˜ ê³ í’ˆì§ˆ ë¡œë´‡ ê¶¤ì ì„ í¬í•¨í•˜ëŠ” ì¤‘ê°„ í›ˆë ¨ ë¡œë´‡ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ í‰ê·  5.5%ì˜ ì¼ë°˜ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëª¨ë“  ëª¨ë¸ ê°€ì¤‘ì¹˜, í›ˆë ¨ ì½”ë“œ, ìˆ˜ì§‘ëœ ë°ì´í„°ì…‹, ê·¸ë¦¬ê³  í–‰ë™ ì¶”ë¡  ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì—¬ MolmoActë¥¼ ìµœì²¨ë‹¨ ë¡œë´‡ ê¸°ë°˜ ëª¨ë¸ì´ì ì¸ì‹ì„ êµ¬ì¡°í™”ëœ ì¶”ë¡ ì„ í†µí•´ ëª©ì  ìˆëŠ” í–‰ë™ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ARMs êµ¬ì¶•ì„ ìœ„í•œ ê°œë°©í˜• ì²­ì‚¬ì§„ìœ¼ë¡œ í™•ë¦½í•©ë‹ˆë‹¤. ë¸”ë¡œê·¸ ê²Œì‹œë¬¼: https://allenai.org/blog/molmoact

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¡œë´‡ì˜ ì¸ì§€, ê³„íš, ì œì–´ë¥¼ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì¸ Action Reasoning Models (ARMs)ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì œì•ˆëœ ëª¨ë¸ MolmoActëŠ” ê´€ì°°ê³¼ ì§€ì‹œë¥¼ ì‹¬ì¸µ ì¸ì‹ í† í°ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³ , ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê³µê°„ ê³„íšì„ ìƒì„±í•˜ë©°, ì •í™•í•œ ì €ìˆ˜ì¤€ í–‰ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. MolmoAct-7B-DëŠ” ì‹œë®¬ë ˆì´ì…˜ ë° ì‹¤ì œ í™˜ê²½ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. ë˜í•œ, MolmoAct Datasetì„ ê³µê°œí•˜ì—¬ ë¡œë´‡ í•™ìŠµì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë¡œë´‡ì˜ ì¸ì‹ì—ì„œ ëª©ì  ìˆëŠ” í–‰ë™ìœ¼ë¡œì˜ ì „í™˜ì„ ìœ„í•œ êµ¬ì¡°ì  ì¶”ë¡ ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Action Reasoning Models (ARMs)ëŠ” ì¸ì‹, ê³„íš, ì œì–´ë¥¼ í†µí•©í•˜ì—¬ ë¡œë´‡ì˜ ì ì‘ì„±ê³¼ ì¼ë°˜í™”ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 2. MolmoAct ëª¨ë¸ì€ ê¹Šì´ ì¸ì‹ì´ ê°€ëŠ¥í•œ ì¸ì‹ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê³µê°„ ê³„íšì„ ìƒì„±í•˜ê³ , ì •í™•í•œ ì €ìˆ˜ì¤€ í–‰ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

- 3. MolmoAct-7B-DëŠ” SimplerEnv Visual Matching ì‘ì—…ì—ì„œ 70.5%ì˜ ì œë¡œìƒ· ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©°, LIBEROì—ì„œ í‰ê·  86.6%ì˜ ì„±ê³µë¥ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 4. MolmoActëŠ” Pi-0-FASTë³´ë‹¤ ì‹¤ì œ í™˜ê²½ì—ì„œ ë‹¨ì¼ ë° ì–‘ì† ì‘ì—… ì§„í–‰ë¥ ì„ ê°ê° 10% ë° 22.7% í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 5. MolmoAct ë°ì´í„°ì…‹ì€ 10,000ê°œ ì´ìƒì˜ ë¡œë´‡ ê²½ë¡œë¥¼ í¬í•¨í•˜ë©°, ì´ë¥¼ í†µí•´ ì¼ë°˜ ì„±ëŠ¥ì´ í‰ê·  5.5% í–¥ìƒë©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 16:38:45*