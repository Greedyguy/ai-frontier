
# Large Multi-modal Models Can Interpret Features in Large Multi-modal Models

**Korean Title:** ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì€ ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ íŠ¹ì§•ì„ í•´ì„í•  ìˆ˜ ìˆë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Open Semantic Features

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (86.9% similar)
- [[Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon]] (84.4% similar)
- [[From Automation to Autonomy A Survey on Large Language Models in Scientific Discovery]] (84.3% similar)
- [[xGen-MM (BLIP-3) A Family of Open Large Multimodal Models]] (84.1% similar)
- [[Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (83.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2411.14982v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2411.14982v2 ë°œí‘œ ìœ í˜•: êµì²´-êµì°¨  
ì´ˆë¡: ìµœê·¼ ëŒ€í˜• ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸(LMMs)ì˜ ë°œì „ì€ í•™ê³„ì™€ ì‚°ì—…ê³„ ëª¨ë‘ì—ì„œ ì¤‘ìš”í•œ ëŒíŒŒêµ¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì œê¸°ë˜ëŠ” ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ëŠ” ì¸ê°„ìœ¼ë¡œì„œ ìš°ë¦¬ê°€ ì–´ë–»ê²Œ ì´ëŸ¬í•œ ëª¨ë¸ì˜ ë‚´ë¶€ ì‹ ê²½ í‘œí˜„ì„ ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ LMMs ë‚´ì˜ ì˜ë¯¸ë¥¼ ì‹ë³„í•˜ê³  í•´ì„í•˜ê¸° ìœ„í•œ ë‹¤ìš©ë„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•¨ìœ¼ë¡œì¨ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì´ˆê¸° ë‹¨ê³„ë¥¼ ë°ŸìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, 1) ë¨¼ì € í¬ì†Œ ì˜¤í† ì¸ì½”ë”(SAE)ë¥¼ ì ìš©í•˜ì—¬ í‘œí˜„ì„ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” íŠ¹ì§•ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. 2) ê·¸ëŸ° ë‹¤ìŒ LMMs ìì²´ì— ì˜í•´ SAEì—ì„œ í•™ìŠµëœ ì—´ë¦° ì˜ë¯¸ë¡ ì  íŠ¹ì§•ì„ í•´ì„í•˜ê¸° ìœ„í•œ ìë™ í•´ì„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LLaVA-OV-72B ëª¨ë¸ì„ í†µí•´ LLaVA-NeXT-8B ëª¨ë¸ì„ ë¶„ì„í•˜ê³ , ì´ëŸ¬í•œ íŠ¹ì§•ì´ ëª¨ë¸ì˜ í–‰ë™ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” LMMsê°€ íŠ¹ì • ì‘ì—…, íŠ¹íˆ EQ í…ŒìŠ¤íŠ¸ì—ì„œ ë›°ì–´ë‚œ ì´ìœ ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë¥¼ ì œê³µí•˜ë©°, ê·¸ë“¤ì˜ ì‹¤ìˆ˜ì˜ ë³¸ì§ˆê³¼ ìˆ˜ì • ê°€ëŠ¥í•œ ì „ëµì„ ì¡°ëª…í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ LMMsì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ë ¥ì„ ì œê³µí•˜ë©° ì¸ê°„ ë‘ë‡Œì˜ ì¸ì§€ ê³¼ì •ê³¼ì˜ ìœ ì‚¬ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(LMMs)ì˜ ë°œì „ì€ í•™ê³„ì™€ ì‚°ì—…ê³„ì— í° ëŒíŒŒêµ¬ë¥¼ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ LMMsì˜ ë‚´ë¶€ ì‹ ê²½ í‘œí˜„ì„ ì´í•´í•˜ê¸° ìœ„í•œ ì´ˆê¸° ë‹¨ê³„ë¡œ, LMMs ë‚´ ì˜ë¯¸ë¥¼ ì‹ë³„í•˜ê³  í•´ì„í•  ìˆ˜ ìˆëŠ” ë‹¤ì¬ë‹¤ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, 1) Sparse Autoencoder(SAE)ë¥¼ ì ìš©í•˜ì—¬ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” íŠ¹ì§•ìœ¼ë¡œ í‘œí˜„ì„ ë¶„ë¦¬í•˜ê³ , 2) LMMs ìì²´ê°€ í•™ìŠµí•œ ê°œë°©í˜• ì˜ë¯¸ íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ í•´ì„í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ LLaVA-NeXT-8B ëª¨ë¸ì„ LLaVA-OV-72B ëª¨ë¸ë¡œ ë¶„ì„í•˜ì—¬ ì´ëŸ¬í•œ íŠ¹ì§•ì´ ëª¨ë¸ì˜ í–‰ë™ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” LMMsê°€ íŠ¹ì • ê³¼ì œì—ì„œ ë›°ì–´ë‚œ ì´ìœ ë¥¼ ê¹Šì´ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•˜ë©°, ì´ë“¤ì˜ ì‹¤ìˆ˜ì™€ ìˆ˜ì • ì „ëµì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” LMMsì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ê³¼ ì¸ê°„ ë‘ë‡Œì˜ ì¸ì§€ ê³¼ì •ê³¼ì˜ ìœ ì‚¬ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(LMMs)ì˜ ë‚´ë¶€ ì‹ ê²½ í‘œí˜„ì„ ì´í•´í•˜ê¸° ìœ„í•œ ë‹¤ëª©ì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

- 2. Sparse Autoencoder(SAE)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” íŠ¹ì§•ìœ¼ë¡œ í‘œí˜„ì„ ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤.

- 3. LMMs ìì²´ê°€ í•™ìŠµí•œ ì—´ë¦° ì˜ë¯¸ì˜ íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ í•´ì„í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

- 4. LLaVA-NeXT-8B ëª¨ë¸ì„ ë¶„ì„í•˜ì—¬ ì´ëŸ¬í•œ íŠ¹ì§•ì´ ëª¨ë¸ì˜ í–‰ë™ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

- 5. ì—°êµ¬ ê²°ê³¼ëŠ” LMMsê°€ íŠ¹ì • ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì´ìœ ë¥¼ ì´í•´í•˜ê³ , ì‹¤ìˆ˜ì˜ ë³¸ì§ˆê³¼ ìˆ˜ì • ì „ëµì„ ì œì‹œí•˜ëŠ” ë° ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-19 16:02:25*