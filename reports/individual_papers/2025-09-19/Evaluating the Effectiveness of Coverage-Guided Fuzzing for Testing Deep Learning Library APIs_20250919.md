
# Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs

**Korean Title:** ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ API í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ í¼ì§•ì˜ íš¨ê³¼ í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-19|2025-09-19]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Feedback Driven Synthesis

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization_20250919|Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization]] (79.7% similar)
- [[CodeLSI Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning]] (78.9% similar)
- [[Protocol-Aware Firmware Rehosting for Effective Fuzzing of Embedded Network Stacks_20250918|Protocol-Aware Firmware Rehosting for Effective Fuzzing of Embedded Network Stacks]] (77.3% similar)
- [[DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (77.0% similar)
- [[A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks_20250919|A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks]] (76.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14626v1 Announce Type: new 
Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to build major AI-enabled applications. Finding bugs in these libraries is important and challenging. Prior approaches have tackled this by performing either API-level fuzzing or model-level fuzzing, but they do not use coverage guidance, which limits their effectiveness and efficiency. This raises an intriguing question: can coverage guided fuzzing (CGF), in particular frameworks like LibFuzzer, be effectively applied to DL libraries, and does it offer meaningful improvements in code coverage, bug detection, and scalability compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge in applying CGF to DL libraries is the need to create a test harness for each API that can transform byte-level fuzzer inputs into valid API inputs. To address this, we propose FlashFuzz, a technique that leverages Large Language Models (LLMs) to automatically synthesize API-level harnesses by combining templates, helper functions, and API documentation. FlashFuzz uses a feedback driven strategy to iteratively synthesize and repair harnesses. With this approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x speedups in input generation. FlashFuzz has discovered 42 previously unknown bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study confirms that CGF can be effectively applied to DL libraries and provides a strong baseline for future testing approaches.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14626v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: PyTorchì™€ ê°™ì€ ë”¥ëŸ¬ë‹(DL) ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì£¼ìš” AI ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë²„ê·¸ë¥¼ ì°¾ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë©´ì„œë„ ë„ì „ì ì¸ ê³¼ì œì…ë‹ˆë‹¤. ì´ì „ ì ‘ê·¼ ë°©ì‹ì€ API ìˆ˜ì¤€ í¼ì§• ë˜ëŠ” ëª¨ë¸ ìˆ˜ì¤€ í¼ì§•ì„ ìˆ˜í–‰í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í–ˆì§€ë§Œ, ì»¤ë²„ë¦¬ì§€ ì§€ì¹¨ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ê·¸ íš¨ê³¼ì„±ê³¼ íš¨ìœ¨ì„±ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì„ ì œê¸°í•©ë‹ˆë‹¤: ì»¤ë²„ë¦¬ì§€ ì§€ì¹¨ í¼ì§•(CGF), íŠ¹íˆ LibFuzzerì™€ ê°™ì€ í”„ë ˆì„ì›Œí¬ê°€ DL ë¼ì´ë¸ŒëŸ¬ë¦¬ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì´ì „ ë°©ë²•ì— ë¹„í•´ ì½”ë“œ ì»¤ë²„ë¦¬ì§€, ë²„ê·¸ íƒì§€ ë° í™•ì¥ì„±ì—ì„œ ì˜ë¯¸ ìˆëŠ” ê°œì„ ì„ ì œê³µí•  ìˆ˜ ìˆì„ê¹Œìš”?  
ìš°ë¦¬ëŠ” ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ìµœì´ˆì˜ ì‹¬ì¸µ ì—°êµ¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. DL ë¼ì´ë¸ŒëŸ¬ë¦¬ì— CGFë¥¼ ì ìš©í•˜ëŠ” ì£¼ìš” ê³¼ì œëŠ” ë°”ì´íŠ¸ ìˆ˜ì¤€ í¼ì € ì…ë ¥ì„ ìœ íš¨í•œ API ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ê° APIì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ í•˜ë„¤ìŠ¤ë¥¼ ìƒì„±í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” FlashFuzzë¼ëŠ” ê¸°ìˆ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ í…œí”Œë¦¿, ë„ìš°ë¯¸ í•¨ìˆ˜ ë° API ë¬¸ì„œë¥¼ ê²°í•©í•˜ì—¬ API ìˆ˜ì¤€ í•˜ë„¤ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ í•©ì„±í•©ë‹ˆë‹¤. FlashFuzzëŠ” í”¼ë“œë°± ê¸°ë°˜ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ í•˜ë„¤ìŠ¤ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•©ì„±í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ FlashFuzzëŠ” 1,151ê°œì˜ PyTorch ë° 662ê°œì˜ TensorFlow APIì— ëŒ€í•œ í•˜ë„¤ìŠ¤ë¥¼ í•©ì„±í•©ë‹ˆë‹¤. ìµœì²¨ë‹¨ í¼ì§• ë°©ë²•(ACETest, PathFinder, TitanFuzz)ê³¼ ë¹„êµí–ˆì„ ë•Œ, FlashFuzzëŠ” ìµœëŒ€ 101.13%ì—ì„œ 212.88% ë” ë†’ì€ ì»¤ë²„ë¦¬ì§€ì™€ 1.0ë°°ì—ì„œ 5.4ë°° ë” ë†’ì€ ìœ íš¨ì„± ë¹„ìœ¨ì„ ë‹¬ì„±í•˜ë©°, ì…ë ¥ ìƒì„±ì—ì„œ 1ë°°ì—ì„œ 1182ë°°ì˜ ì†ë„ í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤. FlashFuzzëŠ” PyTorchì™€ TensorFlowì—ì„œ ì´ì „ì— ì•Œë ¤ì§€ì§€ ì•Šì€ 42ê°œì˜ ë²„ê·¸ë¥¼ ë°œê²¬í–ˆìœ¼ë©°, ì´ ì¤‘ 8ê°œëŠ” ì´ë¯¸ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” CGFê°€ DL ë¼ì´ë¸ŒëŸ¬ë¦¬ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŒì„ í™•ì¸í•˜ê³ , í–¥í›„ í…ŒìŠ¤íŠ¸ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ê°•ë ¥í•œ ê¸°ì¤€ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë²„ê·¸ íƒì§€ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì¸ FlashFuzzë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ API ë° ëª¨ë¸ ìˆ˜ì¤€ í¼ì§• ê¸°ë²•ì€ ì½”ë“œ ì»¤ë²„ë¦¬ì§€ì™€ íš¨ìœ¨ì„±ì— í•œê³„ê°€ ìˆì—ˆìœ¼ë‚˜, FlashFuzzëŠ” ì»¤ë²„ë¦¬ì§€ ìœ ë„ í¼ì§•(CGF)ì„ í™œìš©í•˜ì—¬ ì´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. FlashFuzzëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•´ API ìˆ˜ì¤€ì˜ í…ŒìŠ¤íŠ¸ í•˜ë„¤ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ë©°, PyTorchì™€ TensorFlowì˜ ì´ 1,813ê°œ APIì— ëŒ€í•´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ í¼ì§• ê¸°ë²•ë³´ë‹¤ ìµœëŒ€ 212.88% ë†’ì€ ì»¤ë²„ë¦¬ì§€ì™€ 5.4ë°° ë†’ì€ ìœ íš¨ì„±ì„ ë‹¬ì„±í–ˆìœ¼ë©°, 42ê°œì˜ ìƒˆë¡œìš´ ë²„ê·¸ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” CGFê°€ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ í¼ì§•(CGF)ì˜ íš¨ê³¼ì ì¸ ì ìš© ê°€ëŠ¥ì„±ì„ íƒêµ¬í•œ ìµœì´ˆì˜ ì‹¬ì¸µ ì—°êµ¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

- 2. FlashFuzzëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ API ë¬¸ì„œì™€ í…œí”Œë¦¿ì„ ê²°í•©í•´ API ìˆ˜ì¤€ì˜ í…ŒìŠ¤íŠ¸ í•˜ë„¤ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

- 3. FlashFuzzëŠ” ìµœì‹  í¼ì§• ë°©ë²•ë“¤ì— ë¹„í•´ ìµœëŒ€ 212.88% ë†’ì€ ì»¤ë²„ë¦¬ì§€ì™€ ìµœëŒ€ 5.4ë°° ë†’ì€ ìœ íš¨ì„± ë¹„ìœ¨ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 4. FlashFuzzëŠ” PyTorchì™€ TensorFlowì—ì„œ 42ê°œì˜ ìƒˆë¡œìš´ ë²„ê·¸ë¥¼ ë°œê²¬í–ˆìœ¼ë©°, ê·¸ ì¤‘ 8ê°œëŠ” ì´ë¯¸ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

- 5. ë³¸ ì—°êµ¬ëŠ” CGFê°€ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŒì„ í™•ì¸í•˜ê³ , í–¥í›„ í…ŒìŠ¤íŠ¸ ì ‘ê·¼ë²•ì„ ìœ„í•œ ê°•ë ¥í•œ ê¸°ì¤€ì ì„ ì œê³µí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-19 16:41:05*