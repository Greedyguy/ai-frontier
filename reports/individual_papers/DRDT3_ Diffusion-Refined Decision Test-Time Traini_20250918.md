
# DRDT3: Diffusion-Refined Decision Test-Time Training Model

**Korean Title:** DRDT3: í™•ì‚°-ì •ì œëœ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì‹œê°„ í›ˆë ¨ ëª¨ë¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Diffusion-Refined Decision|Diffusion-Refined Decision]] [[keywords/broad/Recurrent Neural Networks|Recurrent Neural Networks]] [[keywords/broad/Test-Time Training|Test-Time Training]] [[keywords/specific/Trajectory Modelling|Trajectory Modelling]] [[keywords/unique/DRDT3|DRDT3]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Trajectory Modelling
**ğŸ”¬ Broad Technical**: Decision Transformer, Recurrent Neural Networks
**ğŸ”— Specific Connectable**: Generative Modelling
**â­ Unique Technical**: Diffusion-Refined Decision TTT (DRDT3

**ArXiv ID**: [2501.06718](https://arxiv.org/abs/2501.06718)
**Published**: 2025-09-18
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2501.06718.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Decision Transformer` â€¢ 

`Recurrent Neural Networks` â€¢ 

`Trajectory Modelling` â€¢ 

`Diffusion-Refined Decision TTT (DRDT3` â€¢ 

`Generative Diffusion Model`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.06718v2 Announce Type: replace 
Abstract: Decision Transformer (DT), a trajectory modelling method, has shown competitive performance compared to traditional offline reinforcement learning (RL) approaches on various classic control tasks. However, it struggles to learn optimal policies from suboptimal, reward-labelled trajectories. In this study, we explore the use of conditional generative modelling to facilitate trajectory stitching given its high-quality data generation ability. Additionally, recent advancements in Recurrent Neural Networks (RNNs) have shown their linear complexity and competitive sequence modelling performance over Transformers. We leverage the Test-Time Training (TTT) layer, an RNN that updates hidden states during testing, to model trajectories in the form of DT. We introduce a unified framework, called Diffusion-Refined Decision TTT (DRDT3), to achieve performance beyond DT models. Specifically, we propose the Decision TTT (DT3) module, which harnesses the sequence modelling strengths of both self-attention and the TTT layer to capture recent contextual information and make coarse action predictions. DRDT3 iteratively refines the coarse action predictions through the generative diffusion model, progressively moving closer to the optimal actions. We further integrate DT3 with the diffusion model using a unified optimization objective. With experiments on multiple tasks in the D4RL benchmark, our DT3 model without diffusion refinement demonstrates improved performance over standard DT, while DRDT3 further achieves superior results compared to state-of-the-art DT-based and offline RL methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2501.06718v2 ë°œí‘œ ìœ í˜•: ëŒ€ì²´
ìš”ì•½: ê²°ì • íŠ¸ëœìŠ¤í¬ë¨¸(DT)ëŠ” ì „í†µì ì¸ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ(RL) ì ‘ê·¼ë²•ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ì–‘í•œ í´ë˜ì‹ ì œì–´ ì‘ì—…ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê¶¤ì  ëª¨ë¸ë§ ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, DTëŠ” ë¶€ì ì ˆí•œ ë³´ìƒì´ ë¼ë²¨ë§ëœ ê¶¤ì ì—ì„œ ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ í’ˆì§ˆ ë°ì´í„° ìƒì„± ëŠ¥ë ¥ì„ ê°ì•ˆí•˜ì—¬ ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ë§ì˜ ì‚¬ìš©ì„ íƒêµ¬í•˜ì—¬ ê¶¤ì  ìŠ¤í‹°ì¹­ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ìµœê·¼ì˜ ìˆœí™˜ ì‹ ê²½ë§(RNNs)ì˜ ë°œì „ì€ ì„ í˜• ë³µì¡ì„±ê³¼ íŠ¸ëœìŠ¤í¬ë¨¸ë³´ë‹¤ ìš°ìˆ˜í•œ ìˆœì°¨ ëª¨ë¸ë§ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í…ŒìŠ¤íŠ¸ ì‹œê°„ í›ˆë ¨(TTT) ë ˆì´ì–´ë¥¼ í™œìš©í•˜ì—¬ DT í˜•íƒœë¡œ ê¶¤ì ì„ ëª¨ë¸ë§í•˜ëŠ” RNNì¸ TTT ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” DT ëª¨ë¸ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ DRDT3ë¼ëŠ” í†µí•©ëœ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ìµœê·¼ì˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ê³  ëŒ€ëµì ì¸ í–‰ë™ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ self-attentionê³¼ TTT ë ˆì´ì–´ì˜ ìˆœì°¨ ëª¨ë¸ë§ ê°•ì ì„ í™œìš©í•˜ëŠ” Decision TTT (DT3) ëª¨ë“ˆì„ ì œì•ˆí•©ë‹ˆë‹¤. DRDT3ëŠ” ìƒì„± í™•ì‚° ëª¨ë¸ì„ í†µí•´ ëŒ€ëµì ì¸ í–‰ë™ ì˜ˆì¸¡ì„ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•˜ì—¬ ìµœì ì˜ í–‰ë™ì— ì ì  ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í†µí•© ìµœì í™” ëª©í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ DT3ë¥¼ í™•ì‚° ëª¨ë¸ê³¼ í†µí•©í•©ë‹ˆë‹¤. D4RL ë²¤ì¹˜ë§ˆí¬ì˜ ì—¬ëŸ¬ ì‘ì—…ì—ì„œì˜ ì‹¤í—˜ì„ í†µí•´, í™•ì‚° ê°œì„  ì—†ì´ DT3 ëª¨ë¸ì´ í‘œì¤€ DTë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë©°, DRDT3ëŠ” ìµœì²¨ë‹¨ DT ê¸°ë°˜ ë° ì˜¤í”„ë¼ì¸ RL ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ë³¸ ì—°êµ¬ì—ì„œëŠ” ê²°ì • íŠ¸ëœìŠ¤í¬ë¨¸(Decision Transformer, DT)ë¥¼ í†µí•´ ì „í†µì ì¸ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ ë°©ë²•ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ë¶€ì ì ˆí•œ ë³´ìƒì´ ë¼ë²¨ë§ëœ ê¶¤ì ì—ì„œ ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆìŒì„ ë°í˜”ë‹¤. ì´ì— ìš°ë¦¬ëŠ” ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ë§ì„ í™œìš©í•˜ì—¬ ê¶¤ì  ì—°ê²°ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í–ˆìœ¼ë©°, ìµœê·¼ ë°œì „ëœ ìˆœí™˜ ì‹ ê²½ë§(RNNs) ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ DT í˜•íƒœì˜ ê¶¤ì ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ Test-Time Training (TTT) ë ˆì´ì–´ë¥¼ í™œìš©í•œ í†µí•©ëœ í”„ë ˆì„ì›Œí¬ì¸ DRDT3ë¥¼ ì œì•ˆí–ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DRDT3ëŠ” ê¸°ì¡´ DT ëª¨ë¸ ë° ì˜¤í”„ë¼ì¸ RL ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Decision Transformer (DT)ëŠ” ì „í†µì ì¸ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ ì ‘ê·¼ë²•ì— ë¹„í•´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë¶€ì ì ˆí•œ ë³´ìƒì´ ë ˆì´ë¸”ëœ ê¶¤ì ì—ì„œ ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤.

- 2. ìµœê·¼ RNNì˜ ì„ í˜• ë³µì¡ì„±ê³¼ Transformerë³´ë‹¤ ìš°ìˆ˜í•œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ì„±ëŠ¥ì„ í™œìš©í•˜ì—¬ Test-Time Training (TTT) ë ˆì´ì–´ë¥¼ í™œìš©í•œ DT ëª¨ë¸ì¸ DRDT3ë¥¼ ì†Œê°œí•œë‹¤.

- 3. DRDT3ëŠ” DT ëª¨ë¸ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ìµœì ì˜ í–‰ë™ì— ì ì§„ì ìœ¼ë¡œ ê°€ê¹Œì›Œì§€ë„ë¡ ìƒì„± í™•ì‚° ëª¨ë¸ì„ í†µí•´ ê±°ì¹œ í–‰ë™ ì˜ˆì¸¡ì„ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•œë‹¤.


---

*Generated on 2025-09-18 16:46:20*