# REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting

**Korean Title:** ì˜ê²¬ ìš”ì•½ì—ì„œì˜ í¸í–¥ ì™„í™”ë¥¼ ìœ„í•œ ë¹ˆë„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸: REFER

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Fairness in Opinion Summarisation|Fairness in Opinion Summarisation]] [[keywords/specific/Frequency Based Representations|Frequency Based Representations]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/unique/Frequency Framed Prompting|Frequency Framed Prompting]] [[categories/cs.CL|cs.CL]] [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (85.5% similar) [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (85.3% similar) [[2025-09-22/Re-FRAME the Meeting Summarization SCOPE_ Fact-Based Summarization and Personalization via Questions_20250922|Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Fairness in Opinion Summarisation
**ğŸ”— Specific Connectable**: Frequency Framed Prompting
**ğŸ”¬ Broad Technical**: Large Language Models, Natural Language Processing
**â­ Unique Technical**: REFER
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (85.5% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (85.3% similar)
- [[2025-09-22/Re-FRAME the Meeting Summarization SCOPE_ Fact-Based Summarization and Personalization via Questions_20250922|Re-FRAME the Meeting Summarization SCOPE Fact-Based Summarization and Personalization via Questions]] (84.5% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (84.3% similar)
- [[2025-09-22/Think, Verbalize, then Speak_ Bridging Complex Thoughts and Comprehensible Speech_20250922|Think, Verbalize, then Speak Bridging Complex Thoughts and Comprehensible Speech]] (84.3% similar)


**ArXiv ID**: [2509.15723](https://arxiv.org/abs/2509.15723)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15723.pdf)


**ArXiv ID**: [2509.15723](https://arxiv.org/abs/2509.15723)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15723.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Fairness in Opinion Summarisation
**ğŸ”— Specific Connectable**: Frequency Framed Prompting
**â­ Unique Technical**: REFER
**ğŸ”¬ Broad Technical**: Large Language Models, Natural Language Processing

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Natural Language Processing` â€¢ 

`Frequency Framed Prompting` â€¢ 

`REFER` â€¢ 

`Fairness in Opinion Summarisation`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15723v1 Announce Type: new 
Abstract: Individuals express diverse opinions, a fair summary should represent these viewpoints comprehensively. Previous research on fairness in opinion summarisation using large language models (LLMs) relied on hyperparameter tuning or providing ground truth distributional information in prompts. However, these methods face practical limitations: end-users rarely modify default model parameters, and accurate distributional information is often unavailable. Building upon cognitive science research demonstrating that frequency-based representations reduce systematic biases in human statistical reasoning by making reference classes explicit and reducing cognitive load, this study investigates whether frequency framed prompting (REFER) can similarly enhance fairness in LLM opinion summarisation. Through systematic experimentation with different prompting frameworks, we adapted techniques known to improve human reasoning to elicit more effective information processing in language models compared to abstract probabilistic representations.Our results demonstrate that REFER enhances fairness in language models when summarising opinions. This effect is particularly pronounced in larger language models and using stronger reasoning instructions.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15723v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ê°œì¸ë“¤ì€ ë‹¤ì–‘í•œ ì˜ê²¬ì„ í‘œí˜„í•˜ë©°, ê³µì •í•œ ìš”ì•½ì€ ì´ëŸ¬í•œ ê´€ì ì„ í¬ê´„ì ìœ¼ë¡œ ëŒ€í‘œí•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•œ ì˜ê²¬ ìš”ì•½ì˜ ê³µì •ì„±ì— ëŒ€í•œ ì´ì „ ì—°êµ¬ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‹¤ì œ ë¶„í¬ ì •ë³´ ì œê³µì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ì€ ì‹¤ì§ˆì ì¸ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìµœì¢… ì‚¬ìš©ìëŠ” ê¸°ë³¸ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ê±°ì˜ ìˆ˜ì •í•˜ì§€ ì•Šìœ¼ë©°, ì •í™•í•œ ë¶„í¬ ì •ë³´ëŠ” ì¢…ì¢… ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì§€ ê³¼í•™ ì—°êµ¬ì— ë”°ë¥´ë©´ ë¹ˆë„ ê¸°ë°˜ í‘œí˜„ì€ ì°¸ì¡° í´ë˜ìŠ¤ë¥¼ ëª…í™•íˆ í•˜ê³  ì¸ì§€ ë¶€í•˜ë¥¼ ì¤„ì„ìœ¼ë¡œì¨ ì¸ê°„ì˜ í†µê³„ì  ì¶”ë¡ ì—ì„œ ì²´ê³„ì ì¸ í¸í–¥ì„ ì¤„ì¸ë‹¤ëŠ” ì ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ ì—°êµ¬ëŠ” ë¹ˆë„ ê¸°ë°˜ í”„ë ˆì´ë° í”„ë¡¬í”„íŒ…(REFER)ì´ LLM ì˜ê²¬ ìš”ì•½ì—ì„œ ê³µì •ì„±ì„ ìœ ì‚¬í•˜ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ ì¡°ì‚¬í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ í”„ë¡¬í”„íŒ… í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ì²´ê³„ì ì¸ ì‹¤í—˜ì„ í†µí•´, ìš°ë¦¬ëŠ” ì¸ê°„ì˜ ì¶”ë¡ ì„ ê°œì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ ê¸°ë²•ì„ ì¶”ìƒì ì¸ í™•ë¥ ì  í‘œí˜„ë³´ë‹¤ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë” íš¨ê³¼ì ì¸ ì •ë³´ ì²˜ë¦¬ë¥¼ ì´ëŒì–´ë‚´ê¸° ìœ„í•´ ì ì‘ì‹œì¼°ìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” REFERê°€ ì˜ê²¬ì„ ìš”ì•½í•  ë•Œ ì–¸ì–´ ëª¨ë¸ì˜ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ íš¨ê³¼ëŠ” íŠ¹íˆ ë” í° ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ì¶”ë¡  ì§€ì¹¨ì„ ì‚¬ìš©í•  ë•Œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì˜ê²¬ì„ ê³µì •í•˜ê²Œ ìš”ì•½í•˜ê¸° ìœ„í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œì˜ ê³µì •ì„±ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ë‚˜ ì •í™•í•œ ë¶„í¬ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë°©ì‹ì— ì˜ì¡´í–ˆì§€ë§Œ, ì´ëŠ” ì‹¤ìš©ì ì¸ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì¸ì§€ê³¼í•™ì—ì„œì˜ ë¹ˆë„ ê¸°ë°˜ í‘œí˜„ì´ ì¸ê°„ì˜ í†µê³„ì  ì¶”ë¡  í¸í–¥ì„ ì¤„ì¸ë‹¤ëŠ” ì ì— ì°©ì•ˆí•˜ì—¬, ë¹ˆë„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸(REPER)ê°€ LLMì˜ ì˜ê²¬ ìš”ì•½ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, REPERëŠ” íŠ¹íˆ ë” í° ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ì¶”ë¡  ì§€ì¹¨ì„ ì‚¬ìš©í•  ë•Œ ê³µì •ì„±ì„ ì¦ì§„ì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ë‹¤ì–‘í•œ ì˜ê²¬ì„ ê³µì •í•˜ê²Œ ìš”ì•½í•˜ê¸° ìœ„í•´ì„œëŠ” ê° ê´€ì ì„ í¬ê´„ì ìœ¼ë¡œ ëŒ€í‘œí•´ì•¼ í•œë‹¤.

- 2. ê¸°ì¡´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ë‚˜ í”„ë¡¬í”„íŠ¸ì— ì •í™•í•œ ë¶„í¬ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë°©ì‹ì— ì˜ì¡´í–ˆìœ¼ë‚˜, ì´ëŠ” ì‹¤ìš©ì  í•œê³„ê°€ ìˆë‹¤.

- 3. ë¹ˆë„ ê¸°ë°˜ í‘œí˜„ì´ ì¸ê°„ì˜ í†µê³„ì  ì¶”ë¡ ì—ì„œ ì²´ê³„ì  í¸í–¥ì„ ì¤„ì¸ë‹¤ëŠ” ì¸ì§€ê³¼í•™ ì—°êµ¬ì— ê¸°ë°˜í•˜ì—¬, ë³¸ ì—°êµ¬ëŠ” ë¹ˆë„ í”„ë ˆì„ í”„ë¡¬í”„íŠ¸(REFER)ê°€ LLMì˜ ì˜ê²¬ ìš”ì•½ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í–ˆë‹¤.

- 4. ì—°êµ¬ ê²°ê³¼, REFERëŠ” íŠ¹íˆ ë” í° ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ì¶”ë¡  ì§€ì¹¨ì„ ì‚¬ìš©í•  ë•Œ ì˜ê²¬ ìš”ì•½ì˜ ê³µì •ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.

- 5. ë¹ˆë„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ëŠ” ì¶”ìƒì  í™•ë¥  í‘œí˜„ë³´ë‹¤ ì–¸ì–´ ëª¨ë¸ì˜ ì •ë³´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ë” íš¨ê³¼ì ìœ¼ë¡œ ìœ ë„í•œë‹¤.


---

*Generated on 2025-09-22 16:26:31*