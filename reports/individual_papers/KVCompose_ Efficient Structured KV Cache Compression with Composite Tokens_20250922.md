# KVCompose: Efficient Structured KV Cache Compression with Composite Tokens

**Korean Title:** KVCompose: í•©ì„± í† í°ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ êµ¬ì¡°ì  KV ìºì‹œ ì••ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Layer-adaptive Composite Tokens|Layer-adaptive Composite Tokens]] [[keywords/specific/Attention Mechanism|Attention Mechanism]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/unique/KVCompose|KVCompose]] [[categories/cs.LG|cs.LG]] [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (89.9% similar) [[2025-09-22/ConCISE_ Confidence-guided Compression in Step-by-step Efficient Reasoning_20250922|ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning]] (83.3% similar) [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Layer Adaptive Compression
**ğŸ”— Specific Connectable**: Attention Mechanism
**ğŸ”¬ Broad Technical**: Large Language Models
**â­ Unique Technical**: Composite Tokens
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (89.9% similar)
- [[2025-09-22/ConCISE_ Confidence-guided Compression in Step-by-step Efficient Reasoning_20250922|ConCISE Confidence-guided Compression in Step-by-step Efficient Reasoning]] (83.3% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (83.2% similar)
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (82.7% similar)
- [[2025-09-18/Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning_20250918|Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning]] (82.1% similar)


**ArXiv ID**: [2509.05165](https://arxiv.org/abs/2509.05165)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.05165.pdf)


**ArXiv ID**: [2509.05165](https://arxiv.org/abs/2509.05165)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.05165.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Layer-adaptive Composite Tokens
**ğŸ”— Specific Connectable**: Attention Mechanism
**â­ Unique Technical**: KVCompose
**ğŸ”¬ Broad Technical**: Large Language Models

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Attention Mechanism` â€¢ 

`KVCompose` â€¢ 

`Layer-adaptive Composite Tokens`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.05165v2 Announce Type: replace 
Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.
  We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.05165v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ íš¨ìœ¨ì ì¸ ìê¸° íšŒê·€ ë””ì½”ë”©ì„ ìœ„í•´ í‚¤-ê°’(KV) ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìºì‹œ í¬ê¸°ëŠ” ë¬¸ë§¥ ê¸¸ì´ì™€ ëª¨ë¸ ê¹Šì´ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì¶”ë¡ ì—ì„œ ì£¼ìš” ë³‘ëª© í˜„ìƒì´ ë©ë‹ˆë‹¤. ì´ì „ì˜ KV ìºì‹œ ì••ì¶• ë°©ë²•ì€ ì—„ê²©í•œ íœ´ë¦¬ìŠ¤í‹±ì„ ê°•ìš”í•˜ê±°ë‚˜, ì£¼ì˜ í—¤ë“œë³„ ë³€ë™ì„±ìœ¼ë¡œ ì¸í•´ í…ì„œ ë ˆì´ì•„ì›ƒì„ ë°©í•´í•˜ê±°ë‚˜, íŠ¹ìˆ˜í•œ ê³„ì‚° ì»¤ë„ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.  
ìš°ë¦¬ëŠ” ì£¼ì˜ ê¸°ë°˜ì˜ ê³„ì¸µ ì ì‘í˜• ë³µí•© í† í°ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ KV ìºì‹œ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì€ ì£¼ì˜ ì ìˆ˜ë¥¼ ì§‘ê³„í•˜ì—¬ í† í°ì˜ ì¤‘ìš”ì„±ì„ ì¶”ì •í•˜ê³ , í—¤ë“œë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í† í°ì„ ì„ íƒí•˜ë©°, ê¸°ì¡´ ì¶”ë¡  ì—”ì§„ì´ ìš”êµ¬í•˜ëŠ” ê· ì¼í•œ ìºì‹œ êµ¬ì¡°ë¥¼ ì¡´ì¤‘í•˜ëŠ” ë³µí•© í† í°ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤. ì „ì—­ í• ë‹¹ ë©”ì»¤ë‹ˆì¦˜ì€ ê³„ì¸µ ê°„ ìœ ì§€ ì˜ˆì‚°ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì •ë³´ê°€ ë§ì€ í† í°ì„ ê°€ì§„ ê³„ì¸µì— ë” ë§ì€ ìš©ëŸ‰ì„ í• ë‹¹í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë©”ëª¨ë¦¬ë¥¼ í¬ê²Œ ì¤„ì´ë©°, ì´ì „ì˜ êµ¬ì¡°ì  ë° ë°˜êµ¬ì¡°ì  ë°©ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€, ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì´ í‘œì¤€ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ê³¼ ì™„ì „íˆ í˜¸í™˜ë˜ì–´, íš¨ìœ¨ì ì¸ ê¸´ ë¬¸ë§¥ LLM ë°°í¬ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì ì¸ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ë””ì½”ë”©ì„ ìœ„í•œ KV ìºì‹œ ì••ì¶• ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë¹„íš¨ìœ¨ì ì´ê±°ë‚˜ íŠ¹ìˆ˜í•œ ê³„ì‚° ì»¤ë„ì„ ìš”êµ¬í•˜ëŠ” ë°˜ë©´, ì œì•ˆëœ ë°©ë²•ì€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë ˆì´ì–´ ì ì‘í˜• ë³µí•© í† í°ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ì••ì¶•ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì£¼ì˜ ì ìˆ˜ë¥¼ í†µí•´ í† í°ì˜ ì¤‘ìš”ë„ë¥¼ ì¶”ì •í•˜ê³ , í—¤ë“œë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í† í°ì„ ì„ íƒí•˜ì—¬ ê¸°ì¡´ ì¶”ë¡  ì—”ì§„ì˜ ìºì‹œ êµ¬ì¡°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ê¸€ë¡œë²Œ í• ë‹¹ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë ˆì´ì–´ë³„ë¡œ ì €ì¥ ìš©ëŸ‰ì„ ì¡°ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ë©´ì„œ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ì˜ êµ¬ì¡°ì  ë° ë°˜êµ¬ì¡°ì  ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•˜ë©°, í‘œì¤€ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ê³¼ ì™„ì „íˆ í˜¸í™˜ë˜ì–´ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì ì¸ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ë””ì½”ë”©ì„ ìœ„í•´ KV ìºì‹œê°€ í•„ìš”í•˜ì§€ë§Œ, ìºì‹œ í¬ê¸°ëŠ” ë¬¸ë§¥ ê¸¸ì´ì™€ ëª¨ë¸ ê¹Šì´ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì¶”ë¡ ì—ì„œ ì£¼ìš” ë³‘ëª© í˜„ìƒì´ ëœë‹¤.

- 2. ê¸°ì¡´ì˜ KV ìºì‹œ ì••ì¶• ë°©ë²•ì€ ì—„ê²©í•œ íœ´ë¦¬ìŠ¤í‹±ì„ ê°•ìš”í•˜ê±°ë‚˜, ì£¼ì˜ í—¤ë“œë³„ ë³€ë™ì„±ìœ¼ë¡œ í…ì„œ ë ˆì´ì•„ì›ƒì„ ë°©í•´í•˜ê±°ë‚˜, íŠ¹ìˆ˜í•œ ê³„ì‚° ì»¤ë„ì„ ìš”êµ¬í•œë‹¤.

- 3. ì œì•ˆëœ ë°©ë²•ì€ ì£¼ì˜ ê¸°ë°˜, ë ˆì´ì–´ ì ì‘í˜• ë³µí•© í† í°ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ KV ìºì‹œ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•œë‹¤.

- 4. ì´ ë°©ë²•ì€ ì£¼ì˜ ì ìˆ˜ë¥¼ ì§‘ê³„í•˜ì—¬ í† í° ì¤‘ìš”ì„±ì„ ì¶”ì •í•˜ê³ , í—¤ë“œë³„ í† í°ì„ ë…ë¦½ì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ê¸°ì¡´ ì¶”ë¡  ì—”ì§„ì´ ìš”êµ¬í•˜ëŠ” ê· ì¼í•œ ìºì‹œ êµ¬ì¡°ì— ë§ê²Œ ë³µí•© í† í°ìœ¼ë¡œ ì •ë ¬í•œë‹¤.

- 5. ì œì•ˆëœ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ë©´ì„œ ì •í™•ì„±ì„ ìœ ì§€í•˜ë©°, ê¸°ì¡´ì˜ êµ¬ì¡°ì  ë° ë°˜êµ¬ì¡°ì  ë°©ë²•ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•œë‹¤.


---

*Generated on 2025-09-22 16:02:26*