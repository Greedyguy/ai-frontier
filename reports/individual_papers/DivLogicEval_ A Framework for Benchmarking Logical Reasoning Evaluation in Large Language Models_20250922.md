# DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models

**Korean Title:** DivLogicEval: 대형 언어 모델에서 논리적 추론 평가를 벤치마킹하기 위한 프레임워크

## 📋 메타데이터

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Bias Mitigation Metric

## 🔗 유사한 논문
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.6% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.4% similar)
- [[2025-09-19/Large Language Model probabilities cannot distinguish between possible and impossible language_20250919|Large Language Model probabilities cannot distinguish between possible and impossible language]] (84.2% similar)
- [[2025-09-19/Understanding the Thinking Process of Reasoning Models_ A Perspective from Schoenfeld's Episode Theory_20250919|Understanding the Thinking Process of Reasoning Models A Perspective from Schoenfeld's Episode Theory]] (83.6% similar)
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox A practical guide to getting the most out of human ratings]] (83.3% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15587v1 Announce Type: cross 
Abstract: Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.

## 🔍 Abstract (한글 번역)

arXiv:2509.15587v1 발표 유형: 교차  
초록: 자연어에서의 논리적 추론은 대형 언어 모델(LLMs)의 인간 지능을 측정하는 중요한 척도로 인식되어 왔습니다. 일반적인 벤치마크는 여러 추론 기술을 얽히게 하여 논리적 추론 기술에 대한 신뢰할 수 없는 평가를 제공할 수 있습니다. 동시에, 기존의 논리적 추론 벤치마크는 언어의 다양성이 제한적이며, 이상적인 논리적 추론 벤치마크의 분포와는 차이가 있어 편향된 평가 결과를 초래할 수 있습니다. 따라서 본 논문은 다양한 진술로 구성된 자연 문장들을 직관에 반하는 방식으로 구성한 새로운 고전 논리 벤치마크인 DivLogicEval을 제안합니다. 보다 신뢰할 수 있는 평가를 보장하기 위해, LLMs에 내재된 편향성과 무작위성의 영향을 완화하는 새로운 평가 지표도 도입합니다. 실험을 통해 DivLogicEval의 질문에 답하기 위해 요구되는 논리적 추론의 정도를 입증하고, 다양한 인기 있는 LLMs의 논리적 추론 수행 능력을 비교합니다.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 논리적 추론 능력을 평가하기 위한 새로운 벤치마크인 DivLogicEval을 제안합니다. 기존의 논리 추론 벤치마크는 언어 다양성이 부족하고 이상적인 분포와 차이가 있어 편향된 평가 결과를 초래할 수 있습니다. DivLogicEval은 다양한 문장으로 구성되어 있으며, 새로운 평가 지표를 도입하여 LLM의 편향과 무작위성을 줄입니다. 실험을 통해 DivLogicEval에서 논리적 추론이 얼마나 필요한지와 다양한 LLM의 성능을 비교합니다.

## 🎯 주요 포인트

- 1. 자연어에서의 논리적 추론은 대형 언어 모델(LLM)의 인간 지능 측정에 중요한 요소로 인식되고 있습니다.

- 2. 기존의 논리적 추론 벤치마크는 언어 다양성이 부족하고 이상적인 논리적 추론 벤치마크의 분포와 차이가 있어 편향된 평가 결과를 초래할 수 있습니다.

- 3. 본 논문은 다양한 진술로 구성된 자연어 문장으로 이루어진 새로운 고전 논리 벤치마크인 DivLogicEval을 제안합니다.

- 4. 평가의 신뢰성을 높이기 위해 LLM의 편향성과 무작위성을 완화하는 새로운 평가 지표를 도입하였습니다.

- 5. 실험을 통해 DivLogicEval에서 질문에 답하기 위해 요구되는 논리적 추론의 정도와 다양한 LLM의 논리적 추론 수행 능력을 비교하였습니다.

---

*Generated on 2025-09-22 14:05:01*