# Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions

**Korean Title:** ë¹„ì´ìƒì ì¸ ì €í•­ ì†Œìì—ì„œì˜ ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ í•™ìŠµ: ì‘ë‹µ í•¨ìˆ˜ì˜ ì˜í–¥

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Non-ideal Response Functions|Non-ideal Response Functions]] [[keywords/specific/Gradient-based Training|Gradient-based Training]] [[keywords/broad/Analog In-memory Computing|Analog In-memory Computing]] [[keywords/unique/Residual Learning Algorithm|Residual Learning Algorithm]] [[categories/cs.LG|cs.LG]] [[2025-09-22/Training thermodynamic computers by gradient descent_20250922|Training thermodynamic computers by gradient descent]] (81.9% similar) [[2025-09-18/Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (81.1% similar) [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (80.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Non-ideal Response Functions
**ğŸ”— Specific Connectable**: Gradient-based Training
**ğŸ”¬ Broad Technical**: Analog In-memory Computing
**â­ Unique Technical**: Residual Learning Algorithm
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Training thermodynamic computers by gradient descent_20250922|Training thermodynamic computers by gradient descent]] (81.9% similar)
- [[2025-09-18/Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (81.1% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (80.0% similar)
- [[2025-09-22/Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization_20250922|Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization]] (79.5% similar)
- [[2025-09-22/Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers_20250922|Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers]] (79.4% similar)


**ArXiv ID**: [2502.06309](https://arxiv.org/abs/2502.06309)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2502.06309.pdf)


**ArXiv ID**: [2502.06309](https://arxiv.org/abs/2502.06309)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2502.06309.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Non-ideal Response Functions
**ğŸ”— Specific Connectable**: Gradient-based Training
**â­ Unique Technical**: Residual Learning Algorithm
**ğŸ”¬ Broad Technical**: Analog In-memory Computing

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Analog In-memory Computing` â€¢ 

`Gradient-based Training` â€¢ 

`Residual Learning Algorithm` â€¢ 

`Non-ideal Response Functions`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.06309v3 Announce Type: replace 
Abstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear \textit{response functions}, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.06309v3 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ëŒ€ê·œëª¨ ë¹„ì „ ë˜ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ë° ë°°í¬ì— ëŒ€í•œ ê²½ì œì  ë° í™˜ê²½ì  ë¹„ìš©ì´ ê¸‰ê²©íˆ ì¦ê°€í•¨ì— ë”°ë¼, ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ ì»´í“¨íŒ…(AIMC)ì€ ìœ ë§í•œ ì—ë„ˆì§€ íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í›ˆë ¨ ê´€ì , íŠ¹íˆ í›ˆë ¨ ë™ì—­í•™ì€ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AIMC í•˜ë“œì›¨ì–´ì—ì„œëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ê°€ ì €í•­ì„± ìš”ì†Œì˜ ì „ë„ë„ë¡œ í‘œí˜„ë˜ë©° ì—°ì†ì ì¸ ì „ê¸° í„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ê° í„ìŠ¤ì— ëŒ€í•œ ì „ë„ë„ ë³€í™”ëŠ” ì¼ì •í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì„ í˜•ì ì¸ \textit{ë°˜ì‘ í•¨ìˆ˜}ì— ì˜í•´ ë³€í™”ê°€ ì¡°ì •ë˜ì–´ ë¹„ì´ìƒì ì¸ í›ˆë ¨ ë™ì—­í•™ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ë¹„ì´ìƒì ì¸ ë°˜ì‘ í•¨ìˆ˜ë¥¼ ê°€ì§„ AIMC í•˜ë“œì›¨ì–´ì—ì„œì˜ ê¸°ìš¸ê¸° ê¸°ë°˜ í›ˆë ¨ì— ëŒ€í•œ ì´ë¡ ì  ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¹„ëŒ€ì¹­ ë°˜ì‘ í•¨ìˆ˜ê°€ ëª©í‘œì— ì•”ë¬µì ì¸ íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•¨ìœ¼ë¡œì¨ ì•„ë‚ ë¡œê·¸ SGDì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì´ì¤‘ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ì„ê³„ì ì— ì •í™•íˆ ìˆ˜ë ´í•˜ëŠ” ì”ì°¨ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì œí•œëœ ë°˜ì‘ ì„¸ë¶„í™”ì™€ ê°™ì€ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ ê²°í•¨ì„ í•´ê²°í•˜ê¸° ìœ„í•´ í™•ì¥ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” ë°”ë¡œëŠ”, ì´ëŠ” ì¼ë°˜ì ì¸ ë¹„ì´ìƒì  ë°˜ì‘ í•¨ìˆ˜ í´ë˜ìŠ¤ì˜ ì˜í–¥ì„ ì¡°ì‚¬í•œ ìµœì´ˆì˜ ë…¼ë¬¸ì…ë‹ˆë‹¤. ê²°ë¡ ì€ ìš°ë¦¬ì˜ ì´ë¡ ì  í†µì°°ì„ ê²€ì¦í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ì— ì˜í•´ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ ì»´í“¨íŒ…(AIMC) í•˜ë“œì›¨ì–´ì—ì„œ ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ê°€ ì•„ë‚ ë¡œê·¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Analog SGD)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. AIMCì—ì„œ ì €í•­ ìš”ì†Œì˜ ì „ë„ë„ë¡œ í‘œí˜„ë˜ëŠ” ê°€ì¤‘ì¹˜ëŠ” ì „ê¸° í„ìŠ¤ë¥¼ í†µí•´ ì—…ë°ì´íŠ¸ë˜ë©°, ì‹¤ì œë¡œëŠ” ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì„ í˜•ì ì¸ ì‘ë‹µ í•¨ìˆ˜ì— ì˜í•´ ìŠ¤ì¼€ì¼ë§ë˜ì–´ ë¹„ì´ìƒì ì¸ í•™ìŠµ ë™ì ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì€ ì”ì°¨ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ì´ì¤‘ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ìˆ˜ë ´ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì œí•œëœ ì‘ë‹µ ì„¸ë¶„í™”ì™€ ê°™ì€ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´ ê²°í•¨ë„ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ë¡ ì  í†µì°°ì„ ê²€ì¦í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ê°€ AIMC í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ìµœì´ˆë¡œ ì¡°ì‚¬í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ì•„ë‚ ë¡œê·¸ ì¸-ë©”ëª¨ë¦¬ ì»´í“¨íŒ…(AIMC)ì€ ëŒ€í˜• ë¹„ì „ ë° ì–¸ì–´ ëª¨ë¸ì˜ ê²½ì œì , í™˜ê²½ì  ë¹„ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ì—ë„ˆì§€ íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

- 2. AIMC í•˜ë“œì›¨ì–´ì—ì„œ ë¹„ëŒ€ì¹­ì ì´ê³  ë¹„ì„ í˜•ì ì¸ ì‘ë‹µ í•¨ìˆ˜ë¡œ ì¸í•´ í›ˆë ¨ ë™íƒœê°€ ì´ìƒì ì´ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.

- 3. ë³¸ ë…¼ë¬¸ì€ AIMC í•˜ë“œì›¨ì–´ì—ì„œ ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ì— ëŒ€í•œ ê²½ì‚¬ ê¸°ë°˜ í›ˆë ¨ì˜ ì´ë¡ ì  ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.

- 4. ì œì•ˆëœ ì”ì°¨ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ì´ì¤‘ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ì •í™•íˆ ì„ê³„ì ì— ìˆ˜ë ´í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

- 5. ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ì˜ ì´ë¡ ì  í†µì°°ì„ ê²€ì¦í•˜ì˜€ìœ¼ë©°, ì´ëŠ” ë¹„ì´ìƒì ì¸ ì‘ë‹µ í•¨ìˆ˜ì˜ ì˜í–¥ì„ ì¡°ì‚¬í•œ ìµœì´ˆì˜ ì—°êµ¬ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-22 15:54:24*