
# HAM: Hierarchical Adapter Merging for Scalable Continual Learning

**Korean Title:** HAM: í™•ì¥ ê°€ëŠ¥í•œ ì§€ì†ì  í•™ìŠµì„ ìœ„í•œ ê³„ì¸µì  ì–´ëŒ‘í„° ë³‘í•©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Dynamic Learning Scenarios|Dynamic Learning Scenarios]] [[keywords/broad/Continual Learning|Continual Learning]] [[keywords/broad/Parameter-Efficient Fine-Tuning|Parameter-Efficient Fine-Tuning]] [[keywords/specific/Hierarchical Adapters Merging|Hierarchical Adapters Merging]] [[keywords/unique/HAM|HAM]] [[categories/cs.LG|cs.LG]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Dynamic Learning Scenarios
**ğŸ”¬ Broad Technical**: Continual Learning, Parameter-Efficient Fine-Tuning
**ğŸ”— Specific Connectable**: Hierarchical Adapters Merging
**â­ Unique Technical**: HAM

**ArXiv ID**: [2509.13211](https://arxiv.org/abs/2509.13211)
**Published**: 2025-09-18
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.13211.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Continual Learning` â€¢ 

`Parameter-Efficient Fine-Tuning` â€¢ 

`Hierarchical Adapters Merging` â€¢ 

`HAM` â€¢ 

`Dynamic Learning Scenarios`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13211v2 Announce Type: replace 
Abstract: Continual learning is an essential capability of human cognition, yet it poses significant challenges for current deep learning models. The primary issue is that new knowledge can interfere with previously learned information, causing the model to forget earlier knowledge in favor of the new, a phenomenon known as catastrophic forgetting. Although large pre-trained models can partially mitigate forgetting by leveraging their existing knowledge and over-parameterization, they often struggle when confronted with novel data distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, enable efficient adaptation to new knowledge. However, they still face challenges in scaling to dynamic learning scenarios and long sequences of tasks, as maintaining one adapter per task introduces complexity and increases the potential for interference. In this paper, we introduce Hierarchical Adapters Merging (HAM), a novel framework that dynamically combines adapters from different tasks during training. This approach enables HAM to scale effectively, allowing it to manage more tasks than competing baselines with improved efficiency. To achieve this, HAM maintains a fixed set of groups that hierarchically consolidate new adapters. For each task, HAM trains a low-rank adapter along with an importance scalar, then dynamically groups tasks based on adapter similarity. Within each group, adapters are pruned, scaled and merge, facilitating transfer learning between related tasks. Extensive experiments on three vision benchmarks show that HAM significantly outperforms state-of-the-art methods, particularly as the number of tasks increases.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13211v2 ë°œí‘œ ìœ í˜•: ëŒ€ì²´
ìš”ì•½: ì§€ì†ì  í•™ìŠµì€ ì¸ê°„ ì¸ì§€ì˜ í•„ìˆ˜ ëŠ¥ë ¥ì´ì§€ë§Œ, í˜„ì¬ì˜ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì—ëŠ” ì¤‘ìš”í•œ ë„ì „ ê³¼ì œë¥¼ ì œê¸°í•©ë‹ˆë‹¤. ì£¼ìš” ë¬¸ì œëŠ” ìƒˆë¡œìš´ ì§€ì‹ì´ ì´ì „ì— í•™ìŠµí•œ ì •ë³´ì™€ ê°„ì„­í•˜ì—¬ ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ ì„ í˜¸í•˜ëŠ” ë™ì•ˆ ì´ì „ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” í˜„ìƒì¸ ì¬ì•™ì ì¸ ìŠí˜€ì§(catastrophic forgetting)ì…ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì€ ê¸°ì¡´ ì§€ì‹ê³¼ ê³¼ì í•©ì„ í™œìš©í•˜ì—¬ ìŠí˜€ì§ì„ ì¼ë¶€ ì™„í™”í•  ìˆ˜ ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„° ë¶„í¬ì— ì§ë©´í•  ë•Œ ì¢…ì¢… ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì  ì„¸ë°€ ì¡°ì •(Parameter-Efficient Fine-Tuning, PEFT) ë°©ë²•ì¸ LoRAì™€ ê°™ì€ ë°©ë²•ë“¤ì€ ìƒˆë¡œìš´ ì§€ì‹ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ë™ì  í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì™€ ì¥ê¸°ê°„ì˜ ì‘ì—… ì‹œí€€ìŠ¤ë¡œ í™•ì¥í•˜ëŠ” ë° ì—¬ì „íˆ ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ê° ì‘ì—… ë‹¹ í•˜ë‚˜ì˜ ì–´ëŒ‘í„°ë¥¼ ìœ ì§€í•¨ìœ¼ë¡œì¨ ë³µì¡ì„±ì„ ë„ì…í•˜ê³  ê°„ì„­ ê°€ëŠ¥ì„±ì„ ì¦ê°€ì‹œí‚¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” HAM(Hierarchical Adapters Merging)ì´ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ í›ˆë ¨ ì¤‘ì— ì„œë¡œ ë‹¤ë¥¸ ì‘ì—…ì—ì„œ ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°í•©í•¨ìœ¼ë¡œì¨ HAMì´ íš¨ê³¼ì ìœ¼ë¡œ í™•ì¥ë˜ì–´ ê²½ìŸí•˜ëŠ” ê¸°ì¤€ì„ ë³´ë‹¤ ë” ë§ì€ ì‘ì—…ì„ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ HAMì€ ìƒˆë¡œìš´ ì–´ëŒ‘í„°ë¥¼ ê³„ì¸µì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ê³ ì •ëœ ê·¸ë£¹ ì„¸íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ê° ì‘ì—…ì— ëŒ€í•´ HAMì€ ì¤‘ìš”ë„ ìŠ¤ì¹¼ë¼ì™€ í•¨ê»˜ ì €ë­í¬ ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•˜ê³ , ê·¸ëŸ° ë‹¤ìŒ ì–´ëŒ‘í„° ìœ ì‚¬ì„±ì— ê¸°ì´ˆí•˜ì—¬ ì‘ì—…ì„ ë™ì ìœ¼ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤. ê° ê·¸ë£¹ ë‚´ì—ì„œ ì–´ëŒ‘í„°ëŠ” ê°€ì§€ì¹˜ê¸°ë˜ê³ , ìŠ¤ì¼€ì¼ë§ë˜ë©° ë³‘í•©ë˜ì–´ ê´€ë ¨ ì‘ì—… ê°„ì˜ ì „ì´ í•™ìŠµì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ì‹œê° ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìˆ˜í–‰ëœ í¬ê´„ì  ì‹¤í—˜ì€ HAMì´ ìµœì²¨ë‹¨ ë°©ë²•ì„ í¬ê²Œ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ì‘ì—… ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ë”ìš± ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì§€ì†ì  í•™ìŠµ(continual learning)ì´ í˜„ì¬ ì‹¬ì¸µí•™ìŠµ ëª¨ë¸ì— ì¤‘ìš”í•œ ë„ì „ ê³¼ì œë¥¼ ì œê³µí•˜ëŠ”ë°, íŠ¹íˆ ìƒˆë¡œìš´ ì§€ì‹ì´ ì´ì „ì— í•™ìŠµí•œ ì •ë³´ì™€ ê°„ì„­í•˜ì—¬ ì´ì „ ì§€ì‹ì„ ìŠê²Œ ë§Œë“œëŠ” ì¹˜ëª…ì ì¸ ìŠí˜€ì§(catastrophic forgetting) í˜„ìƒì— ëŒ€í•´ ë‹¤ë£¬ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Parameter-Efficient Fine-Tuning (PEFT) ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ì¸ LoRAë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ Hierarchical Adapters Merging (HAM)ì„ ì œì•ˆí•œë‹¤. HAMì€ ë‹¤ë¥¸ ì‘ì—…ì˜ ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•˜ë©°, ê´€ë ¨ ì‘ì—… ê°„ì˜ ì „ì´ í•™ìŠµì„ ìš©ì´í•˜ê²Œ í•œë‹¤. ì‹¤í—˜ ê²°ê³¼, HAMì€ ë‹¤ë¥¸ ê¸°ì¤€ì„  ë°©ë²•ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ë” ë§ì€ ì‘ì—…ì„ ê´€ë¦¬í•˜ë©° ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- ì—°ì† í•™ìŠµì€ ì¸ê°„ ì¸ì§€ì˜ í•„ìˆ˜ ëŠ¥ë ¥ì´ì§€ë§Œ í˜„ì¬ì˜ ì‹¬ì¸µ í•™ìŠµ ëª¨ë¸ì— ì¤‘ìš”í•œ ë„ì „ ê³¼ì œë¥¼ ì œê¸°í•©ë‹ˆë‹¤.

- ìƒˆë¡œìš´ ì§€ì‹ì´ ì´ì „ì— í•™ìŠµí•œ ì •ë³´ì™€ ê°„ì„­í•˜ì—¬ ì´ì „ ì§€ì‹ì„ ìŠê²Œ ë§Œë“œëŠ” ì¹˜ëª…ì ì¸ ìŠìŒ í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤.

- HAMì€ ë‹¤ë¥¸ ì‘ì—…ì—ì„œ ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ê·œëª¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™•ì¥í•˜ê³  ê²½ìŸ ê¸°ì¤€ì„ ë³´ë‹¤ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-18 16:48:34*