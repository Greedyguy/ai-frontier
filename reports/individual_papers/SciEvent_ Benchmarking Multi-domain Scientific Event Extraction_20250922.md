# SciEvent: Benchmarking Multi-domain Scientific Event Extraction

**Korean Title:** SciEvent: 다중 도메인 과학적 이벤트 추출 벤치마킹

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Multi-domain Scientific Information Extraction|Multi-domain Scientific Information Extraction]] [[keywords/specific/Event Extraction|Event Extraction]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/unique/SciEvent|SciEvent]] [[categories/cs.CL|cs.CL]] [[2025-09-19/SNaRe_ Domain-aware Data Generation for Low-Resource Event Detection_20250919|SNaRe: Domain-aware Data Generation for Low-Resource Event Detection]] (82.8% similar) [[2025-09-19/An Evaluation-Centric Paradigm for Scientific Visualization Agents_20250919|An Evaluation-Centric Paradigm for Scientific Visualization Agents]] (80.0% similar) [[2025-09-19/DiCoRe_ Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning_20250919|DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning]] (77.7% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Structured Context-aware Understanding
**🔗 Specific Connectable**: Multi-domain Benchmark
**🔬 Broad Technical**: Event Extraction, Large Language Models
**⭐ Unique Technical**: SciEvent
## 🔗 유사한 논문
- [[2025-09-19/SNaRe_ Domain-aware Data Generation for Low-Resource Event Detection_20250919|SNaRe Domain-aware Data Generation for Low-Resource Event Detection]] (82.8% similar)
- [[2025-09-19/An Evaluation-Centric Paradigm for Scientific Visualization Agents_20250919|An Evaluation-Centric Paradigm for Scientific Visualization Agents]] (80.0% similar)
- [[2025-09-19/DiCoRe_ Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning_20250919|DiCoRe Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning]] (77.7% similar)
- [[2025-09-19/SPICE_ An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation_20250919|SPICE An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation]] (77.5% similar)
- [[2025-09-17/Combining Evidence and Reasoning for Biomedical Fact-Checking_20250917|Combining Evidence and Reasoning for Biomedical Fact-Checking]] (77.2% similar)


**ArXiv ID**: [2509.15620](https://arxiv.org/abs/2509.15620)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15620.pdf)


**ArXiv ID**: [2509.15620](https://arxiv.org/abs/2509.15620)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15620.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Structured Context-aware Understanding
**🔗 Specific Connectable**: Multi-domain Benchmark
**⭐ Unique Technical**: SciEvent
**🔬 Broad Technical**: Event Extraction, Large Language Models

## 🏷️ 추출된 키워드



`Event Extraction` • 

`Large Language Models` • 

`Multi-domain Benchmark` • 

`SciEvent` • 

`Context-aware Understanding`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15620v1 Announce Type: new 
Abstract: Scientific information extraction (SciIE) has primarily relied on entity-relation extraction in narrow domains, limiting its applicability to interdisciplinary research and struggling to capture the necessary context of scientific information, often resulting in fragmented or conflicting statements. In this paper, we introduce SciEvent, a novel multi-domain benchmark of scientific abstracts annotated via a unified event extraction (EE) schema designed to enable structured and context-aware understanding of scientific content. It includes 500 abstracts across five research domains, with manual annotations of event segments, triggers, and fine-grained arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting abstracts into core scientific activities--Background, Method, Result, and Conclusion; and (2) extracting the corresponding triggers and arguments. Experiments with fine-tuned EE models, large language models (LLMs), and human annotators reveal a performance gap, with current models struggling in domains such as sociology and humanities. SciEvent serves as a challenging benchmark and a step toward generalizable, multi-domain SciIE.

## 🔍 Abstract (한글 번역)

arXiv:2509.15620v1 발표 유형: 신규  
초록: 과학 정보 추출(SciIE)은 주로 좁은 분야에서의 엔티티-관계 추출에 의존해 왔으며, 이는 학제 간 연구에의 적용을 제한하고 과학 정보의 필수적인 맥락을 포착하는 데 어려움을 겪어 종종 단편적이거나 상충되는 진술을 초래합니다. 이 논문에서는 과학 콘텐츠의 구조적이고 맥락을 고려한 이해를 가능하게 하기 위해 통합된 이벤트 추출(EE) 스키마를 통해 주석이 달린 새로운 다중 도메인 과학 초록 벤치마크인 SciEvent를 소개합니다. 이는 다섯 가지 연구 분야에 걸쳐 500개의 초록을 포함하며, 이벤트 세그먼트, 트리거 및 세밀한 논거의 수동 주석이 포함되어 있습니다. 우리는 SciIE를 다단계 EE 파이프라인으로 정의합니다: (1) 초록을 핵심 과학 활동인 배경, 방법, 결과 및 결론으로 세분화하고; (2) 해당 트리거와 논거를 추출합니다. 미세 조정된 EE 모델, 대형 언어 모델(LLM) 및 인간 주석자와의 실험은 사회학 및 인문학과 같은 분야에서 현재 모델이 어려움을 겪고 있음을 보여주는 성능 격차를 드러냅니다. SciEvent는 도전적인 벤치마크 역할을 하며, 일반화 가능한 다중 도메인 SciIE를 향한 한 걸음입니다.

## 📝 요약

이 논문은 과학 정보 추출(SciIE)의 한계를 극복하기 위해 SciEvent라는 새로운 다중 도메인 벤치마크를 제안합니다. SciEvent는 통합된 이벤트 추출(EE) 스키마를 통해 과학적 내용을 구조적이고 맥락적으로 이해할 수 있도록 설계되었습니다. 5개 연구 분야에서 500개의 초록을 수집하고, 이벤트 세그먼트, 트리거, 세부적인 인자를 수동으로 주석 처리했습니다. SciIE는 (1) 초록을 배경, 방법, 결과, 결론으로 세분화하고, (2) 해당 트리거와 인자를 추출하는 다단계 EE 파이프라인으로 정의됩니다. 실험 결과, 현재 모델들이 사회학 및 인문학 분야에서 어려움을 겪고 있으며, SciEvent는 일반화 가능한 다중 도메인 SciIE로 나아가기 위한 도전적인 벤치마크로 기능합니다.

## 🎯 주요 포인트


- 1. SciEvent는 통합된 이벤트 추출 스키마를 통해 과학적 콘텐츠의 구조적이고 맥락을 고려한 이해를 가능하게 하는 새로운 다중 도메인 벤치마크입니다.

- 2. SciEvent는 5개 연구 분야의 500개 초록을 포함하며, 이벤트 세그먼트, 트리거, 세부적인 인수에 대한 수작업 주석을 제공합니다.

- 3. SciIE를 다단계 이벤트 추출 파이프라인으로 정의하여, 초록을 핵심 과학 활동인 배경, 방법, 결과, 결론으로 세분화하고 해당 트리거와 인수를 추출합니다.

- 4. 실험 결과, 현재 모델들이 사회학 및 인문학과 같은 분야에서 어려움을 겪고 있으며, 인간 주석자와의 성능 격차가 존재함을 보여줍니다.

- 5. SciEvent는 일반화 가능한 다중 도메인 과학 정보 추출을 향한 도전적인 벤치마크로 기능합니다.


---

*Generated on 2025-09-22 16:24:41*