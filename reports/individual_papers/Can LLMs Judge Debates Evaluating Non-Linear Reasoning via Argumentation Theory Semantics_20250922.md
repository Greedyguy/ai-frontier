# Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics

**Korean Title:** LLMì´ í† ë¡ ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ”ê°€? ë…¼ì¦ ì´ë¡  ì˜ë¯¸ë¡ ì„ í†µí•œ ë¹„ì„ í˜• ì¶”ë¡  í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/specific/Chain-of-Thought|Chain-of-Thought]] [[keywords/specific/In-Context Learning|In-Context Learning]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Computational Argumentation Theory|Computational Argumentation Theory]] [[keywords/unique/Quantitative Argumentation Debate semantics|Quantitative Argumentation Debate semantics]] [[categories/cs.CL|cs.CL]] [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (85.4% similar) [[2025-09-19/CLEAR_ A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models_20250919|CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models]] (85.4% similar) [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Chain-of-Thought, In-Context Learning
**ğŸ”¬ Broad Technical**: Large Language Models, Computational Argumentation Theory
**â­ Unique Technical**: Quantitative Argumentation Debate semantics
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text]] (85.4% similar)
- [[2025-09-19/CLEAR_ A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models_20250919|CLEAR A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models]] (85.4% similar)
- [[2025-09-19/Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision_20250919|Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision]] (85.3% similar)
- [[2025-09-19/Evaluation and Facilitation of Online Discussions in the LLM Era_ A Survey_20250919|Evaluation and Facilitation of Online Discussions in the LLM Era A Survey]] (84.9% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (84.9% similar)


**ArXiv ID**: [2509.15739](https://arxiv.org/abs/2509.15739)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15739.pdf)


**ArXiv ID**: [2509.15739](https://arxiv.org/abs/2509.15739)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15739.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: In-Context Learning, Chain-of-Thought
**â­ Unique Technical**: Quantitative Argumentation Debate semantics
**ğŸ”¬ Broad Technical**: Large Language Models, Computational Argumentation Theory

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Computational Argumentation Theory` â€¢ 

`Chain-of-Thought` â€¢ 

`In-Context Learning` â€¢ 

`Quantitative Argumentation Debate semantics`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15739v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at linear reasoning tasks but remain underexplored on non-linear structures such as those found in natural debates, which are best expressed as argument graphs. We evaluate whether LLMs can approximate structured reasoning from Computational Argumentation Theory (CAT). Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which assigns acceptability scores to arguments based on their attack and support relations. Given only dialogue-formatted debates from two NoDE datasets, models are prompted to rank arguments without access to the underlying graph. We test several LLMs under advanced instruction strategies, including Chain-of-Thought and In-Context Learning. While models show moderate alignment with QuAD rankings, performance degrades with longer inputs or disrupted discourse flow. Advanced prompting helps mitigate these effects by reducing biases related to argument length and position. Our findings highlight both the promise and limitations of LLMs in modeling formal argumentation semantics and motivate future work on graph-aware reasoning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15739v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì„ í˜• ì¶”ë¡  ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìì—° í† ë¡ ì—ì„œ ë°œê²¬ë˜ëŠ” ë¹„ì„ í˜• êµ¬ì¡°ì— ëŒ€í•´ì„œëŠ” ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¹„ì„ í˜• êµ¬ì¡°ëŠ” ì£¼ë¡œ ë…¼ì¦ ê·¸ë˜í”„ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMsê°€ ê³„ì‚° ë…¼ì¦ ì´ë¡ (CAT)ì—ì„œì˜ êµ¬ì¡°ì  ì¶”ë¡ ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê³µê²© ë° ì§€ì› ê´€ê³„ì— ê¸°ë°˜í•˜ì—¬ ë…¼ì¦ì— ìˆ˜ìš© ê°€ëŠ¥ì„± ì ìˆ˜ë¥¼ í• ë‹¹í•˜ëŠ” ì •ëŸ‰ì  ë…¼ì¦ í† ë¡ (QuAD) ì˜ë¯¸ë¡ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‘ ê°œì˜ NoDE ë°ì´í„°ì…‹ì—ì„œ ëŒ€í™” í˜•ì‹ì˜ í† ë¡ ë§Œì„ ì œê³µë°›ì€ ìƒíƒœì—ì„œ, ëª¨ë¸ë“¤ì€ ê¸°ë³¸ ê·¸ë˜í”„ì— ì ‘ê·¼í•˜ì§€ ì•Šê³  ë…¼ì¦ì„ ìˆœìœ„ ë§¤ê¸°ë„ë¡ ìœ ë„ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—°ì‡„ì  ì‚¬ê³ (Chain-of-Thought)ì™€ ë§¥ë½ ë‚´ í•™ìŠµ(In-Context Learning)ì„ í¬í•¨í•œ ê³ ê¸‰ ì§€ì‹œ ì „ëµ í•˜ì— ì—¬ëŸ¬ LLMsë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸ë“¤ì€ QuAD ìˆœìœ„ì™€ ì¤‘ê°„ ì •ë„ì˜ ì¼ì¹˜ë¥¼ ë³´ì´ì§€ë§Œ, ì…ë ¥ì´ ê¸¸ì–´ì§€ê±°ë‚˜ ë‹´ë¡  íë¦„ì´ ë°©í•´ë°›ì„ ê²½ìš° ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ëŠ” ë…¼ì¦ ê¸¸ì´ì™€ ìœ„ì¹˜ì— ê´€ë ¨ëœ í¸í–¥ì„ ì¤„ì„ìœ¼ë¡œì¨ ì´ëŸ¬í•œ íš¨ê³¼ë¥¼ ì™„í™”í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” LLMsê°€ í˜•ì‹ì  ë…¼ì¦ ì˜ë¯¸ë¡ ì„ ëª¨ë¸ë§í•˜ëŠ” ë° ìˆì–´ì„œì˜ ê°€ëŠ¥ì„±ê³¼ í•œê³„ë¥¼ ëª¨ë‘ ê°•ì¡°í•˜ë©°, ê·¸ë˜í”„ ì¸ì‹ ì¶”ë¡ ì— ëŒ€í•œ í–¥í›„ ì—°êµ¬ë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìì—° ë…¼ìŸì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. íŠ¹íˆ, Computational Argumentation Theory(CAT)ì˜ êµ¬ì¡°ì  ì¶”ë¡ ì„ LLMì´ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤. ì—°êµ¬ëŠ” QuAD ì˜ë¯¸ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ë…¼ìŸì˜ ê³µê²© ë° ì§€ì› ê´€ê³„ì— ë”°ë¼ ìˆ˜ìš© ê°€ëŠ¥ì„± ì ìˆ˜ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤. ë‘ ê°œì˜ NoDE ë°ì´í„°ì…‹ì—ì„œ ëŒ€í™” í˜•ì‹ì˜ ë…¼ìŸë§Œì„ ì‚¬ìš©í•˜ì—¬, ëª¨ë¸ì´ ê¸°ë³¸ ê·¸ë˜í”„ ì—†ì´ë„ ë…¼ìŸì„ ìˆœìœ„í™”í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ LLMì„ ì²´ì¸ ì˜¤ë¸Œ ìƒê°(Chain-of-Thought) ë° ì¸ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ(In-Context Learning)ê³¼ ê°™ì€ ê³ ê¸‰ ì§€ì‹œ ì „ëµ í•˜ì— í‰ê°€í•œ ê²°ê³¼, ëª¨ë¸ì€ QuAD ìˆœìœ„ì™€ ì¤‘ê°„ ì •ë„ì˜ ì¼ì¹˜ë¥¼ ë³´ì˜€ìœ¼ë‚˜, ì…ë ¥ì´ ê¸¸ì–´ì§€ê±°ë‚˜ ë‹´ë¡  íë¦„ì´ ë°©í•´ë°›ì„ ë•Œ ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ëŠ” ì´ëŸ¬í•œ íš¨ê³¼ë¥¼ ì™„í™”í•˜ì—¬ ë…¼ìŸ ê¸¸ì´ì™€ ìœ„ì¹˜ì— ë”°ë¥¸ í¸í–¥ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤. ì—°êµ¬ëŠ” LLMì´ í˜•ì‹ì  ë…¼ì¦ ì˜ë¯¸ë¡ ì„ ëª¨ë¸ë§í•˜ëŠ” ë° ìˆì–´ ê°€ëŠ¥ì„±ê³¼ í•œê³„ë¥¼ ë™ì‹œì— ë³´ì—¬ì£¼ë©°, ê·¸ë˜í”„ ì¸ì‹ ì¶”ë¡ ì— ëŒ€í•œ í–¥í›„ ì—°êµ¬ì˜ í•„ìš”ì„±ì„ ì œê¸°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ì„ í˜• ì¶”ë¡  ì‘ì—…ì— ë›°ì–´ë‚˜ì§€ë§Œ ìì—° í† ë¡ ê³¼ ê°™ì€ ë¹„ì„ í˜• êµ¬ì¡°ì—ì„œëŠ” ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ë‹¤.

- 2. ë³¸ ì—°êµ¬ëŠ” LLMsê°€ ê³„ì‚°ì  ë…¼ì¦ ì´ë¡ (CAT)ì˜ êµ¬ì¡°ì  ì¶”ë¡ ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•œë‹¤.

- 3. QuAD ì˜ë¯¸ë¡ ì„ ì‚¬ìš©í•˜ì—¬ LLMsê°€ ê·¸ë˜í”„ ì—†ì´ ëŒ€í™” í˜•ì‹ì˜ í† ë¡ ì—ì„œ ë…¼ì¦ì„ ìˆœìœ„í™”í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ì˜€ë‹¤.

- 4. ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ë…¼ì¦ ê¸¸ì´ì™€ ìœ„ì¹˜ì— ë”°ë¥¸ í¸í–¥ì„ ì¤„ì„ìœ¼ë¡œì¨ ì„±ëŠ¥ ì €í•˜ë¥¼ ì™„í™”í•  ìˆ˜ ìˆì—ˆë‹¤.

- 5. ì—°êµ¬ ê²°ê³¼ëŠ” LLMsê°€ í˜•ì‹ì  ë…¼ì¦ ì˜ë¯¸ë¡ ì„ ëª¨ë¸ë§í•˜ëŠ” ë° ìˆì–´ ê°€ëŠ¥ì„±ê³¼ í•œê³„ë¥¼ ëª¨ë‘ ë³´ì—¬ì£¼ë©°, ê·¸ë˜í”„ ì¸ì‹ ì¶”ë¡ ì— ëŒ€í•œ í–¥í›„ ì—°êµ¬ì˜ í•„ìš”ì„±ì„ ì œê¸°í•œë‹¤.


---

*Generated on 2025-09-22 16:26:58*