# Negotiated Representations to Prevent Overfitting in Machine Learning Applications

**Korean Title:** ê¸°ê³„ í•™ìŠµ ì‘ìš©ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ í˜‘ìƒëœ í‘œí˜„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Continual Learning|Continual Learning]] [[keywords/specific/Overfitting Prevention|Overfitting Prevention]] [[keywords/broad/Machine Learning|Machine Learning]] [[keywords/broad/Classification|Classification]] [[keywords/unique/Negotiation Paradigm|Negotiation Paradigm]] [[categories/cs.LG|cs.LG]] [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (83.8% similar) [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.8% similar) [[2025-09-19/Superpose Task-specific Features for Model Merging_20250919|Superpose Task-specific Features for Model Merging]] (82.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Continual Learning
**ğŸ”— Specific Connectable**: Overfitting Prevention
**ğŸ”¬ Broad Technical**: Machine Learning
**â­ Unique Technical**: Negotiation Paradigm
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (83.8% similar)
- [[2025-09-18/Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (82.8% similar)
- [[2025-09-19/Superpose Task-specific Features for Model Merging_20250919|Superpose Task-specific Features for Model Merging]] (82.4% similar)
- [[2025-09-19/Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning_20250919|Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning]] (81.2% similar)
- [[2025-09-22/Latent learning_ episodic memory complements parametric learning by enabling flexible reuse of experiences_20250922|Latent learning episodic memory complements parametric learning by enabling flexible reuse of experiences]] (81.0% similar)


**ArXiv ID**: [2311.11410](https://arxiv.org/abs/2311.11410)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2311.11410.pdf)


**ArXiv ID**: [2311.11410](https://arxiv.org/abs/2311.11410)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2311.11410.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Continual Learning
**ğŸ”— Specific Connectable**: Overfitting Prevention
**â­ Unique Technical**: Negotiation Paradigm
**ğŸ”¬ Broad Technical**: Machine Learning, Classification

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Machine Learning` â€¢ 

`Classification` â€¢ 

`Overfitting Prevention` â€¢ 

`Negotiation Paradigm` â€¢ 

`Continual Learning`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2311.11410v2 Announce Type: replace 
Abstract: Overfitting is a phenomenon that occurs when a machine learning model is trained for too long and focused too much on the exact fitness of the training samples to the provided training labels and cannot keep track of the predictive rules that would be useful on the test data. This phenomenon is commonly attributed to memorization of particular samples, memorization of the noise, and forced fitness into a data set of limited samples by using a high number of neurons. While it is true that the model encodes various peculiarities as the training process continues, we argue that most of the overfitting occurs in the process of reconciling sharply defined membership ratios. In this study, we present an approach that increases the classification accuracy of machine learning models by allowing the model to negotiate output representations of the samples with previously determined class labels. By setting up a negotiation between the models interpretation of the inputs and the provided labels, we not only increased average classification accuracy but also decreased the rate of overfitting without applying any other regularization tricks. By implementing our negotiation paradigm approach to several low regime machine learning problems by generating overfitting scenarios from publicly available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated that the proposed paradigm has more capacity than its intended purpose. We are sharing the experimental results and inviting the machine learning community to explore the limits of the proposed paradigm. We also aim to incentive the community to exploit the negotiation paradigm to overcome the learning related challenges in other research fields such as continual learning. The Python code of the experimental setup is uploaded to GitHub.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2311.11410v2 ë°œí‘œ ìœ í˜•: êµì²´  
ì´ˆë¡: ê³¼ì í•©ì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì´ ë„ˆë¬´ ì˜¤ë˜ í›ˆë ¨ë˜ì–´ ì œê³µëœ í›ˆë ¨ ë ˆì´ë¸”ì— í›ˆë ¨ ìƒ˜í”Œì˜ ì •í™•í•œ ì í•©ì„±ì— ë„ˆë¬´ ì§‘ì¤‘í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ìœ ìš©í•œ ì˜ˆì¸¡ ê·œì¹™ì„ ì¶”ì í•  ìˆ˜ ì—†ì„ ë•Œ ë°œìƒí•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ì´ í˜„ìƒì€ ì¼ë°˜ì ìœ¼ë¡œ íŠ¹ì • ìƒ˜í”Œì˜ ì•”ê¸°, ë…¸ì´ì¦ˆì˜ ì•”ê¸°, ë§ì€ ìˆ˜ì˜ ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ì—¬ ì œí•œëœ ìƒ˜í”Œì˜ ë°ì´í„° ì„¸íŠ¸ì— ê°•ì œë¡œ ì í•©ì‹œí‚¤ëŠ” ê²ƒì— ê¸°ì¸í•©ë‹ˆë‹¤. í›ˆë ¨ ê³¼ì •ì´ ê³„ì†ë¨ì— ë”°ë¼ ëª¨ë¸ì´ ë‹¤ì–‘í•œ íŠ¹ì´ì ì„ ì¸ì½”ë”©í•œë‹¤ëŠ” ê²ƒì€ ì‚¬ì‹¤ì´ì§€ë§Œ, ìš°ë¦¬ëŠ” ëŒ€ë¶€ë¶„ì˜ ê³¼ì í•©ì´ ëª…í™•í•˜ê²Œ ì •ì˜ëœ ë©¤ë²„ì‹­ ë¹„ìœ¨ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ëª¨ë¸ì´ ì´ì „ì— ê²°ì •ëœ í´ë˜ìŠ¤ ë ˆì´ë¸”ê³¼ ìƒ˜í”Œì˜ ì¶œë ¥ í‘œí˜„ì„ í˜‘ìƒí•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ë¶„ë¥˜ ì •í™•ì„±ì„ ë†’ì´ëŠ” ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì…ë ¥ì— ëŒ€í•œ ëª¨ë¸ì˜ í•´ì„ê³¼ ì œê³µëœ ë ˆì´ë¸” ê°„ì˜ í˜‘ìƒì„ ì„¤ì •í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” í‰ê·  ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì˜€ì„ ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì •ê·œí™” ê¸°ë²•ì„ ì ìš©í•˜ì§€ ì•Šê³ ë„ ê³¼ì í•© ë¹„ìœ¨ì„ ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤. CIFAR 10, CIFAR 100, MNISTì™€ ê°™ì€ ê³µê°œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê³¼ì í•© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ì—¬ ì—¬ëŸ¬ ì €ë ˆì§ ê¸°ê³„ í•™ìŠµ ë¬¸ì œì— ìš°ë¦¬ì˜ í˜‘ìƒ íŒ¨ëŸ¬ë‹¤ì„ ì ‘ê·¼ë²•ì„ êµ¬í˜„í•¨ìœ¼ë¡œì¨, ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì´ ì˜ë„ëœ ëª©ì  ì´ìƒìœ¼ë¡œ ë” ë§ì€ ì—­ëŸ‰ì„ ê°€ì§€ê³  ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ê³µìœ í•˜ê³  ê¸°ê³„ í•™ìŠµ ì»¤ë®¤ë‹ˆí‹°ê°€ ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì˜ í•œê³„ë¥¼ íƒêµ¬í•˜ë„ë¡ ì´ˆëŒ€í•©ë‹ˆë‹¤. ë˜í•œ ì§€ì† í•™ìŠµê³¼ ê°™ì€ ë‹¤ë¥¸ ì—°êµ¬ ë¶„ì•¼ì˜ í•™ìŠµ ê´€ë ¨ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ í˜‘ìƒ íŒ¨ëŸ¬ë‹¤ì„ì„ í™œìš©í•˜ë„ë¡ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë…ë ¤í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ì„¤ì •ì˜ Python ì½”ë“œëŠ” GitHubì— ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê³¼ì í•©ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì§€ë‚˜ì¹˜ê²Œ ë§ì¶°ì ¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ìœ ìš©í•œ ì˜ˆì¸¡ ê·œì¹™ì„ ë†“ì¹˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ì—°êµ¬ì§„ì€ ëª¨ë¸ì´ ì…ë ¥ê³¼ ì£¼ì–´ì§„ ë ˆì´ë¸” ê°„ì˜ 'í˜‘ìƒ'ì„ í†µí•´ ì¶œë ¥ í‘œí˜„ì„ ì¡°ì •í•˜ë„ë¡ í•¨ìœ¼ë¡œì¨, í‰ê·  ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ê³  ê³¼ì í•©ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ CIFAR 10, CIFAR 100, MNIST ë“± ê³µê°œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì‹¤í—˜ì—ì„œ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìœ¼ë©°, ë‹¤ë¥¸ ì—°êµ¬ ë¶„ì•¼ì—ì„œë„ í™œìš© ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹¤í—˜ ì½”ë“œëŠ” GitHubì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ê³¼ì í•©ì€ ëª¨ë¸ì´ í›ˆë ¨ ìƒ˜í”Œì— ì§€ë‚˜ì¹˜ê²Œ ì§‘ì¤‘í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ìœ ìš©í•œ ì˜ˆì¸¡ ê·œì¹™ì„ ìœ ì§€í•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.

- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ëª¨ë¸ì´ ì…ë ¥ í•´ì„ê³¼ ì œê³µëœ ë ˆì´ë¸” ê°„ì˜ í˜‘ìƒì„ í†µí•´ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

- 3. í˜‘ìƒ íŒ¨ëŸ¬ë‹¤ì„ì„ ì ìš©í•˜ì—¬ í‰ê·  ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë†’ì´ê³  ê³¼ì í•©ë¥ ì„ ì¤„ì´ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤.

- 4. ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì€ CIFAR 10, CIFAR 100, MNISTì™€ ê°™ì€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê³¼ì í•© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ì—¬ ë” í° ì ì¬ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

- 5. ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ê°€ ì œì•ˆëœ íŒ¨ëŸ¬ë‹¤ì„ì„ íƒêµ¬í•˜ê³  ë‹¤ë¥¸ ì—°êµ¬ ë¶„ì•¼ì˜ í•™ìŠµ ê´€ë ¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° í™œìš©í•˜ë„ë¡ ì¥ë ¤í•˜ê³ ì í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-22 15:49:34*