
# Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering

**Korean Title:** 오미니-CLST: 오디오 질문 응답을 위한 오류 인식 교육 계획 학습 및 지도된 선택적 사고 체인과 함께.

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Multimodal Audio-Language Understanding|Multimodal Audio-Language Understanding]] [[keywords/broad/Curriculum Learning|Curriculum Learning]] [[keywords/broad/Audio Question Answering|Audio Question Answering]] [[keywords/specific/Selective Chain-of-Thought|Selective Chain-of-Thought]] [[keywords/unique/Omni-CLST|Omni-CLST]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Multimodal Audio-Language Understanding
**🔬 Broad Technical**: Curriculum Learning, Audio Question Answering
**🔗 Specific Connectable**: Selective Chain-of-Thought
**⭐ Unique Technical**: Omni-CLST

**ArXiv ID**: [2509.12275](https://arxiv.org/abs/2509.12275)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.12275.pdf)


## 🏷️ 추출된 키워드



`Curriculum Learning` • 

`Audio Question Answering` • 

`Selective Chain-of-Thought` • 

`Omni-CLST` • 

`Multimodal Audio-Language Understanding`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.12275v2 Announce Type: replace-cross 
Abstract: With the rapid progress of large audio-language models (LALMs), audio question answering (AQA) has emerged as a challenging task requiring both fine-grained audio understanding and complex reasoning. While current methods mainly rely on constructing new datasets via captioning or reasoning traces, existing high-quality AQA data remains underutilized. To address this, we propose Omni-CLST, an error-aware Curriculum Learning framework with guided Selective Chain-of-Thought. The framework efficiently leverages existing high-quality dataset through two key strategies: an error-aware curriculum that organizes samples by difficulty, and a guided thought dropout mechanism that focuses reasoning on challenging cases. Experiments show that Omni-CLST achieves 73.80% on MMAU-mini and a new state of the art of 64.30% on MMAR, demonstrating robust generalization in multimodal audio-language understanding.

## 🔍 Abstract (한글 번역)

arXiv:2509.12275v2 발표 유형: replace-cross
요약: 대규모 오디오-언어 모델(LALMs)의 신속한 발전으로 오디오 질문 응답(AQA)가 섬세한 오디오 이해와 복잡한 추론이 필요한 어려운 작업으로 등장했습니다. 현재 방법들은 주로 자막 또는 추론 트레이스를 통해 새로운 데이터셋을 구축하는 데 의존하고 있지만, 기존의 고품질 AQA 데이터는 여전히 충분히 활용되지 않고 있습니다. 이를 해결하기 위해 우리는 Omni-CLST를 제안합니다. 이는 오류를 인식하는 커리큘럼 학습 프레임워크와 유도된 선택적 사고 체인을 가진 프레임워크입니다. 이 프레임워크는 두 가지 주요 전략을 통해 기존의 고품질 데이터셋을 효율적으로 활용합니다: 난이도에 따라 샘플을 구성하는 오류를 인식하는 커리큘럼 및 어려운 경우에 추론을 집중시키는 유도된 사고 드롭아웃 메커니즘. 실험 결과, Omni-CLST는 MMAU-mini에서 73.80%를 달성하고 MMAR에서 64.30%의 새로운 최고 성능을 보여주며, 다중 모달 오디오-언어 이해에서 견고한 일반화를 보여줍니다.

## 📝 요약

최근 대규모 음성-언어 모델(LALMs)의 급속한 발전으로 음성 질문 응답(AQA)이 등장하며 세밀한 음성 이해와 복잡한 추론이 필요한 어려운 작업으로 부상했다. 기존 방법은 주로 자막 또는 추론 트레이스를 통해 새로운 데이터셋을 구축하는 데 의존하지만, 기존의 고품질 AQA 데이터는 미활용된 채 남아있다. 이를 해결하기 위해 우리는 Omni-CLST를 제안한다. 이는 오류 인식형 커리큘럼 학습 프레임워크와 유도된 선택적 사고 연쇄를 결합한 것이다. 실험 결과, Omni-CLST는 MMAU-mini에서 73.80%를 달성하고 MMAR에서 64.30%의 새로운 최고 성적을 기록하여 다중 모달 음성-언어 이해에서 강건한 일반화 능력을 보여주었다.

## 🎯 주요 포인트


- 대규모 오디오-언어 모델의 빠른 발전으로 오디오 질문 응답(AQA)이 어려운 작업으로 부상하고 있다.

- Omni-CLST는 오디오 이해와 복잡한 추론이 필요한 AQA 작업에 대한 새로운 프레임워크로, 고품질 데이터를 효율적으로 활용한다.

- 오류 인식 커리큘럼과 안내된 사고 중단 메커니즘을 통해 어려운 케이스에 집중하여 이 프레임워크는 뛰어난 성능을 보여준다.


---

*Generated on 2025-09-18 16:36:25*