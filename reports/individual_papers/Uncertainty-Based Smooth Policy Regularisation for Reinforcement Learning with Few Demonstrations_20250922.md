# Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations

**Korean Title:** ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ì˜ ë§¤ë„ëŸ¬ìš´ ì •ì±… ì •ê·œí™”: ì†Œìˆ˜ì˜ ì‹œì—°ì„ í†µí•œ ê°•í™” í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Uncertainty-Based Regularisation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (82.7% similar)
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (82.0% similar)
- [[2025-09-19/Sample Efficient Experience Replay in Non-stationary Environments_20250919|Sample Efficient Experience Replay in Non-stationary Environments]] (81.9% similar)
- [[2025-09-19/LEED_ A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning_20250919|LEED A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning]] (81.8% similar)
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (81.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15981v1 Announce Type: cross 
Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15981v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: í¬ì†Œ ë³´ìƒì„ ì‚¬ìš©í•˜ëŠ” ê°•í™” í•™ìŠµì—ì„œ ì‹œë²”ì€ í•™ìŠµì„ ê°€ì†í™”í•  ìˆ˜ ìˆì§€ë§Œ, ì–¸ì œ ì´ë¥¼ ëª¨ë°©í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ë„ì „ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—ì´ì „íŠ¸ê°€ ì‹œë²”ì„ ëª¨ë°©í•´ì•¼ í•  ë•Œì™€ ìì‹ ì˜ ì •ì±…ì„ ë”°ë¼ì•¼ í•  ë•Œë¥¼ ê²°ì •í•˜ëŠ” ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì„ í•´ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì¸ ì‹œë²”ìœ¼ë¡œë¶€í„°ì˜ ë¶€ë“œëŸ¬ìš´ ì •ì±… ì •ê·œí™”(SPReD)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. SPReDëŠ” ì•™ìƒë¸” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë²” ë° ì •ì±… í–‰ë™ì— ëŒ€í•œ Q-ê°’ ë¶„í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ë¹„êµì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ìƒí˜¸ ë³´ì™„ì ì¸ ë¶ˆí™•ì‹¤ì„± ì¸ì‹ ë°©ë²•ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤: ì‹œë²”ì˜ ìš°ì›”ì„± ê°€ëŠ¥ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì ‘ê·¼ë²•ê³¼ í†µê³„ì  ìœ ì˜ì„±ì— ë”°ë¼ ëª¨ë°©ì„ ì¡°ì •í•˜ëŠ” ì´ì  ê¸°ë°˜ ì ‘ê·¼ë²•. ì´ì§„ ëª¨ë°© ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê¸°ì¡´ ë°©ë²•(ì˜ˆ: Q-í•„í„°)ê³¼ ë‹¬ë¦¬, SPReDëŠ” ì—°ì†ì ì´ê³  ë¶ˆí™•ì‹¤ì„± ë¹„ë¡€ ì •ê·œí™” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤. ê³„ì‚°ì˜ ë‹¨ìˆœí•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , SPReDëŠ” 8ê°œì˜ ë¡œë´‡ ì‘ì—…ì—ì„œ ê¸°ì¡´ ì ‘ê·¼ë²•ì„ ìµœëŒ€ 14ë°° ëŠ¥ê°€í•˜ë©° ì‹œë²”ì˜ í’ˆì§ˆê³¼ ì–‘ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë†€ë¼ìš´ ì„±ê³¼ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/YujieZhu7/SPReDì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í¬ì†Œ ë³´ìƒ í™˜ê²½ì—ì„œ ê°•í™” í•™ìŠµì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ ì œì•ˆëœ SPReD(Smooth Policy Regularisation from Demonstrations)ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. SPReDëŠ” ì—ì´ì „íŠ¸ê°€ ì–¸ì œ ì‹œì—°ì„ ëª¨ë°©í•˜ê³  ì–¸ì œ ìì²´ ì •ì±…ì„ ë”°ë¥¼ì§€ ê²°ì •í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ Q-value ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ì‹œì—°ê³¼ ì •ì±… í–‰ë™ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•˜ê³ , ì‹œì—°ì˜ ìš°ì›”ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì ‘ê·¼ê³¼ í†µê³„ì  ìœ ì˜ì„±ì— ê¸°ë°˜í•œ ì´ì  ê¸°ë°˜ ì ‘ê·¼ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì´ì§„ ëª¨ë°© ê²°ì • ë°©ì‹ê³¼ ë‹¬ë¦¬, SPReDëŠ” ì—°ì†ì ì´ê³  ë¶ˆí™•ì‹¤ì„±ì— ë¹„ë¡€í•˜ëŠ” ì •ê·œí™” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤. SPReDëŠ” 8ê°œì˜ ë¡œë´‡ ê³¼ì œ ì‹¤í—˜ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 14ë°° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‹œì—°ì˜ ì§ˆê³¼ ì–‘ì— ëŒ€í•œ ê°•ì¸ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SPReDëŠ” ê°•í™” í•™ìŠµì—ì„œ ì—ì´ì „íŠ¸ê°€ ì‹œì—°ì„ ëª¨ë°©í•  ì‹œì ì„ ê²°ì •í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 2. SPReDëŠ” ì‹œì—° ë° ì •ì±… í–‰ë™ì— ëŒ€í•œ Q-ê°’ ë¶„í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.

- 3. SPReDëŠ” ì‹œì—°ì˜ ìš°ì›”ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì ‘ê·¼ë²•ê³¼ í†µê³„ì  ìœ ì˜ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë°©ì„ ì¡°ì •í•˜ëŠ” ì¥ì  ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤.

- 4. SPReDëŠ” ê¸°ì¡´ì˜ ì´ì§„ ëª¨ë°© ê²°ì • ë°©ì‹ê³¼ ë‹¬ë¦¬ ì—°ì†ì ì´ê³  ë¶ˆí™•ì‹¤ì„±ì— ë¹„ë¡€í•œ ì •ê·œí™” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤.

- 5. SPReDëŠ” 8ê°œì˜ ë¡œë´‡ ê³¼ì œ ì‹¤í—˜ì—ì„œ ìµœëŒ€ 14ë°°ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•˜ë©° ì‹œì—°ì˜ í’ˆì§ˆê³¼ ì–‘ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:20:55*