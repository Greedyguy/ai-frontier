# DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning

**Korean Title:** DiEP: ë¯¸ë¶„ ê°€ëŠ¥í•œ ì „ë¬¸ê°€ ê°€ì§€ì¹˜ê¸°ë¥¼ í†µí•œ ì ì‘í˜• ì „ë¬¸ê°€ í˜¼í•© ì••ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/specific/Mixture of Experts|Mixture of Experts]] [[keywords/specific/Gradient Based Pruning|Gradient Based Pruning]] [[keywords/broad/Natural Language Processing|Natural Language Processing]] [[keywords/unique/Differentiable Expert Pruning|Differentiable Expert Pruning]] [[categories/cs.CL|cs.CL]] [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (85.0% similar) [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (84.6% similar) [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (83.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Mixture of Experts, Gradient Based Pruning
**ğŸ”¬ Broad Technical**: Natural Language Processing
**â­ Unique Technical**: Differentiable Expert Pruning
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (85.0% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (84.6% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (83.7% similar)
- [[2025-09-19/Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation_20250919|Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation]] (82.5% similar)
- [[2025-09-17/Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection_20250917|Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection]] (82.1% similar)


**ArXiv ID**: [2509.16105](https://arxiv.org/abs/2509.16105)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.16105.pdf)


**ArXiv ID**: [2509.16105](https://arxiv.org/abs/2509.16105)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.16105.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Mixture of Experts, Gradient Based Pruning
**â­ Unique Technical**: Differentiable Expert Pruning
**ğŸ”¬ Broad Technical**: Natural Language Processing

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Natural Language Processing` â€¢ 

`Mixture of Experts` â€¢ 

`Pruning Strategy` â€¢ 

`Differentiable Expert Pruning`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.16105v1 Announce Type: new 
Abstract: Despite the significant breakthrough of Mixture-of-Experts (MoE), the increasing scale of these MoE models presents huge memory and storage challenges. Existing MoE pruning methods, which involve reducing parameter size with a uniform sparsity across all layers, often lead to suboptimal outcomes and performance degradation due to varying expert redundancy in different MoE layers. To address this, we propose a non-uniform pruning strategy, dubbed \textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which adaptively adjusts pruning rates at the layer level while jointly learning inter-layer importance, effectively capturing the varying redundancy across different MoE layers. By transforming the global discrete search space into a continuous one, our method handles exponentially growing non-uniform expert combinations, enabling adaptive gradient-based pruning. Extensive experiments on five advanced MoE models demonstrate the efficacy of our method across various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original performance on Mixtral 8$\times$7B with only half the experts, outperforming other pruning methods by up to 7.1\% on the challenging MMLU dataset.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.16105v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ì „ë¬¸ê°€ í˜¼í•©(Mixture-of-Experts, MoE)ì˜ ìƒë‹¹í•œ ëŒíŒŒêµ¬ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ MoE ëª¨ë¸ì˜ ê·œëª¨ê°€ ì»¤ì§ì— ë”°ë¼ ë©”ëª¨ë¦¬ ë° ì €ì¥ì†Œ ë¬¸ì œì— ì§ë©´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì¸µì— ê±¸ì³ ê· ì¼í•œ í¬ì†Œì„±ì„ ì ìš©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê¸°ì¡´ì˜ MoE ê°€ì§€ì¹˜ê¸° ë°©ë²•ì€ ë‹¤ì–‘í•œ MoE ì¸µì—ì„œ ì „ë¬¸ê°€ì˜ ì¤‘ë³µì„±ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ìµœì ì´ ì•„ë‹Œ ê²°ê³¼ì™€ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” \textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP})ì´ë¼ëŠ” ë¹„ê· ì¼ ê°€ì§€ì¹˜ê¸° ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì „ëµì€ ì¸µ ìˆ˜ì¤€ì—ì„œ ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•˜ë©´ì„œ ì¸µê°„ ì¤‘ìš”ì„±ì„ ê³µë™ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ë‹¤ì–‘í•œ MoE ì¸µì—ì„œì˜ ì¤‘ë³µì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. ì „ì—­ ì´ì‚° íƒìƒ‰ ê³µê°„ì„ ì—°ì†ì ì¸ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ì˜ ë°©ë²•ì€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë¹„ê· ì¼ ì „ë¬¸ê°€ ì¡°í•©ì„ ì²˜ë¦¬í•˜ì—¬ ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ê°€ì§€ì¹˜ê¸°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ì„¯ ê°€ì§€ ì²¨ë‹¨ MoE ëª¨ë¸ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì—ì„œ ìš°ë¦¬ì˜ ë°©ë²•ì˜ íš¨ëŠ¥ì„ ì…ì¦í•©ë‹ˆë‹¤. íŠ¹íˆ, \textbf{DiEP}ëŠ” Mixtral 8$\times$7Bì—ì„œ ì „ë¬¸ê°€ ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ë©´ì„œë„ ì›ë˜ ì„±ëŠ¥ì˜ ì•½ 92\%ë¥¼ ìœ ì§€í•˜ë©°, ë„ì „ì ì¸ MMLU ë°ì´í„°ì…‹ì—ì„œ ë‹¤ë¥¸ ê°€ì§€ì¹˜ê¸° ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 7.1\% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Mixture-of-Experts (MoE) ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë° ì €ì¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¹„ê· ì¼ ê°€ì§€ì¹˜ê¸° ì „ëµì¸ DiEP(Differentiable Expert Pruning)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ MoE ê°€ì§€ì¹˜ê¸° ë°©ë²•ì€ ëª¨ë“  ì¸µì— ê· ì¼í•œ í¬ì†Œì„±ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í–ˆìœ¼ë‚˜, DiEPëŠ” ì¸µë³„ë¡œ ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨ì„ ì¡°ì •í•˜ê³  ì¸µ ê°„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ì¤‘ë³µì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì „ì—­ ì´ì‚° íƒìƒ‰ ê³µê°„ì„ ì—°ì† ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ì‘í˜• ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ê°€ì§€ì¹˜ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DiEPëŠ” Mixtral 8Ã—7B ëª¨ë¸ì—ì„œ ì ˆë°˜ì˜ ì „ë¬¸ê°€ë§Œìœ¼ë¡œë„ ì›ë˜ ì„±ëŠ¥ì˜ ì•½ 92%ë¥¼ ìœ ì§€í•˜ë©°, MMLU ë°ì´í„°ì…‹ì—ì„œ ë‹¤ë¥¸ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 7.1% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Mixture-of-Experts(MoE) ëª¨ë¸ì˜ í™•ì¥ìœ¼ë¡œ ì¸í•´ ë©”ëª¨ë¦¬ ë° ì €ì¥ ë¬¸ì œ ë°œìƒ.

- 2. ê¸°ì¡´ MoE ê°€ì§€ì¹˜ê¸° ë°©ë²•ì€ ëª¨ë“  ì¸µì— ê· ì¼í•œ í¬ì†Œì„±ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œë¥¼ ì•¼ê¸°.

- 3. DiEPëŠ” ì¸µë³„ë¡œ ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨ì„ ì¡°ì •í•˜ì—¬ MoE ì¸µ ê°„ì˜ ì¤‘ë³µì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©.

- 4. DiEPëŠ” ì „ì—­ ì´ì‚° íƒìƒ‰ ê³µê°„ì„ ì—°ì† ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ì‘í˜• ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ê°€ì§€ì¹˜ê¸°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨.

- 5. DiEPëŠ” Mixtral 8Ã—7B ëª¨ë¸ì—ì„œ ì „ë¬¸ê°€ ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ë©´ì„œë„ ì›ë˜ ì„±ëŠ¥ì˜ ì•½ 92%ë¥¼ ìœ ì§€í•˜ë©°, MMLU ë°ì´í„°ì…‹ì—ì„œ ìµœëŒ€ 7.1% ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±.


---

*Generated on 2025-09-22 16:29:03*