# Predicting Language Models' Success at Zero-Shot Probabilistic Prediction

**Korean Title:** ì–¸ì–´ ëª¨ë¸ì˜ ì œë¡œìƒ· í™•ë¥  ì˜ˆì¸¡ ì„±ê³µ ì˜ˆì¸¡

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Zero-shot Probabilistic Prediction

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox A practical guide to getting the most out of human ratings]] (86.8% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (85.3% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM Language of Browsing]] (85.0% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text]] (84.8% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (84.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15356v1 Announce Type: new 
Abstract: Recent work has investigated the capabilities of large language models (LLMs) as zero-shot models for generating individual-level characteristics (e.g., to serve as risk models or augment survey datasets). However, when should a user have confidence that an LLM will provide high-quality predictions for their particular task? To address this question, we conduct a large-scale empirical study of LLMs' zero-shot predictive capabilities across a wide range of tabular prediction tasks. We find that LLMs' performance is highly variable, both on tasks within the same dataset and across different datasets. However, when the LLM performs well on the base prediction task, its predicted probabilities become a stronger signal for individual-level accuracy. Then, we construct metrics to predict LLMs' performance at the task level, aiming to distinguish between tasks where LLMs may perform well and where they are likely unsuitable. We find that some of these metrics, each of which are assessed without labeled data, yield strong signals of LLMs' predictive performance on new tasks.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15356v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ê°œë³„ ìˆ˜ì¤€ì˜ íŠ¹ì„±ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ì œë¡œìƒ· ëª¨ë¸ë¡œì„œì˜ ëŠ¥ë ¥ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤(ì˜ˆ: ìœ„í—˜ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì„¤ë¬¸ ì¡°ì‚¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë³´ê°•í•˜ê¸° ìœ„í•´). ê·¸ëŸ¬ë‚˜ ì‚¬ìš©ìê°€ íŠ¹ì • ì‘ì—…ì— ëŒ€í•´ LLMì´ ê³ í’ˆì§ˆì˜ ì˜ˆì¸¡ì„ ì œê³µí•  ê²ƒì´ë¼ëŠ” í™•ì‹ ì„ ê°€ì ¸ì•¼ í•  ë•ŒëŠ” ì–¸ì œì¼ê¹Œìš”? ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ í‘œ í˜•ì‹ ì˜ˆì¸¡ ì‘ì—…ì— ëŒ€í•œ LLMì˜ ì œë¡œìƒ· ì˜ˆì¸¡ ëŠ¥ë ¥ì„ ëŒ€ê·œëª¨ ì‹¤ì¦ì ìœ¼ë¡œ ì—°êµ¬í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì˜ ì„±ëŠ¥ì´ ë™ì¼í•œ ë°ì´í„°ì…‹ ë‚´ì˜ ì‘ì—…ê³¼ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ ê°„ì—ì„œ ëª¨ë‘ ë§¤ìš° ê°€ë³€ì ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ë³¸ ì˜ˆì¸¡ ì‘ì—…ì—ì„œ LLMì´ ì˜ ìˆ˜í–‰í•  ë•Œ, ì˜ˆì¸¡ëœ í™•ë¥ ì€ ê°œë³„ ìˆ˜ì¤€ì˜ ì •í™•ì„±ì„ ìœ„í•œ ë” ê°•ë ¥í•œ ì‹ í˜¸ê°€ ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ìš°ë¦¬ëŠ” LLMì˜ ì‘ì—… ìˆ˜ì¤€ì—ì„œì˜ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ì§€í‘œë¥¼ êµ¬ì„±í•˜ì—¬, LLMì´ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì‘ì—…ê³¼ ë¶€ì í•©í•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì‘ì—…ì„ êµ¬ë³„í•˜ê³ ì í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì§€í‘œ ì¤‘ ì¼ë¶€ê°€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œ í‰ê°€ë˜ì—ˆì„ ë•Œ, ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ LLMì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì— ëŒ€í•œ ê°•ë ¥í•œ ì‹ í˜¸ë¥¼ ì œê³µí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì œë¡œìƒ· ì˜ˆì¸¡ ëŠ¥ë ¥ì„ ë‹¤ì–‘í•œ í…Œì´ë¸” ì˜ˆì¸¡ ì‘ì—…ì—ì„œ í‰ê°€í•œ ëŒ€ê·œëª¨ ì‹¤ì¦ ì—°êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, LLMì˜ ì„±ëŠ¥ì€ ë™ì¼í•œ ë°ì´í„°ì…‹ ë‚´ ì‘ì—…ê³¼ ë‹¤ë¥¸ ë°ì´í„°ì…‹ ê°„ì—ì„œ ë§¤ìš° ë‹¤ì–‘í•˜ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ë³¸ ì˜ˆì¸¡ ì‘ì—…ì—ì„œ LLMì´ ì˜ ìˆ˜í–‰í•  ê²½ìš°, ì˜ˆì¸¡ í™•ë¥ ì´ ê°œë³„ ìˆ˜ì¤€ì˜ ì •í™•ì„±ì„ ë” ì˜ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì˜ ì‘ì—…ë³„ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ì§€í‘œë¥¼ ê°œë°œí•˜ì—¬, LLMì´ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì‘ì—…ê³¼ ê·¸ë ‡ì§€ ì•Šì€ ì‘ì—…ì„ êµ¬ë¶„í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œ í‰ê°€ëœ ì¼ë¶€ ì§€í‘œëŠ” ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ LLMì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê°•í•˜ê²Œ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì œë¡œìƒ· ì˜ˆì¸¡ ëŠ¥ë ¥ì€ ë°ì´í„°ì…‹ ë‚´ì˜ ë‹¤ì–‘í•œ ê³¼ì œì™€ ë°ì´í„°ì…‹ ê°„ì—ì„œ ì„±ëŠ¥ ë³€ë™ì´ í¬ë‹¤.

- 2. LLMì´ ê¸°ë³¸ ì˜ˆì¸¡ ê³¼ì œì—ì„œ ì˜ ìˆ˜í–‰í•  ë•Œ, ì˜ˆì¸¡ëœ í™•ë¥ ì€ ê°œë³„ ìˆ˜ì¤€ì˜ ì •í™•ì„±ì„ ìœ„í•œ ê°•ë ¥í•œ ì‹ í˜¸ê°€ ëœë‹¤.

- 3. LLMì˜ ê³¼ì œ ìˆ˜ì¤€ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­ì„ êµ¬ì¶•í•˜ì—¬, LLMì´ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê³¼ì œì™€ ë¶€ì í•©í•œ ê³¼ì œë¥¼ êµ¬ë³„í•˜ê³ ì í•œë‹¤.

- 4. ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œ í‰ê°€ëœ ì¼ë¶€ ë©”íŠ¸ë¦­ì€ ìƒˆë¡œìš´ ê³¼ì œì—ì„œ LLMì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì— ëŒ€í•œ ê°•ë ¥í•œ ì‹ í˜¸ë¥¼ ì œê³µí•œë‹¤.

---

*Generated on 2025-09-22 15:11:23*