# TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning

**Korean Title:** TGPO: ê²¬ê³ í•œ ì›¹ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ìœ„í•œ íŠ¸ë¦¬ ê¸°ë°˜ ì„ í˜¸ ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Dynamic Weighting Mechanism

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250917|TGPO Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (99.2% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (82.5% similar)
- [[2025-09-19/WebCoT_ Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback_20250919|WebCoT Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback]] (82.3% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (81.9% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (81.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14172v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.14172v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ ì¸í•´ ëŒ€í˜• ëª¨ë¸ì„ ì›¹ ì—ì´ì „íŠ¸ë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì— í•„ìˆ˜ì ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°•í™” í•™ìŠµì„ í†µí•´ ì›¹ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ëŠ” ë°ì—ëŠ” ì‹ ìš© í• ë‹¹ì˜ ì˜ëª»ëœ ë°°ë¶„, ì§€ë‚˜ì¹˜ê²Œ ë†’ì€ ì£¼ì„ ë¹„ìš©, ë³´ìƒì˜ í¬ì†Œì„± ë“± ì¤‘ìš”í•œ ë¬¸ì œë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Tree-Guided Preference Optimization (TGPO)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœë¥¼ í†µí•©í•˜ì—¬ ê²½ë¡œ ê°„ì˜ ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê²½ë¡œ í‘œí˜„ì„ ì œì•ˆí•˜ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” í•˜ìœ„ ëª©í‘œ ì§„í–‰, ì¤‘ë³µ ê°ì§€, í–‰ë™ ê²€ì¦ì„ í†µí•´ ì„¸ë¶„í™”ëœ ë³´ìƒì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ í¬í•¨í•©ë‹ˆë‹¤. ë˜í•œ, í›ˆë ¨ ì¤‘ì— ë†’ì€ ì˜í–¥ë ¥ì„ ë¯¸ì¹˜ëŠ” ê²°ì • ì§€ì ì„ ìš°ì„ ì‹œí•˜ëŠ” ë™ì  ê°€ì¤‘ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. Online-Mind2Web ë° ìš°ë¦¬ê°€ ìì²´ì ìœ¼ë¡œ êµ¬ì¶•í•œ C-WebShop ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ì€ TGPOê°€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì´ë©°, ë” ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¡œ ë” ë†’ì€ ì„±ê³µë¥ ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ì— ë”°ë¼ ì›¹ ì—ì´ì „íŠ¸ë¥¼ ìë™í™”í•˜ëŠ” ë° í•„ìš”í•œ ê°•í™” í•™ìŠµì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Tree-Guided Preference Optimization (TGPO)ë¼ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. TGPOëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœë¥¼ ë³‘í•©í•˜ì—¬ ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê²½ë¡œ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ, ì„¸ë¶€ì ì¸ ë³´ìƒì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ê³¼ ë†’ì€ ì˜í–¥ë ¥ì„ ê°€ì§„ ê²°ì • ì§€ì ì„ ìš°ì„ ì‹œí•˜ëŠ” ë™ì  ê°€ì¤‘ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, TGPOëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë” ë†’ì€ ì„±ê³µë¥ ê³¼ ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ ì›¹ ì—ì´ì „íŠ¸ì˜ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì´ ì¤‘ìš”í•´ì§€ê³  ìˆë‹¤.

- 2. ì›¹ ì—ì´ì „íŠ¸ì˜ ê°•í™” í•™ìŠµì—ëŠ” ì‹ ìš© í• ë‹¹ ì˜¤ë¥˜, ë†’ì€ ì£¼ì„ ë¹„ìš©, ë³´ìƒ í¬ì†Œì„± ë“±ì˜ ë¬¸ì œê°€ ìˆë‹¤.

- 3. Tree-Guided Preference Optimization (TGPO)ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê¶¤ì  í‘œí˜„ì„ í†µí•´ ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì´ë‹¤.

- 4. TGPOëŠ” ì„¸ë¶€ì ì¸ ë³´ìƒì„ ìë™ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ í¬í•¨í•˜ë©°, ì¤‘ë³µ íƒì§€ì™€ í–‰ë™ ê²€ì¦ì„ í†µí•´ ë³´ìƒì„ ì œê³µí•œë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼ TGPOëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ê³¼ ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ì…ì¦í•˜ì˜€ë‹¤.

---

*Generated on 2025-09-22 15:05:42*