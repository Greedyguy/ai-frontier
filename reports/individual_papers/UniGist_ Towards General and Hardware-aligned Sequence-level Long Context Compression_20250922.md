# UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression

**Korean Title:** UniGist: ì¼ë°˜ì ì´ê³  í•˜ë“œì›¨ì–´ì— ë§ì¶˜ ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì¥ë¬¸ ë§¥ë½ ì••ì¶•ì„ í–¥í•˜ì—¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Long-range Dependency Modeling|Long-range Dependency Modeling]] [[keywords/specific/Sequence-level Compression|Sequence-level Compression]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/unique/UniGist|UniGist]] [[categories/cs.CL|cs.CL]] [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (84.9% similar) [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (83.6% similar) [[2025-09-17/Dense Video Understanding with Gated Residual Tokenization_20250917|Dense Video Understanding with Gated Residual Tokenization]] (80.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Long-range Dependency Modeling
**ğŸ”— Specific Connectable**: Sequence-level Compression
**ğŸ”¬ Broad Technical**: Large Language Models
**â­ Unique Technical**: UniGist
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose Efficient Structured KV Cache Compression with Composite Tokens]] (84.9% similar)
- [[2025-09-19/Value-Guided KV Compression for LLMs via Approximated CUR Decomposition_20250919|Value-Guided KV Compression for LLMs via Approximated CUR Decomposition]] (83.6% similar)
- [[2025-09-17/Dense Video Understanding with Gated Residual Tokenization_20250917|Dense Video Understanding with Gated Residual Tokenization]] (80.4% similar)
- [[2025-09-22/CORE-RAG_ Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning_20250922|CORE-RAG Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning]] (80.1% similar)
- [[2025-09-22/LiMuon_ Light and Fast Muon Optimizer for Large Models_20250922|LiMuon Light and Fast Muon Optimizer for Large Models]] (79.9% similar)


**ArXiv ID**: [2509.15763](https://arxiv.org/abs/2509.15763)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15763.pdf)


**ArXiv ID**: [2509.15763](https://arxiv.org/abs/2509.15763)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15763.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Long-range Dependency Modeling
**ğŸ”— Specific Connectable**: Memory Overhead Reduction
**â­ Unique Technical**: UniGist
**ğŸ”¬ Broad Technical**: Large Language Models, Sequence-level Compression

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Sequence-level Compression` â€¢ 

`UniGist` â€¢ 

`Long-range Dependency Modeling`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15763v1 Announce Type: new 
Abstract: Large language models are increasingly capable of handling long-context inputs, but the memory overhead of key-value (KV) cache remains a major bottleneck for general-purpose deployment. While various compression strategies have been explored, sequence-level compression, which drops the full KV caches for certain tokens, is particularly challenging as it can lead to the loss of important contextual information. To address this, we introduce UniGist, a sequence-level long-context compression framework that efficiently preserves context information by replacing raw tokens with special compression tokens (gists) in a fine-grained manner. We adopt a chunk-free training strategy and design an efficient kernel with a gist shift trick, enabling optimized GPU training. Our scheme also supports flexible inference by allowing the actual removal of compressed tokens, resulting in real-time memory savings. Experiments across multiple long-context tasks demonstrate that UniGist significantly improves compression quality, with especially strong performance in detail-recalling tasks and long-range dependency modeling.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15763v1 ë°œí‘œ ìœ í˜•: ì‹ ê·œ  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ì ì  ë” ê¸´ ë¬¸ë§¥ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆì§€ë§Œ, í‚¤-ê°’(KV) ìºì‹œì˜ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œëŠ” ë²”ìš© ë°°í¬ì˜ ì£¼ìš” ë³‘ëª© í˜„ìƒìœ¼ë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì••ì¶• ì „ëµì´ íƒêµ¬ë˜ì—ˆì§€ë§Œ, íŠ¹ì • í† í°ì— ëŒ€í•´ ì „ì²´ KV ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì••ì¶•ì€ ì¤‘ìš”í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ìƒì„ ìˆ˜ ìˆì–´ íŠ¹íˆ ë„ì „ì ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” UniGistë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. UniGistëŠ” ì›ì‹œ í† í°ì„ íŠ¹ìˆ˜ ì••ì¶• í† í°(ìš”ì•½)ìœ¼ë¡œ ì„¸ë°€í•˜ê²Œ ëŒ€ì²´í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ì¡´í•˜ëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ì¥ë¬¸ë§¥ ì••ì¶• í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì²­í¬ ì—†ëŠ” í›ˆë ¨ ì „ëµì„ ì±„íƒí•˜ê³ , ìš”ì•½ ì´ë™ íŠ¸ë¦­ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì»¤ë„ì„ ì„¤ê³„í•˜ì—¬ ìµœì í™”ëœ GPU í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ìŠ¤í‚´ì€ ë˜í•œ ì••ì¶•ëœ í† í°ì˜ ì‹¤ì œ ì œê±°ë¥¼ í—ˆìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìœ ì—°í•œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì¥ë¬¸ë§¥ ì‘ì—…ì— ëŒ€í•œ ì‹¤í—˜ì€ UniGistê°€ ì••ì¶• í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, íŠ¹íˆ ì„¸ë¶€ ì‚¬í•­ íšŒìƒ ì‘ì—…ê³¼ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ê¸´ ë¬¸ë§¥ ì…ë ¥ ì²˜ë¦¬ ì‹œ ë°œìƒí•˜ëŠ” í‚¤-ê°’(KV) ìºì‹œì˜ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ UniGistë¼ëŠ” ìƒˆë¡œìš´ ì••ì¶• í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. UniGistëŠ” íŠ¹ì • í† í°ì˜ KV ìºì‹œë¥¼ ëŒ€ì²´í•˜ì—¬ ì¤‘ìš”í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ì¡´í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ íŠ¹ë³„í•œ ì••ì¶• í† í°(gist)ì„ ë„ì…í•˜ê³ , GPU ìµœì í™”ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ ì»¤ë„ ì„¤ê³„ì™€ í•¨ê»˜ ì²­í¬ ì—†ëŠ” í•™ìŠµ ì „ëµì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì••ì¶•ëœ í† í°ì˜ ì‹¤ì œ ì œê±°ë¥¼ í†µí•´ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê¸´ ë¬¸ë§¥ ì‘ì—… ì‹¤í—˜ ê²°ê³¼, UniGistëŠ” íŠ¹íˆ ì„¸ë¶€ ì •ë³´ íšŒìƒ ë° ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§ì—ì„œ ë›°ì–´ë‚œ ì••ì¶• ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ê¸´ ë¬¸ë§¥ ì…ë ¥ ì²˜ë¦¬ ëŠ¥ë ¥ì€ í–¥ìƒë˜ê³  ìˆì§€ë§Œ, í‚¤-ê°’(KV) ìºì‹œì˜ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œëŠ” ì—¬ì „íˆ ì£¼ìš”í•œ ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤.

- 2. UniGistëŠ” ì›ì‹œ í† í°ì„ íŠ¹ìˆ˜ ì••ì¶• í† í°(ìš”ì•½)ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ì¡´í•˜ëŠ” ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ê¸´ ë¬¸ë§¥ ì••ì¶• í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- 3. ìš°ë¦¬ëŠ” ì²­í¬ ì—†ëŠ” í›ˆë ¨ ì „ëµê³¼ ìš”ì•½ ì´ë™ ê¸°ë²•ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì»¤ë„ì„ ì„¤ê³„í•˜ì—¬ ìµœì í™”ëœ GPU í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.

- 4. UniGistëŠ” ì••ì¶•ëœ í† í°ì˜ ì‹¤ì œ ì œê±°ë¥¼ í—ˆìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì ˆê°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìœ ì—°í•œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤.

- 5. ë‹¤ì–‘í•œ ê¸´ ë¬¸ë§¥ ì‘ì—… ì‹¤í—˜ì—ì„œ UniGistëŠ” ì••ì¶• í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, íŠ¹íˆ ì„¸ë¶€ ì‚¬í•­ íšŒìƒ ì‘ì—… ë° ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-22 16:27:18*