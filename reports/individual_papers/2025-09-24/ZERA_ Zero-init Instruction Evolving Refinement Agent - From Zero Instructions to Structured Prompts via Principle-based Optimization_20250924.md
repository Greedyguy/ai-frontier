<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:05:17.053967",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Prompt Optimization",
    "ZERA",
    "Structured Prompts",
    "Principle-based Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Prompt Optimization": 0.78,
    "ZERA": 0.82,
    "Structured Prompts": 0.77,
    "Principle-based Optimization": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on prompt optimization for language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Automatic Prompt Optimization",
        "canonical": "Prompt Optimization",
        "aliases": [
          "APO"
        ],
        "category": "unique_technical",
        "rationale": "Key concept introduced in the paper for improving model performance.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-init Instruction Evolving Refinement Agent",
        "canonical": "ZERA",
        "aliases": [
          "Zero-init Instruction Evolving Refinement Agent"
        ],
        "category": "unique_technical",
        "rationale": "The novel framework proposed by the authors, central to the study.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "Structured Prompts",
        "canonical": "Structured Prompts",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the structured approach to prompt refinement discussed in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Principle-based Optimization",
        "canonical": "Principle-based Optimization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Describes the method of optimization used in the framework, relevant for linking.",
        "novelty_score": 0.68,
        "connectivity_score": 0.66,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Automatic Prompt Optimization",
      "resolved_canonical": "Prompt Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-init Instruction Evolving Refinement Agent",
      "resolved_canonical": "ZERA",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Structured Prompts",
      "resolved_canonical": "Structured Prompts",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Principle-based Optimization",
      "resolved_canonical": "Principle-based Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.66,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18158.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18158](https://arxiv.org/abs/2509.18158)

## 🔗 유사한 논문
- [[2025-09-23/PromptSuite_ A Task-Agnostic Framework for Multi-Prompt Generation_20250923|PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation]] (82.7% similar)
- [[2025-09-23/Prompt-with-Me_ in-IDE Structured Prompt Management for LLM-Driven Software Engineering_20250923|Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering]] (82.1% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (81.9% similar)
- [[2025-09-23/Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization_20250923|Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization]] (81.9% similar)
- [[2025-09-23/QA-prompting_ Improving Summarization with Large Language Models using Question-Answering_20250923|QA-prompting: Improving Summarization with Large Language Models using Question-Answering]] (81.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Structured Prompts|Structured Prompts]], [[keywords/Principle-based Optimization|Principle-based Optimization]]
**⚡ Unique Technical**: [[keywords/Prompt Optimization|Prompt Optimization]], [[keywords/ZERA|ZERA]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18158v1 Announce Type: cross 
Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.

## 📝 요약

Automatic Prompt Optimization (APO)의 한계를 극복하기 위해 제안된 ZERA(Zero-init Instruction Evolving Refinement Agent)는 시스템과 사용자 프롬프트를 함께 최적화하는 새로운 프레임워크입니다. ZERA는 자동으로 유추된 가중치를 사용하여 여덟 가지 기준으로 프롬프트를 평가하고 구조화된 비판을 통해 프롬프트를 수정합니다. 이를 통해 최소한의 예제와 짧은 반복 주기로 고품질 프롬프트에 빠르게 수렴할 수 있습니다. 다섯 개의 대형 언어 모델과 아홉 개의 다양한 데이터셋에서 실험한 결과, ZERA는 강력한 기준선보다 일관된 성능 향상을 보였습니다. 각 구성 요소의 기여도를 강조한 추가 연구도 진행되었습니다. ZERA의 구현 및 모든 프롬프트는 공개되어 있습니다.

## 🎯 주요 포인트

- 1. ZERA는 시스템 및 사용자 프롬프트를 동시에 최적화하는 새로운 프레임워크로, 구조화된 비평을 통해 프롬프트를 수정합니다.
- 2. ZERA는 자동으로 추론된 가중치를 사용하여 여덟 가지 일반화 가능한 기준으로 프롬프트를 평가하고, 최소한의 예제와 짧은 반복 주기로 고품질 프롬프트에 빠르게 수렴합니다.
- 3. ZERA는 다섯 개의 대형 언어 모델과 아홉 개의 다양한 데이터셋에 대해 평가되었으며, 일관된 성능 향상을 보여주었습니다.
- 4. 추가적인 제거 연구를 통해 각 구성 요소가 효과적인 프롬프트 구성에 기여하는 바를 강조합니다.
- 5. ZERA의 구현 및 모든 프롬프트는 공개적으로 제공됩니다.


---

*Generated on 2025-09-24 15:05:17*