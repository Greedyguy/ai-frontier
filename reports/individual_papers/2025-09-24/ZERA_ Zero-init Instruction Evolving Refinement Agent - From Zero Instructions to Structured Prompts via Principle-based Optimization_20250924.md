<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:05:17.053967",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Prompt Optimization",
    "ZERA",
    "Structured Prompts",
    "Principle-based Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Prompt Optimization": 0.78,
    "ZERA": 0.82,
    "Structured Prompts": 0.77,
    "Principle-based Optimization": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on prompt optimization for language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Automatic Prompt Optimization",
        "canonical": "Prompt Optimization",
        "aliases": [
          "APO"
        ],
        "category": "unique_technical",
        "rationale": "Key concept introduced in the paper for improving model performance.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-init Instruction Evolving Refinement Agent",
        "canonical": "ZERA",
        "aliases": [
          "Zero-init Instruction Evolving Refinement Agent"
        ],
        "category": "unique_technical",
        "rationale": "The novel framework proposed by the authors, central to the study.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "Structured Prompts",
        "canonical": "Structured Prompts",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights the structured approach to prompt refinement discussed in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Principle-based Optimization",
        "canonical": "Principle-based Optimization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Describes the method of optimization used in the framework, relevant for linking.",
        "novelty_score": 0.68,
        "connectivity_score": 0.66,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Automatic Prompt Optimization",
      "resolved_canonical": "Prompt Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-init Instruction Evolving Refinement Agent",
      "resolved_canonical": "ZERA",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Structured Prompts",
      "resolved_canonical": "Structured Prompts",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Principle-based Optimization",
      "resolved_canonical": "Principle-based Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.66,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18158.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18158](https://arxiv.org/abs/2509.18158)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/PromptSuite_ A Task-Agnostic Framework for Multi-Prompt Generation_20250923|PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation]] (82.7% similar)
- [[2025-09-23/Prompt-with-Me_ in-IDE Structured Prompt Management for LLM-Driven Software Engineering_20250923|Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering]] (82.1% similar)
- [[2025-09-19/PMPO_ Probabilistic Metric Prompt Optimization for Small and Large Language Models_20250919|PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models]] (81.9% similar)
- [[2025-09-23/Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization_20250923|Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization]] (81.9% similar)
- [[2025-09-23/QA-prompting_ Improving Summarization with Large Language Models using Question-Answering_20250923|QA-prompting: Improving Summarization with Large Language Models using Question-Answering]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Structured Prompts|Structured Prompts]], [[keywords/Principle-based Optimization|Principle-based Optimization]]
**âš¡ Unique Technical**: [[keywords/Prompt Optimization|Prompt Optimization]], [[keywords/ZERA|ZERA]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18158v1 Announce Type: cross 
Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.

## ğŸ“ ìš”ì•½

Automatic Prompt Optimization (APO)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ZERA(Zero-init Instruction Evolving Refinement Agent)ëŠ” ì‹œìŠ¤í…œê³¼ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ZERAëŠ” ìë™ìœ¼ë¡œ ìœ ì¶”ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ í‰ê°€í•˜ê³  êµ¬ì¡°í™”ëœ ë¹„íŒì„ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìµœì†Œí•œì˜ ì˜ˆì œì™€ ì§§ì€ ë°˜ë³µ ì£¼ê¸°ë¡œ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ì— ë¹ ë¥´ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì„¯ ê°œì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ì•„í™‰ ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, ZERAëŠ” ê°•ë ¥í•œ ê¸°ì¤€ì„ ë³´ë‹¤ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê° êµ¬ì„± ìš”ì†Œì˜ ê¸°ì—¬ë„ë¥¼ ê°•ì¡°í•œ ì¶”ê°€ ì—°êµ¬ë„ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ZERAì˜ êµ¬í˜„ ë° ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ZERAëŠ” ì‹œìŠ¤í…œ ë° ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì‹œì— ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, êµ¬ì¡°í™”ëœ ë¹„í‰ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
- 2. ZERAëŠ” ìë™ìœ¼ë¡œ ì¶”ë¡ ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ ê°€ì§€ ì¼ë°˜í™” ê°€ëŠ¥í•œ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ í‰ê°€í•˜ê³ , ìµœì†Œí•œì˜ ì˜ˆì œì™€ ì§§ì€ ë°˜ë³µ ì£¼ê¸°ë¡œ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ì— ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤.
- 3. ZERAëŠ” ë‹¤ì„¯ ê°œì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ì•„í™‰ ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- 4. ì¶”ê°€ì ì¸ ì œê±° ì—°êµ¬ë¥¼ í†µí•´ ê° êµ¬ì„± ìš”ì†Œê°€ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì— ê¸°ì—¬í•˜ëŠ” ë°”ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.
- 5. ZERAì˜ êµ¬í˜„ ë° ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ê³µê°œì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:05:17*