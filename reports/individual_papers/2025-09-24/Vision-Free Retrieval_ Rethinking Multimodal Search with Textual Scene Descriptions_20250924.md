<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:18:22.416274",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Zero-Shot Learning",
    "Textual Scene Descriptions",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.88,
    "Zero-Shot Learning": 0.85,
    "Textual Scene Descriptions": 0.7,
    "Multimodal Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper's discussion on multimodal retrieval and links to evolving trends in AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      },
      {
        "surface": "Zero-Shot Performance",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "The paper emphasizes achieving state-of-the-art results in zero-shot settings, highlighting its relevance.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Textual Scene Descriptions",
        "canonical": "Textual Scene Descriptions",
        "aliases": [
          "Structured Image Descriptions"
        ],
        "category": "unique_technical",
        "rationale": "This is a unique approach proposed in the paper, offering a novel method for vision-free retrieval.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      },
      {
        "surface": "Multimodal Search",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal Retrieval"
        ],
        "category": "specific_connectable",
        "rationale": "The paper rethinks traditional multimodal approaches, making this a key concept for linking.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Zero-Shot Performance",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Textual Scene Descriptions",
      "resolved_canonical": "Textual Scene Descriptions",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Multimodal Search",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19203.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19203](https://arxiv.org/abs/2509.19203)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/CLIP-IN_ Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions_20250923|CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions]] (87.7% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (86.5% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (85.6% similar)
- [[2025-09-24/Reading Images Like Texts_ Sequential Image Understanding in Vision-Language Models_20250924|Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models]] (85.6% similar)
- [[2025-09-22/Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays_20250922|Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Textual Scene Descriptions|Textual Scene Descriptions]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19203v1 Announce Type: new 
Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations. However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour. These limitations are reinforced by their dual-encoder design, which induces a modality gap. Additionally, the reliance on vast web-collected data corpora for training makes the process computationally expensive and introduces significant privacy concerns. To address these limitations, in this work, we challenge the necessity of vision encoders for retrieval tasks by introducing a vision-free, single-encoder retrieval pipeline. Departing from the traditional text-to-image retrieval paradigm, we migrate to a text-to-text paradigm with the assistance of VLLM-generated structured image descriptions. We demonstrate that this paradigm shift has significant advantages, including a substantial reduction of the modality gap, improved compositionality, and better performance on short and long caption queries, all attainable with only a few hours of calibration on two GPUs. Additionally, substituting raw images with textual descriptions introduces a more privacy-friendly alternative for retrieval. To further assess generalisation and address some of the shortcomings of prior compositionality benchmarks, we release two benchmarks derived from Flickr30k and COCO, containing diverse compositional queries made of short captions, which we coin subFlickr and subCOCO. Our vision-free retriever matches and often surpasses traditional multimodal models. Importantly, our approach achieves state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks, with models as small as 0.3B parameters. Code is available at: https://github.com/IoannaNti/LexiCLIP

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ ì‹œê°-ì–¸ì–´ ëª¨ë¸(VLM)ì¸ CLIPì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì€ ì–•ì€ ì–¸ì–´ ì´í•´ì™€ ëª¨ë‹¬ë¦¬í‹° ê²©ì°¨ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ë°ì´í„° ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë¹„ìš©ê³¼ ê°œì¸ì •ë³´ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ ì—°êµ¬ëŠ” ì‹œê° ì¸ì½”ë” ì—†ì´ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ë‹¨ì¼ ì¸ì½”ë” ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ì„ ë„ì…í•©ë‹ˆë‹¤. VLLMì´ ìƒì„±í•œ êµ¬ì¡°í™”ëœ ì´ë¯¸ì§€ ì„¤ëª…ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ê°„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ë©°, ì´ëŠ” ëª¨ë‹¬ë¦¬í‹° ê²©ì°¨ë¥¼ ì¤„ì´ê³ , ì¡°í•©ì„±ì„ ê°œì„ í•˜ë©°, ì§§ê³  ê¸´ ìº¡ì…˜ ì¿¼ë¦¬ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ì´ë¯¸ì§€ ëŒ€ì²´ëŠ” ê°œì¸ì •ë³´ ë³´í˜¸ ì¸¡ë©´ì—ì„œë„ ìœ ë¦¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ subFlickrì™€ subCOCOë¥¼ ì œì‹œí•˜ë©°, ì œì•ˆëœ ë°©ë²•ì´ ê¸°ì¡´ ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì—¬ëŸ¬ ê²€ìƒ‰ ë° ì¡°í•©ì„± ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ì‹œê°-ì–¸ì–´ ëª¨ë¸ì€ ì–•ì€ ì–¸ì–´ ì´í•´ì™€ ëª¨ë‹¬ë¦¬í‹° ê²©ì°¨ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì´ëŠ” ì´ì¤‘ ì¸ì½”ë” ì„¤ê³„ë¡œ ì¸í•´ ê°•í™”ë©ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‹œê° ì¸ì½”ë” ì—†ì´ í…ìŠ¤íŠ¸-í…ìŠ¤íŠ¸ íŒ¨ëŸ¬ë‹¤ì„ì„ í™œìš©í•˜ì—¬ ëª¨ë‹¬ë¦¬í‹° ê²©ì°¨ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ì‚¬ìš©í•˜ì—¬ ê°œì¸ ì •ë³´ ë³´í˜¸ë¥¼ ê°•í™”í•˜ê³ , ì§§ê³  ê¸´ ìº¡ì…˜ ì¿¼ë¦¬ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. Flickr30kì™€ COCOì—ì„œ íŒŒìƒëœ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•´ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ë©°, ì œì•ˆëœ ëª¨ë¸ì€ ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì€ ì—¬ëŸ¬ ê²€ìƒ‰ ë° ì¡°í•©ì„± ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì½”ë“œê°€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:18:22*