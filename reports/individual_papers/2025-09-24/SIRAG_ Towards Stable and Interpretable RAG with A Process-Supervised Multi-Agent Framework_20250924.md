<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:37:32.784818",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Retrieval Augmented Generation",
    "Large Language Model",
    "Proximal Policy Optimization",
    "Multi-Agent Framework"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Retrieval Augmented Generation": 0.85,
    "Large Language Model": 0.8,
    "Proximal Policy Optimization": 0.82,
    "Multi-Agent Framework": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Retrieval-Augmented Generation",
        "canonical": "Retrieval Augmented Generation",
        "aliases": [
          "RAG"
        ],
        "category": "specific_connectable",
        "rationale": "RAG is a key concept in the paper, focusing on enhancing LLMs with external knowledge, which is crucial for linking related research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the study and provide a broad technical foundation for linking with other NLP research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "specific_connectable",
        "rationale": "PPO is a specific reinforcement learning technique used in the framework, linking it to other works in machine learning optimization.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Multi-Agent Framework",
        "canonical": "Multi-Agent Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The framework is a novel approach introduced in the paper, providing a unique angle for linking with multi-agent system research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "retriever",
      "generator",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Retrieval-Augmented Generation",
      "resolved_canonical": "Retrieval Augmented Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Multi-Agent Framework",
      "resolved_canonical": "Multi-Agent Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18167.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18167](https://arxiv.org/abs/2509.18167)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/RAG+_ Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning_20250924|RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning]] (92.0% similar)
- [[2025-09-23/GRIL_ Knowledge Graph Retrieval-Integrated Learning with Large Language Models_20250923|GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models]] (91.6% similar)
- [[2025-09-19/Enhancing Retrieval Augmentation via Adversarial Collaboration_20250919|Enhancing Retrieval Augmentation via Adversarial Collaboration]] (90.9% similar)
- [[2025-09-19/Causal-Counterfactual RAG_ The Integration of Causal-Counterfactual Reasoning into RAG_20250919|Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG]] (90.2% similar)
- [[2025-09-22/Relevance to Utility_ Process-Supervised Rewrite for RAG_20250922|Relevance to Utility: Process-Supervised Rewrite for RAG]] (89.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Retrieval Augmented Generation|Retrieval Augmented Generation]], [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]]
**âš¡ Unique Technical**: [[keywords/Multi-Agent Framework|Multi-Agent Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18167v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access external knowledge sources, but the effectiveness of RAG relies on the coordination between the retriever and the generator. Since these components are developed independently, their interaction is often suboptimal: the retriever may return irrelevant or redundant documents, while the generator may fail to fully leverage retrieved evidence. In this work, we propose a process-supervised multi-agent framework to bridge the gap between retriever and generator. The framework introduces two lightweight agents: a Decision Maker, which determines when to continue retrieval or stop for answer generation, and a Knowledge Selector, which filters retrieved documents to retain only the most useful evidence. To provide fine-grained supervision, we employ an LLM-as-a-Judge that evaluates each intermediate action with process-level rewards, ensuring more accurate credit assignment than relying solely on final answer correctness. We further adopt a tree-structured rollout strategy to explore diverse reasoning paths, and train both agents with Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on single-hop and multi-hop question answering benchmarks show that our approach achieves higher accuracy, more stable convergence, and produces more interpretable reasoning trajectories compared with standard RAG baselines. Importantly, the proposed framework is modular and plug-and-play, requiring no modification to the retriever or generator, making it practical for real-world RAG applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì™¸ë¶€ ì§€ì‹ì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Retrieval-Augmented Generation (RAG)ì˜ íš¨ê³¼ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, í”„ë¡œì„¸ìŠ¤ ê°ë… ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‘ ê°œì˜ ê²½ëŸ‰ ì—ì´ì „íŠ¸ë¥¼ ë„ì…í•˜ì—¬, ê²€ìƒ‰ê¸°ì™€ ìƒì„±ê¸° ê°„ì˜ ë¹„íš¨ìœ¨ì ì¸ ìƒí˜¸ì‘ìš©ì„ í•´ê²°í•©ë‹ˆë‹¤. 'ì˜ì‚¬ ê²°ì •ì'ëŠ” ê²€ìƒ‰ì„ ê³„ì†í• ì§€ ë‹µë³€ ìƒì„±ì„ ì‹œì‘í• ì§€ë¥¼ ê²°ì •í•˜ê³ , 'ì§€ì‹ ì„ íƒì'ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ê°€ì¥ ìœ ìš©í•œ ì¦ê±°ë§Œì„ ë‚¨ê¹ë‹ˆë‹¤. LLMì„ íŒì‚¬ë¡œ í™œìš©í•˜ì—¬ ê° ì¤‘ê°„ í–‰ë™ì„ í‰ê°€í•˜ê³ , ì„¸ë¶€ì ì¸ ë³´ìƒì„ ì œê³µí•¨ìœ¼ë¡œì¨ ë” ì •í™•í•œ ì‹ ìš© í• ë‹¹ì„ ë³´ì¥í•©ë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì¶”ë¡  ê²½ë¡œë¥¼ íƒìƒ‰í•˜ê¸° ìœ„í•´ íŠ¸ë¦¬ êµ¬ì¡°ì˜ ë¡¤ì•„ì›ƒ ì „ëµì„ ì±„íƒí•˜ê³ , Proximal Policy Optimization (PPO)ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì—ì´ì „íŠ¸ë¥¼ ì¢…ë‹¨ ê°„ í•™ìŠµí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ë‹¨ì¼ ë° ë‹¤ì¤‘ í™‰ ì§ˆë¬¸ ì‘ë‹µ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë” ë†’ì€ ì •í™•ë„ì™€ ì•ˆì •ì ì¸ ìˆ˜ë ´ì„±ì„ ë³´ì´ë©°, ê¸°ì¡´ RAGë³´ë‹¤ í•´ì„ ê°€ëŠ¥í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ëª¨ë“ˆí˜•ìœ¼ë¡œ, ê²€ìƒ‰ê¸°ë‚˜ ìƒì„±ê¸°ë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ì–´ ì‹¤ì œ RAG ì‘ìš©ì— ì‹¤ìš©ì ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Retrieval-Augmented Generation (RAG)ì˜ íš¨ê³¼ëŠ” ê²€ìƒ‰ê¸°ì™€ ìƒì„±ê¸° ê°„ì˜ ì¡°ì •ì— ë‹¬ë ¤ ìˆìœ¼ë©°, ì´ë“¤ì˜ ìƒí˜¸ì‘ìš©ì€ ì¢…ì¢… ìµœì ì´ ì•„ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê²€ìƒ‰ê¸°ì™€ ìƒì„±ê¸° ê°„ì˜ ê²©ì°¨ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´ í”„ë¡œì„¸ìŠ¤ ê°ë… ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 3. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ê²½ëŸ‰ ì—ì´ì „íŠ¸ì¸ ì˜ì‚¬ê²°ì •ìì™€ ì§€ì‹ ì„ íƒìë¥¼ ë„ì…í•˜ì—¬ ê²€ìƒ‰ ë° ìƒì„± ê³¼ì •ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¸ë‹¤.
- 4. LLM-as-a-Judgeë¥¼ í™œìš©í•˜ì—¬ ê° ì¤‘ê°„ í–‰ë™ì„ í‰ê°€í•˜ê³ , í”„ë¡œì„¸ìŠ¤ ìˆ˜ì¤€ì˜ ë³´ìƒì„ í†µí•´ ë³´ë‹¤ ì •í™•í•œ ì‹ ìš© í• ë‹¹ì„ ë³´ì¥í•œë‹¤.
- 5. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ëª¨ë“ˆì‹ì´ë©°, ê²€ìƒ‰ê¸°ë‚˜ ìƒì„±ê¸°ë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ì–´ ì‹¤ì œ RAG ì‘ìš©ì— ì‹¤ìš©ì ì´ë‹¤.


---

*Generated on 2025-09-24 15:37:32*