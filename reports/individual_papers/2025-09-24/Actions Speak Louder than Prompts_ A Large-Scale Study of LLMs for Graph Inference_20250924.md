<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:39:50.835527",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Graph Inference",
    "Code Generation",
    "Heterophilic Graphs",
    "Node Classification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Graph Inference": 0.82,
    "Code Generation": 0.79,
    "Heterophilic Graphs": 0.77,
    "Node Classification": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to broader discussions on LLM capabilities.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Graph Inference",
        "canonical": "Graph Inference",
        "aliases": [
          "Graph Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Focus of the study, exploring LLM applications in graph-based tasks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Code Generation",
        "canonical": "Code Generation",
        "aliases": [
          "LLM Code Generation"
        ],
        "category": "specific_connectable",
        "rationale": "Identified as a key method for enhancing LLM performance on graph data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "Heterophilic Graphs",
        "canonical": "Heterophilic Graphs",
        "aliases": [
          "Low Homophily Graphs"
        ],
        "category": "specific_connectable",
        "rationale": "Challenges assumptions about LLM performance, relevant for graph studies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Node Classification",
        "canonical": "Node Classification",
        "aliases": [
          "Graph Node Classification"
        ],
        "category": "specific_connectable",
        "rationale": "A primary application area for LLMs in graph machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.72,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Graph Inference",
      "resolved_canonical": "Graph Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Code Generation",
      "resolved_canonical": "Code Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Heterophilic Graphs",
      "resolved_canonical": "Heterophilic Graphs",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Node Classification",
      "resolved_canonical": "Node Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.72,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18487.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18487](https://arxiv.org/abs/2509.18487)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/A Survey of Large Language Models for Data Challenges in Graphs_20250922|A Survey of Large Language Models for Data Challenges in Graphs]] (89.3% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (88.7% similar)
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text?_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text?]] (86.7% similar)
- [[2025-09-22/Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics_20250922|Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics]] (86.7% similar)
- [[2025-09-23/GRIL_ Knowledge Graph Retrieval-Integrated Learning with Large Language Models_20250923|GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models]] (86.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Code Generation|Code Generation]], [[keywords/Heterophilic Graphs|Heterophilic Graphs]], [[keywords/Node Classification|Node Classification]]
**âš¡ Unique Technical**: [[keywords/Graph Inference|Graph Inference]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18487v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used for text-rich graph machine learning tasks such as node classification in high-impact domains like fraud detection and recommendation systems. Yet, despite a surge of interest, the field lacks a principled understanding of the capabilities of LLMs in their interaction with graph data. In this work, we conduct a large-scale, controlled evaluation across several key axes of variability to systematically assess the strengths and weaknesses of LLM-based graph reasoning methods in text-based applications. The axes include the LLM-graph interaction mode, comparing prompting, tool-use, and code generation; dataset domains, spanning citation, web-link, e-commerce, and social networks; structural regimes contrasting homophilic and heterophilic graphs; feature characteristics involving both short- and long-text node attributes; and model configurations with varying LLM sizes and reasoning capabilities. We further analyze dependencies by methodically truncating features, deleting edges, and removing labels to quantify reliance on input types. Our findings provide practical and actionable guidance. (1) LLMs as code generators achieve the strongest overall performance on graph data, with especially large gains on long-text or high-degree graphs where prompting quickly exceeds the token budget. (2) All interaction strategies remain effective on heterophilic graphs, challenging the assumption that LLM-based methods collapse under low homophily. (3) Code generation is able to flexibly adapt its reliance between structure, features, or labels to leverage the most informative input type. Together, these findings provide a comprehensive view of the strengths and limitations of current LLM-graph interaction modes and highlight key design principles for future approaches.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê·¸ë˜í”„ ë°ì´í„°ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ì²´ê³„ì ì¸ í‰ê°€ë¥¼ í†µí•´ LLM ê¸°ë°˜ ê·¸ë˜í”„ ì¶”ë¡  ë°©ë²•ì˜ ê°•ì ê³¼ ì•½ì ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” LLM-ê·¸ë˜í”„ ìƒí˜¸ì‘ìš© ëª¨ë“œ(í”„ë¡¬í”„íŠ¸, ë„êµ¬ ì‚¬ìš©, ì½”ë“œ ìƒì„±), ë°ì´í„°ì…‹ ë„ë©”ì¸(ì¸ìš©, ì›¹ ë§í¬, ì „ììƒê±°ë˜, ì†Œì…œ ë„¤íŠ¸ì›Œí¬), êµ¬ì¡°ì  íŠ¹ì„±(ë™ì§ˆì  ë° ì´ì§ˆì  ê·¸ë˜í”„), íŠ¹ì§•(ì§§ì€ ë° ê¸´ í…ìŠ¤íŠ¸ ë…¸ë“œ ì†ì„±), ëª¨ë¸ êµ¬ì„±(ë‹¤ì–‘í•œ LLM í¬ê¸° ë° ì¶”ë¡  ëŠ¥ë ¥)ì„ í¬í•¨í•œ ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•œ í‰ê°€ì…ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì½”ë“œ ìƒì„± ë°©ì‹ì´ ê¸´ í…ìŠ¤íŠ¸ ë˜ëŠ” ë†’ì€ ì°¨ìˆ˜ì˜ ê·¸ë˜í”„ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì´ì§ˆì  ê·¸ë˜í”„ì—ì„œë„ ëª¨ë“  ìƒí˜¸ì‘ìš© ì „ëµì´ íš¨ê³¼ì ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì½”ë“œ ìƒì„±ì€ êµ¬ì¡°, íŠ¹ì§•, ë ˆì´ë¸” ê°„ì˜ ì˜ì¡´ì„±ì„ ìœ ì—°í•˜ê²Œ ì¡°ì •í•˜ì—¬ ê°€ì¥ ìœ ìµí•œ ì…ë ¥ ìœ í˜•ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLM-ê·¸ë˜í”„ ìƒí˜¸ì‘ìš© ëª¨ë“œì˜ ê°•ì ê³¼ í•œê³„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë³´ì—¬ì£¼ë©°, í–¥í›„ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ì„¤ê³„ ì›ì¹™ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì½”ë“œ ìƒì„± ë°©ì‹ì—ì„œ ê·¸ë˜í”„ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¥ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, íŠ¹íˆ ê¸´ í…ìŠ¤íŠ¸ë‚˜ ë†’ì€ ì°¨ìˆ˜ì˜ ê·¸ë˜í”„ì—ì„œ í° ì´ì ì„ ë³´ì¸ë‹¤.
- 2. ëª¨ë“  ìƒí˜¸ì‘ìš© ì „ëµì€ ì´ì§ˆì ì¸ ê·¸ë˜í”„ì—ì„œë„ íš¨ê³¼ì ì´ë©°, LLM ê¸°ë°˜ ë°©ë²•ì´ ë‚®ì€ ë™ì§ˆì„±ì—ì„œ ë¬´ë„ˆì§ˆ ê²ƒì´ë¼ëŠ” ê°€ì •ì„ ë„ì „í•œë‹¤.
- 3. ì½”ë“œ ìƒì„±ì€ êµ¬ì¡°, íŠ¹ì§•, ë˜ëŠ” ë ˆì´ë¸” ê°„ì˜ ì˜ì¡´ì„±ì„ ìœ ì—°í•˜ê²Œ ì¡°ì •í•˜ì—¬ ê°€ì¥ ì •ë³´ê°€ ë§ì€ ì…ë ¥ ìœ í˜•ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤.
- 4. LLMê³¼ ê·¸ë˜í”„ ë°ì´í„°ì˜ ìƒí˜¸ì‘ìš© ëª¨ë“œì— ëŒ€í•œ ì²´ê³„ì ì¸ í‰ê°€ë¥¼ í†µí•´ LLM ê¸°ë°˜ ê·¸ë˜í”„ ì¶”ë¡  ë°©ë²•ì˜ ê°•ì ê³¼ ì•½ì ì„ ë¶„ì„í•˜ì˜€ë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì§€ì¹¨ì„ ì œê³µí•˜ë©°, í–¥í›„ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ì£¼ìš” ì„¤ê³„ ì›ì¹™ì„ ê°•ì¡°í•œë‹¤.


---

*Generated on 2025-09-24 15:39:50*