<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:46:09.604201",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Machine Learning",
    "XGBoost",
    "Feature Selection",
    "Hyperparameter Tuning",
    "Ensemble Methods"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Machine Learning": 0.85,
    "XGBoost": 0.75,
    "Feature Selection": 0.8,
    "Hyperparameter Tuning": 0.82,
    "Ensemble Methods": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Machine Learning",
        "canonical": "Machine Learning",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Machine Learning is a central theme of the paper, connecting it to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "XGBoost",
        "canonical": "XGBoost",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "XGBoost is highlighted as a key model in the study, offering a specific connection to ensemble learning techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Feature Selection",
        "canonical": "Feature Selection",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Feature Selection is crucial for the study's methodology, linking to broader discussions on data preprocessing.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Hyperparameter Tuning",
        "canonical": "Hyperparameter Tuning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Hyperparameter Tuning is essential for model optimization, providing a link to advanced model training techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Ensemble Methods",
        "canonical": "Ensemble Methods",
        "aliases": [
          "Ensemble Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Ensemble Methods are emphasized for their superior performance, connecting to studies on model improvement strategies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.76,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "Decision Tree",
      "K-Nearest Neighbors",
      "F1-score"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Machine Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "XGBoost",
      "resolved_canonical": "XGBoost",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Feature Selection",
      "resolved_canonical": "Feature Selection",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Hyperparameter Tuning",
      "resolved_canonical": "Hyperparameter Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Ensemble Methods",
      "resolved_canonical": "Ensemble Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.76,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18124.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18124](https://arxiv.org/abs/2509.18124)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques_20250917|Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques]] (79.0% similar)
- [[2025-09-24/Hierarchical Evaluation Function_ A Multi-Metric Approach for Optimizing Demand Forecasting Models_20250924|Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand Forecasting Models]] (78.9% similar)
- [[2025-09-24/From "What to Eat?" to Perfect Recipe_ ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation_20250924|From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation]] (78.8% similar)
- [[2025-09-23/The Impact of Feature Scaling In Machine Learning_ Effects on Regression and Classification Tasks_20250923|The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks]] (78.3% similar)
- [[2025-09-23/AICO_ Feature Significance Tests for Supervised Learning_20250923|AICO: Feature Significance Tests for Supervised Learning]] (78.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Machine Learning|Machine Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Feature Selection|Feature Selection]], [[keywords/Hyperparameter Tuning|Hyperparameter Tuning]], [[keywords/Ensemble Methods|Ensemble Methods]]
**âš¡ Unique Technical**: [[keywords/XGBoost|XGBoost]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18124v1 Announce Type: new 
Abstract: This study explores the application of supervised machine learning algorithms to predict coffee ratings based on a combination of influential textual and numerical attributes extracted from user reviews. Through careful data preprocessing including text cleaning, feature extraction using TF-IDF, and selection with SelectKBest, the study identifies key factors contributing to coffee quality assessments. Six models (Decision Tree, KNearest Neighbors, Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained and evaluated using optimized hyperparameters. Model performance was assessed primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as Multi-layer Perceptron, consistently outperform simpler classifiers (Decision Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1 scores, G-mean and AUC. The findings highlight the essence of rigorous feature selection and hyperparameter tuning in building robust predictive systems for sensory product evaluation, offering a data driven approach to complement traditional coffee cupping by expertise of trained professionals.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì‚¬ìš©ì ë¦¬ë·°ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ë° ìˆ˜ì¹˜ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¤í”¼ í‰ì ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ í…ìŠ¤íŠ¸ ì •ë¦¬, TF-IDFë¥¼ í†µí•œ íŠ¹ì§• ì¶”ì¶œ, SelectKBestë¥¼ í†µí•œ íŠ¹ì§• ì„ íƒì„ ìˆ˜í–‰í•˜ì—¬ ì»¤í”¼ í’ˆì§ˆ í‰ê°€ì— ê¸°ì—¬í•˜ëŠ” ì£¼ìš” ìš”ì†Œë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤. ì˜ì‚¬ê²°ì •ë‚˜ë¬´, K-ìµœê·¼ì ‘ ì´ì›ƒ, ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ , ëœë¤ í¬ë ˆìŠ¤íŠ¸, ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬, XGBoost ë“± 6ê°œì˜ ëª¨ë¸ì„ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ ë° í‰ê°€í–ˆìœ¼ë©°, F1-score, Gmean, AUCë¥¼ ì£¼ìš” ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì•™ìƒë¸” ë°©ë²•(ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬, ëœë¤ í¬ë ˆìŠ¤íŠ¸, XGBoost)ê³¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì´ ë‹¨ìˆœí•œ ë¶„ë¥˜ê¸°(ì˜ì‚¬ê²°ì •ë‚˜ë¬´, K-ìµœê·¼ì ‘ ì´ì›ƒ)ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” íŠ¹ì§• ì„ íƒê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ì „í†µì ì¸ ì»¤í”¼ ê°ë³„ì„ ë³´ì™„í•˜ëŠ” ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì‚¬ìš©ì ë¦¬ë·°ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ë° ìˆ˜ì¹˜ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¤í”¼ í‰ì ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
- 2. í…ìŠ¤íŠ¸ ì •ë¦¬, TF-IDFë¥¼ í™œìš©í•œ íŠ¹ì§• ì¶”ì¶œ, SelectKBestë¥¼ í†µí•œ ì„ íƒ ë“± ì‹ ì¤‘í•œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ì»¤í”¼ í’ˆì§ˆ í‰ê°€ì— ê¸°ì—¬í•˜ëŠ” ì£¼ìš” ìš”ì†Œë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.
- 3. Decision Tree, K-Nearest Neighbors, Multi-layer Perceptron, Random Forest, Extra Trees, XGBoost ë“± ì—¬ì„¯ ê°€ì§€ ëª¨ë¸ì„ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ ë° í‰ê°€í–ˆìŠµë‹ˆë‹¤.
- 4. ê²°ê³¼ì— ë”°ë¥´ë©´, Extra Trees, Random Forest, XGBoostì™€ ê°™ì€ ì•™ìƒë¸” ë°©ë²•ê³¼ Multi-layer Perceptronì´ F1-score, G-mean, AUC ë“±ì˜ í‰ê°€ ì§€í‘œì—ì„œ ë‹¨ìˆœ ë¶„ë¥˜ê¸°ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ê°ê° ì œí’ˆ í‰ê°€ë¥¼ ìœ„í•œ ê°•ë ¥í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•ì— ìˆì–´ ì² ì €í•œ íŠ¹ì§• ì„ íƒê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ì „í†µì ì¸ ì»¤í”¼ ê°ë³„ì„ ë³´ì™„í•˜ëŠ” ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:46:09*