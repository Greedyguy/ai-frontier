<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:21:09.519568",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "3D Scene Reconstruction",
    "Video Diffusion Model",
    "Self-Distillation",
    "3D Gaussian Splatting",
    "Dynamic 3D Scene Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "3D Scene Reconstruction": 0.82,
    "Video Diffusion Model": 0.79,
    "Self-Distillation": 0.78,
    "3D Gaussian Splatting": 0.81,
    "Dynamic 3D Scene Generation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "3D Scene Reconstruction",
        "canonical": "3D Scene Reconstruction",
        "aliases": [
          "3D Reconstruction"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper and connects to various fields like gaming and robotics.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Video Diffusion Model",
        "canonical": "Video Diffusion Model",
        "aliases": [
          "Video Diffusion"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel approach in the paper, linking to advancements in diffusion models.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "Self-Distillation",
        "canonical": "Self-Distillation",
        "aliases": [
          "Self-Distillation Framework"
        ],
        "category": "specific_connectable",
        "rationale": "A key technique in the paper that enhances model training, relevant to self-supervised learning.",
        "novelty_score": 0.61,
        "connectivity_score": 0.77,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "3D Gaussian Splatting",
        "canonical": "3D Gaussian Splatting",
        "aliases": [
          "3DGS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a specific representation method that is novel and central to the paper's methodology.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.81
      },
      {
        "surface": "Dynamic 3D Scene Generation",
        "canonical": "Dynamic 3D Scene Generation",
        "aliases": [
          "Dynamic Scene Generation"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the paper's capability to extend 3D scene generation to dynamic environments.",
        "novelty_score": 0.58,
        "connectivity_score": 0.83,
        "specificity_score": 0.79,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "virtual environments",
      "real-time rendering"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "3D Scene Reconstruction",
      "resolved_canonical": "3D Scene Reconstruction",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Video Diffusion Model",
      "resolved_canonical": "Video Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Self-Distillation",
      "resolved_canonical": "Self-Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.61,
        "connectivity": 0.77,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "3D Gaussian Splatting",
      "resolved_canonical": "3D Gaussian Splatting",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Dynamic 3D Scene Generation",
      "resolved_canonical": "Dynamic 3D Scene Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.83,
        "specificity": 0.79,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19296.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19296](https://arxiv.org/abs/2509.19296)

## 🔗 유사한 논문
- [[2025-09-23/Generating 360{\deg} Video is What You Need For a 3D Scene_20250923|Generating 360{\deg} Video is What You Need For a 3D Scene]] (87.5% similar)
- [[2025-09-19/WorldForge_ Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance_20250919|WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance]] (86.3% similar)
- [[2025-09-19/SPATIALGEN_ Layout-guided 3D Indoor Scene Generation_20250919|SPATIALGEN: Layout-guided 3D Indoor Scene Generation]] (85.6% similar)
- [[2025-09-23/ProDyG_ Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos_20250923|ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos]] (85.5% similar)
- [[2025-09-18/FlightDiffusion_ Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video_20250918|FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video]] (85.4% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/3D Scene Reconstruction|3D Scene Reconstruction]], [[keywords/Self-Distillation|Self-Distillation]], [[keywords/Dynamic 3D Scene Generation|Dynamic 3D Scene Generation]]
**⚡ Unique Technical**: [[keywords/Video Diffusion Model|Video Diffusion Model]], [[keywords/3D Gaussian Splatting|3D Gaussian Splatting]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19296v1 Announce Type: new 
Abstract: The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.

## 📝 요약

이 논문은 게임, 로봇공학, 자율주행 등 다양한 분야에서 중요한 가상 환경 생성을 위한 새로운 방법론을 제안합니다. 기존의 3D 복원 방법은 다중 뷰 데이터에 의존하지만, 이는 항상 쉽게 구할 수 없습니다. 본 연구는 비디오 확산 모델의 2D 한계를 극복하고자, 비디오 확산 모델의 암묵적 3D 지식을 명시적 3D Gaussian Splatting(3DGS) 표현으로 변환하는 자기 증류 프레임워크를 제안합니다. 이 방법은 다중 뷰 학습 데이터 없이도 비디오 확산 모델이 생성한 합성 데이터만으로 3DGS 디코더를 훈련할 수 있습니다. 제안된 모델은 텍스트 프롬프트나 단일 이미지로부터 실시간 3D 장면을 생성할 수 있으며, 단안 비디오 입력으로부터 동적 3D 장면 생성도 가능합니다. 실험 결과, 본 프레임워크는 정적 및 동적 3D 장면 생성에서 최첨단 성능을 달성했습니다.

## 🎯 주요 포인트

- 1. 가상 환경 생성은 게임부터 로봇 공학, 자율 주행, 산업 AI 등 다양한 분야에서 중요하다.
- 2. 현재의 3D 재구성 방법은 다중 뷰 데이터에 의존하지만, 이는 항상 쉽게 구할 수 있는 것은 아니다.
- 3. 본 논문에서는 비디오 확산 모델의 암묵적 3D 지식을 명시적 3D 가우시안 스플래팅 표현으로 증류하는 셀프 디스틸레이션 프레임워크를 제안한다.
- 4. 제안된 프레임워크는 비디오 확산 모델이 생성한 합성 데이터만으로 3D 장면을 실시간으로 렌더링할 수 있다.
- 5. 실험 결과, 제안된 프레임워크는 정적 및 동적 3D 장면 생성에서 최첨단 성능을 달성했다.


---

*Generated on 2025-09-24 16:21:09*