<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:42:12.561837",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Single-stream Policy Optimization",
    "Large Language Model",
    "KL-adaptive value tracker",
    "Advantage Normalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Single-stream Policy Optimization": 0.8,
    "Large Language Model": 0.85,
    "KL-adaptive value tracker": 0.78,
    "Advantage Normalization": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Single-stream Policy Optimization",
        "canonical": "Single-stream Policy Optimization",
        "aliases": [
          "SPO"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach to policy optimization that addresses key limitations in existing methods, offering a unique contribution to the field.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental concept in the paper, connecting to a broad range of research in natural language processing and machine learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "KL-adaptive value tracker",
        "canonical": "KL-adaptive value tracker",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific mechanism introduced in the paper that provides a stable learning signal, crucial for understanding the proposed optimization method.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "advantage normalization",
        "canonical": "Advantage Normalization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A technique that is central to the paper's methodology, offering potential links to other works on optimization techniques in machine learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "baseline estimation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Single-stream Policy Optimization",
      "resolved_canonical": "Single-stream Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "KL-adaptive value tracker",
      "resolved_canonical": "KL-adaptive value tracker",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "advantage normalization",
      "resolved_canonical": "Advantage Normalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Single-stream Policy Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.13232.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.13232](https://arxiv.org/abs/2509.13232)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/NGRPO_ Negative-enhanced Group Relative Policy Optimization_20250924|NGRPO: Negative-enhanced Group Relative Policy Optimization]] (88.3% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (86.6% similar)
- [[2025-09-23/Advancing Speech Understanding in Speech-Aware Language Models with GRPO_20250923|Advancing Speech Understanding in Speech-Aware Language Models with GRPO]] (86.1% similar)
- [[2025-09-24/MAPO_ Mixed Advantage Policy Optimization_20250924|MAPO: Mixed Advantage Policy Optimization]] (85.4% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Advantage Normalization|Advantage Normalization]]
**âš¡ Unique Technical**: [[keywords/Single-stream Policy Optimization|Single-stream Policy Optimization]], [[keywords/KL-adaptive value tracker|KL-adaptive value tracker]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13232v2 Announce Type: replace-cross 
Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì •ì±… ê²½ì‚¬ ìµœì í™”ë¥¼ ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ ê´€ì ì—ì„œ ì¬ê²€í† í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê·¸ë£¹ ê¸°ë°˜ ë°©ë²•ì¸ GRPOëŠ” ë³€ë™ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ ì‹¤ì‹œê°„ ê¸°ì¤€ì„ ì„ ì‚¬ìš©í•˜ì§€ë§Œ, í•™ìŠµ ì‹ í˜¸ ì†Œë©¸ê³¼ ë™ê¸°í™” ë¬¸ì œë¡œ ì¸í•´ í™•ì¥ì„±ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ ì •ì±… ìµœì í™”(SPO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. SPOëŠ” ê·¸ë£¹ ê¸°ì¤€ì„ ì„ KL-ì ì‘í˜• ê°€ì¹˜ ì¶”ì ê¸°ë¡œ ëŒ€ì²´í•˜ê³ , ë°°ì¹˜ ì „ë°˜ì— ê±¸ì³ ì´ì ì„ ì •ê·œí™”í•˜ì—¬ ì•ˆì •ì ì´ê³  ë‚®ì€ ë³€ë™ì„±ì˜ í•™ìŠµ ì‹ í˜¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. SPOëŠ” ê·¸ë£¹ ì—†ì´ë„ ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ê¸´ ì‹œê°„ëŒ€ë‚˜ ë„êµ¬ í†µí•© í™˜ê²½ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ í™•ì¥ë©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SPOëŠ” GRPOë³´ë‹¤ ë” ë§¤ë„ëŸ½ê²Œ ìˆ˜ë ´í•˜ê³  ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ë¶ˆí•„ìš”í•œ ê³„ì‚°ì„ ì œê±°í•©ë‹ˆë‹¤. SPOëŠ” ê¸°ë³¸ ì›ì¹™ì— ê¸°ë°˜í•œ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ, LLM ì¶”ë¡ ì˜ ìƒˆë¡œìš´ ë°œì „ ê²½ë¡œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Qwen3-8Bë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ì—ì„œ SPOëŠ” GRPO ëŒ€ë¹„ í‰ê·  ì •í™•ë„ë¥¼ 3.4%í¬ì¸íŠ¸ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SPOëŠ” ê·¸ë£¹ ê¸°ë°˜ ë°©ë²•ì˜ ë‹¨ì ì„ í•´ê²°í•˜ì—¬ ì•ˆì •ì ì´ê³  ë‚®ì€ ë¶„ì‚°ì˜ í•™ìŠµ ì‹ í˜¸ë¥¼ ì œê³µí•˜ëŠ” ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ ì •ì±… ìµœì í™” ë°©ë²•ì…ë‹ˆë‹¤.
- 2. SPOëŠ” ì§€ì†ì ì¸ KL-ì ì‘í˜• ê°€ì¹˜ ì¶”ì ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë£¹ë³„ ê¸°ì¤€ì„ ì„ ëŒ€ì²´í•˜ê³ , ì „ì—­ì ìœ¼ë¡œ ì´ì ì„ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
- 3. SPOëŠ” ê·¸ë£¹ì´ ì—†ì–´ ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ë‹¤ì–‘í•œ ìƒì„± ì‹œê°„ì˜ ê¸´ ìˆ˜í‰ì„  ë˜ëŠ” ë„êµ¬ í†µí•© ì„¤ì •ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™•ì¥ë©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, SPOëŠ” GRPOë³´ë‹¤ ë” ë¶€ë“œëŸ½ê²Œ ìˆ˜ë ´í•˜ê³  ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, í‡´í™”ëœ ê·¸ë£¹ì— ë‚­ë¹„ë˜ëŠ” ê³„ì‚°ì„ ì œê±°í•©ë‹ˆë‹¤.
- 5. SPOëŠ” ë³µì¡ì„±ì„ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹  ê¸°ë³¸ ì›ì¹™ì— ê¸°ë°˜í•˜ì—¬ LLM ì¶”ë¡ ì˜ ë‹¤ìŒ ë°œì „ì„ ì´ë„ëŠ” ê²½ë¡œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:42:12*