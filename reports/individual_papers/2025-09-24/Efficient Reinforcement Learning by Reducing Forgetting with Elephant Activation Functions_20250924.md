<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:01:05.966071",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Catastrophic Forgetting",
    "Elephant Activation Functions",
    "Sparse Representations"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Catastrophic Forgetting": 0.88,
    "Elephant Activation Functions": 0.92,
    "Sparse Representations": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental concept in the paper, linking to a broad range of related topics in machine learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Catastrophic Forgetting",
        "canonical": "Catastrophic Forgetting",
        "aliases": [
          "Forgetting"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's focus, connecting to studies on neural network memory and learning efficiency.",
        "novelty_score": 0.7,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Elephant Activation Functions",
        "canonical": "Elephant Activation Functions",
        "aliases": [
          "Elephant Functions"
        ],
        "category": "unique_technical",
        "rationale": "A novel concept introduced in the paper, offering new insights into activation function design.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.92
      },
      {
        "surface": "Sparse Representations",
        "canonical": "Sparse Representations",
        "aliases": [
          "Sparse Outputs"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the impact of activation functions on neural network efficiency.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "algorithmic side",
      "training dynamics",
      "value-based algorithms"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Catastrophic Forgetting",
      "resolved_canonical": "Catastrophic Forgetting",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Elephant Activation Functions",
      "resolved_canonical": "Elephant Activation Functions",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Sparse Representations",
      "resolved_canonical": "Sparse Representations",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19159.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19159](https://arxiv.org/abs/2509.19159)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/CUFG_ Curriculum Unlearning Guided by the Forgetting Gradient_20250918|CUFG: Curriculum Unlearning Guided by the Forgetting Gradient]] (83.0% similar)
- [[2025-09-19/Don't Forget the Nonlinearity_ Unlocking Activation Functions in Efficient Fine-Tuning_20250919|Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning]] (82.1% similar)
- [[2025-09-23/Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs_20250923|Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs]] (81.3% similar)
- [[2025-09-19/Sample Efficient Experience Replay in Non-stationary Environments_20250919|Sample Efficient Experience Replay in Non-stationary Environments]] (81.2% similar)
- [[2025-09-22/Latent learning_ episodic memory complements parametric learning by enabling flexible reuse of experiences_20250922|Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Catastrophic Forgetting|Catastrophic Forgetting]], [[keywords/Sparse Representations|Sparse Representations]]
**âš¡ Unique Technical**: [[keywords/Elephant Activation Functions|Elephant Activation Functions]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19159v1 Announce Type: new 
Abstract: Catastrophic forgetting has remained a significant challenge for efficient reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While recent works have proposed effective methods to mitigate this issue, they mainly focus on the algorithmic side. Meanwhile, we do not fully understand what architectural properties of neural networks lead to catastrophic forgetting. This study aims to fill this gap by studying the role of activation functions in the training dynamics of neural networks and their impact on catastrophic forgetting in reinforcement learning setup. Our study reveals that, besides sparse representations, the gradient sparsity of activation functions also plays an important role in reducing forgetting. Based on this insight, we propose a new class of activation functions, elephant activation functions, that can generate both sparse outputs and sparse gradients. We show that by simply replacing classical activation functions with elephant activation functions in the neural networks of value-based algorithms, we can significantly improve the resilience of neural networks to catastrophic forgetting, thus making reinforcement learning more sample-efficient and memory-efficient.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ê°•í™” í•™ìŠµì—ì„œ ë°œìƒí•˜ëŠ” ë§ê° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‹ ê²½ë§ì˜ í™œì„±í™” í•¨ìˆ˜ê°€ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì´ ì£¼ë¡œ ì•Œê³ ë¦¬ì¦˜ ì¸¡ë©´ì— ì§‘ì¤‘í•œ ë°˜ë©´, ë³¸ ì—°êµ¬ëŠ” í™œì„±í™” í•¨ìˆ˜ì˜ ì¶œë ¥ ë° ê¸°ìš¸ê¸° í¬ì†Œì„±ì´ ë§ê° ê°ì†Œì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í¬ì†Œí•œ ì¶œë ¥ê³¼ ê¸°ìš¸ê¸°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ í´ë˜ìŠ¤ì˜ í™œì„±í™” í•¨ìˆ˜ì¸ 'elephant í™œì„±í™” í•¨ìˆ˜'ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë¥¼ ê¸°ì¡´ì˜ í™œì„±í™” í•¨ìˆ˜ ëŒ€ì‹  ì‚¬ìš©í•˜ë©´, ì‹ ê²½ë§ì˜ ë§ê° ì €í•­ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼œ ê°•í™” í•™ìŠµì˜ ìƒ˜í”Œ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¬ë‚œì  ë§ê°ì€ íš¨ìœ¨ì ì¸ ê°•í™” í•™ìŠµì—ì„œ ì—¬ì „íˆ í° ë„ì „ ê³¼ì œë¡œ ë‚¨ì•„ ìˆë‹¤.
- 2. í™œì„±í™” í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í¬ì†Œì„±ì´ ë§ê° ê°ì†Œì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- 3. ìƒˆë¡œìš´ í´ë˜ìŠ¤ì˜ í™œì„±í™” í•¨ìˆ˜ì¸ 'elephant í™œì„±í™” í•¨ìˆ˜'ë¥¼ ì œì•ˆí•˜ì—¬ ë§ê° ì €í•­ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 4. elephant í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê°•í™” í•™ìŠµì˜ ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆë‹¤.
- 5. ê¸°ì¡´ í™œì„±í™” í•¨ìˆ˜ë¥¼ elephant í™œì„±í™” í•¨ìˆ˜ë¡œ ëŒ€ì²´í•¨ìœ¼ë¡œì¨ ì‹ ê²½ë§ì˜ ë§ê° ì €í•­ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-24 15:01:05*