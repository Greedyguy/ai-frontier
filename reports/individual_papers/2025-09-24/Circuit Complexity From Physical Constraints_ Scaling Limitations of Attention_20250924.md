<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:17:24.953798",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "Circuit Complexity",
    "Transformer Expressivity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.85,
    "Circuit Complexity": 0.78,
    "Transformer Expressivity": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "attention mechanisms",
        "canonical": "Attention Mechanism",
        "aliases": [
          "attention"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial for understanding the scaling limitations discussed in the paper.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "circuit complexity",
        "canonical": "Circuit Complexity",
        "aliases": [
          "complexity of circuits"
        ],
        "category": "unique_technical",
        "rationale": "Circuit complexity is central to the paper's argument about physical constraints.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "transformer expressivity",
        "canonical": "Transformer Expressivity",
        "aliases": [
          "expressivity of transformers"
        ],
        "category": "unique_technical",
        "rationale": "Understanding transformer expressivity is key to evaluating model limitations.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "model expressivity",
      "scaling physical circuits"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "attention mechanisms",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "circuit complexity",
      "resolved_canonical": "Circuit Complexity",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "transformer expressivity",
      "resolved_canonical": "Transformer Expressivity",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Circuit Complexity From Physical Constraints: Scaling Limitations of Attention

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19161.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19161](https://arxiv.org/abs/2509.19161)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Towards Interpretable and Efficient Attention_ Compressing All by Contracting a Few_20250923|Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few]] (82.5% similar)
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (82.3% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop: A Novel Regularization Method for Transformer Models]] (80.6% similar)
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (80.1% similar)
- [[2025-09-18/Attention Beyond Neighborhoods_ Reviving Transformer for Graph Clustering_20250918|Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering]] (80.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Circuit Complexity|Circuit Complexity]], [[keywords/Transformer Expressivity|Transformer Expressivity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19161v1 Announce Type: cross 
Abstract: We argue that the standard circuit complexity measures derived from $NC, AC, TC$ provide limited practical information and are now insufficient to further differentiate model expressivity. To address these new limitations, we define a novel notion of local uniformity and a family of circuit complexity classes $RC(\cdot)$ that capture the fundamental constraints of scaling physical circuits. Through the lens of $RC(\cdot)$, we show that attention mechanisms with $\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy of increasingly complex datasets. Our results simultaneously provide a methodology for defining meaningful bounds on transformer expressivity and naturally expose the restricted viability of attention.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ íšŒë¡œ ë³µì¡ë„ ì¸¡ì • ë°©ë²•($NC, AC, TC$)ì´ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ êµ¬ë³„í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŒì„ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì§€ì—­ì  ê· ì¼ì„± ê°œë…ê³¼ $RC(\cdot)$ë¼ëŠ” íšŒë¡œ ë³µì¡ë„ í´ë˜ìŠ¤ ê°€ì¡±ì„ ì •ì˜í•©ë‹ˆë‹¤. $RC(\cdot)$ë¥¼ í†µí•´ $\omega(n^{3/2})$ ì‹¤í–‰ ì‹œê°„ì„ ê°–ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ë³µì¡í•œ ë°ì´í„°ì…‹ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í‘œí˜„ë ¥ì— ëŒ€í•œ ìœ ì˜ë¯¸í•œ ê²½ê³„ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œê³µí•˜ë©°, ì–´í…ì…˜ì˜ ì œí•œëœ ê°€ëŠ¥ì„±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ íšŒë¡œ ë³µì¡ë„ ì¸¡ì • ë°©ë²•ì¸ $NC, AC, TC$ëŠ” ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ êµ¬ë¶„í•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤.
- 2. ìƒˆë¡œìš´ ê°œë…ì¸ ì§€ì—­ì  ê· ì¼ì„±(local uniformity)ê³¼ íšŒë¡œ ë³µì¡ë„ í´ë˜ìŠ¤ $RC(\cdot)$ë¥¼ ì •ì˜í•˜ì—¬ ë¬¼ë¦¬ì  íšŒë¡œì˜ í™•ì¥ ì œì•½ì„ í¬ì°©í•œë‹¤.
- 3. $RC(\cdot)$ë¥¼ í†µí•´ $\omega(n^{3/2})$ ëŸ°íƒ€ì„ì„ ê°€ì§„ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜(attention mechanisms)ì€ ë³µì¡í•œ ë°ì´í„°ì…‹ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒì„ ë³´ì¸ë‹¤.
- 4. ì—°êµ¬ ê²°ê³¼ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í‘œí˜„ë ¥ì— ëŒ€í•œ ì˜ë¯¸ ìˆëŠ” ê²½ê³„ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œê³µí•œë‹¤.
- 5. ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì œí•œëœ ì‹¤í–‰ ê°€ëŠ¥ì„±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚¸ë‹¤.


---

*Generated on 2025-09-24 15:17:24*