<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:49:32.403995",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cognitive Load Theory",
    "Intrinsic Difficulty",
    "Distractor-to-Signal Ratio",
    "Task Length"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Cognitive Load Theory": 0.8,
    "Intrinsic Difficulty": 0.7,
    "Distractor-to-Signal Ratio": 0.72,
    "Task Length": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Essential for linking discussions on language model capabilities and limitations.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cognitive Load Theory",
        "canonical": "Cognitive Load Theory",
        "aliases": [
          "CLT"
        ],
        "category": "unique_technical",
        "rationale": "Central to the benchmark's design and analysis, providing a theoretical framework.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Intrinsic Difficulty",
        "canonical": "Intrinsic Difficulty",
        "aliases": [
          "Intrinsic Complexity"
        ],
        "category": "unique_technical",
        "rationale": "Represents a key parameter in the benchmark affecting LLM performance.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Distractor-to-Signal Ratio",
        "canonical": "Distractor-to-Signal Ratio",
        "aliases": [
          "Distractor Ratio"
        ],
        "category": "unique_technical",
        "rationale": "Critical for understanding extraneous load in cognitive tasks.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Task Length",
        "canonical": "Task Length",
        "aliases": [
          "Task Duration"
        ],
        "category": "unique_technical",
        "rationale": "Identified as a dominant constraint in LLM performance within the benchmark.",
        "novelty_score": 0.6,
        "connectivity_score": 0.55,
        "specificity_score": 0.7,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "performance",
      "analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cognitive Load Theory",
      "resolved_canonical": "Cognitive Load Theory",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Intrinsic Difficulty",
      "resolved_canonical": "Intrinsic Difficulty",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Distractor-to-Signal Ratio",
      "resolved_canonical": "Distractor-to-Signal Ratio",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Task Length",
      "resolved_canonical": "Task Length",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.55,
        "specificity": 0.7,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18458.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18458](https://arxiv.org/abs/2509.18458)

## 🔗 유사한 논문
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (87.8% similar)
- [[2025-09-23/seqBench_ A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs_20250923|seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs]] (87.5% similar)
- [[2025-09-23/How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark_20250923|How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark]] (86.8% similar)
- [[2025-09-23/TurnaboutLLM_ A Deductive Reasoning Benchmark from Detective Games_20250923|TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games]] (86.0% similar)
- [[2025-09-23/Reasoning Core_ A Scalable RL Environment for LLM Symbolic Reasoning_20250923|Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning]] (85.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Cognitive Load Theory|Cognitive Load Theory]], [[keywords/Intrinsic Difficulty|Intrinsic Difficulty]], [[keywords/Distractor-to-Signal Ratio|Distractor-to-Signal Ratio]], [[keywords/Task Length|Task Length]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18458v1 Announce Type: cross 
Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs) often blur critical factors like intrinsic task complexity, distractor interference, and task length. To enable more precise failure analysis, we introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load Theory (CLT). CogniLoad generates natural-language logic puzzles with independently tunable parameters that reflect CLT's core dimensions: intrinsic difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$) regulates extraneous load; and task length ($N$) serves as an operational proxy for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs, CogniLoad reveals distinct performance sensitivities, identifying task length as a dominant constraint and uncovering varied tolerances to intrinsic complexity and U-shaped responses to distractor ratios. By offering systematic, factorial control over these cognitive load dimensions, CogniLoad provides a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.

## 📝 요약

CogniLoad는 대형 언어 모델(LLM)의 장문 맥락 추론 능력을 평가하기 위한 새로운 벤치마크로, 인지 부하 이론(CLT)에 기반하여 설계되었습니다. 이 벤치마크는 내재적 난이도, 방해 요소 비율, 과제 길이 등 CLT의 핵심 차원을 조절 가능한 매개변수로 설정하여 자연어 논리 퍼즐을 생성합니다. 22개의 최첨단 추론 LLM을 평가한 결과, 과제 길이가 주요 제약 조건임을 밝혔고, 내재적 복잡성과 방해 요소 비율에 대한 다양한 민감도를 확인했습니다. CogniLoad는 이러한 인지 부하 차원을 체계적으로 제어하여 LLM의 추론 한계를 분석하고 향후 모델 개발을 위한 방향성을 제시하는 도구로 활용될 수 있습니다.

## 🎯 주요 포인트

- 1. CogniLoad는 인지 부하 이론(CLT)에 기반한 새로운 벤치마크로, 자연어 논리 퍼즐을 생성하여 LLM의 긴 문맥 추론 능력을 평가합니다.
- 2. CogniLoad는 내재적 난이도, 방해 요소 대 신호 비율, 과제 길이 등 CLT의 핵심 차원을 조절 가능한 매개변수로 반영합니다.
- 3. 22개의 최첨단 추론 LLM을 평가한 결과, 과제 길이가 주요 제약 조건으로 드러났으며, 내재적 복잡성과 방해 요소 비율에 대한 다양한 내성을 확인했습니다.
- 4. CogniLoad는 인지 부하 차원에 대한 체계적이고 요인적인 통제를 제공하여 LLM의 추론 한계를 분석하고 향후 모델 개발을 안내하는 도구로 활용됩니다.


---

*Generated on 2025-09-24 13:49:32*