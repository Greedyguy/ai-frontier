<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:50:10.745562",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Chain-of-Thought",
    "Large Language Model",
    "Continuous Space",
    "Self-Distillation",
    "Implicit Chain-of-Thought"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Chain-of-Thought": 0.82,
    "Large Language Model": 0.88,
    "Continuous Space": 0.78,
    "Self-Distillation": 0.8,
    "Implicit Chain-of-Thought": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Chain-of-Thought is a unique reasoning framework that enhances LLMs by encouraging step-by-step reasoning, crucial for understanding the paper's contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion, providing a basis for the proposed reasoning framework.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.88
      },
      {
        "surface": "Continuous Space",
        "canonical": "Continuous Space",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Continuous Space is a novel concept in the paper, representing the latent space where reasoning is compressed.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Self-Distillation",
        "canonical": "Self-Distillation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Self-Distillation is a key mechanism in the paper for transferring reasoning ability from explicit to implicit CoT.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.76,
        "link_intent_score": 0.8
      },
      {
        "surface": "Implicit CoT",
        "canonical": "Implicit Chain-of-Thought",
        "aliases": [
          "Implicit CoT"
        ],
        "category": "unique_technical",
        "rationale": "Implicit CoT is a variant of the Chain-of-Thought reasoning that operates in continuous space, central to the paper's innovation.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Continuous Space",
      "resolved_canonical": "Continuous Space",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Self-Distillation",
      "resolved_canonical": "Self-Distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.76,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Implicit CoT",
      "resolved_canonical": "Implicit Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.21074.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2502.21074](https://arxiv.org/abs/2502.21074)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Soft Tokens, Hard Truths_20250924|Soft Tokens, Hard Truths]] (84.8% similar)
- [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (84.8% similar)
- [[2025-09-23/EvoCoT_ Overcoming the Exploration Bottleneck in Reinforcement Learning_20250923|EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning]] (84.6% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (84.3% similar)
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/Continuous Space|Continuous Space]], [[keywords/Self-Distillation|Self-Distillation]], [[keywords/Implicit Chain-of-Thought|Implicit Chain-of-Thought]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.21074v3 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by encouraging step-by-step reasoning in natural language. However, leveraging a latent continuous space for reasoning may offer benefits in terms of both efficiency and robustness. Prior implicit CoT methods attempt to bypass language completely by reasoning in continuous space but have consistently underperformed compared to the standard explicit CoT approach. We introduce CODI (Continuous Chain-of-Thought via Self-Distillation), a novel training framework that effectively compresses natural language CoT into continuous space. CODI jointly trains a teacher task (Explicit CoT) and a student task (Implicit CoT), distilling the reasoning ability from language into continuous space by aligning the hidden states of a designated token. Our experiments show that CODI is the first implicit CoT approach to match the performance of explicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and outperforming the previous state-of-the-art by 28.2% in accuracy. CODI also demonstrates robustness, generalizable to complex datasets, and interpretability. These results validate that LLMs can reason effectively not only in natural language, but also in a latent continuous space. Code is available at https://github.com/zhenyi4/codi.

## ğŸ“ ìš”ì•½

Chain-of-Thought (CoT) ì¶”ë¡ ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìì—°ì–´ ë‹¨ê³„ë³„ ì¶”ë¡ ì„ ê°•í™”í•˜ì§€ë§Œ, ì—°ì†ì ì¸ ì ì¬ ê³µê°„ì„ í™œìš©í•œ ì¶”ë¡ ì€ íš¨ìœ¨ì„±ê³¼ ê²¬ê³ ì„± ë©´ì—ì„œ ì´ì ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì•”ë¬µì  CoT ë°©ë²•ì€ ì—°ì† ê³µê°„ì—ì„œ ì¶”ë¡ í•˜ë ¤ í–ˆìœ¼ë‚˜ ëª…ì‹œì  CoT ì ‘ê·¼ë²•ì— ë¹„í•´ ì„±ëŠ¥ì´ ë‚®ì•˜ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” CODI(Continuous Chain-of-Thought via Self-Distillation)ë¼ëŠ” ìƒˆë¡œìš´ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. CODIëŠ” ìì—°ì–´ CoTë¥¼ ì—°ì† ê³µê°„ìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•˜ë©°, êµì‚¬ ì‘ì—…(ëª…ì‹œì  CoT)ê³¼ í•™ìƒ ì‘ì—…(ì•”ë¬µì  CoT)ì„ ê³µë™ í›ˆë ¨í•˜ì—¬ ì§€ì •ëœ í† í°ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ì •ë ¬í•¨ìœ¼ë¡œì¨ ì¶”ë¡  ëŠ¥ë ¥ì„ ì¦ë¥˜í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CODIëŠ” GPT-2 ê·œëª¨ì—ì„œ GSM8k ë°ì´í„°ì…‹ì— ëŒ€í•´ ëª…ì‹œì  CoTì™€ ë™ë“±í•œ ì„±ëŠ¥ì„ ìµœì´ˆë¡œ ë‹¬ì„±í–ˆìœ¼ë©°, 3.1ë°° ì••ì¶•ë¥ ê³¼ 28.2%ì˜ ì •í™•ë„ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. CODIëŠ” ë³µì¡í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ë„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” LLMì´ ìì—°ì–´ë¿ë§Œ ì•„ë‹ˆë¼ ì ì¬ ì—°ì† ê³µê°„ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆìŒì„ í™•ì¸ì‹œì¼œ ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CODIëŠ” ìì—°ì–´ ì—°ì‡„ ì‚¬ê³ (CoT)ë¥¼ ì—°ì† ê³µê°„ìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ìƒˆë¡œìš´ í›ˆë ¨ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. CODIëŠ” Explicit CoT(êµì‚¬ ì‘ì—…)ì™€ Implicit CoT(í•™ìƒ ì‘ì—…)ë¥¼ ê³µë™ìœ¼ë¡œ í›ˆë ¨í•˜ì—¬ ì–¸ì–´ ì¶”ë¡  ëŠ¥ë ¥ì„ ì—°ì† ê³µê°„ìœ¼ë¡œ ì¦ë¥˜í•©ë‹ˆë‹¤.
- 3. CODIëŠ” GPT-2 ê·œëª¨ì—ì„œ GSM8k ë°ì´í„°ì…‹ì— ëŒ€í•´ Explicit CoTì™€ ë™ë“±í•œ ì„±ëŠ¥ì„ ì²˜ìŒìœ¼ë¡œ ë‹¬ì„±í•˜ì˜€ìœ¼ë©°, 3.1ë°°ì˜ ì••ì¶•ë¥ ê³¼ 28.2%ì˜ ì •í™•ë„ í–¥ìƒì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.
- 4. CODIëŠ” ë³µì¡í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ë©°, ì—°ì† ê³µê°„ì—ì„œë„ íš¨ê³¼ì ì¸ ì¶”ë¡ ì´ ê°€ëŠ¥í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ìì—°ì–´ë¿ë§Œ ì•„ë‹ˆë¼ ì ì¬ ì—°ì† ê³µê°„ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆìŒì„ í™•ì¸í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:50:10*