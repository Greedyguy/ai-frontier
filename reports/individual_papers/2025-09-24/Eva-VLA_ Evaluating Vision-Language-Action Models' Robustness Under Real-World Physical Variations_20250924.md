<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:07:59.407202",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Physical Variations",
    "3D Transformations",
    "Adversarial Patches",
    "Black-Box Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.88,
    "Physical Variations": 0.8,
    "3D Transformations": 0.79,
    "Adversarial Patches": 0.77,
    "Black-Box Optimization": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language-Action models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLA models"
        ],
        "category": "evolved_concepts",
        "rationale": "This term represents a specific evolution in multimodal learning, linking vision and language with action, which is crucial for robotic manipulation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "real-world physical variations",
        "canonical": "Physical Variations",
        "aliases": [
          "real-world variations"
        ],
        "category": "unique_technical",
        "rationale": "Understanding and modeling physical variations is key to improving the robustness of VLA models in real-world applications.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "object 3D transformations",
        "canonical": "3D Transformations",
        "aliases": [
          "object transformations"
        ],
        "category": "specific_connectable",
        "rationale": "3D transformations are critical for spatial reasoning in computer vision and robotics, providing a strong link to existing research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "adversarial patches",
        "canonical": "Adversarial Patches",
        "aliases": [
          "adversarial examples"
        ],
        "category": "specific_connectable",
        "rationale": "Adversarial patches are a well-known challenge in computer vision, linking to research on model robustness and security.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "continuous black-box optimization",
        "canonical": "Black-Box Optimization",
        "aliases": [
          "continuous optimization"
        ],
        "category": "specific_connectable",
        "rationale": "This optimization technique is vital for efficiently exploring worst-case scenarios in model evaluation.",
        "novelty_score": 0.64,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "robustness",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language-Action models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "real-world physical variations",
      "resolved_canonical": "Physical Variations",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "object 3D transformations",
      "resolved_canonical": "3D Transformations",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "adversarial patches",
      "resolved_canonical": "Adversarial Patches",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "continuous black-box optimization",
      "resolved_canonical": "Black-Box Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.64,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18953.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18953](https://arxiv.org/abs/2509.18953)

## 🔗 유사한 논문
- [[2025-09-23/Evo-0_ Vision-Language-Action Model with Implicit Spatial Understanding_20250923|Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding]] (87.3% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (86.5% similar)
- [[2025-09-19/ForceVLA_ Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation_20250919|ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (86.1% similar)
- [[2025-09-19/CollabVLA_ Self-Reflective Vision-Language-Action Model Dreaming Together with Human_20250919|CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human]] (86.1% similar)
- [[2025-09-24/VLA-LPAF_ Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation_20250924|VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation]] (86.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/3D Transformations|3D Transformations]], [[keywords/Adversarial Patches|Adversarial Patches]], [[keywords/Black-Box Optimization|Black-Box Optimization]]
**⚡ Unique Technical**: [[keywords/Physical Variations|Physical Variations]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18953v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.

## 📝 요약

Eva-VLA는 Vision-Language-Action(VLA) 모델의 현실 세계 물리적 변이에 대한 강건성을 평가하는 최초의 통합 프레임워크입니다. 이 연구는 VLA 모델의 강건성을 평가하기 위해 물리적 변이를 연속적인 최적화 문제로 변환합니다. 주요 도전 과제는 다양한 물리적 변이를 체계적으로 특성화하고, 비용 효율적으로 최악의 시나리오를 발견하는 것입니다. 이를 위해 객체 3D 변환, 조명 변화, 적대적 패치를 포함한 세 가지 주요 변이를 분석하고, 연속적인 블랙박스 최적화 프레임워크를 도입합니다. 실험 결과, 모든 변이 유형에서 60% 이상의 실패율을 보였으며, 특히 객체 변환은 최대 97.8%의 실패율을 초래했습니다. 이 연구는 실험실 성공과 실제 배포 준비 간의 중요한 격차를 드러내며, Eva-VLA는 이러한 격차를 줄이기 위한 실용적인 방법을 제시합니다.

## 🎯 주요 포인트

- 1. Eva-VLA는 Vision-Language-Action(VLA) 모델의 강건성을 평가하는 최초의 통합 프레임워크로, 물리적 변화를 연속적인 최적화 문제로 변환하여 평가합니다.
- 2. VLA 모델의 강건성을 평가하는 주요 도전 과제는 다양한 물리적 변화를 체계적으로 특성화하고, 실세계 데이터 수집 비용 없이 최악의 시나리오를 효율적으로 발견하는 것입니다.
- 3. 실험 결과, 최신 OpenVLA 모델이 다양한 변형 유형에 대해 60% 이상의 실패율을 보였으며, 특히 객체 변환은 장기 작업에서 최대 97.8%의 실패율을 초래했습니다.
- 4. Eva-VLA 프레임워크는 VLA 기반 로봇 조작 모델을 실세계 배포 문제에 대비하여 강화할 수 있는 실용적인 경로를 제공합니다.


---

*Generated on 2025-09-24 14:07:59*