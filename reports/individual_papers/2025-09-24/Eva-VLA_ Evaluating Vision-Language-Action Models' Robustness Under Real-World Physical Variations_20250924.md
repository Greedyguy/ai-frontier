<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:07:59.407202",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Physical Variations",
    "3D Transformations",
    "Adversarial Patches",
    "Black-Box Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.88,
    "Physical Variations": 0.8,
    "3D Transformations": 0.79,
    "Adversarial Patches": 0.77,
    "Black-Box Optimization": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language-Action models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLA models"
        ],
        "category": "evolved_concepts",
        "rationale": "This term represents a specific evolution in multimodal learning, linking vision and language with action, which is crucial for robotic manipulation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "real-world physical variations",
        "canonical": "Physical Variations",
        "aliases": [
          "real-world variations"
        ],
        "category": "unique_technical",
        "rationale": "Understanding and modeling physical variations is key to improving the robustness of VLA models in real-world applications.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "object 3D transformations",
        "canonical": "3D Transformations",
        "aliases": [
          "object transformations"
        ],
        "category": "specific_connectable",
        "rationale": "3D transformations are critical for spatial reasoning in computer vision and robotics, providing a strong link to existing research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "adversarial patches",
        "canonical": "Adversarial Patches",
        "aliases": [
          "adversarial examples"
        ],
        "category": "specific_connectable",
        "rationale": "Adversarial patches are a well-known challenge in computer vision, linking to research on model robustness and security.",
        "novelty_score": 0.58,
        "connectivity_score": 0.8,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      },
      {
        "surface": "continuous black-box optimization",
        "canonical": "Black-Box Optimization",
        "aliases": [
          "continuous optimization"
        ],
        "category": "specific_connectable",
        "rationale": "This optimization technique is vital for efficiently exploring worst-case scenarios in model evaluation.",
        "novelty_score": 0.64,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "robustness",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language-Action models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "real-world physical variations",
      "resolved_canonical": "Physical Variations",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "object 3D transformations",
      "resolved_canonical": "3D Transformations",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "adversarial patches",
      "resolved_canonical": "Adversarial Patches",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.8,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "continuous black-box optimization",
      "resolved_canonical": "Black-Box Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.64,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18953.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18953](https://arxiv.org/abs/2509.18953)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Evo-0_ Vision-Language-Action Model with Implicit Spatial Understanding_20250923|Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding]] (87.3% similar)
- [[2025-09-19/Manipulation Facing Threats_ Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models_20250919|Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models]] (86.5% similar)
- [[2025-09-19/ForceVLA_ Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation_20250919|ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (86.1% similar)
- [[2025-09-19/CollabVLA_ Self-Reflective Vision-Language-Action Model Dreaming Together with Human_20250919|CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human]] (86.1% similar)
- [[2025-09-24/VLA-LPAF_ Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation_20250924|VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation]] (86.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/3D Transformations|3D Transformations]], [[keywords/Adversarial Patches|Adversarial Patches]], [[keywords/Black-Box Optimization|Black-Box Optimization]]
**âš¡ Unique Technical**: [[keywords/Physical Variations|Physical Variations]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18953v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.

## ğŸ“ ìš”ì•½

Eva-VLAëŠ” Vision-Language-Action(VLA) ëª¨ë¸ì˜ í˜„ì‹¤ ì„¸ê³„ ë¬¼ë¦¬ì  ë³€ì´ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í‰ê°€í•˜ëŠ” ìµœì´ˆì˜ í†µí•© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” VLA ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë¬¼ë¦¬ì  ë³€ì´ë¥¼ ì—°ì†ì ì¸ ìµœì í™” ë¬¸ì œë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì£¼ìš” ë„ì „ ê³¼ì œëŠ” ë‹¤ì–‘í•œ ë¬¼ë¦¬ì  ë³€ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íŠ¹ì„±í™”í•˜ê³ , ë¹„ìš© íš¨ìœ¨ì ìœ¼ë¡œ ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°œê²¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ê°ì²´ 3D ë³€í™˜, ì¡°ëª… ë³€í™”, ì ëŒ€ì  íŒ¨ì¹˜ë¥¼ í¬í•¨í•œ ì„¸ ê°€ì§€ ì£¼ìš” ë³€ì´ë¥¼ ë¶„ì„í•˜ê³ , ì—°ì†ì ì¸ ë¸”ë™ë°•ìŠ¤ ìµœì í™” í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ëª¨ë“  ë³€ì´ ìœ í˜•ì—ì„œ 60% ì´ìƒì˜ ì‹¤íŒ¨ìœ¨ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ ê°ì²´ ë³€í™˜ì€ ìµœëŒ€ 97.8%ì˜ ì‹¤íŒ¨ìœ¨ì„ ì´ˆë˜í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì‹¤í—˜ì‹¤ ì„±ê³µê³¼ ì‹¤ì œ ë°°í¬ ì¤€ë¹„ ê°„ì˜ ì¤‘ìš”í•œ ê²©ì°¨ë¥¼ ë“œëŸ¬ë‚´ë©°, Eva-VLAëŠ” ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì‹¤ìš©ì ì¸ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Eva-VLAëŠ” Vision-Language-Action(VLA) ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í‰ê°€í•˜ëŠ” ìµœì´ˆì˜ í†µí•© í”„ë ˆì„ì›Œí¬ë¡œ, ë¬¼ë¦¬ì  ë³€í™”ë¥¼ ì—°ì†ì ì¸ ìµœì í™” ë¬¸ì œë¡œ ë³€í™˜í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
- 2. VLA ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í‰ê°€í•˜ëŠ” ì£¼ìš” ë„ì „ ê³¼ì œëŠ” ë‹¤ì–‘í•œ ë¬¼ë¦¬ì  ë³€í™”ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íŠ¹ì„±í™”í•˜ê³ , ì‹¤ì„¸ê³„ ë°ì´í„° ìˆ˜ì§‘ ë¹„ìš© ì—†ì´ ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë°œê²¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, ìµœì‹  OpenVLA ëª¨ë¸ì´ ë‹¤ì–‘í•œ ë³€í˜• ìœ í˜•ì— ëŒ€í•´ 60% ì´ìƒì˜ ì‹¤íŒ¨ìœ¨ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ ê°ì²´ ë³€í™˜ì€ ì¥ê¸° ì‘ì—…ì—ì„œ ìµœëŒ€ 97.8%ì˜ ì‹¤íŒ¨ìœ¨ì„ ì´ˆë˜í–ˆìŠµë‹ˆë‹¤.
- 4. Eva-VLA í”„ë ˆì„ì›Œí¬ëŠ” VLA ê¸°ë°˜ ë¡œë´‡ ì¡°ì‘ ëª¨ë¸ì„ ì‹¤ì„¸ê³„ ë°°í¬ ë¬¸ì œì— ëŒ€ë¹„í•˜ì—¬ ê°•í™”í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì ì¸ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:07:59*