<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:50:23.211524",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "In-Context Reinforcement Learning",
    "Transformer",
    "Temporal Difference Learning",
    "Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "In-Context Reinforcement Learning": 0.78,
    "Transformer": 0.85,
    "Temporal Difference Learning": 0.7,
    "Neural Network": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "in-context reinforcement learning",
        "canonical": "In-Context Reinforcement Learning",
        "aliases": [
          "ICRL"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper and represents a novel approach to reinforcement learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a key component in the study and have broad applicability across machine learning tasks.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "temporal difference learning",
        "canonical": "Temporal Difference Learning",
        "aliases": [
          "TD Learning"
        ],
        "category": "specific_connectable",
        "rationale": "This is a specific learning method discussed in the paper that connects to reinforcement learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "neural network parameters",
        "canonical": "Neural Network",
        "aliases": [
          "NN Parameters"
        ],
        "category": "broad_technical",
        "rationale": "Neural networks are fundamental to the study and are widely applicable in machine learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "task",
      "performance",
      "parameter updates"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "in-context reinforcement learning",
      "resolved_canonical": "In-Context Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "temporal difference learning",
      "resolved_canonical": "Temporal Difference Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "neural network parameters",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Towards Provable Emergence of In-Context Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18389.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18389](https://arxiv.org/abs/2509.18389)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models_20250918|Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models]] (83.8% similar)
- [[2025-09-22/RLinf_ Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation_20250922|RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation]] (83.2% similar)
- [[2025-09-22/Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents_20250922|Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents]] (82.8% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (82.7% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Temporal Difference Learning|Temporal Difference Learning]]
**âš¡ Unique Technical**: [[keywords/In-Context Reinforcement Learning|In-Context Reinforcement Learning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18389v1 Announce Type: new 
Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í˜„ëŒ€ ê°•í™” í•™ìŠµ(RL) ì—ì´ì „íŠ¸ê°€ ì‚¬ì „ í•™ìŠµ í›„ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ ì—†ì´ ìƒˆë¡œìš´ ê³¼ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” 'ë§¥ë½ ë‚´ ê°•í™” í•™ìŠµ(ICRL)' í˜„ìƒì„ íƒêµ¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ì‚¬ì „ í•™ìŠµëœ ì—ì´ì „íŠ¸ê°€ ìƒˆë¡œìš´ ê³¼ì œì—ì„œ ì¶”ê°€ ì…ë ¥ì¸ 'ë§¥ë½'ì„ í™œìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì €ìë“¤ì€ ICRLì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë§¤ê°œë³€ìˆ˜ê°€ ì‚¬ì „ í•™ìŠµ ì†ì‹¤ì˜ ìµœì†Œí™” ê²°ê³¼ë¼ê³  ê°€ì •í•˜ê³ , ì´ë¥¼ ì‚¬ë¡€ ì—°êµ¬ë¥¼ í†µí•´ ì´ˆê¸°ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤. íŠ¹íˆ, íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì •ì±… í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ì „ í•™ìŠµë  ë•Œ, ì‚¬ì „ í•™ìŠµ ì†ì‹¤ì˜ ì „ì—­ ìµœì†Œí™”ê°€ ICRLì„ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í˜„ëŒ€ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ëŠ” ì‹ ê²½ë§ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì •ì±…ì„ ì ì‘ì‹œí‚´ìœ¼ë¡œì¨ ê³¼ì œë¥¼ í•´ê²°í•œë‹¤.
- 2. ì‚¬ì „ í›ˆë ¨ëœ ì—ì´ì „íŠ¸ëŠ” íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì—†ì´ ìƒˆë¡œìš´ ê³¼ì œì—ì„œ ë§¥ë½ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì •ì±…ì„ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.
- 3. ë§¥ë½ ì •ë³´ê°€ ì¦ê°€í• ìˆ˜ë¡ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ë©°, ì´ë¥¼ ë§¥ë½ ë‚´ ê°•í™” í•™ìŠµ(ICRL)ì´ë¼ê³  í•œë‹¤.
- 4. ICRL í˜„ìƒì€ ì—ì´ì „íŠ¸ ë„¤íŠ¸ì›Œí¬ì˜ ì‚¬ì „ í›ˆë ¨ëœ íŒŒë¼ë¯¸í„°ì— ì˜í•´ ê°€ëŠ¥í•´ì§„ë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ì‚¬ì „ í›ˆë ¨ ì†ì‹¤ì˜ ìµœì†Œí™”ê°€ ICRLì„ ê°€ëŠ¥ì¼€ í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒì„ ì‚¬ë¡€ ì—°êµ¬ë¥¼ í†µí•´ ì…ì¦í•œë‹¤.


---

*Generated on 2025-09-24 14:50:23*