<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:05:10.543473",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Zero-Shot Learning",
    "Transformer",
    "EndoSynth Dataset",
    "Monocular Depth Estimation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Zero-Shot Learning": 0.78,
    "Transformer": 0.82,
    "EndoSynth Dataset": 0.8,
    "Monocular Depth Estimation": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Zero-shot",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is crucial for understanding the model's ability to generalize to unseen data, which is a key aspect of the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer based networks",
        "canonical": "Transformer",
        "aliases": [
          "Transformer networks"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are foundational to the depth estimation models discussed, linking to broader advancements in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "EndoSynth dataset",
        "canonical": "EndoSynth Dataset",
        "aliases": [
          "EndoSynth"
        ],
        "category": "unique_technical",
        "rationale": "The EndoSynth dataset is a novel contribution of the paper, providing a unique resource for future research in endoscopic image analysis.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Monocular metric depth estimation",
        "canonical": "Monocular Depth Estimation",
        "aliases": [
          "Monocular metric depth"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific focus of the paper, addressing a niche area within computer vision that is essential for the study's context.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "depth estimation models",
      "synthetic dataset",
      "real data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Zero-shot",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer based networks",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "EndoSynth dataset",
      "resolved_canonical": "EndoSynth Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Monocular metric depth estimation",
      "resolved_canonical": "Monocular Depth Estimation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Zero-shot Monocular Metric Depth for Endoscopic Images

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18642.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.18642](https://arxiv.org/abs/2509.18642)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Towards Sharper Object Boundaries in Self-Supervised Depth Estimation_20250922|Towards Sharper Object Boundaries in Self-Supervised Depth Estimation]] (84.3% similar)
- [[2025-09-23/BaseBoostDepth_ Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation_20250923|BaseBoostDepth: Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation]] (84.1% similar)
- [[2025-09-23/Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views_20250923|Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views]] (84.1% similar)
- [[2025-09-19/Depth AnyEvent_ A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation_20250919|Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation]] (83.4% similar)
- [[2025-09-22/Data-Efficient Learning for Generalizable Surgical Video Understanding_20250922|Data-Efficient Learning for Generalizable Surgical Video Understanding]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/EndoSynth Dataset|EndoSynth Dataset]], [[keywords/Monocular Depth Estimation|Monocular Depth Estimation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18642v1 Announce Type: new 
Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in the last few years due to the sharp advancements in foundation models and in particular transformer based networks. As we start to see applications to the domain of endoscopic images, there is still a lack of robust benchmarks and high-quality datasets in that area. This paper addresses these limitations by presenting a comprehensive benchmark of state-of-the-art (metric and relative) depth estimation models evaluated on real, unseen endoscopic images, providing critical insights into their generalisation and performance in clinical scenarios. Additionally, we introduce and publish a novel synthetic dataset (EndoSynth) of endoscopic surgical instruments paired with ground truth metric depth and segmentation masks, designed to bridge the gap between synthetic and real-world data. We demonstrate that fine-tuning depth foundation models using our synthetic dataset boosts accuracy on most unseen real data by a significant margin. By providing both a benchmark and a synthetic dataset, this work advances the field of depth estimation for endoscopic images and serves as an important resource for future research. Project page, EndoSynth dataset and trained weights are available at https://github.com/TouchSurgery/EndoSynth.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‚´ì‹œê²½ ì´ë¯¸ì§€ì—ì„œì˜ ë‹¨ì•ˆ ìƒëŒ€ ë° ì ˆëŒ€ ê¹Šì´ ì¶”ì •ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìµœì‹  ëª¨ë¸ë“¤ì„ ì‹¤ì œ ë‚´ì‹œê²½ ì´ë¯¸ì§€ì—ì„œ í‰ê°€í•œ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ìƒˆë¡œìš´ í•©ì„± ë°ì´í„°ì…‹ì¸ EndoSynthë¥¼ ì†Œê°œí•˜ì—¬, ë‚´ì‹œê²½ ìˆ˜ìˆ  ë„êµ¬ì˜ ê¹Šì´ ë° ë¶„í•  ë§ˆìŠ¤í¬ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì„ í™œìš©í•œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì€ ì‹¤ì œ ë°ì´í„°ì—ì„œì˜ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë‚´ì‹œê²½ ì´ë¯¸ì§€ ê¹Šì´ ì¶”ì • ë¶„ì•¼ë¥¼ ë°œì „ì‹œí‚¤ê³ , í–¥í›„ ì—°êµ¬ì— ì¤‘ìš”í•œ ìì›ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ ëª‡ ë…„ê°„ ë‹¨ì•ˆ ìƒëŒ€ ë° ì ˆëŒ€ ê¹Šì´ ì¶”ì •ì€ ê¸°ì´ˆ ëª¨ë¸ê³¼ íŠ¹íˆ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ì˜ ë°œì „ìœ¼ë¡œ ì¸í•´ í° ë°œì „ì„ ì´ë£¨ì—ˆë‹¤.
- 2. ë‚´ì‹œê²½ ì´ë¯¸ì§€ ë¶„ì•¼ì—ì„œëŠ” ì•„ì§ ê²¬ê³ í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•˜ë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì€ ì‹¤ì œ ë³´ì§€ ëª»í•œ ë‚´ì‹œê²½ ì´ë¯¸ì§€ì—ì„œ ìµœì²¨ë‹¨ ê¹Šì´ ì¶”ì • ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ì„ìƒ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì¼ë°˜í™” ë° ì„±ëŠ¥ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•œë‹¤.
- 4. ìƒˆë¡œìš´ í•©ì„± ë°ì´í„°ì…‹ì¸ EndoSynthë¥¼ ë„ì…í•˜ì—¬ í•©ì„± ë° ì‹¤ì œ ë°ì´í„° ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ê³ , ì´ë¥¼ í†µí•´ ëŒ€ë¶€ë¶„ì˜ ë³´ì§€ ëª»í•œ ì‹¤ì œ ë°ì´í„°ì—ì„œ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ë‚´ì‹œê²½ ì´ë¯¸ì§€ì˜ ê¹Šì´ ì¶”ì • ë¶„ì•¼ë¥¼ ë°œì „ì‹œí‚¤ê³ , í–¥í›„ ì—°êµ¬ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ìì›ì„ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-24 16:05:10*