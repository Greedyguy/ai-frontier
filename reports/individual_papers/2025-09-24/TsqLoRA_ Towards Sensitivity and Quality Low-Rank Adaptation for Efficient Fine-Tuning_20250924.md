<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:55:56.632844",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Natural Language Processing",
    "Fine-Tuning",
    "Low-Rank Adaptation",
    "Sensitivity-Aware Adaptation",
    "Quality-Aware Sampling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Natural Language Processing": 0.8,
    "Fine-Tuning": 0.9,
    "Low-Rank Adaptation": 0.85,
    "Sensitivity-Aware Adaptation": 0.75,
    "Quality-Aware Sampling": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Natural Language Processing",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "This is a fundamental domain for the discussed fine-tuning methods, providing a broad context for linking.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      },
      {
        "surface": "fine-tuning",
        "canonical": "Fine-Tuning",
        "aliases": [
          "model adaptation"
        ],
        "category": "specific_connectable",
        "rationale": "Fine-tuning is central to the paper's methodology, offering strong linkage to related adaptation techniques.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.9
      },
      {
        "surface": "low-rank adaptation",
        "canonical": "Low-Rank Adaptation",
        "aliases": [
          "low-rank model adaptation"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific technique introduced in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "sensitivity-aware",
        "canonical": "Sensitivity-Aware Adaptation",
        "aliases": [
          "sensitivity-based tuning"
        ],
        "category": "unique_technical",
        "rationale": "This concept is novel and central to the paper's approach, offering a new perspective on model adaptation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "quality-aware sampling",
        "canonical": "Quality-Aware Sampling",
        "aliases": [
          "data-quality sampling"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel component of the proposed method, emphasizing the importance of data quality in model training.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Natural Language Processing",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "fine-tuning",
      "resolved_canonical": "Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "low-rank adaptation",
      "resolved_canonical": "Low-Rank Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "sensitivity-aware",
      "resolved_canonical": "Sensitivity-Aware Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "quality-aware sampling",
      "resolved_canonical": "Quality-Aware Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18585.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18585](https://arxiv.org/abs/2509.18585)

## 🔗 유사한 논문
- [[2025-09-23/RefLoRA_ Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models_20250923|RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models]] (86.7% similar)
- [[2025-09-23/Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA_20250923|Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA]] (86.1% similar)
- [[2025-09-23/TASO_ Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation_20250923|TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation]] (85.1% similar)
- [[2025-09-23/SEQR_ Secure and Efficient QR-based LoRA Routing_20250923|SEQR: Secure and Efficient QR-based LoRA Routing]] (84.2% similar)
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (83.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**🔗 Specific Connectable**: [[keywords/Fine-Tuning|Fine-Tuning]]
**⚡ Unique Technical**: [[keywords/Low-Rank Adaptation|Low-Rank Adaptation]], [[keywords/Sensitivity-Aware Adaptation|Sensitivity-Aware Adaptation]], [[keywords/Quality-Aware Sampling|Quality-Aware Sampling]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18585v1 Announce Type: cross 
Abstract: Fine-tuning large pre-trained models for downstream tasks has become a fundamental approach in natural language processing. Fully fine-tuning all model parameters is computationally expensive and memory-intensive, especially in resource-constrained environments. Existing parameter-efficient fine-tuning methods reduce the number of trainable parameters but typically overlook the varying sensitivity of different model layers and the importance of training data. In this work, we propose TsqLoRA, a novel method that integrates data-quality-driven selection with sensitivity-aware low-rank adaptation, consisted of two main components: a quality-aware sampling mechanism for selecting the most informative training data, and a dynamic rank allocation module that adjusts the rank of each layer based on its sensitivity to parameter updates. The experimental results demonstrate that TsqLoRA improves fine-tuning efficiency while maintaining or even improving performance on a variety of NLP tasks. Our code will be available at https://github.com/Benjamin-Ricky/TsqLoRA.

## 📝 요약

대규모 사전 학습 모델의 파인튜닝은 자연어 처리에서 중요한 접근법이지만, 모든 모델 파라미터를 완전히 파인튜닝하는 것은 비용이 많이 들고 메모리를 많이 소모합니다. 이를 해결하기 위해 TsqLoRA라는 새로운 방법을 제안합니다. TsqLoRA는 데이터 품질 기반 선택과 민감도 인식 저랭크 적응을 통합하여, 정보가 풍부한 학습 데이터를 선택하는 품질 인식 샘플링 메커니즘과 각 층의 파라미터 업데이트 민감도에 따라 랭크를 조정하는 동적 랭크 할당 모듈로 구성됩니다. 실험 결과, TsqLoRA는 다양한 NLP 과제에서 성능을 유지하거나 향상시키면서 파인튜닝 효율성을 개선함을 보여줍니다.

## 🎯 주요 포인트

- 1. TsqLoRA는 데이터 품질 기반 선택과 민감도 인식 저랭크 적응을 통합한 새로운 방법을 제안합니다.
- 2. 이 방법은 정보성이 높은 훈련 데이터를 선택하는 품질 인식 샘플링 메커니즘을 포함합니다.
- 3. 각 레이어의 민감도에 따라 랭크를 조정하는 동적 랭크 할당 모듈을 제공합니다.
- 4. TsqLoRA는 다양한 NLP 작업에서 성능을 유지하거나 향상시키면서도 미세 조정 효율성을 개선합니다.
- 5. TsqLoRA의 코드는 https://github.com/Benjamin-Ricky/TsqLoRA에서 제공될 예정입니다.


---

*Generated on 2025-09-24 13:55:56*