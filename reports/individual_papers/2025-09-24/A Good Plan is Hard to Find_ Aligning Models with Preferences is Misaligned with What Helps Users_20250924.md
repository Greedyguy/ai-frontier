<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:40:52.638518",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Planorama",
    "Reinforcement Learning from Human Feedback",
    "ChatbotArena"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Planorama": 0.7,
    "Reinforcement Learning from Human Feedback": 0.8,
    "ChatbotArena": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study as it discusses alignment and helpfulness of LLM-generated plans.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Planorama",
        "canonical": "Planorama",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique interface used in the study to evaluate plan helpfulness and user preferences.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Reinforcement Learning from Human Feedback",
        "canonical": "Reinforcement Learning from Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "specific_connectable",
        "rationale": "A key method discussed for aligning LLM plans with user preferences.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "ChatbotArena",
        "canonical": "ChatbotArena",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific evaluation platform mentioned in the context of user preference testing.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.75,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "plan",
      "user preferences",
      "helpfulness"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Planorama",
      "resolved_canonical": "Planorama",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Human Feedback",
      "resolved_canonical": "Reinforcement Learning from Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "ChatbotArena",
      "resolved_canonical": "ChatbotArena",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.75,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18632.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18632](https://arxiv.org/abs/2509.18632)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Improving User Interface Generation Models from Designer Feedback_20250923|Improving User Interface Generation Models from Designer Feedback]] (86.8% similar)
- [[2025-09-23/Challenging the Evaluator_ LLM Sycophancy Under User Rebuttal_20250923|Challenging the Evaluator: LLM Sycophancy Under User Rebuttal]] (85.9% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (85.9% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (85.2% similar)
- [[2025-09-23/Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM_20250923|Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM]] (84.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning from Human Feedback|Reinforcement Learning from Human Feedback]]
**âš¡ Unique Technical**: [[keywords/Planorama|Planorama]], [[keywords/ChatbotArena|ChatbotArena]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18632v1 Announce Type: new 
Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë³µì¡í•œ ì‘ì—…ì„ ë•ê¸° ìœ„í•´ LLMs(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ê°€ ìƒì„±í•˜ëŠ” ê³„íšì˜ ìœ ìš©ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. Planoramaë¼ëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ 126ëª…ì˜ ì‚¬ìš©ìê°€ 300ê°œì˜ ë‹¤ë‹¨ê³„ ì§ˆë¬¸ì— ëŒ€í•´ LLM ê³„íšì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ë„ë¡ í•˜ì˜€ê³ , 4388ê°œì˜ ê³„íš ì‹¤í–‰ê³¼ 5584ê°œì˜ ë¹„êµë¥¼ í†µí•´ ê³„íšì˜ ìœ ìš©ì„±ê³¼ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì‚¬ìš©ì ë° ëª¨ë¸ì˜ ì„ í˜¸ë„ì™€ ì—ì´ì „íŠ¸ì˜ ì„±ê³µì´ ì‹¤ì œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ê³„íšì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤ëŠ” ì ì„ ë°í˜”ìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ê³„íšì˜ ìœ ìš©ì„± ê°„ì˜ ì°¨ì´ëŠ” í‘œë©´ì ì¸ ìš”ì†Œì— ê¸°ì¸í•˜ë©°, ì‹¤ì œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ í†µí•œ í”¼ë“œë°±ì´ í•„ìš”í•˜ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‚¬ìš©ì ë° ëª¨ë¸ì˜ ì„ í˜¸ë„ì™€ ì—ì´ì „íŠ¸ì˜ ì„±ê³µ ì—¬ë¶€ëŠ” ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ê³„íšì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤.
- 2. ì‚¬ìš©ìë³„ ì„ í˜¸ë„ ì°¨ì´ë¡œ ì¸í•œ ê²ƒì´ ì•„ë‹ˆë©°, ì‚¬ìš©ìëŠ” ì„ í˜¸í•˜ëŠ” ê³„íšê³¼ ì„ í˜¸í•˜ì§€ ì•ŠëŠ” ê³„íšì„ ì‚¬ìš©í•  ë•Œ ìœ ì‚¬í•œ ì„±ê³µë¥ ì„ ë³´ì¸ë‹¤.
- 3. ê°„ê²°í•¨ ë° ì§ˆë¬¸ ìœ ì‚¬ì„±ê³¼ ê°™ì€ í‘œë©´ì  ë‹¨ì„œëŠ” ì„ í˜¸ë„ì™€ ê°•í•˜ê²Œ ì—°ê²°ë˜ì§€ë§Œ, ì´ëŸ¬í•œ í¸í–¥ì€ ìœ ìš©ì„±ì„ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤.
- 4. ìœ ìš©í•œ LLMì„ ì •ë ¬í•˜ë ¤ë©´ ê²‰ë³´ê¸°ì˜ ìœ ìš©ì„±ì— ëŒ€í•œ ì„ í˜¸ë„ê°€ ì•„ë‹Œ ì‹¤ì œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì—ì„œ í”¼ë“œë°±ì„ ë°›ì•„ì•¼ í•œë‹¤.


---

*Generated on 2025-09-24 15:40:52*