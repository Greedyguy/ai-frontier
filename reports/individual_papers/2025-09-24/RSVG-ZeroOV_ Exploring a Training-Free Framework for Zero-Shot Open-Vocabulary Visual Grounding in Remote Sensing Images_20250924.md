<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:00:27.353797",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Zero-Shot Learning",
    "Diffusion Model",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Zero-Shot Learning": 0.8,
    "Diffusion Model": 0.75,
    "Attention Mechanism": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the framework and connect with multimodal learning concepts.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Zero-Shot Learning",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a key aspect of the proposed framework, linking to broader machine learning strategies.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Diffusion Model",
        "canonical": "Diffusion Model",
        "aliases": [
          "DM"
        ],
        "category": "unique_technical",
        "rationale": "Diffusion Models are uniquely used in this framework for enhancing object structure information.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Attention Mechanisms are integral to the model's ability to focus on relevant image regions.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.68,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "training-free",
      "framework",
      "segmentation masks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Zero-Shot Learning",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Diffusion Model",
      "resolved_canonical": "Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.68,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18711.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18711](https://arxiv.org/abs/2509.18711)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval_20250922|Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval]] (86.4% similar)
- [[2025-09-24/VLN-Zero_ Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation_20250924|VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation]] (82.6% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.4% similar)
- [[2025-09-18/Improving Generalized Visual Grounding with Instance-aware Joint Learning_20250918|Improving Generalized Visual Grounding with Instance-aware Joint Learning]] (82.3% similar)
- [[2025-09-22/Sparse Multiview Open-Vocabulary 3D Detection_20250922|Sparse Multiview Open-Vocabulary 3D Detection]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Diffusion Model|Diffusion Model]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18711v1 Announce Type: cross 
Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì›ê²© ê°ì§€ ì´ë¯¸ì§€ì—ì„œ ìì—°ì–´ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ë¥¼ ì°¾ëŠ” ì‘ì—…ì¸ ì›ê²© ê°ì§€ ì‹œê°ì  ê·¸ë¼ìš´ë”©(RSVG)ì— ëŒ€í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë‹«íŒ ì§‘í•© ì–´íœ˜ì— ì œí•œë˜ì–´ ìˆì–´ ê°œë°©í˜• ì„¸ê³„ ì‹œë‚˜ë¦¬ì˜¤ì— ì ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì´ ë…¼ë¬¸ì—ì„œëŠ” ê³ ê°€ì˜ ë°ì´í„°ì…‹ì´ë‚˜ ì‹œê°„ ì†Œëª¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ ë™ì‘í•˜ëŠ” \textbf{RSVG-ZeroOV}ë¼ëŠ” í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤: (i) VLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ì™€ ì‹œê°ì  ì˜ì—­ ê°„ì˜ ì˜ë¯¸ì  ìƒê´€ê´€ê³„ë¥¼ í¬ì°©í•˜ëŠ” êµì°¨ ì£¼ì˜ ë§µì„ ìƒì„±, (ii) í™•ì‚° ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê°ì²´ì˜ êµ¬ì¡°ì  ë° í˜•íƒœ ì •ë³´ë¥¼ ë³´ì™„, (iii) ë¶ˆí•„ìš”í•œ í™œì„±í™”ë¥¼ ì–µì œí•˜ì—¬ ê°ì²´ì˜ ì„¸ë¶„í™” ë§ˆìŠ¤í¬ë¥¼ ê°œì„ í•˜ëŠ” ì£¼ì˜ ì§„í™” ëª¨ë“ˆ ë„ì…. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ê¸°ì¡´ì˜ ì•½ì§€ë„ ë° ì œë¡œìƒ· ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì›ê²© ê°ì§€ ì‹œê°ì  ê·¸ë¼ìš´ë”©(RSVG)ì€ ìì—°ì–´ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ì›ê²© ê°ì§€ ì´ë¯¸ì§€ ë‚´ ê°ì²´ë¥¼ ìœ„ì¹˜ ì§€ì •í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ íì‡„í˜• ì–´íœ˜ì— ì œí•œë˜ì–´ ìˆì–´ ê°œë°©í˜• ì„¸ê³„ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì ìš©ì´ ì œí•œì ì…ë‹ˆë‹¤.
- 3. RSVG-ZeroOVëŠ” ê³ ê°€ì˜ ë°ì´í„°ì…‹ê³¼ ì‹œê°„ ì†Œëª¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ë™ê²°ëœ ì¼ë°˜ ê¸°ì´ˆ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì œë¡œìƒ· ê°œë°©í˜• ì–´íœ˜ RSVGë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. RSVG-ZeroOVëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ê³¼ í™•ì‚° ëª¨ë¸(DM)ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ì™€ ì‹œê°ì  ì˜ì—­ ê°„ì˜ ì˜ë¯¸ì  ìƒê´€ê´€ê³„ë¥¼ í¬ì°©í•˜ê³ , ê°ì²´ì˜ êµ¬ì¡° ë° í˜•íƒœ ì •ë³´ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ê¸°ì¡´ì˜ ì•½ì§€ë„ ë° ì œë¡œìƒ· ë°©ë²•ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:00:27*