<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:03:27.433617",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Dynamic Prompt Fusion",
    "Multi-Task Learning",
    "Cross-Domain Adaptation",
    "Task Interference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Dynamic Prompt Fusion": 0.7,
    "Multi-Task Learning": 0.8,
    "Cross-Domain Adaptation": 0.78,
    "Task Interference": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Essential for linking discussions on advanced natural language processing techniques.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Dynamic Prompt Fusion",
        "canonical": "Dynamic Prompt Fusion",
        "aliases": [
          "Prompt Fusion",
          "Dynamic Prompt Scheduling"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel technique introduced in the paper for improving multi-task learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Multi-Task Learning",
        "canonical": "Multi-Task Learning",
        "aliases": [
          "MTL"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's approach and widely applicable in machine learning contexts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cross-Domain Adaptation",
        "canonical": "Cross-Domain Adaptation",
        "aliases": [
          "Domain Adaptation"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for linking studies on model generalization across different domains.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Task Interference",
        "canonical": "Task Interference",
        "aliases": [
          "Negative Transfer"
        ],
        "category": "unique_technical",
        "rationale": "Important for understanding challenges in multi-task learning environments.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "model stability",
      "transferability"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Dynamic Prompt Fusion",
      "resolved_canonical": "Dynamic Prompt Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Multi-Task Learning",
      "resolved_canonical": "Multi-Task Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cross-Domain Adaptation",
      "resolved_canonical": "Cross-Domain Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Task Interference",
      "resolved_canonical": "Task Interference",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18113.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18113](https://arxiv.org/abs/2509.18113)

## 🔗 유사한 논문
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (84.6% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (84.1% similar)
- [[2025-09-23/A non-smooth regularization framework for learning over multitask graphs_20250923|A non-smooth regularization framework for learning over multitask graphs]] (83.1% similar)
- [[2025-09-23/Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling_20250923|Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling]] (83.0% similar)
- [[2025-09-23/MCP_ A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models_20250923|MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models]] (82.5% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Multi-Task Learning|Multi-Task Learning]], [[keywords/Cross-Domain Adaptation|Cross-Domain Adaptation]]
**⚡ Unique Technical**: [[keywords/Dynamic Prompt Fusion|Dynamic Prompt Fusion]], [[keywords/Task Interference|Task Interference]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18113v1 Announce Type: cross 
Abstract: This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.

## 📝 요약

이 연구는 대규모 언어 모델의 일반화 한계를 다루며, 고정된 프롬프트 템플릿에 의존하는 기존 방법과 달리 동적 프롬프트 스케줄링 메커니즘을 갖춘 통합 멀티태스크 학습 프레임워크를 제안합니다. 프롬프트 풀과 태스크 인식 스케줄링 전략을 도입하여 다양한 태스크에 맞춰 프롬프트를 동적으로 조합하고 정렬함으로써 모델의 태스크 간 의미 차이 포착 능력을 향상시킵니다. 프롬프트 융합 시 태스크 임베딩과 게이팅 메커니즘을 사용하여 프롬프트 신호를 세밀하게 제어하며, 태스크 간 유연한 공유 경로를 구축합니다. 또한, 자동 학습 전략을 통해 태스크 간 간섭을 줄이고 부정적 전이를 완화하는 최적화 목표를 설정합니다. 실험 결과, 제안된 프롬프트 스케줄링 방법이 언어 이해 및 지식 추론 태스크에서 성능을 크게 향상시키며, 통합 멀티태스크 모델링 및 도메인 간 적응에 효과적임을 입증했습니다.

## 🎯 주요 포인트

- 1. 본 연구는 고정된 프롬프트 템플릿에 의존하는 기존 방법과 달리, 동적 프롬프트 스케줄링 메커니즘을 갖춘 통합 멀티태스크 학습 프레임워크를 제안합니다.
- 2. 프롬프트 풀과 작업 인식 스케줄링 전략을 통해 다양한 작업에 대한 프롬프트를 동적으로 결합 및 정렬하여 작업 간 의미적 차이를 포착하는 모델의 능력을 향상시킵니다.
- 3. 프롬프트 융합 과정에서 작업 임베딩과 게이팅 메커니즘을 사용하여 프롬프트 신호를 세밀하게 제어하고, 작업별 요구 사항과의 정렬을 보장합니다.
- 4. 제안된 최적화 목표는 공동 멀티태스크 학습에 중점을 두며, 자동 학습 전략을 통해 작업 간섭과 부정적 전이를 효과적으로 완화합니다.
- 5. 실험 결과, 제안된 프롬프트 스케줄링 방법이 언어 이해 및 지식 추론 작업에서 성능을 크게 개선하며, 통합 멀티태스크 모델링 및 크로스 도메인 적응에서의 적용 가능성과 효과성을 입증합니다.


---

*Generated on 2025-09-24 15:03:27*