<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:30:41.963877",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Union-of-Experts",
    "Hierarchical Routing",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.8,
    "Union-of-Experts": 0.85,
    "Hierarchical Routing": 0.78,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "Mixture-of-Experts is a key concept in the paper, central to understanding the proposed Union-of-Experts model.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Union-of-Experts",
        "canonical": "Union-of-Experts",
        "aliases": [
          "UoE"
        ],
        "category": "unique_technical",
        "rationale": "Union-of-Experts is the novel contribution of the paper, representing a new model architecture.",
        "novelty_score": 0.9,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.85
      },
      {
        "surface": "Hierarchical Routing",
        "canonical": "Hierarchical Routing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Hierarchical Routing is a novel mechanism proposed in the paper, crucial for the model's efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Attention Blocks",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Blocks"
        ],
        "category": "specific_connectable",
        "rationale": "Attention Mechanism is extended in the paper, linking it to the broader context of transformer models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "Full Attention",
      "state-of-the-art",
      "efficient transformers"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Union-of-Experts",
      "resolved_canonical": "Union-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Hierarchical Routing",
      "resolved_canonical": "Hierarchical Routing",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Attention Blocks",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2503.02495.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2503.02495](https://arxiv.org/abs/2503.02495)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Symphony-MoE_ Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts_20250924|Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts]] (88.0% similar)
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (87.7% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (85.1% similar)
- [[2025-09-23/Dynamic Expert Specialization_ Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation_20250923|Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation]] (84.8% similar)
- [[2025-09-22/TrueMoE_ Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection_20250922|TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection]] (84.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Union-of-Experts|Union-of-Experts]], [[keywords/Hierarchical Routing|Hierarchical Routing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.02495v3 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. Conventional mixture-of-experts (MoE) architectures suffer from suboptimal coordination dynamics, where isolated expert operations expose the model to overfitting risks. Moreover, they have not been effectively extended to attention blocks, which limits further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes the transformer model into an equivalent group of experts and applies a hierarchical routing mechanism to allocate input subspaces to specialized experts. Our approach advances MoE design with four key innovations: (1) Constructing expert groups by partitioning non-MoE models into functionally equivalent specialists (2) Developing a hierarchical routing paradigm that integrates patch-wise data selection and expert selection strategies. (3) Extending the MoE design to attention blocks. (4) Proposing a hardware-optimized parallelization scheme that exploits batched matrix multiplications for efficient expert computation. The experiments demonstrate that our UoE model surpasses Full Attention, state-of-the-art MoEs and efficient transformers in several tasks across image and natural language domains. In language modeling tasks, UoE achieves an average reduction of 2.38 in perplexity compared to the best-performing MoE method with only 76% of its FLOPs. In the Long Range Arena benchmark, it demonstrates an average score at least 0.68% higher than all comparison models, with only 50% of the FLOPs of the best MoE method. In image classification, it yields an average accuracy improvement of 1.75% over the best model while maintaining comparable FLOPs. The source codes are available at https://github.com/YujiaoYang-work/UoE.

## ğŸ“ ìš”ì•½

Mixture-of-Experts (MoE) ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ Union-of-Experts (UoE) ëª¨ë¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì „ë¬¸ê°€ ê·¸ë£¹ìœ¼ë¡œ ë¶„í•´í•˜ê³  ê³„ì¸µì  ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì…ë ¥ì„ ì „ë¬¸í™”ëœ ì „ë¬¸ê°€ì—ê²Œ í• ë‹¹í•©ë‹ˆë‹¤. UoEëŠ” ë¹„MoE ëª¨ë¸ì„ ê¸°ëŠ¥ì ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì „ë¬¸ê°€ ê·¸ë£¹ì„ êµ¬ì„±í•˜ê³ , íŒ¨ì¹˜ ê¸°ë°˜ ë°ì´í„° ì„ íƒê³¼ ì „ë¬¸ê°€ ì„ íƒ ì „ëµì„ í†µí•©í•œ ê³„ì¸µì  ë¼ìš°íŒ… íŒ¨ëŸ¬ë‹¤ì„ì„ ê°œë°œí•˜ë©°, MoE ë””ìì¸ì„ ì£¼ì˜ ë¸”ë¡ìœ¼ë¡œ í™•ì¥í•˜ê³ , í•˜ë“œì›¨ì–´ ìµœì í™” ë³‘ë ¬í™” ìŠ¤í‚´ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, UoEëŠ” ì´ë¯¸ì§€ ë° ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ê¸°ì¡´ MoE ë° íš¨ìœ¨ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ëŠ¥ê°€í•˜ë©°, ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ìµœìƒì˜ MoE ëŒ€ë¹„ 2.38ì˜ í¼í”Œë ‰ì‹œí‹° ê°ì†Œì™€ 76%ì˜ FLOPsë§Œìœ¼ë¡œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. Long Range Arena ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ìµœìƒì˜ MoE ëŒ€ë¹„ FLOPsì˜ 50%ë§Œìœ¼ë¡œ í‰ê·  0.68% ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©°, ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œëŠ” í‰ê·  1.75%ì˜ ì •í™•ë„ í–¥ìƒì„ ë³´ì…ë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Union-of-Experts (UoE)ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì „ë¬¸ê°€ ê·¸ë£¹ìœ¼ë¡œ ë¶„í•´í•˜ê³  ê³„ì¸µì  ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•˜ì—¬ ì…ë ¥ í•˜ìœ„ ê³µê°„ì„ ì „ë¬¸í™”ëœ ì „ë¬¸ê°€ì—ê²Œ í• ë‹¹í•©ë‹ˆë‹¤.
- 2. UoEëŠ” ë¹„-MoE ëª¨ë¸ì„ ê¸°ëŠ¥ì ìœ¼ë¡œ ë™ë“±í•œ ì „ë¬¸ê°€ë¡œ ë¶„í• í•˜ì—¬ ì „ë¬¸ê°€ ê·¸ë£¹ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
- 3. MoE ì„¤ê³„ë¥¼ ì£¼ì˜ ë¸”ë¡ìœ¼ë¡œ í™•ì¥í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
- 4. í•˜ë“œì›¨ì–´ ìµœì í™” ë³‘ë ¬í™” ë°©ì‹ì„ ì œì•ˆí•˜ì—¬ ë°°ì¹˜ í–‰ë ¬ ê³±ì…ˆì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì „ë¬¸ê°€ ê³„ì‚°ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
- 5. UoE ëª¨ë¸ì€ ì´ë¯¸ì§€ ë° ìì—°ì–´ ë¶„ì•¼ì—ì„œ ìµœì²¨ë‹¨ MoE ë° íš¨ìœ¨ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:30:41*