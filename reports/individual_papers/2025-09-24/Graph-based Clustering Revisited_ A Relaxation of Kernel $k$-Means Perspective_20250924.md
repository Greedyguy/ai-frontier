<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:56:12.255492",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Clustering",
    "Spectral Clustering",
    "Doubly Stochastic Matrix",
    "Kernel k-Means",
    "Block Diagonal Regularization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Graph Clustering": 0.78,
    "Spectral Clustering": 0.79,
    "Doubly Stochastic Matrix": 0.77,
    "Kernel k-Means": 0.75,
    "Block Diagonal Regularization": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "graph-based clustering",
        "canonical": "Graph Clustering",
        "aliases": [
          "graph clustering"
        ],
        "category": "specific_connectable",
        "rationale": "Graph Clustering is a foundational concept that connects to various graph-based algorithms and methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      },
      {
        "surface": "spectral clustering",
        "canonical": "Spectral Clustering",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Spectral Clustering is a well-known method in machine learning, linking to various graph and clustering techniques.",
        "novelty_score": 0.46,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "doubly stochastic normalization",
        "canonical": "Doubly Stochastic Matrix",
        "aliases": [
          "doubly stochastic matrices"
        ],
        "category": "unique_technical",
        "rationale": "Doubly Stochastic Matrices are crucial in optimization and clustering, providing a unique technical perspective.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "kernel k-means",
        "canonical": "Kernel k-Means",
        "aliases": [
          "kernel kmeans"
        ],
        "category": "specific_connectable",
        "rationale": "Kernel k-Means is a variant of k-means clustering that is widely used in machine learning.",
        "novelty_score": 0.52,
        "connectivity_score": 0.83,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "block diagonal regularization",
        "canonical": "Block Diagonal Regularization",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Block Diagonal Regularization is a novel technique enhancing clustering performance, offering a unique technical approach.",
        "novelty_score": 0.71,
        "connectivity_score": 0.69,
        "specificity_score": 0.82,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "graph-based clustering",
      "resolved_canonical": "Graph Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "spectral clustering",
      "resolved_canonical": "Spectral Clustering",
      "decision": "linked",
      "scores": {
        "novelty": 0.46,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "doubly stochastic normalization",
      "resolved_canonical": "Doubly Stochastic Matrix",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "kernel k-means",
      "resolved_canonical": "Kernel k-Means",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.83,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "block diagonal regularization",
      "resolved_canonical": "Block Diagonal Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.71,
        "connectivity": 0.69,
        "specificity": 0.82,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18826.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18826](https://arxiv.org/abs/2509.18826)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Accurate and Efficient Low-Rank Model Merging in Core Space_20250923|Accurate and Efficient Low-Rank Model Merging in Core Space]] (82.4% similar)
- [[2025-09-23/Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA_20250923|Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA]] (81.5% similar)
- [[2025-09-17/Decentralized Optimization with Topology-Independent Communication_20250917|Decentralized Optimization with Topology-Independent Communication]] (81.0% similar)
- [[2025-09-23/RefLoRA_ Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models_20250923|RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models]] (80.9% similar)
- [[2025-09-24/Subspace Clustering of Subspaces_ Unifying Canonical Correlation Analysis and Subspace Clustering_20250924|Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Graph Clustering|Graph Clustering]], [[keywords/Spectral Clustering|Spectral Clustering]], [[keywords/Kernel k-Means|Kernel k-Means]]
**âš¡ Unique Technical**: [[keywords/Doubly Stochastic Matrix|Doubly Stochastic Matrix]], [[keywords/Block Diagonal Regularization|Block Diagonal Regularization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18826v1 Announce Type: new 
Abstract: The well-known graph-based clustering methods, including spectral clustering, symmetric non-negative matrix factorization, and doubly stochastic normalization, can be viewed as relaxations of the kernel $k$-means approach. However, we posit that these methods excessively relax their inherent low-rank, nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical feasibility, potentially limiting their clustering efficacy. In this paper, guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank \textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only relaxes the orthonormal constraint to derive a probabilistic clustering results. Furthermore, we theoretically establish the equivalence between orthogonality and block diagonality under the doubly stochastic constraint. By integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further enhances the clustering performance. To ensure numerical solvability, we transform the non-convex doubly stochastic constraint into a linear convex constraint through the introduction of a class probability parameter. We further theoretically demonstrate the gradient Lipschitz continuity of our LoRD and B-LoRD enables the proposal of a globally convergent projected gradient descent algorithm for their optimization. Extensive experiments validate the effectiveness of our approaches. The code is publicly available at https://github.com/lwl-learning/LoRD.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ì˜ ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ë“¤ì´ ë³¸ë˜ì˜ ì €ì°¨ì›, ë¹„ìŒìˆ˜, ì´ì¤‘ í™•ë¥ ì , ì§êµ ì œì•½ ì¡°ê±´ì„ ê³¼ë„í•˜ê²Œ ì™„í™”í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ íš¨ìœ¨ì„±ì„ ì œí•œí•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì§êµ ì œì•½ë§Œì„ ì™„í™”í•˜ì—¬ í™•ë¥ ì  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” LoRD(ì €ì°¨ì› ì´ì¤‘ í™•ë¥ ì  í´ëŸ¬ìŠ¤í„°ë§) ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, ì´ì¤‘ í™•ë¥ ì  ì œì•½ í•˜ì—ì„œ ì§êµì„±ê³¼ ë¸”ë¡ ëŒ€ê°ì„±ì˜ ë™ë“±ì„±ì„ ì´ë¡ ì ìœ¼ë¡œ ì…ì¦í•˜ê³ , Frobenius ë…¸ë¦„ì„ ìµœëŒ€í™”í•˜ëŠ” ë¸”ë¡ ëŒ€ê° ì •ê·œí™”ë¥¼ í†µí•©í•œ B-LoRDë¥¼ ì œì•ˆí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë¹„ë³¼ë¡ ì´ì¤‘ í™•ë¥ ì  ì œì•½ì„ ì„ í˜• ë³¼ë¡ ì œì•½ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜ì¹˜ì  í•´ë¥¼ ë³´ì¥í•˜ê³ , ì´ë¡ ì ìœ¼ë¡œ LoRDì™€ B-LoRDì˜ ì „ì—­ ìˆ˜ë ´ì„ ë³´ì¥í•˜ëŠ” íˆ¬ì˜ ê²½ì‚¬ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ì œì•ˆëœ ë°©ë²•ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•˜ë©°, ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ë“¤ì€ ì»¤ë„ $k$-í‰ê·  ì ‘ê·¼ë²•ì˜ ì™„í™”ë¡œ ë³¼ ìˆ˜ ìˆìœ¼ë©°, ì´ë“¤ì€ ë³¸ë˜ì˜ ì €ìˆœìœ„, ë¹„ìŒìˆ˜, ì´ì¤‘ í™•ë¥ , ì§êµ ì œì•½ì„ ê³¼ë„í•˜ê²Œ ì™„í™”í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ íš¨ìœ¨ì„±ì„ ì œí•œí•  ìˆ˜ ìˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì§êµ ì œì•½ë§Œì„ ì™„í™”í•˜ì—¬ í™•ë¥ ì  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ì €ìˆœìœ„ ì´ì¤‘ í™•ë¥  í´ëŸ¬ìŠ¤í„°ë§(LoRD) ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.
- 3. ì´ì¤‘ í™•ë¥  ì œì•½ í•˜ì—ì„œ ì§êµì„±ê³¼ ë¸”ë¡ ëŒ€ê°ì„±ì˜ ë™ë“±ì„±ì„ ì´ë¡ ì ìœ¼ë¡œ í™•ë¦½í•˜ê³ , Frobenius ë…¸ë¦„ì˜ ìµœëŒ€í™”ë¥¼ í†µí•´ ë¸”ë¡ ëŒ€ê° ì •ê·œí™”ë¥¼ í†µí•©í•œ B-LoRDë¥¼ ì œì•ˆí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 4. ë¹„ë³¼ë¡ ì´ì¤‘ í™•ë¥  ì œì•½ì„ ì„ í˜• ë³¼ë¡ ì œì•½ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜ì¹˜ì  í•´ê²° ê°€ëŠ¥ì„±ì„ ë³´ì¥í•˜ê³ , LoRDì™€ B-LoRDì˜ ìµœì í™”ë¥¼ ìœ„í•œ ì „ì—­ ìˆ˜ë ´ íˆ¬ì˜ ê²½ì‚¬ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•œë‹¤.
- 5. ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ë°©ë²•ë“¤ì˜ íš¨ê³¼ì„±ì„ ê²€ì¦í•˜ì˜€ìœ¼ë©°, ì½”ë“œëŠ” ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•˜ë‹¤.


---

*Generated on 2025-09-24 14:56:12*