<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:30:14.269514",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Attention Mechanism",
    "Token Lens",
    "Causal Tracing"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Attention Mechanism": 0.9,
    "Token Lens": 0.8,
    "Causal Tracing": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LM"
        ],
        "category": "broad_technical",
        "rationale": "Links to a broad category of models used in NLP and connects with other concepts like attention mechanisms.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.85
      },
      {
        "surface": "Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Central to the paper's mechanism of promoting and suppressing tokens, linking to many related concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.9
      },
      {
        "surface": "Token Lens",
        "canonical": "Token Lens",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel method introduced in the paper, providing insights into token interactions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Causal Tracing",
        "canonical": "Causal Tracing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific technique used in the paper to analyze model behavior, offering unique insights.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Token Lens",
      "resolved_canonical": "Token Lens",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Causal Tracing",
      "resolved_canonical": "Causal Tracing",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2502.20475.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2502.20475](https://arxiv.org/abs/2502.20475)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (83.9% similar)
- [[2025-09-22/Quantifying Self-Awareness of Knowledge in Large Language Models_20250922|Quantifying Self-Awareness of Knowledge in Large Language Models]] (83.8% similar)
- [[2025-09-22/Efficient Real-time Refinement of Language Model Text Generation_20250922|Efficient Real-time Refinement of Language Model Text Generation]] (83.7% similar)
- [[2025-09-23/Adaptive Distraction_ Probing LLM Contextual Robustness with Automated Tree Search_20250923|Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search]] (83.3% similar)
- [[2025-09-23/EAMET_ Robust Massive Model Editing via Embedding Alignment Optimization_20250923|EAMET: Robust Massive Model Editing via Embedding Alignment Optimization]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Token Lens|Token Lens]], [[keywords/Causal Tracing|Causal Tracing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.20475v3 Announce Type: replace-cross 
Abstract: To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets, models, and prompt templates, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì´ í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ì—¬ëŸ¬ ê°œì˜ ì‚¬ì‹¤ì  ë‹µë³€ì„ ì œê³µí•  ë•Œ, ì§€ì‹ì„ íšŒìƒí•˜ê³  ì´ì „ ë‹µë³€ì„ ë°˜ë³µí•˜ì§€ ì•ŠëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” 'ì´‰ì§„ í›„ ì–µì œ' ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•˜ë©°, ëª¨ë¸ì´ ë¨¼ì € ëª¨ë“  ë‹µë³€ì„ íšŒìƒí•œ í›„ ì´ì „ì— ìƒì„±ëœ ë‹µë³€ì„ ì–µì œí•œë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì£¼ì œì™€ ì´ì „ ë‹µë³€ í† í°ì„ ì‚¬ìš©í•´ ì§€ì‹ì„ íšŒìƒí•˜ê³ , ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ MLPsê°€ ê°ê° ì£¼ì œ ì •ë³´ë¥¼ ì „íŒŒí•˜ê³  ë‹µë³€ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ì´í›„ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì´ì „ ë‹µë³€ í† í°ì„ ì–µì œí•˜ê³ , MLPsëŠ” ì–µì œ ì‹ í˜¸ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ì…ì¦ë˜ì—ˆìœ¼ë©°, Token Lensì™€ ë…¸í¬ì•„ì›ƒ ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ë‚´ë¶€ êµ¬ì„± ìš”ì†Œê°€ ì…ë ¥ í† í°ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë³µì¡í•œ ì‚¬ì‹¤ íšŒìƒì„ ì§€ì›í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ ë‚´ë¶€ ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸ì€ í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ì—¬ëŸ¬ ë‹µì„ ì œê³µí•  ë•Œ, ì§€ì‹ì„ íšŒìƒí•˜ê³  ì´ì „ ë‹µë³€ì„ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ì–µì œí•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 2. ëª¨ë¸ì€ ì£¼ì œì™€ ì´ì „ ë‹µë³€ í† í°ì„ í™œìš©í•˜ì—¬ ì§€ì‹ì„ íšŒìƒí•˜ë©°, ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ MLPsê°€ ê°ê° ì£¼ì œ ì •ë³´ë¥¼ ì „íŒŒí•˜ê³  ë‹µë³€ì„ ì´‰ì§„í•©ë‹ˆë‹¤.
- 3. ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì´ì „ ë‹µë³€ í† í°ì„ ì–µì œí•˜ê³ , MLPsëŠ” ì–µì œ ì‹ í˜¸ë¥¼ ì¦í­í•˜ì—¬ ë‹µë³€ ì¤‘ë³µì„ ë°©ì§€í•©ë‹ˆë‹¤.
- 4. Token Lensì™€ ë…¸í¬ì•„ì›ƒ ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ë‚´ë¶€ êµ¬ì„± ìš”ì†Œê°€ ì…ë ¥ í† í°ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì„ ë¶„ì„í•˜ì—¬ ë³µì¡í•œ ì‚¬ì‹¤ íšŒìƒì„ ì§€ì›í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹, ëª¨ë¸, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:30:14*