<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:31:04.381339",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Earth Observation Foundation Models",
    "Transformer",
    "Neural Network",
    "Mamba State-Space Models",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Earth Observation Foundation Models": 0.78,
    "Transformer": 0.81,
    "Neural Network": 0.79,
    "Mamba State-Space Models": 0.77,
    "Few-Shot Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Earth Observation Foundation Models",
        "canonical": "Earth Observation Foundation Models",
        "aliases": [
          "EO Foundation Models"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific type of foundation model tailored for Earth observation data, which is central to the paper's contributions.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision Transformers",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Vision Transformers are a specific application of Transformers in computer vision, linking to broader Transformer research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.81
      },
      {
        "surface": "U-Net Convolutional Neural Network",
        "canonical": "Neural Network",
        "aliases": [
          "U-Net CNN"
        ],
        "category": "broad_technical",
        "rationale": "U-Net is a specific architecture within the broader category of neural networks, relevant for image segmentation tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.82,
        "specificity_score": 0.68,
        "link_intent_score": 0.79
      },
      {
        "surface": "Mamba State-Space Models",
        "canonical": "Mamba State-Space Models",
        "aliases": [
          "Mamba SSM"
        ],
        "category": "unique_technical",
        "rationale": "Mamba SSMs are a novel approach discussed in the paper, offering a unique perspective on state-space modeling.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is a trending concept relevant to the paper's exploration of fine-tuning models with minimal labeled data.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "pretraining dataset",
      "downstream tasks",
      "PhilEO Bench"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Earth Observation Foundation Models",
      "resolved_canonical": "Earth Observation Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision Transformers",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "U-Net Convolutional Neural Network",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.82,
        "specificity": 0.68,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Mamba State-Space Models",
      "resolved_canonical": "Mamba State-Space Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2506.14765.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2506.14765](https://arxiv.org/abs/2506.14765)

## 🔗 유사한 논문
- [[2025-09-22/TESSERA_ Precomputed FAIR Global Pixel Embeddings for Earth Representation and Analysis_20250922|TESSERA: Precomputed FAIR Global Pixel Embeddings for Earth Representation and Analysis]] (82.6% similar)
- [[2025-09-24/MOMEMTO_ Patch-based Memory Gate Model in Time Series Foundation Model_20250924|MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model]] (81.4% similar)
- [[2025-09-23/StefaLand_ An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions_20250923|StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions]] (80.4% similar)
- [[2025-09-23/Less is More_ Unlocking Specialization of Time Series Foundation Models via Structured Pruning_20250923|Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning]] (80.4% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (80.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]], [[keywords/Neural Network|Neural Network]]
**🔗 Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Earth Observation Foundation Models|Earth Observation Foundation Models]], [[keywords/Mamba State-Space Models|Mamba State-Space Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.14765v4 Announce Type: replace 
Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data. To fully exploit this, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for downstream tasks with minimal labeled data. In this paper, we study scaling-up FMs: we train our models on the pretraining dataset MajorTOM 23TB which includes all regions, and the performance on average is competitive versus models pretrained on more specialized datasets which are substantially smaller and include only land. The additional data of oceans and ice do not decrease the performance on land-focused downstream tasks. These results indicate that large FMs trained on global datasets for a wider variety of downstream tasks can be useful for downstream applications that only require a subset of the information included in their training. The second contribution is the exploration of U-Net Convolutional Neural Network (CNN), Vision Transformers (ViT), and Mamba State-Space Models (SSM) as FMs. U-Net captures local correlations amongst pixels, while ViT and Mamba capture local and distant correlations. We develop various models using different architectures, including U-Net, ViT, and Mamba, and different number of parameters. We evaluate the FLoating-point OPerations (FLOPs) needed by the models. We fine-tune on the PhilEO Bench for different downstream tasks: roads, buildings, and land cover. For most n-shots for roads and buildings, U-Net 200M-2T outperforms the other models. Using Mamba, we achieve comparable results on the downstream tasks, with less computational expenses. We also compare with the recent FM TerraMind which we evaluate on PhilEO Bench.

## 📝 요약

이 논문은 지구 관측 위성 데이터의 효율적 활용을 위해 대규모 비지도 데이터셋을 기반으로 EO Foundation Models(FMs)을 사전 학습하는 방법을 연구합니다. MajorTOM 23TB 데이터셋을 사용해 다양한 지역을 포함한 모델을 학습한 결과, 특정 지역에 특화된 작은 데이터셋으로 학습한 모델과 유사한 성능을 보였습니다. 이는 대규모 글로벌 데이터셋을 활용한 FMs이 다양한 다운스트림 작업에 유용할 수 있음을 시사합니다. 또한, U-Net, Vision Transformers(ViT), Mamba State-Space Models(SSM)을 사용하여 다양한 모델을 개발하고, PhilEO Bench에서 도로, 건물, 토지 피복 등의 다운스트림 작업에 대해 평가했습니다. U-Net 200M-2T 모델이 대부분의 경우 우수한 성능을 보였으며, Mamba 모델은 적은 계산 비용으로도 유사한 결과를 얻었습니다.

## 🎯 주요 포인트

- 1. 대규모의 미표기 데이터셋인 MajorTOM 23TB를 사용하여 EO Foundation Models(FMs)을 사전 학습함으로써, 적은 양의 라벨링된 데이터로도 효율적인 다운스트림 작업을 수행할 수 있다.
- 2. 해양과 빙하 데이터를 포함한 대규모 글로벌 데이터셋으로 학습된 FMs는 육지 중심의 다운스트림 작업 성능을 저해하지 않으며, 다양한 다운스트림 작업에 유용하다.
- 3. U-Net, Vision Transformers(ViT), Mamba State-Space Models(SSM) 등 다양한 아키텍처를 탐구하여, 각 모델의 FLOPs를 평가하고 다운스트림 작업에 맞춰 미세 조정하였다.
- 4. U-Net 200M-2T 모델은 도로 및 건물 관련 다운스트림 작업에서 대부분의 n-shot에서 다른 모델보다 우수한 성능을 보였다.
- 5. Mamba 모델은 적은 계산 비용으로도 다운스트림 작업에서 비교 가능한 성능을 달성하였다.


---

*Generated on 2025-09-24 16:31:04*