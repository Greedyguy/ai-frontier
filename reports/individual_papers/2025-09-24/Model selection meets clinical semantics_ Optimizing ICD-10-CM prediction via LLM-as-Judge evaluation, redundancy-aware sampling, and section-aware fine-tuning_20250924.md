<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:28:56.922711",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "ICD-10-CM Code Prediction",
    "Large Language Model",
    "Redundancy-Aware Sampling",
    "Section-Aware Fine-Tuning",
    "Embedding-Based Similarity Measures"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "ICD-10-CM Code Prediction": 0.78,
    "Large Language Model": 0.82,
    "Redundancy-Aware Sampling": 0.75,
    "Section-Aware Fine-Tuning": 0.77,
    "Embedding-Based Similarity Measures": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "ICD-10-CM code prediction",
        "canonical": "ICD-10-CM Code Prediction",
        "aliases": [
          "ICD-10-CM Prediction",
          "ICD Coding"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific task central to the paper's contribution, linking to healthcare analytics.",
        "novelty_score": 0.65,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are a core technology discussed in the paper, facilitating connections to NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.82
      },
      {
        "surface": "redundancy-aware sampling",
        "canonical": "Redundancy-Aware Sampling",
        "aliases": [
          "Redundancy Sampling"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a novel approach to improve data quality, relevant to data processing.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "section-aware fine-tuning",
        "canonical": "Section-Aware Fine-Tuning",
        "aliases": [
          "Section Fine-Tuning"
        ],
        "category": "unique_technical",
        "rationale": "This method enhances model performance by leveraging document structure, linking to model optimization.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "embedding-based similarity measures",
        "canonical": "Embedding-Based Similarity Measures",
        "aliases": [
          "Embedding Similarity"
        ],
        "category": "specific_connectable",
        "rationale": "These measures are crucial for evaluating semantic similarity, connecting to machine learning techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "model selection",
      "input contextualization",
      "training data redundancy"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "ICD-10-CM code prediction",
      "resolved_canonical": "ICD-10-CM Code Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "redundancy-aware sampling",
      "resolved_canonical": "Redundancy-Aware Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "section-aware fine-tuning",
      "resolved_canonical": "Section-Aware Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "embedding-based similarity measures",
      "resolved_canonical": "Embedding-Based Similarity Measures",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18846.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18846](https://arxiv.org/abs/2509.18846)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/SparseDoctor_ Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models_20250923|SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models]] (86.9% similar)
- [[2025-09-22/EHR-MCP_ Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol_20250922|EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol]] (86.6% similar)
- [[2025-09-22/MedCOD_ Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework_20250922|MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework]] (85.9% similar)
- [[2025-09-23/From Scores to Steps_ Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations_20250923|From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations]] (85.4% similar)
- [[2025-09-24/Advances in Large Language Models for Medicine_20250924|Advances in Large Language Models for Medicine]] (85.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Embedding-Based Similarity Measures|Embedding-Based Similarity Measures]]
**âš¡ Unique Technical**: [[keywords/ICD-10-CM Code Prediction|ICD-10-CM Code Prediction]], [[keywords/Redundancy-Aware Sampling|Redundancy-Aware Sampling]], [[keywords/Section-Aware Fine-Tuning|Section-Aware Fine-Tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18846v1 Announce Type: new 
Abstract: Accurate International Classification of Diseases (ICD) coding is critical for clinical documentation, billing, and healthcare analytics, yet it remains a labour-intensive and error-prone task. Although large language models (LLMs) show promise in automating ICD coding, their challenges in base model selection, input contextualization, and training data redundancy limit their effectiveness. We propose a modular framework for ICD-10 Clinical Modification (ICD-10-CM) code prediction that addresses these challenges through principled model selection, redundancy-aware data sampling, and structured input design. The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce aggregation to assess and rank open-source LLMs based on their intrinsic comprehension of ICD-10-CM code definitions. We introduced embedding-based similarity measures, a redundancy-aware sampling strategy to remove semantically duplicated discharge summaries. We leverage structured discharge summaries from Taiwanese hospitals to evaluate contextual effects and examine section-wise content inclusion under universal and section-specific modelling paradigms. Experiments across two institutional datasets demonstrate that the selected base model after fine-tuning consistently outperforms baseline LLMs in internal and external evaluations. Incorporating more clinical sections consistently improves prediction performance. This study uses open-source LLMs to establish a practical and principled approach to ICD-10-CM code prediction. The proposed framework provides a scalable, institution-ready solution for real-world deployment of automated medical coding systems by combining informed model selection, efficient data refinement, and context-aware prompting.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ICD-10-CM ì½”ë“œ ì˜ˆì¸¡ì„ ìœ„í•œ ëª¨ë“ˆí˜• í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„ íƒ, ì…ë ¥ ë§¥ë½í™”, ì¤‘ë³µ ë°ì´í„° ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. LLM í‰ê°€ í”„ë¡œí† ì½œê³¼ Plackett-Luce ì§‘ê³„ë¥¼ í†µí•´ ICD-10-CM ì½”ë“œ ì •ì˜ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ìˆœìœ„ë¥¼ ë§¤ê¹ë‹ˆë‹¤. ë˜í•œ, ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ì¸¡ì • ë° ì¤‘ë³µ ì¸ì‹ ìƒ˜í”Œë§ ì „ëµì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ëŒ€ë§Œ ë³‘ì›ì˜ êµ¬ì¡°í™”ëœ í‡´ì› ìš”ì•½ì„ í™œìš©í•´ ë§¥ë½ íš¨ê³¼ë¥¼ í‰ê°€í•˜ê³ , ë‹¤ì–‘í•œ ëª¨ë¸ë§ íŒ¨ëŸ¬ë‹¤ì„ í•˜ì—ì„œ ì„¹ì…˜ë³„ ì½˜í…ì¸  í¬í•¨ ì—¬ë¶€ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ë‘ ê¸°ê´€ì˜ ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, ì„ íƒëœ ê¸°ë³¸ ëª¨ë¸ì´ ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, ì„ìƒ ì„¹ì…˜ì„ ì¶”ê°€í• ìˆ˜ë¡ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ LLMì„ í™œìš©í•´ ICD-10-CM ì½”ë“œ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹¤ìš©ì ì´ê³  ì›ì¹™ì ì¸ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ë©°, ì‹¤ì œ ì˜ë£Œ ì½”ë”© ì‹œìŠ¤í…œì˜ ìë™í™”ë¥¼ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ICD ì½”ë”©ì˜ ìë™í™”ë¥¼ ìœ„í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í™œìš© ê°€ëŠ¥ì„±ì„ íƒêµ¬í•˜ì§€ë§Œ, ëª¨ë¸ ì„ íƒ, ì…ë ¥ ë§¥ë½í™”, ë°ì´í„° ì¤‘ë³µì„± ë¬¸ì œë¡œ ì¸í•´ íš¨ê³¼ê°€ ì œí•œë¨ì„ ì§€ì í•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ ëª¨ë“ˆí˜• í”„ë ˆì„ì›Œí¬ëŠ” ICD-10-CM ì½”ë“œ ì˜ˆì¸¡ì„ ìœ„í•´ ì›ì¹™ì ì¸ ëª¨ë¸ ì„ íƒ, ì¤‘ë³µ ì¸ì‹ ë°ì´í„° ìƒ˜í”Œë§, êµ¬ì¡°í™”ëœ ì…ë ¥ ì„¤ê³„ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 3. ëŒ€ë§Œ ë³‘ì›ì˜ êµ¬ì¡°í™”ëœ í‡´ì› ìš”ì•½ì„ í™œìš©í•˜ì—¬ ë§¥ë½ íš¨ê³¼ë¥¼ í‰ê°€í•˜ê³ , ë³´í¸ì  ë° ì„¹ì…˜ë³„ ëª¨ë¸ë§ íŒ¨ëŸ¬ë‹¤ì„ í•˜ì—ì„œ ì„¹ì…˜ë³„ ì½˜í…ì¸  í¬í•¨ì„ ê²€í† í•©ë‹ˆë‹¤.
- 4. ë‘ ê°œì˜ ê¸°ê´€ ë°ì´í„°ì…‹ì„ í†µí•œ ì‹¤í—˜ì—ì„œ, ì„ íƒëœ ê¸°ë³¸ ëª¨ë¸ì´ ë¯¸ì„¸ ì¡°ì • í›„ ë‚´ë¶€ ë° ì™¸ë¶€ í‰ê°€ì—ì„œ ì¼ê´€ë˜ê²Œ ê¸°ë³¸ LLMë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ì •ë³´ì— ì…ê°í•œ ëª¨ë¸ ì„ íƒ, íš¨ìœ¨ì ì¸ ë°ì´í„° ì •ì œ, ë§¥ë½ ì¸ì‹ í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ìë™í™”ëœ ì˜ë£Œ ì½”ë”© ì‹œìŠ¤í…œì˜ ì‹¤ì œ ë°°í¬ë¥¼ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ê¸°ê´€ ì¤€ë¹„ëœ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:28:56*