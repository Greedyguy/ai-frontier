<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:14:45.435555",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Trusted Execution Environment",
    "Confidential Computing",
    "Intel SGX"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Trusted Execution Environment": 0.78,
    "Confidential Computing": 0.77,
    "Intel SGX": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus on confidential inference, providing a strong link to existing research on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Trusted Execution Environments",
        "canonical": "Trusted Execution Environment",
        "aliases": [
          "TEEs"
        ],
        "category": "unique_technical",
        "rationale": "TEEs are a novel solution proposed for securing LLM inference, making them a unique technical focus of the paper.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Confidential Compute GPUs",
        "canonical": "Confidential Computing",
        "aliases": [
          "Confidential GPUs"
        ],
        "category": "specific_connectable",
        "rationale": "Confidential computing is a key aspect of the paper's exploration of secure inference environments.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Intel's TDX and SGX",
        "canonical": "Intel SGX",
        "aliases": [
          "Intel TDX"
        ],
        "category": "unique_technical",
        "rationale": "Intel's TEEs are specifically evaluated in the paper, highlighting their role in secure LLM inference.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "cost",
      "security trade-offs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Trusted Execution Environments",
      "resolved_canonical": "Trusted Execution Environment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Confidential Compute GPUs",
      "resolved_canonical": "Confidential Computing",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Intel's TDX and SGX",
      "resolved_canonical": "Intel SGX",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18886.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18886](https://arxiv.org/abs/2509.18886)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (84.1% similar)
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (83.7% similar)
- [[2025-09-23/TranSQL+_ Serving Large Language Models with SQL on Low-Resource Hardware_20250923|TranSQL+: Serving Large Language Models with SQL on Low-Resource Hardware]] (83.7% similar)
- [[2025-09-23/Large Language Models for Cyber Security_ A Systematic Literature Review_20250923|Large Language Models for Cyber Security: A Systematic Literature Review]] (83.4% similar)
- [[2025-09-23/Robust LLM Training Infrastructure at ByteDance_20250923|Robust LLM Training Infrastructure at ByteDance]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Confidential Computing|Confidential Computing]]
**âš¡ Unique Technical**: [[keywords/Trusted Execution Environment|Trusted Execution Environment]], [[keywords/Intel SGX|Intel SGX]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18886v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed on converged Cloud and High-Performance Computing (HPC) infrastructure. However, as LLMs handle confidential inputs and are fine-tuned on costly, proprietary datasets, their heightened security requirements slow adoption in privacy-sensitive sectors such as healthcare and finance. We investigate methods to address this gap and propose Trusted Execution Environments (TEEs) as a solution for securing end-to-end LLM inference. We validate their practicality by evaluating these compute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side, we conduct an in-depth study running full Llama2 inference pipelines (7B, 13B, 70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions (AMX). We derive 12 insights, including that across various data types, batch sizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency overheads, further reduced by AMX. We run LLM inference on NVIDIA H100 Confidential Compute GPUs, contextualizing our CPU findings and observing throughput penalties of 4-8% that diminish as batch and input sizes grow. By comparing performance, cost, and security trade-offs, we show how CPU TEEs can be more cost-effective or secure than their GPU counterparts. To our knowledge, our work is the first to comprehensively demonstrate the performance and practicality of modern TEEs across both CPUs and GPUs for enabling confidential LLMs (cLLMs).

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë³´ì•ˆì„ ê°•í™”í•˜ê¸° ìœ„í•´ ì‹ ë¢° ì‹¤í–‰ í™˜ê²½(TEE)ì„ ì œì•ˆí•©ë‹ˆë‹¤. íŠ¹íˆ, ì˜ë£Œ ë° ê¸ˆìœµ ë¶„ì•¼ì—ì„œì˜ ë¯¼ê°í•œ ë°ì´í„° ì²˜ë¦¬ì— ìˆì–´ LLMì˜ ë³´ì•ˆ ìš”êµ¬ ì‚¬í•­ì´ ë†’ì€ ì ì„ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” CPUì™€ GPU TEEì—ì„œ LLM ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë©°, Intelì˜ TDXì™€ SGXë¥¼ ì‚¬ìš©í•˜ì—¬ Llama2 ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³ , Advanced Matrix Extensions(AMX)ë¡œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. CPU TEEëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•, ë°°ì¹˜ í¬ê¸°, ì…ë ¥ ê¸¸ì´ì— ëŒ€í•´ 10% ë¯¸ë§Œì˜ ì²˜ë¦¬ëŸ‰ ë° 20% ë¯¸ë§Œì˜ ì§€ì—° ì˜¤ë²„í—¤ë“œë¥¼ ë³´ì´ë©°, AMXë¡œ ì´ë¥¼ ë”ìš± ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, NVIDIA H100 Confidential Compute GPUì—ì„œì˜ ì‹¤í–‰ ê²°ê³¼, ë°°ì¹˜ ë° ì…ë ¥ í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ 4-8%ì˜ ì²˜ë¦¬ëŸ‰ íŒ¨ë„í‹°ê°€ ê°ì†Œí•˜ëŠ” ê²ƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” CPUì™€ GPU TEEì˜ ì„±ëŠ¥, ë¹„ìš©, ë³´ì•ˆì˜ ê· í˜•ì„ ë¹„êµí•˜ë©°, CPU TEEê°€ ë” ë¹„ìš© íš¨ìœ¨ì ì´ê±°ë‚˜ ë³´ì•ˆì ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” í˜„ëŒ€ TEEë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë°€ LLMì„ êµ¬í˜„í•œ ìµœì´ˆì˜ í¬ê´„ì  ì—°êµ¬ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë³´ì•ˆ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‹ ë¢° ì‹¤í–‰ í™˜ê²½(TEE)ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. CPUì™€ GPU TEE ë‚´ì—ì„œ LLM ì¶”ë¡  ì‘ì—…ì˜ ì‹¤ìš©ì„±ì„ ê²€ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.
- 3. CPU TEEëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•, ë°°ì¹˜ í¬ê¸°, ì…ë ¥ ê¸¸ì´ì— ëŒ€í•´ 10% ë¯¸ë§Œì˜ ì²˜ë¦¬ëŸ‰ ë° 20%ì˜ ì§€ì—° ì˜¤ë²„í—¤ë“œë¥¼ ë³´ì…ë‹ˆë‹¤.
- 4. GPU TEEì—ì„œëŠ” ë°°ì¹˜ ë° ì…ë ¥ í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ 4-8%ì˜ ì²˜ë¦¬ëŸ‰ íŒ¨ë„í‹°ê°€ ê°ì†Œí•©ë‹ˆë‹¤.
- 5. CPU TEEëŠ” GPU TEEë³´ë‹¤ ë¹„ìš© íš¨ìœ¨ì ì´ê±°ë‚˜ ë³´ì•ˆì„±ì´ ë†’ì„ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:14:45*