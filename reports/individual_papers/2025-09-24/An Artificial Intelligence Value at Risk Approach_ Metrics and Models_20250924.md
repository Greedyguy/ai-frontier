<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:47:54.513615",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "AI Risk Management",
    "Large Language Model",
    "FAIR Model",
    "Data Protection",
    "Information Security"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "AI Risk Management": 0.85,
    "Large Language Model": 0.7,
    "FAIR Model": 0.8,
    "Data Protection": 0.75,
    "Information Security": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "artificial intelligence risk management",
        "canonical": "AI Risk Management",
        "aliases": [
          "AI Risk",
          "AI Risk Handling"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper and connects various aspects of AI risk, providing a comprehensive view on managing AI-related uncertainties.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "large language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are a key component in AI systems, relevant for understanding AI risk in context.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "FAIR model",
        "canonical": "FAIR Model",
        "aliases": [
          "FAIR Framework"
        ],
        "category": "specific_connectable",
        "rationale": "The FAIR model is crucial for quantifying uncertainty in AI risk management, linking to broader risk management strategies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "data protection",
        "canonical": "Data Protection",
        "aliases": [
          "Data Privacy"
        ],
        "category": "specific_connectable",
        "rationale": "Data protection is a critical dimension of AI risk, connecting legal and technical aspects of AI systems.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      },
      {
        "surface": "information security",
        "canonical": "Information Security",
        "aliases": [
          "InfoSec"
        ],
        "category": "specific_connectable",
        "rationale": "Information security is essential for managing AI risks, linking to both operational and technical risk dimensions.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "metrics",
      "models",
      "decision-making"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "artificial intelligence risk management",
      "resolved_canonical": "AI Risk Management",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "large language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "FAIR model",
      "resolved_canonical": "FAIR Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "data protection",
      "resolved_canonical": "Data Protection",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "information security",
      "resolved_canonical": "Information Security",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# An Artificial Intelligence Value at Risk Approach: Metrics and Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18394.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18394](https://arxiv.org/abs/2509.18394)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents_20250922|Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents]] (86.0% similar)
- [[2025-09-18/Position_ AI Safety Must Embrace an Antifragile Perspective_20250918|Position: AI Safety Must Embrace an Antifragile Perspective]] (84.3% similar)
- [[2025-09-22/Algorithmic Fairness_ Not a Purely Technical but Socio-Technical Property_20250922|Algorithmic Fairness: Not a Purely Technical but Socio-Technical Property]] (83.8% similar)
- [[2025-09-23/Towards a Transparent and Interpretable AI Model for Medical Image Classifications_20250923|Towards a Transparent and Interpretable AI Model for Medical Image Classifications]] (83.2% similar)
- [[2025-09-24/Perceptions of AI Across Sectors_ A Comparative Review of Public Attitudes_20250924|Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/FAIR Model|FAIR Model]], [[keywords/Data Protection|Data Protection]], [[keywords/Information Security|Information Security]]
**âš¡ Unique Technical**: [[keywords/AI Risk Management|AI Risk Management]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18394v1 Announce Type: cross 
Abstract: Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems.
  The purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê³µì§€ëŠ¥(AI) ìœ„í—˜ ê´€ë¦¬ì˜ ë³µì¡ì„±ê³¼ ë‹¤ì°¨ì›ì„±ì„ ë‹¤ë£¹ë‹ˆë‹¤. AI ìœ„í—˜ì€ ë²•ì , ìš´ì˜ì , ì¬ì •ì  ì¸¡ë©´ì„ í¬í•¨í•˜ë©°, ìƒˆë¡œìš´ AI ê·œì œì˜ ë“±ì¥ìœ¼ë¡œ ì¸í•´ í˜„ì¬ì˜ ìœ„í—˜ ê´€ë¦¬ ìˆ˜ì¤€ì€ ë¯¸ì„±ìˆ™í•œ ìƒíƒœì…ë‹ˆë‹¤. ë…¼ë¬¸ì€ AI ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ì¶˜ ë§ì¶¤í˜• ìœ„í—˜ ì§€í‘œì™€ ëª¨ë¸ ê°œë°œì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ë°ì´í„° ë³´í˜¸, ê³µì •ì„±, ì •í™•ì„±, ê²¬ê³ ì„±, ì •ë³´ ë³´ì•ˆ ë“± ì—¬ëŸ¬ ì°¨ì›ì—ì„œ AI ìœ„í—˜ì„ ë¶„í•´í•  í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ì¬ë¬´, ë²•ë¬´ ë¶€ì„œ ë° ì •ë¶€ ìœ„í—˜ ê´€ë¦¬ íŒ€ì´ AI ì‹œìŠ¤í…œì˜ ê¸°ìˆ ì  ì¸¡ë©´ì— ëŒ€í•œ ì´í•´ê°€ ë¶€ì¡±í•˜ë‹¤ê³  ì§€ì í•˜ë©°, ë°ì´í„° ê³¼í•™ìì™€ AI ì—”ì§€ë‹ˆì–´ê°€ ì í•©í•œ êµ¬í˜„ìë¡œ ë¶€ìƒí•˜ê³  ìˆìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ AI ì´í•´ê´€ê³„ìë“¤ì—ê²Œ ìœ„í—˜ ê´€ë¦¬ì˜ ê¹Šì´ë¥¼ ì•ˆë‚´í•˜ê³ , ê¸°ë³¸ì ì¸ ìœ„í—˜ ê´€ë¦¬ ì§€ì‹ê³¼ AI ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§ì— ëŒ€í•œ ì´í•´ë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ AI ë§ì¶¤ í™˜ê²½ì—ì„œ ì ìš© ê°€ëŠ¥í•œ ê°„ë‹¨í•œ ì•„ì´ë””ì–´ë¥¼ ì œê³µí•˜ë©°, AI ìœ„í—˜ì˜ ìƒí˜¸ ì˜ì¡´ì„±ì„ ì „ì²´ì ìœ¼ë¡œ ì¡°ë§í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê³µì§€ëŠ¥ ìœ„í—˜ì€ ë²•ì , ìš´ì˜ì , ì¬ì •ì  ì¸¡ë©´ì„ í¬í•¨í•œ ë‹¤ì°¨ì›ì ì¸ íŠ¹ì„±ì„ ì§€ë‹ˆê³  ìˆë‹¤.
- 2. ìƒˆë¡œìš´ AI ê·œì œë¡œ ì¸í•´ ì¸ê³µì§€ëŠ¥ ìœ„í—˜ ê´€ë¦¬ì˜ ì„±ìˆ™ë„ê°€ ë‚®ìœ¼ë©°, íŠ¹ì • AI ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ì¶˜ ë§ì¶¤í˜• ìœ„í—˜ ì§€í‘œì™€ ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤.
- 3. ë°ì´í„° ë³´í˜¸, ê³µì •ì„±, ì •í™•ì„±, ê°•ê±´ì„±, ì •ë³´ ë³´ì•ˆ ë“± ì—¬ëŸ¬ ì°¨ì›ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ ìœ„í—˜ ë¬¸ì œë¥¼ ë¶„í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- 4. AI ì‹œìŠ¤í…œì˜ ìœ„í—˜ ê´€ë¦¬ë¥¼ ìœ„í•´ ì ì ˆí•œ ì§€í‘œì™€ ìœ„í—˜ ëª¨ë¸ì„ ê°œë°œí•˜ì—¬ ì˜ì‚¬ê²°ì •ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì´ëŠ” ê²ƒì´ ì£¼ìš” ê³¼ì œì´ë‹¤.
- 5. ì´ ë…¼ë¬¸ì€ AI ì´í•´ê´€ê³„ìë“¤ì—ê²Œ AI ìœ„í—˜ ê´€ë¦¬ì˜ ê¹Šì´ë¥¼ ì•ˆë‚´í•˜ê³ , AI ìœ„í—˜ì˜ ìƒí˜¸ ì˜ì¡´ì„±ì„ ì „ì²´ì ìœ¼ë¡œ ì¡°ë§í•œë‹¤.


---

*Generated on 2025-09-24 13:47:54*