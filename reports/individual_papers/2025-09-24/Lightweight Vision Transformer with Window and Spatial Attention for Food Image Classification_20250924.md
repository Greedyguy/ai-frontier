<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:05:26.785533",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Attention Mechanism",
    "Attention Mechanism",
    "Food Image Classification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.78,
    "Attention Mechanism": 0.77,
    "Food Image Classification": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Vision Transformers are a specific application of Transformers in computer vision, linking to broader Transformer research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Window Multi-Head Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "WMHAM"
        ],
        "category": "specific_connectable",
        "rationale": "This is a specialized form of attention mechanism, crucial for linking to research on efficient attention models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.85,
        "specificity_score": 0.82,
        "link_intent_score": 0.79
      },
      {
        "surface": "Spatial Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "SAM"
        ],
        "category": "specific_connectable",
        "rationale": "Spatial attention is a key concept in enhancing feature representation, relevant to attention mechanism studies.",
        "novelty_score": 0.6,
        "connectivity_score": 0.83,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Food Image Classification",
        "canonical": "Food Image Classification",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a specialized application area in computer vision, linking to domain-specific research.",
        "novelty_score": 0.72,
        "connectivity_score": 0.7,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "food industry",
      "production quality",
      "automated quality control"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Window Multi-Head Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.85,
        "specificity": 0.82,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Spatial Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.83,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Food Image Classification",
      "resolved_canonical": "Food Image Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.7,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18692.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.18692](https://arxiv.org/abs/2509.18692)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (82.7% similar)
- [[2025-09-24/No Labels Needed_ Zero-Shot Image Classification with Collaborative Self-Learning_20250924|No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning]] (82.4% similar)
- [[2025-09-24/Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning_20250924|Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning]] (82.0% similar)
- [[2025-09-18/RoboEye_ Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching_20250918|RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching]] (81.8% similar)
- [[2025-09-22/Saccadic Vision for Fine-Grained Visual Classification_20250922|Saccadic Vision for Fine-Grained Visual Classification]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Food Image Classification|Food Image Classification]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18692v1 Announce Type: new 
Abstract: With the rapid development of society and continuous advances in science and technology, the food industry increasingly demands higher production quality and efficiency. Food image classification plays a vital role in enabling automated quality control on production lines, supporting food safety supervision, and promoting intelligent agricultural production. However, this task faces challenges due to the large number of parameters and high computational complexity of Vision Transformer models. To address these issues, we propose a lightweight food image classification algorithm that integrates a Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism (SAM). The WMHAM reduces computational cost by capturing local and global contextual features through efficient window partitioning, while the SAM adaptively emphasizes key spatial regions to improve discriminative feature representation. Experiments conducted on the Food-101 and Vireo Food-172 datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%, respectively, while significantly reducing parameters and FLOPs compared with baseline methods. These results confirm that the proposed approach achieves an effective balance between computational efficiency and classification performance, making it well-suited for deployment in resource-constrained environments.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹í’ˆ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê²½ëŸ‰ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. Vision Transformer ëª¨ë¸ì˜ ë†’ì€ ê³„ì‚° ë³µì¡ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ìœˆë„ìš° ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜(WMHAM)ê³¼ ê³µê°„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜(SAM)ì„ í†µí•©í–ˆìŠµë‹ˆë‹¤. WMHAMì€ íš¨ìœ¨ì ì¸ ìœˆë„ìš° ë¶„í• ì„ í†µí•´ ì§€ì—­ ë° ì „ì—­ ë¬¸ë§¥ì  íŠ¹ì§•ì„ í¬ì°©í•˜ì—¬ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê³ , SAMì€ ì£¼ìš” ê³µê°„ ì˜ì—­ì„ ê°•ì¡°í•˜ì—¬ íŠ¹ì§• í‘œí˜„ì„ ê°œì„ í•©ë‹ˆë‹¤. Food-101ê³¼ Vireo Food-172 ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ëª¨ë¸ì€ ê°ê° 95.24%ì™€ 94.33%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ê¸°ì¡´ ë°©ë²•ë“¤ì— ë¹„í•´ íŒŒë¼ë¯¸í„°ì™€ FLOPsë¥¼ í¬ê²Œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ì œì•ˆëœ ì ‘ê·¼ë²•ì´ ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ë¶„ë¥˜ ì„±ëŠ¥ ì‚¬ì´ì—ì„œ íš¨ê³¼ì ì¸ ê· í˜•ì„ ì´ë£¨ì–´, ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œì˜ ì ìš©ì— ì í•©í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹í’ˆ ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” ìë™ í’ˆì§ˆ ê´€ë¦¬, ì‹í’ˆ ì•ˆì „ ê°ë…, ì§€ëŠ¥í˜• ë†ì—… ìƒì‚°ì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- 2. Vision Transformer ëª¨ë¸ì˜ ë†’ì€ ê³„ì‚° ë³µì¡ì„±ê³¼ ë§ì€ íŒŒë¼ë¯¸í„°ê°€ ì‹í’ˆ ì´ë¯¸ì§€ ë¶„ë¥˜ì˜ ê³¼ì œë¡œ ì‘ìš©í•œë‹¤.
- 3. ì œì•ˆëœ ê²½ëŸ‰í™” ì•Œê³ ë¦¬ì¦˜ì€ WMHAMê³¼ SAMì„ í†µí•©í•˜ì—¬ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê³ , ì£¼ìš” ê³µê°„ ì˜ì—­ì„ ê°•ì¡°í•˜ì—¬ íŠ¹ì§• í‘œí˜„ì„ ê°œì„ í•œë‹¤.
- 4. Food-101ê³¼ Vireo Food-172 ë°ì´í„°ì…‹ì—ì„œ ê°ê° 95.24%ì™€ 94.33%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ íŒŒë¼ë¯¸í„°ì™€ FLOPsë¥¼ í¬ê²Œ ì¤„ì˜€ë‹¤.
- 5. ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì€ ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ë¶„ë¥˜ ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ ì´ë£¨ì–´, ìì›ì´ ì œí•œëœ í™˜ê²½ì— ì í•©í•˜ë‹¤.


---

*Generated on 2025-09-24 16:05:26*