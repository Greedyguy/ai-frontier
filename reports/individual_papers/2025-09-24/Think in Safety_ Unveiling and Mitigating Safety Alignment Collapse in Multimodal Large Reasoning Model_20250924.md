<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:50:29.191512",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Safety Alignment",
    "Jailbreak Robustness",
    "Safety-Awareness Benchmarks",
    "Multimodal Tuning Dataset"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Safety Alignment": 0.82,
    "Jailbreak Robustness": 0.8,
    "Safety-Awareness Benchmarks": 0.78,
    "Multimodal Tuning Dataset": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Reasoning Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLRMs"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to current trends in integrating multiple data types for reasoning tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "safety alignment",
        "canonical": "Safety Alignment",
        "aliases": [
          "safety compliance",
          "safety assurance"
        ],
        "category": "unique_technical",
        "rationale": "Addresses a critical aspect of AI model deployment, linking to safety-focused research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "jailbreak robustness",
        "canonical": "Jailbreak Robustness",
        "aliases": [
          "anti-jailbreak",
          "robustness against jailbreak"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a specific challenge in AI safety, relevant for security-focused discussions.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "safety-awareness benchmarks",
        "canonical": "Safety-Awareness Benchmarks",
        "aliases": [
          "safety benchmarks",
          "awareness benchmarks"
        ],
        "category": "unique_technical",
        "rationale": "Provides a framework for evaluating safety, crucial for model assessment and improvement.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "multimodal tuning dataset",
        "canonical": "Multimodal Tuning Dataset",
        "aliases": [
          "multimodal dataset",
          "tuning dataset"
        ],
        "category": "unique_technical",
        "rationale": "Supports the development of safer models by providing structured data for tuning.",
        "novelty_score": 0.72,
        "connectivity_score": 0.75,
        "specificity_score": 0.79,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "systematic exploration",
      "comprehensive evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Reasoning Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "safety alignment",
      "resolved_canonical": "Safety Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "jailbreak robustness",
      "resolved_canonical": "Jailbreak Robustness",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "safety-awareness benchmarks",
      "resolved_canonical": "Safety-Awareness Benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multimodal tuning dataset",
      "resolved_canonical": "Multimodal Tuning Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.75,
        "specificity": 0.79,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.06538.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2505.06538](https://arxiv.org/abs/2505.06538)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Reasoning-to-Defend_ Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking_20250923|Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking]] (87.0% similar)
- [[2025-09-24/Steering Multimodal Large Language Models Decoding for Context-Aware Safety_20250924|Steering Multimodal Large Language Models Decoding for Context-Aware Safety]] (86.6% similar)
- [[2025-09-23/Automating Steering for Safe Multimodal Large Language Models_20250923|Automating Steering for Safe Multimodal Large Language Models]] (85.9% similar)
- [[2025-09-22/Red Teaming Multimodal Language Models_ Evaluating Harm Across Prompt Modalities and Models_20250922|Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models]] (85.8% similar)
- [[2025-09-23/SafeEraser_ Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning_20250923|SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Safety Alignment|Safety Alignment]], [[keywords/Jailbreak Robustness|Jailbreak Robustness]], [[keywords/Safety-Awareness Benchmarks|Safety-Awareness Benchmarks]], [[keywords/Multimodal Tuning Dataset|Multimodal Tuning Dataset]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.06538v3 Announce Type: replace 
Abstract: The rapid development of Multimodal Large Reasoning Models (MLRMs) has demonstrated broad application potential, yet their safety and reliability remain critical concerns that require systematic exploration. To address this gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs across 5 benchmarks and unveil prevalent safety degradation phenomena in most advanced models. Moreover, our analysis reveals distinct safety patterns across different benchmarks: significant safety degradation is observed across jailbreak robustness benchmarks, whereas safety-awareness benchmarks demonstrate less pronounced degradation. In particular, the long thought process in some scenarios even enhances safety performance. Therefore, it is a potential approach to address safety issues in MLRMs by leveraging the intrinsic reasoning capabilities of the model to detect unsafe intent. To operationalize this insight, we construct a multimodal tuning dataset that incorporates a safety-oriented thought process. Experimental results from fine-tuning existing MLRMs with this dataset effectively enhances the safety on both jailbreak robustness and safety-awareness benchmarks. This study provides a new perspective for developing safe MLRMs. Our dataset is available at https://github.com/xinyuelou/Think-in-Safety.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€ê·œëª¨ ì¶”ë¡  ëª¨ë¸(MLRMs)ì˜ ì•ˆì „ì„±ê³¼ ì‹ ë¢°ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬, ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ëª¨ë¸ì—ì„œ ì•ˆì „ì„± ì €í•˜ í˜„ìƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, íƒˆì˜¥ ê²¬ê³ ì„± ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ì•ˆì „ì„± ì €í•˜ê°€ ë‘ë“œëŸ¬ì¡Œìœ¼ë‚˜, ì•ˆì „ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ëœí•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ë‚´ì¬ëœ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ì•ˆì „ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŒì„ ì œì•ˆí•˜ë©°, ì•ˆì „ ì§€í–¥ì  ì‚¬ê³  ê³¼ì •ì„ í¬í•¨í•œ ë‹¤ì¤‘ëª¨ë‹¬ íŠœë‹ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ìœ¼ë¡œ MLRMsë¥¼ ë¯¸ì„¸ ì¡°ì •í•œ ê²°ê³¼, ì•ˆì „ì„±ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì•ˆì „í•œ MLRMs ê°œë°œì— ìƒˆë¡œìš´ ê´€ì ì„ ì œê³µí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì€ [GitHub ë§í¬](https://github.com/xinyuelou/Think-in-Safety)ì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€ê·œëª¨ ì¶”ë¡  ëª¨ë¸(MLRMs)ì˜ ì•ˆì „ì„±ê³¼ ì‹ ë¢°ì„± ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ íƒêµ¬í•˜ê¸° ìœ„í•´ 11ê°œì˜ MLRMsë¥¼ 5ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.
- 2. ë¶„ì„ ê²°ê³¼, íƒˆì˜¥ ë‚´ì„± ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ì•ˆì „ì„± ì €í•˜ê°€ ë‘ë“œëŸ¬ì¡Œì§€ë§Œ, ì•ˆì „ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ëœ ë‘ë“œëŸ¬ì§„ ì•ˆì „ì„± ì €í•˜ê°€ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.
- 3. ëª¨ë¸ì˜ ë‚´ì¬ëœ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ì•ˆì „ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ì ì¬ì  ì ‘ê·¼ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.
- 4. ì•ˆì „ ì§€í–¥ì  ì‚¬ê³  ê³¼ì •ì„ í¬í•¨í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ íŠœë‹ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ ê¸°ì¡´ MLRMsì˜ ì•ˆì „ì„±ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ëŠ” ì•ˆì „í•œ MLRMs ê°œë°œì„ ìœ„í•œ ìƒˆë¡œìš´ ê´€ì ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:50:29*