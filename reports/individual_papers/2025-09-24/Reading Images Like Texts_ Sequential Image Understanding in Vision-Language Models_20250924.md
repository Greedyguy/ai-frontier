<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:17:58.474053",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Dual-Stream Hypothesis",
    "Object Recognition",
    "Spatial Perception",
    "RoPE Scaling Technique"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.9,
    "Dual-Stream Hypothesis": 0.75,
    "Object Recognition": 0.85,
    "Spatial Perception": 0.8,
    "RoPE Scaling Technique": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept is central to the paper and connects with recent trends in multimodal learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "dual-stream hypothesis",
        "canonical": "Dual-Stream Hypothesis",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "It provides a theoretical framework for understanding visual processing in VLMs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "object recognition",
        "canonical": "Object Recognition",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "This is a foundational concept in computer vision and relevant to the paper's focus.",
        "novelty_score": 0.3,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "spatial perception",
        "canonical": "Spatial Perception",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "It is a key component of the paper's analysis of VLMs' internal mechanisms.",
        "novelty_score": 0.65,
        "connectivity_score": 0.72,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "RoPE scaling technique",
        "canonical": "RoPE Scaling Technique",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for enhancing spatial reasoning in VLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "instruction-agnostic token compression",
      "plug-and-play visual decoder"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "dual-stream hypothesis",
      "resolved_canonical": "Dual-Stream Hypothesis",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "object recognition",
      "resolved_canonical": "Object Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "spatial perception",
      "resolved_canonical": "Spatial Perception",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.72,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "RoPE scaling technique",
      "resolved_canonical": "RoPE Scaling Technique",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19191.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19191](https://arxiv.org/abs/2509.19191)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (88.2% similar)
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (87.8% similar)
- [[2025-09-23/SD-VLM_ Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models_20250923|SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models]] (87.4% similar)
- [[2025-09-24/How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective_20250924|How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective]] (87.2% similar)
- [[2025-09-23/Evo-0_ Vision-Language-Action Model with Implicit Spatial Understanding_20250923|Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding]] (86.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Object Recognition|Object Recognition]]
**âš¡ Unique Technical**: [[keywords/Dual-Stream Hypothesis|Dual-Stream Hypothesis]], [[keywords/Spatial Perception|Spatial Perception]], [[keywords/RoPE Scaling Technique|RoPE Scaling Technique]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19191v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the "what" and "where" pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model's perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Vision-Language Models(VLMs)ì˜ ì‹œê° ì •ë³´ ì²˜ë¦¬ ë°©ì‹ì„ ì¸ê°„ì˜ ì‹œê°ê³¼ ìœ ì‚¬í•œ ì´ì¤‘ ê²½ë¡œ ê°€ì„¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ì—°êµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ VLMsëŠ” ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì§ë ¬í™”í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ”ë°, ì´ëŠ” ì¸ê°„ì˜ ë³‘ë ¬ì  ì‹œê° ì²˜ë¦¬ì™€ ë‹¤ë¦…ë‹ˆë‹¤. ì—°êµ¬ëŠ” VLMsì˜ ì‹œê° ì²˜ë¦¬ë¥¼ ê°ì²´ ì¸ì‹ê³¼ ê³µê°„ ì¸ì‹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„í•©ë‹ˆë‹¤. ê°ì²´ ì¸ì‹ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ í† í° ë§µìœ¼ë¡œ ë³€í™˜í•˜ì—¬, ëª¨ë¸ì´ ì–•ì€ ì¸µì—ì„œ ì†ì„± ì¸ì‹ì„ ì‹œì‘ìœ¼ë¡œ ê¹Šì€ ì¸µì—ì„œ ì˜ë¯¸ì  ëª¨í˜¸ì„±ì„ í•´ì†Œí•˜ëŠ” 2ë‹¨ê³„ ê³¼ì •ì„ ê±°ì¹œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê³µê°„ ì¸ì‹ì—ì„œëŠ” VLMsì˜ ìœ„ì¹˜ í‘œí˜„ì— ë‚´ì¬ëœ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë„ì¶œí•˜ê³  ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í–ˆìŠµë‹ˆë‹¤. ì´ ë°œê²¬ì„ ë°”íƒ•ìœ¼ë¡œ, ë””ì½”ë”© íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” í† í° ì••ì¶• ì•Œê³ ë¦¬ì¦˜ê³¼ ê³µê°„ ì¶”ë¡ ì„ ê°•í™”í•˜ëŠ” RoPE ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´ ì´ëŸ¬í•œ ë¶„ì„ì„ ê²€ì¦í•˜ë©°, VLMsì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ê¹Šì€ ì´í•´ì™€ í–¥í›„ ë” ë°œì „ëœ ì•„í‚¤í…ì²˜ ì„¤ê³„ë¥¼ ìœ„í•œ ëª…í™•í•œ ì›ì¹™ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ Vision-Language Models(VLMs)ëŠ” ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì§ë ¬í™”í•˜ì—¬ ì²˜ë¦¬í•˜ë©°, ì´ëŠ” ì¸ê°„ì˜ ë³‘ë ¬ì  ì‹œê° ì²˜ë¦¬ ë°©ì‹ê³¼ ë‹¤ë¥´ë‹¤.
- 2. ì¸ê°„ ì‹œê°ì˜ ì´ì¤‘ ê²½ë¡œ ê°€ì„¤ì— ì˜ê°ì„ ë°›ì•„, VLMì˜ ì‹œê° ì²˜ë¦¬ë¥¼ ê°ì²´ ì¸ì‹ê³¼ ê³µê°„ ì§€ê°ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì—°êµ¬í•˜ì˜€ë‹¤.
- 3. ê°ì²´ ì¸ì‹ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ í† í° ë§µìœ¼ë¡œ ë³€í™˜í•˜ì—¬, ëª¨ë¸ì´ ì´ë¯¸ì§€ ì½˜í…ì¸ ë¥¼ ì–•ì€ ì¸µì—ì„œ ê¹Šì€ ì¸µìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ë‘ ë‹¨ê³„ ê³¼ì •ì„ ë°œê²¬í•˜ì˜€ë‹¤.
- 4. ê³µê°„ ì§€ê°ì—ì„œëŠ” VLMì˜ ìœ„ì¹˜ í‘œí˜„ì— ë‚´ì¬ëœ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë„ì¶œí•˜ê³  ì‹¤ì¦ì ìœ¼ë¡œ ê²€ì¦í•˜ì˜€ë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë””ì½”ë”© íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” í† í° ì••ì¶• ì•Œê³ ë¦¬ì¦˜ê³¼ ê³µê°„ ì¶”ë¡ ì„ ê°•í™”í•˜ëŠ” RoPE ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 16:17:58*