<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:40:26.536052",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Prior-based Data Filtering",
    "Perplexity-based Filtering",
    "Multilingual Corpora"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Prior-based Data Filtering": 0.7,
    "Perplexity-based Filtering": 0.75,
    "Multilingual Corpora": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "This is a foundational concept in NLP, connecting to numerous related topics.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Prior-based Data Filtering",
        "canonical": "Prior-based Data Filtering",
        "aliases": [
          "Token Priors Filtering"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel method for data filtering, relevant for efficient model training.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Perplexity-based Filtering",
        "canonical": "Perplexity-based Filtering",
        "aliases": [
          "PPL Filtering"
        ],
        "category": "specific_connectable",
        "rationale": "A well-known technique in NLP, providing a point of comparison for the proposed method.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multilingual Corpora",
        "canonical": "Multilingual Corpora",
        "aliases": [
          "Multilingual Datasets"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the adaptability of the method across different languages, relevant for global applications.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "data"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Prior-based Data Filtering",
      "resolved_canonical": "Prior-based Data Filtering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Perplexity-based Filtering",
      "resolved_canonical": "Perplexity-based Filtering",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multilingual Corpora",
      "resolved_canonical": "Multilingual Corpora",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18577.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18577](https://arxiv.org/abs/2509.18577)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (83.6% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (83.1% similar)
- [[2025-09-23/LASER_ Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy_20250923|LASER: Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy]] (83.0% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.7% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Perplexity-based Filtering|Perplexity-based Filtering]], [[keywords/Multilingual Corpora|Multilingual Corpora]]
**âš¡ Unique Technical**: [[keywords/Prior-based Data Filtering|Prior-based Data Filtering]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18577v1 Announce Type: new 
Abstract: As large language models (LLMs) are pretrained on massive web corpora, careful selection of data becomes essential to ensure effective and efficient learning. While perplexity (PPL)-based filtering has shown strong performance, it suffers from drawbacks: substantial time costs and inherent unreliability of the model when handling noisy or out-of-distribution samples. In this work, we propose a simple yet powerful alternative: a prior-based data filtering method that estimates token priors using corpus-level term frequency statistics, inspired by linguistic insights on word roles and lexical density. Our approach filters documents based on the mean and standard deviation of token priors, serving as a fast proxy to PPL while requiring no model inference. Despite its simplicity, the prior-based filter achieves the highest average performance across 20 downstream benchmarks, while reducing time cost by over 1000x compared to PPL-based filtering. We further demonstrate its applicability to symbolic languages such as code and math, and its dynamic adaptability to multilingual corpora without supervision

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì„ íƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ê¸°ì¡´ì˜ ë‹¹í˜¹ë„(PPL) ê¸°ë°˜ í•„í„°ë§ì˜ ì‹œê°„ ì†Œëª¨ì™€ ì‹ ë¢°ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ì½”í¼ìŠ¤ ìˆ˜ì¤€ì˜ ìš©ì–´ ë¹ˆë„ í†µê³„ë¥¼ í™œìš©í•˜ì—¬ í† í°ì˜ ì‚¬ì „ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” í•„í„°ë§ ê¸°ë²•ìœ¼ë¡œ, ëª¨ë¸ ì¶”ë¡  ì—†ì´ë„ ë¹ ë¥´ê²Œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ 20ê°œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, PPL ê¸°ë°˜ í•„í„°ë§ì— ë¹„í•´ ì‹œê°„ ë¹„ìš©ì„ 1000ë°° ì´ìƒ ì ˆê°í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì½”ë“œì™€ ìˆ˜í•™ ê°™ì€ ìƒì§•ì  ì–¸ì–´ì™€ ë‹¤êµ­ì–´ ì½”í¼ìŠ¤ì—ë„ ìœ ì—°í•˜ê²Œ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ê³¼ì  í•™ìŠµì„ ìœ„í•´ ë°ì´í„° ì„ íƒì´ ì¤‘ìš”í•˜ë©°, ê¸°ì¡´ì˜ PPL ê¸°ë°˜ í•„í„°ë§ì€ ì‹œê°„ ì†Œëª¨ì™€ ì‹ ë¢°ì„± ë¬¸ì œë¥¼ ê°€ì§„ë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì½”í¼ìŠ¤ ìˆ˜ì¤€ì˜ ìš©ì–´ ë¹ˆë„ í†µê³„ë¥¼ í™œìš©í•œ ì‚¬ì „ ê¸°ë°˜ ë°ì´í„° í•„í„°ë§ ë°©ë²•ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” PPLì˜ ë¹ ë¥¸ ëŒ€ì•ˆì´ ëœë‹¤.
- 3. ì œì•ˆëœ ì‚¬ì „ ê¸°ë°˜ í•„í„°ëŠ” ëª¨ë¸ ì¶”ë¡ ì´ í•„ìš” ì—†ìœ¼ë©°, 20ê°œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœê³  í‰ê·  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ì‹œê°„ ë¹„ìš©ì„ 1000ë°° ì´ìƒ ì ˆê°í•œë‹¤.
- 4. ì´ ë°©ë²•ì€ ì½”ë“œ ë° ìˆ˜í•™ê³¼ ê°™ì€ ìƒì§•ì  ì–¸ì–´ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë©°, ê°ë… ì—†ì´ ë‹¤êµ­ì–´ ì½”í¼ìŠ¤ì— ë™ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-24 15:40:26*