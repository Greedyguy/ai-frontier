<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:03:17.622876",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Architecture, Engineering, and Construction",
    "Knowledge Evaluation",
    "Cognition-oriented Evaluation Framework",
    "LLM-as-a-Judge"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Architecture, Engineering, and Construction": 0.85,
    "Knowledge Evaluation": 0.8,
    "Cognition-oriented Evaluation Framework": 0.78,
    "LLM-as-a-Judge": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus on evaluating their capabilities in the AEC domain.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.5,
        "link_intent_score": 0.9
      },
      {
        "surface": "Architecture, Engineering, and Construction",
        "canonical": "Architecture, Engineering, and Construction",
        "aliases": [
          "AEC"
        ],
        "category": "unique_technical",
        "rationale": "The AEC domain is the specific application area for the benchmark discussed in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Knowledge Evaluation",
        "canonical": "Knowledge Evaluation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Knowledge evaluation is a key aspect of assessing LLM performance in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cognition-oriented Evaluation Framework",
        "canonical": "Cognition-oriented Evaluation Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This framework is a novel approach introduced in the paper to evaluate LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.5,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "LLM-as-a-Judge",
        "canonical": "LLM-as-a-Judge",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique methodology proposed for evaluating LLM responses in the paper.",
        "novelty_score": 0.8,
        "connectivity_score": 0.45,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.5,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Architecture, Engineering, and Construction",
      "resolved_canonical": "Architecture, Engineering, and Construction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Knowledge Evaluation",
      "resolved_canonical": "Knowledge Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cognition-oriented Evaluation Framework",
      "resolved_canonical": "Cognition-oriented Evaluation Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.5,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LLM-as-a-Judge",
      "resolved_canonical": "LLM-as-a-Judge",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.45,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18776.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18776](https://arxiv.org/abs/2509.18776)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (91.2% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (87.0% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (87.0% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (86.0% similar)
- [[2025-09-23/Advanced Financial Reasoning at Scale_ A Comprehensive Evaluation of Large Language Models on CFA Level III_20250923|Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III]] (85.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Architecture, Engineering, and Construction|Architecture, Engineering, and Construction]], [[keywords/Knowledge Evaluation|Knowledge Evaluation]], [[keywords/Cognition-oriented Evaluation Framework|Cognition-oriented Evaluation Framework]], [[keywords/LLM-as-a-Judge|LLM-as-a-Judge]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18776v1 Announce Type: cross 
Abstract: Large language models (LLMs), as a novel information technology, are seeing increasing adoption in the Architecture, Engineering, and Construction (AEC) field. They have shown their potential to streamline processes throughout the building lifecycle. However, the robustness and reliability of LLMs in such a specialized and safety-critical domain remain to be evaluated. To address this challenge, this paper establishes AECBench, a comprehensive benchmark designed to quantify the strengths and limitations of current LLMs in the AEC domain. The benchmark defines 23 representative tasks within a five-level cognition-oriented evaluation framework encompassing Knowledge Memorization, Understanding, Reasoning, Calculation, and Application. These tasks were derived from authentic AEC practice, with scope ranging from codes retrieval to specialized documents generation. Subsequently, a 4,800-question dataset encompassing diverse formats, including open-ended questions, was crafted primarily by engineers and validated through a two-round expert review. Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable and consistent methodology for evaluating complex, long-form responses leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear performance decline across five cognitive levels was revealed. Despite demonstrating proficiency in foundational tasks at the Knowledge Memorization and Understanding levels, the models showed significant performance deficits, particularly in interpreting knowledge from tables in building codes, executing complex reasoning and calculation, and generating domain-specific documents. Consequently, this study lays the groundwork for future research and development aimed at the robust and reliable integration of LLMs into safety-critical engineering practices.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê±´ì¶•, ì—”ì§€ë‹ˆì–´ë§, ê±´ì„¤(AEC) ë¶„ì•¼ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í™œìš© ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ AECBenchë¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. AECBenchëŠ” ì§€ì‹ ì•”ê¸°, ì´í•´, ì¶”ë¡ , ê³„ì‚°, ì‘ìš©ì˜ ë‹¤ì„¯ ê°€ì§€ ì¸ì§€ ìˆ˜ì¤€ì—ì„œ 23ê°œì˜ ëŒ€í‘œ ê³¼ì œë¥¼ í†µí•´ LLMì˜ ê°•ì ê³¼ í•œê³„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. 4,800ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì€ ì£¼ë¡œ ì—”ì§€ë‹ˆì–´ë“¤ì´ ì‘ì„±í•˜ê³  ì „ë¬¸ê°€ ê²€í† ë¥¼ ê±°ì³¤ìŠµë‹ˆë‹¤. ë˜í•œ, LLMì„ ì‹¬íŒìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ë¡ ì„ ë„ì…í•˜ì—¬ ë³µì¡í•œ ì‘ë‹µì„ í‰ê°€í•©ë‹ˆë‹¤. ì•„í™‰ ê°œì˜ LLMì„ í‰ê°€í•œ ê²°ê³¼, ê¸°ë³¸ì ì¸ ì§€ì‹ ì•”ê¸°ì™€ ì´í•´ì—ì„œëŠ” ëŠ¥ìˆ™í–ˆìœ¼ë‚˜, ë³µì¡í•œ ì¶”ë¡ ê³¼ ê³„ì‚°, ë„ë©”ì¸ íŠ¹í™” ë¬¸ì„œ ìƒì„±ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ê°€ ë‘ë“œëŸ¬ì¡ŒìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ì•ˆì „í•œ ì—”ì§€ë‹ˆì–´ë§ ì‹¤ë¬´ í†µí•©ì„ ìœ„í•œ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ê±´ì¶•, ì—”ì§€ë‹ˆì–´ë§ ë° ê±´ì„¤(AEC) ë¶„ì•¼ì—ì„œ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ëŠ” ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.
- 2. AECBenchë¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•´ LLMì˜ ê°•ì ê³¼ í•œê³„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ 23ê°œì˜ ëŒ€í‘œ ê³¼ì œë¥¼ ì •ì˜í•˜ì˜€ë‹¤.
- 3. 4,800ê°œì˜ ì§ˆë¬¸ ë°ì´í„°ì…‹ì€ ì£¼ë¡œ ì—”ì§€ë‹ˆì–´ì— ì˜í•´ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì „ë¬¸ê°€ ê²€í† ë¥¼ í†µí•´ ê²€ì¦ë˜ì—ˆë‹¤.
- 4. LLM-as-a-Judge ì ‘ê·¼ë²•ì„ ë„ì…í•˜ì—¬ ë³µì¡í•œ ì‘ë‹µì„ í‰ê°€í•˜ëŠ” ì¼ê´€ëœ ë°©ë²•ë¡ ì„ ì œê³µí•˜ì˜€ë‹¤.
- 5. LLMì€ ê¸°ë³¸ì ì¸ ì§€ì‹ ì•”ê¸°ì™€ ì´í•´ ìˆ˜ì¤€ì—ì„œëŠ” ëŠ¥ìˆ™í–ˆìœ¼ë‚˜, ë³µì¡í•œ ì¶”ë¡  ë° ê³„ì‚°, ë„ë©”ì¸ íŠ¹í™” ë¬¸ì„œ ìƒì„±ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 14:03:17*