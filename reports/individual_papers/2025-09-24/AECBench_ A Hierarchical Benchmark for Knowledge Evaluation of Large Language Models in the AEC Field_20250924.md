<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:03:17.622876",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Architecture, Engineering, and Construction",
    "Knowledge Evaluation",
    "Cognition-oriented Evaluation Framework",
    "LLM-as-a-Judge"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.9,
    "Architecture, Engineering, and Construction": 0.85,
    "Knowledge Evaluation": 0.8,
    "Cognition-oriented Evaluation Framework": 0.78,
    "LLM-as-a-Judge": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's focus on evaluating their capabilities in the AEC domain.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.5,
        "link_intent_score": 0.9
      },
      {
        "surface": "Architecture, Engineering, and Construction",
        "canonical": "Architecture, Engineering, and Construction",
        "aliases": [
          "AEC"
        ],
        "category": "unique_technical",
        "rationale": "The AEC domain is the specific application area for the benchmark discussed in the paper.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Knowledge Evaluation",
        "canonical": "Knowledge Evaluation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Knowledge evaluation is a key aspect of assessing LLM performance in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.55,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cognition-oriented Evaluation Framework",
        "canonical": "Cognition-oriented Evaluation Framework",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This framework is a novel approach introduced in the paper to evaluate LLMs.",
        "novelty_score": 0.75,
        "connectivity_score": 0.5,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "LLM-as-a-Judge",
        "canonical": "LLM-as-a-Judge",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a unique methodology proposed for evaluating LLM responses in the paper.",
        "novelty_score": 0.8,
        "connectivity_score": 0.45,
        "specificity_score": 0.88,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "evaluation",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.5,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Architecture, Engineering, and Construction",
      "resolved_canonical": "Architecture, Engineering, and Construction",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Knowledge Evaluation",
      "resolved_canonical": "Knowledge Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.55,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cognition-oriented Evaluation Framework",
      "resolved_canonical": "Cognition-oriented Evaluation Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.5,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LLM-as-a-Judge",
      "resolved_canonical": "LLM-as-a-Judge",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.45,
        "specificity": 0.88,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18776.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18776](https://arxiv.org/abs/2509.18776)

## 🔗 유사한 논문
- [[2025-09-23/EngiBench_ A Benchmark for Evaluating Large Language Models on Engineering Problem Solving_20250923|EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving]] (91.2% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (87.0% similar)
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (87.0% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (86.0% similar)
- [[2025-09-23/Advanced Financial Reasoning at Scale_ A Comprehensive Evaluation of Large Language Models on CFA Level III_20250923|Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III]] (85.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**⚡ Unique Technical**: [[keywords/Architecture, Engineering, and Construction|Architecture, Engineering, and Construction]], [[keywords/Knowledge Evaluation|Knowledge Evaluation]], [[keywords/Cognition-oriented Evaluation Framework|Cognition-oriented Evaluation Framework]], [[keywords/LLM-as-a-Judge|LLM-as-a-Judge]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18776v1 Announce Type: cross 
Abstract: Large language models (LLMs), as a novel information technology, are seeing increasing adoption in the Architecture, Engineering, and Construction (AEC) field. They have shown their potential to streamline processes throughout the building lifecycle. However, the robustness and reliability of LLMs in such a specialized and safety-critical domain remain to be evaluated. To address this challenge, this paper establishes AECBench, a comprehensive benchmark designed to quantify the strengths and limitations of current LLMs in the AEC domain. The benchmark defines 23 representative tasks within a five-level cognition-oriented evaluation framework encompassing Knowledge Memorization, Understanding, Reasoning, Calculation, and Application. These tasks were derived from authentic AEC practice, with scope ranging from codes retrieval to specialized documents generation. Subsequently, a 4,800-question dataset encompassing diverse formats, including open-ended questions, was crafted primarily by engineers and validated through a two-round expert review. Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable and consistent methodology for evaluating complex, long-form responses leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear performance decline across five cognitive levels was revealed. Despite demonstrating proficiency in foundational tasks at the Knowledge Memorization and Understanding levels, the models showed significant performance deficits, particularly in interpreting knowledge from tables in building codes, executing complex reasoning and calculation, and generating domain-specific documents. Consequently, this study lays the groundwork for future research and development aimed at the robust and reliable integration of LLMs into safety-critical engineering practices.

## 📝 요약

이 논문은 건축, 엔지니어링, 건설(AEC) 분야에서 대형 언어 모델(LLM)의 활용 가능성을 평가하기 위해 AECBench라는 벤치마크를 제시합니다. AECBench는 지식 암기, 이해, 추론, 계산, 응용의 다섯 가지 인지 수준에서 23개의 대표 과제를 통해 LLM의 강점과 한계를 평가합니다. 4,800개의 질문으로 구성된 데이터셋은 주로 엔지니어들이 작성하고 전문가 검토를 거쳤습니다. 또한, LLM을 심판으로 활용하는 방법론을 도입하여 복잡한 응답을 평가합니다. 아홉 개의 LLM을 평가한 결과, 기본적인 지식 암기와 이해에서는 능숙했으나, 복잡한 추론과 계산, 도메인 특화 문서 생성에서는 성능 저하가 두드러졌습니다. 이 연구는 LLM의 안전한 엔지니어링 실무 통합을 위한 기초를 마련합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 건축, 엔지니어링 및 건설(AEC) 분야에서 프로세스를 간소화하는 잠재력을 보여주고 있다.
- 2. AECBench라는 벤치마크를 통해 LLM의 강점과 한계를 평가하기 위한 23개의 대표 과제를 정의하였다.
- 3. 4,800개의 질문 데이터셋은 주로 엔지니어에 의해 작성되었으며, 전문가 검토를 통해 검증되었다.
- 4. LLM-as-a-Judge 접근법을 도입하여 복잡한 응답을 평가하는 일관된 방법론을 제공하였다.
- 5. LLM은 기본적인 지식 암기와 이해 수준에서는 능숙했으나, 복잡한 추론 및 계산, 도메인 특화 문서 생성에서는 성능 저하를 보였다.


---

*Generated on 2025-09-24 14:03:17*