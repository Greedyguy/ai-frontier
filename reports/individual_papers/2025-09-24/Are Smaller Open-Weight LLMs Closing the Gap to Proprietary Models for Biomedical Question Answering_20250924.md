<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:14:13.601101",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Biomedical Question Answering",
    "Open-weight Model",
    "Ensemble Learning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Biomedical Question Answering": 0.8,
    "Open-weight Model": 0.78,
    "Ensemble Learning": 0.77,
    "Few-Shot Learning": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on comparing model types for biomedical question answering.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Biomedical Question Answering",
        "canonical": "Biomedical Question Answering",
        "aliases": [
          "BioQA"
        ],
        "category": "unique_technical",
        "rationale": "Specific domain application of LLMs discussed in the paper.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Open-weight Models",
        "canonical": "Open-weight Model",
        "aliases": [
          "Open-weight LLMs"
        ],
        "category": "unique_technical",
        "rationale": "Highlights the focus on open-source alternatives to proprietary models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Ensemble Approaches",
        "canonical": "Ensemble Learning",
        "aliases": [
          "Ensemble Methods"
        ],
        "category": "specific_connectable",
        "rationale": "Describes a method used to enhance model performance in the study.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      },
      {
        "surface": "In-context Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "In-context Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant technique for improving model performance with minimal data.",
        "novelty_score": 0.6,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "Task 13B Phase B",
      "BioASQ challenge",
      "DeepSeek-V3"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Biomedical Question Answering",
      "resolved_canonical": "Biomedical Question Answering",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Open-weight Models",
      "resolved_canonical": "Open-weight Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Ensemble Approaches",
      "resolved_canonical": "Ensemble Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "In-context Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18843.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18843](https://arxiv.org/abs/2509.18843)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges_20250922|Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges]] (85.1% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (84.1% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (84.0% similar)
- [[2025-09-23/Beyond Prompting_ An Efficient Embedding Framework for Open-Domain Question Answering_20250923|Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering]] (83.8% similar)
- [[2025-09-23/LLaSA_ A Sensor-Aware LLM for Natural Language Reasoning of Human Activity from IMU Data_20250923|LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human Activity from IMU Data]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Ensemble Learning|Ensemble Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Biomedical Question Answering|Biomedical Question Answering]], [[keywords/Open-weight Model|Open-weight Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18843v1 Announce Type: cross 
Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing, with state-of-the-art models like DeepSeek-V3 now performing comparably to proprietary LLMs. This progression raises the question of whether small open-weight LLMs are capable of effectively replacing larger closed-source models. We are particularly interested in the context of biomedical question-answering, a domain we explored by participating in Task 13B Phase B of the BioASQ challenge. In this work, we compare several open-weight models against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and Claude 3.7 Sonnet. To enhance question answering capabilities, we use various techniques including retrieving the most relevant snippets based on embedding distance, in-context learning, and structured outputs. For certain submissions, we utilize ensemble approaches to leverage the diverse outputs generated by different models for exact-answer questions. Our results demonstrate that open-weight LLMs are comparable to proprietary ones. In some instances, open-weight LLMs even surpassed their closed counterparts, particularly when ensembling strategies were applied. All code is publicly available at https://github.com/evidenceprime/BioASQ-13b.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê³µê°œ ê°€ì¤‘ì¹˜ ë²„ì „ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ DeepSeek-V3ì™€ ê°™ì€ ìµœì‹  ëª¨ë¸ì´ ë…ì  LLMê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŒì„ ë‹¤ë£¹ë‹ˆë‹¤. ì—°êµ¬ëŠ” ë°”ì´ì˜¤ë©”ë””ì»¬ ì§ˆë¬¸-ì‘ë‹µ ë¶„ì•¼ì—ì„œ ì†Œí˜• ê³µê°œ ê°€ì¤‘ì¹˜ LLMì´ ëŒ€í˜• íì‡„í˜• ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. BioASQ ì±Œë¦°ì§€ì˜ Task 13B Phase Bì— ì°¸ì—¬í•˜ì—¬ ì—¬ëŸ¬ ê³µê°œ ê°€ì¤‘ì¹˜ ëª¨ë¸ì„ GPT-4o, GPT-4.1, Claude 3.5 Sonnet, Claude 3.7 Sonnet ë“±ê³¼ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ ì‘ë‹µ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì„ë² ë”© ê±°ë¦¬ ê¸°ë°˜ì˜ ê´€ë ¨ ìŠ¤ë‹ˆí« ê²€ìƒ‰, ë§¥ë½ ë‚´ í•™ìŠµ, êµ¬ì¡°í™”ëœ ì¶œë ¥ ë“± ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì‚¬ìš©í–ˆìœ¼ë©°, ì¼ë¶€ ì œì¶œì—ì„œëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì¶œë ¥ì„ í™œìš©í•œ ì•™ìƒë¸” ì ‘ê·¼ë²•ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ê³µê°œ ê°€ì¤‘ì¹˜ LLMì€ ë…ì  ëª¨ë¸ê³¼ ë¹„êµí•  ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ ì•™ìƒë¸” ì „ëµì„ ì ìš©í–ˆì„ ë•ŒëŠ” íì‡„í˜• ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœì‹  ì˜¤í”ˆ ê°€ì¤‘ì¹˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ë“¤ì´ DeepSeek-V3ì™€ ê°™ì€ ëª¨ë¸ì„ í†µí•´ ë…ì  LLMê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤.
- 2. ì†Œí˜• ì˜¤í”ˆ ê°€ì¤‘ì¹˜ LLMì´ ëŒ€í˜• íì‡„í˜• ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì˜ë¬¸ì´ ì œê¸°ë˜ê³  ìˆë‹¤.
- 3. ë°”ì´ì˜¤ë©”ë””ì»¬ ì§ˆë¬¸-ì‘ë‹µ ë¶„ì•¼ì—ì„œ ì˜¤í”ˆ ê°€ì¤‘ì¹˜ ëª¨ë¸ê³¼ GPT-4o, Claude 3.5 Sonnet ë“±ê³¼ ê°™ì€ ìƒìœ„ ì„±ëŠ¥ ì‹œìŠ¤í…œì„ ë¹„êµí•˜ì˜€ë‹¤.
- 4. ì§ˆë¬¸ ì‘ë‹µ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì„ë² ë”© ê±°ë¦¬ ê¸°ë°˜ì˜ ê´€ë ¨ ìŠ¤ë‹ˆí« ê²€ìƒ‰, ë§¥ë½ ë‚´ í•™ìŠµ, êµ¬ì¡°í™”ëœ ì¶œë ¥ ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
- 5. ì˜¤í”ˆ ê°€ì¤‘ì¹˜ LLMì´ ë…ì  ëª¨ë¸ê³¼ ë¹„êµí•  ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì¼ë¶€ ê²½ìš°ì—ëŠ” ì•™ìƒë¸” ì „ëµì„ í†µí•´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 15:14:13*