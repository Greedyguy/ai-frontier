<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:34:22.754689",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mobile GUI Agents",
    "Reinforcement Learning",
    "Vision-Language Model",
    "Difficulty-Adaptive GRPO",
    "Shortest Path Reward Adjustment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mobile GUI Agents": 0.78,
    "Reinforcement Learning": 0.85,
    "Vision-Language Model": 0.8,
    "Difficulty-Adaptive GRPO": 0.77,
    "Shortest Path Reward Adjustment": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mobile GUI Agents",
        "canonical": "Mobile GUI Agents",
        "aliases": [
          "Mobile Graphical User Interface Agents"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specialized application of reinforcement learning in mobile environments, which is central to the paper's contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A foundational technique used in the framework, connecting to a wide range of related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision Language Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Highlights the integration of visual and linguistic data, a key aspect of modern AI systems.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Difficulty-Adaptive GRPO",
        "canonical": "Difficulty-Adaptive GRPO",
        "aliases": [
          "ADAGRPO"
        ],
        "category": "unique_technical",
        "rationale": "A novel algorithm introduced in the paper, crucial for adapting to task difficulty in RL.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.77
      },
      {
        "surface": "Shortest Path Reward Adjustment",
        "canonical": "Shortest Path Reward Adjustment",
        "aliases": [
          "Reward Adjustment Strategy"
        ],
        "category": "unique_technical",
        "rationale": "A specific strategy for reward shaping in multi-turn tasks, enhancing RL performance.",
        "novelty_score": 0.7,
        "connectivity_score": 0.55,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "framework",
      "environment sampling"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mobile GUI Agents",
      "resolved_canonical": "Mobile GUI Agents",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Difficulty-Adaptive GRPO",
      "resolved_canonical": "Difficulty-Adaptive GRPO",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Shortest Path Reward Adjustment",
      "resolved_canonical": "Shortest Path Reward Adjustment",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.55,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18119.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18119](https://arxiv.org/abs/2509.18119)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/AppAgent v2_ Advanced Agent for Flexible Mobile Interactions_20250918|AppAgent v2: Advanced Agent for Flexible Mobile Interactions]] (83.8% similar)
- [[2025-09-23/Mano Report_20250923|Mano Report]] (83.6% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (83.1% similar)
- [[2025-09-23/Generalizable End-to-End Tool-Use RL with Synthetic CodeGym_20250923|Generalizable End-to-End Tool-Use RL with Synthetic CodeGym]] (82.9% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Mobile GUI Agents|Mobile GUI Agents]], [[keywords/Difficulty-Adaptive GRPO|Difficulty-Adaptive GRPO]], [[keywords/Shortest Path Reward Adjustment|Shortest Path Reward Adjustment]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18119v1 Announce Type: cross 
Abstract: Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MOBILERL to enhance GUI agents in mobile environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted in the AutoGLM products, and also open-sourced at https://github.com/THUDM/MobileRL.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ GUI ì—ì´ì „íŠ¸ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•œ ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ MOBILERLì„ ì œì•ˆí•©ë‹ˆë‹¤. í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì¸ ADAGRPOëŠ” ë‚œì´ë„ ì ì‘í˜• ê¸ì •ì  ì¬ìƒê³¼ ì‹¤íŒ¨ ì»¤ë¦¬í˜ëŸ¼ í•„í„°ë§ì„ í†µí•´ ë‹¤ì–‘í•œ ê³¼ì œ ë‚œì´ë„ì— ëª¨ë¸ì„ ì ì‘ì‹œí‚µë‹ˆë‹¤. ë˜í•œ, ë‹¤ì¤‘ í„´ ê³¼ì œì—ì„œ ë³´ìƒì„ ì¬êµ¬ì„±í•˜ëŠ” ìµœë‹¨ ê²½ë¡œ ë³´ìƒ ì¡°ì • ì „ëµì„ ë„ì…í•˜ì—¬ ê°•í™” í•™ìŠµì˜ ì•ˆì •ì„±ê³¼ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤. MOBILERLì€ Qwen2.5-VL-7B-Instructì™€ GLM-4.1V-9B-Base ëª¨ë¸ì— ì ìš©ë˜ì–´ AndroidWorldì™€ AndroidLabì—ì„œ ê°ê° 75.8%ì™€ 46.8%ì˜ ì„±ê³µë¥ ì„ ê¸°ë¡í•˜ë©° ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” AutoGLM ì œí’ˆì— ì±„íƒë˜ì—ˆìœ¼ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MOBILERLì€ ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ GUI ì—ì´ì „íŠ¸ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•œ ì˜¨ë¼ì¸ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ADAGRPO ì•Œê³ ë¦¬ì¦˜ì€ ë‚œì´ë„ ì ì‘í˜• ê¸ì •ì  ì¬ìƒ ë° ì‹¤íŒ¨ ì»¤ë¦¬í˜ëŸ¼ í•„í„°ë§ì„ í†µí•´ ë‹¤ì–‘í•œ ì‘ì—… ë‚œì´ë„ì— ëª¨ë¸ì„ ì ì‘ì‹œí‚µë‹ˆë‹¤.
- 3. ìµœë‹¨ ê²½ë¡œ ë³´ìƒ ì¡°ì • ì „ëµì„ ë„ì…í•˜ì—¬ ë‹¤ì¤‘ í„´ ì—ì´ì „íŠ¸ ì‘ì—…ì—ì„œ ì‘ì—… ê¸¸ì´ì— ë”°ë¥¸ ë³´ìƒì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
- 4. MOBILERL-9B ëª¨ë¸ì€ AndroidWorldì™€ AndroidLabì—ì„œ ê°ê° 75.8%ì™€ 46.8%ì˜ ì„±ê³µë¥ ë¡œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 5. MOBILERL í”„ë ˆì„ì›Œí¬ëŠ” AutoGLM ì œí’ˆì— ì±„íƒë˜ì—ˆìœ¼ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:34:22*