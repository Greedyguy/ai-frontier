<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:24:10.211241",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hierarchical Reinforcement Learning",
    "Large Language Model",
    "Computer Vision",
    "Meta-actions",
    "Triple-modal State Encoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hierarchical Reinforcement Learning": 0.8,
    "Large Language Model": 0.85,
    "Computer Vision": 0.8,
    "Meta-actions": 0.78,
    "Triple-modal State Encoder": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Hierarchical Reinforcement Learning",
        "canonical": "Hierarchical Reinforcement Learning",
        "aliases": [
          "HRL"
        ],
        "category": "unique_technical",
        "rationale": "Hierarchical Reinforcement Learning is central to the paper's approach and offers a distinct method for computer control.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multi-modal Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "MLLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are a key comparison point in the paper, linking to broader discussions in AI.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision Backbone",
        "canonical": "Computer Vision",
        "aliases": [
          "Vision Model"
        ],
        "category": "broad_technical",
        "rationale": "The vision backbone is crucial for the model's efficiency and links to the field of computer vision.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Meta-actions",
        "canonical": "Meta-actions",
        "aliases": [
          "Meta Action"
        ],
        "category": "unique_technical",
        "rationale": "Meta-actions are a novel mechanism in the paper, enhancing control efficiency and linking to action optimization.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Triple-modal State Encoder",
        "canonical": "Triple-modal State Encoder",
        "aliases": [
          "Three-modal Encoder"
        ],
        "category": "unique_technical",
        "rationale": "This encoder is a unique aspect of the framework, handling diverse inputs and linking to state representation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "inference latency",
      "task instructions",
      "keystrokes",
      "mouse events",
      "inference time"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Hierarchical Reinforcement Learning",
      "resolved_canonical": "Hierarchical Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multi-modal Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision Backbone",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Meta-actions",
      "resolved_canonical": "Meta-actions",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Triple-modal State Encoder",
      "resolved_canonical": "Triple-modal State Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18230.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18230](https://arxiv.org/abs/2509.18230)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (86.4% similar)
- [[2025-09-23/Strategic Coordination for Evolving Multi-agent Systems_ A Hierarchical Reinforcement and Collective Learning Approach_20250923|Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach]] (83.9% similar)
- [[2025-09-19/(P)rior(D)yna(F)low_ A Priori Dynamic Workflow Construction via Multi-Agent Collaboration_20250919|(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration]] (83.3% similar)
- [[2025-09-18/$Agent^2$_ An Agent-Generates-Agent Framework for Reinforcement Learning Automation_20250918|$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation]] (83.2% similar)
- [[2025-09-23/Mano Report_20250923|Mano Report]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Computer Vision|Computer Vision]]
**âš¡ Unique Technical**: [[keywords/Hierarchical Reinforcement Learning|Hierarchical Reinforcement Learning]], [[keywords/Meta-actions|Meta-actions]], [[keywords/Triple-modal State Encoder|Triple-modal State Encoder]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18230v1 Announce Type: new 
Abstract: Controlling desktop applications via software remains a fundamental yet under-served problem. Existing multi-modal large language models (MLLMs) ingest screenshots and task instructions to generate keystrokes and mouse events, but they suffer from prohibitive inference latency, poor sample efficiency on long-horizon sparse-reward tasks, and infeasible on-device deployment. We introduce a lightweight hierarchical reinforcement learning framework, ComputerAgent, that formulates OS control as a two-level option process (manager and subpolicy), employs a triple-modal state encoder (screenshot, task ID, numeric state) to handle visual and contextual diversity, integrates meta-actions with an early-stop mechanism to reduce wasted interactions, and uses a compact vision backbone plus small policy networks for on-device inference (15M parameters). On a suite of 135 real-world desktop tasks, ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on simple scenarios while reducing model size by over four orders of magnitude and halving inference time. These results demonstrate that hierarchical RL offers a practical, scalable alternative to monolithic MLLM-based automation for computer control.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë°ìŠ¤í¬í†± ì• í”Œë¦¬ì¼€ì´ì…˜ ì œì–´ë¥¼ ìœ„í•œ ê²½ëŸ‰ì˜ ê³„ì¸µì  ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ ComputerAgentë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì´ ìŠ¤í¬ë¦°ìƒ·ê³¼ ì‘ì—… ì§€ì‹œë¥¼ í†µí•´ í‚¤ ì…ë ¥ê³¼ ë§ˆìš°ìŠ¤ ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ë¹„í•´, ComputerAgentëŠ” ë§¤ë‹ˆì €ì™€ í•˜ìœ„ ì •ì±…ìœ¼ë¡œ êµ¬ì„±ëœ 2ë‹¨ê³„ ì˜µì…˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ OS ì œì–´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ìŠ¤í¬ë¦°ìƒ·, ì‘ì—… ID, ìˆ«ì ìƒíƒœë¥¼ í¬í•¨í•˜ëŠ” 3ì¤‘ ëª¨ë‹¬ ìƒíƒœ ì¸ì½”ë”ë¥¼ í™œìš©í•˜ì—¬ ì‹œê°ì  ë° ë§¥ë½ì  ë‹¤ì–‘ì„±ì„ ì²˜ë¦¬í•˜ë©°, ë©”íƒ€ ì•¡ì…˜ê³¼ ì¡°ê¸° ì¤‘ì§€ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìƒí˜¸ì‘ìš©ì„ ì¤„ì…ë‹ˆë‹¤. ë˜í•œ, ì†Œí˜• ë¹„ì „ ë°±ë³¸ê³¼ ì‘ì€ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ 1,500ë§Œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ ì¥ì¹˜ ë‚´ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. 135ê°œì˜ ì‹¤ì œ ë°ìŠ¤í¬í†± ì‘ì—…ì—ì„œ ComputerAgentëŠ” ê°„ë‹¨í•œ ì‘ì—…ì—ì„œ 92.1%, ë³µì¡í•œ ì‘ì—…ì—ì„œ 58.8%ì˜ ì„±ê³µë¥ ì„ ê¸°ë¡í•˜ë©°, ëŒ€í˜• MLLM ê¸°ë°˜ ëª¨ë¸ê³¼ ë¹„êµí•´ ëª¨ë¸ í¬ê¸°ë¥¼ í¬ê²Œ ì¤„ì´ê³  ì¶”ë¡  ì‹œê°„ì„ ì ˆë°˜ìœ¼ë¡œ ë‹¨ì¶•í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ê³„ì¸µì  ê°•í™” í•™ìŠµì´ ì»´í“¨í„° ì œì–´ ìë™í™”ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ì•ˆì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ComputerAgentëŠ” ìš´ì˜ ì²´ì œ ì œì–´ë¥¼ ìœ„í•œ ê²½ëŸ‰ ê³„ì¸µí˜• ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, ê´€ë¦¬ìì™€ í•˜ìœ„ ì •ì±…ì˜ ë‘ ë ˆë²¨ ì˜µì…˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ìŠ¤í¬ë¦°ìƒ·, ì‘ì—… ID, ìˆ«ì ìƒíƒœë¥¼ í¬í•¨í•œ ì‚¼ì¤‘ ëª¨ë‹¬ ìƒíƒœ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  ë° ë§¥ë½ì  ë‹¤ì–‘ì„±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- 3. ComputerAgentëŠ” ë©”íƒ€ ì•¡ì…˜ê³¼ ì¡°ê¸° ì¤‘ì§€ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìƒí˜¸ì‘ìš©ì„ ì¤„ì´ê³ , ì†Œí˜• ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì¹˜ ë‚´ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. 135ê°œì˜ ì‹¤ì œ ë°ìŠ¤í¬í†± ì‘ì—…ì—ì„œ ComputerAgentëŠ” ê°„ë‹¨í•œ ì‘ì—…ì—ì„œ 92.1%ì˜ ì„±ê³µë¥ ì„, ì–´ë ¤ìš´ ì‘ì—…ì—ì„œ 58.8%ì˜ ì„±ê³µë¥ ì„ ê¸°ë¡í•˜ë©°, ëŒ€ê·œëª¨ MLLM ê¸°ë°˜ ëª¨ë¸ê³¼ ë¹„êµí•´ ëª¨ë¸ í¬ê¸°ë¥¼ í¬ê²Œ ì¤„ì´ê³  ì¶”ë¡  ì‹œê°„ì„ ì ˆë°˜ìœ¼ë¡œ ë‹¨ì¶•í–ˆìŠµë‹ˆë‹¤.
- 5. ê³„ì¸µí˜• ê°•í™” í•™ìŠµì€ ì»´í“¨í„° ì œì–´ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ì•ˆìœ¼ë¡œ, ë‹¨ì¼ì²´ MLLM ê¸°ë°˜ ìë™í™”ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:24:10*