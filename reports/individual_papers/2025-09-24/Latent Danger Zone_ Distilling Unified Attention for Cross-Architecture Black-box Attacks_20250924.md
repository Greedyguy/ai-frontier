<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:59:24.900495",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Latent Diffusion Model",
    "Black-box Adversarial Attacks",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Latent Diffusion Model": 0.72,
    "Black-box Adversarial Attacks": 0.79,
    "Attention Mechanism": 0.81
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Transformer",
        "canonical": "Transformer",
        "aliases": [
          "ViT"
        ],
        "category": "broad_technical",
        "rationale": "Transformers, including Vision Transformers, are pivotal in modern deep learning architectures, enhancing connectivity across various models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "latent diffusion model",
        "canonical": "Latent Diffusion Model",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This model is central to the proposed method, offering a novel approach to adversarial attack generation.",
        "novelty_score": 0.78,
        "connectivity_score": 0.67,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      },
      {
        "surface": "black-box adversarial attacks",
        "canonical": "Black-box Adversarial Attacks",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The paper focuses on improving these attacks, making it a key concept for linking related research.",
        "novelty_score": 0.71,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "attention maps",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial for understanding model focus areas, enhancing cross-model connectivity.",
        "novelty_score": 0.52,
        "connectivity_score": 0.83,
        "specificity_score": 0.7,
        "link_intent_score": 0.81
      }
    ],
    "ban_list_suggestions": [
      "JAD",
      "CNN"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "latent diffusion model",
      "resolved_canonical": "Latent Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.67,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "black-box adversarial attacks",
      "resolved_canonical": "Black-box Adversarial Attacks",
      "decision": "linked",
      "scores": {
        "novelty": 0.71,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "attention maps",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.52,
        "connectivity": 0.83,
        "specificity": 0.7,
        "link_intent": 0.81
      }
    }
  ]
}
-->

# Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19044.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19044](https://arxiv.org/abs/2509.19044)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/DiffHash_ Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval_20250918|DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval]] (82.3% similar)
- [[2025-09-17/Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics_20250917|Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics]] (81.9% similar)
- [[2025-09-23/Jailbreak-Tuning_ Models Efficiently Learn Jailbreak Susceptibility_20250923|Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility]] (81.4% similar)
- [[2025-09-23/ADVEDM_Fine-grained Adversarial Attack against VLM-based Embodied Agents_20250923|ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents]] (81.3% similar)
- [[2025-09-24/Backdoor Attack with Invisible Triggers Based on Model Architecture Modification_20250924|Backdoor Attack with Invisible Triggers Based on Model Architecture Modification]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Latent Diffusion Model|Latent Diffusion Model]], [[keywords/Black-box Adversarial Attacks|Black-box Adversarial Attacks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19044v1 Announce Type: new 
Abstract: Black-box adversarial attacks remain challenging due to limited access to model internals. Existing methods often depend on specific network architectures or require numerous queries, resulting in limited cross-architecture transferability and high query costs. To address these limitations, we propose JAD, a latent diffusion model framework for black-box adversarial attacks. JAD generates adversarial examples by leveraging a latent diffusion model guided by attention maps distilled from both a convolutional neural network (CNN) and a Vision Transformer (ViT) models. By focusing on image regions that are commonly sensitive across architectures, this approach crafts adversarial perturbations that transfer effectively between different model types. This joint attention distillation strategy enables JAD to be architecture-agnostic, achieving superior attack generalization across diverse models. Moreover, the generative nature of the diffusion framework yields high adversarial sample generation efficiency by reducing reliance on iterative queries. Experiments demonstrate that JAD offers improved attack generalization, generation efficiency, and cross-architecture transferability compared to existing methods, providing a promising and effective paradigm for black-box adversarial attacks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë¸”ë™ë°•ìŠ¤ ì ëŒ€ì  ê³µê²©ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ JADë¼ëŠ” ì ì¬ í™•ì‚° ëª¨ë¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. JADëŠ” CNNê³¼ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT) ëª¨ë¸ì—ì„œ ì¶”ì¶œëœ ì£¼ì˜ ë§µì„ í™œìš©í•˜ì—¬ ì ì¬ í™•ì‚° ëª¨ë¸ì„ í†µí•´ ì ëŒ€ì  ì˜ˆì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ì–‘í•œ ëª¨ë¸ ê°„ì— íš¨ê³¼ì ìœ¼ë¡œ ì „ì´ë˜ëŠ” ì ëŒ€ì  êµë€ì„ ìƒì„±í•˜ë©°, ì•„í‚¤í…ì²˜ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ê³µê²© ì¼ë°˜í™”ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ë˜í•œ, í™•ì‚° í”„ë ˆì„ì›Œí¬ì˜ ìƒì„±ì  íŠ¹ì„± ë•ë¶„ì— ë°˜ë³µì ì¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ì¤„ì—¬ ë†’ì€ íš¨ìœ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, JADëŠ” ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ ê³µê²© ì¼ë°˜í™”, ìƒì„± íš¨ìœ¨ì„±, ì•„í‚¤í…ì²˜ ê°„ ì „ì´ ê°€ëŠ¥ì„±ì—ì„œ ê°œì„ ëœ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. JADëŠ” CNNê³¼ ViT ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ ì£¼ì˜ ë§µì„ í™œìš©í•˜ì—¬ ì ì¬ í™•ì‚° ëª¨ë¸ì„ í†µí•´ ì ëŒ€ì  ì˜ˆì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- 2. ì´ ì ‘ê·¼ë²•ì€ ë‹¤ì–‘í•œ ëª¨ë¸ ìœ í˜• ê°„ì— íš¨ê³¼ì ìœ¼ë¡œ ì „ì´ë˜ëŠ” ì ëŒ€ì  êµë€ì„ ë§Œë“­ë‹ˆë‹¤.
- 3. JADëŠ” ì•„í‚¤í…ì²˜ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ ë›°ì–´ë‚œ ê³µê²© ì¼ë°˜í™”ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. í™•ì‚° í”„ë ˆì„ì›Œí¬ì˜ ìƒì„±ì  íŠ¹ì„± ë•ë¶„ì— ë°˜ë³µì ì¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ì¤„ì—¬ ë†’ì€ íš¨ìœ¨ì„±ìœ¼ë¡œ ì ëŒ€ì  ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, JADëŠ” ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ ê³µê²© ì¼ë°˜í™”, ìƒì„± íš¨ìœ¨ì„±, ì•„í‚¤í…ì²˜ ê°„ ì „ì´ ê°€ëŠ¥ì„±ì—ì„œ í–¥ìƒëœ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:59:24*