<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:35:29.096982",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "CTF-Dojo",
    "Execution-grounded Training",
    "Capture-The-Flag Challenges"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "CTF-Dojo": 0.8,
    "Execution-grounded Training": 0.78,
    "Capture-The-Flag Challenges": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and connect well with existing literature on language models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "CTF-Dojo",
        "canonical": "CTF-Dojo",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "CTF-Dojo is a novel framework introduced in the paper, essential for understanding the specific contributions of the research.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Execution-grounded training",
        "canonical": "Execution-grounded Training",
        "aliases": [
          "Execution-based Training"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is pivotal for linking the paper's approach to broader trends in training methodologies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Capture-The-Flag challenges",
        "canonical": "Capture-The-Flag Challenges",
        "aliases": [
          "CTF Challenges"
        ],
        "category": "unique_technical",
        "rationale": "Capture-The-Flag challenges are central to the experimental setup and are a unique aspect of the paper's contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "CTF-Dojo",
      "resolved_canonical": "CTF-Dojo",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Execution-grounded training",
      "resolved_canonical": "Execution-grounded Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Capture-The-Flag challenges",
      "resolved_canonical": "Capture-The-Flag Challenges",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Training Language Model Agents to Find Vulnerabilities with CTF-Dojo

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2508.18370.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2508.18370](https://arxiv.org/abs/2508.18370)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Generalizable End-to-End Tool-Use RL with Synthetic CodeGym_20250923|Generalizable End-to-End Tool-Use RL with Synthetic CodeGym]] (86.6% similar)
- [[2025-09-19/From Capabilities to Performance_ Evaluating Key Functional Properties of LLM Architectures in Penetration Testing_20250919|From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing]] (86.5% similar)
- [[2025-09-22/Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges_20250922|Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges]] (85.8% similar)
- [[2025-09-23/Large Language Models for Cyber Security_ A Systematic Literature Review_20250923|Large Language Models for Cyber Security: A Systematic Literature Review]] (84.4% similar)
- [[2025-09-23/Sugar-Coated Poison_ Benign Generation Unlocks LLM Jailbreaking_20250923|Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Execution-grounded Training|Execution-grounded Training]]
**âš¡ Unique Technical**: [[keywords/CTF-Dojo|CTF-Dojo]], [[keywords/Capture-The-Flag Challenges|Capture-The-Flag Challenges]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.18370v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¤í–‰ ê°€ëŠ¥í•œ ëŸ°íƒ€ì„ í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í›ˆë ¨ì„ ìœ„í•œ CTF-Dojoë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. CTF-DojoëŠ” 658ê°œì˜ CTF ìŠ¤íƒ€ì¼ ë„ì „ì„ Dockerë¡œ ì»¨í…Œì´ë„ˆí™”í•˜ì—¬ ê²€ì¦ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, CTF-Forgeë¼ëŠ” ìë™í™” íŒŒì´í”„ë¼ì¸ì„ ê°œë°œí•˜ì—¬ ê³µê³µ ìë£Œë¥¼ ì‹ ì†í•˜ê²Œ ì‹¤í–‰ í™˜ê²½ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ì—¬ InterCode-CTF, NYU CTF Bench, Cybenchì—ì„œ ìµœëŒ€ 11.6%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, 32B ëª¨ë¸ì€ Pass@1ì—ì„œ 31.9%ë¥¼ ê¸°ë¡í•˜ë©° ìµœê³  ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. CTF-DojoëŠ” ì‹¤í–‰ ê¸°ë°˜ í›ˆë ¨ì´ ê³ ì„±ëŠ¥ ML ì—ì´ì „íŠ¸ ê°œë°œì— íš¨ê³¼ì ì„ì„ ì¦ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CTF-DojoëŠ” ëŒ€ê·œëª¨ ì‹¤í–‰ ê°€ëŠ¥í•œ ëŸ°íƒ€ì„ í™˜ê²½ìœ¼ë¡œ, 658ê°œì˜ CTF ìŠ¤íƒ€ì¼ ì±Œë¦°ì§€ë¥¼ í†µí•´ LLMsì˜ ê²€ì¦ ê°€ëŠ¥í•œ í”¼ë“œë°± ê¸°ë°˜ í›ˆë ¨ì„ ì§€ì›í•©ë‹ˆë‹¤.
- 2. CTF-ForgeëŠ” ê³µê°œëœ ì•„í‹°íŒ©íŠ¸ë¥¼ ì‹ ì†í•˜ê²Œ ì‹¤í–‰ í™˜ê²½ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ìë™í™” íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ, ì „ë¬¸ê°€ì˜ ìˆ˜ì£¼ê°„ì˜ ì„¤ì • ì‘ì—…ì„ ë¶ˆí•„ìš”í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
- 3. CTF-Dojoì—ì„œ í›ˆë ¨ëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ëŠ” InterCode-CTF, NYU CTF Bench, Cybenchì—ì„œ ìµœëŒ€ 11.6%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 4. 32B ëª¨ë¸ì€ 31.9%ì˜ Pass@1ì„ ê¸°ë¡í•˜ë©°, DeepSeek-V3-0324 ë° Gemini-2.5-Flashì™€ ê°™ì€ ìµœì²¨ë‹¨ ëª¨ë¸ì— í•„ì í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. CTF-DojoëŠ” ì‹¤í–‰ ê¸°ë°˜ í›ˆë ¨ ì‹ í˜¸ê°€ ê³ ì„±ëŠ¥ ML ì—ì´ì „íŠ¸ ë°œì „ì— íš¨ê³¼ì ì´ë©°, ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ë…ì  ì‹œìŠ¤í…œì— ì˜ì¡´í•˜ì§€ ì•Šê³ ë„ ê°€ëŠ¥í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:35:29*