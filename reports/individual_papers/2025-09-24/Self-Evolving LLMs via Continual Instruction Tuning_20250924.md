<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:36:14.483269",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Continual Learning",
    "Mixture-of-Experts",
    "Adversarial Learning",
    "Instruction Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Continual Learning": 0.9,
    "Mixture-of-Experts": 0.8,
    "Adversarial Learning": 0.78,
    "Instruction Tuning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on continual learning and instruction tuning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "CL"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for understanding the self-evolving aspect of the proposed framework.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "Unique approach in the paper for parameter-efficient learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adversarial Learning",
        "canonical": "Adversarial Learning",
        "aliases": [
          "GAN-based Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Used to enhance the shared expert's ability to generalize across tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Instruction Tuning",
        "canonical": "Instruction Tuning",
        "aliases": [
          "Instruction-based Tuning"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's method for improving LLM adaptability.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "self-evolution",
      "parameter isolation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adversarial Learning",
      "resolved_canonical": "Adversarial Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Instruction Tuning",
      "resolved_canonical": "Instruction Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Self-Evolving LLMs via Continual Instruction Tuning

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18133.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18133](https://arxiv.org/abs/2509.18133)

## 🔗 유사한 논문
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (85.8% similar)
- [[2025-09-23/GuiLoMo_ Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors_20250923|GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors]] (85.7% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (85.6% similar)
- [[2025-09-19/LEED_ A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning_20250919|LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning]] (85.5% similar)
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (85.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Continual Learning|Continual Learning]], [[keywords/Adversarial Learning|Adversarial Learning]]
**⚡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Instruction Tuning|Instruction Tuning]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18133v1 Announce Type: cross 
Abstract: In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.

## 📝 요약

MoE-CL은 대규모 언어 모델(LLM)의 지속적 학습을 위한 효율적인 방법론으로, 산업 환경에서의 자기 진화를 지원합니다. 이 방법은 두 가지 전문가 설계를 사용합니다: 각 작업에 대한 LoRA 전문가가 작업별 지식을 보존하고, 공유 LoRA 전문가가 작업 간 전이를 가능하게 합니다. 또한, 공유 경로를 통해 작업과 무관한 노이즈 전이를 방지하기 위해 GAN 내에 작업 인식 판별기를 통합합니다. 이를 통해 공유 전문가는 일반화된 표현을 학습하고, 전용 전문가는 작업별 세부 사항을 유지하여 지식 보존과 작업 간 일반화를 균형 있게 유지합니다. MoE-CL은 MTL5 및 Tencent3 벤치마크에서 효과를 입증했으며, Tencent Video 플랫폼의 콘텐츠 준수 검토에서 수작업 비용을 15.3% 절감했습니다. 이는 MoE-CL이 대규모 산업 배포에 실용적임을 보여줍니다.

## 🎯 주요 포인트

- 1. MoE-CL은 대규모 언어 모델의 지속적인 학습을 위해 설계된 파라미터 효율적인 적대적 전문가 혼합 프레임워크입니다.
- 2. MoE-CL은 각 작업에 대한 독립적인 LoRA 전문가와 공유 LoRA 전문가를 통해 작업 간 전이를 가능하게 합니다.
- 3. GAN 내의 작업 인식 판별기를 통합하여 공유 경로를 통한 작업과 관련 없는 노이즈 전이를 방지합니다.
- 4. MoE-CL은 MTL5 및 Tencent3 벤치마크에서의 실험을 통해 지속적인 지시 조정의 효과성을 입증했습니다.
- 5. Tencent Video 플랫폼의 콘텐츠 준수 검토에서 MoE-CL은 수작업 검토 비용을 15.3% 절감했습니다.


---

*Generated on 2025-09-24 13:36:14*