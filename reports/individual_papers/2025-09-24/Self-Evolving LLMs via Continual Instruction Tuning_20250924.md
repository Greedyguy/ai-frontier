<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:36:14.483269",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Continual Learning",
    "Mixture-of-Experts",
    "Adversarial Learning",
    "Instruction Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Continual Learning": 0.9,
    "Mixture-of-Experts": 0.8,
    "Adversarial Learning": 0.78,
    "Instruction Tuning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on continual learning and instruction tuning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Continual Learning",
        "canonical": "Continual Learning",
        "aliases": [
          "CL"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for understanding the self-evolving aspect of the proposed framework.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "Unique approach in the paper for parameter-efficient learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Adversarial Learning",
        "canonical": "Adversarial Learning",
        "aliases": [
          "GAN-based Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Used to enhance the shared expert's ability to generalize across tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Instruction Tuning",
        "canonical": "Instruction Tuning",
        "aliases": [
          "Instruction-based Tuning"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's method for improving LLM adaptability.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "self-evolution",
      "parameter isolation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Continual Learning",
      "resolved_canonical": "Continual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Adversarial Learning",
      "resolved_canonical": "Adversarial Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Instruction Tuning",
      "resolved_canonical": "Instruction Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Self-Evolving LLMs via Continual Instruction Tuning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18133.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18133](https://arxiv.org/abs/2509.18133)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (85.8% similar)
- [[2025-09-23/GuiLoMo_ Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors_20250923|GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors]] (85.7% similar)
- [[2025-09-22/Towards Robust Visual Continual Learning with Multi-Prototype Supervision_20250922|Towards Robust Visual Continual Learning with Multi-Prototype Supervision]] (85.6% similar)
- [[2025-09-19/LEED_ A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning_20250919|LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning]] (85.5% similar)
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (85.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Continual Learning|Continual Learning]], [[keywords/Adversarial Learning|Adversarial Learning]]
**âš¡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Instruction Tuning|Instruction Tuning]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18133v1 Announce Type: cross 
Abstract: In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.

## ğŸ“ ìš”ì•½

MoE-CLì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì§€ì†ì  í•™ìŠµì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ë°©ë²•ë¡ ìœ¼ë¡œ, ì‚°ì—… í™˜ê²½ì—ì„œì˜ ìê¸° ì§„í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‘ ê°€ì§€ ì „ë¬¸ê°€ ì„¤ê³„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: ê° ì‘ì—…ì— ëŒ€í•œ LoRA ì „ë¬¸ê°€ê°€ ì‘ì—…ë³„ ì§€ì‹ì„ ë³´ì¡´í•˜ê³ , ê³µìœ  LoRA ì „ë¬¸ê°€ê°€ ì‘ì—… ê°„ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ê³µìœ  ê²½ë¡œë¥¼ í†µí•´ ì‘ì—…ê³¼ ë¬´ê´€í•œ ë…¸ì´ì¦ˆ ì „ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ GAN ë‚´ì— ì‘ì—… ì¸ì‹ íŒë³„ê¸°ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê³µìœ  ì „ë¬¸ê°€ëŠ” ì¼ë°˜í™”ëœ í‘œí˜„ì„ í•™ìŠµí•˜ê³ , ì „ìš© ì „ë¬¸ê°€ëŠ” ì‘ì—…ë³„ ì„¸ë¶€ ì‚¬í•­ì„ ìœ ì§€í•˜ì—¬ ì§€ì‹ ë³´ì¡´ê³¼ ì‘ì—… ê°„ ì¼ë°˜í™”ë¥¼ ê· í˜• ìˆê²Œ ìœ ì§€í•©ë‹ˆë‹¤. MoE-CLì€ MTL5 ë° Tencent3 ë²¤ì¹˜ë§ˆí¬ì—ì„œ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìœ¼ë©°, Tencent Video í”Œë«í¼ì˜ ì½˜í…ì¸  ì¤€ìˆ˜ ê²€í† ì—ì„œ ìˆ˜ì‘ì—… ë¹„ìš©ì„ 15.3% ì ˆê°í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” MoE-CLì´ ëŒ€ê·œëª¨ ì‚°ì—… ë°°í¬ì— ì‹¤ìš©ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MoE-CLì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì†ì ì¸ í•™ìŠµì„ ìœ„í•´ ì„¤ê³„ëœ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ ì ëŒ€ì  ì „ë¬¸ê°€ í˜¼í•© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. MoE-CLì€ ê° ì‘ì—…ì— ëŒ€í•œ ë…ë¦½ì ì¸ LoRA ì „ë¬¸ê°€ì™€ ê³µìœ  LoRA ì „ë¬¸ê°€ë¥¼ í†µí•´ ì‘ì—… ê°„ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. GAN ë‚´ì˜ ì‘ì—… ì¸ì‹ íŒë³„ê¸°ë¥¼ í†µí•©í•˜ì—¬ ê³µìœ  ê²½ë¡œë¥¼ í†µí•œ ì‘ì—…ê³¼ ê´€ë ¨ ì—†ëŠ” ë…¸ì´ì¦ˆ ì „ì´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
- 4. MoE-CLì€ MTL5 ë° Tencent3 ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì‹¤í—˜ì„ í†µí•´ ì§€ì†ì ì¸ ì§€ì‹œ ì¡°ì •ì˜ íš¨ê³¼ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 5. Tencent Video í”Œë«í¼ì˜ ì½˜í…ì¸  ì¤€ìˆ˜ ê²€í† ì—ì„œ MoE-CLì€ ìˆ˜ì‘ì—… ê²€í†  ë¹„ìš©ì„ 15.3% ì ˆê°í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:36:14*