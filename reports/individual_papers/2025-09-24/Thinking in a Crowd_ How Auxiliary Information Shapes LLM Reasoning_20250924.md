<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:37:13.193616",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Auxiliary Information",
    "SciAux Dataset",
    "Step-by-step Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Auxiliary Information": 0.78,
    "SciAux Dataset": 0.82,
    "Step-by-step Reasoning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on reasoning and auxiliary information.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Auxiliary Information",
        "canonical": "Auxiliary Information",
        "aliases": [
          "External Information"
        ],
        "category": "unique_technical",
        "rationale": "Key to understanding the impact on LLM reasoning processes.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "SciAux",
        "canonical": "SciAux Dataset",
        "aliases": [
          "ScienceQA Derived Dataset"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new dataset specifically for testing LLM robustness.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "Step-by-step Thinking",
        "canonical": "Step-by-step Reasoning",
        "aliases": [
          "Deliberative Thinking"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the method of reasoning being analyzed.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Auxiliary Information",
      "resolved_canonical": "Auxiliary Information",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "SciAux",
      "resolved_canonical": "SciAux Dataset",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Step-by-step Thinking",
      "resolved_canonical": "Step-by-step Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18163.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18163](https://arxiv.org/abs/2509.18163)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/No Need for Explanations_ LLMs can implicitly learn from mistakes in-context_20250923|No Need for Explanations: LLMs can implicitly learn from mistakes in-context]] (86.7% similar)
- [[2025-09-23/Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning_20250923|Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning]] (86.4% similar)
- [[2025-09-23/How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark_20250923|How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark]] (86.0% similar)
- [[2025-09-23/Neither Stochastic Parroting nor AGI_ LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors_20250923|Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors]] (85.2% similar)
- [[2025-09-24/LightThinker_ Thinking Step-by-Step Compression_20250924|LightThinker: Thinking Step-by-Step Compression]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Step-by-step Reasoning|Step-by-step Reasoning]]
**âš¡ Unique Technical**: [[keywords/Auxiliary Information|Auxiliary Information]], [[keywords/SciAux Dataset|SciAux Dataset]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18163v1 Announce Type: new 
Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative "thinking mode" is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models "think", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì— ì™¸ë¶€ ì •ë³´ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. íŠ¹íˆ, LLMì´ ë‹¨ê³„ë³„ ì‚¬ê³ ë¥¼ í†µí•´ ì •ë³´ë¥¼ ì²˜ë¦¬í•  ë•Œ, ìœ ìš©í•˜ê±°ë‚˜ ë¬´ê´€í•˜ê±°ë‚˜ ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆëŠ” ì •ë³´ê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ScienceQAì—ì„œ íŒŒìƒëœ SciAuxë¼ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ì†Œê°œí•˜ì—¬ ëª¨ë¸ì˜ ê²¬ê³ ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ëª¨ë¸ì˜ ì‚¬ê³  ëª¨ë“œëŠ” ì–‘ë‚ ì˜ ê²€ìœ¼ë¡œ, ìœ ìš©í•œ ì •ë³´ëŠ” ì •í™•ì„±ì„ ë†’ì´ì§€ë§Œ, ì˜ëª»ëœ ì •ë³´ëŠ” ì„±ëŠ¥ì„ í¬ê²Œ ì €í•˜ì‹œí‚µë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‹¨ìˆœíˆ 'ìƒê°'í•˜ëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼, ì •ë³´ë¥¼ ë¹„íŒì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ì–´ì•¼ í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤. SciAux ë°ì´í„°ì…‹ì€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì€ ë³µì¡í•˜ê³  ì§€ì‹ ì§‘ì•½ì ì¸ ë¶„ì•¼ì—ì„œì˜ ì ìš©ì— í•„ìˆ˜ì ì´ë‹¤.
- 2. ì™¸ë¶€ ì •ë³´ê°€ LLMì˜ ì¶”ë¡  ê³¼ì •ì— ë¯¸ì¹˜ëŠ” ì¸ê³¼ì  ì˜í–¥ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ SciAuxë¼ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë„ì…í•˜ì˜€ë‹¤.
- 3. ëª¨ë¸ì˜ 'ìƒê° ëª¨ë“œ'ëŠ” ìœ ìš©í•œ ì •ë³´ë¡œ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆì§€ë§Œ, ì˜ëª»ëœ ì •ë³´ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ” ì·¨ì•½ì ì„ ê°€ì§€ê³  ìˆë‹¤.
- 4. ëª¨ë¸ì´ ë‹¨ìˆœíˆ 'ìƒê°'í•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì •ë³´ì˜ í‰ê°€ ëŠ¥ë ¥ì„ ê°–ì¶”ë„ë¡ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- 5. SciAux ë°ì´í„°ì…‹ì€ https://huggingface.co/datasets/billhdzhao/SciAuxì—ì„œ ì´ìš© ê°€ëŠ¥í•˜ë‹¤.


---

*Generated on 2025-09-24 15:37:13*