<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:14:44.166482",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Chain-of-Thought",
    "Continuous Tokens",
    "Reinforcement Learning",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Chain-of-Thought": 0.78,
    "Continuous Tokens": 0.82,
    "Reinforcement Learning": 0.75,
    "Large Language Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Chain-of-Thought is a unique reasoning approach in LLMs, crucial for understanding the paper's focus on continuous tokens.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "continuous tokens",
        "canonical": "Continuous Tokens",
        "aliases": [
          "soft tokens"
        ],
        "category": "unique_technical",
        "rationale": "Continuous tokens are central to the paper's novel approach, offering a new perspective on token usage in LLMs.",
        "novelty_score": 0.8,
        "connectivity_score": 0.72,
        "specificity_score": 0.88,
        "link_intent_score": 0.82
      },
      {
        "surface": "reinforcement learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a key method used in the paper to train continuous CoTs, linking it to broader machine learning strategies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Llama and Qwen models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Llama",
          "Qwen"
        ],
        "category": "broad_technical",
        "rationale": "These models are specific instances of LLMs, relevant for understanding the context and application of the study.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "training difficulties",
      "computational costs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "continuous tokens",
      "resolved_canonical": "Continuous Tokens",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.72,
        "specificity": 0.88,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "reinforcement learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Llama and Qwen models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Soft Tokens, Hard Truths

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19170.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.19170](https://arxiv.org/abs/2509.19170)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/EvoCoT_ Overcoming the Exploration Bottleneck in Reinforcement Learning_20250923|EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning]] (86.2% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (83.9% similar)
- [[2025-09-23/LTA-thinker_ Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning_20250923|LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning]] (83.3% similar)
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (83.1% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/Continuous Tokens|Continuous Tokens]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19170v1 Announce Type: cross 
Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.
  This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the "soft" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì—°ì†ì ì¸ í† í°ì„ í™œìš©í•œ ì—°ì‡„ ì‚¬ê³ (Chain-of-Thought, CoT) í•™ìŠµ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì´ì‚° í† í° ëŒ€ì‹  ì—°ì† í† í°ì„ ì‚¬ìš©í•˜ë©´ ì—¬ëŸ¬ ì‚¬ê³  ê²½ë¡œë¥¼ ë™ì‹œì— ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆì–´ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì´ í–¥ìƒë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—°ì† í† í°ì˜ ì‹¤ìš©ì„±ì€ í•™ìŠµì˜ ì–´ë ¤ì›€ ë•Œë¬¸ì— ì œí•œì ì´ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì°¸ì¡° ì´ì‚° CoT ì—†ì´ ê°•í™” í•™ìŠµ(RL)ì„ í†µí•´ ì—°ì† CoTë¥¼ í•™ìŠµí•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì²˜ìŒìœ¼ë¡œ ë„ì…í–ˆìŠµë‹ˆë‹¤. "ì†Œí”„íŠ¸" í† í°ê³¼ ì…ë ¥ ì„ë² ë”©ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ RL íƒìƒ‰ì„ ì´‰ì§„í•˜ë©°, ìµœì†Œí•œì˜ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ìˆ˜ë°± ê°œì˜ í† í°ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Llama ë° Qwen ëª¨ë¸ì„ ì‚¬ìš©í•œ ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì—°ì† CoTëŠ” ì´ì‚° CoTì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ pass@32ì—ì„œ ë” ë‚˜ì€ ì„±ê³¼ë¥¼ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤. ìµœì ì˜ ì„±ê³¼ëŠ” ì—°ì† CoTë¡œ í•™ìŠµí•œ í›„ ì´ì‚° í† í°ìœ¼ë¡œ ì¶”ë¡ í•  ë•Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´ëŠ” "ì†Œí”„íŠ¸" ëª¨ë¸ì´ í‘œì¤€ ë°©ì‹ìœ¼ë¡œ ë°°í¬ë  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë˜í•œ, ì—°ì† CoT RL í•™ìŠµì€ ê¸°ë³¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ë” ì˜ ë³´ì¡´í•˜ì—¬ ëª¨ë¸ì˜ ìœ ì—°ì„±ì„ ë†’ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì—°ì†ì ì¸ í† í°ì„ ì‚¬ìš©í•œ Chain-of-Thought(ì—°ì‡„ì  ì‚¬ê³ ) ë°©ì‹ì´ ì´ë¡ ì ìœ¼ë¡œ ë” ë†’ì€ í‘œí˜„ë ¥ì„ ê°€ì§€ë©° íŠ¹ì • ë¬¸ì œë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.
- 2. ì´ ì—°êµ¬ëŠ” ì°¸ì¡° ì´ì‚° í† í° ì—†ì´ ê°•í™” í•™ìŠµì„ í†µí•´ ì—°ì†ì ì¸ CoTë¥¼ í•™ìŠµí•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì²˜ìŒìœ¼ë¡œ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 3. "ì†Œí”„íŠ¸" í† í°ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì„ë² ë”©ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ê°•í™” í•™ìŠµ íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , ìˆ˜ë°± ê°œì˜ ì—°ì†ì ì¸ CoT í† í°ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 4. ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì—°ì†ì ì¸ CoTë¡œ í›ˆë ¨í•œ ëª¨ë¸ì´ ì´ì‚° í† í° CoTì™€ ë¹„êµí•˜ì—¬ ë” ë†’ì€ ë‹¤ì–‘ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. ì—°ì†ì ì¸ CoT ê°•í™” í•™ìŠµì€ ê¸°ë³¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ë” ì˜ ë³´ì¡´í•˜ì—¬, ëª¨ë¸ì˜ ê¸°ë³¸ ì„±ëŠ¥ì— ë¶€ë“œëŸ¬ìš´ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:14:44*