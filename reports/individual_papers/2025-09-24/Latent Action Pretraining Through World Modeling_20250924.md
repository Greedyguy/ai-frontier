<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:22:56.316514",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Latent Action Representations",
    "World Modeling",
    "Self-supervised Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.82,
    "Latent Action Representations": 0.79,
    "World Modeling": 0.75,
    "Self-supervised Learning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language-Action",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLA"
        ],
        "category": "evolved_concepts",
        "rationale": "Connects to the growing field of integrating vision and language for action tasks, relevant for multimodal learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Latent Action Representations",
        "canonical": "Latent Action Representations",
        "aliases": [
          "LAR"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept for unsupervised pretraining in robotics, enhancing task transferability.",
        "novelty_score": 0.78,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "World Modeling",
        "canonical": "World Modeling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Essential for understanding and predicting environmental changes, crucial for self-supervised learning.",
        "novelty_score": 0.67,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Self-supervised Learning",
        "canonical": "Self-supervised Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key technique for training models without labeled data, relevant across multiple domains.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "robotic manipulation tasks",
      "real-world settings"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language-Action",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Latent Action Representations",
      "resolved_canonical": "Latent Action Representations",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "World Modeling",
      "resolved_canonical": "World Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.67,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Self-supervised Learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Latent Action Pretraining Through World Modeling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18428.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.18428](https://arxiv.org/abs/2509.18428)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/VLA-LPAF_ Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation_20250924|VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation]] (87.6% similar)
- [[2025-09-18/GeoAware-VLA_ Implicit Geometry Aware Vision-Language-Action Model_20250918|GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model]] (86.7% similar)
- [[2025-09-23/Evo-0_ Vision-Language-Action Model with Implicit Spatial Understanding_20250923|Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding]] (86.4% similar)
- [[2025-09-24/Bi-VLA_ Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation_20250924|Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation]] (85.9% similar)
- [[2025-09-24/Pure Vision Language Action (VLA) Models_ A Comprehensive Survey_20250924|Pure Vision Language Action (VLA) Models: A Comprehensive Survey]] (85.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]]
**âš¡ Unique Technical**: [[keywords/Latent Action Representations|Latent Action Representations]], [[keywords/World Modeling|World Modeling]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18428v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.

## ğŸ“ ìš”ì•½

Vision-Language-Action (VLA) ëª¨ë¸ì€ ì–¸ì–´ ì§€ì‹œì— ë”°ë¼ ë¡œë´‡ ì¡°ì‘ ì‘ì—…ì„ í•™ìŠµí•˜ëŠ” ë° ì¸ê¸°ë¥¼ ì–»ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ VLA ëª¨ë¸ë“¤ì€ ëŒ€ê·œëª¨ë¡œ ìˆ˜ì§‘ëœ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœê·¼ì—ëŠ” LAPAì™€ villa-Xì™€ ê°™ì€ ì ‘ê·¼ë²•ì´ í”„ë ˆì„ ê°„ì˜ ì¶”ìƒì ì¸ ì‹œê°ì  ë³€í™”ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì…‹ì—ì„œ ë¹„ì§€ë„ ì‚¬ì „ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ëª¨ë¸ í¬ê¸°ê°€ ì»¤ ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ë°°í¬ê°€ ì–´ë µìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” LAWMì´ë¼ëŠ” ëª¨ë¸-ë…ë¦½ì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬, ë ˆì´ë¸”ì´ ì—†ëŠ” ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ í†µí•´ ì ì¬ì  í–‰ë™ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ìê¸° ì§€ë„ í•™ìŠµ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì–‘í•œ ì‘ì—…, í™˜ê²½, êµ¬í˜„ì²´ì— ê±¸ì³ íš¨ê³¼ì ìœ¼ë¡œ ì „ì´í•  ìˆ˜ ìˆìœ¼ë©°, LIBERO ë²¤ì¹˜ë§ˆí¬ ë° ì‹¤ì œ í™˜ê²½ì—ì„œ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ì‹¤ìš©ì ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision-Language-Action(VLA) ëª¨ë¸ì€ ì–¸ì–´ ì§€ì‹œë¥¼ ë”°ë¥´ëŠ” ë¡œë´‡ ì¡°ì‘ ì‘ì—… í•™ìŠµì— ì¸ê¸°ë¥¼ ì–»ê³  ìˆë‹¤.
- 2. ê¸°ì¡´ VLA ëª¨ë¸ì€ ëŒ€ê·œëª¨ ìˆ˜ì‘ì—…ìœ¼ë¡œ ë¼ë²¨ë§ëœ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë‚˜, LAPAì™€ villa-XëŠ” í”„ë ˆì„ ê°„ ì¶”ìƒì  ì‹œê° ë³€í™”ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ë¹„ì§€ë„ ì‚¬ì „ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 3. ëŒ€ê·œëª¨ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•´ ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ë°°í¬ê°€ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, LAWMì€ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ í™œìš©í•œ ìê¸° ì§€ë„ ë°©ì‹ìœ¼ë¡œ ëª¨ë°© í•™ìŠµ ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.
- 4. LAWMì€ ë‹¤ì–‘í•œ ì‘ì—…, í™˜ê²½, êµ¬í˜„ì²´ ê°„ì˜ ì „ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë©°, LIBERO ë²¤ì¹˜ë§ˆí¬ì™€ ì‹¤ì œ í™˜ê²½ì—ì„œ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ì‹¤ìš©ì ì´ë‹¤.
- 5. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ë¡œë´‡ ë…¹í™” ì˜ìƒì´ë‚˜ ì¼ìƒ ë¬¼ì²´ë¥¼ ë‹¤ë£¨ëŠ” ì¸ê°„ì˜ í–‰ë™ ì˜ìƒì—ì„œ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì–»ì–´ í™œìš©í•  ìˆ˜ ìˆë‹¤.


---

*Generated on 2025-09-24 16:22:56*