<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:41:07.876551",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Fair Sequence Policy Optimization",
    "Large Language Model",
    "Length Fairness",
    "Importance Sampling"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Fair Sequence Policy Optimization": 0.8,
    "Large Language Model": 0.85,
    "Length Fairness": 0.78,
    "Importance Sampling": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "FSPO",
        "canonical": "Fair Sequence Policy Optimization",
        "aliases": [
          "FSPO"
        ],
        "category": "unique_technical",
        "rationale": "FSPO is a novel method introduced in the paper, providing a unique approach to sequence-level reinforcement learning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on sequence-level reinforcement learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Length Fairness",
        "canonical": "Length Fairness",
        "aliases": [
          "Length Reweighting"
        ],
        "category": "unique_technical",
        "rationale": "Length Fairness is a key concept introduced to address optimization issues in sequence-level reinforcement learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Importance Sampling",
        "canonical": "Importance Sampling",
        "aliases": [
          "IS"
        ],
        "category": "specific_connectable",
        "rationale": "Importance Sampling is a fundamental technique used in the proposed FSPO method, crucial for understanding its implementation.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "FSPO",
      "resolved_canonical": "Fair Sequence Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Length Fairness",
      "resolved_canonical": "Length Fairness",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Importance Sampling",
      "resolved_canonical": "Importance Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.09177.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.09177](https://arxiv.org/abs/2509.09177)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (81.8% similar)
- [[2025-09-19/FlowRL_ Matching Reward Distributions for LLM Reasoning_20250919|FlowRL: Matching Reward Distributions for LLM Reasoning]] (81.4% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (80.1% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (80.0% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (79.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Importance Sampling|Importance Sampling]]
**âš¡ Unique Technical**: [[keywords/Fair Sequence Policy Optimization|Fair Sequence Policy Optimization]], [[keywords/Length Fairness|Length Fairness]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.09177v2 Announce Type: replace-cross 
Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level reinforcement learning method for LLMs that enforces length-fair clipping on the importance-sampling (IS) weight. We study RL methods with sequence-level IS and identify a mismatch when PPO/GRPO-style clipping is transplanted to sequences: a fixed clip range systematically reweights short vs.\ long responses, distorting the optimization direction. FSPO introduces a simple remedy: we clip the sequence log-IS ratio with a band that scales as $\sqrt{L}$. Theoretically, we formalize length fairness via a Length Reweighting Error (LRE) and prove that small LRE yields a cosine directional guarantee between the clipped and true updates. Empirically, FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets on Qwen3-8B-Base model.

## ğŸ“ ìš”ì•½

FSPO(Fair Sequence Policy Optimization)ëŠ” LLMì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ê°•í™” í•™ìŠµ ë°©ë²•ìœ¼ë¡œ, ì¤‘ìš”ë„ ìƒ˜í”Œë§(IS) ê°€ì¤‘ì¹˜ì— ê¸¸ì´ ê³µì •ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. ê¸°ì¡´ PPO/GRPO ìŠ¤íƒ€ì¼ì˜ í´ë¦¬í•‘ì„ ì‹œí€€ìŠ¤ì— ì ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, FSPOëŠ” ì‹œí€€ìŠ¤ ë¡œê·¸-IS ë¹„ìœ¨ì„ $\sqrt{L}$ë¡œ ìŠ¤ì¼€ì¼ë§ëœ ë°´ë“œë¡œ í´ë¦¬í•‘í•©ë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ, ê¸¸ì´ ì¬ê°€ì¤‘ì¹˜ ì˜¤ë¥˜(LRE)ë¥¼ í†µí•´ ê¸¸ì´ ê³µì •ì„±ì„ ê³µì‹í™”í•˜ê³ , ì‘ì€ LREê°€ í´ë¦¬í•‘ëœ ì—…ë°ì´íŠ¸ì™€ ì‹¤ì œ ì—…ë°ì´íŠ¸ ê°„ì˜ ì½”ì‚¬ì¸ ë°©í–¥ì„±ì„ ë³´ì¥í•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ, FSPOëŠ” ê¸¸ì´ë³„ í´ë¦½ ë¹„ìœ¨ì„ í‰íƒ„í™”í•˜ê³ , í›ˆë ¨ì„ ì•ˆì •í™”í•˜ë©°, Qwen3-8B-Base ëª¨ë¸ì˜ ì—¬ëŸ¬ í‰ê°€ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. FSPOëŠ” LLMì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìˆ˜ì¤€ì˜ ê°•í™” í•™ìŠµ ë°©ë²•ìœ¼ë¡œ, ì¤‘ìš”ë„ ìƒ˜í”Œë§(IS) ê°€ì¤‘ì¹˜ì— ê¸¸ì´ ê³µì • í´ë¦¬í•‘ì„ ì ìš©í•©ë‹ˆë‹¤.
- 2. PPO/GRPO ìŠ¤íƒ€ì¼ì˜ í´ë¦¬í•‘ì„ ì‹œí€€ìŠ¤ì— ì ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, FSPOëŠ” ì‹œí€€ìŠ¤ ë¡œê·¸-IS ë¹„ìœ¨ì„ $\sqrt{L}$ë¡œ ìŠ¤ì¼€ì¼ë§ëœ ë°´ë“œë¡œ í´ë¦¬í•‘í•©ë‹ˆë‹¤.
- 3. ì´ë¡ ì ìœ¼ë¡œ, FSPOëŠ” ê¸¸ì´ ì¬ê°€ì¤‘ì¹˜ ì˜¤ë¥˜(LRE)ë¥¼ í†µí•´ ê¸¸ì´ ê³µì •ì„ í˜•ì‹í™”í•˜ê³ , ì‘ì€ LREê°€ í´ë¦¬í•‘ëœ ì—…ë°ì´íŠ¸ì™€ ì‹¤ì œ ì—…ë°ì´íŠ¸ ê°„ì˜ ì½”ì‚¬ì¸ ë°©í–¥ ë³´ì¥ì„ ì œê³µí•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ì ìœ¼ë¡œ, FSPOëŠ” ê¸¸ì´ êµ¬ê°„ ì „ë°˜ì— ê±¸ì³ í´ë¦½ ë¹„ìœ¨ì„ í‰íƒ„í™”í•˜ê³ , í›ˆë ¨ì„ ì•ˆì •í™”í•˜ë©°, Qwen3-8B-Base ëª¨ë¸ì˜ ì—¬ëŸ¬ í‰ê°€ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:41:07*