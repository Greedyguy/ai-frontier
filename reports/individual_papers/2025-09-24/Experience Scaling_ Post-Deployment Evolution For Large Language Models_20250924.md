<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:28:10.128341",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Experience Scaling",
    "Autonomous Interaction",
    "Collaborative Sharing",
    "Generalization to Unseen Tasks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Experience Scaling": 0.9,
    "Autonomous Interaction": 0.82,
    "Collaborative Sharing": 0.8,
    "Generalization to Unseen Tasks": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion, linking to a well-established concept in NLP.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Experience Scaling",
        "canonical": "Experience Scaling",
        "aliases": [
          "Post-Deployment Evolution"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework for LLM evolution, offering new insights and connections.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Autonomous Interaction",
        "canonical": "Autonomous Interaction",
        "aliases": [
          "Self-Interaction"
        ],
        "category": "unique_technical",
        "rationale": "Describes a key mechanism for LLMs to learn from the environment, enhancing connectivity.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Collaborative Sharing",
        "canonical": "Collaborative Sharing",
        "aliases": [
          "Knowledge Sharing"
        ],
        "category": "unique_technical",
        "rationale": "Highlights a method for LLMs to exchange learned experiences, fostering networked intelligence.",
        "novelty_score": 0.7,
        "connectivity_score": 0.68,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Generalization to Unseen Tasks",
        "canonical": "Generalization to Unseen Tasks",
        "aliases": [
          "Task Generalization"
        ],
        "category": "specific_connectable",
        "rationale": "Focuses on LLMs' ability to adapt, a crucial aspect of machine learning research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "saturation",
      "relevance",
      "efficiency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Experience Scaling",
      "resolved_canonical": "Experience Scaling",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Autonomous Interaction",
      "resolved_canonical": "Autonomous Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Collaborative Sharing",
      "resolved_canonical": "Collaborative Sharing",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.68,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Generalization to Unseen Tasks",
      "resolved_canonical": "Generalization to Unseen Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Experience Scaling: Post-Deployment Evolution For Large Language Models

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18771.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18771](https://arxiv.org/abs/2509.18771)

## 🔗 유사한 논문
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (87.2% similar)
- [[2025-09-23/mmExpert_ Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding_20250923|mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding]] (85.5% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (85.1% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (85.1% similar)
- [[2025-09-17/Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (85.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Generalization to Unseen Tasks|Generalization to Unseen Tasks]]
**⚡ Unique Technical**: [[keywords/Experience Scaling|Experience Scaling]], [[keywords/Autonomous Interaction|Autonomous Interaction]], [[keywords/Collaborative Sharing|Collaborative Sharing]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18771v1 Announce Type: new 
Abstract: Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.

## 📝 요약

이 논문은 대규모 언어 모델(LLM)의 발전을 위해 경험 확장이라는 새로운 프레임워크를 제안합니다. 기존의 모델 크기, 학습 데이터, 계산 능력 확장은 한계에 도달했기 때문에, 이 프레임워크는 환경과의 자율적 상호작용 및 경험 공유를 통해 모델을 지속적으로 발전시킵니다. 이 방법론은 원시 상호작용을 압축된 지식으로 정제하고, 저장된 내용을 주기적으로 갱신하여 관련성과 효율성을 유지합니다. 시뮬레이션된 현실 시나리오에서 검증한 결과, 경험 확장은 정확성을 높이고, 새로운 상황에서도 성능을 유지하며, LLM의 능력을 인간 생성 데이터의 한계를 넘어 확장할 수 있음을 보여줍니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)의 발전은 모델 크기, 학습 데이터, 계산 능력의 확대로 이루어졌으나, 인간이 생성한 텍스트의 한계로 인해 이러한 접근법은 포화 상태에 이르고 있다.
- 2. 경험 확장 프레임워크는 LLM이 환경과의 자율적인 상호작용을 통해 지속적으로 발전하고, 축적된 경험을 협력적으로 공유할 수 있도록 한다.
- 3. 이 프레임워크는 원시 상호작용을 포착하여 이를 간결하고 재사용 가능한 지식으로 정제하고, 저장된 콘텐츠를 주기적으로 개선하여 관련성과 효율성을 유지한다.
- 4. 시뮬레이션된 실제 시나리오에서 경험 확장은 정확성을 향상시키고, 시간이 지나도 성능을 유지하며, 새로운 상황에서도 성과를 유지하는 것으로 검증되었다.
- 5. 구조화된 배포 후 학습은 정적 인간 생성 데이터의 한계를 넘어 LLM의 능력을 확장할 수 있으며, 지속적인 지능 발전을 위한 확장 가능한 경로를 제공한다.


---

*Generated on 2025-09-24 13:28:10*