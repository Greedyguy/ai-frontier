<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:48:56.474465",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Physically-isolated Mixture of Experts",
    "Large Language Model",
    "Multimodal Learning",
    "Neural Network",
    "Inference Architecture"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Physically-isolated Mixture of Experts": 0.85,
    "Large Language Model": 0.8,
    "Multimodal Learning": 0.78,
    "Neural Network": 0.82,
    "Inference Architecture": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "PiMoE",
        "canonical": "Physically-isolated Mixture of Experts",
        "aliases": [
          "PiMoE"
        ],
        "category": "unique_technical",
        "rationale": "PiMoE is a novel architecture proposed in the paper, offering a new paradigm for integrating computation and reasoning.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the discussion and provide a connection to existing research in neural networks and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is relevant due to the discussion of integrating different capabilities within the architecture.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "Neural Networks"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are a fundamental component of the proposed architecture, linking to a wide range of AI research.",
        "novelty_score": 0.25,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Inference Architecture",
        "canonical": "Inference Architecture",
        "aliases": [
          "Inference System"
        ],
        "category": "unique_technical",
        "rationale": "The inference architecture is a key aspect of the PiMoE system, emphasizing its novel approach to computation and reasoning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "system"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "PiMoE",
      "resolved_canonical": "Physically-isolated Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Inference Architecture",
      "resolved_canonical": "Inference Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18169.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18169](https://arxiv.org/abs/2509.18169)

## 🔗 유사한 논문
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (86.7% similar)
- [[2025-09-24/Symphony-MoE_ Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts_20250924|Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts]] (84.6% similar)
- [[2025-09-24/LongCat-Flash-Thinking Technical Report_20250924|LongCat-Flash-Thinking Technical Report]] (84.5% similar)
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (84.1% similar)
- [[2025-09-23/Mini-Omni-Reasoner_ Token-Level Thinking-in-Speaking in Large Speech Models_20250923|Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models]] (84.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Neural Network|Neural Network]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Physically-isolated Mixture of Experts|Physically-isolated Mixture of Experts]], [[keywords/Inference Architecture|Inference Architecture]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18169v1 Announce Type: new 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.

## 📝 요약

PiMoE(Physically-isolated Mixture of Experts)는 복잡한 시스템에서 계산과 추론을 통합하는 새로운 아키텍처로, 기존의 대형 언어 모델(LLM)이 고정밀 수치 계산을 내재화하지 못하는 문제를 해결합니다. PiMoE는 전문가, 텍스트-계산 모듈, 라우터를 별도로 훈련한 후 통합하여, 추론 시 토큰 수준에서 계산과 추론을 지시합니다. 이 접근 방식은 LLM 미세 조정 및 다중 에이전트 시스템과 비교하여 더 높은 정확도와 응답 지연, 토큰 사용, GPU 에너지 소비에서 개선된 성능을 보여줍니다. PiMoE는 차세대 과학 및 산업 지능형 시스템에 효율적이고 해석 가능하며 확장 가능한 패러다임을 제공합니다.

## 🎯 주요 포인트

- 1. PiMoE는 계산 및 추론을 통합하는 새로운 훈련 및 추론 아키텍처로, 전문가와 텍스트-계산 모듈, 라우터를 별도로 훈련한 후 신경망에 계산 기능을 내재적으로 통합합니다.
- 2. PiMoE는 추론 시 라우터가 토큰 수준에서 계산과 추론을 지시하여, 단일 사고 체인 내에서 반복적인 교차를 가능하게 합니다.
- 3. PiMoE는 LLM 미세 조정 및 다중 에이전트 시스템 접근법과 비교하여 두 가지 추론-계산 작업에서 더 높은 정확도와 응답 지연, 토큰 사용량, GPU 에너지 소비에서의 개선을 달성합니다.
- 4. PiMoE는 차세대 과학 또는 산업 지능형 시스템을 위한 효율적이고 해석 가능하며 확장 가능한 패러다임을 제공합니다.


---

*Generated on 2025-09-24 14:48:56*