<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:48:56.474465",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Physically-isolated Mixture of Experts",
    "Large Language Model",
    "Multimodal Learning",
    "Neural Network",
    "Inference Architecture"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Physically-isolated Mixture of Experts": 0.85,
    "Large Language Model": 0.8,
    "Multimodal Learning": 0.78,
    "Neural Network": 0.82,
    "Inference Architecture": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "PiMoE",
        "canonical": "Physically-isolated Mixture of Experts",
        "aliases": [
          "PiMoE"
        ],
        "category": "unique_technical",
        "rationale": "PiMoE is a novel architecture proposed in the paper, offering a new paradigm for integrating computation and reasoning.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the discussion and provide a connection to existing research in neural networks and AI.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is relevant due to the discussion of integrating different capabilities within the architecture.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "Neural Networks"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are a fundamental component of the proposed architecture, linking to a wide range of AI research.",
        "novelty_score": 0.25,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Inference Architecture",
        "canonical": "Inference Architecture",
        "aliases": [
          "Inference System"
        ],
        "category": "unique_technical",
        "rationale": "The inference architecture is a key aspect of the PiMoE system, emphasizing its novel approach to computation and reasoning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance",
      "system"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "PiMoE",
      "resolved_canonical": "Physically-isolated Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.25,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Inference Architecture",
      "resolved_canonical": "Inference Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18169.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18169](https://arxiv.org/abs/2509.18169)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (86.7% similar)
- [[2025-09-24/Symphony-MoE_ Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts_20250924|Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts]] (84.6% similar)
- [[2025-09-24/LongCat-Flash-Thinking Technical Report_20250924|LongCat-Flash-Thinking Technical Report]] (84.5% similar)
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (84.1% similar)
- [[2025-09-23/Mini-Omni-Reasoner_ Token-Level Thinking-in-Speaking in Large Speech Models_20250923|Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Physically-isolated Mixture of Experts|Physically-isolated Mixture of Experts]], [[keywords/Inference Architecture|Inference Architecture]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18169v1 Announce Type: new 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.

## ğŸ“ ìš”ì•½

PiMoE(Physically-isolated Mixture of Experts)ëŠ” ë³µì¡í•œ ì‹œìŠ¤í…œì—ì„œ ê³„ì‚°ê³¼ ì¶”ë¡ ì„ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¡œ, ê¸°ì¡´ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê³ ì •ë°€ ìˆ˜ì¹˜ ê³„ì‚°ì„ ë‚´ì¬í™”í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. PiMoEëŠ” ì „ë¬¸ê°€, í…ìŠ¤íŠ¸-ê³„ì‚° ëª¨ë“ˆ, ë¼ìš°í„°ë¥¼ ë³„ë„ë¡œ í›ˆë ¨í•œ í›„ í†µí•©í•˜ì—¬, ì¶”ë¡  ì‹œ í† í° ìˆ˜ì¤€ì—ì„œ ê³„ì‚°ê³¼ ì¶”ë¡ ì„ ì§€ì‹œí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ LLM ë¯¸ì„¸ ì¡°ì • ë° ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ ë¹„êµí•˜ì—¬ ë” ë†’ì€ ì •í™•ë„ì™€ ì‘ë‹µ ì§€ì—°, í† í° ì‚¬ìš©, GPU ì—ë„ˆì§€ ì†Œë¹„ì—ì„œ ê°œì„ ëœ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. PiMoEëŠ” ì°¨ì„¸ëŒ€ ê³¼í•™ ë° ì‚°ì—… ì§€ëŠ¥í˜• ì‹œìŠ¤í…œì— íš¨ìœ¨ì ì´ê³  í•´ì„ ê°€ëŠ¥í•˜ë©° í™•ì¥ ê°€ëŠ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. PiMoEëŠ” ê³„ì‚° ë° ì¶”ë¡ ì„ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ í›ˆë ¨ ë° ì¶”ë¡  ì•„í‚¤í…ì²˜ë¡œ, ì „ë¬¸ê°€ì™€ í…ìŠ¤íŠ¸-ê³„ì‚° ëª¨ë“ˆ, ë¼ìš°í„°ë¥¼ ë³„ë„ë¡œ í›ˆë ¨í•œ í›„ ì‹ ê²½ë§ì— ê³„ì‚° ê¸°ëŠ¥ì„ ë‚´ì¬ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.
- 2. PiMoEëŠ” ì¶”ë¡  ì‹œ ë¼ìš°í„°ê°€ í† í° ìˆ˜ì¤€ì—ì„œ ê³„ì‚°ê³¼ ì¶”ë¡ ì„ ì§€ì‹œí•˜ì—¬, ë‹¨ì¼ ì‚¬ê³  ì²´ì¸ ë‚´ì—ì„œ ë°˜ë³µì ì¸ êµì°¨ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. PiMoEëŠ” LLM ë¯¸ì„¸ ì¡°ì • ë° ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì ‘ê·¼ë²•ê³¼ ë¹„êµí•˜ì—¬ ë‘ ê°€ì§€ ì¶”ë¡ -ê³„ì‚° ì‘ì—…ì—ì„œ ë” ë†’ì€ ì •í™•ë„ì™€ ì‘ë‹µ ì§€ì—°, í† í° ì‚¬ìš©ëŸ‰, GPU ì—ë„ˆì§€ ì†Œë¹„ì—ì„œì˜ ê°œì„ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. PiMoEëŠ” ì°¨ì„¸ëŒ€ ê³¼í•™ ë˜ëŠ” ì‚°ì—… ì§€ëŠ¥í˜• ì‹œìŠ¤í…œì„ ìœ„í•œ íš¨ìœ¨ì ì´ê³  í•´ì„ ê°€ëŠ¥í•˜ë©° í™•ì¥ ê°€ëŠ¥í•œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:48:56*