<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:27:12.358144",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Token Preference Optimization",
    "Visual-Anchored Reward",
    "Hallucination Mitigation",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Token Preference Optimization": 0.78,
    "Visual-Anchored Reward": 0.77,
    "Hallucination Mitigation": 0.75,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Token Preference Optimization",
        "canonical": "Token Preference Optimization",
        "aliases": [
          "TPO"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel approach introduced in the paper, crucial for linking discussions on optimization techniques in vision-language models.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Visual-Anchored Reward",
        "canonical": "Visual-Anchored Reward",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's contribution, focusing on token-level optimization linked to visual data.",
        "novelty_score": 0.82,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Hallucination Mitigation",
        "canonical": "Hallucination Mitigation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This is a key problem addressed by the paper, relevant for linking to broader discussions on improving model outputs.",
        "novelty_score": 0.7,
        "connectivity_score": 0.78,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLM"
        ],
        "category": "evolved_concepts",
        "rationale": "The paper builds on this concept, which is crucial for linking multimodal learning discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Direct Preference Optimization",
      "self-calibrated rewards"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Token Preference Optimization",
      "resolved_canonical": "Token Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Visual-Anchored Reward",
      "resolved_canonical": "Visual-Anchored Reward",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Hallucination Mitigation",
      "resolved_canonical": "Hallucination Mitigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.78,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2412.14487.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2412.14487](https://arxiv.org/abs/2412.14487)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization_20250923|Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization]] (86.1% similar)
- [[2025-09-23/Preference Distillation via Value based Reinforcement Learning_20250923|Preference Distillation via Value based Reinforcement Learning]] (85.6% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (84.8% similar)
- [[2025-09-22/OSPO_ Object-centric Self-improving Preference Optimization for Text-to-Image Generation_20250922|OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation]] (84.0% similar)
- [[2025-09-22/Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance_20250922|Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance]] (84.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Hallucination Mitigation|Hallucination Mitigation]]
**âš¡ Unique Technical**: [[keywords/Token Preference Optimization|Token Preference Optimization]], [[keywords/Visual-Anchored Reward|Visual-Anchored Reward]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2412.14487v4 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress, existing methods suffer from two drawbacks: 1) Lack of scalable token-level rewards; and 2) Neglect of visual-anchored tokens. To this end, we propose a novel Token Preference Optimization model with self-calibrated rewards (dubbed as TPO), which adaptively attends to visual-correlated tokens without fine-grained annotations. Specifically, we introduce a token-level \emph{visual-anchored} \emph{reward} as the difference of the logistic distributions of generated tokens conditioned on the raw image and the corrupted one. In addition, to highlight the informative visual-anchored tokens, a visual-aware training objective is proposed to enhance more accurate token-level optimization. Extensive experimental results have manifested the state-of-the-art performance of the proposed TPO. For example, by building on top of LLAVA-1.5-7B, our TPO boosts the performance absolute improvement for hallucination benchmarks.

## ğŸ“ ìš”ì•½

Direct Preference Optimization(DPO)ëŠ” ëŒ€ê·œëª¨ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(LVLM)ì—ì„œ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶° ì¶œë ¥ì„ ì¡°ì •í•˜ì—¬ í™˜ê° í˜„ìƒì„ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ë°©ë²•ì€ í™•ì¥ ê°€ëŠ¥í•œ í† í° ìˆ˜ì¤€ ë³´ìƒì˜ ë¶€ì¡±ê³¼ ì‹œê°ì ìœ¼ë¡œ ê³ ì •ëœ í† í°ì„ ê°„ê³¼í•˜ëŠ” ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì‹œê°ì ìœ¼ë¡œ ì—°ê´€ëœ í† í°ì— ì ì‘ì ìœ¼ë¡œ ì£¼ëª©í•˜ëŠ” ìƒˆë¡œìš´ í† í° ì„ í˜¸ ìµœì í™” ëª¨ë¸(TPO)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì„¸ë°€í•œ ì£¼ì„ ì—†ì´ë„ ì‘ë™í•˜ë©°, ì›ë³¸ ì´ë¯¸ì§€ì™€ ì†ìƒëœ ì´ë¯¸ì§€ì— ê¸°ë°˜í•œ ìƒì„± í† í°ì˜ ë¡œì§€ìŠ¤í‹± ë¶„í¬ ì°¨ì´ë¥¼ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ê³ ì •ëœ ë³´ìƒì„ ë„ì…í•©ë‹ˆë‹¤. ë˜í•œ, ì •ë³´ê°€ í’ë¶€í•œ ì‹œê°ì ìœ¼ë¡œ ê³ ì •ëœ í† í°ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ ì‹œê° ì¸ì‹ í•™ìŠµ ëª©í‘œë¥¼ ì œì•ˆí•˜ì—¬ ë” ì •í™•í•œ í† í° ìˆ˜ì¤€ ìµœì í™”ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ TPOê°€ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, LLAVA-1.5-7B ê¸°ë°˜ì—ì„œ í™˜ê° ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Direct Preference Optimization (DPO)ëŠ” ëŒ€ê·œëª¨ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(LVLMs)ì—ì„œ ì¸ê°„ì˜ ì„ í˜¸ë„ì— ë§ì¶° ì¶œë ¥ì„ ì¡°ì •í•˜ì—¬ í™˜ê° í˜„ìƒì„ ì™„í™”í•˜ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ í™•ì¥ ê°€ëŠ¥í•œ í† í° ìˆ˜ì¤€ì˜ ë³´ìƒ ë¶€ì¡±ê³¼ ì‹œê°ì  ì•µì»¤ í† í°ì„ ê°„ê³¼í•˜ëŠ” ë‘ ê°€ì§€ ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ì œì•ˆëœ Token Preference Optimization (TPO) ëª¨ë¸ì€ ì„¸ë°€í•œ ì£¼ì„ ì—†ì´ ì‹œê°ì ìœ¼ë¡œ ì—°ê´€ëœ í† í°ì— ì ì‘ì ìœ¼ë¡œ ì£¼ëª©í•˜ë©°, ìê¸° ë³´ì • ë³´ìƒì„ í†µí•´ ì´ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
- 4. TPOëŠ” ì›ë³¸ ì´ë¯¸ì§€ì™€ ì†ìƒëœ ì´ë¯¸ì§€ì— ì¡°ê±´í™”ëœ ìƒì„± í† í°ì˜ ë¡œì§€ìŠ¤í‹± ë¶„í¬ ì°¨ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ìˆ˜ì¤€ì˜ ì‹œê° ì•µì»¤ ë³´ìƒì„ ë„ì…í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, TPOëŠ” í™˜ê° ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:27:12*