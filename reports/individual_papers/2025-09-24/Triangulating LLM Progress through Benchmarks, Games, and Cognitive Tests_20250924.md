<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:49:39.690822",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Interactive Games",
    "Cognitive Tests",
    "Causal and Logical Reasoning",
    "Core Executive Functions"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Interactive Games": 0.78,
    "Cognitive Tests": 0.77,
    "Causal and Logical Reasoning": 0.82,
    "Core Executive Functions": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on evaluating language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "interactive games",
        "canonical": "Interactive Games",
        "aliases": [
          "Games",
          "Interactive Tests"
        ],
        "category": "unique_technical",
        "rationale": "Highlighted as superior to benchmarks for model discrimination.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "cognitive tests",
        "canonical": "Cognitive Tests",
        "aliases": [
          "Cognitive Assessments"
        ],
        "category": "unique_technical",
        "rationale": "Key to evaluating cognitive abilities of models.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "causal and logical reasoning",
        "canonical": "Causal and Logical Reasoning",
        "aliases": [
          "Reasoning Skills"
        ],
        "category": "specific_connectable",
        "rationale": "Correlates with both static and interactive tests, crucial for model evaluation.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "core executive functions",
        "canonical": "Core Executive Functions",
        "aliases": [
          "Executive Functions"
        ],
        "category": "unique_technical",
        "rationale": "Differentiates models in terms of social/emotional skills.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "standard benchmarks",
      "model performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "interactive games",
      "resolved_canonical": "Interactive Games",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "cognitive tests",
      "resolved_canonical": "Cognitive Tests",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "causal and logical reasoning",
      "resolved_canonical": "Causal and Logical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "core executive functions",
      "resolved_canonical": "Core Executive Functions",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.14359.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2502.14359](https://arxiv.org/abs/2502.14359)

## 🔗 유사한 논문
- [[2025-09-23/LLMsPark_ A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts_20250923|LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts]] (89.2% similar)
- [[2025-09-23/InMind_ Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles_20250923|InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles]] (88.2% similar)
- [[2025-09-23/AIPsychoBench_ Understanding the Psychometric Differences between LLMs and Humans_20250923|AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans]] (88.0% similar)
- [[2025-09-19/Rationality Check! Benchmarking the Rationality of Large Language Models_20250919|Rationality Check! Benchmarking the Rationality of Large Language Models]] (86.9% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (86.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Causal and Logical Reasoning|Causal and Logical Reasoning]]
**⚡ Unique Technical**: [[keywords/Interactive Games|Interactive Games]], [[keywords/Cognitive Tests|Cognitive Tests]], [[keywords/Core Executive Functions|Core Executive Functions]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.14359v3 Announce Type: replace 
Abstract: We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate for the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.

## 📝 요약

이 논문은 세 가지 평가 패러다임을 조사합니다: 표준 벤치마크, 인터랙티브 게임, 인지 테스트. 연구는 다양한 품질의 대형 언어 모델(LLM)을 구별하는 데 있어 벤치마크와 게임 중 어느 것이 더 효과적인지를 분석합니다. 인간의 인지 평가에서 영감을 받아, 효과적인 언어 사용에 필수적인 인지 능력을 측정하는 테스트를 개발하고, 이들이 벤치마크 및 게임 성능과의 상관관계를 조사합니다. 분석 결과, 인터랙티브 게임이 표준 벤치마크보다 모델을 구별하는 데 더 우수하며, 인과 및 논리적 추론은 두 테스트 모두와 상관관계가 있지만, 핵심 실행 기능 및 사회/정서적 기술은 게임과 더 관련이 있음을 발견했습니다. 저자들은 인간 능력 평가에서 영감을 받은 새로운 인터랙티브 벤치마크와 인지 과제 개발을 제안합니다.

## 🎯 주요 포인트

- 1. 상호작용 게임은 표준 벤치마크보다 다양한 품질의 LLM을 구별하는 데 더 효과적이다.
- 2. 인과적 및 논리적 추론은 정적 및 상호작용 테스트 모두와 상관관계가 있다.
- 3. 핵심 실행 기능과 사회/정서적 기술은 게임과 더 높은 상관관계를 보인다.
- 4. 인간의 인지 평가에서 영감을 받은 새로운 상호작용 벤치마크와 LLM에 특화된 인지 과제 개발이 필요하다.


---

*Generated on 2025-09-24 15:49:39*