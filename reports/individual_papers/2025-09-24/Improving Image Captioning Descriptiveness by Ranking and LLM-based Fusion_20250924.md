<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:23:06.670760",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Vision-Language Model",
    "BLIPScore",
    "Caption-Image Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Vision-Language Model": 0.82,
    "BLIPScore": 0.78,
    "Caption-Image Alignment": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's method for fusing captions, linking to broader NLP advancements.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "The paper's focus on image captioning directly ties into the evolving field of Vision-Language Models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "BLIPScore",
        "canonical": "BLIPScore",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "BLIPScore is a novel metric introduced in the paper, crucial for ranking image captions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "caption-image alignment",
        "canonical": "Caption-Image Alignment",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This concept is key to evaluating the effectiveness of image captioning models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "BLIPScore",
      "resolved_canonical": "BLIPScore",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "caption-image alignment",
      "resolved_canonical": "Caption-Image Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2306.11593.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2306.11593](https://arxiv.org/abs/2306.11593)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model_20250923|Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model]] (84.9% similar)
- [[2025-09-24/Align Where the Words Look_ Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning_20250924|Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning]] (83.8% similar)
- [[2025-09-18/Aligning Audio Captions with Human Preferences_20250918|Aligning Audio Captions with Human Preferences]] (83.6% similar)
- [[2025-09-23/Describe-to-Score_ Text-Guided Efficient Image Complexity Assessment_20250923|Describe-to-Score: Text-Guided Efficient Image Complexity Assessment]] (83.4% similar)
- [[2025-09-23/Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization_20250923|Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Caption-Image Alignment|Caption-Image Alignment]]
**âš¡ Unique Technical**: [[keywords/BLIPScore|BLIPScore]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2306.11593v2 Announce Type: replace-cross 
Abstract: State-of-The-Art (SoTA) image captioning models are often trained on the MicroSoft Common Objects in Context (MS-COCO) dataset, which contains human-annotated captions with an average length of approximately ten tokens. Although effective for general scene understanding, these short captions often fail to capture complex scenes and convey detailed information. Moreover, captioning models tend to exhibit bias towards the ``average'' caption, which captures only the more general aspects, thus overlooking finer details. In this paper, we present a novel approach to generate richer and more informative image captions by combining the captions generated from different SoTA captioning models. Our proposed method requires no additional model training: given an image, it leverages pre-trained models from the literature to generate the initial captions, and then ranks them using a newly introduced image-text-based metric, which we name BLIPScore. Subsequently, the top two captions are fused using a Large Language Model (LLM) to produce the final, more detailed description. Experimental results on the MS-COCO and Flickr30k test sets demonstrate the effectiveness of our approach in terms of caption-image alignment and hallucination reduction according to the ALOHa, CAPTURE, and Polos metrics. A subjective study lends additional support to these results, suggesting that the captions produced by our model are generally perceived as more consistent with human judgment. By combining the strengths of diverse SoTA models, our method enhances the quality and appeal of image captions, bridging the gap between automated systems and the rich and informative nature of human-generated descriptions. This advance enables the generation of more suitable captions for the training of both vision-language and captioning models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ MS-COCO ë°ì´í„°ì…‹ì„ í™œìš©í•œ ëª¨ë¸ë“¤ì€ ì§§ì€ ìº¡ì…˜ì„ ìƒì„±í•˜ì—¬ ë³µì¡í•œ ì¥ë©´ì˜ ì„¸ë¶€ì‚¬í•­ì„ ë†“ì¹˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì—¬ëŸ¬ ìµœì‹  ìº¡ì…”ë‹ ëª¨ë¸ì—ì„œ ìƒì„±ëœ ìº¡ì…˜ì„ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•˜ê³  ì •ë³´ê°€ ë§ì€ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì¶”ê°€ì ì¸ ëª¨ë¸ í•™ìŠµ ì—†ì´, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œë¶€í„° ì´ˆê¸° ìº¡ì…˜ì„ ìƒì„±í•˜ê³ , ìƒˆë¡­ê²Œ ì œì•ˆëœ BLIPScoreë¼ëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê¸°ë°˜ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ìƒìœ„ ë‘ ê°œì˜ ìº¡ì…˜ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í†µí•´ ê²°í•©ë˜ì–´ ìµœì¢… ìº¡ì…˜ì„ ë§Œë“­ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì€ MS-COCOì™€ Flickr30k í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ìº¡ì…˜-ì´ë¯¸ì§€ ì •ë ¬ê³¼ í™˜ê° ê°ì†Œ ì¸¡ë©´ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì£¼ê´€ì  ì—°êµ¬ì—ì„œë„ ì¸ê°„ì˜ íŒë‹¨ê³¼ ì¼ì¹˜í•˜ëŠ” ìº¡ì…˜ìœ¼ë¡œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ì–‘í•œ ìµœì‹  ëª¨ë¸ì˜ ê°•ì ì„ ê²°í•©í•˜ì—¬ ìë™í™”ëœ ì‹œìŠ¤í…œê³¼ ì¸ê°„ì´ ìƒì„±í•œ ì„¤ëª…ì˜ ê°„ê·¹ì„ ì¢íˆëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ SoTA ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ì€ MS-COCO ë°ì´í„°ì…‹ì˜ ì§§ì€ ìº¡ì…˜ì— ì˜ì¡´í•˜ì—¬ ë³µì¡í•œ ì¥ë©´ì„ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ SoTA ìº¡ì…”ë‹ ëª¨ë¸ì—ì„œ ìƒì„±ëœ ìº¡ì…˜ì„ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•˜ê³  ì •ë³´ê°€ ë§ì€ ì´ë¯¸ì§€ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ë°©ë²•ì€ ì¶”ê°€ì ì¸ ëª¨ë¸ í›ˆë ¨ ì—†ì´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ í™œìš©í•˜ë©°, BLIPScoreë¼ëŠ” ìƒˆë¡œìš´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê¸°ë°˜ ë©”íŠ¸ë¦­ì„ í†µí•´ ìº¡ì…˜ì„ í‰ê°€í•©ë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ MS-COCO ë° Flickr30k í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ìº¡ì…˜-ì´ë¯¸ì§€ ì •ë ¬ê³¼ í™˜ê° ê°ì†Œ ì¸¡ë©´ì—ì„œ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. ì£¼ê´€ì  ì—°êµ¬ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ìœ¼ë¡œ ìƒì„±ëœ ìº¡ì…˜ì´ ì¸ê°„ì˜ íŒë‹¨ê³¼ ë” ì¼ì¹˜í•˜ëŠ” ê²ƒìœ¼ë¡œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:23:06*