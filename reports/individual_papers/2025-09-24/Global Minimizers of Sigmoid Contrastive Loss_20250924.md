<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:53:32.656475",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Sigmoid Contrastive Loss",
    "SigLIP",
    "Spherical Codes",
    "Modality Gap"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.8,
    "Sigmoid Contrastive Loss": 0.78,
    "SigLIP": 0.77,
    "Spherical Codes": 0.72,
    "Modality Gap": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "contrastive pretraining",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "contrastive learning",
          "contrastive training"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive pretraining is a key technique in self-supervised learning, enhancing connectivity with related models and methods.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Sigmoid Contrastive Loss",
        "canonical": "Sigmoid Contrastive Loss",
        "aliases": [
          "sigmoid loss",
          "contrastive sigmoid"
        ],
        "category": "unique_technical",
        "rationale": "This loss function is central to the paper's contributions and offers a unique perspective on contrastive learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "SigLIP",
        "canonical": "SigLIP",
        "aliases": [
          "SigLIP model",
          "SigLIP2"
        ],
        "category": "unique_technical",
        "rationale": "SigLIP models are a focal point of the paper, representing a novel approach in the field.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "spherical codes",
        "canonical": "Spherical Codes",
        "aliases": [
          "spherical coding"
        ],
        "category": "specific_connectable",
        "rationale": "Spherical codes are relevant for understanding the geometric aspects of the model configurations discussed.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "modality gap",
        "canonical": "Modality Gap",
        "aliases": [
          "modality difference"
        ],
        "category": "unique_technical",
        "rationale": "Understanding the modality gap is crucial for improving model performance across different data types.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "temperature",
      "bias",
      "trainable"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "contrastive pretraining",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Sigmoid Contrastive Loss",
      "resolved_canonical": "Sigmoid Contrastive Loss",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "SigLIP",
      "resolved_canonical": "SigLIP",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "spherical codes",
      "resolved_canonical": "Spherical Codes",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "modality gap",
      "resolved_canonical": "Modality Gap",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Global Minimizers of Sigmoid Contrastive Loss

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18552.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18552](https://arxiv.org/abs/2509.18552)

## 🔗 유사한 논문
- [[2025-09-17/Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation_20250917|Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation]] (82.7% similar)
- [[2025-09-22/Gradient Alignment in Physics-informed Neural Networks_ A Second-Order Optimization Perspective_20250922|Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective]] (82.5% similar)
- [[2025-09-23/Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment_20250923|Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment]] (82.4% similar)
- [[2025-09-23/Depth Edge Alignment Loss_ DEALing with Depth in Weakly Supervised Semantic Segmentation_20250923|Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation]] (81.7% similar)
- [[2025-09-22/Global Pre-fixing, Local Adjusting_ A Simple yet Effective Contrastive Strategy for Continual Learning_20250922|Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning]] (81.3% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Spherical Codes|Spherical Codes]]
**⚡ Unique Technical**: [[keywords/Sigmoid Contrastive Loss|Sigmoid Contrastive Loss]], [[keywords/SigLIP|SigLIP]], [[keywords/Modality Gap|Modality Gap]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18552v1 Announce Type: cross 
Abstract: The meta-task of obtaining and aligning representations through contrastive pretraining is steadily gaining importance since its introduction in CLIP and ALIGN. In this paper we theoretically explain the advantages of synchronizing with trainable inverse temperature and bias under the sigmoid loss, as implemented in the recent SigLIP and SigLIP2 models of Google DeepMind. Temperature and bias can drive the loss function to zero for a rich class of configurations that we call $(\mathsf{m}, \mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m}, \mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object related to spherical codes and are parametrized by a margin $\mathsf{m}$ and relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of constellations to theoretically justify the success of SigLIP on retrieval, to explain the modality gap present in SigLIP, and to identify the necessary dimension for producing high-quality representations. Finally, we propose a reparameterization of the sigmoid loss with explicit relative bias, which improves training dynamics in experiments with synthetic data.

## 📝 요약

이 논문은 대조적 사전 학습을 통해 표현을 얻고 정렬하는 메타 작업의 중요성을 설명합니다. Google DeepMind의 SigLIP 및 SigLIP2 모델에서 사용된 시그모이드 손실 함수의 가변 온도와 바이어스를 동기화하는 이점이 이론적으로 설명됩니다. 저자는 $(\mathsf{m}, \mathsf{b}_{\mathsf{rel}})$-별자리라는 새로운 조합 객체를 소개하며, 이는 구면 코드와 관련이 있고 마진 $\mathsf{m}$과 상대 바이어스 $\mathsf{b}_{\mathsf{rel}}$로 매개변수화됩니다. 이러한 별자리를 통해 SigLIP의 검색 성공을 이론적으로 정당화하고, SigLIP에서 나타나는 모달리티 격차를 설명하며, 고품질 표현을 생성하기 위한 필수 차원을 식별합니다. 마지막으로, 상대 바이어스를 명시적으로 포함한 시그모이드 손실의 재매개변수를 제안하여 합성 데이터 실험에서 학습 역학을 개선합니다.

## 🎯 주요 포인트

- 1. 대조적 사전 훈련을 통한 표현 획득 및 정렬의 중요성이 CLIP 및 ALIGN의 도입 이후 점점 커지고 있습니다.
- 2. SigLIP 및 SigLIP2 모델에서 구현된 시그모이드 손실 하의 학습 가능한 역온도 및 바이어스 동기화의 이점을 이론적으로 설명합니다.
- 3. $(\mathsf{m}, \mathsf{b}_{\mathsf{rel}})$-Constellations라는 새로운 조합 객체를 도입하여 SigLIP의 검색 성공을 이론적으로 정당화합니다.
- 4. SigLIP에서 나타나는 모달리티 격차를 설명하고 고품질 표현 생성을 위한 필요한 차원을 식별합니다.
- 5. 명시적 상대 바이어스를 사용한 시그모이드 손실의 재매개변수를 제안하여 합성 데이터 실험에서 훈련 역학을 개선합니다.


---

*Generated on 2025-09-24 13:53:32*