<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:36:41.798680",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Latent Diffusion Model",
    "Video Counterfactual Explanations",
    "Deep Learning",
    "Temporal Coherence"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Latent Diffusion Model": 0.78,
    "Video Counterfactual Explanations": 0.8,
    "Deep Learning": 0.75,
    "Temporal Coherence": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Latent Diffusion Model",
        "canonical": "Latent Diffusion Model",
        "aliases": [
          "LD-ViCE"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel framework for generating video counterfactual explanations, enhancing understanding of AI model behavior.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Video Counterfactual Explanations",
        "canonical": "Video Counterfactual Explanations",
        "aliases": [
          "Video Counterfactuals"
        ],
        "category": "unique_technical",
        "rationale": "Focuses on generating explanations for video-based AI systems, which is crucial for interpretability in safety-critical domains.",
        "novelty_score": 0.7,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Deep Learning",
        "canonical": "Deep Learning",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Central to the development and functioning of video-based AI systems, providing a foundation for understanding model behavior.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "Temporal Coherence",
        "canonical": "Temporal Coherence",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key for ensuring explanations maintain consistency over time, critical for video analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Latent Diffusion Model",
      "resolved_canonical": "Latent Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Video Counterfactual Explanations",
      "resolved_canonical": "Video Counterfactual Explanations",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Deep Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Temporal Coherence",
      "resolved_canonical": "Temporal Coherence",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.08422.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.08422](https://arxiv.org/abs/2509.08422)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/VideoRFT_ Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning_20250923|VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning]] (83.0% similar)
- [[2025-09-23/VidCLearn_ A Continual Learning Approach for Text-to-Video Generation_20250923|VidCLearn: A Continual Learning Approach for Text-to-Video Generation]] (82.7% similar)
- [[2025-09-23/V-CECE_ Visual Counterfactual Explanations via Conceptual Edits_20250923|V-CECE: Visual Counterfactual Explanations via Conceptual Edits]] (82.6% similar)
- [[2025-09-23/Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment_20250923|Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment]] (82.3% similar)
- [[2025-09-24/Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems_20250924|Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Temporal Coherence|Temporal Coherence]]
**âš¡ Unique Technical**: [[keywords/Latent Diffusion Model|Latent Diffusion Model]], [[keywords/Video Counterfactual Explanations|Video Counterfactual Explanations]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.08422v2 Announce Type: replace-cross 
Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains.

## ğŸ“ ìš”ì•½

LD-ViCEëŠ” ë¹„ë””ì˜¤ ê¸°ë°˜ AI ëª¨ë¸ì˜ í–‰ë™ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ì•ˆì „ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì—ì„œ AIì˜ ì‹ ë¢°ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìµœì²¨ë‹¨ í™•ì‚° ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì ì¬ ê³µê°„ì—ì„œ ì‘ë™í•¨ìœ¼ë¡œì¨ ì„¤ëª… ìƒì„±ì˜ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê³ , ì¶”ê°€ì ì¸ ì •ì œ ê³¼ì •ì„ í†µí•´ í˜„ì‹¤ì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ ë°˜ì‚¬ì‹¤ì  ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. EchoNet-Dynamic, FERV39k, Something-Something V2 ë“± ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì—ì„œ LD-ViCEëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 68% ë†’ì€ R2 ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©°, ì¶”ë¡  ì‹œê°„ì„ ì ˆë°˜ìœ¼ë¡œ ë‹¨ì¶•í–ˆìŠµë‹ˆë‹¤. ì§ˆì  ë¶„ì„ ê²°ê³¼, LD-ViCEëŠ” ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê³  ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ì„¤ëª…ì„ ìƒì„±í•˜ì—¬ ëª¨ë¸ì˜ í–‰ë™ì— ëŒ€í•œ ê·€ì¤‘í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LD-ViCEëŠ” ë¹„ë””ì˜¤ ê¸°ë°˜ AI ëª¨ë¸ì˜ í–‰ë™ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¡œ, ì ì¬ ê³µê°„ì—ì„œ ì‘ë™í•˜ì—¬ ì„¤ëª… ìƒì„±ì˜ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤.
- 2. LD-ViCEëŠ” ìµœì‹  í™•ì‚° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í˜„ì‹¤ì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ ë°˜ì‚¬ì‹¤ì  ì„¤ëª…ì„ ìƒì„±í•˜ë©°, ì¶”ê°€ì ì¸ ì •ì œ ë‹¨ê³„ë¥¼ í†µí•´ ì´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, LD-ViCEëŠ” EchoNet-Dynamic, FERV39k, Something-Something V2 ë“± ì„¸ ê°€ì§€ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
- 4. LD-ViCEëŠ” ìµœì‹  ë°©ë²•ë³´ë‹¤ ìµœëŒ€ 68% ë†’ì€ R2 ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©° ì¶”ë¡  ì‹œê°„ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.
- 5. LD-ViCEëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê³  ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ì„¤ëª…ì„ ìƒì„±í•˜ì—¬ ì•ˆì „ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì—ì„œ AIì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°°í¬ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ì§„ì „ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:36:41*