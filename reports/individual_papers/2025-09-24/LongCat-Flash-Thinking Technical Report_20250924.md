<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:29:57.566935",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Chain-of-Thought",
    "Reinforcement Learning",
    "Dynamic ORchestration for Asynchronous rollout",
    "Agentic Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.78,
    "Chain-of-Thought": 0.77,
    "Reinforcement Learning": 0.8,
    "Dynamic ORchestration for Asynchronous rollout": 0.79,
    "Agentic Reasoning": 0.76
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "unique_technical",
        "rationale": "Mixture-of-Experts is a specialized model architecture that enhances reasoning capabilities, providing unique insights into model design.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Chain-of-Thought is a novel training approach that significantly impacts reasoning models, offering a unique perspective on cognitive processes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational technique in AI, crucial for understanding the training dynamics of reasoning models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Dynamic ORchestration for Asynchronous rollout",
        "canonical": "Dynamic ORchestration for Asynchronous rollout",
        "aliases": [
          "DORA"
        ],
        "category": "unique_technical",
        "rationale": "DORA is a unique framework that significantly accelerates training, providing a novel approach to model optimization.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      },
      {
        "surface": "Agentic Reasoning",
        "canonical": "Agentic Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Agentic Reasoning is a specific reasoning type that enhances model efficiency, relevant for linking to cognitive AI research.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.76
      }
    ],
    "ban_list_suggestions": [
      "training process",
      "state-of-the-art performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Dynamic ORchestration for Asynchronous rollout",
      "resolved_canonical": "Dynamic ORchestration for Asynchronous rollout",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Agentic Reasoning",
      "resolved_canonical": "Agentic Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.76
      }
    }
  ]
}
-->

# LongCat-Flash-Thinking Technical Report

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18883.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18883](https://arxiv.org/abs/2509.18883)

## 🔗 유사한 논문
- [[2025-09-22/LongCat-Flash Technical Report_20250922|LongCat-Flash Technical Report]] (92.9% similar)
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (85.2% similar)
- [[2025-09-22/FLARE_ Faithful Logic-Aided Reasoning and Exploration_20250922|FLARE: Faithful Logic-Aided Reasoning and Exploration]] (84.9% similar)
- [[2025-09-23/MCP_ A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models_20250923|MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models]] (84.9% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (84.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Agentic Reasoning|Agentic Reasoning]]
**⚡ Unique Technical**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/Dynamic ORchestration for Asynchronous rollout|Dynamic ORchestration for Asynchronous rollout]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18883v1 Announce Type: new 
Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.

## 📝 요약

LongCat-Flash-Thinking은 5600억 개의 매개변수를 가진 효율적인 오픈 소스 전문가 혼합(MoE) 추론 모델입니다. 이 모델은 긴 사고 사슬(CoT) 데이터로 시작하여 대규모 강화 학습(RL)로 이어지는 정교한 훈련 과정을 통해 발전되었습니다. 특히, 도메인 병렬 훈련 방식을 도입하여 서로 다른 도메인(STEM, 코드, 에이전트 등)의 최적화를 분리하고, 이를 결합하여 거의 파레토 최적의 모델을 구축했습니다. DORA 시스템을 활용한 비동기 롤아웃은 훈련 속도를 세 배 이상 향상시켰습니다. LongCat-Flash-Thinking은 복잡한 추론 작업에서 최첨단 성능을 보이며, 특히 에이전트 추론에서 평균 토큰 소비를 64.5% 줄이면서도 정확도를 유지했습니다. 이 모델은 추론 시스템과 에이전트 AI 연구의 발전을 촉진하기 위해 공개되었습니다.

## 🎯 주요 포인트

- 1. LongCat-Flash-Thinking은 5600억 개의 매개변수를 가진 효율적인 오픈 소스 전문가 혼합(MoE) 추론 모델입니다.
- 2. 이 모델은 긴 연쇄적 사고(CoT) 데이터의 콜드 스타트와 대규모 강화 학습(RL)을 통해 고급 기능을 개발했습니다.
- 3. 도메인 병렬 학습 방식을 통해 서로 다른 도메인(STEM, 코드, 에이전틱)의 최적화를 분리하고, 이를 결합하여 거의 파레토 최적의 모델을 생성했습니다.
- 4. DORA 시스템을 활용하여 비동기 롤아웃을 위한 대규모 RL 프레임워크로, 동기식 방법에 비해 3배 이상의 학습 속도를 달성했습니다.
- 5. LongCat-Flash-Thinking은 복잡한 추론 작업에서 오픈 소스 모델 중 최첨단 성능을 보여주며, AIME-25에서 평균 토큰 소비를 64.5% 줄이면서도 정확성을 유지했습니다.


---

*Generated on 2025-09-24 13:29:57*