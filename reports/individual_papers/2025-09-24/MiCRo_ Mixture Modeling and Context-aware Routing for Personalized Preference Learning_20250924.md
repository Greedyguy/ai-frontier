<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:21:06.424248",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Mixture Modeling",
    "Context-aware Routing",
    "Reinforcement Learning from Human Feedback"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Mixture Modeling": 0.72,
    "Context-aware Routing": 0.7,
    "Reinforcement Learning from Human Feedback": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are central to the paper's discussion on personalization and alignment, linking to broader discussions in AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Mixture Modeling",
        "canonical": "Mixture Modeling",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a core component of the proposed framework, critical for understanding the method's novelty.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.72
      },
      {
        "surface": "Context-aware Routing",
        "canonical": "Context-aware Routing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is essential for the paper's approach to personalized preference learning, highlighting its novel routing strategy.",
        "novelty_score": 0.78,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Reinforcement Learning from Human Feedback",
        "canonical": "Reinforcement Learning from Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "specific_connectable",
        "rationale": "RLHF is a key technique discussed in the context of aligning LLMs, connecting to broader reinforcement learning topics.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "reward modeling",
      "human preferences"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Mixture Modeling",
      "resolved_canonical": "Mixture Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Context-aware Routing",
      "resolved_canonical": "Context-aware Routing",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Human Feedback",
      "resolved_canonical": "Reinforcement Learning from Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2505.24846.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2505.24846](https://arxiv.org/abs/2505.24846)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (86.0% similar)
- [[2025-09-23/Post-hoc Reward Calibration_ A Case Study on Length Bias_20250923|Post-hoc Reward Calibration: A Case Study on Length Bias]] (85.8% similar)
- [[2025-09-23/R3_ Robust Rubric-Agnostic Reward Models_20250923|R3: Robust Rubric-Agnostic Reward Models]] (85.0% similar)
- [[2025-09-22/BaseReward_ A Strong Baseline for Multimodal Reward Model_20250922|BaseReward: A Strong Baseline for Multimodal Reward Model]] (84.6% similar)
- [[2025-09-18/Aligning Audio Captions with Human Preferences_20250918|Aligning Audio Captions with Human Preferences]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning from Human Feedback|Reinforcement Learning from Human Feedback]]
**âš¡ Unique Technical**: [[keywords/Mixture Modeling|Mixture Modeling]], [[keywords/Context-aware Routing|Context-aware Routing]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.24846v2 Announce Type: replace 
Abstract: Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ(RLHF)ì„ í™œìš©í•˜ì—¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì •ë ¬í•  ë•Œ ë³´ìƒ ëª¨ë¸ë§ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ Bradley-Terry(BT) ëª¨ë¸ ê¸°ë°˜ ë³´ìƒ ëª¨ë¸ë§ì€ ì¸ê°„ì˜ ë‹¤ì–‘í•œ ì„ í˜¸ë„ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í•´ ê°œì¸í™”ì™€ ë‹¤ì›ì  ì •ë ¬ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ MiCRoë¼ëŠ” ë‘ ë‹¨ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ëŒ€ê·œëª¨ ì´ì§„ ì„ í˜¸ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë§¥ë½ ì¸ì‹ í˜¼í•© ëª¨ë¸ë§ ì ‘ê·¼ë²•ì„ í†µí•´ ë‹¤ì–‘í•œ ì¸ê°„ ì„ í˜¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ì˜¨ë¼ì¸ ë¼ìš°íŒ… ì „ëµì„ ë„ì…í•˜ì—¬ ë§¥ë½ì— ë”°ë¼ í˜¼í•© ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•¨ìœ¼ë¡œì¨ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì„ í˜¸ ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MiCRoëŠ” ë‹¤ì–‘í•œ ì¸ê°„ ì„ í˜¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ë©° ê°œì¸í™” ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³´ìƒ ëª¨ë¸ë§ì€ ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ê°•í™” í•™ìŠµì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ì •ë ¬í•˜ê¸° ìœ„í•œ í•µì‹¬ ë‹¨ê³„ì…ë‹ˆë‹¤.
- 2. Bradley-Terry ëª¨ë¸ ê¸°ë°˜ì˜ ë³´ìƒ ëª¨ë¸ë§ì€ ì¸ê°„ì˜ ë‹¤ì–‘í•œ ì„ í˜¸ë„ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í•´ ê°œì¸í™” ë° ë‹¤ì›ì  ì •ë ¬ì„ ì œí•œí•©ë‹ˆë‹¤.
- 3. MiCRoëŠ” ëŒ€ê·œëª¨ ì´ì§„ ì„ í˜¸ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ëª…ì‹œì ì¸ ì„¸ë¶€ ì£¼ì„ ì—†ì´ ê°œì¸í™”ëœ ì„ í˜¸ í•™ìŠµì„ í–¥ìƒì‹œí‚¤ëŠ” ë‘ ë‹¨ê³„ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 4. MiCRoëŠ” ë¬¸ë§¥ ì¸ì‹ í˜¼í•© ëª¨ë¸ë§ ì ‘ê·¼ë²•ê³¼ ì˜¨ë¼ì¸ ë¼ìš°íŒ… ì „ëµì„ í†µí•´ ë‹¤ì–‘í•œ ì¸ê°„ ì„ í˜¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ê³ , íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì„ í˜¸ ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 5. ì—¬ëŸ¬ ì„ í˜¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, MiCRoëŠ” ë‹¤ì–‘í•œ ì¸ê°„ ì„ í˜¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ê³ , í•˜ìœ„ ê°œì¸í™”ë¥¼ í¬ê²Œ ê°œì„ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:21:06*