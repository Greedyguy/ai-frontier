<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:29:09.689763",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixed Advantage Policy Optimization",
    "Group Relative Policy Optimization",
    "Reinforcement Learning",
    "Advantage Function"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixed Advantage Policy Optimization": 0.78,
    "Group Relative Policy Optimization": 0.7,
    "Reinforcement Learning": 0.8,
    "Advantage Function": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixed Advantage Policy Optimization",
        "canonical": "Mixed Advantage Policy Optimization",
        "aliases": [
          "MAPO"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel strategy in reinforcement learning that addresses specific problems in advantage allocation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "Serves as a foundational concept that MAPO builds upon, facilitating understanding of the new method's context.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A core area of study relevant to the paper's contributions, providing a broad context for the research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Advantage Function",
        "canonical": "Advantage Function",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Central to the paper's methodology, it is crucial for understanding the proposed optimization strategy.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "trajectory",
      "samples",
      "certainty"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixed Advantage Policy Optimization",
      "resolved_canonical": "Mixed Advantage Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Advantage Function",
      "resolved_canonical": "Advantage Function",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# MAPO: Mixed Advantage Policy Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18849.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18849](https://arxiv.org/abs/2509.18849)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (86.3% similar)
- [[2025-09-23/GRPO-LEAD_ A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models_20250923|GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models]] (85.3% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (85.1% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (84.7% similar)
- [[2025-09-23/GPO_ Learning from Critical Steps to Improve LLM Reasoning_20250923|GPO: Learning from Critical Steps to Improve LLM Reasoning]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Advantage Function|Advantage Function]]
**âš¡ Unique Technical**: [[keywords/Mixed Advantage Policy Optimization|Mixed Advantage Policy Optimization]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18849v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.

## ğŸ“ ìš”ì•½

ìµœê·¼ ê°•í™” í•™ìŠµì˜ ë°œì „ì€ Group Relative Policy Optimization (GRPO)ì™€ ê°™ì€ ê¸°ì´ˆ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì´ì  ì—­ì „ ë° ì´ì  ë¯¸ëŸ¬ ë¬¸ì œë¡œ ì¸í•´ ì¿¼ë¦¬ ìƒ˜í”Œ ê°„ì˜ ì ì ˆí•œ ì´ì  í• ë‹¹ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” Mixed Advantage Policy Optimization (MAPO)ë¼ëŠ” ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ GRPO ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê²½ë¡œì˜ í™•ì‹¤ì„±ì´ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚¨ì„ ë°œê²¬í•˜ê³ , ë†’ì€ í™•ì‹¤ì„±ì„ ê°€ì§„ ìƒ˜í”Œì— ëŒ€í•´ ì´ì  ë°±ë¶„ìœ¨ í¸ì°¨ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, ìƒ˜í”Œì˜ ê²½ë¡œ í™•ì‹¤ì„±ì— ë”°ë¼ ì´ì  í•¨ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¬ì¡°ì •í•˜ì—¬ ìƒ˜í”Œ íŠ¹ì„±ì— ë§ê²Œ ì ì‘ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. ê´€ë ¨ ìµœì‹  ë°©ë²•ë“¤ê³¼ì˜ ë¹„êµ ë° ë‹¤ì–‘í•œ ì´ì  ë³€í˜•ì— ëŒ€í•œ ì†Œê±° ì—°êµ¬ë¥¼ í†µí•´ ì œì•ˆí•œ ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GRPOëŠ” ê¸°ì´ˆ ëª¨ë¸ì˜ ì¶”ë¡  ì‘ì—… ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ì§€ë§Œ, ì´ì  ì—­ì „ ë° ì´ì  ë¯¸ëŸ¬ ë¬¸ì œë¡œ ì¸í•´ ì¿¼ë¦¬ ìƒ˜í”Œ ê°„ì˜ í•©ë¦¬ì ì¸ ì´ì  í• ë‹¹ì´ ë°©í•´ë°›ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ GRPO ì „ëµì¸ í˜¼í•© ì´ì  ì •ì±… ìµœì í™”(MAPO)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ìš°ë¦¬ëŠ” ê²½ë¡œê°€ ë‹¤ë¥¸ í™•ì‹¤ì„±ì„ ê°€ì§€ê³  ë‚˜íƒ€ë‚œë‹¤ëŠ” ê²ƒì„ ë°í˜€ë‚´ê³ , ë†’ì€ í™•ì‹¤ì„± ê²½ë¡œë¥¼ ê°€ì§„ ìƒ˜í”Œì— ëŒ€í•œ ì´ì  ë°±ë¶„ìœ¨ í¸ì°¨ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ìƒ˜í”Œì˜ ê²½ë¡œ í™•ì‹¤ì„±ì— ë”°ë¼ ì´ì  í•¨ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¬ì¡°ì •í•˜ì—¬ ìƒ˜í”Œë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì ì‘í˜• ì´ì  í•¨ìˆ˜ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
- 5. ê´€ë ¨ ìµœì²¨ë‹¨ ë°©ë²•ê³¼ì˜ ë¹„êµ ë° ë‹¤ì–‘í•œ ì´ì  ë³€í˜•ì— ëŒ€í•œ ì†Œê±° ì—°êµ¬ë¥¼ í†µí•´ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:29:09*