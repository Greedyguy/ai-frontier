<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:26:49.454867",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Safe Reinforcement Learning",
    "Partially Observable Markov Decision Processes",
    "Privileged Information",
    "Asymmetric Actor-Critic Structure"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Safe Reinforcement Learning": 0.85,
    "Partially Observable Markov Decision Processes": 0.79,
    "Privileged Information": 0.81,
    "Asymmetric Actor-Critic Structure": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Safe Reinforcement Learning",
        "canonical": "Safe Reinforcement Learning",
        "aliases": [
          "Safe RL"
        ],
        "category": "unique_technical",
        "rationale": "This term is central to the paper's contribution and connects to the broader topic of reinforcement learning with a focus on safety.",
        "novelty_score": 0.65,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Partially Observable Markov Decision Processes",
        "canonical": "Partially Observable Markov Decision Processes",
        "aliases": [
          "POMDPs"
        ],
        "category": "specific_connectable",
        "rationale": "This is a key concept in reinforcement learning, particularly relevant to the challenges addressed in the paper.",
        "novelty_score": 0.58,
        "connectivity_score": 0.83,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "Privileged Information",
        "canonical": "Privileged Information",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The concept of using privileged information is a novel approach in the context of this research, enhancing the model's safety and performance.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.76,
        "link_intent_score": 0.81
      },
      {
        "surface": "Asymmetric Actor-Critic Structure",
        "canonical": "Asymmetric Actor-Critic Structure",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This structure is a specific innovation in the paper that contributes to improved performance in Safe RL.",
        "novelty_score": 0.7,
        "connectivity_score": 0.64,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Safe Reinforcement Learning",
      "resolved_canonical": "Safe Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Partially Observable Markov Decision Processes",
      "resolved_canonical": "Partially Observable Markov Decision Processes",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.83,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Privileged Information",
      "resolved_canonical": "Privileged Information",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.76,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Asymmetric Actor-Critic Structure",
      "resolved_canonical": "Asymmetric Actor-Critic Structure",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.64,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2508.02159.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2508.02159](https://arxiv.org/abs/2508.02159)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control_20250922|Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control]] (83.9% similar)
- [[2025-09-24/SPiDR_ A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer_20250924|SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer]] (83.1% similar)
- [[2025-09-23/Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality_20250923|Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality]] (82.8% similar)
- [[2025-09-23/Robust Reinforcement Learning with Dynamic Distortion Risk Measures_20250923|Robust Reinforcement Learning with Dynamic Distortion Risk Measures]] (82.2% similar)
- [[2025-09-23/Safe Guaranteed Dynamics Exploration with Probabilistic Models_20250923|Safe Guaranteed Dynamics Exploration with Probabilistic Models]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Partially Observable Markov Decision Processes|Partially Observable Markov Decision Processes]]
**âš¡ Unique Technical**: [[keywords/Safe Reinforcement Learning|Safe Reinforcement Learning]], [[keywords/Privileged Information|Privileged Information]], [[keywords/Asymmetric Actor-Critic Structure|Asymmetric Actor-Critic Structure]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.02159v2 Announce Type: replace 
Abstract: Partial observability presents a significant challenge for Safe Reinforcement Learning (Safe RL), as it impedes the identification of potential risks and rewards. Leveraging specific types of privileged information during training to mitigate the effects of partial observability has yielded notable empirical successes. In this paper, we propose Asymmetric Constrained Partially Observable Markov Decision Processes (ACPOMDPs) to theoretically examine the advantages of incorporating privileged information in Safe RL. Building upon ACPOMDPs, we propose the Privileged Information Guided Dreamer (PIGDreamer), a model-based RL approach that leverages privileged information to enhance the agent's safety and performance through privileged representation alignment and an asymmetric actor-critic structure. Our empirical results demonstrate that PIGDreamer significantly outperforms existing Safe RL methods. Furthermore, compared to alternative privileged RL methods, our approach exhibits enhanced performance, robustness, and efficiency. Codes are available at: https://github.com/hggforget/PIGDreamer.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì•ˆì „ ê°•í™” í•™ìŠµ(Safe RL)ì—ì„œ ë¶€ë¶„ ê´€ì°° ê°€ëŠ¥ì„±ì´ ì ì¬ì  ìœ„í—˜ê³¼ ë³´ìƒì˜ ì‹ë³„ì„ ë°©í•´í•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íŠ¹ì • íŠ¹ê¶Œ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì €ìë“¤ì€ ì´ë¡ ì ìœ¼ë¡œ íŠ¹ê¶Œ ì •ë³´ì˜ ì´ì ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë¹„ëŒ€ì¹­ ì œì•½ ë¶€ë¶„ ê´€ì°° ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(ACPOMDPs)ì„ ì œì•ˆí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ê¶Œ ì •ë³´ ì•ˆë‚´ ë“œë¦¬ë¨¸(PIGDreamer)ë¼ëŠ” ëª¨ë¸ ê¸°ë°˜ RL ì ‘ê·¼ë²•ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. PIGDreamerëŠ” íŠ¹ê¶Œ ì •ë³´ í™œìš©ì„ í†µí•´ ì—ì´ì „íŠ¸ì˜ ì•ˆì „ì„±ê³¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ë¹„ëŒ€ì¹­ ì•¡í„°-í¬ë¦¬í‹± êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, PIGDreamerëŠ” ê¸°ì¡´ì˜ ì•ˆì „ RL ë°©ë²•ë“¤ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ë¥¸ íŠ¹ê¶Œ RL ë°©ë²•ë“¤ì— ë¹„í•´ ì„±ëŠ¥, ê²¬ê³ ì„±, íš¨ìœ¨ì„± ë©´ì—ì„œ ìš°ìˆ˜í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¶€ë¶„ ê´€ì¸¡ì€ ì•ˆì „ ê°•í™” í•™ìŠµ(Safe RL)ì—ì„œ ì ì¬ì  ìœ„í—˜ê³¼ ë³´ìƒì„ ì‹ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ì´ˆë˜í•©ë‹ˆë‹¤.
- 2. í›ˆë ¨ ì¤‘ íŠ¹ì • ìœ í˜•ì˜ íŠ¹ê¶Œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¶€ë¶„ ê´€ì¸¡ì˜ ì˜í–¥ì„ ì™„í™”í•˜ëŠ” ê²ƒì´ ì‹¤ì§ˆì ì¸ ì„±ê³µì„ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Asymmetric Constrained Partially Observable Markov Decision Processes (ACPOMDPs)ë¥¼ ì œì•ˆí•˜ì—¬ Safe RLì—ì„œ íŠ¹ê¶Œ ì •ë³´ì˜ ì´ì ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ PIGDreamerëŠ” íŠ¹ê¶Œ ì •ë³´ í™œìš©ì„ í†µí•´ ì—ì´ì „íŠ¸ì˜ ì•ˆì „ì„±ê³¼ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ê¸°ì¡´ì˜ Safe RL ë°©ë²•ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. PIGDreamerëŠ” ë‹¤ë¥¸ íŠ¹ê¶Œ RL ë°©ë²•ì— ë¹„í•´ ì„±ëŠ¥, ê²¬ê³ ì„± ë° íš¨ìœ¨ì„±ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:26:49*