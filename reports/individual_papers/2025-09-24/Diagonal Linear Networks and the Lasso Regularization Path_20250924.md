<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:55:24.857950",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diagonal Linear Networks",
    "Lasso Regularization Path",
    "Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diagonal Linear Networks": 0.78,
    "Lasso Regularization Path": 0.81,
    "Neural Network": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diagonal Linear Networks",
        "canonical": "Diagonal Linear Networks",
        "aliases": [
          "Diagonal Networks",
          "DLN"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a specific type of neural network architecture with unique properties, crucial for understanding the paper's focus.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Lasso Regularization Path",
        "canonical": "Lasso Regularization Path",
        "aliases": [
          "Lasso Path",
          "Regularization Path"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper's analysis and connects to broader discussions on regularization techniques in machine learning.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.82,
        "link_intent_score": 0.81
      },
      {
        "surface": "Neural Networks",
        "canonical": "Neural Network",
        "aliases": [
          "Neural Nets"
        ],
        "category": "broad_technical",
        "rationale": "This is a fundamental concept in the paper, linking it to the broader field of deep learning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "training",
      "simulations",
      "monotonicity assumption"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diagonal Linear Networks",
      "resolved_canonical": "Diagonal Linear Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Lasso Regularization Path",
      "resolved_canonical": "Lasso Regularization Path",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.82,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Neural Networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Diagonal Linear Networks and the Lasso Regularization Path

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18766.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18766](https://arxiv.org/abs/2509.18766)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (81.0% similar)
- [[2025-09-22/Computing Linear Regions in Neural Networks with Skip Connections_20250922|Computing Linear Regions in Neural Networks with Skip Connections]] (80.5% similar)
- [[2025-09-23/Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks_20250923|Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks]] (80.4% similar)
- [[2025-09-17/Circuit realization and hardware linearization of monotone operator equilibrium networks_20250917|Circuit realization and hardware linearization of monotone operator equilibrium networks]] (80.4% similar)
- [[2025-09-22/Adversarial generalization of unfolding (model-based) networks_20250922|Adversarial generalization of unfolding (model-based) networks]] (80.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Lasso Regularization Path|Lasso Regularization Path]]
**âš¡ Unique Technical**: [[keywords/Diagonal Linear Networks|Diagonal Linear Networks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18766v1 Announce Type: new 
Abstract: Diagonal linear networks are neural networks with linear activation and diagonal weight matrices. Their theoretical interest is that their implicit regularization can be rigorously analyzed: from a small initialization, the training of diagonal linear networks converges to the linear predictor with minimal 1-norm among minimizers of the training loss. In this paper, we deepen this analysis showing that the full training trajectory of diagonal linear networks is closely related to the lasso regularization path. In this connection, the training time plays the role of an inverse regularization parameter. Both rigorous results and simulations are provided to illustrate this conclusion. Under a monotonicity assumption on the lasso regularization path, the connection is exact while in the general case, we show an approximate connection.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê°ì„  ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ ê²½ë¡œê°€ ë¼ì˜ ì •ê·œí™” ê²½ë¡œì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŒì„ ë¶„ì„í•©ë‹ˆë‹¤. ëŒ€ê°ì„  ì„ í˜• ë„¤íŠ¸ì›Œí¬ëŠ” ì„ í˜• í™œì„±í™” í•¨ìˆ˜ì™€ ëŒ€ê°ì„  ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ìœ¼ë¡œ, ì‘ì€ ì´ˆê¸°ê°’ì—ì„œ ì‹œì‘í•˜ì—¬ í›ˆë ¨ ì‹œ ìµœì†Œ 1-ë…¸ë¦„ì„ ê°–ëŠ” ì„ í˜• ì˜ˆì¸¡ê¸°ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” í›ˆë ¨ ì‹œê°„ì´ ì—­ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ ì—­í• ì„ í•œë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ë©°, ì—„ë°€í•œ ê²°ê³¼ì™€ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì´ë¥¼ ì…ì¦í•©ë‹ˆë‹¤. ë¼ì˜ ê²½ë¡œì˜ ë‹¨ì¡°ì„± ê°€ì • í•˜ì—ì„œëŠ” ì •í™•í•œ ì—°ê²°ì´, ì¼ë°˜ì ì¸ ê²½ìš°ì—ëŠ” ê·¼ì‚¬ì ì¸ ì—°ê²°ì´ ì„±ë¦½í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ê°ì„  ì„ í˜• ë„¤íŠ¸ì›Œí¬ëŠ” ì„ í˜• í™œì„±í™” í•¨ìˆ˜ì™€ ëŒ€ê°ì„  ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê°€ì§„ ì‹ ê²½ë§ì´ë‹¤.
- 2. ëŒ€ê°ì„  ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ì€ ìµœì†Œ 1-ë…¸ë¦„ì„ ê°€ì§„ ì„ í˜• ì˜ˆì¸¡ê¸°ë¡œ ìˆ˜ë ´í•œë‹¤.
- 3. ëŒ€ê°ì„  ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ ê²½ë¡œëŠ” ë¼ì˜ ì •ê·œí™” ê²½ë¡œì™€ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆë‹¤.
- 4. í›ˆë ¨ ì‹œê°„ì€ ì—­ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ì˜ ì—­í• ì„ í•œë‹¤.
- 5. ë¼ì˜ ì •ê·œí™” ê²½ë¡œì˜ ë‹¨ì¡°ì„± ê°€ì • í•˜ì— ëŒ€ê°ì„  ì„ í˜• ë„¤íŠ¸ì›Œí¬ì™€ì˜ ì—°ê²°ì€ ì •í™•í•˜ë‹¤.


---

*Generated on 2025-09-24 14:55:24*