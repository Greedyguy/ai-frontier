<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:14:30.337565",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Bilateral Control-Based Imitation Learning",
    "Vision-Language Model",
    "SigLIP",
    "FiLM-based Fusion",
    "Natural Language Processing"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Bilateral Control-Based Imitation Learning": 0.8,
    "Vision-Language Model": 0.85,
    "SigLIP": 0.75,
    "FiLM-based Fusion": 0.78,
    "Natural Language Processing": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Bilateral Control-Based Imitation Learning",
        "canonical": "Bilateral Control-Based Imitation Learning",
        "aliases": [
          "Bi-VLA"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel framework that extends existing bilateral control methods, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Vision-Language Fusion",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Fusion"
        ],
        "category": "evolved_concepts",
        "rationale": "Combining vision and language is a trending concept that enhances model versatility.",
        "novelty_score": 0.7,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "SigLIP",
        "canonical": "SigLIP",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "SigLIP is a specific technique mentioned for fusion, representing a unique technical element.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "FiLM-based Fusion",
        "canonical": "FiLM-based Fusion",
        "aliases": [
          "FiLM"
        ],
        "category": "unique_technical",
        "rationale": "FiLM is a specific method for feature fusion, crucial for understanding the paper's methodology.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Natural Language Instructions",
        "canonical": "Natural Language Processing",
        "aliases": [
          "Language Instructions"
        ],
        "category": "broad_technical",
        "rationale": "Incorporating language instructions is key to the paper's approach, linking it to broader NLP concepts.",
        "novelty_score": 0.6,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "task-specific models",
      "task success rates"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Bilateral Control-Based Imitation Learning",
      "resolved_canonical": "Bilateral Control-Based Imitation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Vision-Language Fusion",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "SigLIP",
      "resolved_canonical": "SigLIP",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "FiLM-based Fusion",
      "resolved_canonical": "FiLM-based Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Natural Language Instructions",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18865.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18865](https://arxiv.org/abs/2509.18865)

## 🔗 유사한 논문
- [[2025-09-19/CollabVLA_ Self-Reflective Vision-Language-Action Model Dreaming Together with Human_20250919|CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human]] (87.5% similar)
- [[2025-09-24/VLA-LPAF_ Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation_20250924|VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation]] (87.4% similar)
- [[2025-09-24/Pure Vision Language Action (VLA) Models_ A Comprehensive Survey_20250924|Pure Vision Language Action (VLA) Models: A Comprehensive Survey]] (86.8% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (86.0% similar)
- [[2025-09-19/ForceVLA_ Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation_20250919|ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (86.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**⚡ Unique Technical**: [[keywords/Bilateral Control-Based Imitation Learning|Bilateral Control-Based Imitation Learning]], [[keywords/SigLIP|SigLIP]], [[keywords/FiLM-based Fusion|FiLM-based Fusion]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18865v1 Announce Type: cross 
Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: https://mertcookimg.github.io/bi-vla/

## 📝 요약

Bi-VLA는 시각-언어 융합을 통한 행동 생성 모방 학습 프레임워크로, 기존의 양방향 제어 기반 모방 학습의 단일 작업 한계를 극복합니다. 이 방법은 로봇 관절 각도, 속도, 토크 데이터를 시각적 특징과 자연어 지시와 융합하여 다양한 작업을 하나의 모델로 처리할 수 있습니다. SigLIP 및 FiLM 기반 융합을 활용하여, 언어적 단서가 필요한 작업과 시각적으로 구별 가능한 작업에서 Bi-VLA의 성능을 검증했습니다. 실제 로봇 실험 결과, Bi-VLA는 시각-언어 조합을 성공적으로 해석하며, 기존 방법보다 작업 성공률을 향상시켰습니다. 이를 통해 Bi-VLA는 다중 작업 처리의 유연성을 입증했습니다.

## 🎯 주요 포인트

- 1. Bi-VLA는 양방향 제어 기반 모방 학습을 확장하여 단일 모델로 여러 작업을 처리할 수 있는 새로운 프레임워크입니다.
- 2. 기존 양방향 제어 방법은 작업별 모델이 필요하여 일반성이 제한되지만, Bi-VLA는 시각적 특징과 자연어 지시를 융합하여 이 한계를 극복합니다.
- 3. Bi-VLA는 시각 및 언어 결합을 성공적으로 해석하고, 기존 양방향 제어 기반 모방 학습에 비해 작업 성공률을 향상시킵니다.
- 4. 실험 결과, Bi-VLA는 실제 작업에서 효과적임을 입증하며, 시각과 언어의 결합이 다재다능성을 크게 향상시킵니다.


---

*Generated on 2025-09-24 15:14:30*