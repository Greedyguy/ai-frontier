<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:41:31.889999",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Models",
    "Token Merge with Attention",
    "Transformer",
    "Attention Mechanism",
    "GPU Efficiency"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Models": 0.85,
    "Token Merge with Attention": 0.88,
    "Transformer": 0.7,
    "Attention Mechanism": 0.82,
    "GPU Efficiency": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Models",
        "canonical": "Diffusion Models",
        "aliases": [
          "Diffusion Processes"
        ],
        "category": "specific_connectable",
        "rationale": "Diffusion models are central to the paper's focus on improving image generation efficiency.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.82,
        "link_intent_score": 0.85
      },
      {
        "surface": "Token Merge with Attention",
        "canonical": "Token Merge with Attention",
        "aliases": [
          "ToMA"
        ],
        "category": "unique_technical",
        "rationale": "This is the core method proposed in the paper, offering a novel approach to token reduction.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational technology discussed in the context of attention complexity.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial for understanding the efficiency improvements discussed.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "GPU Efficiency",
        "canonical": "GPU Efficiency",
        "aliases": [
          "GPU Optimization"
        ],
        "category": "unique_technical",
        "rationale": "The paper emphasizes GPU-aligned efficiency as a key advantage of the proposed method.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "sorting",
      "scattered writes",
      "overheads"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Models",
      "resolved_canonical": "Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.82,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Token Merge with Attention",
      "resolved_canonical": "Token Merge with Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "GPU Efficiency",
      "resolved_canonical": "GPU Efficiency",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# ToMA: Token Merge with Attention for Diffusion Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.10918.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.10918](https://arxiv.org/abs/2509.10918)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Optimizing Inference in Transformer-Based Models_ A Multi-Method Benchmark_20250923|Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark]] (84.4% similar)
- [[2025-09-22/MaskAttn-SDXL_ Controllable Region-Level Text-To-Image Generation_20250922|MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation]] (84.2% similar)
- [[2025-09-23/FG-Attn_ Leveraging Fine-Grained Sparsity In Diffusion Transformers_20250923|FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers]] (84.2% similar)
- [[2025-09-19/Fast Multipole Attention_ A Scalable Multilevel Attention Mechanism for Text and Images_20250919|Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images]] (82.8% similar)
- [[2025-09-23/Seg4Diff_ Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers_20250923|Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Diffusion Models|Diffusion Models]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Token Merge with Attention|Token Merge with Attention]], [[keywords/GPU Efficiency|GPU Efficiency]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.10918v2 Announce Type: replace-cross 
Abstract: Diffusion models excel in high-fidelity image generation but face scalability limits due to transformers' quadratic attention complexity. Plug-and-play token reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens in generated images but rely on GPU-inefficient operations (e.g., sorting, scattered writes), introducing overheads that negate theoretical speedups when paired with optimized attention implementations (e.g., FlashAttention). To bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf method that redesigns token reduction for GPU-aligned efficiency, with three key contributions: 1) a reformulation of token merge as a submodular optimization problem to select diverse tokens; 2) merge/unmerge as an attention-like linear transformation via GPU-friendly matrix operations; and 3) exploiting latent locality and sequential redundancy (pattern reuse) to minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%, respectively (with DINO $\Delta < 0.07$), outperforming prior methods. This work bridges the gap between theoretical and practical efficiency for transformers in diffusion.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³ í™”ì§ˆ ì´ë¯¸ì§€ ìƒì„±ì— ë›°ì–´ë‚œ í™•ì‚° ëª¨ë¸ì˜ í™•ì¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°©ë²•ë¡ ì¸ ToMA(Token Merge with Attention)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ToMAëŠ” GPU íš¨ìœ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ í† í° ë³‘í•©ì„ ì¬ì„¤ê³„í•œ ë°©ë²•ìœ¼ë¡œ, ì„¸ ê°€ì§€ ì£¼ìš” ê¸°ì—¬ë¥¼ í•©ë‹ˆë‹¤. ì²«ì§¸, ë‹¤ì–‘í•œ í† í°ì„ ì„ íƒí•˜ê¸° ìœ„í•´ í† í° ë³‘í•©ì„ ë¶€ë¶„ ëª¨ë“ˆ ìµœì í™” ë¬¸ì œë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤. ë‘˜ì§¸, GPU ì¹œí™”ì ì¸ í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ ë³‘í•©/í•´ì œë¥¼ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ìœ ì‚¬í•œ ì„ í˜• ë³€í™˜ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì…‹ì§¸, ì ì¬ì  ì§€ì—­ì„±ê³¼ ìˆœì°¨ì  ì¤‘ë³µì„±ì„ í™œìš©í•˜ì—¬ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤. ToMAëŠ” SDXL ë° Flux ìƒì„± ì§€ì—° ì‹œê°„ì„ ê°ê° 24% ë° 23% ì¤„ì´ë©°, ì´ë¡ ì  íš¨ìœ¨ì„±ê³¼ ì‹¤ì œ íš¨ìœ¨ì„± ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í™•ì‚° ëª¨ë¸ì˜ ê³ ì¶©ì‹¤ë„ ì´ë¯¸ì§€ ìƒì„±ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì´ì°¨ì  ì£¼ì˜ ë³µì¡ì„± ë•Œë¬¸ì— í™•ì¥ì„±ì— í•œê³„ê°€ ìˆë‹¤.
- 2. ToMeSDì™€ ToFu ê°™ì€ í† í° ê°ì†Œ ë°©ë²•ì€ ìƒì„±ëœ ì´ë¯¸ì§€ì—ì„œ ì¤‘ë³µ í† í°ì„ ë³‘í•©í•˜ì—¬ FLOPsë¥¼ ì¤„ì´ì§€ë§Œ, GPU ë¹„íš¨ìœ¨ì  ì‘ì—…ìœ¼ë¡œ ì¸í•´ ì´ë¡ ì  ì†ë„ í–¥ìƒì„ ìƒì‡„ì‹œí‚¨ë‹¤.
- 3. ToMAëŠ” GPUì— ë§ì¶˜ íš¨ìœ¨ì„±ì„ ìœ„í•´ í† í° ê°ì†Œë¥¼ ì¬ì„¤ê³„í•œ ë°©ë²•ìœ¼ë¡œ, ë‹¤ì–‘í•œ í† í° ì„ íƒì„ ìœ„í•œ ë¶€ë¶„ ëª¨ë“ˆ ìµœì í™” ë¬¸ì œë¡œì˜ ì¬êµ¬ì„±, GPU ì¹œí™”ì  í–‰ë ¬ ì—°ì‚°ì„ í†µí•œ ì£¼ì˜ ìœ ì‚¬ ì„ í˜• ë³€í™˜ìœ¼ë¡œì˜ ë³‘í•©/ë¶„ë¦¬, ì ì¬ì  ì§€ì—­ì„±ê³¼ ìˆœì°¨ì  ì¤‘ë³µì„±ì„ í™œìš©í•˜ì—¬ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•œë‹¤.
- 4. ToMAëŠ” SDXL/Flux ìƒì„± ì§€ì—°ì„ ê°ê° 24%/23% ì¤„ì´ë©°, ì´ì „ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” í™•ì‚° ëª¨ë¸ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì´ë¡ ì  íš¨ìœ¨ì„±ê³¼ ì‹¤ì œ íš¨ìœ¨ì„± ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¸ë‹¤.


---

*Generated on 2025-09-24 14:41:31*