<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:54:28.696835",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reward-Shifted Speculative Sampling",
    "Test-Time Alignment",
    "Reinforcement Learning with Human Feedback"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "Reward-Shifted Speculative Sampling": 0.9,
    "Test-Time Alignment": 0.85,
    "Reinforcement Learning with Human Feedback": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Essential for understanding the context of alignment and efficiency improvements discussed in the paper.",
        "novelty_score": 0.2,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reward-Shifted Speculative Sampling",
        "canonical": "Reward-Shifted Speculative Sampling",
        "aliases": [
          "SSS"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper's contribution, offering a novel approach to test-time alignment.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.9
      },
      {
        "surface": "Test-Time Alignment",
        "canonical": "Test-Time Alignment",
        "aliases": [
          "Inference-Time Alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept for understanding the efficiency and safety improvements in LLMs.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "RLHF Optimal Solution",
        "canonical": "Reinforcement Learning with Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "specific_connectable",
        "rationale": "Relevant for linking to broader discussions on aligning AI with human values.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "inference cost",
      "gold reward scores",
      "efficiency bottleneck"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reward-Shifted Speculative Sampling",
      "resolved_canonical": "Reward-Shifted Speculative Sampling",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Test-Time Alignment",
      "resolved_canonical": "Test-Time Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "RLHF Optimal Solution",
      "resolved_canonical": "Reinforcement Learning with Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2508.15044.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2508.15044](https://arxiv.org/abs/2508.15044)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/LifeAlign_ Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization_20250923|LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization]] (85.4% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (85.1% similar)
- [[2025-09-23/Post-hoc Reward Calibration_ A Case Study on Length Bias_20250923|Post-hoc Reward Calibration: A Case Study on Length Bias]] (84.6% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (83.3% similar)
- [[2025-09-24/DRO-REBEL_ Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment_20250924|DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Test-Time Alignment|Test-Time Alignment]], [[keywords/Reinforcement Learning with Human Feedback|Reinforcement Learning with Human Feedback]]
**âš¡ Unique Technical**: [[keywords/Reward-Shifted Speculative Sampling|Reward-Shifted Speculative Sampling]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2508.15044v3 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-shifted speculative sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¸ê°„ ì„ í˜¸ë„ì— ë§ì¶˜ ì •ë ¬ì€ ê°œë°œì˜ ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ëŠ” ì¶”ë¡  ì‹œ ì¶”ê°€ ê³„ì‚°ì„ í†µí•´ LLMì˜ ì•ˆì „ì„±ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í…ŒìŠ¤íŠ¸ ì‹œ ì •ë ¬ì— ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ì€ ë†’ì€ ì¶”ë¡  ë¹„ìš©ì„ ì´ˆë˜í•˜ì—¬ ì‹¤ìš©ì„±ì„ ì œí•œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‘ì€ ì´ˆì•ˆ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” íˆ¬ê¸°ì  ìƒ˜í”Œë§ ê°€ì†í™”ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼ ì¸ê°„ ì„ í˜¸ë„ì— ë§ì¶˜ ì´ˆì•ˆ ëª¨ë¸ê³¼ ë³€ê²½ë˜ì§€ ì•Šì€ ëª©í‘œ ëª¨ë¸ ê°„ì˜ ë¶„í¬ ë³€í™”ë¥¼ í™œìš©í•˜ëŠ” ë³´ìƒ ì´ë™ íˆ¬ê¸° ìƒ˜í”Œë§(SSS) ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ìš© ê¸°ì¤€ê³¼ ë³´ë„ˆìŠ¤ í† í° ë¶„í¬ë¥¼ ìˆ˜ì •í•˜ì—¬ RLHF ìµœì  ì†”ë£¨ì…˜ì„ ì‹¤ì œë¡œ ì–»ì§€ ì•Šê³ ë„ íšŒë³µí•  ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì€ í…ŒìŠ¤íŠ¸ ì‹œ ì•½í•œ-ê°•í•œ ì •ë ¬ ì‹¤í—˜ì—ì„œ ë‚®ì€ ì¶”ë¡  ë¹„ìš©ìœ¼ë¡œ ìš°ìˆ˜í•œ ë³´ìƒ ì ìˆ˜ë¥¼ ë‹¬ì„±í•˜ì—¬ ê·¸ íš¨ê³¼ì™€ íš¨ìœ¨ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ëŠ” ê²ƒì´ ê°œë°œì˜ ì¤‘ìš”í•œ ë‹¨ê³„ë¡œ ë¶€ê°ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 2. í…ŒìŠ¤íŠ¸ ì‹œì ì—ì„œì˜ ì •ë ¬ì€ LLMì˜ ì•ˆì „ì„±ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ê³„ì‚°ì„ í• ë‹¹í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
- 3. í…ŒìŠ¤íŠ¸ ì‹œì  ì •ë ¬ ê¸°ìˆ ì€ ë†’ì€ ì¶”ë¡  ë¹„ìš©ì„ ì´ˆë˜í•˜ì—¬ ì‹¤ìš©ì„±ì„ ì œí•œí•©ë‹ˆë‹¤.
- 4. ë³´ìƒ ì´ë™ ì¶”ì¸¡ ìƒ˜í”Œë§(SSS) ì•Œê³ ë¦¬ì¦˜ì€ ì´ˆì•ˆ ëª¨ë¸ì„ ì¸ê°„ì˜ ì„ í˜¸ì— ë§ì¶”ì–´ íš¨ìœ¨ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì€ í…ŒìŠ¤íŠ¸ ì‹œì  ì•½-ê°• ì •ë ¬ ì‹¤í—˜ì—ì„œ ë‚®ì€ ì¶”ë¡  ë¹„ìš©ìœ¼ë¡œ ìš°ìˆ˜í•œ ë³´ìƒ ì ìˆ˜ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:54:28*