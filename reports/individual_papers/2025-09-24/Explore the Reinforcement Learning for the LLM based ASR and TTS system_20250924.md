<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:54:42.567243",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning",
    "Automatic Speech Recognition",
    "Text-to-Speech",
    "Group Relative Policy Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning": 0.89,
    "Automatic Speech Recognition": 0.78,
    "Text-to-Speech": 0.77,
    "Group Relative Policy Optimization": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large language models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect well with various machine learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "specific_connectable",
        "rationale": "Reinforcement Learning is a key technique explored in the paper, providing strong links to optimization and learning strategies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.89
      },
      {
        "surface": "Automatic Speech Recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "unique_technical",
        "rationale": "Automatic Speech Recognition is a specific application area discussed, offering unique insights into audio processing.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "Text-to-Speech",
        "canonical": "Text-to-Speech",
        "aliases": [
          "TTS"
        ],
        "category": "unique_technical",
        "rationale": "Text-to-Speech systems are another focus of the study, relevant for linking to speech synthesis technologies.",
        "novelty_score": 0.68,
        "connectivity_score": 0.73,
        "specificity_score": 0.79,
        "link_intent_score": 0.77
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "GRPO is a specific optimization method evaluated in the study, providing a unique angle on policy optimization.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "Automatic Speech Recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Text-to-Speech",
      "resolved_canonical": "Text-to-Speech",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.73,
        "specificity": 0.79,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Explore the Reinforcement Learning for the LLM based ASR and TTS system

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18569.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18569](https://arxiv.org/abs/2509.18569)

## 🔗 유사한 논문
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (87.7% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (87.5% similar)
- [[2025-09-23/SoundMind_ RL-Incentivized Logic Reasoning for Audio-Language Models_20250923|SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models]] (85.7% similar)
- [[2025-09-19/Listening, Imagining \& Refining_ A Heuristic Optimized ASR Correction Framework with LLMs_20250919|Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs]] (85.3% similar)
- [[2025-09-23/Advancing Speech Understanding in Speech-Aware Language Models with GRPO_20250923|Advancing Speech Understanding in Speech-Aware Language Models with GRPO]] (85.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**⚡ Unique Technical**: [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]], [[keywords/Text-to-Speech|Text-to-Speech]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18569v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models. In this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction. For the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance. Our experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.

## 📝 요약

최근 대형 언어 모델(LLM)은 자동 음성 인식(ASR) 및 텍스트-음성 변환(TTS) 시스템에서 중요한 역할을 하고 있습니다. 그러나 강화 학습(RL)의 음성 기반 모델에 대한 적용은 복잡성 때문에 충분히 탐구되지 않았습니다. 본 연구에서는 음성 기반 LLM을 위한 경량 RL 프레임워크를 제안하여 ASR과 TTS 작업에서 RL의 효과를 평가했습니다. ASR 작업에서는 Group Relative Policy Optimization(GRPO) 프레임워크 내에서 다양한 규칙 기반 보상 함수를 실험하고, RL 데이터 구성의 영향을 조사했습니다. TTS 작업에서는 GRPO와 Differentiable Reward Optimization(DiffRO)을 비교하고, 두 접근법을 결합하여 성능을 향상시켰습니다. 실험 결과, 제한된 훈련 데이터와 적은 최적화 단계에서도 RL이 ASR과 TTS 시스템의 성능을 크게 향상시킬 수 있음을 보여주었습니다.

## 🎯 주요 포인트

- 1. 최근 대형 언어 모델(LLM)은 자동 음성 인식(ASR)과 텍스트-음성 변환(TTS) 시스템에서 중요한 역할을 하고 있다.
- 2. 본 연구에서는 오디오 기반 LLM을 위한 경량화된 강화 학습(RL) 프레임워크를 제안하여 오디오 입력과 출력을 처리할 수 있도록 하였다.
- 3. ASR 작업에서는 Group Relative Policy Optimization(GRPO) 프레임워크 내에서 다양한 규칙 기반 보상 함수를 실험하고 RL 데이터 구성의 영향을 조사하였다.
- 4. TTS 작업에서는 GRPO와 Differentiable Reward Optimization(DiffRO)을 비교하고 두 접근 방식을 결합하여 성능을 향상시켰다.
- 5. 실험 결과, 제한된 훈련 데이터와 적은 최적화 단계에서도 RL이 ASR 및 TTS 시스템의 성능을 크게 향상시킬 수 있음을 보여주었다.


---

*Generated on 2025-09-24 13:54:42*