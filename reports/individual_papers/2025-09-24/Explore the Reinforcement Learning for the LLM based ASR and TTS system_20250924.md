<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:54:42.567243",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning",
    "Automatic Speech Recognition",
    "Text-to-Speech",
    "Group Relative Policy Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning": 0.89,
    "Automatic Speech Recognition": 0.78,
    "Text-to-Speech": 0.77,
    "Group Relative Policy Optimization": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "large language models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect well with various machine learning concepts.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "specific_connectable",
        "rationale": "Reinforcement Learning is a key technique explored in the paper, providing strong links to optimization and learning strategies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.82,
        "specificity_score": 0.78,
        "link_intent_score": 0.89
      },
      {
        "surface": "Automatic Speech Recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "unique_technical",
        "rationale": "Automatic Speech Recognition is a specific application area discussed, offering unique insights into audio processing.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.81,
        "link_intent_score": 0.78
      },
      {
        "surface": "Text-to-Speech",
        "canonical": "Text-to-Speech",
        "aliases": [
          "TTS"
        ],
        "category": "unique_technical",
        "rationale": "Text-to-Speech systems are another focus of the study, relevant for linking to speech synthesis technologies.",
        "novelty_score": 0.68,
        "connectivity_score": 0.73,
        "specificity_score": 0.79,
        "link_intent_score": 0.77
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "GRPO is a specific optimization method evaluated in the study, providing a unique angle on policy optimization.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.82,
        "specificity": 0.78,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "Automatic Speech Recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.81,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Text-to-Speech",
      "resolved_canonical": "Text-to-Speech",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.73,
        "specificity": 0.79,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Explore the Reinforcement Learning for the LLM based ASR and TTS system

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18569.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18569](https://arxiv.org/abs/2509.18569)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (87.7% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (87.5% similar)
- [[2025-09-23/SoundMind_ RL-Incentivized Logic Reasoning for Audio-Language Models_20250923|SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models]] (85.7% similar)
- [[2025-09-19/Listening, Imagining \& Refining_ A Heuristic Optimized ASR Correction Framework with LLMs_20250919|Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs]] (85.3% similar)
- [[2025-09-23/Advancing Speech Understanding in Speech-Aware Language Models with GRPO_20250923|Advancing Speech Understanding in Speech-Aware Language Models with GRPO]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]], [[keywords/Text-to-Speech|Text-to-Speech]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18569v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models. In this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction. For the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance. Our experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìë™ ìŒì„± ì¸ì‹(ASR) ë° í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜(TTS) ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°•í™” í•™ìŠµ(RL)ì˜ ìŒì„± ê¸°ë°˜ ëª¨ë¸ì— ëŒ€í•œ ì ìš©ì€ ë³µì¡ì„± ë•Œë¬¸ì— ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ìŒì„± ê¸°ë°˜ LLMì„ ìœ„í•œ ê²½ëŸ‰ RL í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ ASRê³¼ TTS ì‘ì—…ì—ì„œ RLì˜ íš¨ê³¼ë¥¼ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ASR ì‘ì—…ì—ì„œëŠ” Group Relative Policy Optimization(GRPO) í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ ë‹¤ì–‘í•œ ê·œì¹™ ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì‹¤í—˜í•˜ê³ , RL ë°ì´í„° êµ¬ì„±ì˜ ì˜í–¥ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. TTS ì‘ì—…ì—ì„œëŠ” GRPOì™€ Differentiable Reward Optimization(DiffRO)ì„ ë¹„êµí•˜ê³ , ë‘ ì ‘ê·¼ë²•ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œí•œëœ í›ˆë ¨ ë°ì´í„°ì™€ ì ì€ ìµœì í™” ë‹¨ê³„ì—ì„œë„ RLì´ ASRê³¼ TTS ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìë™ ìŒì„± ì¸ì‹(ASR)ê³¼ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜(TTS) ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì˜¤ë””ì˜¤ ê¸°ë°˜ LLMì„ ìœ„í•œ ê²½ëŸ‰í™”ëœ ê°•í™” í•™ìŠµ(RL) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ ì˜¤ë””ì˜¤ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.
- 3. ASR ì‘ì—…ì—ì„œëŠ” Group Relative Policy Optimization(GRPO) í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ ë‹¤ì–‘í•œ ê·œì¹™ ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì‹¤í—˜í•˜ê³  RL ë°ì´í„° êµ¬ì„±ì˜ ì˜í–¥ì„ ì¡°ì‚¬í•˜ì˜€ë‹¤.
- 4. TTS ì‘ì—…ì—ì„œëŠ” GRPOì™€ Differentiable Reward Optimization(DiffRO)ì„ ë¹„êµí•˜ê³  ë‘ ì ‘ê·¼ ë°©ì‹ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œí•œëœ í›ˆë ¨ ë°ì´í„°ì™€ ì ì€ ìµœì í™” ë‹¨ê³„ì—ì„œë„ RLì´ ASR ë° TTS ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.


---

*Generated on 2025-09-24 13:54:42*