<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:39:40.430303",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transparent Earth Model",
    "Transformer",
    "Multimodal Learning",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transparent Earth Model": 0.78,
    "Transformer": 0.8,
    "Multimodal Learning": 0.82,
    "Attention Mechanism": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transparent Earth",
        "canonical": "Transparent Earth Model",
        "aliases": [
          "Transparent Earth Architecture"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel model specifically designed for subsurface property prediction, enhancing unique technical linkage.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "transformer-based architecture",
        "canonical": "Transformer",
        "aliases": [
          "transformer architecture"
        ],
        "category": "broad_technical",
        "rationale": "Connects to existing transformer-based models, facilitating links to a wide range of machine learning applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "multimodal approach"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the growing field of multimodal learning, which is crucial for integrating diverse data types.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "positional encodings",
        "canonical": "Attention Mechanism",
        "aliases": [
          "position encoding"
        ],
        "category": "specific_connectable",
        "rationale": "Enhances connectivity with models utilizing attention mechanisms, particularly in transformer architectures.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "subsurface properties",
      "heterogeneous datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transparent Earth",
      "resolved_canonical": "Transparent Earth Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "transformer-based architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "positional encodings",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.02783.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.02783](https://arxiv.org/abs/2509.02783)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/TESSERA_ Precomputed FAIR Global Pixel Embeddings for Earth Representation and Analysis_20250922|TESSERA: Precomputed FAIR Global Pixel Embeddings for Earth Representation and Analysis]] (84.0% similar)
- [[2025-09-22/The Moon's Many Faces_ A Single Unified Transformer for Multimodal Lunar Reconstruction_20250922|The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction]] (82.6% similar)
- [[2025-09-23/StefaLand_ An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions_20250923|StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions]] (81.7% similar)
- [[2025-09-17/Towards a Physics Foundation Model_20250917|Towards a Physics Foundation Model]] (81.1% similar)
- [[2025-09-23/EarthGPT-X_ A Spatial MLLM for Multi-level Multi-Source Remote Sensing Imagery Understanding with Visual Prompting_20250923|EarthGPT-X: A Spatial MLLM for Multi-level Multi-Source Remote Sensing Imagery Understanding with Visual Prompting]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Transparent Earth Model|Transparent Earth Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.02783v2 Announce Type: replace-cross 
Abstract: We present the Transparent Earth, a transformer-based architecture for reconstructing subsurface properties from heterogeneous datasets that vary in sparsity, resolution, and modality, where each modality represents a distinct type of observation (e.g., stress angle, mantle temperature, tectonic plate type). The model incorporates positional encodings of observations together with modality encodings, derived from a text embedding model applied to a description of each modality. This design enables the model to scale to an arbitrary number of modalities, making it straightforward to add new ones not considered in the initial design. We currently include eight modalities spanning directional angles, categorical classes, and continuous properties such as temperature and thickness. These capabilities support in-context learning, enabling the model to generate predictions either with no inputs or with an arbitrary number of additional observations from any subset of modalities. On validation data, this reduces errors in predicting stress angle by more than a factor of three. The proposed architecture is scalable and demonstrates improved performance with increased parameters. Together, these advances make the Transparent Earth an initial foundation model for the Earth's subsurface that ultimately aims to predict any subsurface property anywhere on Earth.

## ğŸ“ ìš”ì•½

ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ë°€ë„, í•´ìƒë„, ê´€ì¸¡ ìœ í˜•ì„ ê°€ì§„ ì´ì§ˆì ì¸ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° ì§€í•˜ íŠ¹ì„±ì„ ì¬êµ¬ì„±í•˜ëŠ” Transformer ê¸°ë°˜ ì•„í‚¤í…ì²˜ì¸ Transparent Earthë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê° ê´€ì¸¡ ìœ í˜•ì— ëŒ€í•œ ì„¤ëª…ì„ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œë¶€í„° ì–»ì€ ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©ê³¼ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ê²°í•©í•˜ì—¬, ì´ˆê¸° ì„¤ê³„ì— ê³ ë ¤ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ëª¨ë‹¬ë¦¬í‹°ë„ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë°©í–¥ ê°ë„, ë²”ì£¼í˜• í´ë˜ìŠ¤, ì˜¨ë„ ë° ë‘ê»˜ì™€ ê°™ì€ ì—°ì†ì  íŠ¹ì„±ì„ í¬í•¨í•œ 8ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ ì¶”ê°€ ê´€ì¸¡ ì—†ì´ë„ ì˜ˆì¸¡ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ê²€ì¦ ë°ì´í„°ì—ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ê°ë„ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ 3ë°° ì´ìƒ ì¤„ì˜€ìœ¼ë©°, ë§¤ê°œë³€ìˆ˜ ì¦ê°€ì— ë”°ë¼ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” í™•ì¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œì „ì€ ì§€êµ¬ì˜ ì§€í•˜ íŠ¹ì„±ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ì´ˆê¸° ê¸°ë°˜ ëª¨ë¸ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Transparent EarthëŠ” ë‹¤ì–‘í•œ í¬ì†Œì„±, í•´ìƒë„, ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê°€ì§„ ì´ì§ˆì ì¸ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° ì§€í•˜ íŠ¹ì„±ì„ ì¬êµ¬ì„±í•˜ê¸° ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.
- 2. ëª¨ë¸ì€ ê° ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ì„¤ëª…ì— ì ìš©ëœ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ì—ì„œ íŒŒìƒëœ ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©ê³¼ ê´€ì¸¡ì¹˜ì˜ ìœ„ì¹˜ ì¸ì½”ë”©ì„ í†µí•©í•©ë‹ˆë‹¤.
- 3. ì´ ì„¤ê³„ëŠ” ì„ì˜ì˜ ëª¨ë‹¬ë¦¬í‹° ìˆ˜ë¡œ í™•ì¥í•  ìˆ˜ ìˆì–´ ì´ˆê¸° ì„¤ê³„ì—ì„œ ê³ ë ¤ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì¶”ê°€í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤.
- 4. ëª¨ë¸ì€ ë°©í–¥ ê°ë„, ë²”ì£¼í˜• í´ë˜ìŠ¤, ì˜¨ë„ ë° ë‘ê»˜ì™€ ê°™ì€ ì—°ì†ì  íŠ¹ì„±ì„ í¬í•¨í•œ 8ê°œì˜ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í˜„ì¬ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- 5. ê²€ì¦ ë°ì´í„°ì—ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ê°ë„ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ 3ë°° ì´ìƒ ì¤„ì´ë©°, ì§€êµ¬ì˜ ì§€í•˜ íŠ¹ì„±ì„ ì–´ë””ì„œë“  ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ì´ˆê¸° ê¸°ë°˜ ëª¨ë¸ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:39:40*