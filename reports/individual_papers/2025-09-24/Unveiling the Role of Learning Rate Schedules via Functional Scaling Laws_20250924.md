<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:01:29.297709",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Learning Rate Schedule",
    "Functional Scaling Law",
    "Stochastic Gradient Descent"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Learning Rate Schedule": 0.78,
    "Functional Scaling Law": 0.8,
    "Stochastic Gradient Descent": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper, linking to existing knowledge on large-scale model training.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Learning Rate Schedule",
        "canonical": "Learning Rate Schedule",
        "aliases": [
          "LRS",
          "Learning Rate Schedules"
        ],
        "category": "unique_technical",
        "rationale": "Key focus of the study, providing insights into training dynamics.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Functional Scaling Law",
        "canonical": "Functional Scaling Law",
        "aliases": [
          "FSL"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept for understanding training processes.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Stochastic Gradient Descent",
        "canonical": "Stochastic Gradient Descent",
        "aliases": [
          "SGD"
        ],
        "category": "specific_connectable",
        "rationale": "Common optimization method, relevant for linking training methodologies.",
        "novelty_score": 0.2,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "scaling laws",
      "teacher-student kernel regression",
      "population risk"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Learning Rate Schedule",
      "resolved_canonical": "Learning Rate Schedule",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Functional Scaling Law",
      "resolved_canonical": "Functional Scaling Law",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Stochastic Gradient Descent",
      "resolved_canonical": "Stochastic Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.2,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19189.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19189](https://arxiv.org/abs/2509.19189)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Temporal Scaling Law for Large Language Models_20250923|Temporal Scaling Law for Large Language Models]] (90.0% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (83.6% similar)
- [[2025-09-23/Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling_20250923|Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling]] (83.3% similar)
- [[2025-09-23/Bayesian scaling laws for in-context learning_20250923|Bayesian scaling laws for in-context learning]] (83.1% similar)
- [[2025-09-22/Are LLMs Better Formalizers than Solvers on Complex Problems?_20250922|Are LLMs Better Formalizers than Solvers on Complex Problems?]] (82.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Stochastic Gradient Descent|Stochastic Gradient Descent]]
**âš¡ Unique Technical**: [[keywords/Learning Rate Schedule|Learning Rate Schedule]], [[keywords/Functional Scaling Law|Functional Scaling Law]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19189v1 Announce Type: new 
Abstract: Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì í•œë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ìµœì¢… ì†ì‹¤ì— ì§‘ì¤‘í–ˆìœ¼ë‚˜, ì´ ë…¼ë¬¸ì€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„(LRS)ì˜ ì˜í–¥ì„ í¬í•¨í•œ ì†ì‹¤ ë™íƒœë¥¼ ë¶„ì„í•œë‹¤. ì €ìë“¤ì€ ì˜¨ë¼ì¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì„ ì‚¬ìš©í•œ êµì‚¬-í•™ìƒ ì»¤ë„ íšŒê·€ ì„¤ì •ì„ í†µí•´, ì¸êµ¬ ìœ„í—˜ì˜ ë³€í™”ë¥¼ ì„¤ëª…í•˜ëŠ” ê¸°ëŠ¥ì  ìŠ¤ì¼€ì¼ë§ ë²•ì¹™(FSL)ì„ ì œì•ˆí•œë‹¤. FSLì€ LRSì˜ ì˜í–¥ì„ ëª…ì‹œì  ì»¨ë³¼ë£¨ì…˜ í˜•íƒœë¡œ ì„¤ëª…í•˜ì—¬, ì´ë“¤ì˜ íš¨ê³¼ë¥¼ ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ì„¸ ê°€ì§€ LRS(ìƒìˆ˜, ì§€ìˆ˜ì  ê°ì†Œ, ì›Œë°ì—…-ì•ˆì •-ê°ì†Œ)ë¥¼ ë¶„ì„í•˜ê³ , ì´ë¡ ì ìœ¼ë¡œ ëŒ€í˜• ëª¨ë¸ì˜ ë°ì´í„° ë° ê³„ì‚° íš¨ìœ¨ì„±, í•™ìŠµë¥  ê°ì†Œì˜ íš¨ìœ¨ì„±, WSD ìŠ¤ì¼€ì¤„ì˜ ìš°ìˆ˜ì„±ì„ ì„¤ëª…í•œë‹¤. FSLì€ LLM ì‚¬ì „ í›ˆë ¨ì˜ ì†ì‹¤ ê³¡ì„ ì„ ì˜ˆì¸¡í•˜ê³  ìµœì í™”í•˜ëŠ” ë° ìœ ìš©í•˜ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì—ì„œ ì‹¤í—˜ì„ í†µí•´ ê·¸ ì‹¤ìš©ì„±ì„ ì…ì¦í•œë‹¤. ì´ëŠ” LLM í›ˆë ¨ì˜ ì´í•´ë¥¼ ì‹¬í™”í•˜ê³  ëŒ€ê·œëª¨ ëª¨ë¸ í›ˆë ¨ ê°œì„ ì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ ì—°êµ¬ëŠ” ì£¼ë¡œ ìµœì¢… ì†ì‹¤ì— ì´ˆì ì„ ë§ì¶”ì—ˆìœ¼ë‚˜, ë³¸ ë…¼ë¬¸ì€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„(LRS)ì˜ ì˜í–¥ì„ í¬í•¨í•œ ì†ì‹¤ ë™ì—­í•™ì„ ì—°êµ¬í•œë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì˜¨ë¼ì¸ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì„ í†µí•œ êµì‚¬-í•™ìƒ ì»¤ë„ íšŒê·€ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬, í•™ìŠµ ê³¼ì • ì¤‘ ì¸êµ¬ ìœ„í—˜ì˜ ì§„í™”ë¥¼ ì„¤ëª…í•˜ëŠ” ê¸°ëŠ¥ì  ìŠ¤ì¼€ì¼ë§ ë²•ì¹™(FSL)ì„ ë„ì…í•œë‹¤.
- 3. FSLì€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì˜ ì˜í–¥ì„ ëª…ì‹œì ì¸ ì»¨ë³¼ë£¨ì…˜ í˜•íƒœì˜ ê¸°ëŠ¥ì  í•­ìœ¼ë¡œ í¬ì°©í•˜ì—¬ ê·¸ íš¨ê³¼ë¥¼ ì™„ì „íˆ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- 4. ì´ë¡ ì  ë¶„ì„ì„ í†µí•´ ë†’ì€ ìš©ëŸ‰ì˜ ëª¨ë¸ì´ ë°ì´í„° ë° ê³„ì‚° íš¨ìœ¨ì„±ì´ ë” ë†’ê³ , í•™ìŠµë¥  ê°ì†Œê°€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì„¤ëª…í•œë‹¤.
- 5. FSLì€ LLM ì‚¬ì „ í›ˆë ¨ì˜ ì†ì‹¤ ê³¡ì„ ì„ ë§ì¶”ê³  ì˜ˆì¸¡í•˜ë©° ìµœì í™”í•˜ëŠ” ëŒ€ì²´ ëª¨ë¸ë¡œì„œì˜ ì‹¤ìš©ì  ê´€ë ¨ì„±ì„ íƒêµ¬í•œë‹¤.


---

*Generated on 2025-09-24 15:01:29*