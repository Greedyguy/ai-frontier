<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:33:08.788044",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Phantora",
    "Hybrid Simulation",
    "GPU Cluster",
    "Containerized Environment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Phantora": 0.78,
    "Hybrid Simulation": 0.82,
    "GPU Cluster": 0.75,
    "Containerized Environment": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Phantora",
        "canonical": "Phantora",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Phantora is a novel hybrid GPU cluster simulator, providing a unique approach to ML performance estimation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "hybrid simulation",
        "canonical": "Hybrid Simulation",
        "aliases": [
          "hybrid GPU simulation"
        ],
        "category": "specific_connectable",
        "rationale": "Hybrid simulation offers a new method for performance estimation by reusing existing ML frameworks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "GPU cluster",
        "canonical": "GPU Cluster",
        "aliases": [
          "GPU server cluster"
        ],
        "category": "broad_technical",
        "rationale": "GPU clusters are essential for understanding the infrastructure used in ML training workloads.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.75
      },
      {
        "surface": "containerized environment",
        "canonical": "Containerized Environment",
        "aliases": [
          "container environment"
        ],
        "category": "specific_connectable",
        "rationale": "Containerized environments are critical for simulating distributed ML frameworks effectively.",
        "novelty_score": 0.68,
        "connectivity_score": 0.77,
        "specificity_score": 0.7,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "performance estimation",
      "ML training workloads"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Phantora",
      "resolved_canonical": "Phantora",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "hybrid simulation",
      "resolved_canonical": "Hybrid Simulation",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "GPU cluster",
      "resolved_canonical": "GPU Cluster",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "containerized environment",
      "resolved_canonical": "Containerized Environment",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.77,
        "specificity": 0.7,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Phantora: Maximizing Code Reuse in Simulation-based Machine Learning System Performance Estimation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2505.01616.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2505.01616](https://arxiv.org/abs/2505.01616)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state_20250923|Machine learning-driven conservative-to-primitive conversion in hybrid piecewise polytropic and tabulated equations of state]] (82.3% similar)
- [[2025-09-22/Characterizing the Efficiency of Distributed Training_ A Power, Performance, and Thermal Perspective_20250922|Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective]] (81.7% similar)
- [[2025-09-23/LightCode_ Compiling LLM Inference for Photonic-Electronic Systems_20250923|LightCode: Compiling LLM Inference for Photonic-Electronic Systems]] (81.2% similar)
- [[2025-09-23/Robust LLM Training Infrastructure at ByteDance_20250923|Robust LLM Training Infrastructure at ByteDance]] (81.1% similar)
- [[2025-09-18/Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/GPU Cluster|GPU Cluster]]
**ğŸ”— Specific Connectable**: [[keywords/Hybrid Simulation|Hybrid Simulation]], [[keywords/Containerized Environment|Containerized Environment]]
**âš¡ Unique Technical**: [[keywords/Phantora|Phantora]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.01616v2 Announce Type: replace-cross 
Abstract: Modern machine learning (ML) training workloads place substantial demands on both computational and communication resources. Consequently, accurate performance estimation has become increasingly critical for guiding system design decisions, such as the selection of parallelization strategies, cluster configurations, and hardware provisioning. Existing simulation-based performance estimation requires reimplementing the ML framework in a simulator, which demands significant manual effort and is hard to maintain as ML frameworks evolve rapidly.
  This paper introduces Phantora, a hybrid GPU cluster simulator designed for performance estimation of ML training workloads. Phantora executes unmodified ML frameworks as is within a distributed, containerized environment. Each container emulates the behavior of a GPU server in a large-scale cluster, while Phantora intercepts and simulates GPU- and communication-related operations to provide high-fidelity performance estimation. We call this approach hybrid simulation of ML systems, in contrast to traditional methods that simulate static workloads. The primary advantage of hybrid simulation is that it allows direct reuse of ML framework source code in simulation, avoiding the need for reimplementation. Our evaluation shows that Phantora provides accuracy comparable to static workload simulation while supporting three state-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora operates on a single GPU, eliminating the need for the resource-intensive trace collection and workload extraction steps required by traditional trace-based simulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.

## ğŸ“ ìš”ì•½

í˜„ëŒ€ ë¨¸ì‹ ëŸ¬ë‹(ML) í›ˆë ¨ ì‘ì—…ì€ ê³„ì‚° ë° í†µì‹  ìì›ì— ëŒ€í•œ ë†’ì€ ìš”êµ¬ë¥¼ ì œê¸°í•˜ë©°, ì´ì— ë”°ë¼ ì„±ëŠ¥ ì¶”ì •ì˜ ì¤‘ìš”ì„±ì´ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì„±ëŠ¥ ì¶”ì •ì€ ML í”„ë ˆì„ì›Œí¬ë¥¼ ì‹œë®¬ë ˆì´í„°ì— ì¬êµ¬í˜„í•´ì•¼ í•˜ë©°, ì´ëŠ” ë§ì€ ìˆ˜ì‘ì—…ê³¼ ìœ ì§€ë³´ìˆ˜ì˜ ì–´ë ¤ì›€ì„ ìˆ˜ë°˜í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ML í›ˆë ¨ ì‘ì—…ì˜ ì„±ëŠ¥ ì¶”ì •ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ GPU í´ëŸ¬ìŠ¤í„° ì‹œë®¬ë ˆì´í„°ì¸ Phantoraë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. PhantoraëŠ” ìˆ˜ì •ë˜ì§€ ì•Šì€ ML í”„ë ˆì„ì›Œí¬ë¥¼ ë¶„ì‚°ëœ ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ë©°, ê° ì»¨í…Œì´ë„ˆëŠ” ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„°ì˜ GPU ì„œë²„ ë™ì‘ì„ ì—ë®¬ë ˆì´íŠ¸í•©ë‹ˆë‹¤. PhantoraëŠ” GPU ë° í†µì‹  ê´€ë ¨ ì‘ì—…ì„ ê°€ë¡œì±„ê³  ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ë†’ì€ ì •í™•ë„ì˜ ì„±ëŠ¥ ì¶”ì •ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ì‹œë®¬ë ˆì´ì…˜ì€ ML í”„ë ˆì„ì›Œí¬ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì§ì ‘ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ì¬êµ¬í˜„ì˜ í•„ìš”ì„±ì„ ì—†ì• ëŠ” ê²ƒì´ ì£¼ìš” ì¥ì ì…ë‹ˆë‹¤. í‰ê°€ ê²°ê³¼, PhantoraëŠ” ì •ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ê³¼ ìœ ì‚¬í•œ ì •í™•ì„±ì„ ì œê³µí•˜ë©°, ìµœì‹  LLM í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ë˜í•œ, PhantoraëŠ” ë‹¨ì¼ GPUì—ì„œ ì‘ë™í•˜ì—¬ ì „í†µì ì¸ ì¶”ì  ê¸°ë°˜ ì‹œë®¬ë ˆì´í„°ê°€ ìš”êµ¬í•˜ëŠ” ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì ì¸ ë‹¨ê³„ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PhantoraëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. PhantoraëŠ” ML í›ˆë ¨ ì‘ì—…ì˜ ì„±ëŠ¥ ì¶”ì •ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ GPU í´ëŸ¬ìŠ¤í„° ì‹œë®¬ë ˆì´í„°ë¡œ, ìˆ˜ì •ë˜ì§€ ì•Šì€ ML í”„ë ˆì„ì›Œí¬ë¥¼ ë¶„ì‚°ëœ ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
- 2. í•˜ì´ë¸Œë¦¬ë“œ ì‹œë®¬ë ˆì´ì…˜ ì ‘ê·¼ë²•ì€ ML ì‹œìŠ¤í…œì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì§ì ‘ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ ì¬êµ¬í˜„ì˜ í•„ìš”ì„±ì„ ì—†ì•±ë‹ˆë‹¤.
- 3. PhantoraëŠ” ë‹¨ì¼ GPUì—ì„œ ì‘ë™í•˜ì—¬ ì „í†µì ì¸ ì¶”ì  ê¸°ë°˜ ì‹œë®¬ë ˆì´í„°ê°€ ìš”êµ¬í•˜ëŠ” ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì ì¸ ì¶”ì  ìˆ˜ì§‘ ë° ì‘ì—… ì¶”ì¶œ ë‹¨ê³„ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
- 4. PhantoraëŠ” ìµœì‹  LLM í›ˆë ¨ í”„ë ˆì„ì›Œí¬ ì„¸ ê°€ì§€ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•˜ë©°, ì •ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ê³¼ ìœ ì‚¬í•œ ì •í™•ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 5. PhantoraëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì œê³µë˜ë©°, ì„±ëŠ¥ ì¶”ì •ì„ ìœ„í•œ í˜ì‹ ì ì¸ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:33:08*