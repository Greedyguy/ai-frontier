<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:02:53.981013",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Chain-of-Thought",
    "Large Language Model",
    "Failed-Step Fraction",
    "Graph Neural Network"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Chain-of-Thought": 0.85,
    "Large Language Model": 0.8,
    "Failed-Step Fraction": 0.78,
    "Graph Neural Network": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT"
        ],
        "category": "unique_technical",
        "rationale": "Chain-of-Thought is central to the paper's exploration of reasoning processes in large models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Reasoning Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LRM"
        ],
        "category": "broad_technical",
        "rationale": "The study focuses on reasoning capabilities within large language models, a key area of machine learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Failed-Step Fraction",
        "canonical": "Failed-Step Fraction",
        "aliases": [
          "FSF"
        ],
        "category": "unique_technical",
        "rationale": "Failed-Step Fraction is a novel metric introduced in the paper to evaluate reasoning effectiveness.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Graph View of CoT",
        "canonical": "Graph Neural Network",
        "aliases": [
          "Graph View"
        ],
        "category": "specific_connectable",
        "rationale": "The paper uses a graph-based approach to analyze Chain-of-Thought, linking to graph neural networks.",
        "novelty_score": 0.65,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "lengthening",
      "review",
      "test-time scaling"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Reasoning Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Failed-Step Fraction",
      "resolved_canonical": "Failed-Step Fraction",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Graph View of CoT",
      "resolved_canonical": "Graph Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19284.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19284](https://arxiv.org/abs/2509.19284)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework]] (88.0% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (87.8% similar)
- [[2025-09-18/Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (87.7% similar)
- [[2025-09-24/Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints_20250924|Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints]] (87.5% similar)
- [[2025-09-22/ConCISE_ Confidence-guided Compression in Step-by-step Efficient Reasoning_20250922|ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning]] (84.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Network|Graph Neural Network]]
**âš¡ Unique Technical**: [[keywords/Chain-of-Thought|Chain-of-Thought]], [[keywords/Failed-Step Fraction|Failed-Step Fraction]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19284v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the "longer-is-better" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì¶”ë¡  ëª¨ë¸(LRMs)ì—ì„œ íš¨ê³¼ì ì¸ ì‚¬ê³  ê³¼ì •(CoT)ì˜ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” CoTì˜ ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ê²€í† ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ê³  í•˜ì§€ë§Œ, ìµœê·¼ ì—°êµ¬ëŠ” ì§§ì€ ì‚¬ê³  ê³¼ì •ì´ ë” ë‚˜ì„ ìˆ˜ ìˆë‹¤ê³  ì œì•ˆí•©ë‹ˆë‹¤. ì €ìë“¤ì€ ìˆ˜í•™ ë° ê³¼í•™ì  ì¶”ë¡ ì—ì„œ 10ê°œì˜ LRMì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼, CoTì˜ ê¸¸ì´ ì¦ê°€ì™€ ê²€í†  ì¦ê°€ê°€ ì˜¤íˆë ¤ ì •í™•ë„ë¥¼ ë‚®ì¶˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. CoTì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ ê·¸ë˜í”„ ë·°ë¥¼ ë„ì…í•˜ê³ , 'ì‹¤íŒ¨ ë‹¨ê³„ ë¹„ìœ¨(FSF)'ì´ë¼ëŠ” í†µê³„ë¥¼ ì œì•ˆí•˜ì—¬ ì •í™•ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ì‹¤íŒ¨í•œ ê°€ì§€ë¥¼ ì œê±°í•˜ì—¬ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì‹¤íŒ¨ê°€ ì ê³  êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ CoTê°€ ë” íš¨ê³¼ì ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸´ ì‚¬ê³ ì˜ íë¦„(Chain-of-Thought, CoT)ì´ í•­ìƒ íš¨ê³¼ì ì¸ ê²ƒì€ ì•„ë‹ˆë©°, ì§§ì€ ì‚¬ê³ ê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.
- 2. CoTì˜ ê¸¸ì´ ì—°ì¥ê³¼ ë¦¬ë·° ì¦ê°€ê°€ ì˜¤íˆë ¤ ì •í™•ë„ë¥¼ ë‚®ì¶œ ìˆ˜ ìˆë‹¤.
- 3. CoTì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ ê·¸ë˜í”„ ë·°ë¥¼ ë„ì…í•˜ê³ , ì‹¤íŒ¨ ë‹¨ê³„ ë¹„ìœ¨(Failed-Step Fraction, FSF)ì´ ì •í™•ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë°œê²¬í–ˆë‹¤.
- 4. ì‹¤íŒ¨í•œ ê°€ì§€ë¥¼ ì œê±°í•˜ëŠ” í¸ì§‘ì´ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ì´ëŠ” ì‹¤íŒ¨í•œ ê°€ì§€ê°€ ì´í›„ ì¶”ë¡ ì— í¸í–¥ì„ ì¤„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.
- 5. íš¨ê³¼ì ì¸ CoTëŠ” ì‹¤íŒ¨ê°€ ì ê³ , êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ í…ŒìŠ¤íŠ¸ ì‹œê°„ í™•ì¥ì„ ì§€ì›í•´ì•¼ í•œë‹¤.


---

*Generated on 2025-09-24 15:02:53*