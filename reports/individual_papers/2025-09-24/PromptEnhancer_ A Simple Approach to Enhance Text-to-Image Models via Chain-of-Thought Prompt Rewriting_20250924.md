<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:32:47.037134",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text-to-Image Diffusion Models",
    "Chain-of-Thought Rewriter",
    "AlignEvaluator",
    "Image-Text Alignment",
    "HunyuanImage 2.1 Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text-to-Image Diffusion Models": 0.78,
    "Chain-of-Thought Rewriter": 0.8,
    "AlignEvaluator": 0.82,
    "Image-Text Alignment": 0.77,
    "HunyuanImage 2.1 Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "text-to-image diffusion models",
        "canonical": "Text-to-Image Diffusion Models",
        "aliases": [
          "T2I diffusion models"
        ],
        "category": "specific_connectable",
        "rationale": "This term is central to the paper's focus on enhancing image generation, making it a key concept for linking.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Chain-of-Thought rewriter",
        "canonical": "Chain-of-Thought Rewriter",
        "aliases": [
          "CoT rewriter"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel mechanism for prompt rewriting, which is essential to the paper's contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "AlignEvaluator",
        "canonical": "AlignEvaluator",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique component of the proposed framework, crucial for understanding the feedback mechanism.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "image-text alignment",
        "canonical": "Image-Text Alignment",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "A key challenge addressed by the framework, relevant for linking with related works in multimodal learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.77
      },
      {
        "surface": "HunyuanImage 2.1 model",
        "canonical": "HunyuanImage 2.1 Model",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific model used for experiments, important for contextualizing results and comparisons.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "prompt",
      "method",
      "framework"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "text-to-image diffusion models",
      "resolved_canonical": "Text-to-Image Diffusion Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Chain-of-Thought rewriter",
      "resolved_canonical": "Chain-of-Thought Rewriter",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "AlignEvaluator",
      "resolved_canonical": "AlignEvaluator",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "image-text alignment",
      "resolved_canonical": "Image-Text Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "HunyuanImage 2.1 model",
      "resolved_canonical": "HunyuanImage 2.1 Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.04545.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.04545](https://arxiv.org/abs/2509.04545)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/AcT2I_ Evaluating and Improving Action Depiction in Text-to-Image Models_20250922|AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models]] (86.8% similar)
- [[2025-09-23/ComposeMe_ Attribute-Specific Image Prompts for Controllable Human Image Generation_20250923|ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation]] (85.2% similar)
- [[2025-09-18/Iterative Prompt Refinement for Safer Text-to-Image Generation_20250918|Iterative Prompt Refinement for Safer Text-to-Image Generation]] (85.2% similar)
- [[2025-09-24/Understanding-in-Generation_ Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation_20250924|Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation]] (84.8% similar)
- [[2025-09-22/RespoDiff_ Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation_20250922|RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Text-to-Image Diffusion Models|Text-to-Image Diffusion Models]], [[keywords/Image-Text Alignment|Image-Text Alignment]]
**âš¡ Unique Technical**: [[keywords/Chain-of-Thought Rewriter|Chain-of-Thought Rewriter]], [[keywords/AlignEvaluator|AlignEvaluator]], [[keywords/HunyuanImage 2.1 Model|HunyuanImage 2.1 Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.04545v5 Announce Type: replace 
Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated remarkable capabilities in generating high-fidelity images. However, these models often struggle to faithfully render complex user prompts, particularly in aspects like attribute binding, negation, and compositional relationships. This leads to a significant mismatch between user intent and the generated output. To address this challenge, we introduce PromptEnhancer, a novel and universal prompt rewriting framework that enhances any pretrained T2I model without requiring modifications to its weights. Unlike prior methods that rely on model-specific fine-tuning or implicit reward signals like image-reward scores, our framework decouples the rewriter from the generator. We achieve this by training a Chain-of-Thought (CoT) rewriter through reinforcement learning, guided by a dedicated reward model we term the AlignEvaluator. The AlignEvaluator is trained to provide explicit and fine-grained feedback based on a systematic taxonomy of 24 key points, which are derived from a comprehensive analysis of common T2I failure modes. By optimizing the CoT rewriter to maximize the reward from our AlignEvaluator, our framework learns to generate prompts that are more precisely interpreted by T2I models. Extensive experiments on the HunyuanImage 2.1 model demonstrate that PromptEnhancer significantly improves image-text alignment across a wide range of semantic and compositional challenges. Furthermore, we introduce a new, high-quality human preference benchmark to facilitate future research in this direction.

## ğŸ“ ìš”ì•½

ìµœê·¼ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜(T2I) í™•ì‚° ëª¨ë¸ì€ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ, ë³µì¡í•œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” PromptEnhancerë¼ëŠ” ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ì¬ì‘ì„± í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì‚¬ì „ í•™ìŠµëœ T2I ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ ë„ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ê³¼ ë‹¬ë¦¬, PromptEnhancerëŠ” ìƒì„±ê¸°ì™€ ì¬ì‘ì„±ê¸°ë¥¼ ë¶„ë¦¬í•˜ì—¬, ê°•í™” í•™ìŠµì„ í†µí•´ Chain-of-Thought(CoT) ì¬ì‘ì„±ê¸°ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. AlignEvaluatorë¼ëŠ” ë³´ìƒ ëª¨ë¸ì„ í†µí•´ 24ê°€ì§€ ì£¼ìš” ì‹¤íŒ¨ ëª¨ë“œì— ëŒ€í•œ ì²´ê³„ì ì¸ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë°€í•œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, T2I ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë” ì •í™•íˆ í•´ì„í•˜ë„ë¡ ìµœì í™”í•©ë‹ˆë‹¤. HunyuanImage 2.1 ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ PromptEnhancerëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì„ í¬ê²Œ ê°œì„ í–ˆìœ¼ë©°, í–¥í›„ ì—°êµ¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì¸ê°„ ì„ í˜¸ë„ ë²¤ì¹˜ë§ˆí¬ë„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœì‹  í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜ í™•ì‚° ëª¨ë¸ì€ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, ë³µì¡í•œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
- 2. PromptEnhancerëŠ” ì‚¬ì „ í•™ìŠµëœ T2I ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ ë„ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 3. ì´ í”„ë ˆì„ì›Œí¬ëŠ” Chain-of-Thought ì¬ì‘ì„±ê¸°ë¥¼ ê°•í™” í•™ìŠµìœ¼ë¡œ í›ˆë ¨í•˜ì—¬, AlignEvaluatorë¼ëŠ” ë³´ìƒ ëª¨ë¸ì„ í†µí•´ ëª…ì‹œì ì´ê³  ì„¸ë°€í•œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. PromptEnhancerëŠ” HunyuanImage 2.1 ëª¨ë¸ ì‹¤í—˜ì—ì„œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ì„ í¬ê²Œ ê°œì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
- 5. í–¥í›„ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ê³ í’ˆì§ˆ ì¸ê°„ ì„ í˜¸ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:32:47*