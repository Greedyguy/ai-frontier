<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:27:28.277844",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "EventVL",
    "Multimodal Learning",
    "Vision-Language Model",
    "Dynamic Semantic Alignment"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "EventVL": 0.8,
    "Multimodal Learning": 0.78,
    "Vision-Language Model": 0.82,
    "Dynamic Semantic Alignment": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "EventVL",
        "canonical": "EventVL",
        "aliases": [
          "Event Vision-Language Model"
        ],
        "category": "unique_technical",
        "rationale": "EventVL is a novel framework specifically designed for event-based multimodal understanding, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Multimodal Large Language Model",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLM"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Large Language Models are increasingly relevant for linking vision and language tasks, enhancing connectivity in multimodal research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Model",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a key concept in combining visual and textual data, facilitating cross-modal research.",
        "novelty_score": 0.6,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Dynamic Semantic Alignment",
        "canonical": "Dynamic Semantic Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is crucial for improving semantic spaces in event streams, providing a unique approach to semantic alignment.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "event-based",
      "scene description",
      "semantic understanding"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "EventVL",
      "resolved_canonical": "EventVL",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Multimodal Large Language Model",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Model",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Dynamic Semantic Alignment",
      "resolved_canonical": "Dynamic Semantic Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# EventVL: Understand Event Streams via Multimodal Large Language Model

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2501.13707.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2501.13707](https://arxiv.org/abs/2501.13707)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/UnifiedVisual_ A Framework for Constructing Unified Vision-Language Datasets_20250919|UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets]] (85.5% similar)
- [[2025-09-19/V-SEAM_ Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models_20250919|V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models]] (83.7% similar)
- [[2025-09-23/CLIP-IN_ Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions_20250923|CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions]] (83.6% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (83.0% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/EventVL|EventVL]], [[keywords/Dynamic Semantic Alignment|Dynamic Semantic Alignment]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2501.13707v2 Announce Type: replace-cross 
Abstract: The event-based Vision-Language Model (VLM) recently has made good progress for practical vision tasks. However, most of these works just utilize CLIP for focusing on traditional perception tasks, which obstruct model understanding explicitly the sufficient semantics and context from event streams. To address the deficiency, we propose EventVL, the first generative event-based MLLM (Multimodal Large Language Model) framework for explicit semantic understanding. Specifically, to bridge the data gap for connecting different modalities semantics, we first annotate a large event-image/video-text dataset, containing almost 1.4 million high-quality pairs of data, which enables effective learning across various scenes, e.g., drive scene or human motion. After that, we design Event Spatiotemporal Representation to fully explore the comprehensive information by diversely aggregating and segmenting the event stream. To further promote a compact semantic space, Dynamic Semantic Alignment is introduced to improve and complete sparse semantic spaces of events. Extensive experiments show that our EventVL can significantly surpass existing MLLM baselines in event captioning and scene description generation tasks. We hope our research could contribute to the development of the event vision community.

## ğŸ“ ìš”ì•½

EventVLì€ ìµœì´ˆì˜ ìƒì„±ì  ì´ë²¤íŠ¸ ê¸°ë°˜ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Model)ë¡œ, ëª…ì‹œì  ì˜ë¯¸ ì´í•´ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì¶©ë¶„í•œ ì˜ë¯¸ì™€ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë° í•œê³„ê°€ ìˆëŠ” CLIP ê¸°ë°˜ ëª¨ë¸ì˜ ë‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì•½ 140ë§Œ ê°œì˜ ê³ í’ˆì§ˆ ë°ì´í„° ìŒì„ í¬í•¨í•œ ëŒ€ê·œëª¨ ì´ë²¤íŠ¸-ì´ë¯¸ì§€/ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì£¼ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ì§‘ê³„í•˜ê³  ë¶„í• í•˜ì—¬ í¬ê´„ì ì¸ ì •ë³´ë¥¼ íƒìƒ‰í•˜ëŠ” ì´ë²¤íŠ¸ ì‹œê³µê°„ í‘œí˜„ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ë”ë¶ˆì–´, ë™ì  ì˜ë¯¸ ì •ë ¬ì„ í†µí•´ í¬ì†Œí•œ ì˜ë¯¸ ê³µê°„ì„ ê°œì„ í•˜ê³  ì™„ì„±í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, EventVLì€ ì´ë²¤íŠ¸ ìº¡ì…˜ ìƒì„± ë° ì¥ë©´ ì„¤ëª… ìƒì„± ì‘ì—…ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì´ë²¤íŠ¸ ë¹„ì „ ì»¤ë®¤ë‹ˆí‹°ì˜ ë°œì „ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. EventVLì€ ìµœì´ˆì˜ ìƒì„±ì  ì´ë²¤íŠ¸ ê¸°ë°˜ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Model) í”„ë ˆì„ì›Œí¬ë¡œ, ëª…ì‹œì  ì˜ë¯¸ ì´í•´ë¥¼ ëª©í‘œë¡œ í•œë‹¤.
- 2. ë‹¤ì–‘í•œ ì¥ë©´ì—ì„œ íš¨ê³¼ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì•½ 140ë§Œ ê°œì˜ ê³ í’ˆì§ˆ ë°ì´í„° ìŒì„ í¬í•¨í•œ ëŒ€ê·œëª¨ ì´ë²¤íŠ¸-ì´ë¯¸ì§€/ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì˜€ë‹¤.
- 3. ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì˜ í¬ê´„ì ì¸ ì •ë³´ë¥¼ íƒìƒ‰í•˜ê¸° ìœ„í•´ ì´ë²¤íŠ¸ ì‹œê³µê°„ í‘œí˜„(Event Spatiotemporal Representation)ì„ ì„¤ê³„í•˜ì˜€ë‹¤.
- 4. í¬ì†Œí•œ ì˜ë¯¸ ê³µê°„ì„ ê°œì„ í•˜ê³  ì™„ì„±í•˜ê¸° ìœ„í•´ ë™ì  ì˜ë¯¸ ì •ë ¬(Dynamic Semantic Alignment)ì„ ë„ì…í•˜ì˜€ë‹¤.
- 5. EventVLì€ ì´ë²¤íŠ¸ ìº¡ì…˜ ìƒì„± ë° ì¥ë©´ ì„¤ëª… ìƒì„± ì‘ì—…ì—ì„œ ê¸°ì¡´ì˜ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Multimodal Large Language Model) ê¸°ì¤€ì„ í¬ê²Œ ëŠ¥ê°€í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 14:27:28*