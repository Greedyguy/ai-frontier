<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:46:05.140467",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Prototype Embeddings",
    "Conceptual Spaces",
    "Explainable AI"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Prototype Embeddings": 0.78,
    "Conceptual Spaces": 0.82,
    "Explainable AI": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's methodology and connect to a wide range of AI topics.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Prototype Embeddings",
        "canonical": "Prototype Embeddings",
        "aliases": [
          "Prototype-based Embeddings"
        ],
        "category": "unique_technical",
        "rationale": "Prototype Embeddings are a novel approach introduced in the paper, crucial for linking conceptual spaces.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Conceptual Spaces",
        "canonical": "Conceptual Spaces",
        "aliases": [
          "Concept Spaces"
        ],
        "category": "unique_technical",
        "rationale": "Conceptual Spaces are a key theoretical framework in the paper, essential for understanding the proposed method.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Explainable AI",
        "canonical": "Explainable AI",
        "aliases": [
          "XAI"
        ],
        "category": "specific_connectable",
        "rationale": "Explainable AI is a significant application area for the research, providing a bridge to broader AI discussions.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "perceptual features",
      "cognitive science"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Prototype Embeddings",
      "resolved_canonical": "Prototype Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Conceptual Spaces",
      "resolved_canonical": "Conceptual Spaces",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Explainable AI",
      "resolved_canonical": "Explainable AI",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Extracting Conceptual Spaces from LLMs Using Prototype Embeddings

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19269.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.19269](https://arxiv.org/abs/2509.19269)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Capturing Polysemanticity with PRISM_ A Multi-Concept Feature Description Framework_20250922|Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework]] (83.8% similar)
- [[2025-09-23/LLMs as Layout Designers_ A Spatial Reasoning Perspective_20250923|LLMs as Layout Designers: A Spatial Reasoning Perspective]] (82.4% similar)
- [[2025-09-19/Large Multi-modal Models Can Interpret Features in Large Multi-modal Models_20250919|Large Multi-modal Models Can Interpret Features in Large Multi-modal Models]] (81.6% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (81.5% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (81.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Explainable AI|Explainable AI]]
**âš¡ Unique Technical**: [[keywords/Prototype Embeddings|Prototype Embeddings]], [[keywords/Conceptual Spaces|Conceptual Spaces]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19269v1 Announce Type: new 
Abstract: Conceptual spaces represent entities and concepts using cognitively meaningful dimensions, typically referring to perceptual features. Such representations are widely used in cognitive science and have the potential to serve as a cornerstone for explainable AI. Unfortunately, they have proven notoriously difficult to learn, although recent LLMs appear to capture the required perceptual features to a remarkable extent. Nonetheless, practical methods for extracting the corresponding conceptual spaces are currently still lacking. While various methods exist for extracting embeddings from LLMs, extracting conceptual spaces also requires us to encode the underlying features. In this paper, we propose a strategy in which features (e.g. sweetness) are encoded by embedding the description of a corresponding prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the LLM to align the prototype embeddings with the corresponding conceptual space dimensions. Our empirical analysis finds this approach to be highly effective.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°œë…ì  ê³µê°„ì„ ì„¤ëª… ê°€ëŠ¥í•œ AIì˜ ê¸°ì´ˆë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤. ê°œë…ì  ê³µê°„ì€ ì¸ì§€ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ì°¨ì›ì„ ì‚¬ìš©í•˜ì—¬ ê°œì²´ì™€ ê°œë…ì„ í‘œí˜„í•˜ì§€ë§Œ, í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì´ëŸ¬í•œ ì§€ê°ì  íŠ¹ì§•ì„ ì˜ í¬ì°©í•˜ì§€ë§Œ, ì‹¤ì œë¡œ ê°œë…ì  ê³µê°„ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” íŠ¹ì§•(ì˜ˆ: ë‹¨ë§›)ì„ ì„¤ëª…í•˜ëŠ” í”„ë¡œí† íƒ€ì…(ì˜ˆ: ë§¤ìš° ë‹¨ ìŒì‹)ì˜ ì„¤ëª…ì„ ì„ë² ë”©í•˜ì—¬ ì¸ì½”ë”©í•˜ëŠ” ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ë˜í•œ, í”„ë¡œí† íƒ€ì… ì„ë² ë”©ì„ ê°œë…ì  ê³µê°„ ì°¨ì›ê³¼ ì •ë ¬í•˜ê¸° ìœ„í•´ LLMì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. ì‹¤ì¦ ë¶„ì„ ê²°ê³¼, ì´ ì ‘ê·¼ ë°©ì‹ì´ ë§¤ìš° íš¨ê³¼ì ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°œë…ì  ê³µê°„ì€ ì¸ì§€ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ì°¨ì›ì„ ì‚¬ìš©í•˜ì—¬ ê°œì²´ì™€ ê°œë…ì„ í‘œí˜„í•˜ë©°, ì´ëŠ” ì£¼ë¡œ ì§€ê°ì  íŠ¹ì§•ì„ ì°¸ì¡°í•œë‹¤.
- 2. ìµœê·¼ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìš”êµ¬ë˜ëŠ” ì§€ê°ì  íŠ¹ì§•ì„ ìƒë‹¹í•œ ì •ë„ë¡œ í¬ì°©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
- 3. ê°œë…ì  ê³µê°„ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹¤ì§ˆì ì¸ ë°©ë²•ì€ ì•„ì§ ë¶€ì¡±í•˜ë‹¤.
- 4. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í”„ë¡œí† íƒ€ì…ì˜ ì„¤ëª…ì„ ì„ë² ë”©í•˜ì—¬ íŠ¹ì§•ì„ ì¸ì½”ë”©í•˜ëŠ” ì „ëµì„ ì œì•ˆí•œë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ í”„ë¡œí† íƒ€ì… ì„ë² ë”©ì„ ê°œë…ì  ê³µê°„ ì°¨ì›ê³¼ ì •ë ¬í•˜ë„ë¡ LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ íš¨ê³¼ì ì„ì„ ì‹¤ì¦ì ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 15:46:05*