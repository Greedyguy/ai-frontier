<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:16:59.258069",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning on Pre-Training Data",
    "Large Language Model",
    "Reinforcement Learning from Human Feedback",
    "Reinforcement Learning with Verifiable Rewards"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning on Pre-Training Data": 0.92,
    "Large Language Model": 0.85,
    "Reinforcement Learning from Human Feedback": 0.8,
    "Reinforcement Learning with Verifiable Rewards": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning on Pre-Training data",
        "canonical": "Reinforcement Learning on Pre-Training Data",
        "aliases": [
          "RLPT"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel training paradigm that extends reinforcement learning to pre-training data, offering unique insights and connections.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.92
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion, linking to a broad range of topics in language model research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning from Human Feedback",
        "canonical": "Reinforcement Learning from Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "specific_connectable",
        "rationale": "A well-known strategy in reinforcement learning, providing a point of comparison for the novel RLPT approach.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reinforcement Learning with Verifiable Rewards",
        "canonical": "Reinforcement Learning with Verifiable Rewards",
        "aliases": [
          "RLVR"
        ],
        "category": "specific_connectable",
        "rationale": "Another established method in reinforcement learning, relevant for contrasting with RLPT's approach.",
        "novelty_score": 0.45,
        "connectivity_score": 0.7,
        "specificity_score": 0.72,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning on Pre-Training data",
      "resolved_canonical": "Reinforcement Learning on Pre-Training Data",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning from Human Feedback",
      "resolved_canonical": "Reinforcement Learning from Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reinforcement Learning with Verifiable Rewards",
      "resolved_canonical": "Reinforcement Learning with Verifiable Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.7,
        "specificity": 0.72,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Reinforcement Learning on Pre-Training Data

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19249.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.19249](https://arxiv.org/abs/2509.19249)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (90.2% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (87.6% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (87.5% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (87.3% similar)
- [[2025-09-23/EvoCoT_ Overcoming the Exploration Bottleneck in Reinforcement Learning_20250923|EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning]] (86.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning from Human Feedback|Reinforcement Learning from Human Feedback]], [[keywords/Reinforcement Learning with Verifiable Rewards|Reinforcement Learning with Verifiable Rewards]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning on Pre-Training Data|Reinforcement Learning on Pre-Training Data]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19249v1 Announce Type: cross 
Abstract: The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ RLPT(Reinforcement Learning on Pre-Training data)ë¼ëŠ” ìƒˆë¡œìš´ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì§€ë„ í•™ìŠµ ì¤‘ì‹¬ì˜ í™•ì¥ ë°©ë²•ê³¼ ë‹¬ë¦¬, RLPTëŠ” ê°•í™” í•™ìŠµ(RL)ì„ í†µí•´ ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì—ì„œ ë³´ìƒ ì‹ í˜¸ë¥¼ ì§ì ‘ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ì´ ììœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ì¸ê°„ ì£¼ì„ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ì´ì „ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ëª©í‘œë¥¼ ì„¤ì •í•˜ì—¬ ë” ì¼ë°˜í™”ëœ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°œë°œí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, RLPTëŠ” Qwen3-4B-Base ëª¨ë¸ì—ì„œ MMLU, GPQA-Diamond ë“± ì—¬ëŸ¬ í‰ê°€ ì§€í‘œì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” RLPTê°€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™•ì¥í•˜ê³ , ë” ë§ì€ ê³„ì‚° ìì›ì„ í™œìš©í•  ê²½ìš° ì¶”ê°€ì ì¸ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RLPTëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìµœì í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í›ˆë ¨ ìŠ¤ì¼€ì¼ë§ íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ, ê°•í™” í•™ìŠµì„ í†µí•´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ë¥¼ íƒìƒ‰í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ RL ì „ëµê³¼ ë‹¬ë¦¬ RLPTëŠ” ë³´ìƒ ì‹ í˜¸ë¥¼ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì§ì ‘ ë„ì¶œí•˜ì—¬ ì¸ê°„ ì£¼ì„ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì œê±°í•©ë‹ˆë‹¤.
- 3. RLPTëŠ” ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ë¡  ëª©í‘œë¥¼ ì±„íƒí•˜ì—¬ ì´ì „ ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ í›„ì† í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ì •ì±…ì— ë³´ìƒì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ ì¼ë°˜ ë„ë©”ì¸ ë° ìˆ˜í•™ì  ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ RLPTì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. RLPTëŠ” LLMì˜ ì¶”ë¡  ê²½ê³„ë¥¼ í™•ì¥í•˜ê³  RLVR ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ë” ë§ì€ ì»´í“¨íŒ… ìì›ì„ í†µí•´ ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì˜ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:16:59*