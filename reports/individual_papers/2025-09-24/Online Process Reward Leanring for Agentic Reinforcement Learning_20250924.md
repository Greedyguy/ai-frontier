<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:45:31.534684",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning",
    "Online Process Reward Learning",
    "Process Reward Model",
    "Trajectory Preferences"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning": 0.8,
    "Online Process Reward Learning": 0.9,
    "Process Reward Model": 0.75,
    "Trajectory Preferences": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and are a key component in many AI systems, providing strong connectivity.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a fundamental concept in the paper, crucial for understanding the proposed method.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "Online Process Reward Learning",
        "canonical": "Online Process Reward Learning",
        "aliases": [
          "OPRL"
        ],
        "category": "unique_technical",
        "rationale": "This is the novel method introduced in the paper, essential for understanding the contribution.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.9
      },
      {
        "surface": "Process Reward Model",
        "canonical": "Process Reward Model",
        "aliases": [
          "PRM"
        ],
        "category": "unique_technical",
        "rationale": "The Process Reward Model is a key component of the proposed strategy, enabling deeper insights into the methodology.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Trajectory Preferences",
        "canonical": "Trajectory Preferences",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Trajectory Preferences are integral to the reward learning process, offering a unique angle for exploration.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "autonomous agents",
      "interactive environments",
      "temporal credit assignment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Online Process Reward Learning",
      "resolved_canonical": "Online Process Reward Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Process Reward Model",
      "resolved_canonical": "Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Trajectory Preferences",
      "resolved_canonical": "Trajectory Preferences",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Online Process Reward Leanring for Agentic Reinforcement Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19199.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.19199](https://arxiv.org/abs/2509.19199)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (86.4% similar)
- [[2025-09-17/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250917|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (85.7% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (85.1% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.1% similar)
- [[2025-09-18/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250918|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Trajectory Preferences|Trajectory Preferences]]
**âš¡ Unique Technical**: [[keywords/Online Process Reward Learning|Online Process Reward Learning]], [[keywords/Process Reward Model|Process Reward Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19199v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL)ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•™ìŠµì—ì„œ ë°œìƒí•˜ëŠ” ë³´ìƒ í• ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì˜¨ë¼ì¸ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ í•™ìŠµ(OPRL)ì„ ì œì•ˆí•©ë‹ˆë‹¤. OPRLì€ ëª…ì‹œì ì¸ ë‹¨ê³„ ë ˆì´ë¸” ì—†ì´ í‘œì¤€ ì •ì±… ì•Œê³ ë¦¬ì¦˜ê³¼ í†µí•©ë˜ì–´ ê¶¤ì  ê¸°ë°˜ ëª©í‘œë¥¼ í†µí•´ ì•”ë¬µì ì¸ ë‹¨ê³„ ë³´ìƒì„ ìµœì í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¶¤ì  ì„ í˜¸ë„ë¥¼ ì¼ê´€ë˜ê²Œ ë°˜ì˜í•˜ë©°, ì ì¬ì  ê¸°ë°˜ì˜ ë³´ìƒìœ¼ë¡œ ì‘ìš©í•˜ì—¬ í›ˆë ¨ì„ ì•ˆì •í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, OPRLì€ ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë†’ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ë‚®ì€ ë¶„ì‚°ìœ¼ë¡œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì—ì´ì „íŠ¸ í•™ìŠµì— ëŒ€í•œ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì ì  ë” ê°•í™” í•™ìŠµ(RL)ì„ í†µí•´ ì¥ê¸°ì ì¸ ìƒí˜¸ì‘ìš© í™˜ê²½ì—ì„œ ììœ¨ ì—ì´ì „íŠ¸ë¡œ í›ˆë ¨ë˜ê³  ìˆë‹¤.
- 2. í¬ì†Œí•˜ê³  ë•Œë¡œëŠ” ê²€ì¦ ë¶ˆê°€ëŠ¥í•œ ë³´ìƒìœ¼ë¡œ ì¸í•´ ì‹œê°„ì  ì‹ ìš© í• ë‹¹ì´ ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œë¡œ ë‚¨ì•„ ìˆë‹¤.
- 3. Online Process Reward Learning(OPRL)ì€ ì—ì´ì „íŠ¸ RLì„ ìœ„í•œ ì¼ë°˜ì ì¸ ì‹ ìš© í• ë‹¹ ì „ëµìœ¼ë¡œ, ì¶”ê°€ì ì¸ ë¡¤ì•„ì›ƒì´ë‚˜ ëª…ì‹œì ì¸ ë‹¨ê³„ ë ˆì´ë¸” ì—†ì´ í‘œì¤€ ì˜¨-ì •ì±… ì•Œê³ ë¦¬ì¦˜ê³¼ ì›í™œí•˜ê²Œ í†µí•©ëœë‹¤.
- 4. OPRLì€ ê²½ë¡œ ê¸°ë°˜ DPO ëª©í‘œë¥¼ í†µí•´ ê²½ë¡œ ì„ í˜¸ë„ë¥¼ ì•”ë¬µì ì¸ ë‹¨ê³„ ë³´ìƒìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì •ì±… ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ìê¸° ê°•í™” ë£¨í”„ë¥¼ ìƒì„±í•œë‹¤.
- 5. OPRLì€ ì„¸ ê°€ì§€ ì—ì´ì „íŠ¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë” ë†’ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ë‚®ì€ ë¶„ì‚°ì„ í†µí•´ í›ˆë ¨ ì¤‘ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-24 15:45:31*