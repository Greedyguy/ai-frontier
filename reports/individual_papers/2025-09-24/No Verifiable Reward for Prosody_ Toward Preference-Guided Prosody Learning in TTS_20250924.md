<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:52:40.786588",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Text-to-Speech",
    "Prosody",
    "Direct Preference Optimization",
    "Group Relative Policy Optimization",
    "Human Preference Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Text-to-Speech": 0.78,
    "Prosody": 0.77,
    "Direct Preference Optimization": 0.8,
    "Group Relative Policy Optimization": 0.78,
    "Human Preference Optimization": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "text-to-speech",
        "canonical": "Text-to-Speech",
        "aliases": [
          "TTS"
        ],
        "category": "broad_technical",
        "rationale": "Text-to-Speech is a foundational technology in the paper, linking it to broader discussions in speech synthesis.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "prosody",
        "canonical": "Prosody",
        "aliases": [
          "intonation",
          "speech rhythm"
        ],
        "category": "unique_technical",
        "rationale": "Prosody is a key focus of the paper, crucial for linking discussions on speech naturalness.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Direct Preference Optimization",
        "canonical": "Direct Preference Optimization",
        "aliases": [
          "DPO"
        ],
        "category": "unique_technical",
        "rationale": "Direct Preference Optimization is a novel method introduced in the paper, central to its findings.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Group Relative Policy Optimization",
        "canonical": "Group Relative Policy Optimization",
        "aliases": [
          "GRPO"
        ],
        "category": "unique_technical",
        "rationale": "Group Relative Policy Optimization is a comparative method discussed, relevant for linking optimization strategies.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "human preference optimization",
        "canonical": "Human Preference Optimization",
        "aliases": [
          "preference-guided learning"
        ],
        "category": "evolved_concepts",
        "rationale": "Human Preference Optimization is an evolved concept in the paper, highlighting the role of human feedback in learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "text-to-speech",
      "resolved_canonical": "Text-to-Speech",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "prosody",
      "resolved_canonical": "Prosody",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Direct Preference Optimization",
      "resolved_canonical": "Direct Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Group Relative Policy Optimization",
      "resolved_canonical": "Group Relative Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "human preference optimization",
      "resolved_canonical": "Human Preference Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18531.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18531](https://arxiv.org/abs/2509.18531)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/TempFlow-GRPO_ When Timing Matters for GRPO in Flow Models_20250923|TempFlow-GRPO: When Timing Matters for GRPO in Flow Models]] (82.8% similar)
- [[2025-09-23/Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization_20250923|Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization]] (82.7% similar)
- [[2025-09-23/Advancing Speech Understanding in Speech-Aware Language Models with GRPO_20250923|Advancing Speech Understanding in Speech-Aware Language Models with GRPO]] (82.5% similar)
- [[2025-09-23/Preference Distillation via Value based Reinforcement Learning_20250923|Preference Distillation via Value based Reinforcement Learning]] (82.3% similar)
- [[2025-09-17/TGPO_ Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning_20250917|TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (80.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Text-to-Speech|Text-to-Speech]]
**âš¡ Unique Technical**: [[keywords/Prosody|Prosody]], [[keywords/Direct Preference Optimization|Direct Preference Optimization]], [[keywords/Group Relative Policy Optimization|Group Relative Policy Optimization]]
**ğŸš€ Evolved Concepts**: [[keywords/Human Preference Optimization|Human Preference Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18531v1 Announce Type: cross 
Abstract: Recent work reports gains in neural text-to-speech (TTS) with Group Relative Policy Optimization (GRPO). However, in the absence of a verifiable reward for \textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL) lowers error rates yet collapses prosody into monotone, unnatural speech; adding speaker-similarity further destabilizes training and degrades CER. We address this with an \textit{iterative Direct Preference Optimization (DPO)} scheme that uses only a few hundred human-labeled preference pairs per round to directly optimize prosodic naturalness while regularizing to the current model. On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center interactions capturing task-oriented dialogues, our method attains the highest human preference (ELO) with competitive CER, outperforming GRPO and strong commercial baselines. These results suggest that when prosody cannot be rewarded automatically, \textit{human preference optimization} offers a practical and data-efficient path to natural and robust TTS. The demo page is available at \href{https://tts.ch.dev}

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹ ê²½ë§ ê¸°ë°˜ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜(TTS)ì—ì„œ Group Relative Policy Optimization(GRPO)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°©ë²•ë¡ ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ GRPOëŠ” ì „ì‚¬ ì§€í–¥ ì‹ í˜¸(CER/NLL)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¥˜ìœ¨ì„ ë‚®ì¶”ì§€ë§Œ, ì–µì–‘ì´ ë‹¨ì¡°ë¡­ê³  ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„±ì„ ìƒì„±í•˜ëŠ” ë¬¸ì œì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ì¸ê°„ì´ ë¼ë²¨ë§í•œ ì„ í˜¸ ìŒì„ í™œìš©í•˜ì—¬ ì–µì–‘ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì„ ì§ì ‘ ìµœì í™”í•˜ëŠ” ë°˜ë³µì  Direct Preference Optimization(DPO) ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ í•œêµ­ ì½œì„¼í„° ëŒ€í™” ë°ì´í„°ì…‹(KoCC-TTS)ì—ì„œ ë†’ì€ ì¸ê°„ ì„ í˜¸ë„(ELO)ì™€ ê²½ìŸë ¥ ìˆëŠ” CERì„ ê¸°ë¡í•˜ë©°, GRPOì™€ ìƒìš© ê¸°ì¤€ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ìë™ìœ¼ë¡œ ì–µì–‘ì„ ë³´ìƒí•  ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ ì¸ê°„ ì„ í˜¸ ìµœì í™”ê°€ ìì—°ìŠ¤ëŸ½ê³  ê°•ë ¥í•œ TTSë¥¼ ìœ„í•œ ì‹¤ìš©ì ì´ê³  ë°ì´í„° íš¨ìœ¨ì ì¸ ì ‘ê·¼ë²•ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ GRPO ê¸°ë°˜ TTS ëª¨ë¸ì€ ì˜¤ë¥˜ìœ¨ì„ ë‚®ì¶”ì§€ë§Œ ë‹¨ì¡°ë¡­ê³  ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì–µì–‘ì„ ìƒì„±í•˜ëŠ” í•œê³„ë¥¼ ë³´ì…ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ì¸ê°„ì´ ë¼ë²¨ë§í•œ ì„ í˜¸ ìŒì„ í™œìš©í•œ ë°˜ë³µì  Direct Preference Optimization(DPO) ë°©ì‹ì„ ì œì•ˆí•˜ì—¬ ì–µì–‘ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì„ ì§ì ‘ ìµœì í™”í•©ë‹ˆë‹¤.
- 3. KoCC-TTS ë°ì´í„°ì…‹ì„ í™œìš©í•œ ì‹¤í—˜ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ GRPO ë° ìƒìš© ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì¸ê°„ ì„ í˜¸ë„(ELO)ì™€ ê²½ìŸë ¥ ìˆëŠ” CERì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 4. ìë™ìœ¼ë¡œ ì–µì–‘ì„ ë³´ìƒí•  ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ ì¸ê°„ ì„ í˜¸ ìµœì í™”ëŠ” ìì—°ìŠ¤ëŸ½ê³  ê²¬ê³ í•œ TTSë¥¼ ìœ„í•œ ì‹¤ìš©ì ì´ê³  ë°ì´í„° íš¨ìœ¨ì ì¸ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:52:40*