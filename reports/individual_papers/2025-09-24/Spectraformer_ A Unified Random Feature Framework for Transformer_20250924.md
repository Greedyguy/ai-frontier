<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:20:58.588653",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Attention Mechanism",
    "Random Feature Framework",
    "Kernel Approximation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.9,
    "Attention Mechanism": 0.88,
    "Random Feature Framework": 0.82,
    "Kernel Approximation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a fundamental architecture in machine learning, providing a strong link to various applications and research areas.",
        "novelty_score": 0.3,
        "connectivity_score": 0.95,
        "specificity_score": 0.65,
        "link_intent_score": 0.9
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial for understanding the inner workings of Transformers, facilitating connections to other models and techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "Random Feature Framework",
        "canonical": "Random Feature Framework",
        "aliases": [
          "Random Feature Method"
        ],
        "category": "unique_technical",
        "rationale": "This framework represents a novel approach to approximating kernel functions in Transformers, offering unique insights into model efficiency.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Kernel Approximation",
        "canonical": "Kernel Approximation",
        "aliases": [
          "Kernel Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Kernel approximation is a key technique in the paper, linking to broader discussions on efficient computation in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.95,
        "specificity": 0.65,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Random Feature Framework",
      "resolved_canonical": "Random Feature Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Kernel Approximation",
      "resolved_canonical": "Kernel Approximation",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Spectraformer: A Unified Random Feature Framework for Transformer

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2405.15310.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2405.15310](https://arxiv.org/abs/2405.15310)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Inceptive Transformers_ Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages_20250923|Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages]] (83.3% similar)
- [[2025-09-23/Conv-like Scale-Fusion Time Series Transformer_ A Multi-Scale Representation for Variable-Length Long Time Series_20250923|Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series]] (82.6% similar)
- [[2025-09-22/AttentionDrop_ A Novel Regularization Method for Transformer Models_20250922|AttentionDrop: A Novel Regularization Method for Transformer Models]] (82.6% similar)
- [[2025-09-22/Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs_20250922|Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs]] (81.6% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Kernel Approximation|Kernel Approximation]]
**âš¡ Unique Technical**: [[keywords/Random Feature Framework|Random Feature Framework]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2405.15310v5 Announce Type: replace 
Abstract: Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” Transformerì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ì»¤ë„ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ê³  í•™ìŠµí•˜ê¸° ìœ„í•œ í†µí•© í”„ë ˆì„ì›Œí¬ì¸ Spectraformerë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì´ ë¬´ì‘ìœ„ íŠ¹ì§• íŒ¨ëŸ¬ë‹¤ì„ ë‚´ì—ì„œ êµ¬ì„± í•¨ìˆ˜ì™€ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ì¡°í•©ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ ë‹¬ë¦¬, ì´ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì¡°í•©ì˜ ì²´ê³„ì ì¸ ë¹„êµ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. SpectraformerëŠ” Long Range Arena ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœìƒìœ„ ì„±ëŠ¥ì˜ í¬ì†Œ ë° ì €ë­í¬ ë°©ë²•ê³¼ ê²¬ì¤„ ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ë¬´ì‘ìœ„ íŠ¹ì§• ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ ì²˜ìŒìœ¼ë¡œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì •í™•ë„, í•™ìŠµ ì‹œê°„, ë©”ëª¨ë¦¬ ì†Œë¹„ ì¸¡ë©´ì—ì„œ ë‹¤ì–‘í•œ ì´ì ì„ ì œê³µí•˜ëŠ” ì—¬ëŸ¬ ë³€í˜•ì„ ìƒì„±í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì–‘í•œ ì»¤ë„ ê·¼ì‚¬ ë° ì»¤ë„ í•™ìŠµ ê¸°ë²•ì„ ì‚¬ìš©í•œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì„ í˜•í™”ê°€ ìœ ë§í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 2. Spectraformerë¼ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ Transformerì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ì»¤ë„ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. ëœë¤ í”¼ì²˜ ê¸°ë°˜ ì ‘ê·¼ë²•ì´ Long Range Arena ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœìƒìœ„ í¬ì†Œ ë° ì €ë­í¬ ë°©ë²•ê³¼ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë‹¤ëŠ” ê²ƒì„ ì²˜ìŒìœ¼ë¡œ ì‹¤ì¦í•©ë‹ˆë‹¤.
- 4. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì •í™•ë„, í•™ìŠµ ì‹œê°„, ë©”ëª¨ë¦¬ ì†Œë¹„ì—ì„œ ë‹¤ì–‘í•œ ì´ì ì„ ì œê³µí•˜ëŠ” ì—¬ëŸ¬ ë³€í˜•ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 5. Spectraformerì˜ ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìœ¼ë©°, ì—°êµ¬ìë“¤ì´ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:20:58*