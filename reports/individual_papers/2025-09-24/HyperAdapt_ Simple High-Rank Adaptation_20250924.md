<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:58:28.819806",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "HyperAdapt",
    "Parameter-efficient fine-tuning",
    "Large Language Model",
    "High-rank update",
    "GLUE benchmarks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "HyperAdapt": 0.8,
    "Parameter-efficient fine-tuning": 0.78,
    "Large Language Model": 0.7,
    "High-rank update": 0.72,
    "GLUE benchmarks": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "HyperAdapt",
        "canonical": "HyperAdapt",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "HyperAdapt is a novel parameter-efficient fine-tuning method introduced in the paper, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Parameter-efficient fine-tuning",
        "canonical": "Parameter-efficient fine-tuning",
        "aliases": [
          "PEFT"
        ],
        "category": "specific_connectable",
        "rationale": "Parameter-efficient fine-tuning is central to the paper's methodology and connects to broader trends in model adaptation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Foundation models",
        "canonical": "Large Language Model",
        "aliases": [
          "Foundation models"
        ],
        "category": "broad_technical",
        "rationale": "Foundation models are a key concept in machine learning, with strong links to large language models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "High-rank update",
        "canonical": "High-rank update",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The concept of high-rank updates is crucial for understanding the technical innovation of HyperAdapt.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "GLUE benchmarks",
        "canonical": "GLUE benchmarks",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "GLUE benchmarks are widely used for evaluating language models, providing strong connectivity to related research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "fine-tuning",
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "HyperAdapt",
      "resolved_canonical": "HyperAdapt",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Parameter-efficient fine-tuning",
      "resolved_canonical": "Parameter-efficient fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Foundation models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "High-rank update",
      "resolved_canonical": "High-rank update",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "GLUE benchmarks",
      "resolved_canonical": "GLUE benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# HyperAdapt: Simple High-Rank Adaptation

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18629.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18629](https://arxiv.org/abs/2509.18629)

## 🔗 유사한 논문
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (88.7% similar)
- [[2025-09-19/Don't Forget the Nonlinearity_ Unlocking Activation Functions in Efficient Fine-Tuning_20250919|Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning]] (84.2% similar)
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM: Hierarchical Adapter Merging for Scalable Continual Learning]] (84.2% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (83.4% similar)
- [[2025-09-23/QWHA_ Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models_20250923|QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models]] (83.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Parameter-efficient fine-tuning|Parameter-efficient fine-tuning]], [[keywords/GLUE benchmarks|GLUE benchmarks]]
**⚡ Unique Technical**: [[keywords/HyperAdapt|HyperAdapt]], [[keywords/High-rank update|High-rank update]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18629v1 Announce Type: cross 
Abstract: Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.

## 📝 요약

이 논문은 대규모 모델을 특정 응용 분야에 맞게 조정하는 데 필요한 메모리와 계산 비용을 줄이기 위한 HyperAdapt라는 새로운 파라미터 효율적 미세 조정 방법을 제안합니다. HyperAdapt는 사전 학습된 가중치 행렬에 행과 열 방향으로 대각 행렬을 적용하여 고차원의 업데이트를 유도하며, $n \times m$ 행렬에 대해 $n+m$개의 파라미터만을 학습합니다. 이 방법은 기존의 최첨단 방법들보다 학습 파라미터 수를 크게 줄이면서도, 이론적으로 업데이트의 상한을 설정하고 실험적으로 고차원 변환을 일관되게 유도함을 확인했습니다. GLUE, 산술 추론, 상식 추론 벤치마크 실험에서 HyperAdapt는 14B 파라미터 모델까지 전체 미세 조정 및 기존 PEFT 방법과 유사한 성능을 보이며, 훨씬 적은 학습 파라미터를 사용합니다.

## 🎯 주요 포인트

- 1. HyperAdapt는 파라미터 효율적 미세 조정 방법으로, LoRA 등 최신 방법들에 비해 훈련 가능한 파라미터 수를 크게 줄입니다.
- 2. HyperAdapt는 사전 훈련된 가중치 행렬에 대각 행렬을 통한 행 및 열 스케일링을 적용하여 고랭크 업데이트를 유도합니다.
- 3. HyperAdapt는 $n \times m$ 행렬에 대해 $n+m$개의 훈련 가능한 파라미터만 필요로 하며, 이론적으로 업데이트의 랭크 상한을 설정합니다.
- 4. GLUE, 산술 추론, 상식 추론 벤치마크 실험에서 HyperAdapt는 전체 미세 조정 및 최신 PEFT 방법과 비슷하거나 거의 비슷한 성능을 보이면서도 훨씬 적은 훈련 가능한 파라미터를 사용합니다.


---

*Generated on 2025-09-24 13:58:28*