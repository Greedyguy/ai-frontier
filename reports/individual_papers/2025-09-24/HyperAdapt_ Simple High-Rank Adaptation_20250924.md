<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:58:28.819806",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "HyperAdapt",
    "Parameter-efficient fine-tuning",
    "Large Language Model",
    "High-rank update",
    "GLUE benchmarks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "HyperAdapt": 0.8,
    "Parameter-efficient fine-tuning": 0.78,
    "Large Language Model": 0.7,
    "High-rank update": 0.72,
    "GLUE benchmarks": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "HyperAdapt",
        "canonical": "HyperAdapt",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "HyperAdapt is a novel parameter-efficient fine-tuning method introduced in the paper, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Parameter-efficient fine-tuning",
        "canonical": "Parameter-efficient fine-tuning",
        "aliases": [
          "PEFT"
        ],
        "category": "specific_connectable",
        "rationale": "Parameter-efficient fine-tuning is central to the paper's methodology and connects to broader trends in model adaptation.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Foundation models",
        "canonical": "Large Language Model",
        "aliases": [
          "Foundation models"
        ],
        "category": "broad_technical",
        "rationale": "Foundation models are a key concept in machine learning, with strong links to large language models.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "High-rank update",
        "canonical": "High-rank update",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The concept of high-rank updates is crucial for understanding the technical innovation of HyperAdapt.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.72
      },
      {
        "surface": "GLUE benchmarks",
        "canonical": "GLUE benchmarks",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "GLUE benchmarks are widely used for evaluating language models, providing strong connectivity to related research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "fine-tuning",
      "performance",
      "method"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "HyperAdapt",
      "resolved_canonical": "HyperAdapt",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Parameter-efficient fine-tuning",
      "resolved_canonical": "Parameter-efficient fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Foundation models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "High-rank update",
      "resolved_canonical": "High-rank update",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "GLUE benchmarks",
      "resolved_canonical": "GLUE benchmarks",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# HyperAdapt: Simple High-Rank Adaptation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18629.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18629](https://arxiv.org/abs/2509.18629)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (88.7% similar)
- [[2025-09-19/Don't Forget the Nonlinearity_ Unlocking Activation Functions in Efficient Fine-Tuning_20250919|Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning]] (84.2% similar)
- [[2025-09-18/HAM_ Hierarchical Adapter Merging for Scalable Continual Learning_20250918|HAM: Hierarchical Adapter Merging for Scalable Continual Learning]] (84.2% similar)
- [[2025-09-22/Not All Parameters Are Created Equal_ Smart Isolation Boosts Fine-Tuning Performance_20250922|Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance]] (83.4% similar)
- [[2025-09-23/QWHA_ Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models_20250923|QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Parameter-efficient fine-tuning|Parameter-efficient fine-tuning]], [[keywords/GLUE benchmarks|GLUE benchmarks]]
**âš¡ Unique Technical**: [[keywords/HyperAdapt|HyperAdapt]], [[keywords/High-rank update|High-rank update]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18629v1 Announce Type: cross 
Abstract: Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ëª¨ë¸ì„ íŠ¹ì • ì‘ìš© ë¶„ì•¼ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ë° í•„ìš”í•œ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•œ HyperAdaptë¼ëŠ” ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. HyperAdaptëŠ” ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ í–‰ë ¬ì— í–‰ê³¼ ì—´ ë°©í–¥ìœ¼ë¡œ ëŒ€ê° í–‰ë ¬ì„ ì ìš©í•˜ì—¬ ê³ ì°¨ì›ì˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ ë„í•˜ë©°, $n \times m$ í–‰ë ¬ì— ëŒ€í•´ $n+m$ê°œì˜ íŒŒë¼ë¯¸í„°ë§Œì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ë³´ë‹¤ í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì´ë©´ì„œë„, ì´ë¡ ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ì˜ ìƒí•œì„ ì„¤ì •í•˜ê³  ì‹¤í—˜ì ìœ¼ë¡œ ê³ ì°¨ì› ë³€í™˜ì„ ì¼ê´€ë˜ê²Œ ìœ ë„í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. GLUE, ì‚°ìˆ  ì¶”ë¡ , ìƒì‹ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ HyperAdaptëŠ” 14B íŒŒë¼ë¯¸í„° ëª¨ë¸ê¹Œì§€ ì „ì²´ ë¯¸ì„¸ ì¡°ì • ë° ê¸°ì¡´ PEFT ë°©ë²•ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, í›¨ì”¬ ì ì€ í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. HyperAdaptëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì • ë°©ë²•ìœ¼ë¡œ, LoRA ë“± ìµœì‹  ë°©ë²•ë“¤ì— ë¹„í•´ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.
- 2. HyperAdaptëŠ” ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ëŒ€ê° í–‰ë ¬ì„ í†µí•œ í–‰ ë° ì—´ ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•˜ì—¬ ê³ ë­í¬ ì—…ë°ì´íŠ¸ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
- 3. HyperAdaptëŠ” $n \times m$ í–‰ë ¬ì— ëŒ€í•´ $n+m$ê°œì˜ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ í•„ìš”ë¡œ í•˜ë©°, ì´ë¡ ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ì˜ ë­í¬ ìƒí•œì„ ì„¤ì •í•©ë‹ˆë‹¤.
- 4. GLUE, ì‚°ìˆ  ì¶”ë¡ , ìƒì‹ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ HyperAdaptëŠ” ì „ì²´ ë¯¸ì„¸ ì¡°ì • ë° ìµœì‹  PEFT ë°©ë²•ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œë„ í›¨ì”¬ ì ì€ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:58:28*