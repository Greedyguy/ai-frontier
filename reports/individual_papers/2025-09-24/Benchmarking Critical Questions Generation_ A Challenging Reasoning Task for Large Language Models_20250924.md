<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:50:54.275182",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Critical Questions Generation",
    "Large Language Model",
    "Zero-Shot Learning",
    "Automated Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Critical Questions Generation": 0.8,
    "Large Language Model": 0.85,
    "Zero-Shot Learning": 0.78,
    "Automated Reasoning": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Critical Questions Generation",
        "canonical": "Critical Questions Generation",
        "aliases": [
          "CQs-Gen"
        ],
        "category": "unique_technical",
        "rationale": "This is the central task of the paper, offering a unique approach to enhancing critical thinking in AI systems.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are crucial for understanding the paper's context and benchmarking efforts.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Zero-Shot Evaluation",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot evaluation is a trending method that highlights the paper's innovative evaluation approach.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Automated Reasoning",
        "canonical": "Automated Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Automated reasoning is a key application area for the techniques discussed in the paper.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "dataset",
      "evaluation methods",
      "human judgments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Critical Questions Generation",
      "resolved_canonical": "Critical Questions Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Zero-Shot Evaluation",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Automated Reasoning",
      "resolved_canonical": "Automated Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.11341.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2505.11341](https://arxiv.org/abs/2505.11341)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/EquiBench_ Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking_20250923|EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking]] (85.7% similar)
- [[2025-09-23/Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning_20250923|Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning]] (85.3% similar)
- [[2025-09-24/CCQA_ Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs_20250924|CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs]] (85.1% similar)
- [[2025-09-24/CogniLoad_ A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density_20250924|CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density]] (84.6% similar)
- [[2025-09-23/ESGenius_ Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge_20250923|ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge]] (84.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Automated Reasoning|Automated Reasoning]]
**âš¡ Unique Technical**: [[keywords/Critical Questions Generation|Critical Questions Generation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.11341v3 Announce Type: replace 
Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose underlying assumptions and challenge the validity of argumentative reasoning structures. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This paper presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale dataset including ~5K manually annotated questions. We also investigate automatic evaluation methods and propose reference-based techniques as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data and code plus a public leaderboard are provided to encourage further research, not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„íŒì  ì‚¬ê³ ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•œ 'ë¹„íŒì  ì§ˆë¬¸ ìƒì„±(CQs-Gen)' ê³¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” ì•½ 5ì²œ ê°œì˜ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì£¼ì„ëœ ì§ˆë¬¸ì„ í¬í•¨í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ êµ¬ì¶•ê³¼ ìë™ í‰ê°€ ë°©ë²•ì˜ íƒêµ¬ê°€ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì¸ê°„ì˜ íŒë‹¨ê³¼ ê°€ì¥ ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ì°¸ì¡° ê¸°ë°˜ í‰ê°€ ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. 11ê°œì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ëŒ€í•œ ì œë¡œìƒ· í‰ê°€ë¥¼ í†µí•´ ê°•ë ¥í•œ ê¸°ì¤€ì„ ì„ ì„¤ì •í•˜ë©°, ì´ ê³¼ì œì˜ ì–´ë ¤ì›€ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë°ì´í„°ì™€ ì½”ë“œ, ê³µê°œ ë¦¬ë”ë³´ë“œë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ë¿ë§Œ ì•„ë‹ˆë¼ ìë™í™”ëœ ì¶”ë¡ ê³¼ ì¸ê°„ì˜ ë¹„íŒì  ì‚¬ê³ ì— ëŒ€í•œ ì‹¤ì§ˆì  ì´ì ì„ íƒêµ¬í•˜ëŠ” ì¶”ê°€ ì—°êµ¬ë¥¼ ì¥ë ¤í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„íŒì  ì§ˆë¬¸ ìƒì„±(CQs-Gen)ì€ ì‹œìŠ¤í…œì´ ìˆ¨ê²¨ì§„ ê°€ì •ì„ ë“œëŸ¬ë‚´ê³  ë…¼ì¦ êµ¬ì¡°ì˜ íƒ€ë‹¹ì„±ì„ ë„ì „í•˜ëŠ” ì§ˆë¬¸ì„ ìƒì„±í•˜ë„ë¡ í•˜ì—¬ ë¹„íŒì  ì‚¬ê³ ë¥¼ ì´‰ì§„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
- 2. ì ì ˆí•œ ë°ì´í„°ì…‹ê³¼ ìë™ í‰ê°€ ê¸°ì¤€ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì´ ë¶„ì•¼ì˜ ë°œì „ì´ ì €í•´ë˜ê³  ìˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì€ ì•½ 5ì²œ ê°œì˜ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì£¼ì„ì´ ë‹¬ë¦° ì§ˆë¬¸ì„ í¬í•¨í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ CQs-Gen ì‹œìŠ¤í…œ ê°œë°œê³¼ ë²¤ì¹˜ë§ˆí‚¹ì„ ì§€ì›í•œë‹¤.
- 4. ìë™ í‰ê°€ ë°©ë²•ì„ ì¡°ì‚¬í•˜ê³  ì¸ê°„ íŒë‹¨ê³¼ ê°€ì¥ ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ì°¸ì¡° ê¸°ë°˜ ê¸°ë²•ì„ ì œì•ˆí•œë‹¤.
- 5. 11ê°œì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ëŒ€í•œ ì œë¡œìƒ· í‰ê°€ë¥¼ í†µí•´ ê°•ë ¥í•œ ê¸°ì¤€ì„ ì„¤ì •í•˜ê³ , ë°ì´í„°ì™€ ì½”ë“œ, ê³µê°œ ë¦¬ë”ë³´ë“œë¥¼ ì œê³µí•˜ì—¬ ì¶”ê°€ ì—°êµ¬ë¥¼ ì¥ë ¤í•œë‹¤.


---

*Generated on 2025-09-24 15:50:54*