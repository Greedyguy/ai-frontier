<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:15:20.865936",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Sa2VA",
    "Sa2VA-i",
    "Referring Video Object Segmentation",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Sa2VA": 0.8,
    "Sa2VA-i": 0.85,
    "Referring Video Object Segmentation": 0.78,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Sa2VA",
        "canonical": "Sa2VA",
        "aliases": [
          "Sa2VA model"
        ],
        "category": "unique_technical",
        "rationale": "Sa2VA is a unique model central to the paper's contributions and improvements.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Sa2VA-i",
        "canonical": "Sa2VA-i",
        "aliases": [
          "Improved Sa2VA"
        ],
        "category": "unique_technical",
        "rationale": "Sa2VA-i represents the improved version of the Sa2VA model, a key focus of the paper.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "referring video object segmentation",
        "canonical": "Referring Video Object Segmentation",
        "aliases": [
          "video object segmentation"
        ],
        "category": "specific_connectable",
        "rationale": "This is a specific task that the improved model targets, linking it to related works in video segmentation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language models are relevant to the paper's focus on language-guided grounding.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "consistent training",
      "inference procedures",
      "state-of-the-art results"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Sa2VA",
      "resolved_canonical": "Sa2VA",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Sa2VA-i",
      "resolved_canonical": "Sa2VA-i",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "referring video object segmentation",
      "resolved_canonical": "Referring Video Object Segmentation",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# 3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19082.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19082](https://arxiv.org/abs/2509.19082)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Enhancing Sa2VA for Referent Video Object Segmentation_ 2nd Solution for 7th LSVOS RVOS Track_20250922|Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track]] (91.8% similar)
- [[2025-09-23/The 1st Solution for 7th LSVOS RVOS Track_ SaSaSa2VA_20250923|The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA]] (89.4% similar)
- [[2025-09-22/Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge_ 3rd Place Solution_20250922|Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution]] (84.5% similar)
- [[2025-09-18/SAIL-VL2 Technical Report_20250918|SAIL-VL2 Technical Report]] (84.3% similar)
- [[2025-09-23/SAMSON_ 3rd Place Solution of LSVOS 2025 VOS Challenge_20250923|SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge]] (83.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Referring Video Object Segmentation|Referring Video Object Segmentation]]
**âš¡ Unique Technical**: [[keywords/Sa2VA|Sa2VA]], [[keywords/Sa2VA-i|Sa2VA-i]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19082v1 Announce Type: new 
Abstract: Sa2VA is a recent model for language-guided dense grounding in images and video that achieves state-of-the-art results on multiple segmentation benchmarks and that has become widely popular. However, we found that Sa2VA does not perform according to its full potential for referring video object segmentation tasks. We identify inconsistencies between training and inference procedures as the key factor holding it back. To mitigate this issue, we propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and improves the results. In fact, Sa2VA-i sets a new state of the art for multiple video benchmarks and achieves improvements of up to +11.6 J&amp;F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the original Sa2VA-26B model on the MeViS benchmark. We hope that this work will show the importance of seemingly trivial implementation details and that it will provide valuable insights for the referring video segmentation field. We provide the code and updated models at https://github.com/kumuji/sa2va-i

## ğŸ“ ìš”ì•½

Sa2VAëŠ” ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ì—ì„œ ì–¸ì–´ ê¸°ë°˜ ë°€ì§‘ ê·¸ë¼ìš´ë”©ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ë¡œ, ì—¬ëŸ¬ ì„¸ë¶„í™” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¹„ë””ì˜¤ ê°ì²´ ì„¸ë¶„í™” ì‘ì—…ì—ì„œëŠ” ì ì¬ë ¥ì„ ì¶©ë¶„íˆ ë°œíœ˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ê³¼ ì¶”ë¡  ì ˆì°¨ ê°„ì˜ ë¶ˆì¼ì¹˜ ë•Œë¬¸ì´ë¼ê³  íŒë‹¨í•˜ì—¬, ì´ë¥¼ ê°œì„ í•œ Sa2VA-ië¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. Sa2VA-iëŠ” ì—¬ëŸ¬ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìœ¼ë©°, MeViSì—ì„œ ìµœëŒ€ +11.6 J&F, Ref-YT-VOSì—ì„œ +1.4, Ref-DAVISì—ì„œ +3.3, ReVOSì—ì„œ +4.1ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. Sa2VA-i-1B ëª¨ë¸ì€ MeViS ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ Sa2VA-26B ëª¨ë¸ê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ë¹„ë””ì˜¤ ì„¸ë¶„í™” ë¶„ì•¼ì— ìœ ìš©í•œ í†µì°°ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤. ì½”ë“œì™€ ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ì€ GitHubì—ì„œ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Sa2VAëŠ” ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ì—ì„œ ì–¸ì–´ ê¸°ë°˜ì˜ ë°€ì§‘ ê·¸ë¼ìš´ë”©ì„ ìœ„í•œ ìµœì‹  ëª¨ë¸ë¡œ ì—¬ëŸ¬ ì„¸ë¶„í™” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 2. Sa2VAëŠ” ë¹„ë””ì˜¤ ê°ì²´ ë¶„í•  ì‘ì—…ì—ì„œ ìµœëŒ€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ ëª»í•˜ë©°, ì´ëŠ” í›ˆë ¨ê³¼ ì¶”ë¡  ì ˆì°¨ ê°„ì˜ ë¶ˆì¼ì¹˜ ë•Œë¬¸ì…ë‹ˆë‹¤.
- 3. Sa2VA-iëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•œ Sa2VAì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ, ì—¬ëŸ¬ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
- 4. Sa2VA-iëŠ” MeViS, Ref-YT-VOS, Ref-DAVIS, ReVOSì—ì„œ ê°ê° ìµœëŒ€ +11.6, +1.4, +3.3, +4.1ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ì‚¬ì†Œí•´ ë³´ì´ëŠ” êµ¬í˜„ ì„¸ë¶€ ì‚¬í•­ì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì£¼ê³ , ë¹„ë””ì˜¤ ê°ì²´ ë¶„í•  ë¶„ì•¼ì— ê·€ì¤‘í•œ í†µì°°ë ¥ì„ ì œê³µí•˜ê¸°ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:15:20*