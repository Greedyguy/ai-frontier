<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:40:52.981896",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language-Action Model",
    "Perspective-Adaptive Fusion",
    "RoboFlamingo-LPAF",
    "Multiview Observations",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language-Action Model": 0.82,
    "Perspective-Adaptive Fusion": 0.79,
    "RoboFlamingo-LPAF": 0.78,
    "Multiview Observations": 0.75,
    "Vision-Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual-Language-Action",
        "canonical": "Vision-Language-Action Model",
        "aliases": [
          "VLA"
        ],
        "category": "evolved_concepts",
        "rationale": "This concept integrates vision, language, and action, forming a bridge between multimodal learning and robotics.",
        "novelty_score": 0.75,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "Perspective-Adaptive Fusion",
        "canonical": "Perspective-Adaptive Fusion",
        "aliases": [
          "PAF"
        ],
        "category": "unique_technical",
        "rationale": "This technique addresses perspective heterogeneity, enhancing the adaptability of multimodal models.",
        "novelty_score": 0.78,
        "connectivity_score": 0.72,
        "specificity_score": 0.81,
        "link_intent_score": 0.79
      },
      {
        "surface": "RoboFlamingo-LPAF",
        "canonical": "RoboFlamingo-LPAF",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This specific implementation showcases the application of perspective-adaptive fusion in robotic manipulation.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multiview Observations",
        "canonical": "Multiview Observations",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "This concept is crucial for understanding how multiple perspectives are integrated in VLA models.",
        "novelty_score": 0.68,
        "connectivity_score": 0.77,
        "specificity_score": 0.73,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "A core component of the study, linking it to broader trends in multimodal learning.",
        "novelty_score": 0.6,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "task success rate",
      "real-world tasks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual-Language-Action",
      "resolved_canonical": "Vision-Language-Action Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Perspective-Adaptive Fusion",
      "resolved_canonical": "Perspective-Adaptive Fusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.72,
        "specificity": 0.81,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "RoboFlamingo-LPAF",
      "resolved_canonical": "RoboFlamingo-LPAF",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multiview Observations",
      "resolved_canonical": "Multiview Observations",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.77,
        "specificity": 0.73,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18183.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18183](https://arxiv.org/abs/2509.18183)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Evo-0_ Vision-Language-Action Model with Implicit Spatial Understanding_20250923|Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding]] (88.3% similar)
- [[2025-09-23/The Better You Learn, The Smarter You Prune_ Towards Efficient Vision-language-action Models via Differentiable Token Pruning_20250923|The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning]] (87.9% similar)
- [[2025-09-18/GeoAware-VLA_ Implicit Geometry Aware Vision-Language-Action Model_20250918|GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model]] (87.5% similar)
- [[2025-09-19/ForceVLA_ Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation_20250919|ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation]] (87.3% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (86.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multiview Observations|Multiview Observations]]
**âš¡ Unique Technical**: [[keywords/Perspective-Adaptive Fusion|Perspective-Adaptive Fusion]], [[keywords/RoboFlamingo-LPAF|RoboFlamingo-LPAF]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language-Action Model|Vision-Language-Action Model]], [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18183v1 Announce Type: cross 
Abstract: The Visual-Language-Action (VLA) models can follow text instructions according to visual observations of the surrounding environment. This ability to map multimodal inputs to actions is derived from the training of the VLA model on extensive standard demonstrations. These visual observations captured by third-personal global and in-wrist local cameras are inevitably varied in number and perspective across different environments, resulting in significant differences in the visual features. This perspective heterogeneity constrains the generality of VLA models. In light of this, we first propose the lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models using only 2D data. VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space, which effectively and efficiently bridge the gap caused by perspective inconsistency. We instantiate our VLA-LPAF framework with the VLA model RoboFlamingo to construct RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. We also demonstrate the developed viewadaptive characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Visual-Language-Action (VLA) ëª¨ë¸ì˜ ì‹œì  ì ì‘ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ VLA-LPAFë¼ëŠ” ê²½ëŸ‰ ëª¨ë“ˆì„ ì œì•ˆí•©ë‹ˆë‹¤. VLA ëª¨ë¸ì€ ì‹œê°ì  ê´€ì°°ì„ í†µí•´ í…ìŠ¤íŠ¸ ì§€ì‹œë¥¼ ë”°ë¥¼ ìˆ˜ ìˆì§€ë§Œ, ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹œì ì˜ ì°¨ì´ë¡œ ì¸í•´ ì¼ë°˜í™”ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. VLA-LPAFëŠ” ë‹¨ì¼ ì‹œì ì˜ ì´ë¯¸ì§€ë¡œ ë¯¸ì„¸ ì¡°ì •ë˜ë©°, ì ì¬ ê³µê°„ì—ì„œ ë‹¤ì¤‘ ì‹œì  ê´€ì°°ì„ ìœµí•©í•˜ì—¬ ì‹œì  ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤. RoboFlamingo ëª¨ë¸ì— VLA-LPAFë¥¼ ì ìš©í•œ RoboFlamingo-LPAFëŠ” CALVIN, LIBERO, ë§ì¶¤í˜• ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê°ê° ì•½ 8%, 15%, 30%ì˜ ì‘ì—… ì„±ê³µë¥  í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ì‹¤ì œ ì‘ì—…ì„ í†µí•´ ì œì•ˆëœ ëª¨ë¸ì˜ ì‹œì  ì ì‘ íŠ¹ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VLA ëª¨ë¸ì€ ì‹œê°ì  ê´€ì°°ì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì§€ì‹œë¥¼ ë”°ë¥´ëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤.
- 2. ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹œê°ì  íŠ¹ì§•ì˜ ì°¨ì´ëŠ” VLA ëª¨ë¸ì˜ ì¼ë°˜ì„±ì„ ì œí•œí•œë‹¤.
- 3. VLA-LPAF ëª¨ë“ˆì€ 2D ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ VLA ëª¨ë¸ì˜ ì‹œì  ì ì‘ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.
- 4. RoboFlamingo-LPAFëŠ” CALVIN, LIBERO, ë§ì¶¤í˜• ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ê°ê° 8%, 15%, 30%ì˜ ì‘ì—… ì„±ê³µë¥  í–¥ìƒì„ ë³´ì˜€ë‹¤.
- 5. ì œì•ˆëœ RoboFlamingo-LPAFëŠ” ì‹¤ì œ ì‘ì—…ì„ í†µí•´ ì‹œì  ì ì‘ íŠ¹ì„±ì„ ì…ì¦í•˜ì˜€ë‹¤.


---

*Generated on 2025-09-24 13:40:52*