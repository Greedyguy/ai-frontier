<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:22:47.420449",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Transformer",
    "EEGM2",
    "U-shaped Encoder-Decoder Architecture",
    "Mamba-2"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.85,
    "Transformer": 0.8,
    "EEGM2": 0.78,
    "U-shaped Encoder-Decoder Architecture": 0.77,
    "Mamba-2": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "self-supervised learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "SSL"
        ],
        "category": "specific_connectable",
        "rationale": "Self-supervised learning is a key technique in the proposed framework, facilitating connections with other self-supervised methods.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Transformer-based architectures",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the framework's architecture, linking it to a wide range of neural network applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "EEGM2",
        "canonical": "EEGM2",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "EEGM2 is the unique framework introduced in the paper, crucial for linking to specific implementations and results.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "U-shaped encoder-decoder architecture",
        "canonical": "U-shaped Encoder-Decoder Architecture",
        "aliases": [
          "U-Net architecture"
        ],
        "category": "specific_connectable",
        "rationale": "This architecture is pivotal for the model's efficiency and can connect to other encoder-decoder based models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mamba-2",
        "canonical": "Mamba-2",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Mamba-2 is a novel component of the framework, essential for understanding its computational efficiency.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "EEG signals",
      "generalization",
      "memory usage"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "self-supervised learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Transformer-based architectures",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "EEGM2",
      "resolved_canonical": "EEGM2",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "U-shaped encoder-decoder architecture",
      "resolved_canonical": "U-shaped Encoder-Decoder Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mamba-2",
      "resolved_canonical": "Mamba-2",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# An Efficient Self-Supervised Framework for Long-Sequence EEG Modeling

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.17873.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2502.17873](https://arxiv.org/abs/2502.17873)

## 🔗 유사한 논문
- [[2025-09-23/DBConformer_ Dual-Branch Convolutional Transformer for EEG Decoding_20250923|DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding]] (85.2% similar)
- [[2025-09-22/IEFS-GMB_ Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders_20250922|IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders]] (85.0% similar)
- [[2025-09-22/EvoBrain_ Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network_20250922|EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network]] (84.9% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (84.4% similar)
- [[2025-09-17/Personalization on a Budget_ Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection_20250917|Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection]] (84.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/U-shaped Encoder-Decoder Architecture|U-shaped Encoder-Decoder Architecture]]
**⚡ Unique Technical**: [[keywords/EEGM2|EEGM2]], [[keywords/Mamba-2|Mamba-2]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2502.17873v2 Announce Type: replace 
Abstract: Electroencephalogram (EEG) signals generally exhibit low signal-to-noise ratio (SNR) and high inter-subject variability, making generalization across subjects and domains challenging. Recent advances in deep learning, particularly self-supervised learning with Transformer-based architectures, have shown promise in EEG representation learning. However, their quadratic computational complexity increases memory usage and slows inference, making them inefficient for modeling long-range dependencies. Moreover, most existing approaches emphasize either explicit window segmentation of the temporal signal or spectral-only input embedding while neglecting raw temporal dynamics. In this paper, we propose EEGM2, a self-supervised framework that overcomes these limitations. EEGM2 adopts a U-shaped encoder-decoder architecture integrated with Mamba-2 to achieve linear computational complexity, thereby reducing memory usage and improving inference speed. Meanwhile, the selective information propagation mechanism of Mamba-2 enables the model to effectively capture and preserve long-range dependencies in raw EEG signals, where traditional RNN or CNN architectures often struggle. Moreover, EEGM2 employs a self-supervised pre-training objective that reconstructs raw EEG using a combined L1 and spectral (Fourier-based) loss, enhancing generalization by jointly preserving temporal dynamics and spectral characteristics. Experimental results demonstrate that EEGM2 achieves state-of-the-art performance in both short- and long-sequence modeling and classification. Further evaluations show that EEGM2 consistently outperforms existing models, demonstrating strong generalization across subjects and tasks, as well as transferability across domains. Overall, EEGM2 offers an efficient and scalable solution suitable for deployment on resource-constrained brain-computer interface (BCI) devices.

## 📝 요약

EEG 신호의 낮은 신호 대 잡음비와 높은 개인 간 변동성 문제를 해결하기 위해, 본 논문에서는 EEGM2라는 새로운 자기 지도 학습 프레임워크를 제안합니다. EEGM2는 U자형 인코더-디코더 구조와 Mamba-2를 통합하여 선형 계산 복잡성을 달성하고, 메모리 사용량을 줄이며 추론 속도를 향상시킵니다. Mamba-2의 선택적 정보 전파 메커니즘을 통해 장거리 의존성을 효과적으로 포착하며, L1 및 스펙트럼 손실을 활용한 자기 지도 사전 학습 목표로 시간적 역학과 스펙트럼 특성을 동시에 보존합니다. 실험 결과, EEGM2는 짧고 긴 시퀀스 모델링 및 분류에서 최첨단 성능을 보이며, 다양한 주제와 작업에서 강력한 일반화 및 도메인 간 전이 가능성을 보여줍니다. EEGM2는 자원이 제한된 뇌-컴퓨터 인터페이스(BCI) 장치에 적합한 효율적이고 확장 가능한 솔루션을 제공합니다.

## 🎯 주요 포인트

- 1. EEGM2는 U자형 인코더-디코더 아키텍처와 Mamba-2를 통합하여 선형 계산 복잡성을 달성하고 메모리 사용량을 줄이며 추론 속도를 개선합니다.
- 2. Mamba-2의 선택적 정보 전파 메커니즘은 전통적인 RNN이나 CNN 아키텍처가 어려움을 겪는 원시 EEG 신호의 장거리 종속성을 효과적으로 포착하고 보존합니다.
- 3. EEGM2는 L1 및 스펙트럼(푸리에 기반) 손실을 결합한 자기 지도 사전 학습 목표를 사용하여 원시 EEG를 재구성함으로써 시간적 역학과 스펙트럼 특성을 동시에 보존하여 일반화를 향상시킵니다.
- 4. 실험 결과, EEGM2는 단기 및 장기 시퀀스 모델링과 분류에서 최첨단 성능을 달성하며, 기존 모델보다 일관되게 우수한 성능을 보여줍니다.
- 5. EEGM2는 자원 제약이 있는 뇌-컴퓨터 인터페이스(BCI) 장치에 적합한 효율적이고 확장 가능한 솔루션을 제공합니다.


---

*Generated on 2025-09-24 15:22:47*