<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:22:47.420449",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Self-supervised Learning",
    "Transformer",
    "EEGM2",
    "U-shaped Encoder-Decoder Architecture",
    "Mamba-2"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Self-supervised Learning": 0.85,
    "Transformer": 0.8,
    "EEGM2": 0.78,
    "U-shaped Encoder-Decoder Architecture": 0.77,
    "Mamba-2": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "self-supervised learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "SSL"
        ],
        "category": "specific_connectable",
        "rationale": "Self-supervised learning is a key technique in the proposed framework, facilitating connections with other self-supervised methods.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Transformer-based architectures",
        "canonical": "Transformer",
        "aliases": [
          "Transformer models"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are central to the framework's architecture, linking it to a wide range of neural network applications.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "EEGM2",
        "canonical": "EEGM2",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "EEGM2 is the unique framework introduced in the paper, crucial for linking to specific implementations and results.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "U-shaped encoder-decoder architecture",
        "canonical": "U-shaped Encoder-Decoder Architecture",
        "aliases": [
          "U-Net architecture"
        ],
        "category": "specific_connectable",
        "rationale": "This architecture is pivotal for the model's efficiency and can connect to other encoder-decoder based models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mamba-2",
        "canonical": "Mamba-2",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Mamba-2 is a novel component of the framework, essential for understanding its computational efficiency.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "EEG signals",
      "generalization",
      "memory usage"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "self-supervised learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Transformer-based architectures",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "EEGM2",
      "resolved_canonical": "EEGM2",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "U-shaped encoder-decoder architecture",
      "resolved_canonical": "U-shaped Encoder-Decoder Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mamba-2",
      "resolved_canonical": "Mamba-2",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# An Efficient Self-Supervised Framework for Long-Sequence EEG Modeling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.17873.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2502.17873](https://arxiv.org/abs/2502.17873)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DBConformer_ Dual-Branch Convolutional Transformer for EEG Decoding_20250923|DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding]] (85.2% similar)
- [[2025-09-22/IEFS-GMB_ Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders_20250922|IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders]] (85.0% similar)
- [[2025-09-22/EvoBrain_ Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network_20250922|EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network]] (84.9% similar)
- [[2025-09-19/UMind_ A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding_20250919|UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding]] (84.4% similar)
- [[2025-09-17/Personalization on a Budget_ Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection_20250917|Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/U-shaped Encoder-Decoder Architecture|U-shaped Encoder-Decoder Architecture]]
**âš¡ Unique Technical**: [[keywords/EEGM2|EEGM2]], [[keywords/Mamba-2|Mamba-2]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.17873v2 Announce Type: replace 
Abstract: Electroencephalogram (EEG) signals generally exhibit low signal-to-noise ratio (SNR) and high inter-subject variability, making generalization across subjects and domains challenging. Recent advances in deep learning, particularly self-supervised learning with Transformer-based architectures, have shown promise in EEG representation learning. However, their quadratic computational complexity increases memory usage and slows inference, making them inefficient for modeling long-range dependencies. Moreover, most existing approaches emphasize either explicit window segmentation of the temporal signal or spectral-only input embedding while neglecting raw temporal dynamics. In this paper, we propose EEGM2, a self-supervised framework that overcomes these limitations. EEGM2 adopts a U-shaped encoder-decoder architecture integrated with Mamba-2 to achieve linear computational complexity, thereby reducing memory usage and improving inference speed. Meanwhile, the selective information propagation mechanism of Mamba-2 enables the model to effectively capture and preserve long-range dependencies in raw EEG signals, where traditional RNN or CNN architectures often struggle. Moreover, EEGM2 employs a self-supervised pre-training objective that reconstructs raw EEG using a combined L1 and spectral (Fourier-based) loss, enhancing generalization by jointly preserving temporal dynamics and spectral characteristics. Experimental results demonstrate that EEGM2 achieves state-of-the-art performance in both short- and long-sequence modeling and classification. Further evaluations show that EEGM2 consistently outperforms existing models, demonstrating strong generalization across subjects and tasks, as well as transferability across domains. Overall, EEGM2 offers an efficient and scalable solution suitable for deployment on resource-constrained brain-computer interface (BCI) devices.

## ğŸ“ ìš”ì•½

EEG ì‹ í˜¸ì˜ ë‚®ì€ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ì™€ ë†’ì€ ê°œì¸ ê°„ ë³€ë™ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” EEGM2ë¼ëŠ” ìƒˆë¡œìš´ ìê¸° ì§€ë„ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. EEGM2ëŠ” Uìí˜• ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì™€ Mamba-2ë¥¼ í†µí•©í•˜ì—¬ ì„ í˜• ê³„ì‚° ë³µì¡ì„±ì„ ë‹¬ì„±í•˜ê³ , ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©° ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. Mamba-2ì˜ ì„ íƒì  ì •ë³´ ì „íŒŒ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ë©°, L1 ë° ìŠ¤í™íŠ¸ëŸ¼ ì†ì‹¤ì„ í™œìš©í•œ ìê¸° ì§€ë„ ì‚¬ì „ í•™ìŠµ ëª©í‘œë¡œ ì‹œê°„ì  ì—­í•™ê³¼ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ ë™ì‹œì— ë³´ì¡´í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, EEGM2ëŠ” ì§§ê³  ê¸´ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë° ë¶„ë¥˜ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¤ì–‘í•œ ì£¼ì œì™€ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì¼ë°˜í™” ë° ë„ë©”ì¸ ê°„ ì „ì´ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. EEGM2ëŠ” ìì›ì´ ì œí•œëœ ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤(BCI) ì¥ì¹˜ì— ì í•©í•œ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. EEGM2ëŠ” Uìí˜• ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ì™€ Mamba-2ë¥¼ í†µí•©í•˜ì—¬ ì„ í˜• ê³„ì‚° ë³µì¡ì„±ì„ ë‹¬ì„±í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©° ì¶”ë¡  ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 2. Mamba-2ì˜ ì„ íƒì  ì •ë³´ ì „íŒŒ ë©”ì»¤ë‹ˆì¦˜ì€ ì „í†µì ì¸ RNNì´ë‚˜ CNN ì•„í‚¤í…ì²˜ê°€ ì–´ë ¤ì›€ì„ ê²ªëŠ” ì›ì‹œ EEG ì‹ í˜¸ì˜ ì¥ê±°ë¦¬ ì¢…ì†ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ê³  ë³´ì¡´í•©ë‹ˆë‹¤.
- 3. EEGM2ëŠ” L1 ë° ìŠ¤í™íŠ¸ëŸ¼(í‘¸ë¦¬ì— ê¸°ë°˜) ì†ì‹¤ì„ ê²°í•©í•œ ìê¸° ì§€ë„ ì‚¬ì „ í•™ìŠµ ëª©í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ì‹œ EEGë¥¼ ì¬êµ¬ì„±í•¨ìœ¼ë¡œì¨ ì‹œê°„ì  ì—­í•™ê³¼ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ì„ ë™ì‹œì— ë³´ì¡´í•˜ì—¬ ì¼ë°˜í™”ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, EEGM2ëŠ” ë‹¨ê¸° ë° ì¥ê¸° ì‹œí€€ìŠ¤ ëª¨ë¸ë§ê³¼ ë¶„ë¥˜ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. EEGM2ëŠ” ìì› ì œì•½ì´ ìˆëŠ” ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤(BCI) ì¥ì¹˜ì— ì í•©í•œ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:22:47*