<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:38:05.959495",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Prefix-to-Tree",
    "Tree-Estimated Mean Prefix Value for Policy Optimization",
    "Proximal Policy Optimization",
    "Group-relative Outcome Signal"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Prefix-to-Tree": 0.78,
    "Tree-Estimated Mean Prefix Value for Policy Optimization": 0.82,
    "Proximal Policy Optimization": 0.8,
    "Group-relative Outcome Signal": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is central to the paper's focus on improving reasoning in LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Prefix-to-Tree",
        "canonical": "Prefix-to-Tree",
        "aliases": [
          "P2T"
        ],
        "category": "unique_technical",
        "rationale": "Prefix-to-Tree is a novel method introduced in the paper for token-level credit assignment.",
        "novelty_score": 0.92,
        "connectivity_score": 0.67,
        "specificity_score": 0.89,
        "link_intent_score": 0.78
      },
      {
        "surface": "Tree-Estimated Mean Prefix Value for Policy Optimization",
        "canonical": "Tree-Estimated Mean Prefix Value for Policy Optimization",
        "aliases": [
          "TEMPO"
        ],
        "category": "unique_technical",
        "rationale": "TEMPO is a new algorithm proposed in the paper, enhancing GRPO with tree-based corrections.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.91,
        "link_intent_score": 0.82
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "specific_connectable",
        "rationale": "PPO is a well-known reinforcement learning algorithm compared against in the paper.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Group-relative Outcome Signal",
        "canonical": "Group-relative Outcome Signal",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is critical for understanding the improvements TEMPO offers over GRPO.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Prefix-to-Tree",
      "resolved_canonical": "Prefix-to-Tree",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.67,
        "specificity": 0.89,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Tree-Estimated Mean Prefix Value for Policy Optimization",
      "resolved_canonical": "Tree-Estimated Mean Prefix Value for Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.91,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Group-relative Outcome Signal",
      "resolved_canonical": "Group-relative Outcome Signal",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Exploiting Tree Structure for Credit Assignment in RL Training of LLMs

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18314.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18314](https://arxiv.org/abs/2509.18314)

## 🔗 유사한 논문
- [[2025-09-24/Single-stream Policy Optimization_20250924|Single-stream Policy Optimization]] (85.9% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.0% similar)
- [[2025-09-24/NGRPO_ Negative-enhanced Group Relative Policy Optimization_20250924|NGRPO: Negative-enhanced Group Relative Policy Optimization]] (84.8% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (84.8% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (84.7% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]]
**⚡ Unique Technical**: [[keywords/Prefix-to-Tree|Prefix-to-Tree]], [[keywords/Tree-Estimated Mean Prefix Value for Policy Optimization|Tree-Estimated Mean Prefix Value for Policy Optimization]], [[keywords/Group-relative Outcome Signal|Group-relative Outcome Signal]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18314v1 Announce Type: new 
Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over long sequences makes token-level credit assignment the key bottleneck. We study the verifiable-reward setting, where the final answer is checkable and multiple responses can be drawn per prompt. Reasoning tasks in math and medical QA align with this setup, where only a few decision tokens significantly impact the outcome. PPO offers token-level advantages with a learned value model, but it is complex to train both the actor and critic models simultaneously, and it is not easily generalizable, as the token-level values from the critic model can make training prone to overfitting. GRPO is critic-free and supports verifiable rewards, but spreads a single sequence-level return across tokens and ignores branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that converts a group of responses into a prefix tree and computes \emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes. Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated \textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a critic-free algorithm that augments the group-relative outcome signal of GRPO with \emph{branch-gated} temporal-difference corrections derived from the tree. At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO reduces to GRPO; at branching tokens, it supplies precise token-level credit without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B, TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and reaches higher validation accuracy with roughly the same wall-clock time.

## 📝 요약

이 논문은 강화 학습을 통해 대형 언어 모델(LLM)의 추론 능력을 향상시키는 방법을 제시합니다. 특히, 긴 시퀀스에서 지연된 보상이 드문 경우 토큰 수준의 신용 할당이 주요 과제로 부각됩니다. 수학 및 의료 분야의 질문 응답(QA) 작업은 최종 답변이 검증 가능하고 여러 응답이 가능하다는 점에서 이 연구의 설정과 일치합니다. 기존 방법인 PPO는 토큰 수준의 이점을 제공하지만, 배우와 비평가 모델을 동시에 훈련하기 어렵고 과적합의 위험이 있습니다. GRPO는 비평가 없이 검증 가능한 보상을 지원하지만, 단일 시퀀스 반환을 토큰에 분산시켜 분기점을 무시합니다. 이를 해결하기 위해, 우리는 응답 그룹을 접두사 트리로 변환하고 비모수적 접두사 값을 계산하는 P2T를 소개합니다. 이를 바탕으로, 비평가가 없는 알고리즘인 TEMPO를 제안하며, 이는 GRPO의 결과 신호에 트리 기반의 시간차 수정값을 추가합니다. TEMPO는 다양한 벤치마크에서 PPO와 GRPO를 능가하며, 비슷한 시간 내에 더 높은 검증 정확도를 달성합니다.

## 🎯 주요 포인트

- 1. 강화 학습은 LLM의 추론 능력을 향상시키지만, 긴 시퀀스에서의 희소한 지연 보상은 토큰 수준의 크레딧 할당을 주요 병목으로 만듭니다.
- 2. 검증 가능한 보상 설정에서는 최종 답변이 확인 가능하며, 하나의 프롬프트에 대해 여러 응답을 생성할 수 있습니다.
- 3. Prefix-to-Tree (P2T)는 응답 그룹을 접두사 트리로 변환하여 후손 결과를 집계함으로써 비모수적 접두사 값을 계산하는 간단한 절차입니다.
- 4. TEMPO는 GRPO의 그룹 상대적 결과 신호를 트리에서 유도된 분기-게이트된 시간차 수정으로 보강하는 비평가 기반 알고리즘입니다.
- 5. TEMPO는 다양한 벤치마크에서 PPO와 GRPO를 능가하며, 유사한 시간 내에 더 높은 검증 정확도를 달성합니다.


---

*Generated on 2025-09-24 15:38:05*