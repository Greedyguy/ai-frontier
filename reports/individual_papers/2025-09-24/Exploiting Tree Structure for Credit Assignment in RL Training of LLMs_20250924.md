<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:38:05.959495",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Prefix-to-Tree",
    "Tree-Estimated Mean Prefix Value for Policy Optimization",
    "Proximal Policy Optimization",
    "Group-relative Outcome Signal"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Prefix-to-Tree": 0.78,
    "Tree-Estimated Mean Prefix Value for Policy Optimization": 0.82,
    "Proximal Policy Optimization": 0.8,
    "Group-relative Outcome Signal": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is central to the paper's focus on improving reasoning in LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Prefix-to-Tree",
        "canonical": "Prefix-to-Tree",
        "aliases": [
          "P2T"
        ],
        "category": "unique_technical",
        "rationale": "Prefix-to-Tree is a novel method introduced in the paper for token-level credit assignment.",
        "novelty_score": 0.92,
        "connectivity_score": 0.67,
        "specificity_score": 0.89,
        "link_intent_score": 0.78
      },
      {
        "surface": "Tree-Estimated Mean Prefix Value for Policy Optimization",
        "canonical": "Tree-Estimated Mean Prefix Value for Policy Optimization",
        "aliases": [
          "TEMPO"
        ],
        "category": "unique_technical",
        "rationale": "TEMPO is a new algorithm proposed in the paper, enhancing GRPO with tree-based corrections.",
        "novelty_score": 0.95,
        "connectivity_score": 0.7,
        "specificity_score": 0.91,
        "link_intent_score": 0.82
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "specific_connectable",
        "rationale": "PPO is a well-known reinforcement learning algorithm compared against in the paper.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Group-relative Outcome Signal",
        "canonical": "Group-relative Outcome Signal",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is critical for understanding the improvements TEMPO offers over GRPO.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Prefix-to-Tree",
      "resolved_canonical": "Prefix-to-Tree",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.67,
        "specificity": 0.89,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Tree-Estimated Mean Prefix Value for Policy Optimization",
      "resolved_canonical": "Tree-Estimated Mean Prefix Value for Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.7,
        "specificity": 0.91,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Group-relative Outcome Signal",
      "resolved_canonical": "Group-relative Outcome Signal",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Exploiting Tree Structure for Credit Assignment in RL Training of LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18314.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18314](https://arxiv.org/abs/2509.18314)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Single-stream Policy Optimization_20250924|Single-stream Policy Optimization]] (85.9% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (85.0% similar)
- [[2025-09-24/NGRPO_ Negative-enhanced Group Relative Policy Optimization_20250924|NGRPO: Negative-enhanced Group Relative Policy Optimization]] (84.8% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (84.8% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (84.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]]
**âš¡ Unique Technical**: [[keywords/Prefix-to-Tree|Prefix-to-Tree]], [[keywords/Tree-Estimated Mean Prefix Value for Policy Optimization|Tree-Estimated Mean Prefix Value for Policy Optimization]], [[keywords/Group-relative Outcome Signal|Group-relative Outcome Signal]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18314v1 Announce Type: new 
Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over long sequences makes token-level credit assignment the key bottleneck. We study the verifiable-reward setting, where the final answer is checkable and multiple responses can be drawn per prompt. Reasoning tasks in math and medical QA align with this setup, where only a few decision tokens significantly impact the outcome. PPO offers token-level advantages with a learned value model, but it is complex to train both the actor and critic models simultaneously, and it is not easily generalizable, as the token-level values from the critic model can make training prone to overfitting. GRPO is critic-free and supports verifiable rewards, but spreads a single sequence-level return across tokens and ignores branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that converts a group of responses into a prefix tree and computes \emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes. Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated \textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a critic-free algorithm that augments the group-relative outcome signal of GRPO with \emph{branch-gated} temporal-difference corrections derived from the tree. At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO reduces to GRPO; at branching tokens, it supplies precise token-level credit without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B, TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and reaches higher validation accuracy with roughly the same wall-clock time.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì„ í†µí•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì§€ì—°ëœ ë³´ìƒì´ ë“œë¬¸ ê²½ìš° í† í° ìˆ˜ì¤€ì˜ ì‹ ìš© í• ë‹¹ì´ ì£¼ìš” ê³¼ì œë¡œ ë¶€ê°ë©ë‹ˆë‹¤. ìˆ˜í•™ ë° ì˜ë£Œ ë¶„ì•¼ì˜ ì§ˆë¬¸ ì‘ë‹µ(QA) ì‘ì—…ì€ ìµœì¢… ë‹µë³€ì´ ê²€ì¦ ê°€ëŠ¥í•˜ê³  ì—¬ëŸ¬ ì‘ë‹µì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì—ì„œ ì´ ì—°êµ¬ì˜ ì„¤ì •ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ì¸ PPOëŠ” í† í° ìˆ˜ì¤€ì˜ ì´ì ì„ ì œê³µí•˜ì§€ë§Œ, ë°°ìš°ì™€ ë¹„í‰ê°€ ëª¨ë¸ì„ ë™ì‹œì— í›ˆë ¨í•˜ê¸° ì–´ë µê³  ê³¼ì í•©ì˜ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. GRPOëŠ” ë¹„í‰ê°€ ì—†ì´ ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì„ ì§€ì›í•˜ì§€ë§Œ, ë‹¨ì¼ ì‹œí€€ìŠ¤ ë°˜í™˜ì„ í† í°ì— ë¶„ì‚°ì‹œì¼œ ë¶„ê¸°ì ì„ ë¬´ì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì‘ë‹µ ê·¸ë£¹ì„ ì ‘ë‘ì‚¬ íŠ¸ë¦¬ë¡œ ë³€í™˜í•˜ê³  ë¹„ëª¨ìˆ˜ì  ì ‘ë‘ì‚¬ ê°’ì„ ê³„ì‚°í•˜ëŠ” P2Të¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë¹„í‰ê°€ê°€ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ TEMPOë¥¼ ì œì•ˆí•˜ë©°, ì´ëŠ” GRPOì˜ ê²°ê³¼ ì‹ í˜¸ì— íŠ¸ë¦¬ ê¸°ë°˜ì˜ ì‹œê°„ì°¨ ìˆ˜ì •ê°’ì„ ì¶”ê°€í•©ë‹ˆë‹¤. TEMPOëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ PPOì™€ GRPOë¥¼ ëŠ¥ê°€í•˜ë©°, ë¹„ìŠ·í•œ ì‹œê°„ ë‚´ì— ë” ë†’ì€ ê²€ì¦ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµì€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ê¸´ ì‹œí€€ìŠ¤ì—ì„œì˜ í¬ì†Œí•œ ì§€ì—° ë³´ìƒì€ í† í° ìˆ˜ì¤€ì˜ í¬ë ˆë”§ í• ë‹¹ì„ ì£¼ìš” ë³‘ëª©ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
- 2. ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ì„¤ì •ì—ì„œëŠ” ìµœì¢… ë‹µë³€ì´ í™•ì¸ ê°€ëŠ¥í•˜ë©°, í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì—¬ëŸ¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 3. Prefix-to-Tree (P2T)ëŠ” ì‘ë‹µ ê·¸ë£¹ì„ ì ‘ë‘ì‚¬ íŠ¸ë¦¬ë¡œ ë³€í™˜í•˜ì—¬ í›„ì† ê²°ê³¼ë¥¼ ì§‘ê³„í•¨ìœ¼ë¡œì¨ ë¹„ëª¨ìˆ˜ì  ì ‘ë‘ì‚¬ ê°’ì„ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ ì ˆì°¨ì…ë‹ˆë‹¤.
- 4. TEMPOëŠ” GRPOì˜ ê·¸ë£¹ ìƒëŒ€ì  ê²°ê³¼ ì‹ í˜¸ë¥¼ íŠ¸ë¦¬ì—ì„œ ìœ ë„ëœ ë¶„ê¸°-ê²Œì´íŠ¸ëœ ì‹œê°„ì°¨ ìˆ˜ì •ìœ¼ë¡œ ë³´ê°•í•˜ëŠ” ë¹„í‰ê°€ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 5. TEMPOëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ PPOì™€ GRPOë¥¼ ëŠ¥ê°€í•˜ë©°, ìœ ì‚¬í•œ ì‹œê°„ ë‚´ì— ë” ë†’ì€ ê²€ì¦ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:38:05*