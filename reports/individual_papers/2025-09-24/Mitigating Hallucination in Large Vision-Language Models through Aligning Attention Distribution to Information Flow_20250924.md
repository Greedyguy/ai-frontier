<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:29:19.621322",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Attention Mechanism",
    "Semantic Representations",
    "Image Captioning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.9,
    "Attention Mechanism": 0.85,
    "Semantic Representations": 0.78,
    "Image Captioning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "LVLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models represent a significant evolution in multimodal learning, linking visual and textual data processing.",
        "novelty_score": 0.75,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Attention Distribution",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Attention Heads"
        ],
        "category": "specific_connectable",
        "rationale": "Attention distribution is crucial for understanding how models prioritize information, directly linking to the core concept of attention mechanisms.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Semantic Representations",
        "canonical": "Semantic Representations",
        "aliases": [
          "Semantic Embeddings"
        ],
        "category": "unique_technical",
        "rationale": "Semantic representations are key to understanding how models interpret and encode meaning, offering unique insights into model behavior.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Image Captioning",
        "canonical": "Image Captioning",
        "aliases": [
          "Visual Description"
        ],
        "category": "specific_connectable",
        "rationale": "Image captioning is a practical application of vision-language models, linking visual data to linguistic output.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "unidirectional masking mechanism",
      "forward propagation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Attention Distribution",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Semantic Representations",
      "resolved_canonical": "Semantic Representations",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Image Captioning",
      "resolved_canonical": "Image Captioning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Mitigating Hallucination in Large Vision-Language Models through Aligning Attention Distribution to Information Flow

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.14257.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2505.14257](https://arxiv.org/abs/2505.14257)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization_20250923|Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization]] (88.3% similar)
- [[2025-09-23/How Large Language Models are Designed to Hallucinate_20250923|How Large Language Models are Designed to Hallucinate]] (85.1% similar)
- [[2025-09-23/Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models_20250923|Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models]] (84.5% similar)
- [[2025-09-22/ORCA_ Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models_20250922|ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models]] (84.5% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Image Captioning|Image Captioning]]
**âš¡ Unique Technical**: [[keywords/Semantic Representations|Semantic Representations]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.14257v3 Announce Type: replace 
Abstract: Due to the unidirectional masking mechanism, Decoder-Only models propagate information from left to right. LVLMs (Large Vision-Language Models) follow the same architecture, with visual information gradually integrated into semantic representations during forward propagation. Through systematic analysis, we observe that the majority of the visual information is absorbed into the semantic representations. However, the model's attention distribution does not exhibit sufficient emphasis on semantic representations. This misalignment between the attention distribution and the actual information flow undermines the model's visual understanding ability and contributes to hallucinations. To address this issue, we enhance the model's visual understanding by leveraging the core information embedded in semantic representations. Specifically, we identify attention heads that focus on core semantic representations based on their attention distributions. Then, through a two-stage optimization paradigm, we propagate the advantages of these attention heads across the entire model, aligning the attention distribution with the actual information flow. We evaluate our method on three image captioning benchmarks using five different LVLMs, demonstrating its effectiveness in significantly reducing hallucinations. Further experiments reveal a trade-off between reduced hallucinations and richer details. Notably, our method allows for manual adjustment of the model's conservativeness, enabling flexible control to meet diverse real-world requirements.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Decoder-Only ëª¨ë¸ì˜ ë‹¨ë°©í–¥ ë§ˆìŠ¤í‚¹ ë©”ì»¤ë‹ˆì¦˜ì´ ì‹œê°ì  ì •ë³´ì˜ í†µí•©ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ëŒ€ë¶€ë¶„ì˜ ì‹œê°ì  ì •ë³´ëŠ” ì˜ë¯¸ í‘œí˜„ì— í¡ìˆ˜ë˜ì§€ë§Œ, ì£¼ì˜ ë¶„í¬ê°€ ì˜ë¯¸ í‘œí˜„ì— ì¶©ë¶„íˆ ì§‘ì¤‘í•˜ì§€ ì•Šì•„ ëª¨ë¸ì˜ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ ì €í•´í•˜ê³  í™˜ê° í˜„ìƒì„ ìœ ë°œí•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì˜ë¯¸ í‘œí˜„ì— ë‚´ì¬ëœ í•µì‹¬ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ ì‹œê°ì  ì´í•´ë¥¼ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì£¼ì˜ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ì˜ë¯¸ í‘œí˜„ì— ì§‘ì¤‘í•˜ëŠ” ì£¼ì˜ í—¤ë“œë¥¼ ì‹ë³„í•˜ê³ , ë‘ ë‹¨ê³„ ìµœì í™” íŒ¨ëŸ¬ë‹¤ì„ì„ í†µí•´ ëª¨ë¸ ì „ì²´ì— ì´ì ì„ ì „íŒŒí•˜ì—¬ ì£¼ì˜ ë¶„í¬ì™€ ì‹¤ì œ ì •ë³´ íë¦„ì„ ì •ë ¬í–ˆìŠµë‹ˆë‹¤. ì„¸ ê°€ì§€ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ì„¯ ê°€ì§€ LVLMì„ ì‚¬ìš©í•œ í‰ê°€ ê²°ê³¼, í™˜ê° í˜„ìƒì„ í¬ê²Œ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì‹¤í—˜ì—ì„œëŠ” í™˜ê° ê°ì†Œì™€ ì„¸ë¶€ ì •ë³´ í’ë¶€í•¨ ê°„ì˜ ê· í˜•ì„ í™•ì¸í–ˆìœ¼ë©°, ëª¨ë¸ì˜ ë³´ìˆ˜ì„±ì„ ìˆ˜ë™ ì¡°ì •í•˜ì—¬ ë‹¤ì–‘í•œ ìš”êµ¬ì‚¬í•­ì— ë§ì¶œ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Decoder-Only ëª¨ë¸ì€ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì •ë³´ë¥¼ ì „íŒŒí•˜ë©°, LVLMsë„ ê°™ì€ êµ¬ì¡°ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- 2. ì‹œê° ì •ë³´ê°€ ì ì§„ì ìœ¼ë¡œ ì˜ë¯¸ í‘œí˜„ì— í†µí•©ë˜ì§€ë§Œ, ì£¼ì˜ ë¶„í¬ê°€ ì˜ë¯¸ í‘œí˜„ì— ì¶©ë¶„íˆ ì§‘ì¤‘ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- 3. ì£¼ì˜ ë¶„í¬ì™€ ì‹¤ì œ ì •ë³´ íë¦„ì˜ ë¶ˆì¼ì¹˜ëŠ” ëª¨ë¸ì˜ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ ì €í•´í•˜ê³  í™˜ê°ì„ ìœ ë°œí•©ë‹ˆë‹¤.
- 4. í•µì‹¬ ì˜ë¯¸ í‘œí˜„ì— ì§‘ì¤‘í•˜ëŠ” ì£¼ì˜ í—¤ë“œë¥¼ ì‹ë³„í•˜ê³ , ì´ë¥¼ í†µí•´ ëª¨ë¸ ì „ì²´ì˜ ì£¼ì˜ ë¶„í¬ë¥¼ ì‹¤ì œ ì •ë³´ íë¦„ê³¼ ì •ë ¬í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ í™˜ê°ì„ ì¤„ì´ë©´ì„œë„ ëª¨ë¸ì˜ ë³´ìˆ˜ì„±ì„ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ìš”êµ¬ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:29:19*