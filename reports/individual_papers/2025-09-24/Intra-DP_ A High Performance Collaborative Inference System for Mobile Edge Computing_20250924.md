<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:36:43.139404",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mobile Edge Computing",
    "Neural Network",
    "Collaborative Inference",
    "Parallel Computing",
    "Energy-Efficient Inference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mobile Edge Computing": 0.78,
    "Neural Network": 0.8,
    "Collaborative Inference": 0.77,
    "Parallel Computing": 0.79,
    "Energy-Efficient Inference": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mobile Edge Computing",
        "canonical": "Mobile Edge Computing",
        "aliases": [
          "MEC"
        ],
        "category": "broad_technical",
        "rationale": "Mobile Edge Computing is a key concept for linking research on distributed computing and resource optimization in mobile environments.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "deep neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "DNN",
          "deep learning models"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are foundational to understanding and linking various deep learning approaches.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "collaborative inference",
        "canonical": "Collaborative Inference",
        "aliases": [
          "collaborative DNN inference"
        ],
        "category": "unique_technical",
        "rationale": "Collaborative Inference is a novel approach that enhances the efficiency of DNNs in resource-constrained environments.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "parallel computing technique",
        "canonical": "Parallel Computing",
        "aliases": [
          "parallel execution"
        ],
        "category": "specific_connectable",
        "rationale": "Parallel Computing is crucial for optimizing computational efficiency and is widely applicable across various domains.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "energy-efficient inference",
        "canonical": "Energy-Efficient Inference",
        "aliases": [
          "low-power inference"
        ],
        "category": "unique_technical",
        "rationale": "Energy-Efficient Inference is a significant advancement for sustainable AI deployment on mobile devices.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mobile Edge Computing",
      "resolved_canonical": "Mobile Edge Computing",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "deep neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "collaborative inference",
      "resolved_canonical": "Collaborative Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "parallel computing technique",
      "resolved_canonical": "Parallel Computing",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "energy-efficient inference",
      "resolved_canonical": "Energy-Efficient Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.05829.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2507.05829](https://arxiv.org/abs/2507.05829)

## 🔗 유사한 논문
- [[2025-09-23/Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers_20250923|Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers]] (83.8% similar)
- [[2025-09-23/Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference_20250923|Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference]] (83.7% similar)
- [[2025-09-23/An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation_20250923|An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation]] (82.7% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (82.6% similar)
- [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (82.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Mobile Edge Computing|Mobile Edge Computing]], [[keywords/Neural Network|Neural Network]]
**🔗 Specific Connectable**: [[keywords/Parallel Computing|Parallel Computing]]
**⚡ Unique Technical**: [[keywords/Collaborative Inference|Collaborative Inference]], [[keywords/Energy-Efficient Inference|Energy-Efficient Inference]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2507.05829v2 Announce Type: replace-cross 
Abstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices presents significant challenges, particularly in achieving real-time performance while simultaneously coping with limited computational resources and battery life. While Mobile Edge Computing (MEC) offers collaborative inference with GPU servers as a promising solution, existing approaches primarily rely on layer-wise model partitioning and undergo significant transmission bottlenecks caused by the sequential execution of DNN operations. To address this challenge, we present Intra-DP, a high-performance collaborative inference system optimized for DNN inference on MEC. Intra DP employs a novel parallel computing technique based on local operators (i.e., operators whose minimum unit input is not the entire input tensor, such as the convolution kernel). By decomposing their computations (operations) into several independent sub-operations and overlapping the computation and transmission of different sub-operations through parallel execution, Intra-DP mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient inference. The evaluation demonstrates that Intra-DP reduces per-inference latency by up to 50% and energy consumption by up to 75% compared to state-of-the-art baselines, without sacrificing accuracy.

## 📝 요약

이 논문은 자원이 제한된 모바일 기기에서 딥러닝 모델을 실시간으로 실행하는 문제를 해결하기 위해 Intra-DP라는 협력적 추론 시스템을 제안합니다. 기존의 모바일 엣지 컴퓨팅(MEC) 방식은 계층별 모델 분할에 의존하여 전송 병목 현상이 발생하는 반면, Intra-DP는 로컬 연산자 기반의 병렬 컴퓨팅 기법을 사용하여 이러한 문제를 해결합니다. 이를 통해 여러 독립적인 하위 연산으로 분해하고, 병렬 실행을 통해 연산과 전송을 중첩시켜 전송 병목을 완화합니다. 실험 결과, Intra-DP는 최신 기법 대비 추론 지연 시간을 최대 50%, 에너지 소비를 최대 75%까지 줄이면서도 정확성을 유지합니다.

## 🎯 주요 포인트

- 1. Intra-DP는 MEC에서 DNN 추론을 최적화한 고성능 협력 추론 시스템입니다.
- 2. Intra-DP는 로컬 연산자 기반의 새로운 병렬 컴퓨팅 기법을 사용하여 전송 병목 현상을 완화합니다.
- 3. Intra-DP는 독립적인 서브 연산으로 계산을 분해하고 병렬 실행을 통해 계산과 전송을 중첩시킵니다.
- 4. 평가 결과, Intra-DP는 최신 기준 대비 추론 지연 시간을 최대 50% 줄이고 에너지 소비를 최대 75% 절감합니다.
- 5. Intra-DP는 정확성을 유지하면서 빠르고 에너지 효율적인 추론을 달성합니다.


---

*Generated on 2025-09-24 14:36:43*