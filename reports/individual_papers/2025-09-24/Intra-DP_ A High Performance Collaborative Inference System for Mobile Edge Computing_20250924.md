<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:36:43.139404",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mobile Edge Computing",
    "Neural Network",
    "Collaborative Inference",
    "Parallel Computing",
    "Energy-Efficient Inference"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mobile Edge Computing": 0.78,
    "Neural Network": 0.8,
    "Collaborative Inference": 0.77,
    "Parallel Computing": 0.79,
    "Energy-Efficient Inference": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mobile Edge Computing",
        "canonical": "Mobile Edge Computing",
        "aliases": [
          "MEC"
        ],
        "category": "broad_technical",
        "rationale": "Mobile Edge Computing is a key concept for linking research on distributed computing and resource optimization in mobile environments.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "deep neural networks",
        "canonical": "Neural Network",
        "aliases": [
          "DNN",
          "deep learning models"
        ],
        "category": "broad_technical",
        "rationale": "Neural Networks are foundational to understanding and linking various deep learning approaches.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      },
      {
        "surface": "collaborative inference",
        "canonical": "Collaborative Inference",
        "aliases": [
          "collaborative DNN inference"
        ],
        "category": "unique_technical",
        "rationale": "Collaborative Inference is a novel approach that enhances the efficiency of DNNs in resource-constrained environments.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "parallel computing technique",
        "canonical": "Parallel Computing",
        "aliases": [
          "parallel execution"
        ],
        "category": "specific_connectable",
        "rationale": "Parallel Computing is crucial for optimizing computational efficiency and is widely applicable across various domains.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.79
      },
      {
        "surface": "energy-efficient inference",
        "canonical": "Energy-Efficient Inference",
        "aliases": [
          "low-power inference"
        ],
        "category": "unique_technical",
        "rationale": "Energy-Efficient Inference is a significant advancement for sustainable AI deployment on mobile devices.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mobile Edge Computing",
      "resolved_canonical": "Mobile Edge Computing",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "deep neural networks",
      "resolved_canonical": "Neural Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "collaborative inference",
      "resolved_canonical": "Collaborative Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "parallel computing technique",
      "resolved_canonical": "Parallel Computing",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "energy-efficient inference",
      "resolved_canonical": "Energy-Efficient Inference",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2507.05829.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2507.05829](https://arxiv.org/abs/2507.05829)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers_20250923|Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers]] (83.8% similar)
- [[2025-09-23/Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference_20250923|Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference]] (83.7% similar)
- [[2025-09-23/An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation_20250923|An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation]] (82.7% similar)
- [[2025-09-23/PDTrim_ Targeted Pruning for Prefill-Decode Disaggregation in Inference_20250923|PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference]] (82.6% similar)
- [[2025-09-19/eIQ Neutron_ Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations_20250919|eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Mobile Edge Computing|Mobile Edge Computing]], [[keywords/Neural Network|Neural Network]]
**ğŸ”— Specific Connectable**: [[keywords/Parallel Computing|Parallel Computing]]
**âš¡ Unique Technical**: [[keywords/Collaborative Inference|Collaborative Inference]], [[keywords/Energy-Efficient Inference|Energy-Efficient Inference]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.05829v2 Announce Type: replace-cross 
Abstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices presents significant challenges, particularly in achieving real-time performance while simultaneously coping with limited computational resources and battery life. While Mobile Edge Computing (MEC) offers collaborative inference with GPU servers as a promising solution, existing approaches primarily rely on layer-wise model partitioning and undergo significant transmission bottlenecks caused by the sequential execution of DNN operations. To address this challenge, we present Intra-DP, a high-performance collaborative inference system optimized for DNN inference on MEC. Intra DP employs a novel parallel computing technique based on local operators (i.e., operators whose minimum unit input is not the entire input tensor, such as the convolution kernel). By decomposing their computations (operations) into several independent sub-operations and overlapping the computation and transmission of different sub-operations through parallel execution, Intra-DP mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient inference. The evaluation demonstrates that Intra-DP reduces per-inference latency by up to 50% and energy consumption by up to 75% compared to state-of-the-art baselines, without sacrificing accuracy.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìì›ì´ ì œí•œëœ ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Intra-DPë¼ëŠ” í˜‘ë ¥ì  ì¶”ë¡  ì‹œìŠ¤í…œì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ëª¨ë°”ì¼ ì—£ì§€ ì»´í“¨íŒ…(MEC) ë°©ì‹ì€ ê³„ì¸µë³„ ëª¨ë¸ ë¶„í• ì— ì˜ì¡´í•˜ì—¬ ì „ì†¡ ë³‘ëª© í˜„ìƒì´ ë°œìƒí•˜ëŠ” ë°˜ë©´, Intra-DPëŠ” ë¡œì»¬ ì—°ì‚°ì ê¸°ë°˜ì˜ ë³‘ë ¬ ì»´í“¨íŒ… ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ ë…ë¦½ì ì¸ í•˜ìœ„ ì—°ì‚°ìœ¼ë¡œ ë¶„í•´í•˜ê³ , ë³‘ë ¬ ì‹¤í–‰ì„ í†µí•´ ì—°ì‚°ê³¼ ì „ì†¡ì„ ì¤‘ì²©ì‹œì¼œ ì „ì†¡ ë³‘ëª©ì„ ì™„í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, Intra-DPëŠ” ìµœì‹  ê¸°ë²• ëŒ€ë¹„ ì¶”ë¡  ì§€ì—° ì‹œê°„ì„ ìµœëŒ€ 50%, ì—ë„ˆì§€ ì†Œë¹„ë¥¼ ìµœëŒ€ 75%ê¹Œì§€ ì¤„ì´ë©´ì„œë„ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Intra-DPëŠ” MECì—ì„œ DNN ì¶”ë¡ ì„ ìµœì í™”í•œ ê³ ì„±ëŠ¥ í˜‘ë ¥ ì¶”ë¡  ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- 2. Intra-DPëŠ” ë¡œì»¬ ì—°ì‚°ì ê¸°ë°˜ì˜ ìƒˆë¡œìš´ ë³‘ë ¬ ì»´í“¨íŒ… ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì „ì†¡ ë³‘ëª© í˜„ìƒì„ ì™„í™”í•©ë‹ˆë‹¤.
- 3. Intra-DPëŠ” ë…ë¦½ì ì¸ ì„œë¸Œ ì—°ì‚°ìœ¼ë¡œ ê³„ì‚°ì„ ë¶„í•´í•˜ê³  ë³‘ë ¬ ì‹¤í–‰ì„ í†µí•´ ê³„ì‚°ê³¼ ì „ì†¡ì„ ì¤‘ì²©ì‹œí‚µë‹ˆë‹¤.
- 4. í‰ê°€ ê²°ê³¼, Intra-DPëŠ” ìµœì‹  ê¸°ì¤€ ëŒ€ë¹„ ì¶”ë¡  ì§€ì—° ì‹œê°„ì„ ìµœëŒ€ 50% ì¤„ì´ê³  ì—ë„ˆì§€ ì†Œë¹„ë¥¼ ìµœëŒ€ 75% ì ˆê°í•©ë‹ˆë‹¤.
- 5. Intra-DPëŠ” ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë¹ ë¥´ê³  ì—ë„ˆì§€ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:36:43*