<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:02:21.993500",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Attention Mechanism",
    "Feed-Forward Network",
    "Supervised Fine-tuning",
    "Contextual Knowledge"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Attention Mechanism": 0.78,
    "Feed-Forward Network": 0.72,
    "Supervised Fine-tuning": 0.8,
    "Contextual Knowledge": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on context length and fine-tuning.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multi-Head Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "MHA",
          "Multi-Head Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Key component analyzed for its role in context length effects.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Feed-Forward Network",
        "canonical": "Feed-Forward Network",
        "aliases": [
          "FFN"
        ],
        "category": "unique_technical",
        "rationale": "Analyzed independently for its contribution to model behavior.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Supervised Fine-tuning",
        "canonical": "Supervised Fine-tuning",
        "aliases": [
          "SFT"
        ],
        "category": "unique_technical",
        "rationale": "Central to the study of context length effects on model performance.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.8
      },
      {
        "surface": "Contextual Knowledge",
        "canonical": "Contextual Knowledge",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Describes a key finding related to knowledge preference bias in the study.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "short-context tasks",
      "real-world applications",
      "knowledge preference bias"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multi-Head Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Feed-Forward Network",
      "resolved_canonical": "Feed-Forward Network",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Supervised Fine-tuning",
      "resolved_canonical": "Supervised Fine-tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Contextual Knowledge",
      "resolved_canonical": "Contextual Knowledge",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18762.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18762](https://arxiv.org/abs/2509.18762)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels_20250923|Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels]] (86.7% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (86.3% similar)
- [[2025-09-22/LiteLong_ Resource-Efficient Long-Context Data Synthesis for LLMs_20250922|LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs]] (86.0% similar)
- [[2025-09-23/Language Modeling with Learned Meta-Tokens_20250923|Language Modeling with Learned Meta-Tokens]] (85.5% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Feed-Forward Network|Feed-Forward Network]], [[keywords/Supervised Fine-tuning|Supervised Fine-tuning]], [[keywords/Contextual Knowledge|Contextual Knowledge]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18762v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved impressive performance across natural language processing (NLP) tasks. As real-world applications increasingly demand longer context windows, continued pretraining and supervised fine-tuning (SFT) on long-context data has become a common approach. While the effects of data length in continued pretraining have been extensively studied, their implications for SFT remain unclear. In this work, we systematically investigate how SFT data length influences LLM behavior on short-context tasks. Counterintuitively, we find that long-context SFT improves short-context performance, contrary to the commonly observed degradation from long-context pretraining. To uncover the underlying mechanisms of this phenomenon, we first decouple and analyze two key components, Multi-Head Attention (MHA) and Feed-Forward Network (FFN), and show that both independently benefit from long-context SFT. We further study their interaction and reveal a knowledge preference bias: long-context SFT promotes contextual knowledge, while short-context SFT favors parametric knowledge, making exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that hybrid training mitigates this bias, offering explainable guidance for fine-tuning LLMs.

## ğŸ“ ìš”ì•½

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ê¸´ ë¬¸ë§¥ ì°½ì„ ìš”êµ¬í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„, ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì§€ì†ì  ì‚¬ì „ í•™ìŠµê³¼ ì§€ë„ ë¯¸ì„¸ ì¡°ì •(SFT)ì´ ì¼ë°˜ì ì¸ ì ‘ê·¼ ë°©ì‹ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” SFT ë°ì´í„° ê¸¸ì´ê°€ LLMì˜ ì§§ì€ ë¬¸ë§¥ ì‘ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ë†€ëê²Œë„, ê¸´ ë¬¸ë§¥ SFTê°€ ì§§ì€ ë¬¸ë§¥ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²°ê³¼ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ í˜„ìƒì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë°íˆê¸° ìœ„í•´, ë‹¤ì¤‘ í—¤ë“œ ì£¼ì˜(MHA)ì™€ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(FFN)ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë¶„ì„í•œ ê²°ê³¼, ë‘ ìš”ì†Œ ëª¨ë‘ ê¸´ ë¬¸ë§¥ SFTì—ì„œ ì´ì ì„ ì–»ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë˜í•œ, ê¸´ ë¬¸ë§¥ SFTëŠ” ë¬¸ë§¥ì  ì§€ì‹ì„ ì´‰ì§„í•˜ê³ , ì§§ì€ ë¬¸ë§¥ SFTëŠ” ë§¤ê°œë³€ìˆ˜ì  ì§€ì‹ì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë°í˜”ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í•˜ì´ë¸Œë¦¬ë“œ í›ˆë ¨ì´ ì´ëŸ¬í•œ í¸í–¥ì„ ì™„í™”í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.
- 2. ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ê¸´ ë¬¸ë§¥ ì°½ì„ ìš”êµ¬í•˜ë©°, ì´ë¥¼ ìœ„í•´ ê¸´ ë¬¸ë§¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì§€ì†ì  ì‚¬ì „ í›ˆë ¨ê³¼ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT)ì´ ì¼ë°˜ì ì¸ ì ‘ê·¼ ë°©ì‹ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤.
- 3. ì—°êµ¬ ê²°ê³¼, ê¸´ ë¬¸ë§¥ SFTê°€ ì§§ì€ ë¬¸ë§¥ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê¸´ ë¬¸ë§¥ ì‚¬ì „ í›ˆë ¨ì—ì„œ ê´€ì°°ë˜ëŠ” ì„±ëŠ¥ ì €í•˜ì™€ëŠ” ë°˜ëŒ€ë˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.
- 4. Multi-Head Attention(MHA)ì™€ Feed-Forward Network(FFN) ëª¨ë‘ ê¸´ ë¬¸ë§¥ SFTë¡œë¶€í„° ë…ë¦½ì ìœ¼ë¡œ ì´ì ì„ ì–»ëŠ” ê²ƒìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. í•˜ì´ë¸Œë¦¬ë“œ í›ˆë ¨ì´ ì§€ì‹ ì„ í˜¸ í¸í–¥ì„ ì™„í™”í•˜ì—¬ LLMì˜ ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ ì„¤ëª… ê°€ëŠ¥í•œ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:02:21*