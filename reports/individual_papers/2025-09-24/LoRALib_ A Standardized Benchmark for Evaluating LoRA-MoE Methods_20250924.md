<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:37:05.708820",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Low-Rank Adaptation",
    "Mixture of Experts",
    "LoRA-MoE Methods",
    "OpenCompass Testing Tool"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Low-Rank Adaptation": 0.8,
    "Mixture of Experts": 0.78,
    "LoRA-MoE Methods": 0.82,
    "OpenCompass Testing Tool": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "LoRA",
        "canonical": "Low-Rank Adaptation",
        "aliases": [
          "LoRA"
        ],
        "category": "unique_technical",
        "rationale": "LoRA is a key method discussed in the paper, offering a unique approach to parameter-efficient fine-tuning.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "MoE",
        "canonical": "Mixture of Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "MoE is a significant concept that enhances model adaptability, making it a strong candidate for linking.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "LoRA-MoE",
        "canonical": "LoRA-MoE Methods",
        "aliases": [
          "LoRA-MoE"
        ],
        "category": "unique_technical",
        "rationale": "Combining LoRA with MoE represents a novel approach that the paper focuses on, warranting a unique technical category.",
        "novelty_score": 0.88,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "OpenCompass",
        "canonical": "OpenCompass Testing Tool",
        "aliases": [
          "OpenCompass"
        ],
        "category": "unique_technical",
        "rationale": "OpenCompass is a specific tool used in the paper's experiments, relevant for linking technical implementations.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "benchmark",
      "datasets",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "LoRA",
      "resolved_canonical": "Low-Rank Adaptation",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "MoE",
      "resolved_canonical": "Mixture of Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LoRA-MoE",
      "resolved_canonical": "LoRA-MoE Methods",
      "decision": "linked",
      "scores": {
        "novelty": 0.88,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "OpenCompass",
      "resolved_canonical": "OpenCompass Testing Tool",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18137.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18137](https://arxiv.org/abs/2509.18137)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA_20250923|Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA]] (89.0% similar)
- [[2025-09-23/GuiLoMo_ Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors_20250923|GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors]] (87.1% similar)
- [[2025-09-23/TASO_ Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation_20250923|TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation]] (85.7% similar)
- [[2025-09-23/RefLoRA_ Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models_20250923|RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models]] (85.5% similar)
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (84.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Mixture of Experts|Mixture of Experts]]
**âš¡ Unique Technical**: [[keywords/Low-Rank Adaptation|Low-Rank Adaptation]], [[keywords/LoRA-MoE Methods|LoRA-MoE Methods]], [[keywords/OpenCompass Testing Tool|OpenCompass Testing Tool]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18137v1 Announce Type: cross 
Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation (LoRA) can save significant costs in storage and computing, but its strong adaptability to a single task is often accompanied by insufficient cross-task generalization capabilities. To improve this, existing work combines LoRA with mixture-of-experts (MoE) to enhance the model's adaptability through expert modules and routing mechanisms. However, existing LoRA-MoE methods lack unified standards in models, datasets, hyperparameters, and evaluation methods, making it difficult to conduct fair comparisons between different methods. To this end, we proposed a unified benchmark named LoRALib. Specifically, we standardized datasets from $40$ downstream tasks into a unified format, fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules across $17$ model architectures. Based on this LoRA library, we conduct large-scale experiments on $3$ representative LoRA-MoE methods and different LoRA selection mechanisms using the open-sourced testing tool OpenCompass. Extensive experiments show that LoRAMoE performs best, and that prioritizing LoRAs relevant to the target task can further improve the performance of MoE. We hope these findings will inspire future work. Our datasets and LoRA library are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset and https://huggingface.co/YaoLuzjut/models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì¸ ì €ìˆœìœ„ ì ì‘(LoRA)ì˜ ë‹¨ì¼ ì‘ì—… ì ì‘ë ¥ì€ ë†’ì§€ë§Œ, ì‘ì—… ê°„ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LoRAì™€ ì „ë¬¸ê°€ í˜¼í•©(MoE)ì„ ê²°í•©í•œ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ LoRALibë¼ëŠ” í†µí•© ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí•˜ì—¬, 40ê°œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ 17ê°œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ 680ê°œì˜ LoRA ëª¨ë“ˆì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ ì‹¤í—˜ ê²°ê³¼, LoRAMoEê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ëª©í‘œ ì‘ì—…ê³¼ ê´€ë ¨ëœ LoRAë¥¼ ìš°ì„ ì‹œí•  ë•Œ MoEì˜ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” í–¥í›„ ì—°êµ¬ì— ì˜ê°ì„ ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. ë°ì´í„°ì…‹ê³¼ LoRA ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LoRAëŠ” ì €ì¥ ë° ê³„ì‚° ë¹„ìš©ì„ ì ˆê°í•˜ëŠ” íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ ì¡°ì •(PEFT) ë°©ë²•ì´ì§€ë§Œ, ë‹¨ì¼ ì‘ì—…ì— ëŒ€í•œ ì ì‘ë ¥ì´ ê°•í•œ ë°˜ë©´, ì‘ì—… ê°„ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ëŠ” LoRAì™€ ì „ë¬¸ê°€ ëª¨ë“ˆ ë° ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•œ mixture-of-experts (MoE)ë¥¼ ê²°í•©í•˜ì—¬ ëª¨ë¸ì˜ ì ì‘ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ ì í•œë‹¤.
- 3. ë‹¤ì–‘í•œ LoRA-MoE ë°©ë²• ê°„ì˜ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ ëª¨ë¸, ë°ì´í„°ì…‹, í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° í‰ê°€ ë°©ë²•ì— ëŒ€í•œ í†µì¼ëœ ê¸°ì¤€ì´ í•„ìš”í•˜ë‹¤.
- 4. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” 40ê°œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ í‘œì¤€í™”í•˜ê³ , ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ 17ê°œì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ 680ê°œì˜ LoRA ëª¨ë“ˆì„ ì–»ëŠ” LoRALib ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆí–ˆë‹¤.
- 5. ëŒ€ê·œëª¨ ì‹¤í—˜ ê²°ê³¼, LoRAMoEê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ëª©í‘œ ì‘ì—…ê³¼ ê´€ë ¨ëœ LoRAë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•˜ë©´ MoEì˜ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤.


---

*Generated on 2025-09-24 13:37:05*