<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:24:39.924458",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Transformer",
    "LongLLaVA",
    "Vision-Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.8,
    "Transformer": 0.85,
    "LongLLaVA": 0.7,
    "Vision-Language Model": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multi-modal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to ongoing research in integrating multiple data types in AI models.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Transformer blocks",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A foundational architecture in modern deep learning models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "LongLLaVA",
        "canonical": "LongLLaVA",
        "aliases": [
          "Long-Context Large Language and Vision Assistant"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel model architecture with specific capabilities in processing large image sets.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "video understanding",
        "canonical": "Vision-Language Model",
        "aliases": [],
        "category": "evolved_concepts",
        "rationale": "Highlights the integration of vision and language for comprehensive media analysis.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "performance degradation",
      "high computational costs"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multi-modal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Transformer blocks",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "LongLLaVA",
      "resolved_canonical": "LongLLaVA",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "video understanding",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2409.02889.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2409.02889](https://arxiv.org/abs/2409.02889)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (85.9% similar)
- [[2025-09-24/OptMerge_ Unifying Multimodal LLM Capabilities and Modalities via Model Merging_20250924|OptMerge: Unifying Multimodal LLM Capabilities and Modalities via Model Merging]] (85.7% similar)
- [[2025-09-24/LCMF_ Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA_20250924|LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA]] (85.7% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (85.5% similar)
- [[2025-09-22/MANZANO_ A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer_20250922|MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer]] (85.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/LongLLaVA|LongLLaVA]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2409.02889v3 Announce Type: replace-cross 
Abstract: Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is critical for advancing video understanding and high-resolution image analysis. Achieving this requires systematic improvements in model architecture, data construction, and training strategies, particularly to address challenges such as performance degradation with increasing image counts and high computational costs. In this paper, we propose a hybrid architecture that integrates Mamba and Transformer blocks, introduce data construction methods that capture both temporal and spatial dependencies, and employ a progressive training strategy. Our released model, LongLLaVA (\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant), demonstrates an effective balance between efficiency and performance. LongLLaVA achieves competitive results across various benchmarks while maintaining high throughput and low memory consumption. Notably, it can process nearly one thousand images on a single A100 80GB GPU, underscoring its potential for a wide range of multi-modal applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ì¥ê¸° ë¬¸ë§¥ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í™•ì¥í•˜ì—¬ ë¹„ë””ì˜¤ ì´í•´ì™€ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë¶„ì„ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ Mambaì™€ Transformer ë¸”ë¡ì„ í†µí•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ê³ , ì‹œê°„ì  ë° ê³µê°„ì  ì˜ì¡´ì„±ì„ í¬ì°©í•˜ëŠ” ë°ì´í„° êµ¬ì„± ë°©ë²•ê³¼ ì ì§„ì  í•™ìŠµ ì „ëµì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ëª¨ë¸ì¸ LongLLaVAëŠ” íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì„ ì´ë£¨ë©°, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, A100 80GB GPUì—ì„œ ê±°ì˜ ì²œ ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì‘ìš©ì— ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MLLMsì˜ ì¥ê¸° ë¬¸ë§¥ ì²˜ë¦¬ ëŠ¥ë ¥ í™•ì¥ì€ ë¹„ë””ì˜¤ ì´í•´ì™€ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë¶„ì„ ë°œì „ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.
- 2. ì œì•ˆëœ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ëŠ” Mambaì™€ Transformer ë¸”ë¡ì„ í†µí•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 3. ë°ì´í„° êµ¬ì¶• ë°©ë²•ì€ ì‹œê°„ì  ë° ê³µê°„ì  ì˜ì¡´ì„±ì„ í¬ì°©í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. LongLLaVA ëª¨ë¸ì€ ë†’ì€ ì²˜ë¦¬ëŸ‰ê³¼ ë‚®ì€ ë©”ëª¨ë¦¬ ì†Œë¹„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 5. LongLLaVAëŠ” ë‹¨ì¼ A100 80GB GPUì—ì„œ ê±°ì˜ ì²œ ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:24:39*