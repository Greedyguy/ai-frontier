<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:09:50.244734",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Machine Learning",
    "Proximal Policy Optimization",
    "Reward Function Mutation",
    "Robotic Skill Diversification"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Machine Learning": 0.85,
    "Proximal Policy Optimization": 0.8,
    "Reward Function Mutation": 0.75,
    "Robotic Skill Diversification": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a key method used in the study, linking it to the broader field of Machine Learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Proximal Policy Optimization",
        "canonical": "Proximal Policy Optimization",
        "aliases": [
          "PPO"
        ],
        "category": "specific_connectable",
        "rationale": "PPO is a specific algorithm used in the study, which can connect to other works using similar reinforcement learning techniques.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Reward Function Mutation",
        "canonical": "Reward Function Mutation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This concept is central to the study's methodology and represents a novel approach within reinforcement learning.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      },
      {
        "surface": "Robotic Skill Diversification",
        "canonical": "Robotic Skill Diversification",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The study's main contribution is the diversification of robotic skills, which is a unique aspect of the research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "simulation environment",
      "Franka Emika Panda",
      "Gaussian noise"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Proximal Policy Optimization",
      "resolved_canonical": "Proximal Policy Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Reward Function Mutation",
      "resolved_canonical": "Reward Function Mutation",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Robotic Skill Diversification",
      "resolved_canonical": "Robotic Skill Diversification",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18463.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18463](https://arxiv.org/abs/2509.18463)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/reWordBench_ Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs_20250922|reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs]] (82.3% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (82.1% similar)
- [[2025-09-22/Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery_20250922|Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery]] (82.0% similar)
- [[2025-09-23/The Sound of Simulation_ Learning Multimodal Sim-to-Real Robot Policies with Generative Audio_20250923|The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio]] (81.7% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Machine Learning|Machine Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Proximal Policy Optimization|Proximal Policy Optimization]]
**âš¡ Unique Technical**: [[keywords/Reward Function Mutation|Reward Function Mutation]], [[keywords/Robotic Skill Diversification|Robotic Skill Diversification]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18463v1 Announce Type: cross 
Abstract: This paper explores how deliberate mutations of reward function in reinforcement learning can produce diversified skill variations in robotic manipulation tasks, examined with a liquid pouring use case. To this end, we developed a new reward function mutation framework that is based on applying Gaussian noise to the weights of the different terms in the reward function. Inspired by the cost-benefit tradeoff model from human motor control, we designed the reward function with the following key terms: accuracy, time, and effort. The study was performed in a simulation environment created in NVIDIA Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a glass with a liquid that needed to be poured into a container. The reinforcement learning algorithm was based on Proximal Policy Optimization. We systematically explored how different configurations of mutated weights in the rewards function would affect the learned policy. The resulting policies exhibit a wide range of behaviours: from variations in execution of the originally intended pouring task to novel skills useful for unexpected tasks, such as container rim cleaning, liquid mixing, and watering. This approach offers promising directions for robotic systems to perform diversified learning of specific tasks, while also potentially deriving meaningful skills for future tasks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì—ì„œ ë³´ìƒ í•¨ìˆ˜ì˜ ì˜ë„ì ì¸ ë³€ì´ë¥¼ í†µí•´ ë¡œë´‡ ì¡°ì‘ ì‘ì—…ì—ì„œ ë‹¤ì–‘í•œ ê¸°ìˆ  ë³€í™”ë¥¼ ìœ ë„í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë³´ìƒ í•¨ìˆ˜ì˜ ë‹¤ì–‘í•œ í•­ëª© ê°€ì¤‘ì¹˜ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë³´ìƒ í•¨ìˆ˜ ë³€ì´ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì¸ê°„ ìš´ë™ ì œì–´ì˜ ë¹„ìš©-í¸ìµ ëª¨ë¸ì—ì„œ ì˜ê°ì„ ë°›ì•„ ì •í™•ì„±, ì‹œê°„, ë…¸ë ¥ì´ë¼ëŠ” í•µì‹¬ í•­ëª©ìœ¼ë¡œ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. NVIDIA Isaac Simì˜ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ í”„ë‘ì¹´ ì—ë¯¸ì¹´ íŒë‹¤ ë¡œë´‡ íŒ”ì„ ì‚¬ìš©í•˜ì—¬ ì•¡ì²´ë¥¼ ìš©ê¸°ì— ë¶“ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. Proximal Policy Optimization ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ, ë³€ì´ëœ ê°€ì¤‘ì¹˜ êµ¬ì„±ì— ë”°ë¼ í•™ìŠµëœ ì •ì±…ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íƒêµ¬í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì›ë˜ ì˜ë„ëœ ë¶“ê¸° ì‘ì—…ì˜ ì‹¤í–‰ ë³€í˜•ë¶€í„° ìš©ê¸° ê°€ì¥ìë¦¬ ì²­ì†Œ, ì•¡ì²´ í˜¼í•©, ë¬¼ì£¼ê¸°ì™€ ê°™ì€ ìƒˆë¡œìš´ ê¸°ìˆ ê¹Œì§€ ë‹¤ì–‘í•œ í–‰ë™ì´ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë¡œë´‡ ì‹œìŠ¤í…œì´ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ë‹¤ì–‘í™”ëœ í•™ìŠµì„ ìˆ˜í–‰í•˜ê³ , ë¯¸ë˜ ì‘ì—…ì— ìœ ìš©í•œ ì˜ë¯¸ ìˆëŠ” ê¸°ìˆ ì„ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµì—ì„œ ë³´ìƒ í•¨ìˆ˜ì˜ ì˜ë„ì ì¸ ë³€ì´ê°€ ë¡œë´‡ ì¡°ì‘ ì‘ì—…ì—ì„œ ë‹¤ì–‘í•œ ê¸°ìˆ  ë³€í™”ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒì„ íƒêµ¬í–ˆìŠµë‹ˆë‹¤.
- 2. ë³´ìƒ í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë³´ìƒ í•¨ìˆ˜ ë³€ì´ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- 3. ë³´ìƒ í•¨ìˆ˜ëŠ” ì¸ê°„ ìš´ë™ ì œì–´ì˜ ë¹„ìš©-ì´ìµ ëª¨ë¸ì—ì„œ ì˜ê°ì„ ë°›ì•„ ì •í™•ì„±, ì‹œê°„, ë…¸ë ¥ì˜ ì£¼ìš” í•­ëª©ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- 4. ì—°êµ¬ëŠ” NVIDIA Isaac Simì˜ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, Franka Emika Panda ë¡œë´‡ íŒ”ì´ ì•¡ì²´ë¥¼ ìš©ê¸°ì— ë¶“ëŠ” ì‘ì—…ì„ í¬í•¨í–ˆìŠµë‹ˆë‹¤.
- 5. ë³€í˜•ëœ ë³´ìƒ í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ êµ¬ì„±ì´ í•™ìŠµëœ ì •ì±…ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íƒêµ¬í•˜ì—¬, ë‹¤ì–‘í•œ ìƒˆë¡œìš´ ê¸°ìˆ ì„ ê°œë°œí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:09:50*