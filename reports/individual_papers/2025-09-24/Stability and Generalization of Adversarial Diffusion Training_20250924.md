<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:02:40.287236",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Adversarial Training",
    "Algorithmic Stability",
    "Generalization Gap",
    "Decentralized Networks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Adversarial Training": 0.82,
    "Algorithmic Stability": 0.78,
    "Generalization Gap": 0.8,
    "Decentralized Networks": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Adversarial Training",
        "canonical": "Adversarial Training",
        "aliases": [
          "Robust Training"
        ],
        "category": "specific_connectable",
        "rationale": "Adversarial Training is a key concept in enhancing model robustness and is central to the paper's analysis.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Algorithmic Stability",
        "canonical": "Algorithmic Stability",
        "aliases": [
          "Stability Analysis"
        ],
        "category": "unique_technical",
        "rationale": "Algorithmic Stability is crucial for understanding generalization in adversarial contexts, providing a novel perspective in the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Generalization Gap",
        "canonical": "Generalization Gap",
        "aliases": [
          "Generalization Error"
        ],
        "category": "specific_connectable",
        "rationale": "The Generalization Gap is a significant challenge in adversarial training, linking to broader discussions on model performance.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Decentralized Networks",
        "canonical": "Decentralized Networks",
        "aliases": [
          "Distributed Networks"
        ],
        "category": "unique_technical",
        "rationale": "Decentralized Networks are a novel setting for adversarial training, expanding the scope of traditional analyses.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "Convergence",
      "Training Steps"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Adversarial Training",
      "resolved_canonical": "Adversarial Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Algorithmic Stability",
      "resolved_canonical": "Algorithmic Stability",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Generalization Gap",
      "resolved_canonical": "Generalization Gap",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Decentralized Networks",
      "resolved_canonical": "Decentralized Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Stability and Generalization of Adversarial Diffusion Training

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19234.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19234](https://arxiv.org/abs/2509.19234)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Adversarial generalization of unfolding (model-based) networks_20250922|Adversarial generalization of unfolding (model-based) networks]] (85.0% similar)
- [[2025-09-23/Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks_20250923|Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks]] (82.5% similar)
- [[2025-09-24/Algorithms for Adversarially Robust Deep Learning_20250924|Algorithms for Adversarially Robust Deep Learning]] (81.3% similar)
- [[2025-09-24/A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization_20250924|A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization]] (80.8% similar)
- [[2025-09-24/Latent Danger Zone_ Distilling Unified Attention for Cross-Architecture Black-box Attacks_20250924|Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks]] (80.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Adversarial Training|Adversarial Training]], [[keywords/Generalization Gap|Generalization Gap]]
**âš¡ Unique Technical**: [[keywords/Algorithmic Stability|Algorithmic Stability]], [[keywords/Decentralized Networks|Decentralized Networks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19234v1 Announce Type: new 
Abstract: Algorithmic stability is an established tool for analyzing generalization. While adversarial training enhances model robustness, it often suffers from robust overfitting and an enlarged generalization gap. Although recent work has established the convergence of adversarial training in decentralized networks, its generalization properties remain unexplored. This work presents a stability-based generalization analysis of adversarial training under the diffusion strategy for convex losses. We derive a bound showing that the generalization error grows with both the adversarial perturbation strength and the number of training steps, a finding consistent with single-agent case but novel for decentralized settings. Numerical experiments on logistic regression validate these theoretical predictions.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¶„ì‚° ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ ì ëŒ€ì  í›ˆë ¨ì˜ ì¼ë°˜í™” íŠ¹ì„±ì„ ì•ˆì •ì„± ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. íŠ¹íˆ, ë³¼ë¡ ì†ì‹¤ í•¨ìˆ˜ì— ëŒ€í•œ í™•ì‚° ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™” ì˜¤ë¥˜ê°€ ì ëŒ€ì  êµë€ ê°•ë„ì™€ í›ˆë ¨ ë‹¨ê³„ ìˆ˜ì— ë”°ë¼ ì¦ê°€í•œë‹¤ëŠ” ê²½ê³„ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¨ì¼ ì—ì´ì „íŠ¸ ê²½ìš°ì™€ ì¼ì¹˜í•˜ë©´ì„œë„ ë¶„ì‚° í™˜ê²½ì—ì„œëŠ” ìƒˆë¡œìš´ ë°œê²¬ì…ë‹ˆë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ì— ëŒ€í•œ ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì´ë¡ ì  ì˜ˆì¸¡ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë¶„ì‚° ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ ì ëŒ€ì  í›ˆë ¨ì˜ ì¼ë°˜í™” íŠ¹ì„±ì„ ì²˜ìŒìœ¼ë¡œ ê·œëª…í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì•Œê³ ë¦¬ì¦˜ ì•ˆì •ì„±ì€ ì¼ë°˜í™” ë¶„ì„ì— ìœ ìš©í•œ ë„êµ¬ë¡œ ìë¦¬ ì¡ê³  ìˆë‹¤.
- 2. ì ëŒ€ì  í›ˆë ¨ì€ ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ê°•ê±´ ê³¼ì í•© ë° ì¼ë°˜í™” ê°„ê²© í™•ëŒ€ ë¬¸ì œë¥¼ ê²ªëŠ”ë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” ë¶„ì‚° ë„¤íŠ¸ì›Œí¬ì—ì„œ ì ëŒ€ì  í›ˆë ¨ì˜ ìˆ˜ë ´ì„±ì„ í™•ë¦½í•œ ìµœê·¼ ì—°êµ¬ì™€ ë‹¬ë¦¬, ì¼ë°˜í™” íŠ¹ì„±ì„ íƒêµ¬í•œë‹¤.
- 4. í™•ì‚° ì „ëµ í•˜ì—ì„œ ë³¼ë¡ ì†ì‹¤ì— ëŒ€í•œ ì ëŒ€ì  í›ˆë ¨ì˜ ì•ˆì •ì„± ê¸°ë°˜ ì¼ë°˜í™” ë¶„ì„ì„ ì œì‹œí•œë‹¤.
- 5. ì¼ë°˜í™” ì˜¤ë¥˜ê°€ ì ëŒ€ì  êµë€ ê°•ë„ì™€ í›ˆë ¨ ë‹¨ê³„ ìˆ˜ì— ë”°ë¼ ì¦ê°€í•œë‹¤ëŠ” ê²½ê³„ë¥¼ ë„ì¶œí•˜ì˜€ìœ¼ë©°, ì´ëŠ” ë‹¨ì¼ ì—ì´ì „íŠ¸ ê²½ìš°ì™€ ì¼ì¹˜í•˜ì§€ë§Œ ë¶„ì‚° ì„¤ì •ì—ì„œëŠ” ìƒˆë¡œìš´ ë°œê²¬ì´ë‹¤.


---

*Generated on 2025-09-24 15:02:40*