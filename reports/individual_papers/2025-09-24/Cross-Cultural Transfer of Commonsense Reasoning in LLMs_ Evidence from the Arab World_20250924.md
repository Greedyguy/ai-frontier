<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:32:44.095035",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Cultural Transfer",
    "Commonsense Reasoning",
    "In-Context Learning",
    "Demonstration-Based Reinforcement"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "Cross-Cultural Transfer": 0.85,
    "Commonsense Reasoning": 0.8,
    "In-Context Learning": 0.7,
    "Demonstration-Based Reinforcement": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's exploration of cross-cultural reasoning, linking to broader discussions on LLMs.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cross-Cultural Transfer",
        "canonical": "Cross-Cultural Transfer",
        "aliases": [
          "Cultural Transfer"
        ],
        "category": "unique_technical",
        "rationale": "Key concept of the paper, focusing on the transferability of reasoning across cultures.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Commonsense Reasoning",
        "canonical": "Commonsense Reasoning",
        "aliases": [
          "Common Sense Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding the paper's focus on reasoning capabilities in LLMs.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "In-Context Learning",
        "canonical": "In-Context Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Describes a method evaluated in the paper, relevant for linking to learning techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Demonstration-Based Reinforcement",
        "canonical": "Demonstration-Based Reinforcement",
        "aliases": [
          "DITTO"
        ],
        "category": "unique_technical",
        "rationale": "A specific method used in the study, highlighting a unique approach to alignment.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cross-Cultural Transfer",
      "resolved_canonical": "Cross-Cultural Transfer",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Commonsense Reasoning",
      "resolved_canonical": "Commonsense Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Demonstration-Based Reinforcement",
      "resolved_canonical": "Demonstration-Based Reinforcement",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World

## π“‹ λ©”νƒ€λ°μ΄ν„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19265.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.19265](https://arxiv.org/abs/2509.19265)

## π”— μ μ‚¬ν• λ…Όλ¬Έ
- [[2025-09-23/Fluent but Foreign_ Even Regional LLMs Lack Cultural Alignment_20250923|Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment]] (88.9% similar)
- [[2025-09-23/MAKIEval_ A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs_20250923|MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs]] (88.2% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (86.7% similar)
- [[2025-09-23/DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India_ Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context_20250923|DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context]] (86.5% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.2% similar)

## π·οΈ μΉ΄ν…κ³ λ¦¬ν™”λ ν‚¤μ›λ“
**π§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**π”— Specific Connectable**: [[keywords/Commonsense Reasoning|Commonsense Reasoning]], [[keywords/In-Context Learning|In-Context Learning]]
**β΅ Unique Technical**: [[keywords/Cross-Cultural Transfer|Cross-Cultural Transfer]], [[keywords/Demonstration-Based Reinforcement|Demonstration-Based Reinforcement]]

## π“‹ μ €μ μ •λ³΄

**Authors:** 

## π“„ Abstract (μ›λ¬Έ)

arXiv:2509.19265v1 Announce Type: new 
Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.

## π“ μ”μ•½

μ΄ λ…Όλ¬Έμ€ λ€ν• μ–Έμ–΄ λ¨λΈ(LLM)μ μ„κµ¬ μ¤‘μ‹¬μ  νΈν–¥μ„ κ·Ήλ³µν•κ³  λ‹¤μ–‘ν• λ¬Έν™”μ  λ§¥λ½μ—μ„μ μ„±λ¥μ„ κ°μ„ ν•κΈ° μ„ν•΄ λ¬Έν™” κ°„ μ „μ΄λ¥Ό νƒκµ¬ν•©λ‹λ‹¤. μ•„λ μ„Έκ³„μ 13κ°κµ­μ„ λ€μƒμΌλ΅ ν• μƒμ‹ μ¶”λ΅  λ°μ΄ν„°μ…‹μ„ ν™μ©ν•μ—¬, κ²½λ‰ν™”λ μ •λ ¬ λ°©λ²•μΈ λ§¥λ½ λ‚΄ ν•™μµκ³Ό μ‹μ—° κΈ°λ° κ°•ν™”(DITTO)λ¥Ό ν‰κ°€ν–μµλ‹λ‹¤. μ—°κµ¬ κ²°κ³Ό, ν• κµ­κ°€μ λ¬Έν™”μ  μμ‹ 12κ°λ§μΌλ΅λ„ λ‹¤λ¥Έ κµ­κ°€μ—μ„ ν‰κ·  10%μ μ„±λ¥ ν–¥μƒμ„ μ΄λ£° μ μμμ„ ν™•μΈν–μµλ‹λ‹¤. λν•, μΈλ„λ„¤μ‹μ•„μ™€ λ―Έκµ­μ μμ‹κ°€ μ•„λ λ¬Έν™” λ‚΄ μ •λ ¬μ„ λ¥κ°€ν•  μ μμμ„ λ³΄μ—¬μ£Όμ–΄, λ¬Έν™” μƒμ‹μ μ „μ΄ κ°€λ¥μ„±μ„ μ…μ¦ν–μµλ‹λ‹¤. μ΄λ¬ν• κ²°κ³Όλ” μ €μλ“¤μ΄ μ μ•ν• λ°©λ²•μ΄ μ €μμ› λ¬Έν™” ν™κ²½μ— LLMμ„ μ μ‘μ‹ν‚¤λ” λ° μ λ§ν• μ ‘κ·Όλ²•μ„μ„ μ‹μ‚¬ν•©λ‹λ‹¤.

## π― μ£Όμ” ν¬μΈνΈ

- 1. λ€ν• μ–Έμ–΄ λ¨λΈ(LLMs)μ€ μ„κµ¬ μ¤‘μ‹¬μ νΈν–¥μ„ λ°μν•μ—¬ λ‹¤μ–‘ν• λ¬Έν™”μ  λ§¥λ½μ—μ„μ ν¨κ³Όκ°€ μ ν•μ μ΄λ‹¤.
- 2. λ³Έ μ—°κµ¬λ” μ•„λ μ„Έκ³„μ—μ„μ μƒμ‹ μ¶”λ΅ μ λ¬Έν™” κ°„ μ „μ΄λ¥Ό μ΅°μ‚¬ν•λ©°, 13κ° μ•„λ κµ­κ°€λ¥Ό μ•„μ°λ¥΄λ” λ¬Έν™”μ μΌλ΅ κΈ°λ°λ λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•λ‹¤.
- 3. 12κ°μ νΉμ • λ¬Έν™” μμ‹λ§μΌλ΅λ„ λ‹¤κµ­μ–΄ λ¨λΈμ—μ„ λ‹¤λ¥Έ κµ­κ°€μ μ„±λ¥μ„ ν‰κ·  10% ν–¥μƒμ‹ν‚¬ μ μμμ„ λ°κ²¬ν–λ‹¤.
- 4. μΈλ„λ„¤μ‹μ•„μ™€ λ―Έκµ­μ λ¬Έν™” μ™Έ μ‹μ—°μ΄ MCQ μ¶”λ΅ μ—μ„ λ¬Έν™” λ‚΄ μ •λ ¬μ„ λ¥κ°€ν•  μ μμμ„ λ³΄μ—¬μ¤€λ‹¤.
- 5. μ΄λ¬ν• κ²°κ³Όλ” ν¨μ¨μ μΈ λ¬Έν™” κ°„ μ •λ ¬μ΄ κ°€λ¥ν•¨μ„ μ‹μ‚¬ν•λ©°, LLMsλ¥Ό μ €μμ› λ¬Έν™” ν™κ²½μ— μ μ‘μ‹ν‚¤λ” μ λ§ν• μ ‘κ·Όλ²•μ„ μ μ‹ν•λ‹¤.


---

*Generated on 2025-09-24 13:32:44*