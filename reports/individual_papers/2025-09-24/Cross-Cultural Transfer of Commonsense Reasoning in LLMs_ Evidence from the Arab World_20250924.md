<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:32:44.095035",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Cross-Cultural Transfer",
    "Commonsense Reasoning",
    "In-Context Learning",
    "Demonstration-Based Reinforcement"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.8,
    "Cross-Cultural Transfer": 0.85,
    "Commonsense Reasoning": 0.8,
    "In-Context Learning": 0.7,
    "Demonstration-Based Reinforcement": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's exploration of cross-cultural reasoning, linking to broader discussions on LLMs.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Cross-Cultural Transfer",
        "canonical": "Cross-Cultural Transfer",
        "aliases": [
          "Cultural Transfer"
        ],
        "category": "unique_technical",
        "rationale": "Key concept of the paper, focusing on the transferability of reasoning across cultures.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Commonsense Reasoning",
        "canonical": "Commonsense Reasoning",
        "aliases": [
          "Common Sense Reasoning"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding the paper's focus on reasoning capabilities in LLMs.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "In-Context Learning",
        "canonical": "In-Context Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Describes a method evaluated in the paper, relevant for linking to learning techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Demonstration-Based Reinforcement",
        "canonical": "Demonstration-Based Reinforcement",
        "aliases": [
          "DITTO"
        ],
        "category": "unique_technical",
        "rationale": "A specific method used in the study, highlighting a unique approach to alignment.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Cross-Cultural Transfer",
      "resolved_canonical": "Cross-Cultural Transfer",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Commonsense Reasoning",
      "resolved_canonical": "Commonsense Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "In-Context Learning",
      "resolved_canonical": "In-Context Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Demonstration-Based Reinforcement",
      "resolved_canonical": "Demonstration-Based Reinforcement",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19265.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.19265](https://arxiv.org/abs/2509.19265)

## 🔗 유사한 논문
- [[2025-09-23/Fluent but Foreign_ Even Regional LLMs Lack Cultural Alignment_20250923|Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment]] (88.9% similar)
- [[2025-09-23/MAKIEval_ A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs_20250923|MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs]] (88.2% similar)
- [[2025-09-22/CultureScope_ A Dimensional Lens for Probing Cultural Understanding in LLMs_20250922|CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs]] (86.7% similar)
- [[2025-09-23/DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India_ Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context_20250923|DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context]] (86.5% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (85.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Commonsense Reasoning|Commonsense Reasoning]], [[keywords/In-Context Learning|In-Context Learning]]
**⚡ Unique Technical**: [[keywords/Cross-Cultural Transfer|Cross-Cultural Transfer]], [[keywords/Demonstration-Based Reinforcement|Demonstration-Based Reinforcement]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19265v1 Announce Type: new 
Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.

## 📝 요약

이 논문은 대형 언어 모델(LLM)의 서구 중심적 편향을 극복하고 다양한 문화적 맥락에서의 성능을 개선하기 위해 문화 간 전이를 탐구합니다. 아랍 세계의 13개국을 대상으로 한 상식 추론 데이터셋을 활용하여, 경량화된 정렬 방법인 맥락 내 학습과 시연 기반 강화(DITTO)를 평가했습니다. 연구 결과, 한 국가의 문화적 예시 12개만으로도 다른 국가에서 평균 10%의 성능 향상을 이룰 수 있음을 확인했습니다. 또한, 인도네시아와 미국의 예시가 아랍 문화 내 정렬을 능가할 수 있음을 보여주어, 문화 상식의 전이 가능성을 입증했습니다. 이러한 결과는 저자들이 제안한 방법이 저자원 문화 환경에 LLM을 적응시키는 데 유망한 접근법임을 시사합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLMs)은 서구 중심의 편향을 반영하여 다양한 문화적 맥락에서의 효과가 제한적이다.
- 2. 본 연구는 아랍 세계에서의 상식 추론의 문화 간 전이를 조사하며, 13개 아랍 국가를 아우르는 문화적으로 기반된 데이터셋을 사용한다.
- 3. 12개의 특정 문화 예시만으로도 다국어 모델에서 다른 국가의 성능을 평균 10% 향상시킬 수 있음을 발견했다.
- 4. 인도네시아와 미국의 문화 외 시연이 MCQ 추론에서 문화 내 정렬을 능가할 수 있음을 보여준다.
- 5. 이러한 결과는 효율적인 문화 간 정렬이 가능함을 시사하며, LLMs를 저자원 문화 환경에 적응시키는 유망한 접근법을 제시한다.


---

*Generated on 2025-09-24 13:32:44*