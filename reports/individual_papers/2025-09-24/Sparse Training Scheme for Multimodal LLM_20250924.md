<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:39:04.233228",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Sparse Training Scheme",
    "Visual Token Compression",
    "Layer Dynamic Skipping",
    "Multimodal Data"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Sparse Training Scheme": 0.78,
    "Visual Token Compression": 0.72,
    "Layer Dynamic Skipping": 0.75,
    "Multimodal Data": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the trending concept of integrating multiple modalities in language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sparse Training Scheme",
        "canonical": "Sparse Training Scheme",
        "aliases": [
          "STS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel training framework that could be a focal point for future research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Visual Token Compressor",
        "canonical": "Visual Token Compression",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific technique within the proposed framework, enhancing understanding of model efficiency.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Layer Dynamic Skipper",
        "canonical": "Layer Dynamic Skipping",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights a method for reducing computational overhead, relevant for optimizing model training.",
        "novelty_score": 0.7,
        "connectivity_score": 0.63,
        "specificity_score": 0.79,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multimodal Data",
        "canonical": "Multimodal Data",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Essential for understanding the context in which the training scheme operates.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "training process",
      "benchmarks",
      "computational overhead"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sparse Training Scheme",
      "resolved_canonical": "Sparse Training Scheme",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Visual Token Compressor",
      "resolved_canonical": "Visual Token Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Layer Dynamic Skipper",
      "resolved_canonical": "Layer Dynamic Skipping",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.63,
        "specificity": 0.79,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multimodal Data",
      "resolved_canonical": "Multimodal Data",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Sparse Training Scheme for Multimodal LLM

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18150.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18150](https://arxiv.org/abs/2509.18150)

## 🔗 유사한 논문
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (87.1% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (86.9% similar)
- [[2025-09-23/L-MTP_ Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models_20250923|L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models]] (86.4% similar)
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (86.1% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Multimodal Data|Multimodal Data]]
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Sparse Training Scheme|Sparse Training Scheme]], [[keywords/Visual Token Compression|Visual Token Compression]], [[keywords/Layer Dynamic Skipping|Layer Dynamic Skipping]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18150v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance across a variety of domains. However, training MLLMs is often inefficient due to the significantly longer input sequences introduced by multimodal data and the low utilization of inter-layer computations. To address this challenge, we shift the focus to the training process itself and propose a novel training-efficient framework based on sparse representations, termed the Sparse Training Scheme (STS). This scheme consists of two key components: the Visual Token Compressor, which reduces the information load by compressing visual tokens, and the Layer Dynamic Skipper, which mitigates the computational overhead by dynamically skipping unnecessary layers in the language model during both forward and backward passes. Our approach is broadly applicable to diverse MLLM architectures and has been extensively evaluated on multiple benchmarks, demonstrating its effectiveness and efficiency.

## 📝 요약

다중모달 대형 언어 모델(MLLMs)은 다양한 분야에서 뛰어난 성능을 보이지만, 멀티모달 데이터로 인해 입력 시퀀스가 길어지고 계층 간 계산 활용도가 낮아 비효율적인 훈련 문제가 발생합니다. 이를 해결하기 위해, 우리는 희소 표현을 기반으로 한 새로운 훈련 효율 프레임워크인 Sparse Training Scheme(STS)을 제안합니다. STS는 시각적 토큰을 압축하여 정보 부하를 줄이는 Visual Token Compressor와, 불필요한 계층을 동적으로 건너뛰어 계산 부담을 줄이는 Layer Dynamic Skipper로 구성됩니다. 이 접근법은 다양한 MLLM 구조에 적용 가능하며, 여러 벤치마크에서 효과성과 효율성을 입증했습니다.

## 🎯 주요 포인트

- 1. 다중모달 대형 언어 모델(MLLM)은 다양한 분야에서 뛰어난 성능을 보인다.
- 2. MLLM의 훈련 비효율성 문제를 해결하기 위해 희소 표현 기반의 Sparse Training Scheme(STS)을 제안한다.
- 3. STS는 시각적 토큰을 압축하여 정보 부하를 줄이는 Visual Token Compressor와 불필요한 레이어를 동적으로 건너뛰는 Layer Dynamic Skipper로 구성된다.
- 4. 제안된 방법은 다양한 MLLM 아키텍처에 적용 가능하며, 여러 벤치마크에서 효과성과 효율성을 입증했다.


---

*Generated on 2025-09-24 13:39:04*