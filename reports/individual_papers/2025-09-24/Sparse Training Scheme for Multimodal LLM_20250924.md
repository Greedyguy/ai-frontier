<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:39:04.233228",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Sparse Training Scheme",
    "Visual Token Compression",
    "Layer Dynamic Skipping",
    "Multimodal Data"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Sparse Training Scheme": 0.78,
    "Visual Token Compression": 0.72,
    "Layer Dynamic Skipping": 0.75,
    "Multimodal Data": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "MLLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the trending concept of integrating multiple modalities in language models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Sparse Training Scheme",
        "canonical": "Sparse Training Scheme",
        "aliases": [
          "STS"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel training framework that could be a focal point for future research.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Visual Token Compressor",
        "canonical": "Visual Token Compression",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Represents a specific technique within the proposed framework, enhancing understanding of model efficiency.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Layer Dynamic Skipper",
        "canonical": "Layer Dynamic Skipping",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Highlights a method for reducing computational overhead, relevant for optimizing model training.",
        "novelty_score": 0.7,
        "connectivity_score": 0.63,
        "specificity_score": 0.79,
        "link_intent_score": 0.75
      },
      {
        "surface": "Multimodal Data",
        "canonical": "Multimodal Data",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Essential for understanding the context in which the training scheme operates.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "training process",
      "benchmarks",
      "computational overhead"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Sparse Training Scheme",
      "resolved_canonical": "Sparse Training Scheme",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Visual Token Compressor",
      "resolved_canonical": "Visual Token Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Layer Dynamic Skipper",
      "resolved_canonical": "Layer Dynamic Skipping",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.63,
        "specificity": 0.79,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Multimodal Data",
      "resolved_canonical": "Multimodal Data",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Sparse Training Scheme for Multimodal LLM

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18150.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18150](https://arxiv.org/abs/2509.18150)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (87.1% similar)
- [[2025-09-19/Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models]] (86.9% similar)
- [[2025-09-23/L-MTP_ Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models_20250923|L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models]] (86.4% similar)
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (86.1% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Multimodal Data|Multimodal Data]]
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Sparse Training Scheme|Sparse Training Scheme]], [[keywords/Visual Token Compression|Visual Token Compression]], [[keywords/Layer Dynamic Skipping|Layer Dynamic Skipping]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18150v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance across a variety of domains. However, training MLLMs is often inefficient due to the significantly longer input sequences introduced by multimodal data and the low utilization of inter-layer computations. To address this challenge, we shift the focus to the training process itself and propose a novel training-efficient framework based on sparse representations, termed the Sparse Training Scheme (STS). This scheme consists of two key components: the Visual Token Compressor, which reduces the information load by compressing visual tokens, and the Layer Dynamic Skipper, which mitigates the computational overhead by dynamically skipping unnecessary layers in the language model during both forward and backward passes. Our approach is broadly applicable to diverse MLLM architectures and has been extensively evaluated on multiple benchmarks, demonstrating its effectiveness and efficiency.

## ğŸ“ ìš”ì•½

ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLMs)ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¡œ ì¸í•´ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§€ê³  ê³„ì¸µ ê°„ ê³„ì‚° í™œìš©ë„ê°€ ë‚®ì•„ ë¹„íš¨ìœ¨ì ì¸ í›ˆë ¨ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í¬ì†Œ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ í›ˆë ¨ íš¨ìœ¨ í”„ë ˆì„ì›Œí¬ì¸ Sparse Training Scheme(STS)ì„ ì œì•ˆí•©ë‹ˆë‹¤. STSëŠ” ì‹œê°ì  í† í°ì„ ì••ì¶•í•˜ì—¬ ì •ë³´ ë¶€í•˜ë¥¼ ì¤„ì´ëŠ” Visual Token Compressorì™€, ë¶ˆí•„ìš”í•œ ê³„ì¸µì„ ë™ì ìœ¼ë¡œ ê±´ë„ˆë›°ì–´ ê³„ì‚° ë¶€ë‹´ì„ ì¤„ì´ëŠ” Layer Dynamic Skipperë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë‹¤ì–‘í•œ MLLM êµ¬ì¡°ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íš¨ê³¼ì„±ê³¼ íš¨ìœ¨ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- 2. MLLMì˜ í›ˆë ¨ ë¹„íš¨ìœ¨ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í¬ì†Œ í‘œí˜„ ê¸°ë°˜ì˜ Sparse Training Scheme(STS)ì„ ì œì•ˆí•œë‹¤.
- 3. STSëŠ” ì‹œê°ì  í† í°ì„ ì••ì¶•í•˜ì—¬ ì •ë³´ ë¶€í•˜ë¥¼ ì¤„ì´ëŠ” Visual Token Compressorì™€ ë¶ˆí•„ìš”í•œ ë ˆì´ì–´ë¥¼ ë™ì ìœ¼ë¡œ ê±´ë„ˆë›°ëŠ” Layer Dynamic Skipperë¡œ êµ¬ì„±ëœë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ì–‘í•œ MLLM ì•„í‚¤í…ì²˜ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íš¨ê³¼ì„±ê³¼ íš¨ìœ¨ì„±ì„ ì…ì¦í–ˆë‹¤.


---

*Generated on 2025-09-24 13:39:04*