<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:51:59.561392",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Active Partial Rollouts",
    "Large Language Model",
    "Rollout Generation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Active Partial Rollouts": 0.88,
    "Large Language Model": 0.82,
    "Rollout Generation": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is central to the paper's methodology and connects to a wide range of related research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Active Partial Rollouts in Reinforcement Learning",
        "canonical": "Active Partial Rollouts",
        "aliases": [
          "APRIL"
        ],
        "category": "unique_technical",
        "rationale": "APRIL is a novel method introduced in the paper, crucial for understanding its contributions to RL efficiency.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are a key application area for the discussed RL techniques, providing strong connectivity to related fields.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Rollout Generation",
        "canonical": "Rollout Generation",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Rollout Generation is a critical process in RL discussed extensively in the paper, highlighting its computational challenges.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Active Partial Rollouts in Reinforcement Learning",
      "resolved_canonical": "Active Partial Rollouts",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Rollout Generation",
      "resolved_canonical": "Rollout Generation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18521.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18521](https://arxiv.org/abs/2509.18521)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/RLinf_ Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation_20250922|RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation]] (88.0% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (86.0% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (85.6% similar)
- [[2025-09-23/Towards Sample-Efficiency and Generalization of Transfer and Inverse Reinforcement Learning_ A Comprehensive Literature Review_20250923|Towards Sample-Efficiency and Generalization of Transfer and Inverse Reinforcement Learning: A Comprehensive Literature Review]] (85.5% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Active Partial Rollouts|Active Partial Rollouts]], [[keywords/Rollout Generation|Rollout Generation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18521v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to enhance reasoning and coding capabilities. To meet the community's growing RL needs, numerous RL frameworks have been proposed. Most of these frameworks primarily rely on inference engines for rollout generation and training engines for policy updates. However, RL training remains computationally expensive, with rollout generation accounting for more than 90% of total runtime. In addition, its efficiency is often constrained by the long-tail distribution of rollout response lengths, where a few lengthy responses stall entire batches, leaving GPUs idle and underutilized. As model and rollout sizes continue to grow, this bottleneck increasingly limits scalability. To address this challenge, we propose Active Partial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the rollout phase, APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps. This strategy ensures that no rollouts are discarded while substantially reducing GPU idle time. Experiments show that APRIL improves rollout throughput by at most 44% across commonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8% higher final accuracy across tasks. Moreover, APRIL is both framework and hardware agnostic, already integrated into the slime RL framework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies system-level and algorithmic considerations in proposing APRIL, with the aim of advancing RL training efficiency and inspiring further optimizations in RL systems.

## ğŸ“ ìš”ì•½

ê°•í™” í•™ìŠµ(RL)ì€ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ì— í•µì‹¬ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ RL í›ˆë ¨ì€ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ë©°, ë¡¤ì•„ì›ƒ ìƒì„±ì´ ì „ì²´ ì‹¤í–‰ ì‹œê°„ì˜ 90% ì´ìƒì„ ì°¨ì§€í•©ë‹ˆë‹¤. ì´ì— ë”°ë¼ ë¡¤ì•„ì›ƒ ì‘ë‹µ ê¸¸ì´ì˜ ê¸´ ê¼¬ë¦¬ ë¶„í¬ë¡œ ì¸í•´ íš¨ìœ¨ì„±ì´ ì œí•œë©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê°•í™” í•™ìŠµì—ì„œì˜ ëŠ¥ë™ì  ë¶€ë¶„ ë¡¤ì•„ì›ƒ(APRIL)ì„ ì œì•ˆí•©ë‹ˆë‹¤. APRILì€ ë¡¤ì•„ì›ƒ ìš”ì²­ì„ ì´ˆê³¼í• ë‹¹í•˜ê³  ëª©í‘œ ì‘ë‹µ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œí•˜ë©°, ë¶ˆì™„ì „í•œ ì‘ë‹µì„ ì¬í™œìš©í•˜ì—¬ GPU ìœ íœ´ ì‹œê°„ì„ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, APRILì€ ë¡¤ì•„ì›ƒ ì²˜ë¦¬ëŸ‰ì„ ìµœëŒ€ 44% ê°œì„ í•˜ê³ , ìˆ˜ë ´ ì†ë„ë¥¼ ê°€ì†í™”í•˜ë©°, ìµœì¢… ì •í™•ë„ë¥¼ ìµœëŒ€ 8% í–¥ìƒì‹œí‚µë‹ˆë‹¤. APRILì€ í”„ë ˆì„ì›Œí¬ ë° í•˜ë“œì›¨ì–´ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ìŠ¬ë¼ì„ RL í”„ë ˆì„ì›Œí¬ì— í†µí•©ë˜ì–´ NVIDIA ë° AMD GPUì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” RL í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ì¶”ê°€ ìµœì í™”ë¥¼ ìœ„í•œ ì˜ê°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê°•í™” í•™ìŠµ(RL)ì€ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ë°œì „ì— í•µì‹¬ì ì¸ ì—­í• ì„ í•˜ê³  ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ì¶”ë¡  ë° ì½”ë”© ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìˆë‹¤.
- 2. RL í›ˆë ¨ì€ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ë©°, ë¡¤ì•„ì›ƒ ìƒì„±ì´ ì „ì²´ ì‹¤í–‰ ì‹œê°„ì˜ 90% ì´ìƒì„ ì°¨ì§€í•˜ì—¬ ë³‘ëª© í˜„ìƒì´ ë°œìƒí•œë‹¤.
- 3. APRILì€ ë¡¤ì•„ì›ƒ ìš”ì²­ì„ ì´ˆê³¼ í• ë‹¹í•˜ê³  ëª©í‘œ ì‘ë‹µ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œí•˜ë©°, ë¯¸ì™„ì„± ì‘ë‹µì„ ì¬í™œìš©í•˜ì—¬ GPU ìœ íœ´ ì‹œê°„ì„ í¬ê²Œ ì¤„ì¸ë‹¤.
- 4. APRILì€ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” RL ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë¡¤ì•„ì›ƒ ì²˜ë¦¬ëŸ‰ì„ ìµœëŒ€ 44% ê°œì„ í•˜ê³ , ìˆ˜ë ´ ì†ë„ë¥¼ ê°€ì†í™”í•˜ë©°, ìµœì¢… ì •í™•ë„ë¥¼ ìµœëŒ€ 8% í–¥ìƒì‹œí‚¨ë‹¤.
- 5. APRILì€ í”„ë ˆì„ì›Œí¬ ë° í•˜ë“œì›¨ì–´ì— êµ¬ì• ë°›ì§€ ì•Šìœ¼ë©°, ì´ë¯¸ ìŠ¬ë¼ì„ RL í”„ë ˆì„ì›Œí¬ì— í†µí•©ë˜ì–´ NVIDIA ë° AMD GPUì—ì„œ ë°°í¬ ê°€ëŠ¥í•˜ë‹¤.


---

*Generated on 2025-09-24 13:51:59*