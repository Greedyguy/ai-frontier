<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:23:21.948764",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Few-Shot Learning",
    "Pre-Trained Model",
    "Meta-Learning",
    "Formal Diversity"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Few-Shot Learning": 0.9,
    "Pre-Trained Model": 0.85,
    "Meta-Learning": 0.88,
    "Formal Diversity": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "few-shot learning",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "few-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is a key focus of the paper, providing a direct link to current trends and discussions in the field.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "pre-trained model",
        "canonical": "Pre-Trained Model",
        "aliases": [
          "PT model"
        ],
        "category": "unique_technical",
        "rationale": "The comparison between pre-trained models and meta-learning is central to the paper's argument.",
        "novelty_score": 0.7,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "meta-learning",
        "canonical": "Meta-Learning",
        "aliases": [
          "MAML",
          "Model Agnostic Meta-Learning"
        ],
        "category": "specific_connectable",
        "rationale": "Meta-Learning is a core concept compared against pre-training, crucial for understanding the paper's findings.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "formal diversity",
        "canonical": "Formal Diversity",
        "aliases": [
          "diversity coefficient"
        ],
        "category": "unique_technical",
        "rationale": "Formal diversity is a novel metric used in the paper to evaluate dataset characteristics, offering a unique perspective.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "effect size",
      "classical statistical thresholds"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "few-shot learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "pre-trained model",
      "resolved_canonical": "Pre-Trained Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "meta-learning",
      "resolved_canonical": "Meta-Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "formal diversity",
      "resolved_canonical": "Formal Diversity",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Is Pre-training Truly Better Than Meta-Learning?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2306.13841.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2306.13841](https://arxiv.org/abs/2306.13841)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (85.0% similar)
- [[2025-09-23/Language Modeling with Learned Meta-Tokens_20250923|Language Modeling with Learned Meta-Tokens]] (83.0% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (82.1% similar)
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (81.4% similar)
- [[2025-09-23/Scaling, Simplification, and Adaptation_ Lessons from Pretraining on Machine-Translated Text_20250923|Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text]] (81.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]], [[keywords/Meta-Learning|Meta-Learning]]
**âš¡ Unique Technical**: [[keywords/Pre-Trained Model|Pre-Trained Model]], [[keywords/Formal Diversity|Formal Diversity]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2306.13841v2 Announce Type: replace-cross 
Abstract: In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is high, MAML beats PT on average. The caveat is that the magnitude of the average difference between a PT vs. MAML using the effect size is low (according to classical statistical thresholds) -- less than 0.2. Nevertheless, this observation is contrary to the currently held belief that a pre-trained model is always better than a meta-learning model. Our extensive experiments consider 21 few-shot learning benchmarks, including the large-scale few-shot learning dataset Meta-Data set. We also show no significant difference between a MAML model vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a pre-trained model does not always beat a meta-learned model and that the formal diversity of a dataset is a driving factor.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì†Œìˆ˜ ìƒ· í•™ìŠµì—ì„œ ê³ ì •ëœ ì‚¬ì „ í•™ìŠµ ëª¨ë¸(PT)ì´ ìµœì¢… ë ˆì´ì–´ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ í‘œì¤€ ë©”íƒ€ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê¸°ì¡´ì˜ ë¯¿ìŒì„ ì¬í‰ê°€í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ PTì™€ ëª¨ë¸ ë¬´ê´€ ë©”íƒ€ í•™ìŠµ(MAML)ì„ ë¹„êµí•˜ë©°, ë™ì¼í•œ ì•„í‚¤í…ì²˜ì™€ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³µì •í•œ ë¹„êµë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤. íš¨ê³¼ í¬ê¸°(Cohen's d)ë¥¼ í†µí•´ PTì™€ MAML ê°„ì˜ ì‹¤ì§ˆì  ì°¨ì´ë¥¼ í‰ê°€í•˜ê³ , ë°ì´í„°ì…‹ì˜ í˜•ì‹ì  ë‹¤ì–‘ì„±ì„ ì¸¡ì •í•˜ëŠ” ë‹¤ì–‘ì„± ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë°ì´í„°ì…‹ì˜ í˜•ì‹ì  ë‹¤ì–‘ì„±ì´ ë‚®ì„ ë•ŒëŠ” PTê°€ MAMLì„ í‰ê· ì ìœ¼ë¡œ ëŠ¥ê°€í•˜ê³ , ë‹¤ì–‘ì„±ì´ ë†’ì„ ë•ŒëŠ” MAMLì´ PTë¥¼ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‘ ëª¨ë¸ ê°„ì˜ í‰ê·  ì°¨ì´ëŠ” ì‘ìœ¼ë©°, ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ í•­ìƒ ë©”íƒ€ í•™ìŠµ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê¸°ì¡´ ë¯¿ìŒì— ë°˜í•˜ëŠ” ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. 21ê°œì˜ ì†Œìˆ˜ ìƒ· í•™ìŠµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì„ í†µí•´ ë°ì´í„°ì…‹ì˜ í˜•ì‹ì  ë‹¤ì–‘ì„±ì´ ì¤‘ìš”í•œ ìš”ì¸ì„ì„ ê²°ë¡ ì§€ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(PT)ì€ ë°ì´í„°ì…‹ì˜ í˜•ì‹ì  ë‹¤ì–‘ì„±ì´ ë‚®ì„ ë•Œ MAMLë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ ìš°ìˆ˜í•˜ë‹¤.
- 2. ë°ì´í„°ì…‹ì˜ í˜•ì‹ì  ë‹¤ì–‘ì„±ì´ ë†’ì„ ë•ŒëŠ” MAMLì´ PTë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ ìš°ìˆ˜í•˜ë‹¤.
- 3. PTì™€ MAML ê°„ì˜ í‰ê·  ì°¨ì´ëŠ” íš¨ê³¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‚®ìœ¼ë©°, ì´ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ í•­ìƒ ë©”íƒ€ í•™ìŠµ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê¸°ì¡´ ë¯¿ìŒê³¼ ë°˜ëŒ€ëœë‹¤.
- 4. 21ê°œì˜ ì†Œìˆ˜ ìƒ· í•™ìŠµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ í•­ìƒ ë©”íƒ€ í•™ìŠµ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ ì•ŠìŒì„ í™•ì¸í–ˆë‹¤.
- 5. ë°ì´í„°ì…‹ì˜ í˜•ì‹ì  ë‹¤ì–‘ì„±ì´ ëª¨ë¸ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.


---

*Generated on 2025-09-24 14:23:21*