<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:14:18.081541",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Chain of Step Reasoning",
    "Vision-Language Model",
    "Fine-grained Rewards",
    "Reinforcement Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Chain of Step Reasoning": 0.78,
    "Vision-Language Model": 0.82,
    "Fine-grained Rewards": 0.77,
    "Reinforcement Learning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Chain of Step Reasoning",
        "canonical": "Chain of Step Reasoning",
        "aliases": [
          "Step-by-Step Reasoning",
          "Sequential Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's approach in improving vision-language models by breaking down reasoning into finer steps.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Systems",
          "Multimodal Models"
        ],
        "category": "evolved_concepts",
        "rationale": "This is a key focus of the paper, addressing the integration of visual and linguistic data.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Fine-grained Rewards",
        "canonical": "Fine-grained Rewards",
        "aliases": [
          "Detailed Rewards",
          "Granular Rewards"
        ],
        "category": "unique_technical",
        "rationale": "The paper emphasizes the importance of fine-grained rewards for assessing reasoning steps, which is a novel approach.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement learning is a crucial method used in the paper to optimize the vision-language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "framework",
      "baseline",
      "empirical analysis"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Chain of Step Reasoning",
      "resolved_canonical": "Chain of Step Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Fine-grained Rewards",
      "resolved_canonical": "Fine-grained Rewards",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19003.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19003](https://arxiv.org/abs/2509.19003)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (86.9% similar)
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (86.5% similar)
- [[2025-09-23/ProReason_ Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom_20250923|ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom]] (86.2% similar)
- [[2025-09-18/Uni-cot_ Towards Unified Chain-of-Thought Reasoning Across Text and Vision_20250918|Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision]] (85.5% similar)
- [[2025-09-23/Step Guided Reasoning_ Improving Mathematical Reasoning using Guidance Generation and Step Reasoning_20250923|Step Guided Reasoning: Improving Mathematical Reasoning using Guidance Generation and Step Reasoning]] (84.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Chain of Step Reasoning|Chain of Step Reasoning]], [[keywords/Fine-grained Rewards|Fine-grained Rewards]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19003v1 Announce Type: new 
Abstract: Chain of thought reasoning has demonstrated remarkable success in large language models, yet its adaptation to vision-language reasoning remains an open challenge with unclear best practices. Existing attempts typically employ reasoning chains at a coarse-grained level, which struggles to perform fine-grained structured reasoning and, more importantly, are difficult to evaluate the reward and quality of intermediate reasoning. In this work, we delve into chain of step reasoning for vision-language models, enabling assessing reasoning step quality accurately and leading to effective reinforcement learning and inference-time scaling with fine-grained rewards. We present a simple, effective, and fully transparent framework, including the step-level reasoning data, process reward model (PRM), and reinforcement learning training. With the proposed approaches, our models set strong baselines with consistent improvements on challenging vision-language benchmarks. More importantly, we conduct a thorough empirical analysis and ablation study, unveiling the impact of each component and several intriguing properties of inference-time scaling. We believe this paper serves as a baseline for vision-language models and offers insights into more complex multimodal reasoning. Our dataset, PRM, and code will be available at https://github.com/baaivision/CoS.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì„±ê³µì„ ê±°ë‘” ì‚¬ê³ ì˜ ì—°ì‡„(chain of thought) ì¶”ë¡ ì„ ì‹œê°-ì–¸ì–´ ëª¨ë¸ì— ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ëŒ€ê°œ ê±°ì¹œ ìˆ˜ì¤€ì˜ ì¶”ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ì„¸ë°€í•œ êµ¬ì¡°ì  ì¶”ë¡ ì— í•œê³„ê°€ ìˆì—ˆê³ , ì¤‘ê°„ ì¶”ë¡ ì˜ ë³´ìƒê³¼ í’ˆì§ˆ í‰ê°€ê°€ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì„¸ë°€í•œ ë³´ìƒì„ í†µí•´ ì¶”ë¡  ë‹¨ê³„ì˜ í’ˆì§ˆì„ ì •í™•íˆ í‰ê°€í•˜ê³ , íš¨ê³¼ì ì¸ ê°•í™” í•™ìŠµê³¼ ì¶”ë¡  ì‹œê°„ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì²´ê³„ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¨ê³„ë³„ ì¶”ë¡  ë°ì´í„°, í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM), ê°•í™” í•™ìŠµ í›ˆë ¨ì„ í¬í•¨í•˜ë©°, ì´ë¥¼ í†µí•´ ë„ì „ì ì¸ ì‹œê°-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ê° êµ¬ì„± ìš”ì†Œì˜ ì˜í–¥ì„ ë¶„ì„í•˜ê³  í¥ë¯¸ë¡œìš´ íŠ¹ì„±ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì‹œê°-ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ì¤€ì ì„ ì œê³µí•˜ë©°, ë³µì¡í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì¶”ë¡ ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹, PRM, ì½”ë“œê°€ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì‹œê°-ì–¸ì–´ ëª¨ë¸ì—ì„œì˜ ë‹¨ê³„ì  ì¶”ë¡ (chain of step reasoning)ì„ í†µí•´ ì¤‘ê°„ ì¶”ë¡ ì˜ í’ˆì§ˆì„ ì •í™•íˆ í‰ê°€í•˜ê³  ê°•í™” í•™ìŠµ ë° ì¶”ë¡  ì‹œ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¨ê³„ ìˆ˜ì¤€ì˜ ì¶”ë¡  ë°ì´í„°, í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸(PRM), ê°•í™” í•™ìŠµ í›ˆë ¨ì„ í¬í•¨í•˜ì—¬ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì´ê³  íˆ¬ëª…í•œ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- 3. ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì€ ë„ì „ì ì¸ ì‹œê°-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ê°œì„ ì„ ë³´ì´ë©° ê°•ë ¥í•œ ê¸°ì¤€ì ì„ ì„¤ì •í•©ë‹ˆë‹¤.
- 4. ì² ì €í•œ ì‹¤ì¦ ë¶„ì„ê³¼ ì†Œê±° ì—°êµ¬ë¥¼ í†µí•´ ê° êµ¬ì„± ìš”ì†Œì˜ ì˜í–¥ê³¼ ì¶”ë¡  ì‹œ í™•ì¥ì˜ ì—¬ëŸ¬ í¥ë¯¸ë¡œìš´ íŠ¹ì„±ì„ ë°í˜€ëƒ…ë‹ˆë‹¤.
- 5. ë³¸ ë…¼ë¬¸ì€ ì‹œê°-ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ì¤€ì  ì—­í• ì„ í•˜ë©°, ë” ë³µì¡í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì¶”ë¡ ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:14:18*