<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:57:11.828953",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Green Online Learning",
    "Heterogeneous Online Ensembles",
    "Markov Decision Process",
    "Stochastic Model",
    "Near-Optimal Performance"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Green Online Learning": 0.78,
    "Heterogeneous Online Ensembles": 0.82,
    "Markov Decision Process": 0.75,
    "Stochastic Model": 0.7,
    "Near-Optimal Performance": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "green online learning",
        "canonical": "Green Online Learning",
        "aliases": [
          "sustainable online learning"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach focusing on sustainability in online learning, which is not covered by existing terms.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "heterogeneous online ensembles",
        "canonical": "Heterogeneous Online Ensembles",
        "aliases": [
          "HEROS"
        ],
        "category": "unique_technical",
        "rationale": "Represents a specific new ensemble method that balances resource constraints and predictive performance.",
        "novelty_score": 0.9,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.82
      },
      {
        "surface": "Markov decision process",
        "canonical": "Markov Decision Process",
        "aliases": [
          "MDP"
        ],
        "category": "broad_technical",
        "rationale": "A well-established method that is crucial for understanding decision-making in the proposed framework.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "stochastic model",
        "canonical": "Stochastic Model",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Essential for theoretical validation of the proposed policies, linking to probabilistic modeling concepts.",
        "novelty_score": 0.35,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "near-optimal performance",
        "canonical": "Near-Optimal Performance",
        "aliases": [
          "optimal performance"
        ],
        "category": "specific_connectable",
        "rationale": "Highlights the efficiency of the proposed policy, relevant for performance comparison discussions.",
        "novelty_score": 0.45,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "ensemble methods",
      "predictive capabilities"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "green online learning",
      "resolved_canonical": "Green Online Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "heterogeneous online ensembles",
      "resolved_canonical": "Heterogeneous Online Ensembles",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Markov decision process",
      "resolved_canonical": "Markov Decision Process",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "stochastic model",
      "resolved_canonical": "Stochastic Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.35,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "near-optimal performance",
      "resolved_canonical": "Near-Optimal Performance",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Lift What You Can: Green Online Learning with Heterogeneous Ensembles

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18962.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18962](https://arxiv.org/abs/2509.18962)

## 🔗 유사한 논문
- [[2025-09-22/Inference Offloading for Cost-Sensitive Binary Classification at the Edge_20250922|Inference Offloading for Cost-Sensitive Binary Classification at the Edge]] (80.5% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (80.3% similar)
- [[2025-09-23/From Uniform to Heterogeneous_ Tailoring Policy Optimization to Every Token's Nature_20250923|From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature]] (80.2% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (80.2% similar)
- [[2025-09-22/Autoguided Online Data Curation for Diffusion Model Training_20250922|Autoguided Online Data Curation for Diffusion Model Training]] (79.8% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Markov Decision Process|Markov Decision Process]], [[keywords/Stochastic Model|Stochastic Model]]
**🔗 Specific Connectable**: [[keywords/Near-Optimal Performance|Near-Optimal Performance]]
**⚡ Unique Technical**: [[keywords/Green Online Learning|Green Online Learning]], [[keywords/Heterogeneous Online Ensembles|Heterogeneous Online Ensembles]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18962v1 Announce Type: new 
Abstract: Ensemble methods for stream mining necessitate managing multiple models and updating them as data distributions evolve. Considering the calls for more sustainability, established methods are however not sufficiently considerate of ensemble members' computational expenses and instead overly focus on predictive capabilities. To address these challenges and enable green online learning, we propose heterogeneous online ensembles (HEROS). For every training step, HEROS chooses a subset of models from a pool of models initialized with diverse hyperparameter choices under resource constraints to train. We introduce a Markov decision process to theoretically capture the trade-offs between predictive performance and sustainability constraints. Based on this framework, we present different policies for choosing which models to train on incoming data. Most notably, we propose the novel $\zeta$-policy, which focuses on training near-optimal models at reduced costs. Using a stochastic model, we theoretically prove that our $\zeta$-policy achieves near optimal performance while using fewer resources compared to the best performing policy. In our experiments across 11 benchmark datasets, we find empiric evidence that our $\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating highly accurate performance, in some cases even outperforming competitors, and simultaneously being much more resource-friendly.

## 📝 요약

이 논문은 데이터 스트림 마이닝에서 지속 가능성을 고려한 새로운 앙상블 방법론인 HEROS를 제안합니다. 기존 방법들이 예측 성능에 치중하는 반면, HEROS는 다양한 하이퍼파라미터로 초기화된 모델 풀에서 자원 제약을 고려해 일부 모델만 선택하여 훈련합니다. 이를 위해 마르코프 결정 과정을 도입하여 예측 성능과 지속 가능성 간의 균형을 이론적으로 설명하고, 새로운 $\zeta$-정책을 제안합니다. $\zeta$-정책은 자원을 적게 사용하면서도 최적에 가까운 성능을 달성하는 것을 목표로 하며, 11개의 벤치마크 데이터셋 실험에서 높은 정확도와 자원 효율성을 입증했습니다.

## 🎯 주요 포인트

- 1. 스트림 마이닝을 위한 앙상블 방법은 데이터 분포 변화에 따라 여러 모델을 관리하고 업데이트해야 합니다.
- 2. 기존 방법들은 예측 능력에 초점을 맞추는 반면, 계산 비용을 충분히 고려하지 않아 지속 가능성이 부족합니다.
- 3. HEROS는 다양한 하이퍼파라미터로 초기화된 모델 풀에서 자원 제약 하에 훈련할 모델을 선택하는 이질적 온라인 앙상블 방법을 제안합니다.
- 4. 제안된 $\zeta$-정책은 비용을 줄이면서도 최적에 가까운 모델을 훈련하는 데 중점을 둡니다.
- 5. 실험 결과, $\zeta$-정책은 높은 정확도를 유지하면서도 자원을 덜 사용하여 경쟁자들을 능가하는 성능을 보였습니다.


---

*Generated on 2025-09-24 14:57:11*