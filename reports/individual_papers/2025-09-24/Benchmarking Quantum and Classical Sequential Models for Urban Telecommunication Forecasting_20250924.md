<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:37:43.875591",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Quantum LSTM",
    "Quantum Adaptive Self-Attention",
    "Quantum Receptance Weighted Key-Value",
    "Quantum Fast Weight Programmers",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Quantum LSTM": 0.78,
    "Quantum Adaptive Self-Attention": 0.77,
    "Quantum Receptance Weighted Key-Value": 0.76,
    "Quantum Fast Weight Programmers": 0.75,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Quantum LSTM",
        "canonical": "Quantum LSTM",
        "aliases": [
          "QLSTM"
        ],
        "category": "unique_technical",
        "rationale": "Quantum LSTM represents a novel integration of quantum computing principles with classical LSTM models, offering unique insights into sequence modeling.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Quantum Adaptive Self-Attention",
        "canonical": "Quantum Adaptive Self-Attention",
        "aliases": [
          "QASA"
        ],
        "category": "unique_technical",
        "rationale": "This model combines quantum computing with adaptive self-attention mechanisms, providing a unique approach to temporal sequence analysis.",
        "novelty_score": 0.72,
        "connectivity_score": 0.68,
        "specificity_score": 0.83,
        "link_intent_score": 0.77
      },
      {
        "surface": "Quantum Receptance Weighted Key-Value",
        "canonical": "Quantum Receptance Weighted Key-Value",
        "aliases": [
          "QRWKV"
        ],
        "category": "unique_technical",
        "rationale": "QRWKV introduces a quantum-enhanced method for handling key-value pairs in sequence models, offering distinct advantages in temporal data processing.",
        "novelty_score": 0.73,
        "connectivity_score": 0.67,
        "specificity_score": 0.82,
        "link_intent_score": 0.76
      },
      {
        "surface": "Quantum Fast Weight Programmers",
        "canonical": "Quantum Fast Weight Programmers",
        "aliases": [
          "QFWP"
        ],
        "category": "unique_technical",
        "rationale": "QFWP provides a unique approach to fast weight updates in neural networks, leveraging quantum computing for enhanced performance.",
        "novelty_score": 0.74,
        "connectivity_score": 0.66,
        "specificity_score": 0.84,
        "link_intent_score": 0.75
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial in modern sequence models, and linking to this concept can enhance understanding of the quantum adaptations.",
        "novelty_score": 0.45,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "forecasting",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Quantum LSTM",
      "resolved_canonical": "Quantum LSTM",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Quantum Adaptive Self-Attention",
      "resolved_canonical": "Quantum Adaptive Self-Attention",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.68,
        "specificity": 0.83,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Quantum Receptance Weighted Key-Value",
      "resolved_canonical": "Quantum Receptance Weighted Key-Value",
      "decision": "linked",
      "scores": {
        "novelty": 0.73,
        "connectivity": 0.67,
        "specificity": 0.82,
        "link_intent": 0.76
      }
    },
    {
      "candidate_surface": "Quantum Fast Weight Programmers",
      "resolved_canonical": "Quantum Fast Weight Programmers",
      "decision": "linked",
      "scores": {
        "novelty": 0.74,
        "connectivity": 0.66,
        "specificity": 0.84,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2508.04488.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2508.04488](https://arxiv.org/abs/2508.04488)

## 🔗 유사한 논문
- [[2025-09-22/Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning_20250922|Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning]] (80.4% similar)
- [[2025-09-23/Does quantization affect models' performance on long-context tasks?_20250923|Does quantization affect models' performance on long-context tasks?]] (80.0% similar)
- [[2025-09-23/Quantum Adaptive Self-Attention for Financial Rebalancing_ An Empirical Study on Automated Market Makers in Decentralized Finance_20250923|Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical Study on Automated Market Makers in Decentralized Finance]] (79.8% similar)
- [[2025-09-23/How Can Quantum Deep Learning Improve Large Language Models?_20250923|How Can Quantum Deep Learning Improve Large Language Models?]] (79.6% similar)
- [[2025-09-17/Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment_20250917|Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment]] (79.1% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Quantum LSTM|Quantum LSTM]], [[keywords/Quantum Adaptive Self-Attention|Quantum Adaptive Self-Attention]], [[keywords/Quantum Receptance Weighted Key-Value|Quantum Receptance Weighted Key-Value]], [[keywords/Quantum Fast Weight Programmers|Quantum Fast Weight Programmers]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2508.04488v2 Announce Type: replace-cross 
Abstract: In this study, we evaluate the performance of classical and quantum-inspired sequential models in forecasting univariate time series of incoming SMS activity (SMS-in) using the Milan Telecommunication Activity Dataset. Due to data completeness limitations, we focus exclusively on the SMS-in signal for each spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM (QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance Weighted Key-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varying input sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained to predict the next 10-minute SMS-in value based solely on historical values within a given sequence window. Our findings indicate that different models exhibit varying sensitivities to sequence length, suggesting that quantum enhancements are not universally advantageous. Rather, the effectiveness of quantum modules is highly dependent on the specific task and architectural design, reflecting inherent trade-offs among model size, parameterization strategies, and temporal modeling capabilities.

## 📝 요약

이 연구에서는 밀라노 통신 활동 데이터셋을 활용하여 SMS 수신 활동의 단변량 시계열 예측 성능을 평가했습니다. LSTM, 양자 LSTM(QLSTM), 양자 적응형 자기 주의(QASA), 양자 수용 가중 키-값(QRWKV), 양자 빠른 가중치 프로그래머(QFWP) 등 다섯 가지 모델을 비교했습니다. 각 모델은 주어진 시퀀스 창 내의 과거 값을 기반으로 다음 10분의 SMS 수신 값을 예측하도록 훈련되었습니다. 연구 결과, 모델들은 시퀀스 길이에 따라 다른 민감도를 보였으며, 양자 기술의 향상이 모든 경우에 유리하지 않음을 발견했습니다. 양자 모듈의 효과는 특정 과제와 아키텍처 설계에 크게 의존하며, 이는 모델 크기, 매개변수화 전략 및 시간 모델링 능력 간의 고유한 절충을 반영합니다.

## 🎯 주요 포인트

- 1. 본 연구는 밀라노 통신 활동 데이터셋을 사용하여 SMS-in 시계열 예측에서 고전적 및 양자 영감을 받은 순차 모델의 성능을 평가합니다.
- 2. LSTM, QLSTM, QASA, QRWKV, QFWP 등 5개의 모델을 다양한 입력 시퀀스 길이에서 비교합니다.
- 3. 연구 결과, 양자 모듈의 효과는 특정 작업과 아키텍처 설계에 따라 다르며, 시퀀스 길이에 대한 민감도가 모델마다 다르게 나타납니다.
- 4. 양자 향상이 보편적으로 유리하지 않으며, 모델 크기, 매개변수화 전략, 시간 모델링 능력 간의 고유한 절충이 존재합니다.


---

*Generated on 2025-09-24 14:37:43*