<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:28:53.388358",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Probabilistic Finite Automata",
    "Symbolic Neural Networks",
    "Matrix-Vector Multiplication",
    "Gradient Descent"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Probabilistic Finite Automata": 0.78,
    "Symbolic Neural Networks": 0.79,
    "Matrix-Vector Multiplication": 0.65,
    "Gradient Descent": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "probabilistic finite automata",
        "canonical": "Probabilistic Finite Automata",
        "aliases": [
          "PFA",
          "probabilistic automata"
        ],
        "category": "unique_technical",
        "rationale": "Probabilistic finite automata are central to the paper's contributions and provide a unique link between symbolic computation and neural networks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "symbolic feedforward neural networks",
        "canonical": "Symbolic Neural Networks",
        "aliases": [
          "symbolic feedforward networks"
        ],
        "category": "unique_technical",
        "rationale": "This term represents a novel approach to simulating PFAs, bridging symbolic computation and neural networks.",
        "novelty_score": 0.82,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.79
      },
      {
        "surface": "matrix-vector products",
        "canonical": "Matrix-Vector Multiplication",
        "aliases": [
          "matrix-vector products"
        ],
        "category": "broad_technical",
        "rationale": "Matrix-vector multiplication is a fundamental operation in the proposed neural network architecture for simulating PFAs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.6,
        "specificity_score": 0.5,
        "link_intent_score": 0.65
      },
      {
        "surface": "gradient descent-based optimization",
        "canonical": "Gradient Descent",
        "aliases": [
          "gradient descent optimization"
        ],
        "category": "broad_technical",
        "rationale": "Gradient descent is a key method for training the symbolic neural networks to simulate PFAs.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.55,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "state distributions",
      "stochastic matrices",
      "soft updates"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "probabilistic finite automata",
      "resolved_canonical": "Probabilistic Finite Automata",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "symbolic feedforward neural networks",
      "resolved_canonical": "Symbolic Neural Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "matrix-vector products",
      "resolved_canonical": "Matrix-Vector Multiplication",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.6,
        "specificity": 0.5,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "gradient descent-based optimization",
      "resolved_canonical": "Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.55,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.10034.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.10034](https://arxiv.org/abs/2509.10034)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Learning From Simulators_ A Theory of Simulation-Grounded Learning_20250924|Learning From Simulators: A Theory of Simulation-Grounded Learning]] (81.4% similar)
- [[2025-09-22/Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows_20250922|Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows]] (79.8% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (79.5% similar)
- [[2025-09-18/Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning_20250918|Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning]] (79.5% similar)
- [[2025-09-24/Fully Learnable Neural Reward Machines_20250924|Fully Learnable Neural Reward Machines]] (79.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Matrix-Vector Multiplication|Matrix-Vector Multiplication]], [[keywords/Gradient Descent|Gradient Descent]]
**âš¡ Unique Technical**: [[keywords/Probabilistic Finite Automata|Probabilistic Finite Automata]], [[keywords/Symbolic Neural Networks|Symbolic Neural Networks]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.10034v2 Announce Type: replace 
Abstract: We present a formal and constructive theory showing that probabilistic finite automata (PFAs) can be exactly simulated using symbolic feedforward neural networks. Our architecture represents state distributions as vectors and transitions as stochastic matrices, enabling probabilistic state propagation via matrix-vector products. This yields a parallel, interpretable, and differentiable simulation of PFA dynamics using soft updates-without recurrence. We formally characterize probabilistic subset construction, $\varepsilon$-closure, and exact simulation via layered symbolic computation, and prove equivalence between PFAs and specific classes of neural networks. We further show that these symbolic simulators are not only expressive but learnable: trained with standard gradient descent-based optimization on labeled sequence data, they recover the exact behavior of ground-truth PFAs. This learnability, formalized in Proposition 5.1, is the crux of this work. Our results unify probabilistic automata theory with neural architectures under a rigorous algebraic framework, bridging the gap between symbolic computation and deep learning.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í™•ë¥ ì  ìœ í•œ ì˜¤í† ë§ˆíƒ€(PFA)ë¥¼ ìƒì§•ì  í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì„ í†µí•´ ì •í™•íˆ ëª¨ì‚¬í•  ìˆ˜ ìˆëŠ” ì´ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤. ìƒíƒœ ë¶„í¬ë¥¼ ë²¡í„°ë¡œ, ì „ì´ë¥¼ í™•ë¥  í–‰ë ¬ë¡œ í‘œí˜„í•˜ì—¬ í–‰ë ¬-ë²¡í„° ê³±ì„ í†µí•´ í™•ë¥ ì  ìƒíƒœ ì „ì´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìˆœí™˜ ì—†ì´ PFAì˜ ë™ì‘ì„ ë³‘ë ¬ì ì´ê³  í•´ì„ ê°€ëŠ¥í•˜ë©° ë¯¸ë¶„ ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ í™•ë¥ ì  ë¶€ë¶„ì§‘í•© êµ¬ì„±, $\varepsilon$-íí¬, ì •í™•í•œ ì‹œë®¬ë ˆì´ì…˜ì„ ê³„ì¸µì  ìƒì§• ê³„ì‚°ìœ¼ë¡œ íŠ¹ì§•ì§“ê³ , PFAì™€ íŠ¹ì • ì‹ ê²½ë§ í´ë˜ìŠ¤ ê°„ì˜ ë™ë“±ì„±ì„ ì¦ëª…í•©ë‹ˆë‹¤. ë˜í•œ, ì´ëŸ¬í•œ ìƒì§•ì  ì‹œë®¬ë ˆì´í„°ê°€ í•™ìŠµ ê°€ëŠ¥í•˜ë©°, í‘œì¤€ ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ë¼ë²¨ì´ ìˆëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì‹¤ì œ PFAì˜ ë™ì‘ì„ ì •í™•íˆ ë³µì›í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ í•™ìŠµ ê°€ëŠ¥ì„±ì€ ë³¸ ì—°êµ¬ì˜ í•µì‹¬ ê¸°ì—¬ë¡œ, í™•ë¥ ì  ì˜¤í† ë§ˆíƒ€ ì´ë¡ ê³¼ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì—„ê²©í•œ ëŒ€ìˆ˜ì  í‹€ ì•ˆì—ì„œ í†µí•©í•˜ì—¬ ìƒì§• ê³„ì‚°ê³¼ ë”¥ëŸ¬ë‹ ê°„ì˜ ê²©ì°¨ë¥¼ í•´ì†Œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í™•ë¥ ì  ìœ í•œ ì˜¤í† ë§ˆíƒ€(PFAs)ë¥¼ ê¸°í˜¸ì  í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì„ í†µí•´ ì •í™•íˆ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆëŠ” ì´ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 2. ìƒíƒœ ë¶„í¬ë¥¼ ë²¡í„°ë¡œ, ì „ì´ë¥¼ í™•ë¥ ì  í–‰ë ¬ë¡œ í‘œí˜„í•˜ì—¬ í–‰ë ¬-ë²¡í„° ê³±ì„ í†µí•´ í™•ë¥ ì  ìƒíƒœ ì „íŒŒë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. PFAsì™€ íŠ¹ì • ì‹ ê²½ë§ í´ë˜ìŠ¤ ê°„ì˜ ë™ë“±ì„±ì„ ì¦ëª…í•˜ë©°, ì´ ê¸°í˜¸ì  ì‹œë®¬ë ˆì´í„°ê°€ í•™ìŠµ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. í‘œì¤€ ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ ìµœì í™”ë¥¼ í†µí•´ ë ˆì´ë¸”ëœ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬, ì‹¤ì œ PFAsì˜ ì •í™•í•œ ë™ì‘ì„ ë³µì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. ë³¸ ì—°êµ¬ëŠ” í™•ë¥ ì  ì˜¤í† ë§ˆíƒ€ ì´ë¡ ê³¼ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì—„ê²©í•œ ëŒ€ìˆ˜ì  í”„ë ˆì„ì›Œí¬ í•˜ì— í†µí•©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:28:53*