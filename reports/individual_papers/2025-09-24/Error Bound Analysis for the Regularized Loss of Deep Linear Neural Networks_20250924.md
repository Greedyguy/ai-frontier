<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:31:56.901728",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Regularization",
    "Error Bound Analysis",
    "Gradient Descent"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Deep Learning": 0.85,
    "Regularization": 0.82,
    "Error Bound Analysis": 0.79,
    "Gradient Descent": 0.87
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "deep linear networks",
        "canonical": "Deep Learning",
        "aliases": [
          "deep linear neural networks"
        ],
        "category": "broad_technical",
        "rationale": "Deep linear networks are a specific type of neural network architecture, closely related to the broader field of deep learning.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "regularized loss",
        "canonical": "Regularization",
        "aliases": [
          "regularized squared loss"
        ],
        "category": "specific_connectable",
        "rationale": "Regularization is a key concept in optimizing neural networks, providing a link to discussions on improving model generalization.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "error bound",
        "canonical": "Error Bound Analysis",
        "aliases": [
          "error bounds"
        ],
        "category": "unique_technical",
        "rationale": "Error bound analysis is crucial for understanding the convergence properties of optimization algorithms in neural networks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.79
      },
      {
        "surface": "gradient descent",
        "canonical": "Gradient Descent",
        "aliases": [
          "GD"
        ],
        "category": "specific_connectable",
        "rationale": "Gradient descent is a fundamental optimization method used in training neural networks, linking to broader optimization techniques.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.87
      }
    ],
    "ban_list_suggestions": [
      "optimization foundations",
      "numerical experiments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "deep linear networks",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "regularized loss",
      "resolved_canonical": "Regularization",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "error bound",
      "resolved_canonical": "Error Bound Analysis",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "gradient descent",
      "resolved_canonical": "Gradient Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.87
      }
    }
  ]
}
-->

# Error Bound Analysis for the Regularized Loss of Deep Linear Neural Networks

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2502.11152.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2502.11152](https://arxiv.org/abs/2502.11152)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Diagonal Linear Networks and the Lasso Regularization Path_20250924|Diagonal Linear Networks and the Lasso Regularization Path]] (84.8% similar)
- [[2025-09-17/A Neural Network for the Identical Kuramoto Equation_ Architectural Considerations and Performance Evaluation_20250917|A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation]] (84.3% similar)
- [[2025-09-23/Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks_20250923|Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks]] (83.3% similar)
- [[2025-09-22/Flavors of Margin_ Implicit Bias of Steepest Descent in Homogeneous Neural Networks_20250922|Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks]] (82.4% similar)
- [[2025-09-22/Adversarial generalization of unfolding (model-based) networks_20250922|Adversarial generalization of unfolding (model-based) networks]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Regularization|Regularization]], [[keywords/Gradient Descent|Gradient Descent]]
**âš¡ Unique Technical**: [[keywords/Error Bound Analysis|Error Bound Analysis]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.11152v3 Announce Type: replace-cross 
Abstract: The optimization foundations of deep linear networks have recently received significant attention. However, due to their inherent non-convexity and hierarchical structure, analyzing the loss functions of deep linear networks remains a challenging task. In this work, we study the local geometric landscape of the regularized squared loss of deep linear networks around each critical point. Specifically, we derive a closed-form characterization of the critical point set and establish an error bound for the regularized loss under mild conditions on network width and regularization parameters. Notably, this error bound quantifies the distance from a point to the critical point set in terms of the current gradient norm, which can be used to derive linear convergence of first-order methods. To support our theoretical findings, we conduct numerical experiments and demonstrate that gradient descent converges linearly to a critical point when optimizing the regularized loss of deep linear networks.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¬ì¸µ ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ ìµœì í™” ê¸°ì´ˆë¥¼ ì—°êµ¬í•˜ë©°, íŠ¹íˆ ë¹„ë³¼ë¡ì„±ê³¼ ê³„ì¸µ êµ¬ì¡°ë¡œ ì¸í•´ ì†ì‹¤ í•¨ìˆ˜ ë¶„ì„ì´ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì €ìë“¤ì€ ì‹¬ì¸µ ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ ì •ê·œí™”ëœ ì œê³± ì†ì‹¤ì˜ êµ­ì†Œ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³ , ì„ê³„ì  ì§‘í•©ì˜ ë‹«íŒ í˜•íƒœë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë„¤íŠ¸ì›Œí¬ì˜ í­ê³¼ ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì¡°ê±´ í•˜ì—ì„œ ì •ê·œí™”ëœ ì†ì‹¤ì— ëŒ€í•œ ì˜¤ë¥˜ ê²½ê³„ë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ ê²½ê³„ëŠ” í˜„ì¬ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ì„ê³„ì  ì§‘í•©ê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ì •ëŸ‰í™”í•˜ë©°, ì´ë¥¼ í†µí•´ 1ì°¨ ë°©ë²•ì˜ ì„ í˜• ìˆ˜ë ´ì„ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¡ ì  ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•˜ê¸° ìœ„í•´ ìˆ˜ì¹˜ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìœ¼ë©°, ê·¸ë˜ë””ì–¸íŠ¸ í•˜ê°•ë²•ì´ ì •ê·œí™”ëœ ì†ì‹¤ì„ ìµœì í™”í•  ë•Œ ì„ê³„ì ìœ¼ë¡œ ì„ í˜• ìˆ˜ë ´í•¨ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë”¥ ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ ìµœì í™” ê¸°ì´ˆëŠ” ë¹„ì„ í˜•ì„±ê³¼ ê³„ì¸µ êµ¬ì¡°ë¡œ ì¸í•´ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë”¥ ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ ì •ê·œí™”ëœ ì œê³± ì†ì‹¤ì˜ êµ­ì†Œ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- 3. ê° ì„ê³„ì  ì£¼ë³€ì—ì„œ ì„ê³„ì  ì§‘í•©ì˜ ë‹«íŒ í˜•íƒœë¥¼ ìœ ë„í•˜ê³ , ë„¤íŠ¸ì›Œí¬ í­ê³¼ ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì¡°ê±´ í•˜ì— ì˜¤ì°¨ ê²½ê³„ë¥¼ í™•ë¦½í•©ë‹ˆë‹¤.
- 4. ì˜¤ì°¨ ê²½ê³„ëŠ” í˜„ì¬ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ì„ í†µí•´ ì„ê³„ì  ì§‘í•©ê³¼ì˜ ê±°ë¦¬ë¥¼ ì •ëŸ‰í™”í•˜ë©°, ì´ë¥¼ í†µí•´ 1ì°¨ ë°©ë²•ì˜ ì„ í˜• ìˆ˜ë ´ì„ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸ê°€ ë”¥ ì„ í˜• ë„¤íŠ¸ì›Œí¬ì˜ ì •ê·œí™”ëœ ì†ì‹¤ì„ ìµœì í™”í•  ë•Œ ì„ê³„ì ìœ¼ë¡œ ì„ í˜• ìˆ˜ë ´í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:31:56*