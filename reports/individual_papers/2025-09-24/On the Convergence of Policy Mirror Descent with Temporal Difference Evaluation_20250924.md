<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:13:40.708491",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Policy Mirror Descent",
    "Temporal Difference Learning",
    "Machine Learning",
    "Generative Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Policy Mirror Descent": 0.8,
    "Temporal Difference Learning": 0.75,
    "Machine Learning": 0.7,
    "Generative Model": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Policy Mirror Descent",
        "canonical": "Policy Mirror Descent",
        "aliases": [
          "PMD"
        ],
        "category": "unique_technical",
        "rationale": "A specific reinforcement learning framework that is central to the paper's contributions.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Temporal Difference Evaluation",
        "canonical": "Temporal Difference Learning",
        "aliases": [
          "TD Learning",
          "TD Evaluation"
        ],
        "category": "specific_connectable",
        "rationale": "A key method in reinforcement learning that connects to broader temporal difference methods.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Machine Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "A fundamental area of machine learning that underpins the study.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      },
      {
        "surface": "Generative Model",
        "canonical": "Generative Model",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Important for understanding sample complexity in the context of the paper.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "result",
      "evaluation"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Policy Mirror Descent",
      "resolved_canonical": "Policy Mirror Descent",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Temporal Difference Evaluation",
      "resolved_canonical": "Temporal Difference Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Machine Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Generative Model",
      "resolved_canonical": "Generative Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18822.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18822](https://arxiv.org/abs/2509.18822)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Deep Reinforcement Learning with Gradient Eligibility Traces_20250922|Deep Reinforcement Learning with Gradient Eligibility Traces]] (83.4% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (82.2% similar)
- [[2025-09-23/Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs_20250923|Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs]] (81.9% similar)
- [[2025-09-23/Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm_20250923|Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm]] (81.7% similar)
- [[2025-09-22/Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses_20250922|Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses]] (81.7% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Machine Learning|Machine Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Temporal Difference Learning|Temporal Difference Learning]], [[keywords/Generative Model|Generative Model]]
**âš¡ Unique Technical**: [[keywords/Policy Mirror Descent|Policy Mirror Descent]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18822v1 Announce Type: cross 
Abstract: Policy mirror descent (PMD) is a general policy optimization framework in reinforcement learning, which can cover a wide range of typical policy optimization methods by specifying different mirror maps. Existing analysis of PMD requires exact or approximate evaluation (for example unbiased estimation via Monte Carlo simulation) of action values solely based on policy. In this paper, we consider policy mirror descent with temporal difference evaluation (TD-PMD). It is shown that, given the access to exact policy evaluations, the dimension-free $O(1/T)$ sublinear convergence still holds for TD-PMD with any constant step size and any initialization. In order to achieve this result, new monotonicity and shift invariance arguments have been developed. The dimension free $\gamma$-rate linear convergence of TD-PMD is also established provided the step size is selected adaptively. For the two common instances of TD-PMD (i.e., TD-PQA and TD-NPG), it is further shown that they enjoy the convergence in the policy domain. Additionally, we investigate TD-PMD in the inexact setting and give the sample complexity for it to achieve the last iterate $\varepsilon$-optimality under a generative model, which improves the last iterate sample complexity for PMD over the dependence on $1/(1-\gamma)$.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì—ì„œ ì •ì±… ìµœì í™” í”„ë ˆì„ì›Œí¬ì¸ ì •ì±… ë¯¸ëŸ¬ ê°•í•˜(PMD)ì˜ ë³€í˜•ì¸ TD-PMDë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ PMD ë¶„ì„ì€ ì •ì±…ì— ê¸°ë°˜í•œ í–‰ë™ ê°€ì¹˜ì˜ ì •í™•í•œ í‰ê°€ë¥¼ ìš”êµ¬í•˜ì§€ë§Œ, ì´ ì—°êµ¬ì—ì„œëŠ” ì‹œì°¨ ì°¨ì´ í‰ê°€ë¥¼ ì‚¬ìš©í•˜ëŠ” TD-PMDë¥¼ ë„ì…í•˜ì—¬ ì´ëŸ¬í•œ ì œí•œì„ ì™„í™”í•©ë‹ˆë‹¤. TD-PMDëŠ” ì •í™•í•œ ì •ì±… í‰ê°€ê°€ ê°€ëŠ¥í•  ë•Œ, ì°¨ì›ì— ìƒê´€ì—†ì´ $O(1/T)$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ìœ ì§€í•˜ë©°, ì ì‘ì ì¸ ìŠ¤í… í¬ê¸°ë¥¼ ì„ íƒí•˜ë©´ ì„ í˜• ìˆ˜ë ´ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. TD-PQAì™€ TD-NPG ê°™ì€ TD-PMDì˜ ì¼ë°˜ì ì¸ ì‚¬ë¡€ì—ì„œë„ ì •ì±… ë„ë©”ì¸ ë‚´ ìˆ˜ë ´ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìƒì„± ëª¨ë¸ í•˜ì—ì„œ ê·¼ì‚¬ ì„¤ì •ì—ì„œì˜ ìƒ˜í”Œ ë³µì¡ì„±ì„ ë¶„ì„í•˜ì—¬, ê¸°ì¡´ PMDë³´ë‹¤ $1/(1-\gamma)$ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì •ì±… ë¯¸ëŸ¬ í•˜ê°•(PMD)ì€ ê°•í™” í•™ìŠµì—ì„œ ë‹¤ì–‘í•œ ì •ì±… ìµœì í™” ë°©ë²•ì„ í¬ê´„í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ì •ì±… ìµœì í™” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹œì°¨ ì°¨ì´ í‰ê°€(TD)ë¥¼ ì‚¬ìš©í•˜ëŠ” ì •ì±… ë¯¸ëŸ¬ í•˜ê°•(TD-PMD)ì„ ê³ ë ¤í•©ë‹ˆë‹¤.
- 3. TD-PMDëŠ” ì •í™•í•œ ì •ì±… í‰ê°€ì— ì ‘ê·¼í•  ìˆ˜ ìˆì„ ë•Œ, ì°¨ì›ì— ë¬´ê´€í•˜ê²Œ $O(1/T)$ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- 4. TD-PMDì˜ ë‘ ê°€ì§€ ì¼ë°˜ì ì¸ ì‚¬ë¡€(TD-PQA ë° TD-NPG)ëŠ” ì •ì±… ë„ë©”ì¸ì—ì„œì˜ ìˆ˜ë ´ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- 5. ìƒì„± ëª¨ë¸ í•˜ì—ì„œ TD-PMDì˜ ìƒ˜í”Œ ë³µì¡ì„±ì„ ë¶„ì„í•˜ì—¬, ë§ˆì§€ë§‰ ë°˜ë³µì—ì„œ $\varepsilon$-ìµœì ì„±ì„ ë‹¬ì„±í•˜ëŠ” ë° í•„ìš”í•œ ìƒ˜í”Œ ë³µì¡ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:13:40*