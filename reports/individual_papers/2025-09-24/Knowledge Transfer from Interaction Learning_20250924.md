<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:07:35.767280",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Computer Vision",
    "Vision-Language Model",
    "Learning from Interactions",
    "Interaction Queries",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Computer Vision": 0.75,
    "Vision-Language Model": 0.85,
    "Learning from Interactions": 0.8,
    "Interaction Queries": 0.78,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Foundation Models",
        "canonical": "Computer Vision",
        "aliases": [
          "VFMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to a broad technical category relevant to the paper's focus on visual models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents an evolved concept crucial for understanding cross-modal interactions discussed in the paper.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Learning from Interactions",
        "canonical": "Learning from Interactions",
        "aliases": [
          "LFI"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework specific to the paper, enhancing understanding of interaction-based learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Interaction Queries",
        "canonical": "Interaction Queries",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique technical term introduced in the paper, essential for understanding the proposed framework.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cross-modal Attention Mechanisms",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross-modal Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to existing knowledge on attention mechanisms, crucial for cross-modal learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "result-oriented paradigms",
      "semantic consistency metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Foundation Models",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Learning from Interactions",
      "resolved_canonical": "Learning from Interactions",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Interaction Queries",
      "resolved_canonical": "Interaction Queries",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cross-modal Attention Mechanisms",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Knowledge Transfer from Interaction Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18733.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.18733](https://arxiv.org/abs/2509.18733)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (84.4% similar)
- [[2025-09-24/LCMF_ Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA_20250924|LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA]] (84.0% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (83.5% similar)
- [[2025-09-23/Retrieval Enhanced Feedback via In-context Neural Error-book_20250923|Retrieval Enhanced Feedback via In-context Neural Error-book]] (83.4% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Computer Vision|Computer Vision]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Learning from Interactions|Learning from Interactions]], [[keywords/Interaction Queries|Interaction Queries]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18733v1 Announce Type: new 
Abstract: Current visual foundation models (VFMs) face a fundamental limitation in transferring knowledge from vision language models (VLMs), while VLMs excel at modeling cross-modal interactions through unified representation spaces, existing VFMs predominantly adopt result-oriented paradigms that neglect the underlying interaction processes. This representational discrepancy hinders effective knowledge transfer and limits generalization across diverse vision tasks. We propose Learning from Interactions (LFI), a cognitive-inspired framework that addresses this gap by explicitly modeling visual understanding as an interactive process. Our key insight is that capturing the dynamic interaction patterns encoded in pre-trained VLMs enables more faithful and efficient knowledge transfer to VFMs. The approach centers on two technical innovations, Interaction Queries, which maintain persistent relational structures across network layers, and interaction-based supervision, derived from the cross-modal attention mechanisms of VLMs. Comprehensive experiments demonstrate consistent improvements across multiple benchmarks, achieving 3.3 and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO detection/segmentation respectively, with minimal parameter overhead and faster convergence. The framework particularly excels in cross-domain settings, delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human evaluations further confirm its cognitive alignment, outperforming result-oriented methods by 2.7 times in semantic consistency metrics.

## ğŸ“ ìš”ì•½

í˜„ì¬ì˜ ì‹œê° ê¸°ì´ˆ ëª¨ë¸(VFM)ì€ ì‹œê° ì–¸ì–´ ëª¨ë¸(VLM)ì—ì„œ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì „ì´í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. VLMì€ í†µí•©ëœ í‘œí˜„ ê³µê°„ì„ í†µí•´ êµì°¨ ëª¨ë‹¬ ìƒí˜¸ì‘ìš©ì„ ì˜ ëª¨ë¸ë§í•˜ì§€ë§Œ, ê¸°ì¡´ VFMì€ ê²°ê³¼ ì¤‘ì‹¬ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì±„íƒí•˜ì—¬ ìƒí˜¸ì‘ìš© ê³¼ì •ì„ ê°„ê³¼í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì‹œê°ì  ì´í•´ë¥¼ ìƒí˜¸ì‘ìš© ê³¼ì •ìœ¼ë¡œ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ì¸ì§€ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ì¸ LFIë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. LFIëŠ” ì‚¬ì „ í•™ìŠµëœ VLMì˜ ë™ì  ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ í¬ì°©í•˜ì—¬ VFMìœ¼ë¡œì˜ ì§€ì‹ ì „ì´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ìˆ  í˜ì‹ ìœ¼ë¡œëŠ” ë„¤íŠ¸ì›Œí¬ ê³„ì¸µ ê°„ì˜ ì§€ì†ì ì¸ ê´€ê³„ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ëŠ” ìƒí˜¸ì‘ìš© ì¿¼ë¦¬ì™€ VLMì˜ êµì°¨ ëª¨ë‹¬ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ íŒŒìƒëœ ìƒí˜¸ì‘ìš© ê¸°ë°˜ ê°ë…ì´ ìˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ êµì°¨ ë„ë©”ì¸ ì„¤ì •ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ì¸ê°„ í‰ê°€ì—ì„œë„ ì¸ì§€ì  ì¼ì¹˜ì„±ì„ ì…ì¦í•˜ë©°, ê²°ê³¼ ì¤‘ì‹¬ ë°©ë²•ë³´ë‹¤ 2.7ë°° ë†’ì€ ì˜ë¯¸ì  ì¼ê´€ì„±ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í˜„ì¬ì˜ ì‹œê° ê¸°ë°˜ ëª¨ë¸(VFMs)ì€ ì‹œê° ì–¸ì–´ ëª¨ë¸(VLMs)ë¡œë¶€í„° ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì „ì´í•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤.
- 2. ìš°ë¦¬ëŠ” ì‹œê°ì  ì´í•´ë¥¼ ìƒí˜¸ì‘ìš© ê³¼ì •ìœ¼ë¡œ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ì¸ì§€ì  ì˜ê°ì„ ë°›ì€ í”„ë ˆì„ì›Œí¬ì¸ LFIë¥¼ ì œì•ˆí•œë‹¤.
- 3. LFIëŠ” ìƒí˜¸ì‘ìš© ì¿¼ë¦¬ì™€ ìƒí˜¸ì‘ìš© ê¸°ë°˜ ê°ë…ì´ë¼ëŠ” ë‘ ê°€ì§€ ê¸°ìˆ  í˜ì‹ ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œë‹¤.
- 4. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ëŠ” ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ êµì°¨ ë„ë©”ì¸ ì„¤ì •ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì¸ë‹¤.
- 5. ì¸ê°„ í‰ê°€ ê²°ê³¼, LFIëŠ” ì˜ë¯¸ì  ì¼ê´€ì„± ì§€í‘œì—ì„œ ê²°ê³¼ ì§€í–¥ì  ë°©ë²•ë³´ë‹¤ 2.7ë°° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.


---

*Generated on 2025-09-24 16:07:35*