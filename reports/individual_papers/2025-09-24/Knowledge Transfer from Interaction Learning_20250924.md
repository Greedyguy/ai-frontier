<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:07:35.767280",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Computer Vision",
    "Vision-Language Model",
    "Learning from Interactions",
    "Interaction Queries",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Computer Vision": 0.75,
    "Vision-Language Model": 0.85,
    "Learning from Interactions": 0.8,
    "Interaction Queries": 0.78,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Foundation Models",
        "canonical": "Computer Vision",
        "aliases": [
          "VFMs"
        ],
        "category": "broad_technical",
        "rationale": "Links to a broad technical category relevant to the paper's focus on visual models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.75
      },
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents an evolved concept crucial for understanding cross-modal interactions discussed in the paper.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Learning from Interactions",
        "canonical": "Learning from Interactions",
        "aliases": [
          "LFI"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel framework specific to the paper, enhancing understanding of interaction-based learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Interaction Queries",
        "canonical": "Interaction Queries",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique technical term introduced in the paper, essential for understanding the proposed framework.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Cross-modal Attention Mechanisms",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross-modal Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Connects to existing knowledge on attention mechanisms, crucial for cross-modal learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "result-oriented paradigms",
      "semantic consistency metrics"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Foundation Models",
      "resolved_canonical": "Computer Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Learning from Interactions",
      "resolved_canonical": "Learning from Interactions",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Interaction Queries",
      "resolved_canonical": "Interaction Queries",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Cross-modal Attention Mechanisms",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Knowledge Transfer from Interaction Learning

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18733.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.18733](https://arxiv.org/abs/2509.18733)

## 🔗 유사한 논문
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (84.4% similar)
- [[2025-09-24/LCMF_ Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA_20250924|LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA]] (84.0% similar)
- [[2025-09-23/When Big Models Train Small Ones_ Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs_20250923|When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs]] (83.5% similar)
- [[2025-09-23/Retrieval Enhanced Feedback via In-context Neural Error-book_20250923|Retrieval Enhanced Feedback via In-context Neural Error-book]] (83.4% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (83.4% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Computer Vision|Computer Vision]]
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Learning from Interactions|Learning from Interactions]], [[keywords/Interaction Queries|Interaction Queries]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18733v1 Announce Type: new 
Abstract: Current visual foundation models (VFMs) face a fundamental limitation in transferring knowledge from vision language models (VLMs), while VLMs excel at modeling cross-modal interactions through unified representation spaces, existing VFMs predominantly adopt result-oriented paradigms that neglect the underlying interaction processes. This representational discrepancy hinders effective knowledge transfer and limits generalization across diverse vision tasks. We propose Learning from Interactions (LFI), a cognitive-inspired framework that addresses this gap by explicitly modeling visual understanding as an interactive process. Our key insight is that capturing the dynamic interaction patterns encoded in pre-trained VLMs enables more faithful and efficient knowledge transfer to VFMs. The approach centers on two technical innovations, Interaction Queries, which maintain persistent relational structures across network layers, and interaction-based supervision, derived from the cross-modal attention mechanisms of VLMs. Comprehensive experiments demonstrate consistent improvements across multiple benchmarks, achieving 3.3 and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO detection/segmentation respectively, with minimal parameter overhead and faster convergence. The framework particularly excels in cross-domain settings, delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human evaluations further confirm its cognitive alignment, outperforming result-oriented methods by 2.7 times in semantic consistency metrics.

## 📝 요약

현재의 시각 기초 모델(VFM)은 시각 언어 모델(VLM)에서 지식을 효과적으로 전이하는 데 한계가 있습니다. VLM은 통합된 표현 공간을 통해 교차 모달 상호작용을 잘 모델링하지만, 기존 VFM은 결과 중심의 패러다임을 채택하여 상호작용 과정을 간과합니다. 이를 해결하기 위해, 우리는 시각적 이해를 상호작용 과정으로 명시적으로 모델링하는 인지 기반 프레임워크인 LFI를 제안합니다. LFI는 사전 학습된 VLM의 동적 상호작용 패턴을 포착하여 VFM으로의 지식 전이를 개선합니다. 주요 기술 혁신으로는 네트워크 계층 간의 지속적인 관계 구조를 유지하는 상호작용 쿼리와 VLM의 교차 모달 주의 메커니즘에서 파생된 상호작용 기반 감독이 있습니다. 실험 결과, 다양한 벤치마크에서 일관된 성능 향상을 보였으며, 특히 교차 도메인 설정에서 뛰어난 성과를 보였습니다. 인간 평가에서도 인지적 일치성을 입증하며, 결과 중심 방법보다 2.7배 높은 의미적 일관성을 달성했습니다.

## 🎯 주요 포인트

- 1. 현재의 시각 기반 모델(VFMs)은 시각 언어 모델(VLMs)로부터 지식을 효과적으로 전이하는 데 한계가 있다.
- 2. 우리는 시각적 이해를 상호작용 과정으로 명시적으로 모델링하는 인지적 영감을 받은 프레임워크인 LFI를 제안한다.
- 3. LFI는 상호작용 쿼리와 상호작용 기반 감독이라는 두 가지 기술 혁신을 중심으로 한다.
- 4. 제안된 프레임워크는 여러 벤치마크에서 일관된 성능 향상을 보여주며, 특히 교차 도메인 설정에서 뛰어난 성과를 보인다.
- 5. 인간 평가 결과, LFI는 의미적 일관성 지표에서 결과 지향적 방법보다 2.7배 우수한 성능을 보였다.


---

*Generated on 2025-09-24 16:07:35*