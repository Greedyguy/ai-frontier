<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:33:27.829751",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision Diffusion Model",
    "Transformer",
    "Action Recognition",
    "Generalization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision Diffusion Model": 0.78,
    "Transformer": 0.85,
    "Action Recognition": 0.82,
    "Generalization": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision Diffusion Model",
        "canonical": "Vision Diffusion Model",
        "aliases": [
          "VDM"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel approach in action recognition, enhancing semantic feature extraction.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A key component in aggregating features, linking to existing transformer-based models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Action Recognition",
        "canonical": "Action Recognition",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Central to the paper's theme, connecting to broader research in recognizing human and animal actions.",
        "novelty_score": 0.4,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      },
      {
        "surface": "Generalization",
        "canonical": "Generalization",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Focuses on the model's ability to adapt across different domains, a key research challenge.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.65,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "model",
      "process",
      "features"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision Diffusion Model",
      "resolved_canonical": "Vision Diffusion Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Action Recognition",
      "resolved_canonical": "Action Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Generalization",
      "resolved_canonical": "Generalization",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.65,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Diffusion-Based Action Recognition Generalizes to Untrained Domains

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.08908.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.08908](https://arxiv.org/abs/2509.08908)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/KRAST_ Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models_20250923|KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models]] (83.4% similar)
- [[2025-09-22/Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception_20250922|Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception]] (82.1% similar)
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (82.0% similar)
- [[2025-09-23/DINOv3-Diffusion Policy_ Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning_20250923|DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning]] (81.7% similar)
- [[2025-09-23/KungfuBot2_ Learning Versatile Motion Skills for Humanoid Whole-Body Control_20250923|KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control]] (81.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Action Recognition|Action Recognition]], [[keywords/Generalization|Generalization]]
**âš¡ Unique Technical**: [[keywords/Vision Diffusion Model|Vision Diffusion Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.08908v3 Announce Type: replace 
Abstract: Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: https://www.vision.caltech.edu/actiondiff. Code: https://github.com/frankyaoxiao/ActionDiff

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ì´ ë‹¤ì–‘í•œ ë§¥ë½ê³¼ ì‹œì ì—ì„œ ë™ì¼í•œ í–‰ë™ì„ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì— ì ìš©í•˜ê³ ì í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ Vision Diffusion Model(VDM)ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ íŠ¹ì§•ì„ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì§‘ê³„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ í™•ì‚° ê³¼ì •ì˜ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì¡°ê±´í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í”½ì…€ ìˆ˜ì¤€ì˜ ì„¸ë¶€ì‚¬í•­ë³´ë‹¤ ì˜ë¯¸ë¡ ì  ì •ë³´ë¥¼ ê°•ì¡°í•¨ìœ¼ë¡œì¨ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë™ë¬¼ ì¢…, ì‹œì , ë…¹í™” ë§¥ë½ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ í–‰ë™ì„ ë¶„ë¥˜í•˜ëŠ” ì‹¤í—˜ì„ í†µí•´ ì´ ì ‘ê·¼ë²•ì˜ ì¼ë°˜í™” íŠ¹ì„±ì„ íƒêµ¬í•˜ì˜€ìœ¼ë©°, ëª¨ë“  ì¼ë°˜í™” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì‹  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì—¬ ê¸°ê³„ì˜ í–‰ë™ ì¸ì‹ì´ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ê²¬ê³ ì„±ì— í•œ ê±¸ìŒ ë” ë‹¤ê°€ì„°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê°„ì€ ë‹¤ì–‘í•œ ë§¥ë½ê³¼ ê´€ì ì˜ ë³€í™”ë¥¼ ì´ˆì›”í•˜ì—¬ ë™ì¼í•œ í–‰ë™ì„ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë°˜ë©´, í˜„ì¬ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì´ëŸ¬í•œ ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.
- 2. Vision Diffusion Model(VDM)ì„ í™œìš©í•˜ì—¬ ì¶”ì¶œëœ íŠ¹ì§•ì„ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì§‘ê³„í•˜ì—¬ ì¸ê°„ê³¼ ìœ ì‚¬í•œ í–‰ë™ ì¸ì‹ì„ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3. í™•ì‚° ê³¼ì •ì˜ ì´ˆê¸° íƒ€ì„ìŠ¤í…ì— ì¡°ê±´í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í”½ì…€ ìˆ˜ì¤€ì˜ ì„¸ë¶€ì‚¬í•­ë³´ë‹¤ ì˜ë¯¸ë¡ ì  ì •ë³´ë¥¼ ê°•ì¡°í•¨ìœ¼ë¡œì¨ ì¼ë°˜í™”ê°€ í–¥ìƒë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 4. ì œì•ˆëœ ëª¨ë¸ì€ ë™ë¬¼ ì¢…, ì‹œì , ë…¹í™” ë§¥ë½ì— ê±¸ì¹œ í–‰ë™ ë¶„ë¥˜ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ì—¬ ê¸°ê³„ì˜ í–‰ë™ ì¸ì‹ì´ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ê²¬ê³ ì„±ì— ê°€ê¹Œì›Œì¡ŒìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:33:27*