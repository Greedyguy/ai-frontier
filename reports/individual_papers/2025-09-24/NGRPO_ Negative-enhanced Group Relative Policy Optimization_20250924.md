<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:05:28.261704",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Advantage Calibration",
    "Asymmetric Clipping",
    "Mathematical Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.82,
    "Advantage Calibration": 0.78,
    "Asymmetric Clipping": 0.77,
    "Mathematical Reasoning": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion on reasoning capabilities, linking to broader AI research.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.82
      },
      {
        "surface": "Advantage Calibration",
        "canonical": "Advantage Calibration",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This is a novel mechanism introduced in the paper, crucial for converting homogeneous errors into learning signals.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Asymmetric Clipping",
        "canonical": "Asymmetric Clipping",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A unique technique proposed to stabilize exploration pressure, enhancing the learning process.",
        "novelty_score": 0.7,
        "connectivity_score": 0.58,
        "specificity_score": 0.79,
        "link_intent_score": 0.77
      },
      {
        "surface": "Mathematical Reasoning",
        "canonical": "Mathematical Reasoning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "The paper's focus on improving mathematical reasoning links it to specific AI tasks and benchmarks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Advantage Calibration",
      "resolved_canonical": "Advantage Calibration",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Asymmetric Clipping",
      "resolved_canonical": "Asymmetric Clipping",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.58,
        "specificity": 0.79,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Mathematical Reasoning",
      "resolved_canonical": "Mathematical Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# NGRPO: Negative-enhanced Group Relative Policy Optimization

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18851.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18851](https://arxiv.org/abs/2509.18851)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/GRPO-LEAD_ A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models_20250923|GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models]] (87.9% similar)
- [[2025-09-24/MAPO_ Mixed Advantage Policy Optimization_20250924|MAPO: Mixed Advantage Policy Optimization]] (87.4% similar)
- [[2025-09-23/GRPOformer_ Advancing Hyperparameter Optimization via Group Relative Policy Optimization_20250923|GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization]] (85.9% similar)
- [[2025-09-22/PVPO_ Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning_20250922|PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning]] (85.6% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (85.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Mathematical Reasoning|Mathematical Reasoning]]
**âš¡ Unique Technical**: [[keywords/Advantage Calibration|Advantage Calibration]], [[keywords/Asymmetric Clipping|Asymmetric Clipping]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18851v1 Announce Type: cross 
Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

## ğŸ“ ìš”ì•½

RLVRëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ëŒ€í‘œì ì¸ RLVR ì•Œê³ ë¦¬ì¦˜ì¸ GRPOëŠ” ëª¨ë“  ì‘ë‹µì´ ì „ë¶€ ë§ê±°ë‚˜ í‹€ë¦° ê²½ìš° í•™ìŠµì´ ì–´ë ¤ìš´ í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. íŠ¹íˆ, ëª¨ë‘ í‹€ë¦° ê·¸ë£¹ì—ì„œëŠ” í•™ìŠµ ì‹ í˜¸ê°€ ì‚¬ë¼ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” NGRPO(Negative-enhanced Group Relative Policy Optimization)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. NGRPOëŠ” Advantage Calibrationì„ ë„ì…í•˜ì—¬ ê°€ìƒì˜ ìµœëŒ€ ë³´ìƒ ìƒ˜í”Œì„ ê°€ì •í•¨ìœ¼ë¡œì¨, í‹€ë¦° ìƒ˜í”Œì˜ ì´ì ì´ 0ì´ ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, Asymmetric Clippingì„ í†µí•´ ê¸ì •ì  ìƒ˜í”Œì˜ ì—…ë°ì´íŠ¸ë¥¼ ì™„í™”í•˜ê³  ë¶€ì •ì  ìƒ˜í”Œì—ëŠ” ë” ì—„ê²©í•œ ì œí•œì„ ê°€í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤. Qwen2.5-Math-7B ì‹¤í—˜ ê²°ê³¼, NGRPOëŠ” MATH500, AMC23, AIME2025 ë“±ì˜ ìˆ˜í•™ì  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” NGRPOê°€ ë™ì§ˆì  ì˜¤ë¥˜ë¡œë¶€í„° íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. GRPO ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë“  ì‘ë‹µì´ ì™„ì „íˆ ë§ê±°ë‚˜ í‹€ë¦° ê²½ìš° í•™ìŠµ ì‹ í˜¸ë¥¼ ìƒëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
- 2. NGRPOëŠ” ë™ì§ˆì ì¸ ì˜¤ë¥˜ë¥¼ ê°•ë ¥í•œ í•™ìŠµ ì‹ í˜¸ë¡œ ì „í™˜í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- 3. Advantage Calibrationì„ í†µí•´ ë™ì§ˆì ìœ¼ë¡œ í‹€ë¦° ìƒ˜í”Œì˜ ì´ì ì´ 0ì´ ë˜ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
- 4. Asymmetric Clippingì„ ì‚¬ìš©í•˜ì—¬ ê¸ì •ì  ìƒ˜í”Œì˜ ì—…ë°ì´íŠ¸ í¬ê¸°ë¥¼ ì™„í™”í•˜ê³  ë¶€ì •ì  ìƒ˜í”Œì—ëŠ” ë” ì—„ê²©í•œ ì œì•½ì„ ê°€í•©ë‹ˆë‹¤.
- 5. NGRPOëŠ” ìˆ˜í•™ì  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë™ì§ˆì  ì˜¤ë¥˜ë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:05:28*