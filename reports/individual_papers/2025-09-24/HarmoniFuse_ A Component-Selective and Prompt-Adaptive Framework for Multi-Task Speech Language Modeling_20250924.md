<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:46:59.016479",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "HarmoniFuse Framework",
    "Multi-Task Speech Language Modeling",
    "Automatic Speech Recognition",
    "Speech Emotion Recognition",
    "Gated Speech Encoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "HarmoniFuse Framework": 0.78,
    "Multi-Task Speech Language Modeling": 0.82,
    "Automatic Speech Recognition": 0.75,
    "Speech Emotion Recognition": 0.8,
    "Gated Speech Encoder": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "HarmoniFuse",
        "canonical": "HarmoniFuse Framework",
        "aliases": [
          "HarmoniFuse"
        ],
        "category": "unique_technical",
        "rationale": "HarmoniFuse represents a novel framework specifically designed for multi-task speech language modeling, offering unique insights into task-specific component selection and fusion.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "multi-task speech language modeling",
        "canonical": "Multi-Task Speech Language Modeling",
        "aliases": [
          "multi-task SLM"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper and connects with broader themes in multi-task learning and speech processing.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "automatic speech recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "broad_technical",
        "rationale": "ASR is a fundamental task in speech processing, providing a strong link to related research in speech and language technologies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "speech emotion recognition",
        "canonical": "Speech Emotion Recognition",
        "aliases": [
          "SER"
        ],
        "category": "specific_connectable",
        "rationale": "SER is a specialized task that complements ASR, enhancing the understanding of emotional context in speech.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "gated speech encoder",
        "canonical": "Gated Speech Encoder",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This component is a unique aspect of the proposed framework, crucial for extracting task-specific acoustic features.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "HarmoniFuse",
      "resolved_canonical": "HarmoniFuse Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multi-task speech language modeling",
      "resolved_canonical": "Multi-Task Speech Language Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "automatic speech recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "speech emotion recognition",
      "resolved_canonical": "Speech Emotion Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "gated speech encoder",
      "resolved_canonical": "Gated Speech Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18570.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18570](https://arxiv.org/abs/2509.18570)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Whisper-UT_ A Unified Translation Framework for Speech and Text_20250923|Whisper-UT: A Unified Translation Framework for Speech and Text]] (83.4% similar)
- [[2025-09-22/AS-ASR_ A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition_20250922|AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition]] (82.5% similar)
- [[2025-09-24/Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs_20250924|Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs]] (82.4% similar)
- [[2025-09-19/SpeechOp_ Inference-Time Task Composition for Generative Speech Processing_20250919|SpeechOp: Inference-Time Task Composition for Generative Speech Processing]] (82.4% similar)
- [[2025-09-24/DeepResonance_ Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning_20250924|DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning]] (82.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]]
**ğŸ”— Specific Connectable**: [[keywords/Multi-Task Speech Language Modeling|Multi-Task Speech Language Modeling]], [[keywords/Speech Emotion Recognition|Speech Emotion Recognition]]
**âš¡ Unique Technical**: [[keywords/HarmoniFuse Framework|HarmoniFuse Framework]], [[keywords/Gated Speech Encoder|Gated Speech Encoder]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18570v1 Announce Type: cross 
Abstract: Recent advances in large language models have facilitated the development of unified speech language models (SLMs) capable of supporting multiple speech tasks within a shared architecture. However, tasks such as automatic speech recognition (ASR) and speech emotion recognition (SER) rely on distinct types of information: ASR primarily depends on linguistic content, whereas SER requires the integration of both linguistic and paralinguistic cues. Existing multitask SLMs typically adopt naive parameter sharing or prompt-based conditioning without explicitly modeling the differences in information composition required by each task. Such designs risk task interference and performance degradation, especially under limited data conditions. To address these limitations, we propose HarmoniFuse, a component-selective and prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse is designed to harmonize heterogeneous task demands by selecting and fusing task-relevant components of speech representations. Specifically, it integrates a gated speech encoder to extract task-specific acoustic features and a prompt-adaptive dynamic fusion module to aggregate transformer layers based on task characteristics. In addition, a batch-interleaved training strategy enables leveraging separate ASR and SER datasets without requiring joint annotation. Experimental results demonstrate that HarmoniFuse improves both ASR and SER performance, offering a scalable and robust solution for multitask speech understanding under realistic data constraints.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ ì—¬ëŸ¬ ìŒì„± ì‘ì—…ì„ ì§€ì›í•˜ëŠ” í†µí•© ìŒì„± ì–¸ì–´ ëª¨ë¸(SLM)ì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìë™ ìŒì„± ì¸ì‹(ASR)ê³¼ ìŒì„± ê°ì • ì¸ì‹(SER)ê³¼ ê°™ì€ ì‘ì—…ì€ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ì— ì˜ì¡´í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë©€í‹°íƒœìŠ¤í¬ SLMì€ ì •ë³´ êµ¬ì„±ì˜ ì°¨ì´ë¥¼ ëª…í™•íˆ ëª¨ë¸ë§í•˜ì§€ ì•Šì•„ ì‘ì—… ê°„ ê°„ì„­ê³¼ ì„±ëŠ¥ ì €í•˜ì˜ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” HarmoniFuseë¼ëŠ” ë©€í‹°íƒœìŠ¤í¬ ìŒì„± ì–¸ì–´ ëª¨ë¸ë§ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. HarmoniFuseëŠ” ì‘ì—… ê´€ë ¨ ìŒì„± í‘œí˜„ ìš”ì†Œë¥¼ ì„ íƒí•˜ê³  ìœµí•©í•˜ì—¬ ì´ì§ˆì ì¸ ì‘ì—… ìš”êµ¬ë¥¼ ì¡°í™”ë¡­ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, HarmoniFuseëŠ” ASRê³¼ SER ì„±ëŠ¥ì„ ëª¨ë‘ í–¥ìƒì‹œì¼œ í˜„ì‹¤ì ì¸ ë°ì´í„° ì œì•½ í•˜ì—ì„œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ê²¬ê³ í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ ë‹¤ì¤‘ ìŒì„± ì‘ì—…ì„ ì§€ì›í•˜ëŠ” í†µí•© ìŒì„± ì–¸ì–´ ëª¨ë¸(SLM)ì´ ê°œë°œë˜ì—ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ë©€í‹°íƒœìŠ¤í¬ SLMì€ ì •ë³´ êµ¬ì„±ì˜ ì°¨ì´ë¥¼ ëª…í™•íˆ ëª¨ë¸ë§í•˜ì§€ ì•Šì•„ ì‘ì—… ê°„ ê°„ì„­ê³¼ ì„±ëŠ¥ ì €í•˜ì˜ ìœ„í—˜ì´ ìˆë‹¤.
- 3. HarmoniFuseëŠ” ì‘ì—… ê´€ë ¨ ìŒì„± í‘œí˜„ ìš”ì†Œë¥¼ ì„ íƒí•˜ê³  ìœµí•©í•˜ì—¬ ì´ì§ˆì ì¸ ì‘ì—… ìš”êµ¬ë¥¼ ì¡°í™”ë¡­ê²Œ ì²˜ë¦¬í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì´ë‹¤.
- 4. HarmoniFuseëŠ” ì‘ì—…ë³„ ìŒí–¥ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ê²Œì´íŠ¸ ìŒì„± ì¸ì½”ë”ì™€ ì‘ì—… íŠ¹ì„±ì— ê¸°ë°˜í•œ í”„ë¡¬í”„íŠ¸ ì ì‘ ë™ì  ìœµí•© ëª¨ë“ˆì„ í†µí•©í•œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, HarmoniFuseëŠ” ASRê³¼ SER ì„±ëŠ¥ì„ ëª¨ë‘ ê°œì„ í•˜ì—¬ í˜„ì‹¤ì ì¸ ë°ì´í„° ì œì•½ í•˜ì—ì„œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ê°•ë ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•œë‹¤.


---

*Generated on 2025-09-24 15:46:59*