<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:46:59.016479",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "HarmoniFuse Framework",
    "Multi-Task Speech Language Modeling",
    "Automatic Speech Recognition",
    "Speech Emotion Recognition",
    "Gated Speech Encoder"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "HarmoniFuse Framework": 0.78,
    "Multi-Task Speech Language Modeling": 0.82,
    "Automatic Speech Recognition": 0.75,
    "Speech Emotion Recognition": 0.8,
    "Gated Speech Encoder": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "HarmoniFuse",
        "canonical": "HarmoniFuse Framework",
        "aliases": [
          "HarmoniFuse"
        ],
        "category": "unique_technical",
        "rationale": "HarmoniFuse represents a novel framework specifically designed for multi-task speech language modeling, offering unique insights into task-specific component selection and fusion.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "multi-task speech language modeling",
        "canonical": "Multi-Task Speech Language Modeling",
        "aliases": [
          "multi-task SLM"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper and connects with broader themes in multi-task learning and speech processing.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.82
      },
      {
        "surface": "automatic speech recognition",
        "canonical": "Automatic Speech Recognition",
        "aliases": [
          "ASR"
        ],
        "category": "broad_technical",
        "rationale": "ASR is a fundamental task in speech processing, providing a strong link to related research in speech and language technologies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "speech emotion recognition",
        "canonical": "Speech Emotion Recognition",
        "aliases": [
          "SER"
        ],
        "category": "specific_connectable",
        "rationale": "SER is a specialized task that complements ASR, enhancing the understanding of emotional context in speech.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "gated speech encoder",
        "canonical": "Gated Speech Encoder",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This component is a unique aspect of the proposed framework, crucial for extracting task-specific acoustic features.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "HarmoniFuse",
      "resolved_canonical": "HarmoniFuse Framework",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "multi-task speech language modeling",
      "resolved_canonical": "Multi-Task Speech Language Modeling",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "automatic speech recognition",
      "resolved_canonical": "Automatic Speech Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "speech emotion recognition",
      "resolved_canonical": "Speech Emotion Recognition",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "gated speech encoder",
      "resolved_canonical": "Gated Speech Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18570.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18570](https://arxiv.org/abs/2509.18570)

## 🔗 유사한 논문
- [[2025-09-23/Whisper-UT_ A Unified Translation Framework for Speech and Text_20250923|Whisper-UT: A Unified Translation Framework for Speech and Text]] (83.4% similar)
- [[2025-09-22/AS-ASR_ A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition_20250922|AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition]] (82.5% similar)
- [[2025-09-24/Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs_20250924|Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs]] (82.4% similar)
- [[2025-09-19/SpeechOp_ Inference-Time Task Composition for Generative Speech Processing_20250919|SpeechOp: Inference-Time Task Composition for Generative Speech Processing]] (82.4% similar)
- [[2025-09-24/DeepResonance_ Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning_20250924|DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning]] (82.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Automatic Speech Recognition|Automatic Speech Recognition]]
**🔗 Specific Connectable**: [[keywords/Multi-Task Speech Language Modeling|Multi-Task Speech Language Modeling]], [[keywords/Speech Emotion Recognition|Speech Emotion Recognition]]
**⚡ Unique Technical**: [[keywords/HarmoniFuse Framework|HarmoniFuse Framework]], [[keywords/Gated Speech Encoder|Gated Speech Encoder]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18570v1 Announce Type: cross 
Abstract: Recent advances in large language models have facilitated the development of unified speech language models (SLMs) capable of supporting multiple speech tasks within a shared architecture. However, tasks such as automatic speech recognition (ASR) and speech emotion recognition (SER) rely on distinct types of information: ASR primarily depends on linguistic content, whereas SER requires the integration of both linguistic and paralinguistic cues. Existing multitask SLMs typically adopt naive parameter sharing or prompt-based conditioning without explicitly modeling the differences in information composition required by each task. Such designs risk task interference and performance degradation, especially under limited data conditions. To address these limitations, we propose HarmoniFuse, a component-selective and prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse is designed to harmonize heterogeneous task demands by selecting and fusing task-relevant components of speech representations. Specifically, it integrates a gated speech encoder to extract task-specific acoustic features and a prompt-adaptive dynamic fusion module to aggregate transformer layers based on task characteristics. In addition, a batch-interleaved training strategy enables leveraging separate ASR and SER datasets without requiring joint annotation. Experimental results demonstrate that HarmoniFuse improves both ASR and SER performance, offering a scalable and robust solution for multitask speech understanding under realistic data constraints.

## 📝 요약

최근 대형 언어 모델의 발전으로 여러 음성 작업을 지원하는 통합 음성 언어 모델(SLM)이 개발되었습니다. 그러나 자동 음성 인식(ASR)과 음성 감정 인식(SER)과 같은 작업은 서로 다른 정보에 의존합니다. 기존의 멀티태스크 SLM은 정보 구성의 차이를 명확히 모델링하지 않아 작업 간 간섭과 성능 저하의 위험이 있습니다. 이를 해결하기 위해, 우리는 HarmoniFuse라는 멀티태스크 음성 언어 모델링 프레임워크를 제안합니다. HarmoniFuse는 작업 관련 음성 표현 요소를 선택하고 융합하여 이질적인 작업 요구를 조화롭게 처리합니다. 실험 결과, HarmoniFuse는 ASR과 SER 성능을 모두 향상시켜 현실적인 데이터 제약 하에서 확장 가능하고 견고한 솔루션을 제공합니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델의 발전으로 다중 음성 작업을 지원하는 통합 음성 언어 모델(SLM)이 개발되었다.
- 2. 기존의 멀티태스크 SLM은 정보 구성의 차이를 명확히 모델링하지 않아 작업 간 간섭과 성능 저하의 위험이 있다.
- 3. HarmoniFuse는 작업 관련 음성 표현 요소를 선택하고 융합하여 이질적인 작업 요구를 조화롭게 처리하는 프레임워크이다.
- 4. HarmoniFuse는 작업별 음향 특징을 추출하는 게이트 음성 인코더와 작업 특성에 기반한 프롬프트 적응 동적 융합 모듈을 통합한다.
- 5. 실험 결과, HarmoniFuse는 ASR과 SER 성능을 모두 개선하여 현실적인 데이터 제약 하에서 확장 가능하고 강력한 솔루션을 제공한다.


---

*Generated on 2025-09-24 15:46:59*