<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:27:43.244087",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Large Language Models",
    "Spatiotemporal Token Reduction Model",
    "Temporal Encoder",
    "Mamba State Space Model",
    "Video Reasoning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Large Language Models": 0.82,
    "Spatiotemporal Token Reduction Model": 0.79,
    "Temporal Encoder": 0.81,
    "Mamba State Space Model": 0.77,
    "Video Reasoning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Video-LLMs",
        "canonical": "Multimodal Large Language Models",
        "aliases": [
          "Video Large Language Models",
          "Video-LLM"
        ],
        "category": "evolved_concepts",
        "rationale": "This term represents a specialized evolution of LLMs for video data, linking to multimodal learning.",
        "novelty_score": 0.75,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "STORM",
        "canonical": "Spatiotemporal Token Reduction Model",
        "aliases": [
          "STORM Architecture"
        ],
        "category": "unique_technical",
        "rationale": "STORM is a novel architecture specifically designed for efficient long video understanding, offering unique insights.",
        "novelty_score": 0.88,
        "connectivity_score": 0.72,
        "specificity_score": 0.91,
        "link_intent_score": 0.79
      },
      {
        "surface": "temporal encoder",
        "canonical": "Temporal Encoder",
        "aliases": [
          "time-based encoder"
        ],
        "category": "specific_connectable",
        "rationale": "Temporal encoders are crucial for capturing dynamic patterns in video data, enhancing connectivity to temporal modeling.",
        "novelty_score": 0.65,
        "connectivity_score": 0.83,
        "specificity_score": 0.76,
        "link_intent_score": 0.81
      },
      {
        "surface": "Mamba State Space Model",
        "canonical": "Mamba State Space Model",
        "aliases": [
          "Mamba SSM"
        ],
        "category": "unique_technical",
        "rationale": "This model is a key component for integrating temporal information, offering a unique approach to video token processing.",
        "novelty_score": 0.92,
        "connectivity_score": 0.68,
        "specificity_score": 0.89,
        "link_intent_score": 0.77
      },
      {
        "surface": "video reasoning",
        "canonical": "Video Reasoning",
        "aliases": [
          "video analysis",
          "video comprehension"
        ],
        "category": "specific_connectable",
        "rationale": "Video reasoning is essential for understanding and interpreting video content, linking to broader video analysis techniques.",
        "novelty_score": 0.58,
        "connectivity_score": 0.79,
        "specificity_score": 0.73,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "improvement"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Video-LLMs",
      "resolved_canonical": "Multimodal Large Language Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "STORM",
      "resolved_canonical": "Spatiotemporal Token Reduction Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.88,
        "connectivity": 0.72,
        "specificity": 0.91,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "temporal encoder",
      "resolved_canonical": "Temporal Encoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.83,
        "specificity": 0.76,
        "link_intent": 0.81
      }
    },
    {
      "candidate_surface": "Mamba State Space Model",
      "resolved_canonical": "Mamba State Space Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.68,
        "specificity": 0.89,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "video reasoning",
      "resolved_canonical": "Video Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.79,
        "specificity": 0.73,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# STORM: Token-Efficient Long Video Understanding for Multimodal LLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2503.04130.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2503.04130](https://arxiv.org/abs/2503.04130)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding_20250919|Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding]] (84.9% similar)
- [[2025-09-24/LongLLaVA_ Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture_20250924|LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture]] (84.2% similar)
- [[2025-09-24/Sparse Training Scheme for Multimodal LLM_20250924|Sparse Training Scheme for Multimodal LLM]] (84.2% similar)
- [[2025-09-23/TempSamp-R1_ Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs_20250923|TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs]] (83.9% similar)
- [[2025-09-23/VQToken_ Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models_20250923|VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models]] (83.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Temporal Encoder|Temporal Encoder]], [[keywords/Video Reasoning|Video Reasoning]]
**âš¡ Unique Technical**: [[keywords/Spatiotemporal Token Reduction Model|Spatiotemporal Token Reduction Model]], [[keywords/Mamba State Space Model|Mamba State Space Model]]
**ğŸš€ Evolved Concepts**: [[keywords/Multimodal Large Language Models|Multimodal Large Language Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2503.04130v4 Announce Type: replace 
Abstract: Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to $8\times$ and the decoding latency by 2.4-2.9$\times$ for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm

## ğŸ“ ìš”ì•½

ìµœê·¼ ë¹„ë””ì˜¤ ê¸°ë°˜ ë‹¤ì¤‘ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Video-LLMs)ì€ ì´ë¯¸ì§€ í”„ë ˆì„ ì‹œí€€ìŠ¤ë¡œ ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë¹„ë””ì˜¤ ì´í•´ë¥¼ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ë°©ë²•ë“¤ì€ í”„ë ˆì„ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê°„ì  íŒ¨í„´ì„ ì˜ í¬ì°©í•˜ì§€ ëª»í•˜ê³  ê¸´ ë¹„ë””ì˜¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ STORMì´ë¼ëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. STORMì€ ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ LLM ì‚¬ì´ì— ì „ìš© ì‹œê°„ ì¸ì½”ë”ë¥¼ ë„ì…í•˜ì—¬ Mamba State Space Modelì„ í™œìš©í•´ ì´ë¯¸ì§€ í† í°ì— ì‹œê°„ ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìƒì„±ëœ í’ë¶€í•œ í‘œí˜„ì€ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ ì „ë°˜ì˜ í”„ë ˆì„ ê°„ ë™ì  ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©°, í…ŒìŠ¤íŠ¸ ì‹œ ìƒ˜í”Œë§ ë° í•™ìŠµ ê¸°ë°˜ ì‹œê°„ ë° ê³µê°„ í’€ë§ì„ í¬í•¨í•œ íš¨ìœ¨ì ì¸ í† í° ê°ì†Œ ì „ëµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ  í†µí•©ìœ¼ë¡œ STORMì€ í•™ìŠµ ë° ì¶”ë¡  ì§€ì—°ì„ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ ê¸´ ì‹œê°„ì  ë§¥ë½ì—ì„œ íš¨ìœ¨ì ì´ê³  ê²¬ê³ í•œ ë¹„ë””ì˜¤ ì´í•´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê¸´ ë¹„ë””ì˜¤ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ STORMì€ ìµœëŒ€ 8ë°°ì˜ ê³„ì‚° ë¹„ìš© ì ˆê°ê³¼ 2.4-2.9ë°°ì˜ ë””ì½”ë”© ì§€ì—° ê°ì†Œë¥¼ ë‹¬ì„±í•˜ë©°, ì„±ëŠ¥ì„ 5% ì´ìƒ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. STORMì€ ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ LLM ì‚¬ì´ì— ì „ìš© ì‹œê°„ ì¸ì½”ë”ë¥¼ ë„ì…í•˜ì—¬ ë™ì  íŒ¨í„´ì„ í¬ì°©í•˜ê³  ê¸´ ë¹„ë””ì˜¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- 2. Mamba State Space Modelì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ í† í°ì— ì‹œê°„ ì •ë³´ë¥¼ í†µí•©, í”„ë ˆì„ ê°„ì˜ ì—­í•™ì„ ë³´ì¡´í•˜ëŠ” í’ë¶€í•œ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 3. STORMì€ í…ŒìŠ¤íŠ¸ ì‹œ ìƒ˜í”Œë§ ë° í•™ìŠµ ê¸°ë°˜ì˜ ì‹œê°„ ë° ê³µê°„ í’€ë§ì„ í¬í•¨í•œ í† í° ê°ì†Œ ì „ëµì„ í†µí•´ LLMì˜ ê³„ì‚° ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ ê¸´ ë¹„ë””ì˜¤ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ STORMì€ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, ê³„ì‚° ë¹„ìš©ì„ ìµœëŒ€ 8ë°°, ë””ì½”ë”© ì§€ì—°ì„ 2.4-2.9ë°° ì¤„ì…ë‹ˆë‹¤.
- 5. STORMì€ í›ˆë ¨ ë° ì¶”ë¡  ì§€ì—°ì„ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ í™•ì¥ëœ ì‹œê°„ì  ë§¥ë½ì—ì„œ íš¨ìœ¨ì ì´ê³  ê²¬ê³ í•œ ë¹„ë””ì˜¤ ì´í•´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:27:43*