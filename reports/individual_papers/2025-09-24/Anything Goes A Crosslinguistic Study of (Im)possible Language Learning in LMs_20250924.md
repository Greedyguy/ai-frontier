<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:49:54.555549",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Typologically Unattested Languages",
    "Greenberg's Universal 20",
    "Word Order Manipulation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Typologically Unattested Languages": 0.7,
    "Greenberg's Universal 20": 0.65,
    "Word Order Manipulation": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "language models",
        "canonical": "Large Language Model",
        "aliases": [
          "LMs"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, linking to a broad technical category enhances connectivity across language-related research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "typologically unattested languages",
        "canonical": "Typologically Unattested Languages",
        "aliases": [
          "impossible languages"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a unique concept that challenges existing language models, fostering discussions on linguistic diversity.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Greenberg's Universal 20",
        "canonical": "Greenberg's Universal 20",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific linguistic theory that provides a basis for testing language models, linking to linguistic theory discussions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.65
      },
      {
        "surface": "word order manipulation",
        "canonical": "Word Order Manipulation",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Highlights a specific experimental technique relevant to language processing studies.",
        "novelty_score": 0.55,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "language learning",
      "experiments",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "language models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "typologically unattested languages",
      "resolved_canonical": "Typologically Unattested Languages",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Greenberg's Universal 20",
      "resolved_canonical": "Greenberg's Universal 20",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.65
      }
    },
    {
      "candidate_surface": "word order manipulation",
      "resolved_canonical": "Word Order Manipulation",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2502.18795.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2502.18795](https://arxiv.org/abs/2502.18795)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Large Language Model probabilities cannot distinguish between possible and impossible language_20250919|Large Language Model probabilities cannot distinguish between possible and impossible language]] (87.7% similar)
- [[2025-09-22/How do Language Models Generate Slang_ A Systematic Comparison between Human and Machine-Generated Slang Usages_20250922|How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages]] (87.6% similar)
- [[2025-09-23/Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements_20250923|Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements]] (87.2% similar)
- [[2025-09-23/Learning to vary_ Teaching LMs to reproduce human linguistic variability in next-word prediction_20250923|Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction]] (86.8% similar)
- [[2025-09-23/Can Language Models Follow Multiple Turns of Entangled Instructions?_20250923|Can Language Models Follow Multiple Turns of Entangled Instructions?]] (85.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Word Order Manipulation|Word Order Manipulation]]
**âš¡ Unique Technical**: [[keywords/Typologically Unattested Languages|Typologically Unattested Languages]], [[keywords/Greenberg's Universal 20|Greenberg's Universal 20]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.18795v3 Announce Type: replace 
Abstract: Do language models (LMs) offer insights into human language learning? A common argument against this idea is that because their architecture and training paradigm are so vastly different from humans, LMs can learn arbitrary inputs as easily as natural languages. We test this claim by training LMs to model impossible and typologically unattested languages. Unlike previous work, which has focused exclusively on English, we conduct experiments on 12 languages from 4 language families with two newly constructed parallel corpora. Our results show that while GPT-2 small can largely distinguish attested languages from their impossible counterparts, it does not achieve perfect separation between all the attested languages and all the impossible ones. We further test whether GPT-2 small distinguishes typologically attested from unattested languages with different NP orders by manipulating word order based on Greenberg's Universal 20. We find that the model's perplexity scores do not distinguish attested vs. unattested word orders, while its performance on the generalization test does. These findings suggest that LMs exhibit some human-like inductive biases, though these biases are weaker than those found in human learners.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸(LM)ì´ ì¸ê°„ì˜ ì–¸ì–´ í•™ìŠµì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. íŠ¹íˆ, LMì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ìœ í˜•í•™ì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤. 12ê°œì˜ ì–¸ì–´ì™€ ë‘ ê°œì˜ ìƒˆë¡œìš´ ë³‘ë ¬ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼, GPT-2 smallì€ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¡´ì¬í•˜ëŠ” ì–¸ì–´ì™€ ë¶ˆê°€ëŠ¥í•œ ì–¸ì–´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, ì™„ë²½í•œ êµ¬ë¶„ì€ ì´ë£¨ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, Greenbergì˜ ë³´í¸ì  20ì„ ê¸°ë°˜ìœ¼ë¡œ ëª…ì‚¬êµ¬(NP) ìˆœì„œë¥¼ ì¡°ì‘í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼, ëª¨ë¸ì˜ í˜¼ë€ë„(perplexity) ì ìˆ˜ëŠ” ì¡´ì¬í•˜ëŠ” ìˆœì„œì™€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìˆœì„œë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í–ˆì§€ë§Œ, ì¼ë°˜í™” í…ŒìŠ¤íŠ¸ì—ì„œëŠ” êµ¬ë¶„í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” LMì´ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ê·€ë‚©ì  í¸í–¥ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ê·¸ ê°•ë„ëŠ” ì¸ê°„ í•™ìŠµìë³´ë‹¤ ì•½í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸(LM)ì€ ì¸ê°„ ì–¸ì–´ í•™ìŠµì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ë…¼ë€ì´ ìˆë‹¤.
- 2. ì—°êµ¬ëŠ” ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ìœ í˜•í•™ì ìœ¼ë¡œ ê´€ì°°ë˜ì§€ ì•Šì€ ì–¸ì–´ë¥¼ ëª¨ë¸ë§í•˜ëŠ” LMsì˜ ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í–ˆë‹¤.
- 3. GPT-2 smallì€ ê°€ëŠ¥ ì–¸ì–´ì™€ ë¶ˆê°€ëŠ¥ ì–¸ì–´ë¥¼ ì–´ëŠ ì •ë„ êµ¬ë³„í•  ìˆ˜ ìˆì§€ë§Œ, ì™„ë²½í•œ ë¶„ë¦¬ëŠ” ì´ë£¨ì§€ ëª»í–ˆë‹¤.
- 4. Greenbergì˜ Universal 20ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ ìˆœì„œë¥¼ ì¡°ì‘í•˜ì—¬ ìœ í˜•í•™ì ìœ¼ë¡œ ê´€ì°°ëœ ì–¸ì–´ì™€ ê´€ì°°ë˜ì§€ ì•Šì€ ì–¸ì–´ë¥¼ êµ¬ë³„í•  ìˆ˜ ìˆëŠ”ì§€ ì¶”ê°€ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.
- 5. ì—°êµ¬ ê²°ê³¼, LMsëŠ” ì¸ê°„ í•™ìŠµìë³´ë‹¤ ì•½í•˜ì§€ë§Œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ê·€ë‚©ì  í¸í–¥ì„ ì¼ë¶€ ë³´ì¸ë‹¤.


---

*Generated on 2025-09-24 15:49:54*