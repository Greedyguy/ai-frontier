<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:18:42.504095",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Compositionality",
    "Long-Caption Understanding",
    "Object-Attribute Binding",
    "Data Quality"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Compositionality": 0.78,
    "Long-Caption Understanding": 0.8,
    "Object-Attribute Binding": 0.72,
    "Data Quality": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's exploration of compositionality and long-caption understanding, making them a key concept for linking.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "Compositionality",
        "canonical": "Compositionality",
        "aliases": [
          "Compositional Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Compositionality is a unique technical focus of the paper, crucial for understanding the interaction with long-caption understanding.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Long-Caption Understanding",
        "canonical": "Long-Caption Understanding",
        "aliases": [
          "Dense Caption Understanding"
        ],
        "category": "unique_technical",
        "rationale": "Long-Caption Understanding is a specific challenge addressed in the paper, relevant for linking to related research in vision-language tasks.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.82,
        "link_intent_score": 0.8
      },
      {
        "surface": "Object-Attribute Binding",
        "canonical": "Object-Attribute Binding",
        "aliases": [
          "Attribute Binding"
        ],
        "category": "unique_technical",
        "rationale": "Object-Attribute Binding is a specific aspect of compositionality discussed in the paper, important for linking to related cognitive and computational models.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "Data Quality",
        "canonical": "Data Quality",
        "aliases": [
          "Dataset Quality"
        ],
        "category": "broad_technical",
        "rationale": "Data Quality is highlighted as a critical factor affecting model performance, relevant for linking to discussions on dataset curation and model training.",
        "novelty_score": 0.3,
        "connectivity_score": 0.82,
        "specificity_score": 0.6,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Compositionality",
      "resolved_canonical": "Compositionality",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Long-Caption Understanding",
      "resolved_canonical": "Long-Caption Understanding",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.82,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Object-Attribute Binding",
      "resolved_canonical": "Object-Attribute Binding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Data Quality",
      "resolved_canonical": "Data Quality",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.82,
        "specificity": 0.6,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19207.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19207](https://arxiv.org/abs/2509.19207)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Reading Images Like Texts_ Sequential Image Understanding in Vision-Language Models_20250924|Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models]] (86.5% similar)
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (83.9% similar)
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (83.1% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.6% similar)
- [[2025-09-24/Vision-Free Retrieval_ Rethinking Multimodal Search with Textual Scene Descriptions_20250924|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Data Quality|Data Quality]]
**âš¡ Unique Technical**: [[keywords/Compositionality|Compositionality]], [[keywords/Long-Caption Understanding|Long-Caption Understanding]], [[keywords/Object-Attribute Binding|Object-Attribute Binding]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19207v1 Announce Type: new 
Abstract: Contrastive vision-language models (VLMs) have made significant progress in binding visual and textual information, but understanding long, dense captions remains an open challenge. We hypothesize that compositionality, the capacity to reason about object-attribute bindings and inter-object relationships, is key to understanding longer captions. In this paper, we investigate the interaction between compositionality and long-caption understanding, asking whether training for one property enhances the other. We train and evaluate a range of models that target each of these capabilities. Our results reveal a bidirectional relationship: compositional training improves performance on long-caption retrieval, and training on long captions promotes compositionality. However, these gains are sensitive to data quality and model design. We find that training on poorly structured captions, or with limited parameter updates, fails to support generalization. Likewise, strategies that aim at retaining general alignment, such as freezing positional embeddings, do not improve compositional understanding. Overall, we find that compositional understanding and long-caption understanding are intertwined capabilities that can be jointly learned through training on dense, grounded descriptions. Despite these challenges, we show that models trained on high-quality, long-caption data can achieve strong performance in both tasks, offering practical guidance for improving VLM generalization.

## ğŸ“ ìš”ì•½

ëŒ€ì¡°ì  ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê²°í•©í•˜ëŠ” ë° ì§„ì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ê¸´ ìº¡ì…˜ì„ ì´í•´í•˜ëŠ” ë°ëŠ” ì—¬ì „íˆ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ê°ì²´ ì†ì„± ê²°í•© ë° ê°ì²´ ê°„ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì¸ êµ¬ì„±ì„±ì´ ê¸´ ìº¡ì…˜ ì´í•´ì— ì¤‘ìš”í•˜ë‹¤ê³  ê°€ì •í•˜ê³ , ë‘ ê°€ì§€ ëŠ¥ë ¥ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, êµ¬ì„±ì„± í›ˆë ¨ì´ ê¸´ ìº¡ì…˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ê¸´ ìº¡ì…˜ í›ˆë ¨ì´ êµ¬ì„±ì„±ì„ ì´‰ì§„í•˜ëŠ” ìƒí˜¸ ê´€ê³„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì„±ê³¼ëŠ” ë°ì´í„° í’ˆì§ˆê³¼ ëª¨ë¸ ì„¤ê³„ì— ë¯¼ê°í•˜ê²Œ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. êµ¬ì¡°ê°€ ì˜ëª»ëœ ìº¡ì…˜ì´ë‚˜ ì œí•œëœ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¡œ í›ˆë ¨í•  ê²½ìš° ì¼ë°˜í™”ì— ì‹¤íŒ¨í•˜ë©°, ìœ„ì¹˜ ì„ë² ë”©ì„ ê³ ì •í•˜ëŠ” ì „ëµë„ êµ¬ì„±ì„± ì´í•´ë¥¼ ê°œì„ í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê³ í’ˆì§ˆì˜ ê¸´ ìº¡ì…˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í›ˆë ¨ì€ ë‘ ê°€ì§€ ê³¼ì œì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, VLM ì¼ë°˜í™” ê°œì„ ì„ ìœ„í•œ ì‹¤ì§ˆì ì¸ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€ì¡°ì  ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs)ì€ ì‹œê°ì  ë° í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê²°í•©í•˜ëŠ” ë° ì§„ì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ê¸´ ìº¡ì…˜ì„ ì´í•´í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ë„ì „ ê³¼ì œì…ë‹ˆë‹¤.
- 2. ë³¸ ì—°êµ¬ëŠ” ê°ì²´-ì†ì„± ê²°í•© ë° ê°ì²´ ê°„ ê´€ê³„ì— ëŒ€í•œ ì¶”ë¡  ëŠ¥ë ¥ì¸ êµ¬ì„±ì„±ì´ ê¸´ ìº¡ì…˜ ì´í•´ì˜ í•µì‹¬ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
- 3. êµ¬ì„±ì„± í›ˆë ¨ì€ ê¸´ ìº¡ì…˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ê¸´ ìº¡ì…˜ í›ˆë ¨ì€ êµ¬ì„±ì„±ì„ ì´‰ì§„í•˜ëŠ” ì–‘ë°©í–¥ ê´€ê³„ê°€ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
- 4. ë°ì´í„° í’ˆì§ˆê³¼ ëª¨ë¸ ì„¤ê³„ì— ë”°ë¼ ì´ëŸ¬í•œ ì„±ê³¼ê°€ ë¯¼ê°í•˜ê²Œ ë³€í•˜ë©°, êµ¬ì¡°ê°€ ì˜ëª»ëœ ìº¡ì…˜ì´ë‚˜ ì œí•œëœ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¡œëŠ” ì¼ë°˜í™”ê°€ ì–´ë µìŠµë‹ˆë‹¤.
- 5. ê³ í’ˆì§ˆì˜ ê¸´ ìº¡ì…˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì€ ë‘ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” VLM ì¼ë°˜í™”ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ì‹¤ì§ˆì ì¸ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:18:42*