<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:10:04.060983",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Learning",
    "Vision-Language Model",
    "Christian Iconography",
    "Zero-Shot Learning",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Learning": 0.85,
    "Vision-Language Model": 0.87,
    "Christian Iconography": 0.78,
    "Zero-Shot Learning": 0.8,
    "Few-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal Large Language Models",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal LLMs"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is a key concept in linking language and vision tasks, relevant to the study's focus on multimodal models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLMs"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models represent an evolved concept crucial for understanding the integration of visual and textual data.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.87
      },
      {
        "surface": "Christian Iconography",
        "canonical": "Christian Iconography",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Christian Iconography is a unique technical domain that the study specifically addresses, providing a niche context for linking.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Zero-shot",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is a specific connectable concept relevant to the study's evaluation of model performance without prior examples.",
        "novelty_score": 0.45,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      },
      {
        "surface": "Few-shot",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is crucial for understanding the study's approach to enhancing model performance with minimal examples.",
        "novelty_score": 0.48,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "performance",
      "method",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal Large Language Models",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.87
      }
    },
    {
      "candidate_surface": "Christian Iconography",
      "resolved_canonical": "Christian Iconography",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Zero-shot",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Few-shot",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18839.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.18839](https://arxiv.org/abs/2509.18839)

## 🔗 유사한 논문
- [[2025-09-22/LLMs Can Compensate for Deficiencies in Visual Representations_20250922|LLMs Can Compensate for Deficiencies in Visual Representations]] (83.8% similar)
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (83.7% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (83.6% similar)
- [[2025-09-24/Large Language Models Do Multi-Label Classification Differently_20250924|Large Language Models Do Multi-Label Classification Differently]] (83.3% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (83.2% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Christian Iconography|Christian Iconography]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18839v1 Announce Type: new 
Abstract: This study evaluates the capabilities of Multimodal Large Language Models (LLMs) and Vision Language Models (VLMs) in the task of single-label classification of Christian Iconography. The goal was to assess whether general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5, can interpret the Iconography, typically addressed by supervised classifiers, and evaluate their performance. Two research questions guided the analysis: (RQ1) How do multimodal LLMs perform on image classification of Christian saints? And (RQ2), how does performance vary when enriching input with contextual information or few-shot exemplars? We conducted a benchmarking study using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include the top 10 most frequent classes. Models were tested under three conditions: (1) classification using class labels, (2) classification with Iconclass descriptions, and (3) few-shot learning with five exemplars. Results were compared against ResNet50 baselines fine-tuned on the same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset, where Siglip reached the highest accuracy score, suggesting model sensitivity to image size and metadata alignment. Enriching prompts with class descriptions generally improved zero-shot performance, while few-shot learning produced lower results, with only occasional and minimal increments in accuracy. We conclude that general-purpose multimodal LLMs are capable of classification in visually complex cultural heritage domains. These results support the application of LLMs as metadata curation tools in digital humanities workflows, suggesting future research on prompt optimization and the expansion of the study to other classification strategies and models.

## 📝 요약

이 연구는 기독교 도상학의 단일 레이블 분류에서 다중 모달 대형 언어 모델(LLMs)과 비전 언어 모델(VLMs)의 능력을 평가했습니다. CLIP, SigLIP, GPT-4o, Gemini 2.5와 같은 일반 목적의 VLMs와 LLMs가 도상학을 해석할 수 있는지를 조사하고 그 성능을 평가했습니다. 연구는 두 가지 질문을 중심으로 진행되었습니다: (1) 다중 모달 LLMs가 기독교 성인의 이미지 분류에서 어떻게 수행되는가? (2) 맥락 정보나 예시를 추가하면 성능이 어떻게 변하는가? ArtDL, ICONCLASS, Wikidata 데이터셋을 사용하여 세 가지 조건에서 모델을 테스트했습니다. 결과적으로 Gemini-2.5 Pro와 GPT-4o가 ResNet50 기준을 초과했으며, SigLIP은 Wikidata에서 가장 높은 정확도를 기록했습니다. 클래스 설명을 추가하면 성능이 향상되었으나, 예시를 추가한 학습에서는 성능 증가가 미미했습니다. 연구는 일반 목적의 다중 모달 LLMs가 복잡한 문화유산 분야에서 분류가 가능함을 보여주며, 디지털 인문학에서 메타데이터 관리 도구로서의 가능성을 제시합니다.

## 🎯 주요 포인트

- 1. 이 연구는 기독교 도상학의 단일 레이블 분류 작업에서 멀티모달 대형 언어 모델(LLMs)과 비전 언어 모델(VLMs)의 성능을 평가합니다.
- 2. Gemini-2.5 Pro와 GPT-4o는 ResNet50 기반 모델보다 우수한 성능을 보였습니다.
- 3. SigLIP 모델은 Wikidata 데이터셋에서 가장 높은 정확도를 기록했으며, 이는 이미지 크기와 메타데이터 정렬에 대한 모델의 민감성을 시사합니다.
- 4. 클래스 설명을 포함한 프롬프트는 제로샷 성능을 개선했지만, 몇 샷 학습은 정확도 증가가 미미했습니다.
- 5. 일반 목적의 멀티모달 LLMs는 시각적으로 복잡한 문화유산 분야에서 분류가 가능하며, 디지털 인문학 워크플로우에서 메타데이터 큐레이션 도구로 활용될 수 있습니다.


---

*Generated on 2025-09-24 16:10:04*