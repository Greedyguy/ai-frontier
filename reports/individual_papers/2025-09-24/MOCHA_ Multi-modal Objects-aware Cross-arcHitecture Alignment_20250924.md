<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:43:03.949686",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "MOCHA",
    "Vision-Language Model",
    "Object Detection",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "MOCHA": 0.78,
    "Vision-Language Model": 0.8,
    "Object Detection": 0.78,
    "Few-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "MOCHA",
        "canonical": "MOCHA",
        "aliases": [
          "Multi-modal Objects-aware Cross-arcHitecture Alignment"
        ],
        "category": "unique_technical",
        "rationale": "MOCHA represents a novel approach in the paper, focusing on cross-architecture alignment, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision-Language Teacher",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Teacher"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's methodology, providing a basis for the knowledge distillation process.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Object Detector Student",
        "canonical": "Object Detection",
        "aliases": [
          "Object Detector Student"
        ],
        "category": "specific_connectable",
        "rationale": "Object Detection is a key component in the paper, linking it to broader computer vision tasks.",
        "novelty_score": 0.45,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Few-Shot Regimes",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Regimes"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is crucial for the paper's evaluation, highlighting its relevance in personalized detection benchmarks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "knowledge distillation",
      "translation module",
      "dual-objective loss"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "MOCHA",
      "resolved_canonical": "MOCHA",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision-Language Teacher",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Object Detector Student",
      "resolved_canonical": "Object Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Few-Shot Regimes",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.14001.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.14001](https://arxiv.org/abs/2509.14001)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-17/MOCHA_ Multi-modal Objects-aware Cross-arcHitecture Alignment_20250917|MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment]] (98.8% similar)
- [[2025-09-23/MO R-CNN_ Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image_20250923|MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image]] (84.8% similar)
- [[2025-09-19/Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities_20250919|Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities]] (84.7% similar)
- [[2025-09-24/LCMF_ Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA_20250924|LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA]] (84.0% similar)
- [[2025-09-23/MCP_ A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models_20250923|MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Object Detection|Object Detection]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/MOCHA|MOCHA]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.14001v2 Announce Type: replace-cross 
Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a knowledge distillation approach that transfers region-level multimodal semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight vision-only object detector student (e.g., YOLO). A translation module maps student features into a joint space, where the training of the student and translator is guided by a dual-objective loss that enforces both local alignment and global relational consistency. Unlike prior approaches focused on dense or global alignment, MOCHA operates at the object level, enabling efficient transfer of semantics without modifying the teacher or requiring textual input at inference. We validate our method across four personalized detection benchmarks under few-shot regimes. Results show consistent gains over baselines, with a +10.1 average score improvement. Despite its compact architecture, MOCHA reaches performance on par with larger multimodal models, proving its suitability for real-world deployment.

## ğŸ“ ìš”ì•½

MOCHAëŠ” ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì—ì„œ ê²½ëŸ‰ ë¹„ì „ ì „ìš© ê°ì²´ íƒì§€ê¸°ë¡œ ì§€ì—­ ìˆ˜ì¤€ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ ì˜ë¯¸ë¥¼ ì „ì´í•˜ëŠ” ì§€ì‹ ì¦ë¥˜ ë°©ë²•ì…ë‹ˆë‹¤. í•™ìƒ ëª¨ë¸ì˜ íŠ¹ì§•ì„ ê³µë™ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì§€ì—­ ì •ë ¬ê³¼ ì „ì—­ ê´€ê³„ ì¼ê´€ì„±ì„ ë™ì‹œì— ìœ ì§€í•˜ëŠ” ì´ì¤‘ ëª©í‘œ ì†ì‹¤ë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. MOCHAëŠ” ê°ì²´ ìˆ˜ì¤€ì—ì„œ ì˜ë¯¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì „ì´í•˜ë©°, êµì‚¬ ëª¨ë¸ ìˆ˜ì •ì´ë‚˜ ì¶”ë¡  ì‹œ í…ìŠ¤íŠ¸ ì…ë ¥ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë„¤ ê°€ì§€ ê°œì¸í™”ëœ íƒì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ MOCHAëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í‰ê·  10.1ì  í–¥ìƒëœ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ê²½ëŸ‰ êµ¬ì¡°ì„ì—ë„ ëŒ€í˜• ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì—¬ ì‹¤ì œ ì ìš©ì— ì í•©í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. MOCHAëŠ” ëŒ€í˜• ë¹„ì „-ì–¸ì–´ êµì‚¬ ëª¨ë¸ì˜ ì§€ì—­ ìˆ˜ì¤€ ë‹¤ì¤‘ ëª¨ë‹¬ ì˜ë¯¸ë¥¼ ê²½ëŸ‰ ë¹„ì „ ì „ìš© ê°ì²´ íƒì§€ í•™ìƒ ëª¨ë¸ë¡œ ì „ì´í•˜ëŠ” ì§€ì‹ ì¦ë¥˜ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.
- 2. í•™ìƒ íŠ¹ì§•ì„ ê³µë™ ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë²ˆì—­ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬, ì´ì¤‘ ëª©í‘œ ì†ì‹¤ì„ í†µí•´ ì§€ì—­ ì •ë ¬ê³¼ ì „ì—­ ê´€ê³„ ì¼ê´€ì„±ì„ ë™ì‹œì— ê°•í™”í•©ë‹ˆë‹¤.
- 3. MOCHAëŠ” ê°ì²´ ìˆ˜ì¤€ì—ì„œ ì‘ë™í•˜ì—¬ êµì‚¬ ëª¨ë¸ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ë¡  ì‹œ í…ìŠ¤íŠ¸ ì…ë ¥ì„ ìš”êµ¬í•˜ì§€ ì•Šê³ ë„ íš¨ìœ¨ì ì¸ ì˜ë¯¸ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 4. ë„¤ ê°€ì§€ ê°œì¸í™”ëœ íƒì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ MOCHAì˜ ì„±ëŠ¥ì„ ê²€ì¦í•œ ê²°ê³¼, ê¸°ì¤€ ëª¨ë¸ ëŒ€ë¹„ í‰ê·  10.1ì ì˜ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. MOCHAëŠ” ì»´íŒ©íŠ¸í•œ ì•„í‚¤í…ì²˜ì—ë„ ë¶ˆêµ¬í•˜ê³  ëŒ€í˜• ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì—¬ ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì í•©ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:43:03*