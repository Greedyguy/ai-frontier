<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:51:37.339352",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multi-label Classification",
    "Distribution Alignment",
    "Zero-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multi-label Classification": 0.78,
    "Distribution Alignment": 0.77,
    "Zero-Shot Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the study and connect with a wide range of topics in NLP and AI.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multi-label Classification",
        "canonical": "Multi-label Classification",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The paper focuses on how LLMs handle multi-label classification, a distinct task in machine learning.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Distribution Alignment",
        "canonical": "Distribution Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Distribution alignment is introduced as a novel task for improving LLM performance in subjective tasks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Zero-shot Methods",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-shot methods are highlighted as a way to improve alignment and predictive performance.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multi-label Classification",
      "resolved_canonical": "Multi-label Classification",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Distribution Alignment",
      "resolved_canonical": "Distribution Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Zero-shot Methods",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Large Language Models Do Multi-Label Classification Differently

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2505.17510.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2505.17510](https://arxiv.org/abs/2505.17510)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (88.9% similar)
- [[2025-09-23/Probabilistic Token Alignment for Large Language Model Fusion_20250923|Probabilistic Token Alignment for Large Language Model Fusion]] (87.4% similar)
- [[2025-09-19/Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (86.9% similar)
- [[2025-09-22/A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages_20250922|A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages]] (86.5% similar)
- [[2025-09-23/Measuring Scalar Constructs in Social Science with LLMs_20250923|Measuring Scalar Constructs in Social Science with LLMs]] (86.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Multi-label Classification|Multi-label Classification]], [[keywords/Distribution Alignment|Distribution Alignment]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.17510v2 Announce Type: replace 
Abstract: Multi-label classification is prevalent in real-world settings, but the behavior of Large Language Models (LLMs) in this setting is understudied. We investigate how autoregressive LLMs perform multi-label classification, focusing on subjective tasks, by analyzing the output distributions of the models at each label generation step. We find that the initial probability distribution for the first label often does not reflect the eventual final output, even in terms of relative order and find LLMs tend to suppress all but one label at each generation step. We further observe that as model scale increases, their token distributions exhibit lower entropy and higher single-label confidence, but the internal relative ranking of the labels improves. Finetuning methods such as supervised finetuning and reinforcement learning amplify this phenomenon. We introduce the task of distribution alignment for multi-label settings: aligning LLM-derived label distributions with empirical distributions estimated from annotator responses in subjective tasks. We propose both zero-shot and supervised methods which improve both alignment and predictive performance over existing approaches. We find one method -- taking the max probability over all label generation distributions instead of just using the initial probability distribution -- improves both distribution alignment and overall F1 classification without adding any additional computation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. íŠ¹íˆ ì£¼ê´€ì ì¸ ì‘ì—…ì— ì´ˆì ì„ ë§ì¶°, ê° ë¼ë²¨ ìƒì„± ë‹¨ê³„ì—ì„œ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ì²« ë²ˆì§¸ ë¼ë²¨ì˜ ì´ˆê¸° í™•ë¥  ë¶„í¬ê°€ ìµœì¢… ì¶œë ¥ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©°, LLMì´ ê° ë‹¨ê³„ì—ì„œ í•˜ë‚˜ì˜ ë¼ë²¨ë§Œì„ ê°•ì¡°í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ í† í° ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì•„ì§€ê³  ë‹¨ì¼ ë¼ë²¨ì— ëŒ€í•œ í™•ì‹ ì´ ë†’ì•„ì§€ì§€ë§Œ, ë¼ë²¨ ê°„ì˜ ìƒëŒ€ì  ìˆœìœ„ëŠ” ê°œì„ ë©ë‹ˆë‹¤. ë˜í•œ, ì§€ë„ í•™ìŠµ ë° ê°•í™” í•™ìŠµì„ í†µí•œ ë¯¸ì„¸ ì¡°ì •ì´ ì´ëŸ¬í•œ í˜„ìƒì„ ê°•í™”í•©ë‹ˆë‹¤. ì €ìë“¤ì€ ì£¼ê´€ì ì¸ ì‘ì—…ì—ì„œ LLMì´ ìƒì„±í•œ ë¼ë²¨ ë¶„í¬ë¥¼ ì£¼ì„ì ì‘ë‹µì˜ ê²½í—˜ì  ë¶„í¬ì™€ ì •ë ¬í•˜ëŠ” 'ë¶„í¬ ì •ë ¬' ê³¼ì œë¥¼ ì œì•ˆí•˜ê³ , ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ì œë¡œìƒ· ë° ì§€ë„ í•™ìŠµ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, ëª¨ë“  ë¼ë²¨ ìƒì„± ë¶„í¬ì—ì„œ ìµœëŒ€ í™•ë¥ ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ë¶„í¬ ì •ë ¬ê³¼ F1 ë¶„ë¥˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ì—ì„œ ì´ˆê¸° ë ˆì´ë¸” í™•ë¥  ë¶„í¬ê°€ ìµœì¢… ì¶œë ¥ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
- 2. ëª¨ë¸ì˜ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ í† í° ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì•„ì§€ê³  ë‹¨ì¼ ë ˆì´ë¸”ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ, ë ˆì´ë¸”ì˜ ìƒëŒ€ì  ìˆœìœ„ëŠ” ê°œì„ ë©ë‹ˆë‹¤.
- 3. ê°ë… í•™ìŠµ ë° ê°•í™” í•™ìŠµê³¼ ê°™ì€ íŒŒì¸íŠœë‹ ë°©ë²•ì€ ë‹¨ì¼ ë ˆì´ë¸” ì‹ ë¢°ë„ë¥¼ ë”ìš± ì¦í­ì‹œí‚µë‹ˆë‹¤.
- 4. ì£¼ê´€ì  ì‘ì—…ì—ì„œ LLM íŒŒìƒ ë ˆì´ë¸” ë¶„í¬ë¥¼ ì£¼ì„ì ì‘ë‹µì—ì„œ ì¶”ì •ëœ ê²½í—˜ì  ë¶„í¬ì™€ ì •ë ¬í•˜ëŠ” 'ë¶„í¬ ì •ë ¬' ì‘ì—…ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- 5. ì´ˆê¸° í™•ë¥  ë¶„í¬ ëŒ€ì‹  ëª¨ë“  ë ˆì´ë¸” ìƒì„± ë¶„í¬ì—ì„œ ìµœëŒ€ í™•ë¥ ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ë¶„í¬ ì •ë ¬ê³¼ F1 ë¶„ë¥˜ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:51:37*