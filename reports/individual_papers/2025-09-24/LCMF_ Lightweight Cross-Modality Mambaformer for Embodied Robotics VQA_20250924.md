<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:55:31.123036",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "Multimodal Learning",
    "Human-Robot Interaction",
    "Large Language Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.85,
    "Multimodal Learning": 0.89,
    "Human-Robot Interaction": 0.78,
    "Large Language Model": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Cross-Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-Attention is a specific form of the Attention Mechanism, crucial for linking multimodal data.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multimodal semantic learning",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal semantic"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal Learning is key for integrating data from different modalities, enhancing connectivity.",
        "novelty_score": 0.55,
        "connectivity_score": 0.92,
        "specificity_score": 0.82,
        "link_intent_score": 0.89
      },
      {
        "surface": "Human-Robot Interaction",
        "canonical": "Human-Robot Interaction",
        "aliases": [
          "HRI"
        ],
        "category": "unique_technical",
        "rationale": "Human-Robot Interaction is a unique technical domain that benefits from specific multimodal applications.",
        "novelty_score": 0.68,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Large Language Model Agents",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM Agents"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are a broad technical category relevant for understanding language-based tasks.",
        "novelty_score": 0.5,
        "connectivity_score": 0.87,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "Mambaformer",
      "LCMF",
      "EQA video tasks"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Cross-Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multimodal semantic learning",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.92,
        "specificity": 0.82,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "Human-Robot Interaction",
      "resolved_canonical": "Human-Robot Interaction",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Large Language Model Agents",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.87,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18576.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18576](https://arxiv.org/abs/2509.18576)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Surgical-MambaLLM_ Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery_20250923|Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery]] (85.7% similar)
- [[2025-09-24/VLA-LPAF_ Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation_20250924|VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation]] (84.9% similar)
- [[2025-09-23/Open Vision Reasoner_ Transferring Linguistic Cognitive Behavior for Visual Reasoning_20250923|Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning]] (84.6% similar)
- [[2025-09-23/LEO-MINI_ An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts_20250923|LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts]] (84.4% similar)
- [[2025-09-23/MCP_ A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models_20250923|MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models]] (84.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Human-Robot Interaction|Human-Robot Interaction]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18576v1 Announce Type: cross 
Abstract: Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ë¡œë´‡ì˜ ì§€ëŠ¥ì  ì˜ì‚¬ê²°ì •ì— ì¤‘ìš”í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ì˜ë¯¸ í•™ìŠµì˜ ê¸°ìˆ ì  ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LCMF ê²½ëŸ‰í™” ì—°ì† ì£¼ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. Cross-Attentionê³¼ ì„ íƒì  ë§¤ê°œë³€ìˆ˜ ê³µìœ  ìƒíƒœ ê³µê°„ ëª¨ë¸(SSM)ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ì´ì§ˆì  ë°ì´í„°ì˜ íš¨ìœ¨ì  ìœµí•©ê³¼ ì˜ë¯¸ ë³´ì™„ ì •ë ¬ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, LCMFëŠ” VQA ì‘ì—…ì—ì„œ 74.29%ì˜ ì •í™•ë„ë¡œ ê¸°ì¡´ ë‹¤ì¤‘ ëª¨ë‹¬ ê¸°ì¤€ì„ ëŠ¥ê°€í•˜ë©°, EQA ë¹„ë””ì˜¤ ì‘ì—…ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, í‰ê·  ëŒ€ë¹„ 4.35ë°°ì˜ FLOPs ê°ì†Œì™€ ì ì€ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©ìœ¼ë¡œ ìì› ì œí•œ í™˜ê²½ì—ì„œ íš¨ìœ¨ì ì¸ ì¸ê°„-ë¡œë´‡ ìƒí˜¸ì‘ìš©(HRI) ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ëŠ” ì´ì¢… ë°ì´í„°ì˜ íš¨ê³¼ì ì¸ ìœµí•©ê³¼ ìì› ì œì•½ í™˜ê²½ì—ì„œì˜ ê³„ì‚° íš¨ìœ¨ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê²½ëŸ‰ LCMF ê³„ë‹¨ì‹ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. LCMF í”„ë ˆì„ì›Œí¬ëŠ” Cross-Attentionê³¼ ì„ íƒì  íŒŒë¼ë¯¸í„° ê³µìœ  ìƒíƒœ ê³µê°„ ëª¨ë¸(SSMs)ì˜ ì¥ì ì„ í†µí•©í•˜ì—¬ ì´ì¢… ëª¨ë‹¬ë¦¬í‹°ì˜ íš¨ìœ¨ì ì¸ ìœµí•©ê³¼ ì˜ë¯¸ì  ë³´ì™„ ì •ë ¬ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 3. ì‹¤í—˜ ê²°ê³¼, LCMFëŠ” VQA ì‘ì—…ì—ì„œ 74.29%ì˜ ì •í™•ë„ë¡œ ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ë©°, EQA ë¹„ë””ì˜¤ ì‘ì—…ì—ì„œ LLM ì—ì´ì „íŠ¸ ë¶„í¬ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì¤‘ê°„ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 4. LCMFì˜ ê²½ëŸ‰ ì„¤ê³„ëŠ” ë¹„êµ ê°€ëŠ¥í•œ ê¸°ì¤€ì„ ì˜ í‰ê·  ëŒ€ë¹„ FLOPsë¥¼ 4.35ë°° ê°ì†Œì‹œí‚¤ë©°, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ 166.51M íŒŒë¼ë¯¸í„°ì™€ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ 219M íŒŒë¼ë¯¸í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ìì› ì œì•½ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íš¨ìœ¨ì ì¸ ì¸ê°„-ë¡œë´‡ ìƒí˜¸ì‘ìš©(HRI) ì‘ìš© ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:55:31*