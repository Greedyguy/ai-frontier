<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:44:41.252378",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Guided Chain-of-Thought",
    "Semantic Divergence",
    "Antisemitic Content Detection",
    "In-Context Definition"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Guided Chain-of-Thought": 0.8,
    "Semantic Divergence": 0.78,
    "Antisemitic Content Detection": 0.82,
    "In-Context Definition": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the study, connecting to other works on language models.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Guided-CoT",
        "canonical": "Guided Chain-of-Thought",
        "aliases": [
          "Guided-CoT"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel prompting technique specific to this study.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "semantic divergence",
        "canonical": "Semantic Divergence",
        "aliases": [
          "semantic difference",
          "meaning divergence"
        ],
        "category": "specific_connectable",
        "rationale": "Key to understanding model errors and rationale differences.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "antisemitic content",
        "canonical": "Antisemitic Content Detection",
        "aliases": [
          "antisemitism detection",
          "hateful content detection"
        ],
        "category": "unique_technical",
        "rationale": "Specific application of LLMs in detecting hate speech.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.82
      },
      {
        "surface": "in-context definition",
        "canonical": "In-Context Definition",
        "aliases": [
          "contextual definition",
          "in-situ definition"
        ],
        "category": "specific_connectable",
        "rationale": "Important for understanding the policy guidelines used.",
        "novelty_score": 0.6,
        "connectivity_score": 0.7,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "decoding configuration",
      "model sizes",
      "reasoning capability"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Guided-CoT",
      "resolved_canonical": "Guided Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "semantic divergence",
      "resolved_canonical": "Semantic Divergence",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "antisemitic content",
      "resolved_canonical": "Antisemitic Content Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "in-context definition",
      "resolved_canonical": "In-Context Definition",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.7,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Evaluating Large Language Models for Detecting Antisemitism

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18293.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18293](https://arxiv.org/abs/2509.18293)

## 🔗 유사한 논문
- [[2025-09-22/It Depends_ Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge_20250922|It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge]] (86.8% similar)
- [[2025-09-23/Sugar-Coated Poison_ Benign Generation Unlocks LLM Jailbreaking_20250923|Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking]] (86.6% similar)
- [[2025-09-23/Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates_20250923|Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates]] (86.2% similar)
- [[2025-09-23/Adaptive Distraction_ Probing LLM Contextual Robustness with Automated Tree Search_20250923|Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search]] (86.0% similar)
- [[2025-09-23/MIST_ Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning_20250923|MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning]] (86.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Semantic Divergence|Semantic Divergence]], [[keywords/In-Context Definition|In-Context Definition]]
**⚡ Unique Technical**: [[keywords/Guided Chain-of-Thought|Guided Chain-of-Thought]], [[keywords/Antisemitic Content Detection|Antisemitic Content Detection]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18293v1 Announce Type: cross 
Abstract: Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition as a policy guideline. We explore various prompting techniques and design a new CoT-like prompt, Guided-CoT. Guided-CoT handles the in-context policy well, increasing performance across all evaluated models, regardless of decoding configuration, model sizes, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability.

## 📝 요약

이 연구는 증오 발언 탐지를 위한 자동화 도구의 중요성을 강조하며, 특히 반유대주의 콘텐츠 탐지에 초점을 맞추고 있습니다. 연구에서는 8개의 오픈소스 대형 언어 모델(LLM)의 성능을 평가하고, 정책 지침으로서의 맥락 내 정의를 활용하여 다양한 프롬프트 기법을 탐구합니다. 새로운 CoT 유사 프롬프트인 Guided-CoT를 설계하여 모든 평가 모델의 성능을 향상시켰으며, 특히 Llama 3.1 70B 모델이 미세 조정된 GPT-3.5를 능가하는 성과를 보였습니다. 또한, 모델 오류를 분석하고 모델 생성 논리의 의미적 차이를 정량화하는 지표를 도입하여 LLM 간의 유용성, 설명 가능성, 신뢰성의 차이를 강조합니다.

## 🎯 주요 포인트

- 1. 증오성 콘텐츠 탐지는 도전적이면서도 중요한 문제이며, 자동화 도구인 머신러닝 모델이 이를 돕지만 지속적인 훈련이 필요합니다.
- 2. 본 연구에서는 반유대주의 콘텐츠를 탐지하기 위해 8개의 오픈소스 대형 언어 모델(LLM)의 성능을 평가하고, 맥락 내 정의를 정책 지침으로 활용했습니다.
- 3. 다양한 프롬프트 기법을 탐구하고 새로운 CoT 유사 프롬프트인 Guided-CoT를 설계하여 모든 평가된 모델의 성능을 향상시켰습니다.
- 4. Llama 3.1 70B 모델이 미세 조정된 GPT-3.5보다 우수한 성능을 보였습니다.
- 5. LLM의 오류를 분석하고 모델 생성 논리의 의미적 차이를 정량화하는 지표를 도입하여 LLM 간의 유용성, 설명 가능성, 신뢰성의 차이를 강조했습니다.


---

*Generated on 2025-09-24 13:44:41*