<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:55:15.768079",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Ranking Blind Spot",
    "Decision Objective Hijacking",
    "Decision Criteria Hijacking",
    "Passage Ranking"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Ranking Blind Spot": 0.78,
    "Decision Objective Hijacking": 0.72,
    "Decision Criteria Hijacking": 0.7,
    "Passage Ranking": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on ranking and decision processes.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.67,
        "link_intent_score": 0.85
      },
      {
        "surface": "Ranking Blind Spot",
        "canonical": "Ranking Blind Spot",
        "aliases": [
          "Decision Hijacking"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specific to LLM evaluation vulnerabilities.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Decision Objective Hijacking",
        "canonical": "Decision Objective Hijacking",
        "aliases": [
          "Objective Hijacking"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific attack method relevant to LLM ranking systems.",
        "novelty_score": 0.87,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Decision Criteria Hijacking",
        "canonical": "Decision Criteria Hijacking",
        "aliases": [
          "Criteria Hijacking"
        ],
        "category": "unique_technical",
        "rationale": "Highlights another attack vector in LLM-based ranking systems.",
        "novelty_score": 0.85,
        "connectivity_score": 0.58,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      },
      {
        "surface": "Passage Ranking",
        "canonical": "Passage Ranking",
        "aliases": [
          "Document Ranking"
        ],
        "category": "specific_connectable",
        "rationale": "Key task in information retrieval that the paper addresses.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "evaluation systems",
      "content providers",
      "document positioning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.67,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Ranking Blind Spot",
      "resolved_canonical": "Ranking Blind Spot",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Decision Objective Hijacking",
      "resolved_canonical": "Decision Objective Hijacking",
      "decision": "linked",
      "scores": {
        "novelty": 0.87,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Decision Criteria Hijacking",
      "resolved_canonical": "Decision Criteria Hijacking",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.58,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Passage Ranking",
      "resolved_canonical": "Passage Ranking",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18575.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18575](https://arxiv.org/abs/2509.18575)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (87.3% similar)
- [[2025-09-22/Bias Beware_ The Impact of Cognitive Biases on LLM-Driven Product Recommendations_20250922|Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations]] (86.1% similar)
- [[2025-09-23/Challenging the Evaluator_ LLM Sycophancy Under User Rebuttal_20250923|Challenging the Evaluator: LLM Sycophancy Under User Rebuttal]] (85.6% similar)
- [[2025-09-23/LLMsPark_ A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts_20250923|LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts]] (85.0% similar)
- [[2025-09-23/Justice in Judgment_ Unveiling (Hidden) Bias in LLM-assisted Peer Reviews_20250923|Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews]] (85.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Passage Ranking|Passage Ranking]]
**âš¡ Unique Technical**: [[keywords/Ranking Blind Spot|Ranking Blind Spot]], [[keywords/Decision Objective Hijacking|Decision Objective Hijacking]], [[keywords/Decision Criteria Hijacking|Decision Criteria Hijacking]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18575v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the "Ranking Blind Spot", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì •ë³´ ê²€ìƒ‰ ëŠ¥ë ¥, íŠ¹íˆ ë¬¸ì„œ ìˆœìœ„ ë§¤ê¸°ê¸°ì—ì„œì˜ ì„±ëŠ¥ì„ ì¡°ì‚¬í•˜ë©°, "ìˆœìœ„ ë§¹ì "ì´ë¼ëŠ” LLMì˜ ë¹„êµ í‰ê°€ ê³¼ì •ì—ì„œì˜ íŠ¹ì§•ì„ ì‹ë³„í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ë‘ ê°€ì§€ ì ‘ê·¼ë²•ì„ í†µí•´ ì´ ìˆœìœ„ ë§¹ì ì´ LLM í‰ê°€ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤: ì²«ì§¸, ìŒë³„ ìˆœìœ„ ì‹œìŠ¤í…œì˜ í‰ê°€ ëª©í‘œë¥¼ ë³€ê²½í•˜ëŠ” "ê²°ì • ëª©í‘œ í•˜ì´ì¬í‚¹", ë‘˜ì§¸, ìˆœìœ„ ì²´ê³„ ì „ë°˜ì˜ ê´€ë ¨ì„± ê¸°ì¤€ì„ ìˆ˜ì •í•˜ëŠ” "ê²°ì • ê¸°ì¤€ í•˜ì´ì¬í‚¹"ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ì½˜í…ì¸  ì œê³µìê°€ LLM ê¸°ë°˜ ìˆœìœ„ ì‹œìŠ¤í…œì„ ì¡°ì‘í•˜ì—¬ íŠ¹ì • ë¬¸ì„œë¥¼ ìƒìœ„ì— ë°°ì¹˜í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ê³µê²©ì´ ë‹¤ì–‘í•œ LLMì—ì„œ íš¨ê³¼ì ì´ë©° ì—¬ëŸ¬ ìˆœìœ„ ì²´ê³„ì— ì¼ë°˜í™”ë  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ê°•ë ¥í•œ LLMì¼ìˆ˜ë¡ ì´ëŸ¬í•œ ê³µê²©ì— ë” ì·¨ì•½í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ì˜ ì½”ë“œëŠ” GitHubì—ì„œ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì •ë³´ ê²€ìƒ‰ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ë‹¤ì¤‘ ë¬¸ì„œ ë¹„êµ ì‘ì—…ì—ì„œëŠ” "Ranking Blind Spot"ì´ë¼ëŠ” ì•½ì ì„ ë³´ì¸ë‹¤.
- 2. "Ranking Blind Spot"ëŠ” LLMì˜ ë¹„êµ í‰ê°€ ê³¼ì •ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê²°ì • ê³¼ì •ì˜ íŠ¹ì„±ìœ¼ë¡œ, í‰ê°€ ëª©í‘œì™€ ê´€ë ¨ ê¸°ì¤€ì„ ë³€ê²½í•˜ì—¬ ë¬¸ì„œì˜ ìˆœìœ„ë¥¼ ì¡°ì‘í•  ìˆ˜ ìˆë‹¤.
- 3. ì•…ì˜ì ì¸ ì½˜í…ì¸  ì œê³µìëŠ” LLM ê¸°ë°˜ì˜ ë­í‚¹ ì‹œìŠ¤í…œì„ ê³µê²©í•˜ì—¬ íŠ¹ì • ë¬¸ì„œë¥¼ ìƒìœ„ì— ë…¸ì¶œì‹œí‚´ìœ¼ë¡œì¨ ì¶”ê°€ì ì¸ ë…¸ì¶œì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
- 4. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ê³µê²© ë°©ë²•ì€ ë‹¤ì–‘í•œ LLMì— íš¨ê³¼ì ì´ë©° ì—¬ëŸ¬ ë­í‚¹ ì²´ê³„ì— ì¼ë°˜í™”ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.
- 5. ê°•ë ¥í•œ LLMì¼ìˆ˜ë¡ ì´ëŸ¬í•œ ê³µê²©ì— ë” ì·¨ì•½í•˜ë‹¤ëŠ” ì ì´ ë°œê²¬ë˜ì—ˆë‹¤.


---

*Generated on 2025-09-24 13:55:15*