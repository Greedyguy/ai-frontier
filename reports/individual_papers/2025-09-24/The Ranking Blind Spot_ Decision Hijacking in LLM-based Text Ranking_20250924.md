<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:55:15.768079",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Ranking Blind Spot",
    "Decision Objective Hijacking",
    "Decision Criteria Hijacking",
    "Passage Ranking"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Ranking Blind Spot": 0.78,
    "Decision Objective Hijacking": 0.72,
    "Decision Criteria Hijacking": 0.7,
    "Passage Ranking": 0.68
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's discussion on ranking and decision processes.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.67,
        "link_intent_score": 0.85
      },
      {
        "surface": "Ranking Blind Spot",
        "canonical": "Ranking Blind Spot",
        "aliases": [
          "Decision Hijacking"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel concept specific to LLM evaluation vulnerabilities.",
        "novelty_score": 0.92,
        "connectivity_score": 0.65,
        "specificity_score": 0.82,
        "link_intent_score": 0.78
      },
      {
        "surface": "Decision Objective Hijacking",
        "canonical": "Decision Objective Hijacking",
        "aliases": [
          "Objective Hijacking"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific attack method relevant to LLM ranking systems.",
        "novelty_score": 0.87,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Decision Criteria Hijacking",
        "canonical": "Decision Criteria Hijacking",
        "aliases": [
          "Criteria Hijacking"
        ],
        "category": "unique_technical",
        "rationale": "Highlights another attack vector in LLM-based ranking systems.",
        "novelty_score": 0.85,
        "connectivity_score": 0.58,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      },
      {
        "surface": "Passage Ranking",
        "canonical": "Passage Ranking",
        "aliases": [
          "Document Ranking"
        ],
        "category": "specific_connectable",
        "rationale": "Key task in information retrieval that the paper addresses.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.68
      }
    ],
    "ban_list_suggestions": [
      "evaluation systems",
      "content providers",
      "document positioning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.67,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Ranking Blind Spot",
      "resolved_canonical": "Ranking Blind Spot",
      "decision": "linked",
      "scores": {
        "novelty": 0.92,
        "connectivity": 0.65,
        "specificity": 0.82,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Decision Objective Hijacking",
      "resolved_canonical": "Decision Objective Hijacking",
      "decision": "linked",
      "scores": {
        "novelty": 0.87,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Decision Criteria Hijacking",
      "resolved_canonical": "Decision Criteria Hijacking",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.58,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Passage Ranking",
      "resolved_canonical": "Passage Ranking",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.68
      }
    }
  ]
}
-->

# The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18575.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18575](https://arxiv.org/abs/2509.18575)

## 🔗 유사한 논문
- [[2025-09-23/Breaking the Reviewer_ Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks_20250923|Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks]] (87.3% similar)
- [[2025-09-22/Bias Beware_ The Impact of Cognitive Biases on LLM-Driven Product Recommendations_20250922|Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations]] (86.1% similar)
- [[2025-09-23/Challenging the Evaluator_ LLM Sycophancy Under User Rebuttal_20250923|Challenging the Evaluator: LLM Sycophancy Under User Rebuttal]] (85.6% similar)
- [[2025-09-23/LLMsPark_ A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts_20250923|LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts]] (85.0% similar)
- [[2025-09-23/Justice in Judgment_ Unveiling (Hidden) Bias in LLM-assisted Peer Reviews_20250923|Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews]] (85.0% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Passage Ranking|Passage Ranking]]
**⚡ Unique Technical**: [[keywords/Ranking Blind Spot|Ranking Blind Spot]], [[keywords/Decision Objective Hijacking|Decision Objective Hijacking]], [[keywords/Decision Criteria Hijacking|Decision Criteria Hijacking]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18575v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the "Ranking Blind Spot", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot

## 📝 요약

이 연구는 대형 언어 모델(LLM)의 정보 검색 능력, 특히 문서 순위 매기기에서의 성능을 조사하며, "순위 맹점"이라는 LLM의 비교 평가 과정에서의 특징을 식별합니다. 연구는 두 가지 접근법을 통해 이 순위 맹점이 LLM 평가 시스템에 미치는 영향을 분석합니다: 첫째, 쌍별 순위 시스템의 평가 목표를 변경하는 "결정 목표 하이재킹", 둘째, 순위 체계 전반의 관련성 기준을 수정하는 "결정 기준 하이재킹"입니다. 이러한 접근법은 콘텐츠 제공자가 LLM 기반 순위 시스템을 조작하여 특정 문서를 상위에 배치할 수 있음을 보여줍니다. 실험 결과, 제안된 공격이 다양한 LLM에서 효과적이며 여러 순위 체계에 일반화될 수 있음을 입증했습니다. 또한, 강력한 LLM일수록 이러한 공격에 더 취약하다는 것을 발견했습니다. 연구의 코드는 GitHub에서 공개되어 있습니다.

## 🎯 주요 포인트

- 1. 대형 언어 모델(LLM)은 정보 검색 작업에서 강력한 성능을 보이지만, 다중 문서 비교 작업에서는 "Ranking Blind Spot"이라는 약점을 보인다.
- 2. "Ranking Blind Spot"는 LLM의 비교 평가 과정에서 나타나는 결정 과정의 특성으로, 평가 목표와 관련 기준을 변경하여 문서의 순위를 조작할 수 있다.
- 3. 악의적인 콘텐츠 제공자는 LLM 기반의 랭킹 시스템을 공격하여 특정 문서를 상위에 노출시킴으로써 추가적인 노출을 얻을 수 있다.
- 4. 실험 결과, 제안된 공격 방법은 다양한 LLM에 효과적이며 여러 랭킹 체계에 일반화될 수 있음을 보여준다.
- 5. 강력한 LLM일수록 이러한 공격에 더 취약하다는 점이 발견되었다.


---

*Generated on 2025-09-24 13:55:15*