<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:40:12.693713",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Sentence Embeddings",
    "Self-supervised Learning",
    "Causal Graph",
    "Attention Mechanism"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Sentence Embeddings": 0.75,
    "Self-supervised Learning": 0.8,
    "Causal Graph": 0.7,
    "Attention Mechanism": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "ChatGPT",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "OpenAI Chatbot"
        ],
        "category": "broad_technical",
        "rationale": "ChatGPT is a prominent example of a Large Language Model, providing a direct link to discussions around LLMs.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "sentence embeddings",
        "canonical": "Sentence Embeddings",
        "aliases": [
          "sentence vectors"
        ],
        "category": "unique_technical",
        "rationale": "Sentence embeddings are crucial for understanding text structure, offering a unique technical insight into text analysis.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "contrastive learning",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "contrastive training"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive learning is a form of self-supervised learning, enhancing connections to related learning paradigms.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "causal graph",
        "canonical": "Causal Graph",
        "aliases": [
          "causal diagram"
        ],
        "category": "unique_technical",
        "rationale": "Causal graphs are used to isolate structural features, providing a unique perspective on data analysis.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.7
      },
      {
        "surface": "attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "attention model"
        ],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are foundational in modern NLP models, linking to a wide array of related research.",
        "novelty_score": 0.5,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "word-level detectors",
      "modified text",
      "training data content"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "ChatGPT",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "sentence embeddings",
      "resolved_canonical": "Sentence Embeddings",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "contrastive learning",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "causal graph",
      "resolved_canonical": "Causal Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18535.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18535](https://arxiv.org/abs/2509.18535)

## 🔗 유사한 논문
- [[2025-09-23/Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation_20250923|Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation]] (88.3% similar)
- [[2025-09-24/Diversity Boosts AI-Generated Text Detection_20250924|Diversity Boosts AI-Generated Text Detection]] (86.1% similar)
- [[2025-09-22/DNA-DetectLLM_ Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm_20250922|DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm]] (85.7% similar)
- [[2025-09-19/JU-NLP at Touch\'e_ Covert Advertisement in Conversational AI-Generation and Detection Strategies_20250919|JU-NLP at Touch\'e: Covert Advertisement in Conversational AI-Generation and Detection Strategies]] (83.4% similar)
- [[2025-09-18/AgentCTG_ Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation_20250918|AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation]] (82.9% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**🔗 Specific Connectable**: [[keywords/Self-supervised Learning|Self-supervised Learning]], [[keywords/Attention Mechanism|Attention Mechanism]]
**⚡ Unique Technical**: [[keywords/Sentence Embeddings|Sentence Embeddings]], [[keywords/Causal Graph|Causal Graph]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18535v1 Announce Type: new 
Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse, highlighting the need for robust detection of AI-generated text. Current word-level detectors are vulnerable to paraphrasing or simple prompts (PSP), suffer from biases induced by ChatGPT's word-level patterns (CWP) and training data content, degrade on modified text, and often require large models or online LLM interaction. To tackle these issues, we introduce a novel task to detect both original and PSP-modified AI-generated texts, and propose a lightweight framework that classifies texts based on their internal structure, which remains invariant under word-level changes. Our approach encodes sentence embeddings from pre-trained language models and models their relationships via attention. We employ contrastive learning to mitigate embedding biases from autoregressive generation and incorporate a causal graph with counterfactual methods to isolate structural features from topic-related biases. Experiments on two curated datasets, including abstract comparisons and revised life FAQs, validate the effectiveness of our method.

## 📝 요약

ChatGPT의 오용 문제를 해결하기 위해 AI 생성 텍스트를 감지하는 새로운 과제를 제안합니다. 기존의 단어 수준 감지기는 패러프레이징에 취약하고, ChatGPT의 단어 패턴과 훈련 데이터로 인한 편향을 겪으며, 수정된 텍스트에서 성능이 저하됩니다. 이를 해결하기 위해, 우리는 단어 수준의 변화에도 불변인 내부 구조를 기반으로 텍스트를 분류하는 경량 프레임워크를 제안합니다. 사전 학습된 언어 모델의 문장 임베딩을 인코딩하고, 주의 메커니즘을 통해 관계를 모델링합니다. 또한, 대조 학습을 통해 생성 편향을 완화하고, 인과 그래프와 반사실적 방법을 사용하여 구조적 특징을 주제 관련 편향으로부터 분리합니다. 두 개의 데이터셋 실험을 통해 제안된 방법의 효과를 검증했습니다.

## 🎯 주요 포인트

- 1. ChatGPT의 오용 우려로 AI 생성 텍스트를 감지하는 강력한 방법의 필요성이 대두되고 있다.
- 2. 기존의 단어 수준 탐지기는 패러프레이징이나 간단한 프롬프트에 취약하며, 대규모 모델이나 온라인 LLM 상호작용이 필요하다.
- 3. 본 연구는 원본 및 수정된 AI 생성 텍스트를 탐지하는 새로운 과제를 제안하고, 단어 수준 변경에도 불변인 내부 구조를 기반으로 텍스트를 분류하는 경량 프레임워크를 제안한다.
- 4. 사전 학습된 언어 모델의 문장 임베딩을 인코딩하고, 주의 메커니즘을 통해 관계를 모델링하여 구조적 특징을 주제 관련 편향에서 분리한다.
- 5. 두 개의 큐레이션된 데이터셋에서 실험을 통해 제안된 방법의 효과성을 검증하였다.


---

*Generated on 2025-09-24 15:40:12*