<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:20:43.695189",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Hierarchical Segment-Graph Memory",
    "Local Semantic Graphs",
    "Global Graph Memory",
    "Hierarchical Query Processing",
    "Natural Language Processing"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Hierarchical Segment-Graph Memory": 0.8,
    "Local Semantic Graphs": 0.75,
    "Global Graph Memory": 0.7,
    "Hierarchical Query Processing": 0.72,
    "Natural Language Processing": 0.6
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Hierarchical Segment-Graph Memory",
        "canonical": "Hierarchical Segment-Graph Memory",
        "aliases": [
          "HSGM"
        ],
        "category": "unique_technical",
        "rationale": "HSGM is a novel framework introduced in the paper, providing a unique method for scalable long-text semantics.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Local Semantic Graphs",
        "canonical": "Local Semantic Graphs",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Local Semantic Graphs are a key component of the HSGM framework, crucial for understanding its segmentation approach.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Global Graph Memory",
        "canonical": "Global Graph Memory",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Global Graph Memory is essential for the hierarchical structure of HSGM, enabling efficient semantic processing.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      },
      {
        "surface": "Hierarchical Query Processing",
        "canonical": "Hierarchical Query Processing",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This process is vital for the retrieval and reasoning capabilities of HSGM, enhancing its semantic parsing efficiency.",
        "novelty_score": 0.68,
        "connectivity_score": 0.62,
        "specificity_score": 0.77,
        "link_intent_score": 0.72
      },
      {
        "surface": "Natural Language Processing",
        "canonical": "Natural Language Processing",
        "aliases": [
          "NLP"
        ],
        "category": "broad_technical",
        "rationale": "NLP is the broader field in which the HSGM framework is applied, providing context for its significance.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.6
      }
    ],
    "ban_list_suggestions": [
      "semantic parsing",
      "incremental updates",
      "fine-grained reasoning"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Hierarchical Segment-Graph Memory",
      "resolved_canonical": "Hierarchical Segment-Graph Memory",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Local Semantic Graphs",
      "resolved_canonical": "Local Semantic Graphs",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Global Graph Memory",
      "resolved_canonical": "Global Graph Memory",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Hierarchical Query Processing",
      "resolved_canonical": "Hierarchical Query Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.62,
        "specificity": 0.77,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Natural Language Processing",
      "resolved_canonical": "Natural Language Processing",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.6
      }
    }
  ]
}
-->

# HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18168.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18168](https://arxiv.org/abs/2509.18168)

## 🔗 유사한 논문
- [[2025-09-22/UniGist_ Towards General and Hardware-aligned Sequence-level Long Context Compression_20250922|UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression]] (84.2% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (84.2% similar)
- [[2025-09-22/SEMMA_ A Semantic Aware Knowledge Graph Foundation Model_20250922|SEMMA: A Semantic Aware Knowledge Graph Foundation Model]] (82.6% similar)
- [[2025-09-23/DescriptorMedSAM_ Language-Image Fusion with Multi-Aspect Text Guidance for Medical Image Segmentation_20250923|DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance for Medical Image Segmentation]] (82.4% similar)
- [[2025-09-23/SEDM_ Scalable Self-Evolving Distributed Memory for Agents_20250923|SEDM: Scalable Self-Evolving Distributed Memory for Agents]] (82.2% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Natural Language Processing|Natural Language Processing]]
**⚡ Unique Technical**: [[keywords/Hierarchical Segment-Graph Memory|Hierarchical Segment-Graph Memory]], [[keywords/Local Semantic Graphs|Local Semantic Graphs]], [[keywords/Global Graph Memory|Global Graph Memory]], [[keywords/Hierarchical Query Processing|Hierarchical Query Processing]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18168v1 Announce Type: new 
Abstract: Semantic parsing of long documents remains challenging due to quadratic growth in pairwise composition and memory requirements. We introduce \textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that decomposes an input of length $N$ into $M$ meaningful segments, constructs \emph{Local Semantic Graphs} on each segment, and extracts compact \emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports \emph{incremental updates} -- only newly arrived segments incur local graph construction and summary-node integration -- while \emph{Hierarchical Query Processing} locates relevant segments via top-$K$ retrieval over summary nodes and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to $O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive Frobenius-norm bounds on the approximation error introduced by node summarization and sparsification thresholds. Empirically, on three benchmarks -- long-document AMR parsing, segment-level semantic role labeling (OntoNotes), and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of baseline accuracy. Our approach unlocks scalable, accurate semantic modeling for ultra-long texts, enabling real-time and resource-constrained NLP applications.

## 📝 요약

이 논문은 긴 문서의 의미 구문 분석 문제를 해결하기 위해 \textbf{Hierarchical Segment-Graph Memory (HSGM)}라는 새로운 프레임워크를 제안합니다. HSGM은 입력 데이터를 의미 있는 세그먼트로 분할하고, 각 세그먼트에 대해 로컬 의미 그래프를 구축하여 요약 노드를 추출한 후, 이를 통해 글로벌 그래프 메모리를 형성합니다. 이 방법은 새롭게 도착한 세그먼트에 대해서만 로컬 그래프를 구축하고 요약 노드를 통합하는 \emph{증분 업데이트}를 지원하며, \emph{계층적 쿼리 처리}를 통해 관련 세그먼트를 찾아내고 세부적인 추론을 수행합니다. 이론적으로 HSGM은 최악의 경우 복잡도를 $O(N^2)$에서 $O\!\left(N\,k + (N/k)^2\right)$로 줄이며, 노드 요약과 희소화 임계값에 의해 발생하는 근사 오차에 대한 Frobenius-노름 경계를 도출합니다. 실험적으로, HSGM은 긴 문서 AMR 파싱, 세그먼트 수준의 의미 역할 레이블링(OntoNotes), 법적 사건 추출 등 세 가지 벤치마크에서 \emph{2-4배의 추론 속도 향상}, \emph{최대 메모리 사용량 60% 이상 감소}, \emph{기준 정확도의 95% 이상}을 달성했습니다. 이 접근법은 초장문 텍스트에 대한 확장 가능하고 정확한 의미 모델링을 가능하게 하여 실시간 및 자원 제약이 있는 NLP 응용 프로그램에 기여합니다.

## 🎯 주요 포인트

- 1. HSGM은 입력 문서를 의미 있는 세그먼트로 분해하여 각 세그먼트에 로컬 의미 그래프를 구축하고 요약 노드를 추출하여 글로벌 그래프 메모리를 형성합니다.
- 2. HSGM는 증분 업데이트를 지원하여 새로 도착한 세그먼트만 로컬 그래프 구축과 요약 노드 통합을 수행합니다.
- 3. 이론적으로, HSGM은 최악의 경우 복잡도를 $O(N^2)$에서 $O(N\,k + (N/k)^2)$로 줄이며, 세그먼트 크기 $k \ll N$입니다.
- 4. HSGM는 세 가지 벤치마크에서 2-4배의 추론 속도 향상, 60% 이상의 메모리 사용량 감소, 95% 이상의 정확도를 달성했습니다.
- 5. 이 접근 방식은 초장문 텍스트에 대한 확장 가능하고 정확한 의미 모델링을 가능하게 하여 실시간 및 자원 제약이 있는 NLP 애플리케이션을 지원합니다.


---

*Generated on 2025-09-24 13:20:43*