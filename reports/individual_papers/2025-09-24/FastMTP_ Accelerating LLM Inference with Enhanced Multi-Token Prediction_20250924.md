<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:46:32.921227",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Multi-Token Prediction",
    "Speculative Decoding",
    "Dynamic Vocabulary Compression"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Multi-Token Prediction": 0.78,
    "Speculative Decoding": 0.72,
    "Dynamic Vocabulary Compression": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's discussion on inference acceleration and are a key concept in modern NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Multi-Token Prediction",
        "canonical": "Multi-Token Prediction",
        "aliases": [
          "MTP"
        ],
        "category": "unique_technical",
        "rationale": "The paper introduces FastMTP, which is a novel method for enhancing multi-token prediction, a unique technical concept.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Speculative Decoding is a specific technique discussed in the paper that contributes to inference speedup, making it a unique technical concept.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "Dynamic Vocabulary Compression",
        "canonical": "Dynamic Vocabulary Compression",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This technique is crucial for reducing computational overhead and is a unique technical concept introduced in the paper.",
        "novelty_score": 0.68,
        "connectivity_score": 0.55,
        "specificity_score": 0.78,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "efficiency"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Multi-Token Prediction",
      "resolved_canonical": "Multi-Token Prediction",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Dynamic Vocabulary Compression",
      "resolved_canonical": "Dynamic Vocabulary Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.55,
        "specificity": 0.78,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18362.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18362](https://arxiv.org/abs/2509.18362)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/L-MTP_ Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models_20250923|L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models]] (92.2% similar)
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (83.9% similar)
- [[2025-09-23/Spiffy_ Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding_20250923|Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding]] (83.7% similar)
- [[2025-09-24/Sparse Training Scheme for Multimodal LLM_20250924|Sparse Training Scheme for Multimodal LLM]] (83.5% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (83.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Multi-Token Prediction|Multi-Token Prediction]], [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Dynamic Vocabulary Compression|Dynamic Vocabulary Compression]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18362v1 Announce Type: cross 
Abstract: As large language models (LLMs) become increasingly powerful, the sequential nature of autoregressive generation creates a fundamental throughput bottleneck that limits the practical deployment. While Multi-Token Prediction (MTP) has demonstrated remarkable benefits for model training efficiency and performance, its inherent potential for inference acceleration remains largely unexplored. This paper introduces FastMTP, a simple yet effective method that improves multi-step draft quality by aligning MTP training with its inference pattern, significantly enhancing speculative decoding performance. Our approach fine-tunes a single MTP head with position-shared weights on self-distilled data, enabling it to capture dependencies among consecutive future tokens and maintain high acceptance rates across multiple recursive draft steps. By integrating language-aware dynamic vocabulary compression into the MTP head, we further reduce computational overhead in the drafting process. Experimental results across seven diverse benchmarks demonstrate that FastMTP achieves an average of 2.03x speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires only lightweight training and seamlessly integrates with existing inference frameworks, offering a practical and rapidly deployable solution for accelerating LLM inference.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ FastMTPë¼ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. FastMTPëŠ” ë‹¤ì¤‘ í† í° ì˜ˆì¸¡(MTP) í›ˆë ¨ì„ ì¶”ë¡  íŒ¨í„´ê³¼ ì •ë ¬ì‹œì¼œ ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ ê°œì„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìœ„ì¹˜ ê³µìœ  ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ë‹¨ì¼ MTP í—¤ë“œë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì—°ì†ì ì¸ ë¯¸ë˜ í† í° ê°„ì˜ ì˜ì¡´ì„±ì„ í¬ì°©í•˜ê³ , ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì´ˆì•ˆ ì‘ì„±ì—ì„œ ë†’ì€ ìˆ˜ìš©ë¥ ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, ì–¸ì–´ ì¸ì‹ ë™ì  ì–´íœ˜ ì••ì¶•ì„ í†µí•©í•˜ì—¬ ê³„ì‚° ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, FastMTPëŠ” í‘œì¤€ ë‹¤ìŒ í† í° ì˜ˆì¸¡ë³´ë‹¤ í‰ê·  2.03ë°° ë¹ ë¥¸ ì†ë„ë¥¼ ê¸°ë¡í•˜ë©°, ê¸°ì¡´ MTPë³´ë‹¤ 82% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. FastMTPëŠ” ê²½ëŸ‰ í›ˆë ¨ë§Œ í•„ìš”í•˜ë©° ê¸°ì¡´ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ì— ì‰½ê²Œ í†µí•©ë  ìˆ˜ ìˆì–´ ì‹¤ìš©ì ì´ê³  ë¹ ë¥´ê²Œ ë°°í¬ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. FastMTPëŠ” MTP í›ˆë ¨ì„ ì¶”ë¡  íŒ¨í„´ê³¼ ì •ë ¬í•˜ì—¬ ë‹¤ë‹¨ê³„ ì´ˆì•ˆ í’ˆì§ˆì„ ê°œì„ í•˜ê³  ì¶”ì¸¡ ë””ì½”ë”© ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- 2. FastMTPëŠ” ìœ„ì¹˜ ê³µìœ  ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ë‹¨ì¼ MTP í—¤ë“œë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì—°ì†ì ì¸ ë¯¸ë˜ í† í° ê°„ì˜ ì¢…ì†ì„±ì„ í¬ì°©í•˜ê³  ì—¬ëŸ¬ ë°˜ë³µ ì´ˆì•ˆ ë‹¨ê³„ì—ì„œ ë†’ì€ ìˆ˜ìš©ë¥ ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 3. ì–¸ì–´ ì¸ì‹ ë™ì  ì–´íœ˜ ì••ì¶•ì„ MTP í—¤ë“œì— í†µí•©í•˜ì—¬ ì´ˆì•ˆ ì‘ì„± ê³¼ì •ì—ì„œ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì…ë‹ˆë‹¤.
- 4. FastMTPëŠ” í‘œì¤€ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì— ë¹„í•´ í‰ê·  2.03ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•˜ë©°, ì¶œë ¥ í’ˆì§ˆì˜ ì†ì‹¤ ì—†ì´ ë°”ë‹ë¼ MTPë³´ë‹¤ 82% ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- 5. FastMTPëŠ” ê²½ëŸ‰ í›ˆë ¨ë§Œ í•„ìš”í•˜ë©° ê¸°ì¡´ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ì™€ ì›í™œí•˜ê²Œ í†µí•©ë˜ì–´ LLM ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ì‹¤ìš©ì ì´ê³  ì‹ ì†í•˜ê²Œ ë°°í¬ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:46:32*