<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:36:00.546387",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Transformer",
    "Scientific Method",
    "Path Integration",
    "Explainability"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Transformer": 0.85,
    "Scientific Method": 0.7,
    "Path Integration": 0.72,
    "Explainability": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "transformer-based machine learning",
        "canonical": "Transformer",
        "aliases": [
          "transformer",
          "transformer model"
        ],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational technology in machine learning, connecting to various applications and methods.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "scientific method",
        "canonical": "Scientific Method",
        "aliases": [
          "scientific approach",
          "scientific process"
        ],
        "category": "unique_technical",
        "rationale": "The scientific method is a distinct approach to knowledge acquisition, contrasting with machine learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "path-integration techniques",
        "canonical": "Path Integration",
        "aliases": [
          "path integration method",
          "path integral"
        ],
        "category": "specific_connectable",
        "rationale": "Path integration is a mathematical technique that can be linked to transformer operations.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.72
      },
      {
        "surface": "explainability",
        "canonical": "Explainability",
        "aliases": [
          "model interpretability",
          "interpretability"
        ],
        "category": "evolved_concepts",
        "rationale": "Explainability is crucial for understanding machine learning models and their decision-making processes.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "random-like character",
      "physical problem",
      "general comments"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "transformer-based machine learning",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "scientific method",
      "resolved_canonical": "Scientific Method",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "path-integration techniques",
      "resolved_canonical": "Path Integration",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "explainability",
      "resolved_canonical": "Explainability",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# Two ways to knowledge?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18131.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18131](https://arxiv.org/abs/2509.18131)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Hierarchical Self-Attention_ Generalizing Neural Attention Mechanics to Multi-Scale Problems_20250922|Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems]] (81.2% similar)
- [[2025-09-23/Measure-to-measure interpolation using Transformers_20250923|Measure-to-measure interpolation using Transformers]] (81.0% similar)
- [[2025-09-23/Scaling Efficient LLMs_20250923|Scaling Efficient LLMs]] (79.2% similar)
- [[2025-09-22/RMT-KD_ Random Matrix Theoretic Causal Knowledge Distillation_20250922|RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation]] (78.3% similar)
- [[2025-09-22/Negotiated Representations to Prevent Overfitting in Machine Learning Applications_20250922|Negotiated Representations to Prevent Overfitting in Machine Learning Applications]] (78.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Path Integration|Path Integration]]
**âš¡ Unique Technical**: [[keywords/Scientific Method|Scientific Method]]
**ğŸš€ Evolved Concepts**: [[keywords/Explainability|Explainability]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18131v1 Announce Type: cross 
Abstract: It is shown that the weight matrices of transformer-based machine learning applications to the solution of two representative physical applications show a random-like character which bears no directly recognizable link to the physical and mathematical structure of the physical problem under study. This suggests that machine learning and the scientific method may represent two distinct and potentially complementary paths to knowledge, even though a strict notion of explainability in terms of direct correspondence between network parameters and physical structures may remain out of reach. It is also observed that drawing a parallel between transformer operation and (generalized) path-integration techniques may account for the random-like nature of the weights, but still does not resolve the tension with explainability. We conclude with some general comments on the hazards of gleaning knowledge without the benefit of Insight.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹ì´ ë‘ ê°€ì§€ ë¬¼ë¦¬ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì—ì„œ ìƒì„±í•˜ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ ë¬´ì‘ìœ„ì  íŠ¹ì„±ì„ ë³´ì´ë©°, ì´ëŠ” ë¬¸ì œì˜ ë¬¼ë¦¬ì  ë° ìˆ˜í•™ì  êµ¬ì¡°ì™€ ì§ì ‘ì ì¸ ì—°ê´€ì„±ì´ ì—†ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ë¨¸ì‹ ëŸ¬ë‹ê³¼ ê³¼í•™ì  ë°©ë²•ì´ ì„œë¡œ ë‹¤ë¥¸ ì§€ì‹ íšë“ ê²½ë¡œë¥¼ ì œê³µí•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë˜í•œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì‘ë™ì„ ê²½ë¡œ ì ë¶„ ê¸°ë²•ê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ì„¤ëª… ê°€ëŠ¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í†µì°° ì—†ì´ ì§€ì‹ì„ ì–»ëŠ” ê²ƒì˜ ìœ„í—˜ì„±ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ë…¼í‰ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ê¸°ê³„ í•™ìŠµì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì€ ë¬¼ë¦¬ì  ë¬¸ì œì˜ êµ¬ì¡°ì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì§€ ì•ŠëŠ” ë¬´ì‘ìœ„ì  íŠ¹ì„±ì„ ë³´ì¸ë‹¤.
- 2. ê¸°ê³„ í•™ìŠµê³¼ ê³¼í•™ì  ë°©ë²•ì€ ì§€ì‹ íšë“ì˜ ë‘ ê°€ì§€ ìƒì´í•˜ê³  ì ì¬ì ìœ¼ë¡œ ìƒí˜¸ ë³´ì™„ì ì¸ ê²½ë¡œë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
- 3. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì‘ë™ê³¼ ê²½ë¡œ ì ë¶„ ê¸°ë²• ê°„ì˜ ìœ ì‚¬ì„±ì€ ê°€ì¤‘ì¹˜ì˜ ë¬´ì‘ìœ„ì  íŠ¹ì„±ì„ ì„¤ëª…í•  ìˆ˜ ìˆì§€ë§Œ, ì„¤ëª… ê°€ëŠ¥ì„± ë¬¸ì œëŠ” ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•ŠëŠ”ë‹¤.
- 4. í†µì°° ì—†ì´ ì§€ì‹ì„ ì–»ëŠ” ê²ƒì˜ ìœ„í—˜ì„±ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ë…¼í‰ì´ ì œì‹œëœë‹¤.


---

*Generated on 2025-09-24 13:36:00*