<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:07:25.159706",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Zero-Shot Learning",
    "Vision-Language Model",
    "Self-supervised Learning",
    "Pre-trained Model"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Zero-Shot Learning": 0.9,
    "Vision-Language Model": 0.85,
    "Self-supervised Learning": 0.8,
    "Pre-trained Model": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Zero-Shot Image Classification",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot Classification"
        ],
        "category": "specific_connectable",
        "rationale": "This concept is central to the paper's approach and connects well with existing discussions on learning paradigms without labeled data.",
        "novelty_score": 0.7,
        "connectivity_score": 0.85,
        "specificity_score": 0.8,
        "link_intent_score": 0.9
      },
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-language models are pivotal in the proposed framework, linking to multimodal learning discussions.",
        "novelty_score": 0.65,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Self-Learning Cycle",
        "canonical": "Self-supervised Learning",
        "aliases": [
          "Self-Learning"
        ],
        "category": "specific_connectable",
        "rationale": "The self-learning cycle is a key mechanism in the paper, aligning with self-supervised learning techniques.",
        "novelty_score": 0.68,
        "connectivity_score": 0.82,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "Pre-trained Visual Model",
        "canonical": "Pre-trained Model",
        "aliases": [
          "Pre-trained Visual"
        ],
        "category": "broad_technical",
        "rationale": "Pre-trained models are a foundational element in transfer learning, relevant to the paper's methodology.",
        "novelty_score": 0.55,
        "connectivity_score": 0.79,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      }
    ],
    "ban_list_suggestions": [
      "deep learning",
      "classification performance",
      "annotated datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Zero-Shot Image Classification",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.85,
        "specificity": 0.8,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Self-Learning Cycle",
      "resolved_canonical": "Self-supervised Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.82,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Pre-trained Visual Model",
      "resolved_canonical": "Pre-trained Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.79,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    }
  ]
}
-->

# No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18938.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18938](https://arxiv.org/abs/2509.18938)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/ViLReF_ An Expert Knowledge Enabled Vision-Language Retinal Foundation Model_20250923|ViLReF: An Expert Knowledge Enabled Vision-Language Retinal Foundation Model]] (85.9% similar)
- [[2025-09-22/Robust Vision-Language Models via Tensor Decomposition_ A Defense Against Adversarial Attacks_20250922|Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks]] (84.9% similar)
- [[2025-09-23/Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification_20250923|Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification]] (84.5% similar)
- [[2025-09-23/Catching the Details_ Self-Distilled RoI Predictors for Fine-Grained MLLM Perception_20250923|Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception]] (84.2% similar)
- [[2025-09-24/VLN-Zero_ Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation_20250924|VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation]] (84.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Pre-trained Model|Pre-trained Model]]
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Self-supervised Learning|Self-supervised Learning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18938v1 Announce Type: cross 
Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has significantly advanced classification performance, its typical reliance on extensive annotated datasets presents a major obstacle in many practical scenarios where such data is scarce. Vision-language models (VLMs) and transfer learning with pre-trained visual models appear as promising techniques to deal with this problem. This paper proposes a novel zero-shot image classification framework that combines a VLM and a pre-trained visual model within a self-learning cycle. Requiring only the set of class names and no labeled training data, our method utilizes a confidence-based pseudo-labeling strategy to train a lightweight classifier directly on the test data, enabling dynamic adaptation. The VLM identifies high-confidence samples, and the pre-trained visual model enhances their visual representations. These enhanced features then iteratively train the classifier, allowing the system to capture complementary semantic and visual cues without supervision. Notably, our approach avoids VLM fine-tuning and the use of large language models, relying on the visual-only model to reduce the dependence on semantic representation. Experimental evaluations on ten diverse datasets demonstrate that our approach outperforms the baseline zero-shot method.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì£¼ì„ì´ ë¶€ì¡±í•œ ë°ì´í„°ì…‹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ê³¼ ì‚¬ì „ í•™ìŠµëœ ì‹œê° ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ìì²´ í•™ìŠµ ì‚¬ì´í´ì„ í˜•ì„±í•©ë‹ˆë‹¤. í´ë˜ìŠ¤ ì´ë¦„ë§Œ í•„ìš”ë¡œ í•˜ë©°, ë¼ë²¨ì´ ì—†ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì‹ ë¢° ê¸°ë°˜ì˜ ê°€ì§œ ë¼ë²¨ë§ ì „ëµì„ ì‚¬ìš©í•´ ê²½ëŸ‰ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. VLMì€ ë†’ì€ ì‹ ë¢°ë„ì˜ ìƒ˜í”Œì„ ì‹ë³„í•˜ê³ , ì‚¬ì „ í•™ìŠµëœ ì‹œê° ëª¨ë¸ì€ ì´ë“¤ì˜ ì‹œê°ì  í‘œí˜„ì„ ê°•í™”í•©ë‹ˆë‹¤. ì´ ê°•í™”ëœ íŠ¹ì§•ì„ í†µí•´ ë¶„ë¥˜ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµì‹œì¼œ, ê°ë… ì—†ì´ ë³´ì™„ì ì¸ ì˜ë¯¸ì™€ ì‹œê°ì  ë‹¨ì„œë¥¼ í¬ì°©í•©ë‹ˆë‹¤. VLMì˜ ë¯¸ì„¸ ì¡°ì •ì´ë‚˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì‹œê° ëª¨ë¸ì— ì˜ì¡´í•˜ì—¬ ì˜ë¯¸ í‘œí˜„ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ 10ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ ì œë¡œìƒ· ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ì£¼ì„ì´ ë¶€ì¡±í•œ ë°ì´í„°ì…‹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ VLMê³¼ ì‚¬ì „ í•™ìŠµëœ ë¹„ì£¼ì–¼ ëª¨ë¸ì„ ê²°í•©í•œ ìƒˆë¡œìš´ ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì œì•ˆëœ ë°©ë²•ì€ í´ë˜ìŠ¤ ì´ë¦„ë§Œ í•„ìš”ë¡œ í•˜ë©°, ë¼ë²¨ì´ ì—†ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ê²½ëŸ‰í™”ëœ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ì‹ ë¢° ê¸°ë°˜ì˜ ì˜ì‚¬ ë¼ë²¨ë§ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 3. VLMì€ ë†’ì€ ì‹ ë¢°ë„ì˜ ìƒ˜í”Œì„ ì‹ë³„í•˜ê³ , ì‚¬ì „ í•™ìŠµëœ ë¹„ì£¼ì–¼ ëª¨ë¸ì€ ì´ë“¤ì˜ ì‹œê°ì  í‘œí˜„ì„ ê°•í™”í•˜ì—¬ ë¶„ë¥˜ê¸°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.
- 4. ë³¸ ì ‘ê·¼ë²•ì€ VLMì˜ ë¯¸ì„¸ ì¡°ì •ê³¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ìš©ì„ í”¼í•˜ë©°, ì‹œê°ì  ëª¨ë¸ì— ì˜ì¡´í•˜ì—¬ ì˜ë¯¸ í‘œí˜„ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì¤„ì…ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ 10ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ì˜ ì œë¡œìƒ· ë°©ë²•ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:07:25*