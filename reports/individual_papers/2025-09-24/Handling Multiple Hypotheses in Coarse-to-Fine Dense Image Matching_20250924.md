<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:33:12.669895",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Dense Image Matching",
    "Coarse-to-Fine Mechanism",
    "Attention Mechanism",
    "BEAMER Architecture"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Dense Image Matching": 0.78,
    "Coarse-to-Fine Mechanism": 0.72,
    "Attention Mechanism": 0.8,
    "BEAMER Architecture": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Dense Image Matching",
        "canonical": "Dense Image Matching",
        "aliases": [
          "Dense Matching",
          "Pixel Correspondence"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific technique central to the paper, offering unique insights into image processing tasks.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Coarse-to-Fine Mechanism",
        "canonical": "Coarse-to-Fine Mechanism",
        "aliases": [
          "Hierarchical Matching",
          "Multi-Scale Approach"
        ],
        "category": "unique_technical",
        "rationale": "This technique is crucial for understanding the hierarchical processing approach in image matching.",
        "novelty_score": 0.68,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.72
      },
      {
        "surface": "Cross-Attention Layers",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Links to existing concepts in neural network architectures, enhancing understanding of attention mechanisms.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "BEAMER",
        "canonical": "BEAMER Architecture",
        "aliases": [
          "BEAMER Model"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a novel architecture that may become a reference point for future research in dense matching.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.9,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "hypothesis",
      "scale"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Dense Image Matching",
      "resolved_canonical": "Dense Image Matching",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Coarse-to-Fine Mechanism",
      "resolved_canonical": "Coarse-to-Fine Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Cross-Attention Layers",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "BEAMER",
      "resolved_canonical": "BEAMER Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.9,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.08805.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.08805](https://arxiv.org/abs/2509.08805)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Efficient Beam Search for Large Language Models Using Trie-Based Decoding_20250923|Efficient Beam Search for Large Language Models Using Trie-Based Decoding]] (81.1% similar)
- [[2025-09-22/DistillMatch_ Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching_20250922|DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching]] (81.0% similar)
- [[2025-09-23/CoBEVMoE_ Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception_20250923|CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception]] (80.7% similar)
- [[2025-09-24/Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable_20250924|Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable]] (80.2% similar)
- [[2025-09-24/Do It Yourself_ Learning Semantic Correspondence from Pseudo-Labels_20250924|Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels]] (79.8% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Dense Image Matching|Dense Image Matching]], [[keywords/Coarse-to-Fine Mechanism|Coarse-to-Fine Mechanism]], [[keywords/BEAMER Architecture|BEAMER Architecture]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.08805v2 Announce Type: replace 
Abstract: Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë°€ì§‘ ì´ë¯¸ì§€ ë§¤ì¹­ ë¬¸ì œë¥¼ ë‹¤ë£¨ë©°, ê¸°ì¡´ ë°©ë²•ë“¤ì´ ê° ìŠ¤ì¼€ì¼ì—ì„œ ë‹¨ì¼ ëŒ€ì‘ ê°€ì„¤ì„ ìƒì„±í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, ê° ìŠ¤ì¼€ì¼ì—ì„œ ì—¬ëŸ¬ ëŒ€ì‘ ê°€ì„¤ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë¹” ì„œì¹˜ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°€ì„¤ì„ ì „íŒŒí•˜ê³ , ì´ë¥¼ êµì°¨ ì£¼ì˜ ë ˆì´ì–´ì— í†µí•©í•˜ì—¬ ìƒˆë¡œìš´ ë°€ì§‘ ë§¤ì¹­ ì•„í‚¤í…ì²˜ì¸ BEAMERë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. BEAMERëŠ” ê¹Šì´ ë¶ˆì—°ì†ì„±ì´ë‚˜ íƒ€ê²Ÿ ì´ë¯¸ì§€ê°€ ì†ŒìŠ¤ ì´ë¯¸ì§€ì˜ ê°•í•œ ì¤Œì¸ì¸ ê²½ìš°ì—ë„ ë³´ë‹¤ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë°€ì§‘ ì´ë¯¸ì§€ ë§¤ì¹­ì€ ì†ŒìŠ¤ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì— ëŒ€í•´ ë¶€ë¶„ì ìœ¼ë¡œ ê²¹ì¹˜ëŠ” íƒ€ê²Ÿ ì´ë¯¸ì§€ì—ì„œ ëŒ€ì‘ì ì„ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
- 2. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ê° ìŠ¤ì¼€ì¼ì—ì„œ ì†ŒìŠ¤ ìœ„ì¹˜ë‹¹ ë‹¨ì¼ ëŒ€ì‘ ê°€ì„¤ì„ ìƒì„±í•˜ëŠ” ì¡°ë°€-ëŒ€-ì¡°ë°€ ë©”ì»¤ë‹ˆì¦˜ì— ì˜ì¡´í•œë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê° ìŠ¤ì¼€ì¼ì—ì„œ ì†ŒìŠ¤ ìœ„ì¹˜ë‹¹ ë‹¤ì¤‘ ëŒ€ì‘ ê°€ì„¤ì„ ì˜ˆì¸¡í•˜ëŠ” ì•„ì´ë””ì–´ë¥¼ íƒêµ¬í•œë‹¤.
- 4. BEAMERë¼ëŠ” ìƒˆë¡œìš´ ë°€ì§‘ ë§¤ì¹­ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•˜ë©°, ì´ëŠ” ë‹¤ì¤‘ ê°€ì„¤ì„ ë³´ì¡´í•˜ê³  ìŠ¤ì¼€ì¼ ê°„ì— ì „íŒŒí•˜ëŠ” í•™ìŠµì„ í†µí•´ ê¹Šì´ ë¶ˆì—°ì†ì„±ì´ë‚˜ íƒ€ê²Ÿ ì´ë¯¸ì§€ê°€ ì†ŒìŠ¤ ì´ë¯¸ì§€ì˜ ê°•í•œ ì¤Œì¸ì¼ ë•Œ ë” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤.
- 5. BEAMERëŠ” ë‹¤ì¤‘ ê°€ì„¤ì„ í¬ë¡œìŠ¤-ì–´í…ì…˜ ë ˆì´ì–´ì— í†µí•©í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-24 16:33:12*