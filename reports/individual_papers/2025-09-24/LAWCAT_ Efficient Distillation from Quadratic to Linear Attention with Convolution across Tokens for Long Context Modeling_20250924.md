<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:50:10.496921",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Attention Mechanism",
    "LAWCAT",
    "Convolutional Layers",
    "Mistral-7B",
    "Llama3.2-1B"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Attention Mechanism": 0.78,
    "LAWCAT": 0.8,
    "Convolutional Layers": 0.7,
    "Mistral-7B": 0.75,
    "Llama3.2-1B": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Linear Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Linearized Attention"
        ],
        "category": "specific_connectable",
        "rationale": "Links to the broader concept of attention mechanisms, crucial for understanding the paper's contribution.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "LAWCAT",
        "canonical": "LAWCAT",
        "aliases": [
          "Linear Attention with Convolution Across Time"
        ],
        "category": "unique_technical",
        "rationale": "Represents the novel framework introduced in the paper, essential for understanding its unique contribution.",
        "novelty_score": 0.9,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "Conv1D layers",
        "canonical": "Convolutional Layers",
        "aliases": [
          "1D Convolutional Layers"
        ],
        "category": "broad_technical",
        "rationale": "Connects to the well-known concept of convolutional layers, relevant for understanding the model architecture.",
        "novelty_score": 0.4,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      },
      {
        "surface": "Mistral-7B",
        "canonical": "Mistral-7B",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Specific model variant used in the study, important for contextualizing results.",
        "novelty_score": 0.8,
        "connectivity_score": 0.5,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Llama3.2-1B",
        "canonical": "Llama3.2-1B",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Another specific model variant, highlighting the paper's experimental scope.",
        "novelty_score": 0.82,
        "connectivity_score": 0.52,
        "specificity_score": 0.82,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "long-context applications",
      "pre-training tokens"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Linear Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "LAWCAT",
      "resolved_canonical": "LAWCAT",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Conv1D layers",
      "resolved_canonical": "Convolutional Layers",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Mistral-7B",
      "resolved_canonical": "Mistral-7B",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.5,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Llama3.2-1B",
      "resolved_canonical": "Llama3.2-1B",
      "decision": "linked",
      "scores": {
        "novelty": 0.82,
        "connectivity": 0.52,
        "specificity": 0.82,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18467.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18467](https://arxiv.org/abs/2509.18467)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Language Modeling with Learned Meta-Tokens_20250923|Language Modeling with Learned Meta-Tokens]] (82.5% similar)
- [[2025-09-23/EG-MLA_ Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs_20250923|EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs]] (81.7% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar)
- [[2025-09-23/TACO_ Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration_20250923|TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration]] (81.6% similar)
- [[2025-09-23/Scaling Efficient LLMs_20250923|Scaling Efficient LLMs]] (81.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Convolutional Layers|Convolutional Layers]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/LAWCAT|LAWCAT]], [[keywords/Mistral-7B|Mistral-7B]], [[keywords/Llama3.2-1B|Llama3.2-1B]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18467v1 Announce Type: cross 
Abstract: Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications. While recent linear-complexity alternatives are increasingly powerful, effectively training them from scratch is still resource-intensive. To overcome these limitations, we propose LAWCAT (Linear Attention with Convolution Across Time), a novel linearization framework designed to efficiently transfer the capabilities of pre-trained transformers into a performant linear attention architecture. LAWCAT integrates causal Conv1D layers to enhance local dependency modeling and employs normalized gated linear attention to improve generalization across varying context lengths. Our comprehensive evaluations demonstrate that, distilling Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval accuracy up to 22K tokens, significantly extending its effective context window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark (QA2\&amp;QA3, 0K-16K context length), requiring less than 0.1\% pre-training tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT thus provides an efficient pathway to high-performance, long-context linear models suitable for edge deployment, reducing reliance on extensive long-sequence training data and computational resources.

## ğŸ“ ìš”ì•½

LAWCATì€ ê¸´ ë¬¸ë§¥ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°œìƒí•˜ëŠ” ë³€í™˜ê¸° ì•„í‚¤í…ì²˜ì˜ ê³„ì‚° ë³µì¡ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ìƒˆë¡œìš´ ì„ í˜•í™” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‚¬ì „ í›ˆë ¨ëœ ë³€í™˜ê¸°ì˜ ëŠ¥ë ¥ì„ ì„ í˜• ì£¼ì˜ ì•„í‚¤í…ì²˜ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì „ì´ì‹œí‚¤ë©°, Conv1D ë ˆì´ì–´ë¥¼ í†µí•©í•˜ì—¬ ì§€ì—­ ì˜ì¡´ì„± ëª¨ë¸ë§ì„ ê°•í™”í•˜ê³ , ì •ê·œí™”ëœ ê²Œì´íŠ¸ ì„ í˜• ì£¼ì˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë¬¸ë§¥ ê¸¸ì´ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. Mistral-7Bë¥¼ 1K ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¡œ ì¦ë¥˜í•˜ë©´ 22K í† í°ê¹Œì§€ 90% ì´ìƒì˜ íŒ¨ìŠ¤í‚¤ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ê³ , Llama3.2-1B LAWCAT ë³€í˜•ì€ S-NIAH ë° BABILong ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, 8K í† í°ì„ ì´ˆê³¼í•˜ëŠ” ì‹œí€€ìŠ¤ì—ì„œ FlashAttention-2ë³´ë‹¤ ë¹ ë¥¸ ì†ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. LAWCATì€ ê¸´ ë¬¸ë§¥ ì„ í˜• ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ê²½ë¡œë¥¼ ì œê³µí•˜ì—¬ ì—£ì§€ ë°°í¬ì— ì í•©í•˜ë©°, ê¸´ ì‹œí€€ìŠ¤ í›ˆë ¨ ë°ì´í„°ì™€ ê³„ì‚° ìì›ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì¤„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. LAWCATì€ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëŠ¥ë ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì„ í˜• ì£¼ì˜ ì•„í‚¤í…ì²˜ë¡œ ì „ì´ì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì„ í˜•í™” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. LAWCATì€ ì¸ê³¼ì  Conv1D ë ˆì´ì–´ë¥¼ í†µí•©í•˜ì—¬ ì§€ì—­ì  ì˜ì¡´ì„± ëª¨ë¸ë§ì„ ê°•í™”í•˜ê³ , ì •ê·œí™”ëœ ê²Œì´íŠ¸ ì„ í˜• ì£¼ì˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë¬¸ë§¥ ê¸¸ì´ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
- 3. Mistral-7Bë¥¼ 1K ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¡œ ì¦ë¥˜í•œ ê²°ê³¼, 22K í† í°ê¹Œì§€ 90% ì´ìƒì˜ íŒ¨ìŠ¤í‚¤ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
- 4. LAWCATì€ 8K ì´ìƒì˜ ì‹œí€€ìŠ¤ì— ëŒ€í•´ FlashAttention-2ë³´ë‹¤ ë¹ ë¥¸ í”„ë¦¬í•„ ì†ë„ë¥¼ ë³´ì´ë©°, ì—£ì§€ ë°°í¬ì— ì í•©í•œ ê³ ì„±ëŠ¥ ì¥ê¸° ë¬¸ë§¥ ì„ í˜• ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.
- 5. Llama3.2-1B LAWCAT ë³€í˜•ì€ S-NIAH ë° BABILong ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ëŒ€ë¹„ 0.1% ë¯¸ë§Œì˜ ì‚¬ì „ í•™ìŠµ í† í°ë§Œì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:50:10*