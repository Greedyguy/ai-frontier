<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:57:33.753405",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Flow Marching",
    "Physics-Pretrained Variational Autoencoder",
    "Transformer",
    "Few-Shot Learning",
    "Kolmogorov Turbulence"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Flow Marching": 0.78,
    "Physics-Pretrained Variational Autoencoder": 0.77,
    "Transformer": 0.8,
    "Few-Shot Learning": 0.75,
    "Kolmogorov Turbulence": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Flow Marching",
        "canonical": "Flow Marching",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Flow Marching is a novel algorithm introduced in the paper, providing a unique approach to neural operator learning and flow matching.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Physics-Pretrained Variational Autoencoder",
        "canonical": "Physics-Pretrained Variational Autoencoder",
        "aliases": [
          "P2VAE"
        ],
        "category": "unique_technical",
        "rationale": "P2VAE is a specialized model introduced in the paper, offering a new method for embedding physical states, which is key for linking related research.",
        "novelty_score": 0.78,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model architecture discussed in the paper, relevant for linking with a wide range of machine learning research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      },
      {
        "surface": "Few-Shot Adaptation",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Adaptation"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is highlighted in the paper as a method for adapting models to new tasks with minimal data, connecting to ongoing research in efficient learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Kolmogorov Turbulence",
        "canonical": "Kolmogorov Turbulence",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Kolmogorov Turbulence is a specific application domain mentioned in the paper, providing a unique link to research in fluid dynamics and turbulence modeling.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Flow Marching",
      "resolved_canonical": "Flow Marching",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Physics-Pretrained Variational Autoencoder",
      "resolved_canonical": "Physics-Pretrained Variational Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Few-Shot Adaptation",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Kolmogorov Turbulence",
      "resolved_canonical": "Kolmogorov Turbulence",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Flow marching for a generative PDE foundation model

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18611.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18611](https://arxiv.org/abs/2509.18611)

## 🔗 유사한 논문
- [[2025-09-24/HazeFlow_ Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing_20250924|HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing]] (84.9% similar)
- [[2025-09-17/Towards a Physics Foundation Model_20250917|Towards a Physics Foundation Model]] (83.1% similar)
- [[2025-09-23/Equilibrium flow_ From Snapshots to Dynamics_20250923|Equilibrium flow: From Snapshots to Dynamics]] (82.7% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (82.1% similar)
- [[2025-09-19/FlowDrive_ Energy Flow Field for End-to-End Autonomous Driving_20250919|FlowDrive: Energy Flow Field for End-to-End Autonomous Driving]] (82.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**⚡ Unique Technical**: [[keywords/Flow Marching|Flow Marching]], [[keywords/Physics-Pretrained Variational Autoencoder|Physics-Pretrained Variational Autoencoder]], [[keywords/Kolmogorov Turbulence|Kolmogorov Turbulence]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18611v1 Announce Type: cross 
Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal trajectories has recently shown promise for building generalizable models of dynamical systems. Yet most existing PDE foundation models rely on deterministic Transformer architectures, which lack generative flexibility for many science and engineering applications. We propose Flow Marching, an algorithm that bridges neural operator learning with flow matching motivated by an analysis of error accumulation in physical dynamical systems, and we build a generative PDE foundation model on top of it. By jointly sampling the noise level and the physical time step between adjacent states, the model learns a unified velocity field that transports a noisy current state toward its clean successor, reducing long-term rollout drift while enabling uncertainty-aware ensemble generations. Alongside this core algorithm, we introduce a Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states into a compact latent space, and an efficient Flow Marching Transformer (FMT) that combines a diffusion-forcing scheme with latent temporal pyramids, achieving up to 15x greater computational efficiency than full-length video diffusion models and thereby enabling large-scale pretraining at substantially reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE families and train suites of P2VAEs and FMTs at multiple scales. On downstream evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot adaptation, demonstrate long-term rollout stability over deterministic counterparts, and present uncertainty-stratified ensemble results, highlighting the importance of generative PDE foundation models for real-world applications.

## 📝 요약

이 논문에서는 대규모 편미분 방정식(PDE) 기반 시공간 궤적을 사전 학습하여 동적 시스템의 일반화 가능한 모델을 구축하는 방법을 제안합니다. 기존의 결정론적 트랜스포머 모델의 한계를 극복하기 위해, 저자들은 Flow Marching 알고리즘을 개발하여 신경 연산자 학습과 흐름 매칭을 결합했습니다. 이 알고리즘은 물리적 상태를 압축된 잠재 공간에 임베딩하는 Physics-Pretrained Variational Autoencoder (P2VAE)와 결합하여, 효율적인 Flow Marching Transformer (FMT)를 통해 최대 15배의 계산 효율성을 달성합니다. 12개의 서로 다른 PDE 계열에서 250만 개의 궤적을 사용하여 모델을 학습한 결과, 새로운 Kolmogorov 난류에 대한 적응과 장기적인 안정성을 보여주며, 불확실성을 고려한 앙상블 결과를 제시합니다. 이 연구는 실세계 응용을 위한 생성적 PDE 기반 모델의 중요성을 강조합니다.

## 🎯 주요 포인트

- 1. Flow Marching 알고리즘은 물리적 동적 시스템의 오류 누적 분석을 통해 신경 연산자 학습과 흐름 매칭을 연결하여 생성적 PDE 기반 모델을 구축합니다.
- 2. 모델은 인접 상태 간의 물리적 시간 단계와 노이즈 수준을 공동 샘플링하여 장기 롤아웃 드리프트를 줄이고 불확실성 인식 앙상블 생성을 가능하게 합니다.
- 3. Physics-Pretrained Variational Autoencoder (P2VAE)는 물리적 상태를 압축된 잠재 공간에 임베딩하며, Flow Marching Transformer (FMT)는 확산 강제 스킴과 잠재 시간 피라미드를 결합하여 최대 15배의 계산 효율성을 달성합니다.
- 4. 약 250만 개의 궤적을 포함하는 12개의 서로 다른 PDE 계열의 코퍼스를 구성하고, 여러 규모에서 P2VAE와 FMT를 훈련하여 대규모 사전 훈련을 저비용으로 실현합니다.
- 5. 미지의 Kolmogorov 난류에 대한 소수 샷 적응, 장기 롤아웃 안정성, 불확실성 계층화된 앙상블 결과를 통해 생성적 PDE 기반 모델의 중요성을 강조합니다.


---

*Generated on 2025-09-24 13:57:33*