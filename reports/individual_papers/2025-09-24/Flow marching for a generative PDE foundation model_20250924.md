<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:57:33.753405",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Flow Marching",
    "Physics-Pretrained Variational Autoencoder",
    "Transformer",
    "Few-Shot Learning",
    "Kolmogorov Turbulence"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Flow Marching": 0.78,
    "Physics-Pretrained Variational Autoencoder": 0.77,
    "Transformer": 0.8,
    "Few-Shot Learning": 0.75,
    "Kolmogorov Turbulence": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Flow Marching",
        "canonical": "Flow Marching",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Flow Marching is a novel algorithm introduced in the paper, providing a unique approach to neural operator learning and flow matching.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.78
      },
      {
        "surface": "Physics-Pretrained Variational Autoencoder",
        "canonical": "Physics-Pretrained Variational Autoencoder",
        "aliases": [
          "P2VAE"
        ],
        "category": "unique_technical",
        "rationale": "P2VAE is a specialized model introduced in the paper, offering a new method for embedding physical states, which is key for linking related research.",
        "novelty_score": 0.78,
        "connectivity_score": 0.72,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Transformer",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "Transformers are a foundational model architecture discussed in the paper, relevant for linking with a wide range of machine learning research.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.5,
        "link_intent_score": 0.8
      },
      {
        "surface": "Few-Shot Adaptation",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot Adaptation"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is highlighted in the paper as a method for adapting models to new tasks with minimal data, connecting to ongoing research in efficient learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.75
      },
      {
        "surface": "Kolmogorov Turbulence",
        "canonical": "Kolmogorov Turbulence",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Kolmogorov Turbulence is a specific application domain mentioned in the paper, providing a unique link to research in fluid dynamics and turbulence modeling.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Flow Marching",
      "resolved_canonical": "Flow Marching",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Physics-Pretrained Variational Autoencoder",
      "resolved_canonical": "Physics-Pretrained Variational Autoencoder",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.72,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.5,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Few-Shot Adaptation",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Kolmogorov Turbulence",
      "resolved_canonical": "Kolmogorov Turbulence",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Flow marching for a generative PDE foundation model

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18611.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18611](https://arxiv.org/abs/2509.18611)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/HazeFlow_ Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing_20250924|HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing]] (84.9% similar)
- [[2025-09-17/Towards a Physics Foundation Model_20250917|Towards a Physics Foundation Model]] (83.1% similar)
- [[2025-09-23/Equilibrium flow_ From Snapshots to Dynamics_20250923|Equilibrium flow: From Snapshots to Dynamics]] (82.7% similar)
- [[2025-09-22/StFT_ Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction_20250922|StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction]] (82.1% similar)
- [[2025-09-19/FlowDrive_ Energy Flow Field for End-to-End Autonomous Driving_20250919|FlowDrive: Energy Flow Field for End-to-End Autonomous Driving]] (82.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Flow Marching|Flow Marching]], [[keywords/Physics-Pretrained Variational Autoencoder|Physics-Pretrained Variational Autoencoder]], [[keywords/Kolmogorov Turbulence|Kolmogorov Turbulence]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18611v1 Announce Type: cross 
Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal trajectories has recently shown promise for building generalizable models of dynamical systems. Yet most existing PDE foundation models rely on deterministic Transformer architectures, which lack generative flexibility for many science and engineering applications. We propose Flow Marching, an algorithm that bridges neural operator learning with flow matching motivated by an analysis of error accumulation in physical dynamical systems, and we build a generative PDE foundation model on top of it. By jointly sampling the noise level and the physical time step between adjacent states, the model learns a unified velocity field that transports a noisy current state toward its clean successor, reducing long-term rollout drift while enabling uncertainty-aware ensemble generations. Alongside this core algorithm, we introduce a Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states into a compact latent space, and an efficient Flow Marching Transformer (FMT) that combines a diffusion-forcing scheme with latent temporal pyramids, achieving up to 15x greater computational efficiency than full-length video diffusion models and thereby enabling large-scale pretraining at substantially reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE families and train suites of P2VAEs and FMTs at multiple scales. On downstream evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot adaptation, demonstrate long-term rollout stability over deterministic counterparts, and present uncertainty-stratified ensemble results, highlighting the importance of generative PDE foundation models for real-world applications.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€ê·œëª¨ í¸ë¯¸ë¶„ ë°©ì •ì‹(PDE) ê¸°ë°˜ ì‹œê³µê°„ ê¶¤ì ì„ ì‚¬ì „ í•™ìŠµí•˜ì—¬ ë™ì  ì‹œìŠ¤í…œì˜ ì¼ë°˜í™” ê°€ëŠ¥í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê²°ì •ë¡ ì  íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ Flow Marching ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬ ì‹ ê²½ ì—°ì‚°ì í•™ìŠµê³¼ íë¦„ ë§¤ì¹­ì„ ê²°í•©í–ˆìŠµë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë¬¼ë¦¬ì  ìƒíƒœë¥¼ ì••ì¶•ëœ ì ì¬ ê³µê°„ì— ì„ë² ë”©í•˜ëŠ” Physics-Pretrained Variational Autoencoder (P2VAE)ì™€ ê²°í•©í•˜ì—¬, íš¨ìœ¨ì ì¸ Flow Marching Transformer (FMT)ë¥¼ í†µí•´ ìµœëŒ€ 15ë°°ì˜ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. 12ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ PDE ê³„ì—´ì—ì„œ 250ë§Œ ê°œì˜ ê¶¤ì ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œ ê²°ê³¼, ìƒˆë¡œìš´ Kolmogorov ë‚œë¥˜ì— ëŒ€í•œ ì ì‘ê³¼ ì¥ê¸°ì ì¸ ì•ˆì •ì„±ì„ ë³´ì—¬ì£¼ë©°, ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ ì•™ìƒë¸” ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì‹¤ì„¸ê³„ ì‘ìš©ì„ ìœ„í•œ ìƒì„±ì  PDE ê¸°ë°˜ ëª¨ë¸ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Flow Marching ì•Œê³ ë¦¬ì¦˜ì€ ë¬¼ë¦¬ì  ë™ì  ì‹œìŠ¤í…œì˜ ì˜¤ë¥˜ ëˆ„ì  ë¶„ì„ì„ í†µí•´ ì‹ ê²½ ì—°ì‚°ì í•™ìŠµê³¼ íë¦„ ë§¤ì¹­ì„ ì—°ê²°í•˜ì—¬ ìƒì„±ì  PDE ê¸°ë°˜ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
- 2. ëª¨ë¸ì€ ì¸ì ‘ ìƒíƒœ ê°„ì˜ ë¬¼ë¦¬ì  ì‹œê°„ ë‹¨ê³„ì™€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì„ ê³µë™ ìƒ˜í”Œë§í•˜ì—¬ ì¥ê¸° ë¡¤ì•„ì›ƒ ë“œë¦¬í”„íŠ¸ë¥¼ ì¤„ì´ê³  ë¶ˆí™•ì‹¤ì„± ì¸ì‹ ì•™ìƒë¸” ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
- 3. Physics-Pretrained Variational Autoencoder (P2VAE)ëŠ” ë¬¼ë¦¬ì  ìƒíƒœë¥¼ ì••ì¶•ëœ ì ì¬ ê³µê°„ì— ì„ë² ë”©í•˜ë©°, Flow Marching Transformer (FMT)ëŠ” í™•ì‚° ê°•ì œ ìŠ¤í‚´ê³¼ ì ì¬ ì‹œê°„ í”¼ë¼ë¯¸ë“œë¥¼ ê²°í•©í•˜ì—¬ ìµœëŒ€ 15ë°°ì˜ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- 4. ì•½ 250ë§Œ ê°œì˜ ê¶¤ì ì„ í¬í•¨í•˜ëŠ” 12ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ PDE ê³„ì—´ì˜ ì½”í¼ìŠ¤ë¥¼ êµ¬ì„±í•˜ê³ , ì—¬ëŸ¬ ê·œëª¨ì—ì„œ P2VAEì™€ FMTë¥¼ í›ˆë ¨í•˜ì—¬ ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ì„ ì €ë¹„ìš©ìœ¼ë¡œ ì‹¤í˜„í•©ë‹ˆë‹¤.
- 5. ë¯¸ì§€ì˜ Kolmogorov ë‚œë¥˜ì— ëŒ€í•œ ì†Œìˆ˜ ìƒ· ì ì‘, ì¥ê¸° ë¡¤ì•„ì›ƒ ì•ˆì •ì„±, ë¶ˆí™•ì‹¤ì„± ê³„ì¸µí™”ëœ ì•™ìƒë¸” ê²°ê³¼ë¥¼ í†µí•´ ìƒì„±ì  PDE ê¸°ë°˜ ëª¨ë¸ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:57:33*