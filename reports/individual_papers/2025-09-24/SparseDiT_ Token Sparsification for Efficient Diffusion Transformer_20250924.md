<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:26:57.274162",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Diffusion Transformer",
    "Token Sparsification",
    "Attention Mechanism",
    "Generative Quality",
    "Sampling Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Diffusion Transformer": 0.8,
    "Token Sparsification": 0.78,
    "Attention Mechanism": 0.82,
    "Generative Quality": 0.7,
    "Sampling Optimization": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Diffusion Transformers",
        "canonical": "Diffusion Transformer",
        "aliases": [
          "DiT"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific architecture discussed in the paper, crucial for understanding the context of SparseDiT.",
        "novelty_score": 0.85,
        "connectivity_score": 0.7,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Token Sparsification",
        "canonical": "Token Sparsification",
        "aliases": [
          "Sparse Tokens"
        ],
        "category": "unique_technical",
        "rationale": "A novel technique introduced in the paper to improve computational efficiency in transformers.",
        "novelty_score": 0.88,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Self-Attention"
        ],
        "category": "specific_connectable",
        "rationale": "A fundamental component in transformers, relevant for understanding architectural inefficiencies.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Generative Quality",
        "canonical": "Generative Quality",
        "aliases": [
          "Generative Performance"
        ],
        "category": "unique_technical",
        "rationale": "Key aspect of the paper's focus on maintaining quality while improving efficiency.",
        "novelty_score": 0.65,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.7
      },
      {
        "surface": "Sampling Optimization",
        "canonical": "Sampling Optimization",
        "aliases": [
          "Sampling Efficiency"
        ],
        "category": "specific_connectable",
        "rationale": "Important for linking to broader discussions on improving inference speed in generative models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Diffusion Transformers",
      "resolved_canonical": "Diffusion Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.7,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Token Sparsification",
      "resolved_canonical": "Token Sparsification",
      "decision": "linked",
      "scores": {
        "novelty": 0.88,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Generative Quality",
      "resolved_canonical": "Generative Quality",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Sampling Optimization",
      "resolved_canonical": "Sampling Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# SparseDiT: Token Sparsification for Efficient Diffusion Transformer

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2412.06028.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2412.06028](https://arxiv.org/abs/2412.06028)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/DiCo_ Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling_20250923|DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling]] (88.2% similar)
- [[2025-09-24/Foresight_ Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation_20250924|Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation]] (87.6% similar)
- [[2025-09-17/BWCache_ Accelerating Video Diffusion Transformers through Block-Wise Caching_20250917|BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching]] (86.6% similar)
- [[2025-09-23/LRQ-DiT_ Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation_20250923|LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation]] (86.6% similar)
- [[2025-09-22/LowDiff_ Efficient Diffusion Sampling with Low-Resolution Condition_20250922|LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition]] (86.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Sampling Optimization|Sampling Optimization]]
**âš¡ Unique Technical**: [[keywords/Diffusion Transformer|Diffusion Transformer]], [[keywords/Token Sparsification|Token Sparsification]], [[keywords/Generative Quality|Generative Quality]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2412.06028v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiT) are renowned for their impressive generative performance; however, they are significantly constrained by considerable computational costs due to the quadratic complexity in self-attention and the extensive sampling steps required. While advancements have been made in expediting the sampling process, the underlying architectural inefficiencies within DiT remain underexplored. We introduce SparseDiT, a novel framework that implements token sparsification across spatial and temporal dimensions to enhance computational efficiency while preserving generative quality. Spatially, SparseDiT employs a tri-segment architecture that allocates token density based on feature requirements at each layer: Poolingformer in the bottom layers for efficient global feature extraction, Sparse-Dense Token Modules (SDTM) in the middle layers to balance global context with local detail, and dense tokens in the top layers to refine high-frequency details. Temporally, SparseDiT dynamically modulates token density across denoising stages, progressively increasing token count as finer details emerge in later timesteps. This synergy between SparseDiT spatially adaptive architecture and its temporal pruning strategy enables a unified framework that balances efficiency and fidelity throughout the generation process. Our experiments demonstrate SparseDiT effectiveness, achieving a 55% reduction in FLOPs and a 175% improvement in inference speed on DiT-XL with similar FID score on 512x512 ImageNet, a 56% reduction in FLOPs across video generation datasets, and a 69% improvement in inference speed on PixArt-$\alpha$ on text-to-image generation task with a 0.24 FID score decrease. SparseDiT provides a scalable solution for high-quality diffusion-based generation compatible with sampling optimization techniques.

## ğŸ“ ìš”ì•½

SparseDiTëŠ” Diffusion Transformers(DiT)ì˜ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. DiTëŠ” ë›°ì–´ë‚œ ìƒì„± ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìê°€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë³µì¡ì„±ê³¼ ìƒ˜í”Œë§ ë‹¨ê³„ë¡œ ì¸í•´ ë†’ì€ ê³„ì‚° ë¹„ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤. SparseDiTëŠ” ê³µê°„ì  ë° ì‹œê°„ì  ì°¨ì›ì—ì„œ í† í° í¬ì†Œí™”ë¥¼ êµ¬í˜„í•˜ì—¬ ì´ëŸ¬í•œ ë¹„íš¨ìœ¨ì„±ì„ í•´ê²°í•©ë‹ˆë‹¤. ê³µê°„ì ìœ¼ë¡œëŠ” ì„¸ ê°€ì§€ ì„¸ê·¸ë¨¼íŠ¸ êµ¬ì¡°ë¥¼ í†µí•´ ê° ì¸µì˜ íŠ¹ì§• ìš”êµ¬ì— ë”°ë¼ í† í° ë°€ë„ë¥¼ ì¡°ì ˆí•˜ë©°, ì‹œê°„ì ìœ¼ë¡œëŠ” ë””ë…¸ì´ì§• ë‹¨ê³„ì— ë”°ë¼ í† í° ë°€ë„ë¥¼ ë™ì ìœ¼ë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SparseDiTëŠ” DiT-XLì—ì„œ FLOPsë¥¼ 55% ê°ì†Œì‹œí‚¤ê³  ì¶”ë¡  ì†ë„ë¥¼ 175% í–¥ìƒì‹œí‚¤ë©´ì„œë„ ìœ ì‚¬í•œ FID ì ìˆ˜ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë¹„ë””ì˜¤ ìƒì„± ë°ì´í„°ì…‹ì—ì„œ FLOPsë¥¼ 56% ì¤„ì´ê³ , í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ì‘ì—…ì—ì„œ ì¶”ë¡  ì†ë„ë¥¼ 69% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. SparseDiTëŠ” ìƒ˜í”Œë§ ìµœì í™” ê¸°ìˆ ê³¼ í˜¸í™˜ë˜ëŠ” ê³ í’ˆì§ˆ ìƒì„±ì˜ í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. SparseDiTëŠ” ê³µê°„ì  ë° ì‹œê°„ì  ì°¨ì›ì—ì„œ í† í° í¬ì†Œí™”ë¥¼ êµ¬í˜„í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë†’ì´ë©´ì„œ ìƒì„± í’ˆì§ˆì„ ìœ ì§€í•©ë‹ˆë‹¤.
- 2. ê³µê°„ì ìœ¼ë¡œ SparseDiTëŠ” ê° ì¸µì˜ ê¸°ëŠ¥ ìš”êµ¬ì— ë”°ë¼ í† í° ë°€ë„ë¥¼ í• ë‹¹í•˜ëŠ” ì‚¼ë¶„í•  ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 3. ì‹œê°„ì ìœ¼ë¡œ SparseDiTëŠ” ë””ë…¸ì´ì§• ë‹¨ê³„ì— ë”°ë¼ í† í° ë°€ë„ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ì—¬ ì„¸ë¶€ ì‚¬í•­ì´ ë‚˜íƒ€ë‚ ìˆ˜ë¡ í† í° ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
- 4. SparseDiTëŠ” DiT-XLì—ì„œ FLOPsë¥¼ 55% ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ 175% í–¥ìƒì‹œí‚¤ë©´ì„œ ìœ ì‚¬í•œ FID ì ìˆ˜ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- 5. SparseDiTëŠ” ìƒ˜í”Œë§ ìµœì í™” ê¸°ìˆ ê³¼ í˜¸í™˜ë˜ëŠ” ê³ í’ˆì§ˆ í™•ì‚° ê¸°ë°˜ ìƒì„±ì˜ í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:26:57*