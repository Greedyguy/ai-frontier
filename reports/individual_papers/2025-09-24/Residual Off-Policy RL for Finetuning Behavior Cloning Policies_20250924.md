<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:19:41.817611",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Behavior Cloning",
    "Reinforcement Learning",
    "Residual Learning",
    "Off-Policy Reinforcement Learning",
    "High-Degree-of-Freedom Systems"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Behavior Cloning": 0.82,
    "Reinforcement Learning": 0.85,
    "Residual Learning": 0.78,
    "Off-Policy Reinforcement Learning": 0.8,
    "High-Degree-of-Freedom Systems": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Behavior Cloning",
        "canonical": "Behavior Cloning",
        "aliases": [
          "BC"
        ],
        "category": "specific_connectable",
        "rationale": "Behavior Cloning is a key component of the proposed method, linking it to imitation learning techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is central to the paper's methodology, connecting it to a broad range of machine learning applications.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Residual Learning",
        "canonical": "Residual Learning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Residual Learning is a novel approach in the context of combining BC and RL, enhancing the paper's unique contribution.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Off-Policy RL",
        "canonical": "Off-Policy Reinforcement Learning",
        "aliases": [
          "Off-Policy RL"
        ],
        "category": "specific_connectable",
        "rationale": "Off-Policy RL is crucial for the sample-efficient learning aspect, linking to advanced RL techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "High-Degree-of-Freedom Systems",
        "canonical": "High-Degree-of-Freedom Systems",
        "aliases": [
          "High-DoF Systems"
        ],
        "category": "unique_technical",
        "rationale": "The focus on High-Degree-of-Freedom Systems highlights the paper's applicability to complex robotic systems.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "performance",
      "demonstration",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Behavior Cloning",
      "resolved_canonical": "Behavior Cloning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Residual Learning",
      "resolved_canonical": "Residual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Off-Policy RL",
      "resolved_canonical": "Off-Policy Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "High-Degree-of-Freedom Systems",
      "resolved_canonical": "High-Degree-of-Freedom Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Residual Off-Policy RL for Finetuning Behavior Cloning Policies

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19301.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19301](https://arxiv.org/abs/2509.19301)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/World4RL_ Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation_20250924|World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation]] (86.4% similar)
- [[2025-09-23/Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation_20250923|Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation]] (86.2% similar)
- [[2025-09-18/Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (86.1% similar)
- [[2025-09-23/Latent Policy Steering with Embodiment-Agnostic Pretrained World Models_20250923|Latent Policy Steering with Embodiment-Agnostic Pretrained World Models]] (85.5% similar)
- [[2025-09-24/Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training_20250924|Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Behavior Cloning|Behavior Cloning]], [[keywords/Off-Policy Reinforcement Learning|Off-Policy Reinforcement Learning]]
**âš¡ Unique Technical**: [[keywords/Residual Learning|Residual Learning]], [[keywords/High-Degree-of-Freedom Systems|High-Degree-of-Freedom Systems]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19301v1 Announce Type: cross 
Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems. We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world. Project website: https://residual-offpolicy-rl.github.io

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í–‰ë™ ë³µì œ(BC)ì™€ ê°•í™” í•™ìŠµ(RL)ì„ ê²°í•©í•œ ì”ì°¨ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. BC ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ë²¼ìš´ ì”ì°¨ ë³´ì •ì„ ì˜¤í”„ë¼ì¸ RLë¡œ í•™ìŠµí•˜ì—¬ ë°ì´í„° íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í¬ì†Œí•œ ì´ì§„ ë³´ìƒ ì‹ í˜¸ë§Œìœ¼ë¡œë„ ë†’ì€ ììœ ë„ë¥¼ ê°€ì§„ ì‹œìŠ¤í…œì˜ ì¡°ì‘ ì •ì±…ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì¸ê°„í˜• ë¡œë´‡ì—ì„œì˜ ì„±ê³µì ì¸ RL í›ˆë ¨ì„ ìµœì´ˆë¡œ ì‹œì—°í•˜ì˜€ìœ¼ë©°, ë‹¤ì–‘í•œ ì‹œê° ê¸°ë°˜ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì‹¤ì œ í™˜ê²½ì—ì„œ RLì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì ì¸ ê²½ë¡œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í–‰ë™ ë³µì œ(BC)ì˜ ìµœê·¼ ë°œì „ì€ ì¸ìƒì ì¸ ì‹œê°-ìš´ë™ ì œì–´ ì •ì±…ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìœ¼ë‚˜, ì¸ê°„ ì‹œì—°ì˜ ì§ˆê³¼ ë°ì´í„° ìˆ˜ì§‘ì˜ ìˆ˜ì‘ì—…, ì˜¤í”„ë¼ì¸ ë°ì´í„° ì¦ê°€ì˜ í•œê³„ê°€ ë¬¸ì œë¡œ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.
- 2. ê°•í™” í•™ìŠµ(RL)ì€ í™˜ê²½ê³¼ì˜ ììœ¨ì  ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨ì‹œí‚¤ë©°, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ì‹¤ì œ ë¡œë´‡ì— ì§ì ‘ ì ìš©í•˜ê¸°ì—ëŠ” ìƒ˜í”Œ ë¹„íš¨ìœ¨ì„±, ì•ˆì „ ë¬¸ì œ, í¬ì†Œí•œ ë³´ìƒ í•™ìŠµì˜ ì–´ë ¤ì›€ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ëŠ” BCì™€ RLì˜ ì¥ì ì„ ê²°í•©í•œ ì”ì°¨ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ë©°, BC ì •ì±…ì„ ë¸”ë™ë°•ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ìƒ˜í”Œ íš¨ìœ¨ì ì¸ ì˜¤í”„í´ë¦¬ì‹œ RLì„ í†µí•´ ê²½ëŸ‰ì˜ ì”ì°¨ ë³´ì •ì„ í•™ìŠµí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ í¬ì†Œí•œ ì´ì§„ ë³´ìƒ ì‹ í˜¸ë§Œì„ í•„ìš”ë¡œ í•˜ë©°, ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ í™˜ê²½ ëª¨ë‘ì—ì„œ ê³ ììœ ë„ ì‹œìŠ¤í…œì˜ ì¡°ì‘ ì •ì±…ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5. íŠ¹íˆ, ì¸ê°„í˜• ë¡œë´‡ì˜ ì •êµí•œ ì†ì„ ì‚¬ìš©í•œ ìµœì´ˆì˜ ì„±ê³µì ì¸ ì‹¤ì œ RL í›ˆë ¨ì„ ì‹œì—°í•˜ì˜€ìœ¼ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ê¸°ë°˜ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  RLì˜ ì‹¤ì„¸ê³„ ë°°í¬ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì¸ ê²½ë¡œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:19:41*