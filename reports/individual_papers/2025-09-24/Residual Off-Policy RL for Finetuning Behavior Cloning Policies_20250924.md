<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:19:41.817611",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Behavior Cloning",
    "Reinforcement Learning",
    "Residual Learning",
    "Off-Policy Reinforcement Learning",
    "High-Degree-of-Freedom Systems"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Behavior Cloning": 0.82,
    "Reinforcement Learning": 0.85,
    "Residual Learning": 0.78,
    "Off-Policy Reinforcement Learning": 0.8,
    "High-Degree-of-Freedom Systems": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Behavior Cloning",
        "canonical": "Behavior Cloning",
        "aliases": [
          "BC"
        ],
        "category": "specific_connectable",
        "rationale": "Behavior Cloning is a key component of the proposed method, linking it to imitation learning techniques.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.72,
        "link_intent_score": 0.82
      },
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is central to the paper's methodology, connecting it to a broad range of machine learning applications.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Residual Learning",
        "canonical": "Residual Learning",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Residual Learning is a novel approach in the context of combining BC and RL, enhancing the paper's unique contribution.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Off-Policy RL",
        "canonical": "Off-Policy Reinforcement Learning",
        "aliases": [
          "Off-Policy RL"
        ],
        "category": "specific_connectable",
        "rationale": "Off-Policy RL is crucial for the sample-efficient learning aspect, linking to advanced RL techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.77,
        "link_intent_score": 0.8
      },
      {
        "surface": "High-Degree-of-Freedom Systems",
        "canonical": "High-Degree-of-Freedom Systems",
        "aliases": [
          "High-DoF Systems"
        ],
        "category": "unique_technical",
        "rationale": "The focus on High-Degree-of-Freedom Systems highlights the paper's applicability to complex robotic systems.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "approach",
      "performance",
      "demonstration",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Behavior Cloning",
      "resolved_canonical": "Behavior Cloning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.72,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Residual Learning",
      "resolved_canonical": "Residual Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Off-Policy RL",
      "resolved_canonical": "Off-Policy Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.77,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "High-Degree-of-Freedom Systems",
      "resolved_canonical": "High-Degree-of-Freedom Systems",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Residual Off-Policy RL for Finetuning Behavior Cloning Policies

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19301.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19301](https://arxiv.org/abs/2509.19301)

## 🔗 유사한 논문
- [[2025-09-24/World4RL_ Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation_20250924|World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation]] (86.4% similar)
- [[2025-09-23/Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation_20250923|Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation]] (86.2% similar)
- [[2025-09-18/Self-Improving Embodied Foundation Models_20250918|Self-Improving Embodied Foundation Models]] (86.1% similar)
- [[2025-09-23/Latent Policy Steering with Embodiment-Agnostic Pretrained World Models_20250923|Latent Policy Steering with Embodiment-Agnostic Pretrained World Models]] (85.5% similar)
- [[2025-09-24/Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training_20250924|Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training]] (85.3% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Behavior Cloning|Behavior Cloning]], [[keywords/Off-Policy Reinforcement Learning|Off-Policy Reinforcement Learning]]
**⚡ Unique Technical**: [[keywords/Residual Learning|Residual Learning]], [[keywords/High-Degree-of-Freedom Systems|High-Degree-of-Freedom Systems]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19301v1 Announce Type: cross 
Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems. We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world. Project website: https://residual-offpolicy-rl.github.io

## 📝 요약

이 논문은 행동 복제(BC)와 강화 학습(RL)을 결합한 잔차 학습 프레임워크를 제안합니다. BC 정책을 기반으로 가벼운 잔차 보정을 오프라인 RL로 학습하여 데이터 효율성을 높입니다. 이 방법은 희소한 이진 보상 신호만으로도 높은 자유도를 가진 시스템의 조작 정책을 효과적으로 개선할 수 있습니다. 특히, 인간형 로봇에서의 성공적인 RL 훈련을 최초로 시연하였으며, 다양한 시각 기반 작업에서 최첨단 성능을 보여줍니다. 이 연구는 실제 환경에서 RL을 적용할 수 있는 실용적인 경로를 제시합니다.

## 🎯 주요 포인트

- 1. 행동 복제(BC)의 최근 발전은 인상적인 시각-운동 제어 정책을 가능하게 했으나, 인간 시연의 질과 데이터 수집의 수작업, 오프라인 데이터 증가의 한계가 문제로 남아있습니다.
- 2. 강화 학습(RL)은 환경과의 자율적 상호작용을 통해 에이전트를 훈련시키며, 다양한 분야에서 성공을 거두었지만, 실제 로봇에 직접 적용하기에는 샘플 비효율성, 안전 문제, 희소한 보상 학습의 어려움이 존재합니다.
- 3. 본 연구는 BC와 RL의 장점을 결합한 잔차 학습 프레임워크를 제안하며, BC 정책을 블랙박스 기반으로 사용하고, 샘플 효율적인 오프폴리시 RL을 통해 경량의 잔차 보정을 학습합니다.
- 4. 제안된 방법은 희소한 이진 보상 신호만을 필요로 하며, 시뮬레이션과 실제 환경 모두에서 고자유도 시스템의 조작 정책을 효과적으로 개선할 수 있습니다.
- 5. 특히, 인간형 로봇의 정교한 손을 사용한 최초의 성공적인 실제 RL 훈련을 시연하였으며, 이는 다양한 비전 기반 작업에서 최첨단 성능을 보여주고 RL의 실세계 배포를 위한 실용적인 경로를 제시합니다.


---

*Generated on 2025-09-24 15:19:41*