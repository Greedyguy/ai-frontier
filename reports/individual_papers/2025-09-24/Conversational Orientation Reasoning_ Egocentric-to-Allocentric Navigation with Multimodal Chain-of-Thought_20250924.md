<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:42:54.906958",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Multimodal Chain-of-Thought",
    "Conversational Orientation Reasoning",
    "Egocentric-to-Allocentric Navigation",
    "Multimodal Learning",
    "Chain-of-Thought"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Multimodal Chain-of-Thought": 0.92,
    "Conversational Orientation Reasoning": 0.88,
    "Egocentric-to-Allocentric Navigation": 0.86,
    "Multimodal Learning": 0.8,
    "Chain-of-Thought": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Multimodal chain-of-thought",
        "canonical": "Multimodal Chain-of-Thought",
        "aliases": [
          "MCoT"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's methodology, combining multimodal inputs with chain-of-thought reasoning, and is not widely covered in existing vocabularies.",
        "novelty_score": 0.85,
        "connectivity_score": 0.72,
        "specificity_score": 0.88,
        "link_intent_score": 0.92
      },
      {
        "surface": "Conversational Orientation Reasoning",
        "canonical": "Conversational Orientation Reasoning",
        "aliases": [
          "COR"
        ],
        "category": "unique_technical",
        "rationale": "Introduces a new benchmark for spatial reasoning in conversational agents, which is a novel contribution of the paper.",
        "novelty_score": 0.9,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.88
      },
      {
        "surface": "Egocentric-to-Allocentric Navigation",
        "canonical": "Egocentric-to-Allocentric Navigation",
        "aliases": [
          "Egocentric Navigation",
          "Allocentric Navigation"
        ],
        "category": "unique_technical",
        "rationale": "Describes a specific navigation challenge addressed by the paper, linking spatial reasoning with conversational AI.",
        "novelty_score": 0.78,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.86
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "The paper's approach integrates multiple data modalities, which is a key aspect of the methodology.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.6,
        "link_intent_score": 0.8
      },
      {
        "surface": "Chain-of-Thought",
        "canonical": "Chain-of-Thought",
        "aliases": [
          "CoT"
        ],
        "category": "specific_connectable",
        "rationale": "Chain-of-thought reasoning is a significant concept in the paper, linking language and spatial reasoning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.78,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "Traditional Chinese",
      "ASR-transcribed",
      "Taiwan-LLM-13B-v2.0-Chat"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Multimodal chain-of-thought",
      "resolved_canonical": "Multimodal Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.72,
        "specificity": 0.88,
        "link_intent": 0.92
      }
    },
    {
      "candidate_surface": "Conversational Orientation Reasoning",
      "resolved_canonical": "Conversational Orientation Reasoning",
      "decision": "linked",
      "scores": {
        "novelty": 0.9,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Egocentric-to-Allocentric Navigation",
      "resolved_canonical": "Egocentric-to-Allocentric Navigation",
      "decision": "linked",
      "scores": {
        "novelty": 0.78,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.86
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.6,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Chain-of-Thought",
      "resolved_canonical": "Chain-of-Thought",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.78,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18200.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18200](https://arxiv.org/abs/2509.18200)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Audio-Reasoner_ Improving Reasoning Capability in Large Audio Language Models_20250923|Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models]] (83.1% similar)
- [[2025-09-22/Cache-of-Thought_ Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning_20250922|Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning]] (82.8% similar)
- [[2025-09-19/ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (82.8% similar)
- [[2025-09-23/WISE_ Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification_20250923|WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification]] (82.7% similar)
- [[2025-09-23/AuditoryBench++_ Can Language Models Understand Auditory Knowledge without Hearing?_20250923|AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]], [[keywords/Chain-of-Thought|Chain-of-Thought]]
**âš¡ Unique Technical**: [[keywords/Multimodal Chain-of-Thought|Multimodal Chain-of-Thought]], [[keywords/Conversational Orientation Reasoning|Conversational Orientation Reasoning]], [[keywords/Egocentric-to-Allocentric Navigation|Egocentric-to-Allocentric Navigation]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18200v1 Announce Type: cross 
Abstract: Conversational agents must translate egocentric utterances (e.g., "on my right") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ìì•„ ì¤‘ì‹¬ì  ë°œí™”ë¥¼ ì ˆëŒ€ì  ë°©í–¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëŒ€í™”í˜• ì—ì´ì „íŠ¸ì˜ ê³¼ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ Conversational Orientation Reasoning (COR)ì„ ì†Œê°œí•˜ë©°, ì´ëŠ” ë¹„ì˜ì–´ê¶Œ ë° ASR ì „ì‚¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìì•„ ì¤‘ì‹¬-ì ˆëŒ€ì  ë°©í–¥ ì¶”ë¡ ì„ í‰ê°€í•©ë‹ˆë‹¤. ì œì•ˆëœ ë‹¤ì¤‘ ëª¨ë‹¬ ì²´ì¸ ì˜¤ë¸Œ ì‚¬ê³ (MCoT) í”„ë ˆì„ì›Œí¬ëŠ” ASR ì „ì‚¬ ìŒì„±ê³¼ ëœë“œë§ˆí¬ ì¢Œí‘œë¥¼ í†µí•©í•˜ì—¬ ì„¸ ë‹¨ê³„ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤: ê³µê°„ ê´€ê³„ ì¶”ì¶œ, ì¢Œí‘œë¥¼ ì ˆëŒ€ ë°©í–¥ìœ¼ë¡œ ë§¤í•‘, ì‚¬ìš©ì ë°©í–¥ ì¶”ë¡ . ì´ ëª¨ë¸ì€ Taiwan-LLM-13B-v2.0-Chatì„ ê¸°ë°˜ìœ¼ë¡œ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì „ëµì„ í†µí•´ ê°œë°œë˜ì—ˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ MCoTëŠ” ê¹¨ë—í•œ ì „ì‚¬ì—ì„œ 100%, ASR ì „ì‚¬ì—ì„œ 98.1%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, MCoTëŠ” ì¡ìŒì´ ìˆëŠ” ëŒ€í™” ì¡°ê±´ì—ì„œë„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì–¸ì–´ì  ë³€ì´, ë„ë©”ì¸ ë³€í™”, ì°¸ì¡° ëª¨í˜¸ì„±ì— ëŒ€í•œ ë†’ì€ ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” MCoTì˜ êµ¬ì¡°í™”ëœ ê³µê°„ ì¶”ë¡ ì´ í•´ì„ ê°€ëŠ¥í•˜ê³  ìì› íš¨ìœ¨ì ì¸ ë‚´ë¹„ê²Œì´ì…˜ì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í™”í˜• ì—ì´ì „íŠ¸ëŠ” ìì•„ ì¤‘ì‹¬ ë°œí™”ë¥¼ ê°ê´€ì  ë°©í–¥ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•˜ë©°, ì´ëŠ” GPS ì‹ í˜¸ê°€ ì•½í•œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.
- 2. ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì¸ Conversational Orientation Reasoning (COR)ì€ ë¹„ì˜ì–´ê¶Œ ë° ASR ì „ì‚¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìì•„ ì¤‘ì‹¬ì—ì„œ ê°ê´€ì  ë°©í–¥ìœ¼ë¡œì˜ ì¶”ë¡ ì„ ë‹¤ë£¹ë‹ˆë‹¤.
- 3. MCoT í”„ë ˆì„ì›Œí¬ëŠ” ASR ì „ì‚¬ëœ ìŒì„±ê³¼ ëœë“œë§ˆí¬ ì¢Œí‘œë¥¼ í†µí•©í•˜ì—¬ êµ¬ì¡°í™”ëœ 3ë‹¨ê³„ ì¶”ë¡  ê³¼ì •ì„ í†µí•´ ì‚¬ìš©ì ë°©í–¥ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.
- 4. MCoTëŠ” ê¹¨ë—í•œ ì „ì‚¬ì—ì„œ 100%, ASR ì „ì‚¬ì—ì„œ 98.1%ì˜ ë°©í–¥ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ë‹¨ì¼ ëª¨ë‹¬ ë° ë¹„êµ¬ì¡°ì  ê¸°ì¤€ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- 5. MCoTëŠ” ì†ŒìŒì´ ìˆëŠ” ëŒ€í™” ì¡°ê±´ì—ì„œë„ ê°•ë ¥í•˜ë©°, ë‹¤êµ­ì–´ ì½”ë“œ ì „í™˜ ë° ì–¸ì–´ì  ë³€ì´ì— ëŒ€í•œ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:42:54*