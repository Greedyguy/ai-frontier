<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:00:30.923283",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Reinforcement Learning with Human Feedback",
    "Distributionally Robust Optimization",
    "Wasserstein Distributionally Robust Optimization",
    "Kullback-Leibler Distributionally Robust Optimization"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Reinforcement Learning with Human Feedback": 0.8,
    "Distributionally Robust Optimization": 0.78,
    "Wasserstein Distributionally Robust Optimization": 0.7,
    "Kullback-Leibler Distributionally Robust Optimization": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on aligning models with human intent.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Reinforcement Learning with Human Feedback",
        "canonical": "Reinforcement Learning with Human Feedback",
        "aliases": [
          "RLHF"
        ],
        "category": "unique_technical",
        "rationale": "Key method discussed for aligning LLMs with human intent.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.8
      },
      {
        "surface": "Distributionally Robust Optimization",
        "canonical": "Distributionally Robust Optimization",
        "aliases": [
          "DRO"
        ],
        "category": "specific_connectable",
        "rationale": "A foundational approach in the paper for improving model robustness.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Wasserstein-DPO",
        "canonical": "Wasserstein Distributionally Robust Optimization",
        "aliases": [
          "Wasserstein-DPO"
        ],
        "category": "unique_technical",
        "rationale": "Specific variant of DRO discussed with optimal parametric rates.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.7
      },
      {
        "surface": "KL-DPO",
        "canonical": "Kullback-Leibler Distributionally Robust Optimization",
        "aliases": [
          "KL-DPO"
        ],
        "category": "unique_technical",
        "rationale": "Another variant of DRO achieving optimal parametric rates.",
        "novelty_score": 0.68,
        "connectivity_score": 0.65,
        "specificity_score": 0.88,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Reinforcement Learning with Human Feedback",
      "resolved_canonical": "Reinforcement Learning with Human Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Distributionally Robust Optimization",
      "resolved_canonical": "Distributionally Robust Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Wasserstein-DPO",
      "resolved_canonical": "Wasserstein Distributionally Robust Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "KL-DPO",
      "resolved_canonical": "Kullback-Leibler Distributionally Robust Optimization",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.65,
        "specificity": 0.88,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19104.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19104](https://arxiv.org/abs/2509.19104)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (84.3% similar)
- [[2025-09-23/Re-Align_ Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization_20250923|Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization]] (83.7% similar)
- [[2025-09-19/FlowRL_ Matching Reward Distributions for LLM Reasoning_20250919|FlowRL: Matching Reward Distributions for LLM Reasoning]] (83.4% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (83.2% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (83.2% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Distributionally Robust Optimization|Distributionally Robust Optimization]]
**âš¡ Unique Technical**: [[keywords/Reinforcement Learning with Human Feedback|Reinforcement Learning with Human Feedback]], [[keywords/Wasserstein Distributionally Robust Optimization|Wasserstein Distributionally Robust Optimization]], [[keywords/Kullback-Leibler Distributionally Robust Optimization|Kullback-Leibler Distributionally Robust Optimization]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19104v1 Announce Type: new 
Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ê°•í™” í•™ìŠµ(RLHF)ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ DRO-REBELì´ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ RLHF ë°©ë²•ì€ ë³´ìƒ ì˜¤ë²„í”¼íŒ… ë¬¸ì œë¡œ ì¸í•´ ëª¨ë¸ì´ í›ˆë ¨ ì¤‘ ê´€ì°°ëœ ì„ í˜¸ í–‰ë™ì—ì„œ ë²—ì–´ë‚˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. DRO-REBELì€ Wasserstein, KL, $\chi^2$ ëª¨í˜¸ì„± ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, Fenchel ì´ì¤‘ì„±ì„ í†µí•´ ê°„ë‹¨í•œ ìƒëŒ€ ë³´ìƒ íšŒê·€ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ì˜ PPO ë°©ì‹ì˜ í´ë¦¬í•‘ì´ë‚˜ ë³´ì¡° ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í”¼í•˜ë©´ì„œ í™•ì¥ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DRO-REBELì€ ë‹¤ì–‘í•œ ë°ì´í„° ê·œëª¨ì™€ ëª¨ë¸ í¬ê¸°ì—ì„œ ê°•ë ¥í•œ ìµœì•…ì˜ ê²½ìš°ì˜ ê²¬ê³ ì„±ì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, íŠ¹íˆ $\chi^2$-REBELì´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë˜í•œ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ì¡°ê±´ í•˜ì—ì„œ ìµœì ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. RLHFëŠ” LLMsë¥¼ ì¸ê°„ì˜ ì˜ë„ì— ë§ì¶”ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ì§€ë§Œ, ê¸°ì¡´ì˜ ì˜¤í”„ë¼ì¸ RLHF ì ‘ê·¼ë²•ì€ ë³´ìƒ ëª…ì„¸ ì˜¤ë¥˜ë¡œ ì¸í•´ ê³¼ì í•© ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.
- 2. DRO-REBELì€ Fenchel ì´ì¤‘ì„±ì„ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ ìƒëŒ€ ë³´ìƒ íšŒê·€ë¡œ ì¶•ì†Œë˜ëŠ” ì—…ë°ì´íŠ¸ë¥¼ ì œê³µí•˜ì—¬ í™•ì¥ì„±ì„ ìœ ì§€í•˜ê³  PPO ìŠ¤íƒ€ì¼ì˜ í´ë¦¬í•‘ì´ë‚˜ ë³´ì¡° ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í”¼í•©ë‹ˆë‹¤.
- 3. DRO-REBELì€ í‘œì¤€ ì„ í˜• ë³´ìƒ ë° ë¡œê·¸ ì„ í˜• ì •ì±… í´ë˜ìŠ¤ í•˜ì—ì„œ ì´ì „ DRO-DPO ì ‘ê·¼ë²•ë³´ë‹¤ ë” ì—„ê²©í•œ ìƒìˆ˜ë¡œ $O(n^{-1/4})$ ì¶”ì • ê²½ê³„ë¥¼ í™•ë¦½í•˜ê³ , ì§€ì—­í™”ëœ Rademacher ë³µì¡ì„± ë¶„ì„ì„ í†µí•´ ë¯¸ë‹ˆë§¥ìŠ¤ ìµœì  $O(n^{-1/2})$ ì†ë„ë¥¼ íšŒë³µí•©ë‹ˆë‹¤.
- 4. Wasserstein, KL, $\chi^2$ì˜ ì„¸ ê°€ì§€ ë°œì‚°ì— ëŒ€í•œ ì‹¤ìš©ì ì¸ SGD ì•Œê³ ë¦¬ì¦˜ì„ ë„ì¶œí•˜ì˜€ìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ $\chi^2$-REBELì´ ì¼ê´€ë˜ê²Œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- 5. ì œì–´ëœ ë°˜ê²½-ì»¤ë²„ë¦¬ì§€ ì—°êµ¬ëŠ” ë°˜ê²½ì´ ê²½í—˜ì  ë°œì‚° ì§‘ì¤‘ ì†ë„ë³´ë‹¤ ë¹ ë¥´ê²Œ ìˆ˜ì¶•í•˜ë©´ ë¯¸ë‹ˆë§¥ìŠ¤ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ì†ë„ë¥¼ ë‹¬ì„±í•˜ì§€ë§Œ ì»¤ë²„ë¦¬ì§€ë¥¼ í¬ê¸°í•˜ê²Œ ë˜ë©°, ì»¤ë²„ë¦¬ì§€ë¥¼ ë³´ì¥í•˜ëŠ” ë°˜ê²½ì€ $O(n^{-1/4})$ ì†ë„ë¥¼ ì´ˆë˜í•œë‹¤ëŠ” ë¬´ìƒì  êµí™˜ì„ ê²€ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:00:30*