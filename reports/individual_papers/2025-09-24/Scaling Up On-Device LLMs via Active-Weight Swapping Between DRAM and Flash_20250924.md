<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:23:31.806890",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Active Weight DRAM-Flash Swapping",
    "Sparsity-aware Self-distillation",
    "ActiveFlow"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Active Weight DRAM-Flash Swapping": 0.78,
    "Sparsity-aware Self-distillation": 0.72,
    "ActiveFlow": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on scaling model deployment on devices.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Active Weight DRAM-Flash Swapping",
        "canonical": "Active Weight DRAM-Flash Swapping",
        "aliases": [
          "DRAM-Flash Swapping",
          "Active Weight Swapping"
        ],
        "category": "unique_technical",
        "rationale": "A novel technique introduced in the paper for efficient memory usage.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sparsity-aware Self-distillation",
        "canonical": "Sparsity-aware Self-distillation",
        "aliases": [
          "Self-distillation",
          "Sparsity-aware Distillation"
        ],
        "category": "unique_technical",
        "rationale": "A specific technique that aligns with the paper's focus on model efficiency.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      },
      {
        "surface": "ActiveFlow",
        "canonical": "ActiveFlow",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "The primary framework introduced in the paper, central to its contributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.55,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "adaptive DRAM usage",
      "hot weight cache",
      "computation-involved weights"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Active Weight DRAM-Flash Swapping",
      "resolved_canonical": "Active Weight DRAM-Flash Swapping",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sparsity-aware Self-distillation",
      "resolved_canonical": "Sparsity-aware Self-distillation",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "ActiveFlow",
      "resolved_canonical": "ActiveFlow",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.55,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2504.08378.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2504.08378](https://arxiv.org/abs/2504.08378)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MobiZO_ Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines_20250923|MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines]] (85.2% similar)
- [[2025-09-23/70% Size, 100% Accuracy_ Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float_20250923|70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float]] (85.1% similar)
- [[2025-09-18/LLM-I_ LLMs are Naturally Interleaved Multimodal Creators_20250918|LLM-I: LLMs are Naturally Interleaved Multimodal Creators]] (83.2% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (83.0% similar)
- [[2025-09-24/LCMF_ Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA_20250924|LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA]] (82.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Active Weight DRAM-Flash Swapping|Active Weight DRAM-Flash Swapping]], [[keywords/Sparsity-aware Self-distillation|Sparsity-aware Self-distillation]], [[keywords/ActiveFlow|ActiveFlow]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2504.08378v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëª¨ë°”ì¼ ì¥ì¹˜ì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°°í¬ë¥¼ ìœ„í•œ ActiveFlowë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ActiveFlowëŠ” DRAM ìš©ëŸ‰ì˜ ì œì•½ì„ ê·¹ë³µí•˜ê³  ë°°í¬ ê°€ëŠ¥í•œ ëª¨ë¸ í¬ê¸°ë¥¼ í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ì ì‘í˜• DRAM ì‚¬ìš©ì„ ì‹¤í˜„í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (1) Cross-layer active weights preloading ê¸°ë²•ì„ í†µí•´ ì—¬ëŸ¬ ë ˆì´ì–´ì˜ í™œì„± ê°€ì¤‘ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ê³„ì‚°ê³¼ ë°ì´í„° ë¡œë”©ì„ ì¤‘ì²©ì‹œí‚µë‹ˆë‹¤. (2) Sparsity-aware self-distillation ê¸°ë²•ìœ¼ë¡œ í™œì„± ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ë°€ì§‘ ëª¨ë¸ ì¶œë ¥ ë¶„í¬ì™€ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤. (3) Active weight DRAM-flash swapping pipelineì„ í†µí•´ DRAM ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ActiveFlowëŠ” ê¸°ì¡´ ìµœì í™” ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥-ë¹„ìš©ì˜ íŒŒë ˆí†  ìµœì ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ActiveFlowëŠ” í˜„ëŒ€ LLMì˜ ì ì‘í˜• DRAM ì‚¬ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë°°í¬ ê°€ëŠ¥í•œ ëª¨ë¸ í¬ê¸°ë¥¼ í™•ì¥í•  ìˆ˜ ìˆëŠ” ì²« ë²ˆì§¸ LLM ì¶”ë¡  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. ì´ í”„ë ˆì„ì›Œí¬ëŠ” í™œì„± ê°€ì¤‘ì¹˜ DRAM-í”Œë˜ì‹œ ìŠ¤ì™€í•‘ì´ë¼ëŠ” ìƒˆë¡œìš´ ê°œë…ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì„¸ ê°€ì§€ í˜ì‹ ì ì¸ ê¸°ìˆ ì„ í†µí•©í•©ë‹ˆë‹¤.
- 3. í¬ë¡œìŠ¤ ë ˆì´ì–´ í™œì„± ê°€ì¤‘ì¹˜ í”„ë¦¬ë¡œë”©ì€ í˜„ì¬ ë ˆì´ì–´ì˜ í™œì„±í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í›„ì† ë ˆì´ì–´ì˜ í™œì„± ê°€ì¤‘ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ê³„ì‚°ê³¼ ë°ì´í„° ë¡œë”©ì„ ê²¹ì¹˜ê²Œ í•©ë‹ˆë‹¤.
- 4. í¬ì†Œì„± ì¸ì‹ ìê¸° ì¦ë¥˜ëŠ” í™œì„± ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ë°€ì§‘ ëª¨ë¸ ì¶œë ¥ ë¶„í¬ì— ë§ì¶”ê³ , ë§¥ë½ì  í¬ì†Œì„±ìœ¼ë¡œ ì¸í•œ ê·¼ì‚¬ì¹˜ë¥¼ ë³´ìƒí•©ë‹ˆë‹¤.
- 5. í™œì„± ê°€ì¤‘ì¹˜ DRAM-í”Œë˜ì‹œ ìŠ¤ì™€í•‘ íŒŒì´í”„ë¼ì¸ì€ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë”°ë¼ DRAM ê³µê°„ í• ë‹¹ì„ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥-ë¹„ìš©ì˜ íŒŒë ˆí†  ì „ì„ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:23:31*