<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:29:56.117010",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Video-to-text Information Bottleneck Evaluation",
    "Grounding",
    "Utility",
    "Human Decision-Making"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Video-to-text Information Bottleneck Evaluation": 0.8,
    "Grounding": 0.78,
    "Utility": 0.77,
    "Human Decision-Making": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on video-to-text summarization, linking it to recent advancements in multimodal AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Video-to-text Information Bottleneck Evaluation",
        "canonical": "Video-to-text Information Bottleneck Evaluation",
        "aliases": [
          "VIBE"
        ],
        "category": "unique_technical",
        "rationale": "VIBE is a novel evaluation method introduced in the paper, crucial for understanding its contribution to video summarization.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Grounding",
        "canonical": "Grounding",
        "aliases": [
          "Alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Grounding is a key metric for evaluating how well summaries align with visual content, essential for linking to evaluation methods.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Utility",
        "canonical": "Utility",
        "aliases": [
          "Informative Value"
        ],
        "category": "specific_connectable",
        "rationale": "Utility measures the informative value of summaries, linking it to task performance improvements.",
        "novelty_score": 0.48,
        "connectivity_score": 0.72,
        "specificity_score": 0.68,
        "link_intent_score": 0.77
      },
      {
        "surface": "Human Decision-Making",
        "canonical": "Human Decision-Making",
        "aliases": [
          "Decision Support"
        ],
        "category": "broad_technical",
        "rationale": "Human decision-making is a broad concept but central to the paper's goal of improving task performance through better summaries.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Video-to-text Information Bottleneck Evaluation",
      "resolved_canonical": "Video-to-text Information Bottleneck Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Grounding",
      "resolved_canonical": "Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Utility",
      "resolved_canonical": "Utility",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.72,
        "specificity": 0.68,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Human Decision-Making",
      "resolved_canonical": "Human Decision-Making",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.17423.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2505.17423](https://arxiv.org/abs/2505.17423)

## 🔗 유사한 논문
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (85.7% similar)
- [[2025-09-23/Advancing Reference-free Evaluation of Video Captions with Factual Analysis_20250923|Advancing Reference-free Evaluation of Video Captions with Factual Analysis]] (84.9% similar)
- [[2025-09-23/SD-VSum_ A Method and Dataset for Script-Driven Video Summarization_20250923|SD-VSum: A Method and Dataset for Script-Driven Video Summarization]] (84.8% similar)
- [[2025-09-23/VideoRFT_ Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning_20250923|VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning]] (84.5% similar)
- [[2025-09-24/LD-ViCE_ Latent Diffusion Model for Video Counterfactual Explanations_20250924|LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations]] (84.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Human Decision-Making|Human Decision-Making]]
**🔗 Specific Connectable**: [[keywords/Grounding|Grounding]], [[keywords/Utility|Utility]]
**⚡ Unique Technical**: [[keywords/Video-to-text Information Bottleneck Evaluation|Video-to-text Information Bottleneck Evaluation]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2505.17423v3 Announce Type: replace 
Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still require human supervision. For example, tasks like traffic officers reviewing hour-long dashcam footage or researchers screening conference videos can benefit from concise summaries that reduce cognitive load and save time. Yet current vision-language models (VLMs) often produce verbose, redundant outputs that hinder task performance. Existing video caption evaluation depends on costly human annotations and overlooks the summaries' utility in downstream tasks. We address these gaps with Video-to-text Information Bottleneck Evaluation (VIBE), an annotation-free method that scores VLM outputs using two metrics: grounding (how well the summary aligns with visual content) and utility (how informative it is for the task). VIBE selects from randomly sampled VLM outputs by ranking them according to the two scores to support effective human decision-making. Human studies on LearningPaper24, SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE consistently improve performance-boosting task accuracy by up to 61.23% and reducing response time by 75.77% compared to naive VLM summaries or raw video.

## 📝 요약

이 논문은 비디오 요약의 효율성과 정확성을 높이기 위한 새로운 평가 방법인 VIBE(Video-to-text Information Bottleneck Evaluation)를 제안합니다. 기존의 비전-언어 모델(VLM)은 종종 장황하고 중복된 출력을 생성하여 작업 성과를 저해합니다. VIBE는 주석 없이 요약의 시각적 내용과의 일치도(grounding)와 작업에 대한 정보 제공도(utility)를 평가하여 VLM 출력을 점수화합니다. 이 방법은 무작위로 샘플링된 VLM 출력 중에서 두 점수를 기준으로 최적의 요약을 선택하여 인간의 의사 결정을 지원합니다. 연구 결과, VIBE로 선택된 요약은 작업 정확도를 최대 61.23% 향상시키고 응답 시간을 75.77% 단축시켜, 기존의 VLM 요약이나 원본 비디오보다 뛰어난 성능을 보였습니다.

## 🎯 주요 포인트

- 1. 많은 의사 결정 작업에서 정확성과 효율성이 중요하며, 이를 위해 인간의 감독이 여전히 필요하다.
- 2. 현재의 비전-언어 모델(VLM)은 장황하고 중복된 출력을 생성하여 작업 성과를 방해한다.
- 3. VIBE는 주석 없이 VLM 출력을 평가하는 방법으로, 시각적 콘텐츠와의 정합성(grounding)과 작업에 대한 정보성(utility)을 기준으로 점수를 매긴다.
- 4. VIBE는 무작위로 샘플링된 VLM 출력 중에서 두 점수를 기준으로 순위를 매겨 효과적인 인간 의사 결정을 지원한다.
- 5. 인간 연구 결과, VIBE가 선택한 요약은 작업 정확도를 최대 61.23% 향상시키고 반응 시간을 75.77% 단축시켰다.


---

*Generated on 2025-09-24 16:29:56*