<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:29:56.117010",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Video-to-text Information Bottleneck Evaluation",
    "Grounding",
    "Utility",
    "Human Decision-Making"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Video-to-text Information Bottleneck Evaluation": 0.8,
    "Grounding": 0.78,
    "Utility": 0.77,
    "Human Decision-Making": 0.65
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "VLM",
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on video-to-text summarization, linking it to recent advancements in multimodal AI.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Video-to-text Information Bottleneck Evaluation",
        "canonical": "Video-to-text Information Bottleneck Evaluation",
        "aliases": [
          "VIBE"
        ],
        "category": "unique_technical",
        "rationale": "VIBE is a novel evaluation method introduced in the paper, crucial for understanding its contribution to video summarization.",
        "novelty_score": 0.95,
        "connectivity_score": 0.6,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Grounding",
        "canonical": "Grounding",
        "aliases": [
          "Alignment"
        ],
        "category": "specific_connectable",
        "rationale": "Grounding is a key metric for evaluating how well summaries align with visual content, essential for linking to evaluation methods.",
        "novelty_score": 0.5,
        "connectivity_score": 0.75,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Utility",
        "canonical": "Utility",
        "aliases": [
          "Informative Value"
        ],
        "category": "specific_connectable",
        "rationale": "Utility measures the informative value of summaries, linking it to task performance improvements.",
        "novelty_score": 0.48,
        "connectivity_score": 0.72,
        "specificity_score": 0.68,
        "link_intent_score": 0.77
      },
      {
        "surface": "Human Decision-Making",
        "canonical": "Human Decision-Making",
        "aliases": [
          "Decision Support"
        ],
        "category": "broad_technical",
        "rationale": "Human decision-making is a broad concept but central to the paper's goal of improving task performance through better summaries.",
        "novelty_score": 0.4,
        "connectivity_score": 0.7,
        "specificity_score": 0.6,
        "link_intent_score": 0.65
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "task"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Video-to-text Information Bottleneck Evaluation",
      "resolved_canonical": "Video-to-text Information Bottleneck Evaluation",
      "decision": "linked",
      "scores": {
        "novelty": 0.95,
        "connectivity": 0.6,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Grounding",
      "resolved_canonical": "Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.75,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Utility",
      "resolved_canonical": "Utility",
      "decision": "linked",
      "scores": {
        "novelty": 0.48,
        "connectivity": 0.72,
        "specificity": 0.68,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Human Decision-Making",
      "resolved_canonical": "Human Decision-Making",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.7,
        "specificity": 0.6,
        "link_intent": 0.65
      }
    }
  ]
}
-->

# VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2505.17423.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2505.17423](https://arxiv.org/abs/2505.17423)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Eye Gaze Tells You Where to Compute_ Gaze-Driven Efficient VLMs_20250923|Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs]] (85.7% similar)
- [[2025-09-23/Advancing Reference-free Evaluation of Video Captions with Factual Analysis_20250923|Advancing Reference-free Evaluation of Video Captions with Factual Analysis]] (84.9% similar)
- [[2025-09-23/SD-VSum_ A Method and Dataset for Script-Driven Video Summarization_20250923|SD-VSum: A Method and Dataset for Script-Driven Video Summarization]] (84.8% similar)
- [[2025-09-23/VideoRFT_ Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning_20250923|VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning]] (84.5% similar)
- [[2025-09-24/LD-ViCE_ Latent Diffusion Model for Video Counterfactual Explanations_20250924|LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations]] (84.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Human Decision-Making|Human Decision-Making]]
**ğŸ”— Specific Connectable**: [[keywords/Grounding|Grounding]], [[keywords/Utility|Utility]]
**âš¡ Unique Technical**: [[keywords/Video-to-text Information Bottleneck Evaluation|Video-to-text Information Bottleneck Evaluation]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2505.17423v3 Announce Type: replace 
Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still require human supervision. For example, tasks like traffic officers reviewing hour-long dashcam footage or researchers screening conference videos can benefit from concise summaries that reduce cognitive load and save time. Yet current vision-language models (VLMs) often produce verbose, redundant outputs that hinder task performance. Existing video caption evaluation depends on costly human annotations and overlooks the summaries' utility in downstream tasks. We address these gaps with Video-to-text Information Bottleneck Evaluation (VIBE), an annotation-free method that scores VLM outputs using two metrics: grounding (how well the summary aligns with visual content) and utility (how informative it is for the task). VIBE selects from randomly sampled VLM outputs by ranking them according to the two scores to support effective human decision-making. Human studies on LearningPaper24, SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE consistently improve performance-boosting task accuracy by up to 61.23% and reducing response time by 75.77% compared to naive VLM summaries or raw video.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¹„ë””ì˜¤ ìš”ì•½ì˜ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ í‰ê°€ ë°©ë²•ì¸ VIBE(Video-to-text Information Bottleneck Evaluation)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì€ ì¢…ì¢… ì¥í™©í•˜ê³  ì¤‘ë³µëœ ì¶œë ¥ì„ ìƒì„±í•˜ì—¬ ì‘ì—… ì„±ê³¼ë¥¼ ì €í•´í•©ë‹ˆë‹¤. VIBEëŠ” ì£¼ì„ ì—†ì´ ìš”ì•½ì˜ ì‹œê°ì  ë‚´ìš©ê³¼ì˜ ì¼ì¹˜ë„(grounding)ì™€ ì‘ì—…ì— ëŒ€í•œ ì •ë³´ ì œê³µë„(utility)ë¥¼ í‰ê°€í•˜ì—¬ VLM ì¶œë ¥ì„ ì ìˆ˜í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ëœ VLM ì¶œë ¥ ì¤‘ì—ì„œ ë‘ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ ìš”ì•½ì„ ì„ íƒí•˜ì—¬ ì¸ê°„ì˜ ì˜ì‚¬ ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, VIBEë¡œ ì„ íƒëœ ìš”ì•½ì€ ì‘ì—… ì •í™•ë„ë¥¼ ìµœëŒ€ 61.23% í–¥ìƒì‹œí‚¤ê³  ì‘ë‹µ ì‹œê°„ì„ 75.77% ë‹¨ì¶•ì‹œì¼œ, ê¸°ì¡´ì˜ VLM ìš”ì•½ì´ë‚˜ ì›ë³¸ ë¹„ë””ì˜¤ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë§ì€ ì˜ì‚¬ ê²°ì • ì‘ì—…ì—ì„œ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•˜ë©°, ì´ë¥¼ ìœ„í•´ ì¸ê°„ì˜ ê°ë…ì´ ì—¬ì „íˆ í•„ìš”í•˜ë‹¤.
- 2. í˜„ì¬ì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì€ ì¥í™©í•˜ê³  ì¤‘ë³µëœ ì¶œë ¥ì„ ìƒì„±í•˜ì—¬ ì‘ì—… ì„±ê³¼ë¥¼ ë°©í•´í•œë‹¤.
- 3. VIBEëŠ” ì£¼ì„ ì—†ì´ VLM ì¶œë ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ì‹œê°ì  ì½˜í…ì¸ ì™€ì˜ ì •í•©ì„±(grounding)ê³¼ ì‘ì—…ì— ëŒ€í•œ ì •ë³´ì„±(utility)ì„ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸´ë‹¤.
- 4. VIBEëŠ” ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ëœ VLM ì¶œë ¥ ì¤‘ì—ì„œ ë‘ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ë¥¼ ë§¤ê²¨ íš¨ê³¼ì ì¸ ì¸ê°„ ì˜ì‚¬ ê²°ì •ì„ ì§€ì›í•œë‹¤.
- 5. ì¸ê°„ ì—°êµ¬ ê²°ê³¼, VIBEê°€ ì„ íƒí•œ ìš”ì•½ì€ ì‘ì—… ì •í™•ë„ë¥¼ ìµœëŒ€ 61.23% í–¥ìƒì‹œí‚¤ê³  ë°˜ì‘ ì‹œê°„ì„ 75.77% ë‹¨ì¶•ì‹œì¼°ë‹¤.


---

*Generated on 2025-09-24 16:29:56*