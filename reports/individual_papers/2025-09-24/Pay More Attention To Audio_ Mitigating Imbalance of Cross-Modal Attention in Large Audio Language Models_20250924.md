<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:48:01.726120",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Transformer",
    "Attention Mechanism",
    "More Attention To Audio",
    "Multimodal Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.78,
    "Transformer": 0.8,
    "Attention Mechanism": 0.85,
    "More Attention To Audio": 0.9,
    "Multimodal Learning": 0.82
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Audio-Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LALMs"
        ],
        "category": "broad_technical",
        "rationale": "Connects to existing research on large models, bridging audio and language processing.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "Transformer architecture",
        "canonical": "Transformer",
        "aliases": [],
        "category": "broad_technical",
        "rationale": "A foundational model architecture relevant to many multi-modal systems.",
        "novelty_score": 0.3,
        "connectivity_score": 0.92,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Central to the paper's method, linking to broader attention-based model research.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.85
      },
      {
        "surface": "MATA",
        "canonical": "More Attention To Audio",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A novel method proposed by the paper, crucial for understanding its contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.9
      },
      {
        "surface": "Multimodal fusion layers",
        "canonical": "Multimodal Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Key to understanding the integration of audio and text in models.",
        "novelty_score": 0.55,
        "connectivity_score": 0.8,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      }
    ],
    "ban_list_suggestions": [
      "audio-textual attention imbalance",
      "acoustic cues",
      "open-source model"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Audio-Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Transformer architecture",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.92,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "MATA",
      "resolved_canonical": "More Attention To Audio",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.9
      }
    },
    {
      "candidate_surface": "Multimodal fusion layers",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.8,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    }
  ]
}
-->

# Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18816.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18816](https://arxiv.org/abs/2509.18816)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Does Audio Matter for Modern Video-LLMs and Their Benchmarks?_20250923|Does Audio Matter for Modern Video-LLMs and Their Benchmarks?]] (86.2% similar)
- [[2025-09-24/An overview of neural architectures for self-supervised audio representation learning from masked spectrograms_20250924|An overview of neural architectures for self-supervised audio representation learning from masked spectrograms]] (85.0% similar)
- [[2025-09-23/Audio-Reasoner_ Improving Reasoning Capability in Large Audio Language Models_20250923|Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models]] (84.8% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (84.3% similar)
- [[2025-09-22/Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding_20250922|Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding]] (83.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]], [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/More Attention To Audio|More Attention To Audio]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18816v1 Announce Type: cross 
Abstract: Large Audio-Language Models (LALMs) often suffer from audio-textual attention imbalance, prioritizing text over acoustic information, particularly in the multi-modal fusion layers of the Transformer architecture. This bias hinders their ability to fully utilize acoustic cues, causing suboptimal performance on audio reasoning tasks. To mitigate this, we propose \textbf{MATA}, a novel training-free method that dynamically pushes LALMs to pay \textbf{M}ore \textbf{A}ttention \textbf{T}o \textbf{A}udio tokens within the self-attention mechanism. Specifically, MATA intervenes post raw attention scoring, targeting only the last token in intermediate layers without introducing additional parameters or computational overhead. Experiments on the MMAU and MMAR benchmarks confirm MATA's effectiveness, with consistent performance gains. Notably, on MMAR, MATA enables an open-source model to surpass the proprietary Gemini 2.0 Flash for the first time. Our work provides an efficient solution to mitigate attention bias and opens a new research direction for enhancing the audio-processing capabilities of multi-modal models.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì˜¤ë””ì˜¤-ì–¸ì–´ ëª¨ë¸(LALMs)ì´ í…ìŠ¤íŠ¸ì— ë¹„í•´ ì˜¤ë””ì˜¤ ì •ë³´ì— ëŒ€í•œ ì£¼ì˜ê°€ ë¶€ì¡±í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ MATAë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. MATAëŠ” ì¶”ê°€ì ì¸ ë§¤ê°œë³€ìˆ˜ë‚˜ ê³„ì‚° ë¶€ë‹´ ì—†ì´, ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ì˜¤ë””ì˜¤ í† í°ì— ë” ë§ì€ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ë„ë¡ ëª¨ë¸ì„ ìœ ë„í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, MATAëŠ” MMAUì™€ MMAR ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ MMARì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì´ Gemini 2.0 Flashë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì˜¤ë””ì˜¤-ì–¸ì–´ ëª¨ë¸(LALMs)ì€ ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ì£¼ì˜ë ¥ ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•´ ìŒí–¥ ì •ë³´ë¥¼ ì¶©ë¶„íˆ í™œìš©í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
- 2. MATAëŠ” LALMsê°€ ì˜¤ë””ì˜¤ í† í°ì— ë” ë§ì€ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ë„ë¡ ìœ ë„í•˜ëŠ” ìƒˆë¡œìš´ í›ˆë ¨ ì—†ëŠ” ë°©ë²•ì´ë‹¤.
- 3. MATAëŠ” ì¤‘ê°„ ë ˆì´ì–´ì˜ ë§ˆì§€ë§‰ í† í°ì—ë§Œ ê°œì…í•˜ì—¬ ì¶”ê°€ì ì¸ ë§¤ê°œë³€ìˆ˜ë‚˜ ê³„ì‚° ë¶€ë‹´ ì—†ì´ ì£¼ì˜ë ¥ í¸í–¥ì„ ì™„í™”í•œë‹¤.
- 4. MMAU ë° MMAR ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ MATAì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìœ¼ë©°, íŠ¹íˆ MMARì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ì´ Gemini 2.0 Flashë¥¼ ì²˜ìŒìœ¼ë¡œ ëŠ¥ê°€í–ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•œë‹¤.


---

*Generated on 2025-09-24 15:48:01*