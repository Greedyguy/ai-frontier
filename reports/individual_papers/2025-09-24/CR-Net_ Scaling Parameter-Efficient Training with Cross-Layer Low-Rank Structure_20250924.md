<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:58:14.262926",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Low-rank Architecture",
    "Large Language Model",
    "CR-Net",
    "Activation Recomputation"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Low-rank Architecture": 0.82,
    "Large Language Model": 0.85,
    "CR-Net": 0.88,
    "Activation Recomputation": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Low-rank architectures",
        "canonical": "Low-rank Architecture",
        "aliases": [
          "Low-rank Models"
        ],
        "category": "unique_technical",
        "rationale": "Low-rank architectures are central to the paper's contribution and provide a novel approach to parameter efficiency in large models.",
        "novelty_score": 0.75,
        "connectivity_score": 0.65,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Large language model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large language models are the primary context for the application of the proposed method.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-layer Low-Rank residual Network",
        "canonical": "CR-Net",
        "aliases": [
          "Cross-layer Low-Rank Network"
        ],
        "category": "unique_technical",
        "rationale": "CR-Net is the novel framework introduced in the paper, representing a significant advancement in low-rank model training.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.88
      },
      {
        "surface": "Activation recomputation strategy",
        "canonical": "Activation Recomputation",
        "aliases": [
          "Memory Reduction Strategy"
        ],
        "category": "specific_connectable",
        "rationale": "This strategy is crucial for reducing memory requirements, a key challenge addressed by the paper.",
        "novelty_score": 0.65,
        "connectivity_score": 0.75,
        "specificity_score": 0.72,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "performance",
      "experiment"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Low-rank architectures",
      "resolved_canonical": "Low-rank Architecture",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.65,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Large language model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-layer Low-Rank residual Network",
      "resolved_canonical": "CR-Net",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "Activation recomputation strategy",
      "resolved_canonical": "Activation Recomputation",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.75,
        "specificity": 0.72,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18993.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18993](https://arxiv.org/abs/2509.18993)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Sparsity May Be All You Need_ Sparse Random Parameter Adaptation_20250922|Sparsity May Be All You Need: Sparse Random Parameter Adaptation]] (84.5% similar)
- [[2025-09-24/HyperAdapt_ Simple High-Rank Adaptation_20250924|HyperAdapt: Simple High-Rank Adaptation]] (84.4% similar)
- [[2025-09-23/Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks_20250923|Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks]] (83.9% similar)
- [[2025-09-23/A geometric framework for momentum-based optimizers for low-rank training_20250923|A geometric framework for momentum-based optimizers for low-rank training]] (83.8% similar)
- [[2025-09-23/DeepInsert_ Early Layer Bypass for Efficient and Performant Multimodal Understanding_20250923|DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding]] (83.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Activation Recomputation|Activation Recomputation]]
**âš¡ Unique Technical**: [[keywords/Low-rank Architecture|Low-rank Architecture]], [[keywords/CR-Net|CR-Net]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18993v1 Announce Type: new 
Abstract: Low-rank architectures have become increasingly important for efficient large language model (LLM) pre-training, providing substantial reductions in both parameter complexity and memory/computational demands. Despite these advantages, current low-rank methods face three critical shortcomings: (1) compromised model performance, (2) considerable computational overhead, and (3) limited activation memory savings. To address these limitations, we propose Cross-layer Low-Rank residual Network (CR-Net), an innovative parameter-efficient framework inspired by our discovery that inter-layer activation residuals possess low-rank properties. CR-Net implements this insight through a dual-path architecture that efficiently reconstructs layer activations by combining previous-layer outputs with their low-rank differences, thereby maintaining high-rank information with minimal parameters. We further develop a specialized activation recomputation strategy tailored for CR-Net that dramatically reduces memory requirements. Extensive pre-training experiments across model scales from 60M to 7B parameters demonstrate that CR-Net consistently outperforms state-of-the-art low-rank frameworks while requiring fewer computational resources and less memory.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íš¨ìœ¨ì ì¸ ì‚¬ì „ í•™ìŠµì„ ìœ„í•œ ì €ë­í¬(low-rank) ì•„í‚¤í…ì²˜ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ë£¨ë©°, ê¸°ì¡´ ë°©ë²•ì˜ ì„±ëŠ¥ ì €í•˜, ë†’ì€ ê³„ì‚° ë¹„ìš©, ì œí•œëœ ë©”ëª¨ë¦¬ ì ˆì•½ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ CR-Netì„ ì œì•ˆí•©ë‹ˆë‹¤. CR-Netì€ ì¸µ ê°„ í™œì„±í™” ì”ì°¨ì˜ ì €ë­í¬ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬, ì´ì „ ì¸µì˜ ì¶œë ¥ê³¼ ì €ë­í¬ ì°¨ì´ë¥¼ ê²°í•©í•˜ëŠ” ì´ì¤‘ ê²½ë¡œ ì•„í‚¤í…ì²˜ë¡œ ê³ ë­í¬ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ë§¤ê°œë³€ìˆ˜ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤. ë˜í•œ, CR-Netì— íŠ¹í™”ëœ í™œì„±í™” ì¬ê³„ì‚° ì „ëµì„ ê°œë°œí•˜ì—¬ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ í¬ê²Œ ì¤„ì˜€ìŠµë‹ˆë‹¤. 60Mì—ì„œ 7B ë§¤ê°œë³€ìˆ˜ ê·œëª¨ì˜ ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì—ì„œ, CR-Netì€ ê¸°ì¡´ ì €ë­í¬ í”„ë ˆì„ì›Œí¬ë³´ë‹¤ ì ì€ ìì›ìœ¼ë¡œ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì €ì°¨ì› ì•„í‚¤í…ì²˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ì‚¬ì „ í›ˆë ¨ì„ ìœ„í•´ ì¤‘ìš”í•´ì§€ê³  ìˆìœ¼ë©°, ë§¤ê°œë³€ìˆ˜ ë³µì¡ì„±ê³¼ ë©”ëª¨ë¦¬/ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì„ í¬ê²Œ ì¤„ì—¬ì¤€ë‹¤.
- 2. ê¸°ì¡´ ì €ì°¨ì› ë°©ë²•ì€ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜, ìƒë‹¹í•œ ê³„ì‚° ì˜¤ë²„í—¤ë“œ, ì œí•œëœ í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê°ì´ë¼ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ì ì„ ê°€ì§€ê³  ìˆë‹¤.
- 3. CR-Netì€ ì¸µ ê°„ í™œì„±í™” ì”ì°¨ì˜ ì €ì°¨ì› íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•˜ë©°, ì´ì „ ì¸µ ì¶œë ¥ê³¼ ì €ì°¨ì› ì°¨ì´ë¥¼ ê²°í•©í•˜ì—¬ ì¸µ í™œì„±í™”ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¬êµ¬ì„±í•œë‹¤.
- 4. CR-Netì— íŠ¹í™”ëœ í™œì„±í™” ì¬ê³„ì‚° ì „ëµì„ ê°œë°œí•˜ì—¬ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ í¬ê²Œ ì¤„ì˜€ë‹¤.
- 5. 60Mì—ì„œ 7B ë§¤ê°œë³€ìˆ˜ì— ì´ë¥´ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ ê·œëª¨ì˜ ì‚¬ì „ í›ˆë ¨ ì‹¤í—˜ì—ì„œ CR-Netì€ ìµœì‹  ì €ì°¨ì› í”„ë ˆì„ì›Œí¬ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë” ì ì€ ê³„ì‚° ìì›ê³¼ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•œë‹¤.


---

*Generated on 2025-09-24 14:58:14*