<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:55:12.057742",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "MOMEMTO",
    "Time Series Foundation Model",
    "Attention Mechanism",
    "Few-Shot Learning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "MOMEMTO": 0.8,
    "Time Series Foundation Model": 0.75,
    "Attention Mechanism": 0.78,
    "Few-Shot Learning": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "MOMEMTO",
        "canonical": "MOMEMTO",
        "aliases": [
          "Patch-based Memory Gate Model"
        ],
        "category": "unique_technical",
        "rationale": "MOMEMTO is a novel model specifically designed for anomaly detection in time series, making it a unique technical concept.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Time Series Foundation Model",
        "canonical": "Time Series Foundation Model",
        "aliases": [
          "TFM"
        ],
        "category": "unique_technical",
        "rationale": "This is a specific type of foundation model tailored for time series data, offering a unique perspective in the field.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Attention Mechanism",
        "canonical": "Attention Mechanism",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Attention mechanisms are crucial in organizing and updating memory items, linking to broader neural network concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Few-Shot Learning",
        "canonical": "Few-Shot Learning",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Few-shot learning scenarios are highlighted in the paper, providing a connection to current trends in machine learning.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "reconstruction-based deep models",
      "training costs",
      "baseline methods"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "MOMEMTO",
      "resolved_canonical": "MOMEMTO",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Time Series Foundation Model",
      "resolved_canonical": "Time Series Foundation Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Attention Mechanism",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Few-Shot Learning",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18751.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18751](https://arxiv.org/abs/2509.18751)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Less is More_ Unlocking Specialization of Time Series Foundation Models via Structured Pruning_20250923|Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning]] (84.1% similar)
- [[2025-09-22/Two Facets of the Same Optimization Coin_ Model Degradation and Representation Collapse in Graph Foundation Models_20250922|Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models]] (83.2% similar)
- [[2025-09-23/MTM_ A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification_20250923|MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification]] (83.0% similar)
- [[2025-09-18/Masked Feature Modeling Enhances Adaptive Segmentation_20250918|Masked Feature Modeling Enhances Adaptive Segmentation]] (83.0% similar)
- [[2025-09-24/AdaMixT_ Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting_20250924|AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/MOMEMTO|MOMEMTO]], [[keywords/Time Series Foundation Model|Time Series Foundation Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18751v1 Announce Type: new 
Abstract: Recently reconstruction-based deep models have been widely used for time series anomaly detection, but as their capacity and representation capability increase, these models tend to over-generalize, often reconstructing unseen anomalies accurately. Prior works have attempted to mitigate this by incorporating a memory architecture that stores prototypes of normal patterns. Nevertheless, these approaches suffer from high training costs and have yet to be effectively integrated with time series foundation models (TFMs). To address these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection, enhanced with a patch-based memory module to mitigate over-generalization. The memory module is designed to capture representative normal patterns from multiple domains and enables a single model to be jointly fine-tuned across multiple datasets through a multi-domain training strategy. MOMEMTO initializes memory items with latent representations from a pre-trained encoder, organizes them into patch-level units, and updates them via an attention mechanism. We evaluate our method using 23 univariate benchmark datasets. Experimental results demonstrate that MOMEMTO, as a single model, achieves higher scores on AUC and VUS metrics compared to baseline methods, and further enhances the performance of its backbone TFM, particularly in few-shot learning scenarios.

## ğŸ“ ìš”ì•½

ìµœê·¼ ì‹œê³„ì—´ ì´ìƒ íƒì§€ì— ì¬êµ¬ì„± ê¸°ë°˜ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì´ ëª¨ë¸ë“¤ì€ ê³¼ë„í•œ ì¼ë°˜í™”ë¡œ ì¸í•´ ë³´ì§€ ëª»í•œ ì´ìƒì¹˜ë„ ì •í™•íˆ ì¬êµ¬ì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì •ìƒ íŒ¨í„´ì˜ í”„ë¡œí† íƒ€ì…ì„ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ êµ¬ì¡°ë¥¼ ë„ì…í–ˆìœ¼ë‚˜, ë†’ì€ í•™ìŠµ ë¹„ìš©ê³¼ ì‹œê³„ì—´ ê¸°ì´ˆ ëª¨ë¸(TFM)ê³¼ì˜ í†µí•© ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ TFMì¸ \textbf{MOMEMTO}ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ íŒ¨ì¹˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ëª¨ë“ˆì„ í†µí•´ ê³¼ë„í•œ ì¼ë°˜í™”ë¥¼ ì™„í™”í•˜ë©°, ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ëŒ€í‘œì ì¸ ì •ìƒ íŒ¨í„´ì„ ìº¡ì²˜í•˜ì—¬ ë‹¤ì¤‘ ë„ë©”ì¸ í•™ìŠµ ì „ëµì„ í†µí•´ ë‹¨ì¼ ëª¨ë¸ì´ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì—ì„œ ê³µë™ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. MOMEMTOëŠ” ì‚¬ì „ í•™ìŠµëœ ì¸ì½”ë”ì˜ ì ì¬ í‘œí˜„ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•­ëª©ì„ ì´ˆê¸°í™”í•˜ê³ , ì´ë¥¼ íŒ¨ì¹˜ ìˆ˜ì¤€ì˜ ë‹¨ìœ„ë¡œ ì¡°ì§í•˜ë©°, ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. 23ê°œì˜ ë‹¨ë³€ëŸ‰ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì‹¤í—˜ ê²°ê³¼, MOMEMTOëŠ” AUC ë° VUS ì§€í‘œì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, íŠ¹íˆ ì†Œìˆ˜ ìƒ· í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë°±ë³¸ TFMì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ìµœê·¼ ì¬êµ¬ì„± ê¸°ë°˜ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì‹œê³„ì—´ ì´ìƒ íƒì§€ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ì§€ë§Œ, ëª¨ë¸ì˜ ìš©ëŸ‰ê³¼ í‘œí˜„ ëŠ¥ë ¥ì´ ì¦ê°€í•¨ì— ë”°ë¼ ë¯¸ì§€ì˜ ì´ìƒì¹˜ë¥¼ ì •í™•í•˜ê²Œ ì¬êµ¬ì„±í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.
- 2. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì •ìƒ íŒ¨í„´ì˜ í”„ë¡œí† íƒ€ì…ì„ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•˜ì—¬ ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ë ¤ í–ˆìœ¼ë‚˜, ë†’ì€ í›ˆë ¨ ë¹„ìš©ê³¼ ì‹œê³„ì—´ ê¸°ì´ˆ ëª¨ë¸(TFM)ê³¼ì˜ íš¨ê³¼ì ì¸ í†µí•©ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤.
- 3. ì œì•ˆëœ MOMEMTOëŠ” íŒ¨ì¹˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ëª¨ë“ˆì„ í†µí•´ ê³¼ë„í•œ ì¼ë°˜í™”ë¥¼ ì™„í™”í•˜ëŠ” ì‹œê³„ì—´ ì´ìƒ íƒì§€ìš© TFMìœ¼ë¡œ, ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ëŒ€í‘œì ì¸ ì •ìƒ íŒ¨í„´ì„ í¬ì°©í•˜ê³  ë‹¤ì¤‘ ë„ë©”ì¸ í›ˆë ¨ ì „ëµì„ í†µí•´ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í†µí•©ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.
- 4. MOMEMTOëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì¸ì½”ë”ì˜ ì ì¬ í‘œí˜„ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•­ëª©ì„ ì´ˆê¸°í™”í•˜ê³ , íŒ¨ì¹˜ ìˆ˜ì¤€ì˜ ë‹¨ìœ„ë¡œ ì¡°ì§í•˜ë©°, ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì—…ë°ì´íŠ¸í•œë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, MOMEMTOëŠ” ë‹¨ì¼ ëª¨ë¸ë¡œì„œ 23ê°œì˜ ë‹¨ë³€ëŸ‰ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ AUC ë° VUS ì§€í‘œì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©°, íŠ¹íˆ ì†Œìˆ˜ ìƒ· í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë°±ë³¸ TFMì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¨ë‹¤.


---

*Generated on 2025-09-24 14:55:12*