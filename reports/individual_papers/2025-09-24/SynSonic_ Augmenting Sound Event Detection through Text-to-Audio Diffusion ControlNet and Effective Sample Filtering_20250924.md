<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:56:44.298586",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Sound Event Detection",
    "Text-to-Audio Diffusion",
    "ControlNet",
    "Polyphonic Sound Detection Scores"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Sound Event Detection": 0.8,
    "Text-to-Audio Diffusion": 0.85,
    "ControlNet": 0.75,
    "Polyphonic Sound Detection Scores": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Sound Event Detection",
        "canonical": "Sound Event Detection",
        "aliases": [
          "SED"
        ],
        "category": "unique_technical",
        "rationale": "Central to the paper, it connects with audio processing and machine learning domains.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "text-to-audio diffusion",
        "canonical": "Text-to-Audio Diffusion",
        "aliases": [
          "audio diffusion"
        ],
        "category": "unique_technical",
        "rationale": "Represents a novel generative approach specific to audio synthesis, linking to diffusion models.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.85
      },
      {
        "surface": "ControlNet",
        "canonical": "ControlNet",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "A specific model used in the paper, linking to neural network control techniques.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      },
      {
        "surface": "Polyphonic Sound Detection Scores",
        "canonical": "Polyphonic Sound Detection Scores",
        "aliases": [
          "PSDS1",
          "PSDS2"
        ],
        "category": "unique_technical",
        "rationale": "Key evaluation metric in the paper, connecting to sound detection performance.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "augmentation",
      "model performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Sound Event Detection",
      "resolved_canonical": "Sound Event Detection",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "text-to-audio diffusion",
      "resolved_canonical": "Text-to-Audio Diffusion",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "ControlNet",
      "resolved_canonical": "ControlNet",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Polyphonic Sound Detection Scores",
      "resolved_canonical": "Polyphonic Sound Detection Scores",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18603.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18603](https://arxiv.org/abs/2509.18603)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior_20250919|Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior]] (81.9% similar)
- [[2025-09-23/SynParaSpeech_ Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding_20250923|SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding]] (81.8% similar)
- [[2025-09-23/Extract and Diffuse_ Latent Integration for Improved Diffusion-based Speech and Vocal Enhancement_20250923|Extract and Diffuse: Latent Integration for Improved Diffusion-based Speech and Vocal Enhancement]] (81.5% similar)
- [[2025-09-19/SpeechOp_ Inference-Time Task Composition for Generative Speech Processing_20250919|SpeechOp: Inference-Time Task Composition for Generative Speech Processing]] (81.0% similar)
- [[2025-09-22/Generating Moving 3D Soundscapes with Latent Diffusion Models_20250922|Generating Moving 3D Soundscapes with Latent Diffusion Models]] (80.9% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**âš¡ Unique Technical**: [[keywords/Sound Event Detection|Sound Event Detection]], [[keywords/Text-to-Audio Diffusion|Text-to-Audio Diffusion]], [[keywords/ControlNet|ControlNet]], [[keywords/Polyphonic Sound Detection Scores|Polyphonic Sound Detection Scores]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18603v1 Announce Type: cross 
Abstract: Data synthesis and augmentation are essential for Sound Event Detection (SED) due to the scarcity of temporally labeled data. While augmentation methods like SpecAugment and Mix-up can enhance model performance, they remain constrained by the diversity of existing samples. Recent generative models offer new opportunities, yet their direct application to SED is challenging due to the lack of precise temporal annotations and the risk of introducing noise through unreliable filtering. To address these challenges and enable generative-based augmentation for SED, we propose SynSonic, a data augmentation method tailored for this task. SynSonic leverages text-to-audio diffusion models guided by an energy-envelope ControlNet to generate temporally coherent sound events. A joint score filtering strategy with dual classifiers ensures sample quality, and we explore its practical integration into training pipelines. Experimental results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1 and PSDS2), enhancing both temporal localization and sound class discrimination.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì†Œë¦¬ ì´ë²¤íŠ¸ ê°ì§€(SED)ì—ì„œ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ SynSonicì´ë¼ëŠ” ë°ì´í„° ì¦ê°• ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì¦ê°• ë°©ë²•ë“¤ì€ ìƒ˜í”Œ ë‹¤ì–‘ì„±ì˜ í•œê³„ê°€ ìˆì§€ë§Œ, SynSonicì€ í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ ë³€í™˜ í™•ì‚° ëª¨ë¸ê³¼ ì—ë„ˆì§€-ì—”ë²¨ë¡œí”„ ControlNetì„ í™œìš©í•˜ì—¬ ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ì†Œë¦¬ ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë˜í•œ, ì´ì¤‘ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•œ ê³µë™ ì ìˆ˜ í•„í„°ë§ ì „ëµì„ í†µí•´ ìƒ˜í”Œì˜ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SynSonicì€ ë‹¤ì„± ì†Œë¦¬ ê°ì§€ ì ìˆ˜(PSDS1 ë° PSDS2)ë¥¼ ê°œì„ í•˜ì—¬ ì‹œê°„ì  ìœ„ì¹˜ ì§€ì •ê³¼ ì†Œë¦¬ í´ë˜ìŠ¤ êµ¬ë³„ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë°ì´í„° í•©ì„±ê³¼ ì¦ê°•ì€ ì‹œê°„ì ìœ¼ë¡œ ë¼ë²¨ë§ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì†Œë¦¬ ì´ë²¤íŠ¸ ê°ì§€(SED)ì—ì„œ í•„ìˆ˜ì ì…ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ ì¦ê°• ë°©ë²•ì¸ SpecAugmentì™€ Mix-upì€ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ê¸°ì¡´ ìƒ˜í”Œì˜ ë‹¤ì–‘ì„±ì— ì˜í•´ ì œí•œë©ë‹ˆë‹¤.
- 3. SynSonicì€ ì—ë„ˆì§€-ì—”ë²¨ë¡œí”„ ControlNetìœ¼ë¡œ ì•ˆë‚´ë˜ëŠ” í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ í™•ì‚° ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ì†Œë¦¬ ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- 4. ì´ì¤‘ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•œ ê³µë™ ì ìˆ˜ í•„í„°ë§ ì „ëµì„ í†µí•´ ìƒ˜í”Œ í’ˆì§ˆì„ ë³´ì¥í•˜ë©°, ì´ë¥¼ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼ SynSonicì´ ë‹¤ì„± ì†Œë¦¬ ê°ì§€ ì ìˆ˜(PSDS1 ë° PSDS2)ë¥¼ ê°œì„ í•˜ì—¬ ì‹œê°„ì  ìœ„ì¹˜ ì§€ì •ê³¼ ì†Œë¦¬ í´ë˜ìŠ¤ êµ¬ë³„ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:56:44*