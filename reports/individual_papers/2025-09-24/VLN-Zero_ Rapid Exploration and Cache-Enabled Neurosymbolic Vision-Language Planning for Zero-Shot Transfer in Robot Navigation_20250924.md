<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:56:12.983743",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Zero-Shot Learning",
    "Vision-Language Model",
    "Neurosymbolic Planning",
    "Scene Graph"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Zero-Shot Learning": 0.85,
    "Vision-Language Model": 0.89,
    "Neurosymbolic Planning": 0.78,
    "Scene Graph": 0.81
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Zero-Shot Transfer",
        "canonical": "Zero-Shot Learning",
        "aliases": [
          "Zero-Shot Transfer",
          "Zero-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Zero-Shot Learning is crucial for linking to related works on learning without prior exposure to specific tasks.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.78,
        "link_intent_score": 0.85
      },
      {
        "surface": "Vision-Language Navigation",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language Navigation",
          "VLN"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are a rapidly evolving area, connecting to multimodal learning and integration.",
        "novelty_score": 0.65,
        "connectivity_score": 0.9,
        "specificity_score": 0.82,
        "link_intent_score": 0.89
      },
      {
        "surface": "Neurosymbolic Planning",
        "canonical": "Neurosymbolic Planning",
        "aliases": [
          "Neurosymbolic Reasoning"
        ],
        "category": "unique_technical",
        "rationale": "Neurosymbolic Planning is a novel approach combining symbolic reasoning with neural networks, relevant for advanced AI systems.",
        "novelty_score": 0.72,
        "connectivity_score": 0.75,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Scene Graph",
        "canonical": "Scene Graph",
        "aliases": [
          "Symbolic Scene Graph"
        ],
        "category": "specific_connectable",
        "rationale": "Scene Graphs are essential for linking to works on structured visual representations and symbolic reasoning.",
        "novelty_score": 0.58,
        "connectivity_score": 0.82,
        "specificity_score": 0.8,
        "link_intent_score": 0.81
      }
    ],
    "ban_list_suggestions": [
      "Rapid Exploration",
      "Cache-Enabled Execution"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Zero-Shot Transfer",
      "resolved_canonical": "Zero-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.78,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Vision-Language Navigation",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.9,
        "specificity": 0.82,
        "link_intent": 0.89
      }
    },
    {
      "candidate_surface": "Neurosymbolic Planning",
      "resolved_canonical": "Neurosymbolic Planning",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.75,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Scene Graph",
      "resolved_canonical": "Scene Graph",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.82,
        "specificity": 0.8,
        "link_intent": 0.81
      }
    }
  ]
}
-->

# VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18592.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18592](https://arxiv.org/abs/2509.18592)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-19/Handle Object Navigation as Weighted Traveling Repairman Problem_20250919|Handle Object Navigation as Weighted Traveling Repairman Problem]] (85.8% similar)
- [[2025-09-18/FSR-VLN_ Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph_20250918|FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph]] (85.1% similar)
- [[2025-09-22/Walk and Read Less_ Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning_20250922|Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning]] (84.6% similar)
- [[2025-09-22/A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning_20250922|A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning]] (84.0% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (83.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Zero-Shot Learning|Zero-Shot Learning]], [[keywords/Scene Graph|Scene Graph]]
**âš¡ Unique Technical**: [[keywords/Neurosymbolic Planning|Neurosymbolic Planning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18592v1 Announce Type: cross 
Abstract: Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.

## ğŸ“ ìš”ì•½

VLN-ZeroëŠ” ìƒˆë¡œìš´ í™˜ê²½ì—ì„œì˜ ë¹ ë¥¸ ì ì‘ì„ ëª©í‘œë¡œ í•˜ëŠ” ë¹„ì „-ì–¸ì–´ ë‚´ë¹„ê²Œì´ì…˜ í”„ë ˆì„ì›Œí¬ë¡œ, ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. íƒìƒ‰ ë‹¨ê³„ì—ì„œëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•´ ìƒì§•ì  ì¥ë©´ ê·¸ë˜í”„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì¶•í•˜ê³ , ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤. ë°°ì¹˜ ë‹¨ê³„ì—ì„œëŠ” ì‹ ê²½-ìƒì§•ì  ê³„íšìê°€ ì¥ë©´ ê·¸ë˜í”„ì™€ í™˜ê²½ ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íšì„ ìƒì„±í•˜ë©°, ìºì‹œ ê¸°ëŠ¥ì„ í†µí•´ ì´ì „ì— ê³„ì‚°ëœ ê²½ë¡œë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì ì‘ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ê¸°ì¡´ ë°©ë²•ì˜ ì¼ë°˜í™” ë¬¸ì œì™€ ê³„ì‚° ë¹„íš¨ìœ¨ì„±ì„ ê·¹ë³µí•˜ì—¬, ìƒˆë¡œìš´ í™˜ê²½ì—ì„œì˜ ê²¬ê³ í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì˜ì‚¬ê²°ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. VLN-ZeroëŠ” ìµœì‹  ëª¨ë¸ ëŒ€ë¹„ ì„±ê³µë¥ ì´ ë‘ ë°° ë†’ê³ , ëª©í‘œ ë„ë‹¬ ì‹œê°„ì´ ì ˆë°˜ìœ¼ë¡œ ë‹¨ì¶•ë˜ë©°, í‰ê· ì ìœ¼ë¡œ 55% ì ì€ VLM í˜¸ì¶œì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VLN-ZeroëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ìƒì§•ì  ì¥ë©´ ê·¸ë˜í”„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì¶•í•˜ê³  ì œë¡œìƒ· ì‹ ê²½ìƒì§•ì  ë‚´ë¹„ê²Œì´ì…˜ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë‘ ë‹¨ê³„ì˜ ë¹„ì „-ì–¸ì–´ ë‚´ë¹„ê²Œì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
- 2. íƒìƒ‰ ë‹¨ê³„ì—ì„œëŠ” êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ VLM ê¸°ë°˜ ê²€ìƒ‰ì„ ì •ë³´ê°€ í’ë¶€í•˜ê³  ë‹¤ì–‘í•œ ê²½ë¡œë¡œ ì•ˆë‚´í•˜ì—¬ ê°„ê²°í•œ ì¥ë©´ ê·¸ë˜í”„ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤.
- 3. ë°°í¬ ë‹¨ê³„ì—ì„œëŠ” ì‹ ê²½ìƒì§•ì  ê³„íšìê°€ ì¥ë©´ ê·¸ë˜í”„ì™€ í™˜ê²½ ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íšì„ ìƒì„±í•˜ë©°, ìºì‹œ ì§€ì› ì‹¤í–‰ ëª¨ë“ˆì´ ì´ì „ì— ê³„ì‚°ëœ ì‘ì—…-ìœ„ì¹˜ ê²½ë¡œë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì ì‘ì„ ê°€ì†í™”í•©ë‹ˆë‹¤.
- 4. VLN-ZeroëŠ” ë¹ ë¥¸ íƒìƒ‰, ìƒì§•ì  ì¶”ë¡ , ìºì‹œ ì§€ì› ì‹¤í–‰ì„ ê²°í•©í•˜ì—¬ ê¸°ì¡´ ë¹„ì „-ì–¸ì–´ ë‚´ë¹„ê²Œì´ì…˜ ë°©ë²•ì˜ ê³„ì‚° ë¹„íš¨ìœ¨ì„±ê³¼ ì¼ë°˜í™” ë¶€ì¡± ë¬¸ì œë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.
- 5. VLN-ZeroëŠ” ìµœì‹  ì œë¡œìƒ· ëª¨ë¸ ëŒ€ë¹„ 2ë°° ë†’ì€ ì„±ê³µë¥ ì„ ê¸°ë¡í•˜ë©°, ëŒ€ë¶€ë¶„ì˜ ë¯¸ì„¸ ì¡°ì •ëœ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê³ , ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ëª©í‘œ ìœ„ì¹˜ì— ë„ë‹¬í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³  í‰ê· ì ìœ¼ë¡œ 55% ì ì€ VLM í˜¸ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:56:12*