<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:35:16.055684",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Speculative Decoding",
    "Large Language Model",
    "Branch Parallelism",
    "Token Rollback"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Speculative Decoding": 0.8,
    "Large Language Model": 0.85,
    "Branch Parallelism": 0.78,
    "Token Rollback": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [
          "SD"
        ],
        "category": "unique_technical",
        "rationale": "This is a novel technique central to the paper's contribution, offering a unique approach to LLM inference acceleration.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Large Language Model",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "LLMs are a fundamental component of the study, providing context for the speculative decoding technique.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Branch Parallelism",
        "canonical": "Branch Parallelism",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Branch parallelism is a key innovation in the proposed framework, enhancing the speculative decoding process.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "Token Rollback",
        "canonical": "Token Rollback",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Token rollback is a critical challenge addressed by the paper, impacting the efficiency of speculative decoding.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Large Language Model",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Branch Parallelism",
      "resolved_canonical": "Branch Parallelism",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Token Rollback",
      "resolved_canonical": "Token Rollback",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.01979.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2506.01979](https://arxiv.org/abs/2506.01979)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/CARD_ A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference_20250922|CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference]] (89.1% similar)
- [[2025-09-23/SpecVLM_ Fast Speculative Decoding in Vision-Language Models_20250923|SpecVLM: Fast Speculative Decoding in Vision-Language Models]] (85.8% similar)
- [[2025-09-23/Spiffy_ Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding_20250923|Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding]] (85.7% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (84.3% similar)
- [[2025-09-23/Spec-VLA_ Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance_20250923|Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance]] (83.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**âš¡ Unique Technical**: [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/Branch Parallelism|Branch Parallelism]], [[keywords/Token Rollback|Token Rollback]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.01979v2 Announce Type: replace-cross 
Abstract: Speculative decoding (SD) has emerged as a promising technique to accelerate LLM inference by employing a small draft model to propose draft tokens in advance, and validating them in parallel with the large target model. However, the existing SD methods still remain constrained by their serialized execution, which causes the mutual waiting bubbles between the draft and target models. To address this challenge, we draw inspiration from branch prediction in modern processors and propose a novel framework \textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first take an in-depth analysis of the potential of branch parallelism in SD, and recognize that the key challenge lies in the trade-offs between parallelization and token rollback. Based on the analysis, we introduce parallel speculative branches to preemptively hedge against likely rejections. Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft lengths with a hybrid combination of the implicit draft model confidence and explicit reusing of target model features. Extensive experiments across various models and benchmarks show that SpecBranch achieves over \textbf{1.8}$\times \sim$ \textbf{4.5}$\times$ speedups against the auto-regressive decoding and reduces rollback tokens by $\textbf{50}$\% for poorly aligned models, while maintaining an identical sampling distribution.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ LLM ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê¸°ë²•ì¸ Speculative Decoding(SD)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì ì œì•ˆëœ \textbf{SpecBranch}ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ SD ë°©ë²•ì€ ì§ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì¸í•´ ì´ˆì•ˆ ëª¨ë¸ê³¼ ëŒ€ìƒ ëª¨ë¸ ê°„ì˜ ìƒí˜¸ ëŒ€ê¸° ì‹œê°„ì´ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, í˜„ëŒ€ í”„ë¡œì„¸ì„œì˜ ë¶„ê¸° ì˜ˆì¸¡ì—ì„œ ì˜ê°ì„ ë°›ì•„ SDì—ì„œ ë¶„ê¸° ë³‘ë ¬ì„±ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ë³‘ë ¬ ë¶„ê¸°ë¥¼ í†µí•´ ì˜ˆìƒë˜ëŠ” ê±°ë¶€ë¥¼ ì‚¬ì „ì— ëŒ€ë¹„í•˜ê³ , ì´ˆì•ˆ ëª¨ë¸ì˜ ì‹ ë¢°ë„ì™€ ëŒ€ìƒ ëª¨ë¸ì˜ íŠ¹ì§•ì„ ê²°í•©í•˜ì—¬ ì ì‘í˜• ì´ˆì•ˆ ê¸¸ì´ë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ ë³‘ë ¬ì„±ì„ í–¥ìƒì‹œí‚¨ ê²ƒì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, SpecBranchëŠ” ê¸°ì¡´ ìë™ íšŒê·€ ë””ì½”ë”© ëŒ€ë¹„ 1.8ë°°ì—ì„œ 4.5ë°°ì˜ ì†ë„ í–¥ìƒì„ ì´ë£¨ì—ˆê³ , ì˜ ë§ì§€ ì•ŠëŠ” ëª¨ë¸ì—ì„œ ë¡¤ë°± í† í°ì„ 50% ì¤„ì´ë©´ì„œë„ ë™ì¼í•œ ìƒ˜í”Œë§ ë¶„í¬ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Speculative decoding(ì¶”ë¡  ê°€ì†í™” ê¸°ë²•)ì€ ì´ˆì•ˆ ëª¨ë¸ê³¼ ëŒ€í˜• ëª©í‘œ ëª¨ë¸ì˜ ë³‘ë ¬ ê²€ì¦ì„ í†µí•´ LLM ì¶”ë¡ ì„ ê°€ì†í™”í•˜ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ì€ ì§ë ¬ ì‹¤í–‰ì˜ ì œì•½ì„ ë°›ìŠµë‹ˆë‹¤.
- 2. SpecBranchë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ëŠ” í˜„ëŒ€ í”„ë¡œì„¸ì„œì˜ ë¶„ê¸° ì˜ˆì¸¡ì—ì„œ ì˜ê°ì„ ë°›ì•„ SDì—ì„œì˜ ë¶„ê¸° ë³‘ë ¬ì„±ì„ êµ¬í˜„í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 3. SpecBranchëŠ” ë³‘ë ¬í™”ë¥¼ í†µí•œ í† í° ë¡¤ë°±ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ ì ì¬ì ì¸ ë¶„ê¸° ë³‘ë ¬ì„±ì„ ë¶„ì„í•˜ê³ , ì‚¬ì „ ë°©ì–´ë¥¼ ìœ„í•œ ë³‘ë ¬ ì¶”ì¸¡ ë¶„ê¸°ë¥¼ ë„ì…í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SpecBranchëŠ” ìë™ íšŒê·€ ë””ì½”ë”©ì— ë¹„í•´ 1.8ë°°ì—ì„œ 4.5ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•˜ê³ , ì˜ëª» ì •ë ¬ëœ ëª¨ë¸ì—ì„œ ë¡¤ë°± í† í°ì„ 50% ì¤„ì´ë©´ì„œ ë™ì¼í•œ ìƒ˜í”Œë§ ë¶„í¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:35:16*