<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:39:18.935622",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Architecture Search",
    "Hypernetwork",
    "Few-Shot Learning",
    "Global Encoding Scheme"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Architecture Search": 0.82,
    "Hypernetwork": 0.77,
    "Few-Shot Learning": 0.79,
    "Global Encoding Scheme": 0.74
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Architecture Search",
        "canonical": "Neural Architecture Search",
        "aliases": [
          "NAS"
        ],
        "category": "specific_connectable",
        "rationale": "Neural Architecture Search is a key concept in optimizing neural network architectures and connects well with machine learning and neural network topics.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Hypernetwork",
        "canonical": "Hypernetwork",
        "aliases": [
          "Hypernetworks"
        ],
        "category": "unique_technical",
        "rationale": "Hypernetwork is a unique approach to enhance architecture representation, offering novel insights into neural network design.",
        "novelty_score": 0.72,
        "connectivity_score": 0.69,
        "specificity_score": 0.81,
        "link_intent_score": 0.77
      },
      {
        "surface": "Few-Shot Scenarios",
        "canonical": "Few-Shot Learning",
        "aliases": [
          "Few-Shot"
        ],
        "category": "specific_connectable",
        "rationale": "Few-Shot Learning is increasingly relevant for efficient model training and connects with broader machine learning strategies.",
        "novelty_score": 0.58,
        "connectivity_score": 0.88,
        "specificity_score": 0.76,
        "link_intent_score": 0.79
      },
      {
        "surface": "Global Encoding Scheme",
        "canonical": "Global Encoding Scheme",
        "aliases": [
          "Global Encoding"
        ],
        "category": "unique_technical",
        "rationale": "This scheme is central to the paper's method for capturing macro-structure information, offering a unique perspective on architecture representation.",
        "novelty_score": 0.65,
        "connectivity_score": 0.67,
        "specificity_score": 0.79,
        "link_intent_score": 0.74
      }
    ],
    "ban_list_suggestions": [
      "performance evaluations",
      "proxy datasets"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Architecture Search",
      "resolved_canonical": "Neural Architecture Search",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Hypernetwork",
      "resolved_canonical": "Hypernetwork",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.69,
        "specificity": 0.81,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Few-Shot Scenarios",
      "resolved_canonical": "Few-Shot Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.58,
        "connectivity": 0.88,
        "specificity": 0.76,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Global Encoding Scheme",
      "resolved_canonical": "Global Encoding Scheme",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.67,
        "specificity": 0.79,
        "link_intent": 0.74
      }
    }
  ]
}
-->

# HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18151.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18151](https://arxiv.org/abs/2509.18151)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Deep Hierarchical Learning with Nested Subspace Networks_20250923|Deep Hierarchical Learning with Nested Subspace Networks]] (84.4% similar)
- [[2025-09-23/Neural Attention Search_20250923|Neural Attention Search]] (83.4% similar)
- [[2025-09-23/Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs_20250923|Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs]] (82.1% similar)
- [[2025-09-22/Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data_20250922|Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data]] (81.7% similar)
- [[2025-09-23/SINF_ Semantic Neural Network Inference with Semantic Subgraphs_20250923|SINF: Semantic Neural Network Inference with Semantic Subgraphs]] (81.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Neural Architecture Search|Neural Architecture Search]], [[keywords/Few-Shot Learning|Few-Shot Learning]]
**âš¡ Unique Technical**: [[keywords/Hypernetwork|Hypernetwork]], [[keywords/Global Encoding Scheme|Global Encoding Scheme]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18151v1 Announce Type: cross 
Abstract: Time-intensive performance evaluations significantly impede progress in Neural Architecture Search (NAS). To address this, neural predictors leverage surrogate models trained on proxy datasets, allowing for direct performance predictions for new architectures. However, these predictors often exhibit poor generalization due to their limited ability to capture intricate relationships among various architectures. In this paper, we propose HyperNAS, a novel neural predictor paradigm for enhancing architecture representation learning. HyperNAS consists of two primary components: a global encoding scheme and a shared hypernetwork. The global encoding scheme is devised to capture the comprehensive macro-structure information, while the shared hypernetwork serves as an auxiliary task to enhance the investigation of inter-architecture patterns. To ensure training stability, we further develop a dynamic adaptive multi-task loss to facilitate personalized exploration on the Pareto front. Extensive experiments across five representative search spaces, including ViTs, demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1 accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least 5.0$\times$ fewer samples.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ Neural Architecture Search(NAS)ì˜ ì„±ëŠ¥ í‰ê°€ ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ HyperNASë¼ëŠ” ìƒˆë¡œìš´ ì‹ ê²½ ì˜ˆì¸¡ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. HyperNASëŠ” ì•„í‚¤í…ì²˜ í‘œí˜„ í•™ìŠµì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ê¸€ë¡œë²Œ ì¸ì½”ë”© ìŠ¤í‚´ê³¼ ê³µìœ  í•˜ì´í¼ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê¸€ë¡œë²Œ ì¸ì½”ë”© ìŠ¤í‚´ì€ ì „ì²´ì ì¸ ë§¤í¬ë¡œ êµ¬ì¡° ì •ë³´ë¥¼ í¬ì°©í•˜ê³ , ê³µìœ  í•˜ì´í¼ë„¤íŠ¸ì›Œí¬ëŠ” ì•„í‚¤í…ì²˜ ê°„ íŒ¨í„´ì„ ì¡°ì‚¬í•˜ëŠ” ë³´ì¡° ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë˜í•œ, Pareto ì „ì„ ì—ì„œì˜ ê°œì¸í™”ëœ íƒìƒ‰ì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ë™ì  ì ì‘í˜• ë‹¤ì¤‘ ì‘ì—… ì†ì‹¤ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, HyperNASëŠ” íŠ¹íˆ ì ì€ ìƒ˜í”Œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, CIFAR-10ì—ì„œ 97.60%, ImageNetì—ì„œ 82.4%ì˜ ìµœê³  ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©°, ìµœì†Œ 5ë°° ì ì€ ìƒ˜í”Œë¡œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Neural Architecture Search(NAS)ì—ì„œ ì„±ëŠ¥ í‰ê°€ì˜ ì‹œê°„ ì†Œëª¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ HyperNASë¼ëŠ” ìƒˆë¡œìš´ ì‹ ê²½ ì˜ˆì¸¡ì íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. HyperNASëŠ” ê¸€ë¡œë²Œ ì¸ì½”ë”© ìŠ¤í‚´ê³¼ ê³µìœ  í•˜ì´í¼ë„¤íŠ¸ì›Œí¬ë¼ëŠ” ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- 3. ê¸€ë¡œë²Œ ì¸ì½”ë”© ìŠ¤í‚´ì€ ì „ì²´ì ì¸ ë§¤í¬ë¡œ êµ¬ì¡° ì •ë³´ë¥¼ í¬ì°©í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ê³µìœ  í•˜ì´í¼ë„¤íŠ¸ì›Œí¬ëŠ” ì•„í‚¤í…ì²˜ ê°„ íŒ¨í„´ ì¡°ì‚¬ë¥¼ ê°•í™”í•˜ëŠ” ë³´ì¡° ì‘ì—…ìœ¼ë¡œ ì‘ìš©í•©ë‹ˆë‹¤.
- 4. ë‹¤ì–‘í•œ ê²€ìƒ‰ ê³µê°„ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, íŠ¹íˆ few-shot ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ HyperNASì˜ ì¥ì ì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.
- 5. HyperNASëŠ” CIFAR-10ì—ì„œ 97.60%ì˜ top-1 ì •í™•ë„ì™€ ImageNetì—ì„œ 82.4%ì˜ top-1 ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©°, ìµœì†Œ 5ë°° ì ì€ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:39:18*