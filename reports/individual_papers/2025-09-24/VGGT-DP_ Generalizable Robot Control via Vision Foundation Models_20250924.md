<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:03:36.062924",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Visual Imitation Learning",
    "Vision Foundation Models",
    "Proprioceptive Feedback",
    "Transformer",
    "Spatial Grounding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Visual Imitation Learning": 0.78,
    "Vision Foundation Models": 0.82,
    "Proprioceptive Feedback": 0.79,
    "Transformer": 0.8,
    "Spatial Grounding": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Imitation Learning",
        "canonical": "Visual Imitation Learning",
        "aliases": [
          "Visual Learning from Demonstrations"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's approach and links to broader themes in robotic learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision Foundation Models",
        "canonical": "Vision Foundation Models",
        "aliases": [
          "Vision Models",
          "Visual Foundation Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents a new trend in leveraging large pre-trained models for vision tasks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Proprioceptive Feedback",
        "canonical": "Proprioceptive Feedback",
        "aliases": [
          "Proprioception"
        ],
        "category": "specific_connectable",
        "rationale": "Key to understanding the integration of sensory feedback in robotic control.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      },
      {
        "surface": "Visual Geometry Grounded Transformer",
        "canonical": "Transformer",
        "aliases": [
          "VGGT"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader category of Transformer models, highlighting its application in vision.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spatial Grounding",
        "canonical": "Spatial Grounding",
        "aliases": [
          "Spatial Understanding"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding how the model improves spatial perception in robotics.",
        "novelty_score": 0.68,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "MetaWorld",
      "DP",
      "DP3"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Imitation Learning",
      "resolved_canonical": "Visual Imitation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision Foundation Models",
      "resolved_canonical": "Vision Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Proprioceptive Feedback",
      "resolved_canonical": "Proprioceptive Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Visual Geometry Grounded Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spatial Grounding",
      "resolved_canonical": "Spatial Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# VGGT-DP: Generalizable Robot Control via Vision Foundation Models

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18778.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18778](https://arxiv.org/abs/2509.18778)

## 🔗 유사한 논문
- [[2025-09-22/GP3_ A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation_20250922|GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation]] (87.6% similar)
- [[2025-09-24/Do You Need Proprioceptive States in Visuomotor Policies?_20250924|Do You Need Proprioceptive States in Visuomotor Policies?]] (86.7% similar)
- [[2025-09-23/DINOv3-Diffusion Policy_ Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning_20250923|DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning]] (86.7% similar)
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (85.6% similar)
- [[2025-09-24/PEEK_ Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies_20250924|PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies]] (85.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Transformer|Transformer]]
**🔗 Specific Connectable**: [[keywords/Proprioceptive Feedback|Proprioceptive Feedback]], [[keywords/Spatial Grounding|Spatial Grounding]]
**⚡ Unique Technical**: [[keywords/Visual Imitation Learning|Visual Imitation Learning]]
**🚀 Evolved Concepts**: [[keywords/Vision Foundation Models|Vision Foundation Models]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18778v1 Announce Type: cross 
Abstract: Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders, limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.

## 📝 요약

이 논문은 로봇이 전문가의 시연을 통해 조작 기술을 학습할 수 있는 시각 모방 학습 프레임워크를 제안합니다. 기존 접근법이 주로 정책 설계에 초점을 맞추는 반면, 이 연구는 시각 인코더의 구조와 용량을 개선하여 공간 이해와 일반화를 향상시킵니다. 생물학적 시각 시스템에서 영감을 받아, 사전 학습된 3D 인식 모델의 기하학적 사전 정보와 고유수용성 피드백을 통합한 VGGT-DP라는 비주모터 정책 프레임워크를 제안합니다. VGGT를 시각 인코더로 채택하고, 로봇의 내부 상태와 인식을 정렬하는 고유수용성 안내 시각 학습 전략을 도입하여 공간적 기반과 폐쇄 루프 제어를 개선합니다. 또한, 추론 지연을 줄이기 위해 프레임별 토큰 재사용 메커니즘을 설계하고, 정책의 강건성과 과적합 감소를 위해 무작위 토큰 가지치기를 적용합니다. 실험 결과, VGGT-DP는 정밀도가 중요한 장기 시나리오에서 기존의 강력한 기준선보다 뛰어난 성능을 보였습니다.

## 🎯 주요 포인트

- 1. VGGT-DP는 사전 학습된 3D 인식 모델의 기하학적 사전 지식과 고유수용 피드백을 통합하여 시각 모터 정책 프레임워크를 제안합니다.
- 2. VGGT-DP는 Visual Geometry Grounded Transformer (VGGT)를 시각 인코더로 사용하고, 고유수용 기반 시각 학습 전략을 도입하여 로봇의 내부 상태와 지각을 정렬합니다.
- 3. 프레임 단위 토큰 재사용 메커니즘을 설계하여 다중 뷰 토큰을 효율적인 공간 표현으로 압축하여 추론 지연을 줄입니다.
- 4. 랜덤 토큰 가지치기를 적용하여 정책의 강건성을 향상시키고 과적합을 줄입니다.
- 5. VGGT-DP는 정밀도가 중요한 상황과 장기 시나리오에서 DP 및 DP3와 같은 강력한 기준선보다 뛰어난 성능을 보입니다.


---

*Generated on 2025-09-24 14:03:36*