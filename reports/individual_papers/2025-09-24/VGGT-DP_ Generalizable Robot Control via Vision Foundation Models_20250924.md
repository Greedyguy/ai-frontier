<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:03:36.062924",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Visual Imitation Learning",
    "Vision Foundation Models",
    "Proprioceptive Feedback",
    "Transformer",
    "Spatial Grounding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Visual Imitation Learning": 0.78,
    "Vision Foundation Models": 0.82,
    "Proprioceptive Feedback": 0.79,
    "Transformer": 0.8,
    "Spatial Grounding": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Visual Imitation Learning",
        "canonical": "Visual Imitation Learning",
        "aliases": [
          "Visual Learning from Demonstrations"
        ],
        "category": "unique_technical",
        "rationale": "This concept is central to the paper's approach and links to broader themes in robotic learning.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Vision Foundation Models",
        "canonical": "Vision Foundation Models",
        "aliases": [
          "Vision Models",
          "Visual Foundation Models"
        ],
        "category": "evolved_concepts",
        "rationale": "Represents a new trend in leveraging large pre-trained models for vision tasks.",
        "novelty_score": 0.72,
        "connectivity_score": 0.75,
        "specificity_score": 0.78,
        "link_intent_score": 0.82
      },
      {
        "surface": "Proprioceptive Feedback",
        "canonical": "Proprioceptive Feedback",
        "aliases": [
          "Proprioception"
        ],
        "category": "specific_connectable",
        "rationale": "Key to understanding the integration of sensory feedback in robotic control.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.77,
        "link_intent_score": 0.79
      },
      {
        "surface": "Visual Geometry Grounded Transformer",
        "canonical": "Transformer",
        "aliases": [
          "VGGT"
        ],
        "category": "broad_technical",
        "rationale": "Links to the broader category of Transformer models, highlighting its application in vision.",
        "novelty_score": 0.55,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Spatial Grounding",
        "canonical": "Spatial Grounding",
        "aliases": [
          "Spatial Understanding"
        ],
        "category": "specific_connectable",
        "rationale": "Essential for understanding how the model improves spatial perception in robotics.",
        "novelty_score": 0.68,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "MetaWorld",
      "DP",
      "DP3"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Visual Imitation Learning",
      "resolved_canonical": "Visual Imitation Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Vision Foundation Models",
      "resolved_canonical": "Vision Foundation Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.75,
        "specificity": 0.78,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Proprioceptive Feedback",
      "resolved_canonical": "Proprioceptive Feedback",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.77,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Visual Geometry Grounded Transformer",
      "resolved_canonical": "Transformer",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Spatial Grounding",
      "resolved_canonical": "Spatial Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# VGGT-DP: Generalizable Robot Control via Vision Foundation Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18778.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18778](https://arxiv.org/abs/2509.18778)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/GP3_ A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation_20250922|GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation]] (87.6% similar)
- [[2025-09-24/Do You Need Proprioceptive States in Visuomotor Policies?_20250924|Do You Need Proprioceptive States in Visuomotor Policies?]] (86.7% similar)
- [[2025-09-23/DINOv3-Diffusion Policy_ Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning_20250923|DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning]] (86.7% similar)
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (85.6% similar)
- [[2025-09-24/PEEK_ Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies_20250924|PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies]] (85.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Transformer|Transformer]]
**ğŸ”— Specific Connectable**: [[keywords/Proprioceptive Feedback|Proprioceptive Feedback]], [[keywords/Spatial Grounding|Spatial Grounding]]
**âš¡ Unique Technical**: [[keywords/Visual Imitation Learning|Visual Imitation Learning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision Foundation Models|Vision Foundation Models]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18778v1 Announce Type: cross 
Abstract: Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders, limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¡œë´‡ì´ ì „ë¬¸ê°€ì˜ ì‹œì—°ì„ í†µí•´ ì¡°ì‘ ê¸°ìˆ ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì‹œê° ëª¨ë°© í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ë²•ì´ ì£¼ë¡œ ì •ì±… ì„¤ê³„ì— ì´ˆì ì„ ë§ì¶”ëŠ” ë°˜ë©´, ì´ ì—°êµ¬ëŠ” ì‹œê° ì¸ì½”ë”ì˜ êµ¬ì¡°ì™€ ìš©ëŸ‰ì„ ê°œì„ í•˜ì—¬ ê³µê°„ ì´í•´ì™€ ì¼ë°˜í™”ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìƒë¬¼í•™ì  ì‹œê° ì‹œìŠ¤í…œì—ì„œ ì˜ê°ì„ ë°›ì•„, ì‚¬ì „ í•™ìŠµëœ 3D ì¸ì‹ ëª¨ë¸ì˜ ê¸°í•˜í•™ì  ì‚¬ì „ ì •ë³´ì™€ ê³ ìœ ìˆ˜ìš©ì„± í”¼ë“œë°±ì„ í†µí•©í•œ VGGT-DPë¼ëŠ” ë¹„ì£¼ëª¨í„° ì •ì±… í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. VGGTë¥¼ ì‹œê° ì¸ì½”ë”ë¡œ ì±„íƒí•˜ê³ , ë¡œë´‡ì˜ ë‚´ë¶€ ìƒíƒœì™€ ì¸ì‹ì„ ì •ë ¬í•˜ëŠ” ê³ ìœ ìˆ˜ìš©ì„± ì•ˆë‚´ ì‹œê° í•™ìŠµ ì „ëµì„ ë„ì…í•˜ì—¬ ê³µê°„ì  ê¸°ë°˜ê³¼ íì‡„ ë£¨í”„ ì œì–´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ë˜í•œ, ì¶”ë¡  ì§€ì—°ì„ ì¤„ì´ê¸° ìœ„í•´ í”„ë ˆì„ë³„ í† í° ì¬ì‚¬ìš© ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ê³„í•˜ê³ , ì •ì±…ì˜ ê°•ê±´ì„±ê³¼ ê³¼ì í•© ê°ì†Œë¥¼ ìœ„í•´ ë¬´ì‘ìœ„ í† í° ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, VGGT-DPëŠ” ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ì¥ê¸° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê¸°ì¡´ì˜ ê°•ë ¥í•œ ê¸°ì¤€ì„ ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. VGGT-DPëŠ” ì‚¬ì „ í•™ìŠµëœ 3D ì¸ì‹ ëª¨ë¸ì˜ ê¸°í•˜í•™ì  ì‚¬ì „ ì§€ì‹ê³¼ ê³ ìœ ìˆ˜ìš© í”¼ë“œë°±ì„ í†µí•©í•˜ì—¬ ì‹œê° ëª¨í„° ì •ì±… í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. VGGT-DPëŠ” Visual Geometry Grounded Transformer (VGGT)ë¥¼ ì‹œê° ì¸ì½”ë”ë¡œ ì‚¬ìš©í•˜ê³ , ê³ ìœ ìˆ˜ìš© ê¸°ë°˜ ì‹œê° í•™ìŠµ ì „ëµì„ ë„ì…í•˜ì—¬ ë¡œë´‡ì˜ ë‚´ë¶€ ìƒíƒœì™€ ì§€ê°ì„ ì •ë ¬í•©ë‹ˆë‹¤.
- 3. í”„ë ˆì„ ë‹¨ìœ„ í† í° ì¬ì‚¬ìš© ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ê³„í•˜ì—¬ ë‹¤ì¤‘ ë·° í† í°ì„ íš¨ìœ¨ì ì¸ ê³µê°„ í‘œí˜„ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì¶”ë¡  ì§€ì—°ì„ ì¤„ì…ë‹ˆë‹¤.
- 4. ëœë¤ í† í° ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•˜ì—¬ ì •ì±…ì˜ ê°•ê±´ì„±ì„ í–¥ìƒì‹œí‚¤ê³  ê³¼ì í•©ì„ ì¤„ì…ë‹ˆë‹¤.
- 5. VGGT-DPëŠ” ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ìƒí™©ê³¼ ì¥ê¸° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ DP ë° DP3ì™€ ê°™ì€ ê°•ë ¥í•œ ê¸°ì¤€ì„ ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:03:36*