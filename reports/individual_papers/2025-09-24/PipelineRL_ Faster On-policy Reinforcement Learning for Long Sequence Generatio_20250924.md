<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:00:49.945979",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Large Language Model",
    "PipelineRL",
    "On-policy Training"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Large Language Model": 0.83,
    "PipelineRL": 0.78,
    "On-policy Training": 0.8
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Reinforcement Learning",
        "canonical": "Reinforcement Learning",
        "aliases": [
          "RL"
        ],
        "category": "broad_technical",
        "rationale": "Reinforcement Learning is a foundational concept that connects with various advanced AI techniques.",
        "novelty_score": 0.45,
        "connectivity_score": 0.9,
        "specificity_score": 0.6,
        "link_intent_score": 0.85
      },
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus and link to numerous NLP advancements.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.83
      },
      {
        "surface": "PipelineRL",
        "canonical": "PipelineRL",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "PipelineRL is a novel approach introduced in the paper, offering unique insights into RL efficiency.",
        "novelty_score": 0.85,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "On-policy Training",
        "canonical": "On-policy Training",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "On-policy Training is crucial for understanding the data generation strategy in RL.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Reinforcement Learning",
      "resolved_canonical": "Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.9,
        "specificity": 0.6,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.83
      }
    },
    {
      "candidate_surface": "PipelineRL",
      "resolved_canonical": "PipelineRL",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "On-policy Training",
      "resolved_canonical": "On-policy Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    }
  ]
}
-->

# PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19128.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.19128](https://arxiv.org/abs/2509.19128)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-24/Reinforcement Learning on Pre-Training Data_20250924|Reinforcement Learning on Pre-Training Data]] (89.3% similar)
- [[2025-09-24/APRIL_ Active Partial Rollouts in Reinforcement Learning to tame long-tail generation_20250924|APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation]] (88.2% similar)
- [[2025-09-22/RLinf_ Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation_20250922|RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation]] (86.9% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (86.4% similar)
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/On-policy Training|On-policy Training]]
**âš¡ Unique Technical**: [[keywords/PipelineRL|PipelineRL]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19128v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining high AI accelerator utilization without generating stale, off-policy data that harms common RL algorithms. This paper introduces PipelineRL, an approach designed to achieve a superior trade-off between hardware efficiency and data on-policyness for LLM training. PipelineRL employs concurrent asynchronous data generation and model training, distinguished by the novel in-flight weight updates. This mechanism allows the LLM generation engine to receive updated model weights with minimal interruption during the generation of token sequences, thereby maximizing both the accelerator utilization and the freshness of training data. Experiments conducted on long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL achieves approximately $\sim 2x$ faster learning compared to conventional RL baselines while maintaining highly on-policy training data. A scalable and modular open-source implementation of PipelineRL is also released as a key contribution.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•˜ê¸° ìœ„í•´ ê°•í™” í•™ìŠµ(RL)ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ RL ë°©ë²•ì˜ í™•ì¥ì—ëŠ” AI ê°€ì†ê¸°ì˜ íš¨ìœ¨ì  í™œìš©ê³¼ ì˜¤ë˜ëœ ë°ì´í„° ìƒì„± ë¬¸ì œë¡œ ì¸í•œ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì€ PipelineRLì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë™ì‹œ ë¹„ë™ê¸° ë°ì´í„° ìƒì„±ê³¼ ëª¨ë¸ í›ˆë ¨ì„ í†µí•´ í•˜ë“œì›¨ì–´ íš¨ìœ¨ì„±ê³¼ ë°ì´í„°ì˜ ìµœì‹ ì„±ì„ ë™ì‹œì— ë†’ì…ë‹ˆë‹¤. íŠ¹íˆ, ìƒˆë¡œìš´ 'in-flight' ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬, í† í° ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ì—ë„ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, 128ê°œì˜ H100 GPUë¥¼ ì‚¬ìš©í•œ ì¥ê¸° ì¶”ë¡  ì‘ì—…ì—ì„œ PipelineRLì€ ê¸°ì¡´ RL ê¸°ì¤€ë³´ë‹¤ ì•½ 2ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„ë¥¼ ë³´ì´ë©°, ë°ì´í„°ì˜ ìµœì‹ ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ë˜í•œ, í™•ì¥ ê°€ëŠ¥í•˜ê³  ëª¨ë“ˆí™”ëœ ì˜¤í”ˆ ì†ŒìŠ¤ êµ¬í˜„ë„ ì œê³µë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. PipelineRLì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì„ ìœ„í•œ í•˜ë“œì›¨ì–´ íš¨ìœ¨ì„±ê³¼ ë°ì´í„°ì˜ ì •ì±… ì¤€ìˆ˜ì„±ì„ ê· í˜• ìˆê²Œ í–¥ìƒì‹œí‚¤ëŠ” ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. ì´ ë°©ë²•ì€ ë™ì‹œ ë¹„ë™ê¸° ë°ì´í„° ìƒì„±ê³¼ ëª¨ë¸ í›ˆë ¨ì„ í†µí•´, ìƒˆë¡œìš´ ë¹„í–‰ ì¤‘ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì˜ ì‹ ì„ ë„ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
- 3. 128ê°œì˜ H100 GPUë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ì—ì„œ PipelineRLì€ ê¸°ì¡´ ê°•í™” í•™ìŠµ(RL) ê¸°ì¤€ë³´ë‹¤ ì•½ 2ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- 4. PipelineRLì˜ í™•ì¥ ê°€ëŠ¥í•˜ê³  ëª¨ë“ˆí™”ëœ ì˜¤í”ˆ ì†ŒìŠ¤ êµ¬í˜„ì´ ì£¼ìš” ê¸°ì—¬ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:00:49*