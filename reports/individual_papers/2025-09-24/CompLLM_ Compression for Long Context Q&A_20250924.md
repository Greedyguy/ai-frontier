<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:45:49.025378",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Attention Mechanism",
    "Context Compression",
    "CompLLM",
    "Time To First Token"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Attention Mechanism": 0.88,
    "Context Compression": 0.8,
    "CompLLM": 0.78,
    "Time To First Token": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLMs"
        ],
        "category": "broad_technical",
        "rationale": "Large Language Models are central to the paper's focus on context compression, linking to broader discussions in NLP.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "self-attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "self-attention"
        ],
        "category": "specific_connectable",
        "rationale": "Self-attention is a key component of Transformers, relevant for understanding the computational challenges addressed.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.8,
        "link_intent_score": 0.88
      },
      {
        "surface": "context compression",
        "canonical": "Context Compression",
        "aliases": [
          "soft context compression"
        ],
        "category": "unique_technical",
        "rationale": "The paper introduces a novel approach to context compression, which is central to its contribution.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "CompLLM",
        "canonical": "CompLLM",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "CompLLM is the specific technique introduced in the paper, representing a unique contribution.",
        "novelty_score": 0.8,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Time To First Token",
        "canonical": "Time To First Token",
        "aliases": [
          "TTFT"
        ],
        "category": "unique_technical",
        "rationale": "Time To First Token is a performance metric used to evaluate the efficiency of the proposed method.",
        "novelty_score": 0.7,
        "connectivity_score": 0.6,
        "specificity_score": 0.75,
        "link_intent_score": 0.72
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "self-attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.8,
        "link_intent": 0.88
      }
    },
    {
      "candidate_surface": "context compression",
      "resolved_canonical": "Context Compression",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "CompLLM",
      "resolved_canonical": "CompLLM",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Time To First Token",
      "resolved_canonical": "Time To First Token",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.6,
        "specificity": 0.75,
        "link_intent": 0.72
      }
    }
  ]
}
-->

# CompLLM: Compression for Long Context Q&A

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19228.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.19228](https://arxiv.org/abs/2509.19228)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/AttnComp_ Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation_20250923|AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation]] (89.0% similar)
- [[2025-09-22/KVCompose_ Efficient Structured KV Cache Compression with Composite Tokens_20250922|KVCompose: Efficient Structured KV Cache Compression with Composite Tokens]] (86.6% similar)
- [[2025-09-24/When Long Helps Short_ How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models_20250924|When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models]] (86.2% similar)
- [[2025-09-23/Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning_20250923|Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning]] (85.3% similar)
- [[2025-09-24/LightThinker_ Thinking Step-by-Step Compression_20250924|LightThinker: Thinking Step-by-Step Compression]] (85.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]]
**âš¡ Unique Technical**: [[keywords/Context Compression|Context Compression]], [[keywords/CompLLM|CompLLM]], [[keywords/Time To First Token|Time To First Token]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19228v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê¸´ ë¬¸ë§¥ì„ ì²˜ë¦¬í•  ë•Œì˜ ê³„ì‚°ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ CompLLMì´ë¼ëŠ” ì†Œí”„íŠ¸ ì••ì¶• ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë¬¸ë§¥ì„ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì••ì¶•í•˜ì—¬ ê³„ì‚° ì¬ì‚¬ìš©ì´ ì–´ë ¤ì› ìœ¼ë‚˜, CompLLMì€ ë¬¸ë§¥ì„ ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ì–´ ë…ë¦½ì ìœ¼ë¡œ ì••ì¶•í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì••ì¶• ë‹¨ê³„ê°€ ë¬¸ë§¥ ê¸¸ì´ì— ì„ í˜•ì ìœ¼ë¡œ í™•ì¥ë˜ê³ , ì§§ì€ ì‹œí€€ìŠ¤ì— í›ˆë ¨ëœ ëª¨ë¸ì´ ê¸´ ë¬¸ë§¥ì— ì¼ë°˜í™”í•  ìˆ˜ ìˆìœ¼ë©°, ì••ì¶•ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìºì‹±í•˜ì—¬ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, CompLLMì€ 2ë°° ì••ì¶•ë¥ ì—ì„œ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ì‹œ ì´ˆê¸° í† í° ìƒì„± ì‹œê°„ì„ ìµœëŒ€ 4ë°° ë‹¨ì¶•í•˜ê³ , KV ìºì‹œ í¬ê¸°ë¥¼ 50% ì¤„ì´ë©°, ë¹„ì••ì¶• ë¬¸ë§¥ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ê³ , ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ì´ë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. CompLLMì€ ë¬¸ë§¥ì„ ë…ë¦½ì ì¸ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ì–´ ì••ì¶•í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì••ì¶• ë‹¨ê³„ê°€ ë¬¸ë§¥ ê¸¸ì´ì— ì„ í˜•ì ìœ¼ë¡œ í™•ì¥ë©ë‹ˆë‹¤.
- 2. ì´ ê¸°ìˆ ì€ ì§§ì€ ì‹œí€€ìŠ¤ì— í›ˆë ¨ëœ ëª¨ë¸ì´ 100k í† í°ì˜ ê¸´ ë¬¸ë§¥ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆë„ë¡ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 3. ì••ì¶•ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìºì‹œí•˜ì—¬ ë‹¤ë¥¸ ì¿¼ë¦¬ì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¬ì‚¬ìš©ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- 4. CompLLMì€ 2ë°° ì••ì¶•ë¥ ë¡œ ê¸´ ë¬¸ë§¥ì—ì„œ ì²« ë²ˆì§¸ í† í° ìƒì„± ì‹œê°„ì„ ìµœëŒ€ 4ë°°ê¹Œì§€ ë‹¨ì¶•í•˜ê³  KV ìºì‹œ í¬ê¸°ë¥¼ 50% ì¤„ì…ë‹ˆë‹¤.
- 5. ì••ì¶•ë˜ì§€ ì•Šì€ ë¬¸ë§¥ê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ê±°ë‚˜ ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:45:49*