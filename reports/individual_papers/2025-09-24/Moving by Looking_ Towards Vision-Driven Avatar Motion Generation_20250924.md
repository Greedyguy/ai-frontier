<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:20:40.448890",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Egocentric Vision",
    "Human-like Avatar",
    "Motion Prior Model",
    "Q-learning",
    "Obstacle Avoidance"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Egocentric Vision": 0.78,
    "Human-like Avatar": 0.77,
    "Motion Prior Model": 0.75,
    "Q-learning": 0.8,
    "Obstacle Avoidance": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "egocentric vision",
        "canonical": "Egocentric Vision",
        "aliases": [
          "first-person vision",
          "ego-vision"
        ],
        "category": "unique_technical",
        "rationale": "Egocentric vision is central to the paper's approach and is a unique concept that enhances understanding of vision-driven motion.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "human-like avatar",
        "canonical": "Human-like Avatar",
        "aliases": [
          "avatar",
          "virtual human"
        ],
        "category": "unique_technical",
        "rationale": "The concept of human-like avatars is crucial for linking discussions on virtual representations and behavior modeling.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "motion prior model",
        "canonical": "Motion Prior Model",
        "aliases": [
          "motion model",
          "prior model"
        ],
        "category": "unique_technical",
        "rationale": "This model is a key component in the paper's methodology, providing a basis for linking to motion generation techniques.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "Q-learning",
        "canonical": "Q-learning",
        "aliases": [
          "Q learning",
          "reinforcement learning"
        ],
        "category": "broad_technical",
        "rationale": "Q-learning is a fundamental technique in reinforcement learning, relevant for linking to AI training methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "obstacle avoidance",
        "canonical": "Obstacle Avoidance",
        "aliases": [
          "collision avoidance",
          "obstacle navigation"
        ],
        "category": "specific_connectable",
        "rationale": "Obstacle avoidance is a specific behavior that connects to broader discussions on navigation and interaction in virtual environments.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "perception",
      "scene",
      "training avatars"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "egocentric vision",
      "resolved_canonical": "Egocentric Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "human-like avatar",
      "resolved_canonical": "Human-like Avatar",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "motion prior model",
      "resolved_canonical": "Motion Prior Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Q-learning",
      "resolved_canonical": "Q-learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "obstacle avoidance",
      "resolved_canonical": "Obstacle Avoidance",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Moving by Looking: Towards Vision-Driven Avatar Motion Generation

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19259.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19259](https://arxiv.org/abs/2509.19259)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (83.7% similar)
- [[2025-09-23/How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control_20250923|How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control]] (82.9% similar)
- [[2025-09-19/FMGS-Avatar_ Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction_20250919|FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction]] (82.9% similar)
- [[2025-09-18/Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation_20250918|Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation]] (82.7% similar)
- [[2025-09-24/Audio-Driven Universal Gaussian Head Avatars_20250924|Audio-Driven Universal Gaussian Head Avatars]] (82.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Q-learning|Q-learning]]
**ğŸ”— Specific Connectable**: [[keywords/Obstacle Avoidance|Obstacle Avoidance]]
**âš¡ Unique Technical**: [[keywords/Egocentric Vision|Egocentric Vision]], [[keywords/Human-like Avatar|Human-like Avatar]], [[keywords/Motion Prior Model|Motion Prior Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19259v1 Announce Type: new 
Abstract: The way we perceive the world fundamentally shapes how we move, whether it is how we navigate in a room or how we interact with other humans. Current human motion generation methods, neglect this interdependency and use task-specific ``perception'' that differs radically from that of humans. We argue that the generation of human-like avatar behavior requires human-like perception. Consequently, in this work we present CLOPS, the first human avatar that solely uses egocentric vision to perceive its surroundings and navigate. Using vision as the primary driver of motion however, gives rise to a significant challenge for training avatars: existing datasets have either isolated human motion, without the context of a scene, or lack scale. We overcome this challenge by decoupling the learning of low-level motion skills from learning of high-level control that maps visual input to motion. First, we train a motion prior model on a large motion capture dataset. Then, a policy is trained using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior. Our experiments empirically demonstrate that egocentric vision can give rise to human-like motion characteristics in our avatars. For example, the avatars walk such that they avoid obstacles present in their visual field. These findings suggest that equipping avatars with human-like sensors, particularly egocentric vision, holds promise for training avatars that behave like humans.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ ì•„ë°”íƒ€ì˜ í–‰ë™ ìƒì„±ì— ìˆì–´ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê°ì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ë©°, ì´ë¥¼ ìœ„í•´ CLOPSë¼ëŠ” ì•„ë°”íƒ€ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. CLOPSëŠ” ì£¼ê´€ì  ì‹œê°ì„ í†µí•´ ì£¼ë³€ í™˜ê²½ì„ ì¸ì‹í•˜ê³  ì´ë™í•˜ëŠ” ìµœì´ˆì˜ ì•„ë°”íƒ€ë¡œ, ê¸°ì¡´ ë°ì´í„°ì…‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì €ìˆ˜ì¤€ì˜ ìš´ë™ ê¸°ìˆ  í•™ìŠµê³¼ ì‹œê° ì…ë ¥ì„ ìš´ë™ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê³ ìˆ˜ì¤€ ì œì–´ í•™ìŠµì„ ë¶„ë¦¬í•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ëª¨ì…˜ ìº¡ì²˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ì…˜ í”„ë¼ì´ì–´ ëª¨ë¸ì„ í›ˆë ¨í•œ í›„, Q-ëŸ¬ë‹ì„ ì‚¬ìš©í•´ ì‹œê° ì…ë ¥ì„ ê³ ìˆ˜ì¤€ ì œì–´ ëª…ë ¹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì •ì±…ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì£¼ê´€ì  ì‹œê°ì„ í†µí•´ ì•„ë°”íƒ€ê°€ ì¥ì• ë¬¼ì„ í”¼í•˜ëŠ” ë“± ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìš´ë™ íŠ¹ì„±ì„ ë³´ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì„¼ì„œë¥¼ ê°–ì¶˜ ì•„ë°”íƒ€ê°€ ì¸ê°„ì²˜ëŸ¼ í–‰ë™í•  ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì¸ê°„ ì•„ë°”íƒ€ì˜ í–‰ë™ ìƒì„±ì—ëŠ” ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì§€ê°ì´ í•„ìš”í•˜ë‹¤ëŠ” ì£¼ì¥ì„ ì œì‹œí•©ë‹ˆë‹¤.
- 2. CLOPSëŠ” ìµœì´ˆë¡œ ìì•„ ì¤‘ì‹¬ ì‹œê°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì£¼ë³€ í™˜ê²½ì„ ì¸ì‹í•˜ê³  íƒìƒ‰í•˜ëŠ” ì¸ê°„ ì•„ë°”íƒ€ì…ë‹ˆë‹¤.
- 3. ì €ìëŠ” ì €ìˆ˜ì¤€ì˜ ìš´ë™ ê¸°ìˆ  í•™ìŠµê³¼ ì‹œê° ì…ë ¥ì„ ìš´ë™ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê³ ìˆ˜ì¤€ ì œì–´ í•™ìŠµì„ ë¶„ë¦¬í•˜ì—¬ í›ˆë ¨ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- 4. ìì•„ ì¤‘ì‹¬ ì‹œê° ì…ë ¥ì„ ê³ ìˆ˜ì¤€ ì œì–´ ëª…ë ¹ìœ¼ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ Q-ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ì •ì±…ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
- 5. ì‹¤í—˜ ê²°ê³¼, ìì•„ ì¤‘ì‹¬ ì‹œê°ì´ ì•„ë°”íƒ€ì—ê²Œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìš´ë™ íŠ¹ì„±ì„ ë¶€ì—¬í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 16:20:40*