<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T16:20:40.448890",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Egocentric Vision",
    "Human-like Avatar",
    "Motion Prior Model",
    "Q-learning",
    "Obstacle Avoidance"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Egocentric Vision": 0.78,
    "Human-like Avatar": 0.77,
    "Motion Prior Model": 0.75,
    "Q-learning": 0.8,
    "Obstacle Avoidance": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "egocentric vision",
        "canonical": "Egocentric Vision",
        "aliases": [
          "first-person vision",
          "ego-vision"
        ],
        "category": "unique_technical",
        "rationale": "Egocentric vision is central to the paper's approach and is a unique concept that enhances understanding of vision-driven motion.",
        "novelty_score": 0.75,
        "connectivity_score": 0.68,
        "specificity_score": 0.85,
        "link_intent_score": 0.78
      },
      {
        "surface": "human-like avatar",
        "canonical": "Human-like Avatar",
        "aliases": [
          "avatar",
          "virtual human"
        ],
        "category": "unique_technical",
        "rationale": "The concept of human-like avatars is crucial for linking discussions on virtual representations and behavior modeling.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.8,
        "link_intent_score": 0.77
      },
      {
        "surface": "motion prior model",
        "canonical": "Motion Prior Model",
        "aliases": [
          "motion model",
          "prior model"
        ],
        "category": "unique_technical",
        "rationale": "This model is a key component in the paper's methodology, providing a basis for linking to motion generation techniques.",
        "novelty_score": 0.68,
        "connectivity_score": 0.72,
        "specificity_score": 0.82,
        "link_intent_score": 0.75
      },
      {
        "surface": "Q-learning",
        "canonical": "Q-learning",
        "aliases": [
          "Q learning",
          "reinforcement learning"
        ],
        "category": "broad_technical",
        "rationale": "Q-learning is a fundamental technique in reinforcement learning, relevant for linking to AI training methods.",
        "novelty_score": 0.4,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "obstacle avoidance",
        "canonical": "Obstacle Avoidance",
        "aliases": [
          "collision avoidance",
          "obstacle navigation"
        ],
        "category": "specific_connectable",
        "rationale": "Obstacle avoidance is a specific behavior that connects to broader discussions on navigation and interaction in virtual environments.",
        "novelty_score": 0.55,
        "connectivity_score": 0.78,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "perception",
      "scene",
      "training avatars"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "egocentric vision",
      "resolved_canonical": "Egocentric Vision",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.68,
        "specificity": 0.85,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "human-like avatar",
      "resolved_canonical": "Human-like Avatar",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.8,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "motion prior model",
      "resolved_canonical": "Motion Prior Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.72,
        "specificity": 0.82,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Q-learning",
      "resolved_canonical": "Q-learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "obstacle avoidance",
      "resolved_canonical": "Obstacle Avoidance",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.78,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Moving by Looking: Towards Vision-Driven Avatar Motion Generation

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CV|cs.CV]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19259.pdf)
**Category**: cs.CV
**Published**: 2025-09-24
**ArXiv ID**: [2509.19259](https://arxiv.org/abs/2509.19259)

## 🔗 유사한 논문
- [[2025-09-23/Look, Focus, Act_ Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers_20250923|Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers]] (83.7% similar)
- [[2025-09-23/How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control_20250923|How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control]] (82.9% similar)
- [[2025-09-19/FMGS-Avatar_ Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction_20250919|FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction]] (82.9% similar)
- [[2025-09-18/Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation_20250918|Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation]] (82.7% similar)
- [[2025-09-24/Audio-Driven Universal Gaussian Head Avatars_20250924|Audio-Driven Universal Gaussian Head Avatars]] (82.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Q-learning|Q-learning]]
**🔗 Specific Connectable**: [[keywords/Obstacle Avoidance|Obstacle Avoidance]]
**⚡ Unique Technical**: [[keywords/Egocentric Vision|Egocentric Vision]], [[keywords/Human-like Avatar|Human-like Avatar]], [[keywords/Motion Prior Model|Motion Prior Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.19259v1 Announce Type: new 
Abstract: The way we perceive the world fundamentally shapes how we move, whether it is how we navigate in a room or how we interact with other humans. Current human motion generation methods, neglect this interdependency and use task-specific ``perception'' that differs radically from that of humans. We argue that the generation of human-like avatar behavior requires human-like perception. Consequently, in this work we present CLOPS, the first human avatar that solely uses egocentric vision to perceive its surroundings and navigate. Using vision as the primary driver of motion however, gives rise to a significant challenge for training avatars: existing datasets have either isolated human motion, without the context of a scene, or lack scale. We overcome this challenge by decoupling the learning of low-level motion skills from learning of high-level control that maps visual input to motion. First, we train a motion prior model on a large motion capture dataset. Then, a policy is trained using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior. Our experiments empirically demonstrate that egocentric vision can give rise to human-like motion characteristics in our avatars. For example, the avatars walk such that they avoid obstacles present in their visual field. These findings suggest that equipping avatars with human-like sensors, particularly egocentric vision, holds promise for training avatars that behave like humans.

## 📝 요약

이 논문은 인간 아바타의 행동 생성에 있어 인간과 유사한 지각이 필요하다는 점을 강조하며, 이를 위해 CLOPS라는 아바타를 제안합니다. CLOPS는 주관적 시각을 통해 주변 환경을 인식하고 이동하는 최초의 아바타로, 기존 데이터셋의 한계를 극복하기 위해 저수준의 운동 기술 학습과 시각 입력을 운동으로 매핑하는 고수준 제어 학습을 분리합니다. 대규모 모션 캡처 데이터셋으로 모션 프라이어 모델을 훈련한 후, Q-러닝을 사용해 시각 입력을 고수준 제어 명령으로 변환하는 정책을 훈련합니다. 실험 결과, 주관적 시각을 통해 아바타가 장애물을 피하는 등 인간과 유사한 운동 특성을 보임을 확인했습니다. 이러한 결과는 인간과 유사한 센서를 갖춘 아바타가 인간처럼 행동할 가능성을 시사합니다.

## 🎯 주요 포인트

- 1. 인간 아바타의 행동 생성에는 인간과 유사한 지각이 필요하다는 주장을 제시합니다.
- 2. CLOPS는 최초로 자아 중심 시각만을 사용하여 주변 환경을 인식하고 탐색하는 인간 아바타입니다.
- 3. 저자는 저수준의 운동 기술 학습과 시각 입력을 운동으로 매핑하는 고수준 제어 학습을 분리하여 훈련 문제를 해결합니다.
- 4. 자아 중심 시각 입력을 고수준 제어 명령으로 매핑하기 위해 Q-러닝을 사용하여 정책을 훈련합니다.
- 5. 실험 결과, 자아 중심 시각이 아바타에게 인간과 유사한 운동 특성을 부여할 수 있음을 입증합니다.


---

*Generated on 2025-09-24 16:20:40*