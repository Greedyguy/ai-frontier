<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T15:38:20.964558",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Model",
    "Speculative Decoding",
    "Parameter Offloading",
    "SubSpec"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Large Language Model": 0.85,
    "Speculative Decoding": 0.78,
    "Parameter Offloading": 0.72,
    "SubSpec": 0.79
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Large Language Models",
        "canonical": "Large Language Model",
        "aliases": [
          "LLM",
          "Large Language Models"
        ],
        "category": "broad_technical",
        "rationale": "Central to the paper's focus on accelerating large language models, providing strong links to related research in NLP.",
        "novelty_score": 0.3,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.85
      },
      {
        "surface": "Speculative Decoding",
        "canonical": "Speculative Decoding",
        "aliases": [
          "Speculative Decoding"
        ],
        "category": "unique_technical",
        "rationale": "A novel technique introduced in the paper that enhances the performance of LLMs, offering a unique connection point.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Parameter Offloading",
        "canonical": "Parameter Offloading",
        "aliases": [
          "Offloading",
          "Parameter Offloading"
        ],
        "category": "specific_connectable",
        "rationale": "Key concept in the paper's methodology for managing memory constraints in LLM deployment.",
        "novelty_score": 0.55,
        "connectivity_score": 0.75,
        "specificity_score": 0.7,
        "link_intent_score": 0.72
      },
      {
        "surface": "SubSpec",
        "canonical": "SubSpec",
        "aliases": [
          "SubSpec"
        ],
        "category": "unique_technical",
        "rationale": "A specific method proposed by the authors, crucial for understanding the paper's contributions.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.85,
        "link_intent_score": 0.79
      }
    ],
    "ban_list_suggestions": [
      "method",
      "experiment",
      "performance"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Large Language Models",
      "resolved_canonical": "Large Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.3,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Speculative Decoding",
      "resolved_canonical": "Speculative Decoding",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Parameter Offloading",
      "resolved_canonical": "Parameter Offloading",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.75,
        "specificity": 0.7,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "SubSpec",
      "resolved_canonical": "SubSpec",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.85,
        "link_intent": 0.79
      }
    }
  ]
}
-->

# Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.CL|cs.CL]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18344.pdf)
**Category**: cs.CL
**Published**: 2025-09-24
**ArXiv ID**: [2509.18344](https://arxiv.org/abs/2509.18344)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/SpecVLM_ Fast Speculative Decoding in Vision-Language Models_20250923|SpecVLM: Fast Speculative Decoding in Vision-Language Models]] (89.6% similar)
- [[2025-09-22/ViSpec_ Accelerating Vision-Language Models with Vision-Aware Speculative Decoding_20250922|ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding]] (88.3% similar)
- [[2025-09-24/SBVR_ Summation of BitVector Representation for Efficient LLM Quantization_20250924|SBVR: Summation of BitVector Representation for Efficient LLM Quantization]] (87.9% similar)
- [[2025-09-23/70% Size, 100% Accuracy_ Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float_20250923|70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float]] (86.6% similar)
- [[2025-09-22/IMPQ_ Interaction-Aware Layerwise Mixed Precision Quantization for LLMs_20250922|IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs]] (86.4% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Large Language Model|Large Language Model]]
**ğŸ”— Specific Connectable**: [[keywords/Parameter Offloading|Parameter Offloading]]
**âš¡ Unique Technical**: [[keywords/Speculative Decoding|Speculative Decoding]], [[keywords/SubSpec|SubSpec]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18344v1 Announce Type: new 
Abstract: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë©”ëª¨ë¦¬ê°€ ì œí•œëœ ì†Œë¹„ììš© GPUì—ì„œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°°í¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ëª¨ë¸ ì••ì¶•ì€ í’ˆì§ˆ ì €í•˜ë¥¼, íŒŒë¼ë¯¸í„° ì˜¤í”„ë¡œë”©ì€ ëŠë¦° ì¶”ë¡  ì†ë„ë¥¼ ì´ˆë˜í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ SubSpecì´ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. SubSpecì€ ì˜¤í”„ë¡œë”©ëœ LLMì˜ ì¼ë¶€ë¥¼ ì €ë¹„íŠ¸ ì–‘ìí™”í•˜ì—¬ ëŒ€ì²´ ë ˆì´ì–´ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ê³ ë„ë¡œ ì •ë ¬ëœ ì´ˆì•ˆ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì¶”ê°€ í›ˆë ¨ ì—†ì´ë„ ë†’ì€ í† í° ìˆ˜ìš© ê¸¸ì´ë¥¼ ë‹¬ì„±í•˜ë©°, GPUì— ìƒì£¼í•˜ëŠ” ë ˆì´ì–´ì™€ KV-ìºì‹œë¥¼ ê³µìœ í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì…ë‹ˆë‹¤. SubSpecì€ Qwen2.5 7B ëª¨ë¸ì—ì„œ 9.1ë°°, Qwen2.5 32B ëª¨ë¸ì—ì„œ í‰ê·  12.5ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í° ëª¨ë¸ í¬ê¸°ëŠ” ë©”ëª¨ë¦¬ê°€ ì œí•œëœ ì†Œë¹„ììš© GPUì—ì„œì˜ ë°°í¬ë¥¼ ì–´ë µê²Œ í•©ë‹ˆë‹¤.
- 2. ì¶”ì¸¡ ë””ì½”ë”©ì€ ë¹ ë¥¸ ì´ˆì•ˆ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì´ˆì•ˆ í† í°ì„ ì œì•ˆí•˜ê³ , ì´ë¥¼ ëª©í‘œ LLMì´ ë³‘ë ¬ë¡œ ê²€ì¦í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ ì˜¤í”„ë¡œë“œë¥¼ ê°€ì†í™”í•˜ëŠ” ìœ ë§í•œ ë°©ë²•ì…ë‹ˆë‹¤.
- 3. SubSpecì€ ì†ì‹¤ ì—†ì´ í›ˆë ¨ì´ í•„ìš” ì—†ëŠ” ë°©ì‹ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ì˜¤í”„ë¡œë“œë¥¼ ê°€ì†í™”í•˜ëŠ” í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. SubSpecì€ ì˜¤í”„ë¡œë“œëœ ëª©í‘œ LLM ë¶€ë¶„ì—ì„œ ì €ë¹„íŠ¸ ì–‘ìí™” ëŒ€ì²´ ë ˆì´ì–´ë¥¼ ìƒì„±í•˜ì—¬ ë†’ì€ ì •ë ¬ì„±ì„ ê°€ì§„ ì´ˆì•ˆ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
- 5. SubSpecì€ Qwen2.5 7B ëª¨ë¸ì—ì„œ 9.1ë°°, Qwen2.5 32B ëª¨ë¸ì—ì„œ í‰ê·  12.5ë°°ì˜ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 15:38:20*