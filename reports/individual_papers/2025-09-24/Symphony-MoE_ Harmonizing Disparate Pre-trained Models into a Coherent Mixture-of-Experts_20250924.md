<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:53:13.827151",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Pre-trained Models",
    "Activation-based Functional Alignment",
    "Router Training",
    "Multi-domain Tasks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.85,
    "Pre-trained Models": 0.78,
    "Activation-based Functional Alignment": 0.8,
    "Router Training": 0.75,
    "Multi-domain Tasks": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "Mixture-of-Experts is a key concept in scalable model architectures, facilitating connections to various expert model discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "pre-trained models",
        "canonical": "Pre-trained Models",
        "aliases": [
          "pretrained models"
        ],
        "category": "broad_technical",
        "rationale": "Pre-trained models are foundational in modern machine learning, linking to discussions on model initialization and transfer learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "activation-based functional alignment",
        "canonical": "Activation-based Functional Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This novel technique addresses parameter misalignment, offering unique insights into model harmonization strategies.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "router training",
        "canonical": "Router Training",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Router training is crucial for coordinating expert models, linking to optimization and training strategies.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "multi-domain tasks",
        "canonical": "Multi-domain Tasks",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Multi-domain tasks highlight the versatility of MoE models, connecting to discussions on generalization and adaptability.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "upcycling",
      "shared backbone"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "pre-trained models",
      "resolved_canonical": "Pre-trained Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "activation-based functional alignment",
      "resolved_canonical": "Activation-based Functional Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "router training",
      "resolved_canonical": "Router Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "multi-domain tasks",
      "resolved_canonical": "Multi-domain Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18542.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18542](https://arxiv.org/abs/2509.18542)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (88.6% similar)
- [[2025-09-23/Dynamic Expert Specialization_ Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation_20250923|Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation]] (88.4% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (86.5% similar)
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (86.3% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (85.1% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Pre-trained Models|Pre-trained Models]]
**ğŸ”— Specific Connectable**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Multi-domain Tasks|Multi-domain Tasks]]
**âš¡ Unique Technical**: [[keywords/Activation-based Functional Alignment|Activation-based Functional Alignment]], [[keywords/Router Training|Router Training]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18542v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë‹¤ì–‘í•œ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì—ì„œ ì „ë¬¸ê°€ë¥¼ ê°€ì ¸ì™€ ê°•ë ¥í•œ í˜¼í•© ì „ë¬¸ê°€(MoE) ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ MoE ëª¨ë¸ì€ ë‹¨ì¼ ì‚¬ì „ í•™ìŠµ ë°€ì§‘ ëª¨ë¸ì„ ë³µì œí•˜ì—¬ ì „ë¬¸ê°€ë¥¼ êµ¬ì„±í•˜ëŠ”ë°, ì´ëŠ” ì „ë¬¸ê°€ ë‹¤ì–‘ì„±ì„ ì œí•œí•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Symphony-MoEë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ ì—¬ëŸ¬ ì„œë¡œ ë‹¤ë¥¸ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ ì „ë¬¸ê°€ë¥¼ ì¡°í™”ë¡­ê²Œ í†µí•©í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ë ˆì´ì–´ ì¸ì‹ ìœµí•© ì „ëµê³¼ í™œì„±í™” ê¸°ë°˜ ê¸°ëŠ¥ ì •ë ¬ì„ í†µí•´ í›ˆë ¨ ì—†ì´ ëª¨ë¸ ê°„ì˜ ì¡°í™”ë¥¼ ì´ë£¨ê³ , ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ê²½ëŸ‰ì˜ ë¼ìš°í„° í›ˆë ¨ì„ í†µí•´ ì „ì²´ ì•„í‚¤í…ì²˜ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ë°©ë²•ì€ ì´ì§ˆì ì¸ ì¶œì²˜ì˜ ì „ë¬¸ê°€ë¥¼ ì„±ê³µì ìœ¼ë¡œ í†µí•©í•˜ì—¬ ë‹¤ì¤‘ ë„ë©”ì¸ ì‘ì—… ë° ë¶„í¬ ì™¸ ì¼ë°˜í™”ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Mixture-of-Experts (MoE) ëª¨ë¸ì€ ëŒ€ê·œëª¨ íŒŒë¼ë¯¸í„° ì§‘í•©ì„ í¬ì†Œí•˜ê²Œ í™œì„±í™”í•˜ì—¬ ì„±ëŠ¥ì„ í™•ì¥í•˜ë©´ì„œë„ ê³„ì‚° ë¹„ìš©ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ì˜ MoE ëª¨ë¸ì€ ë‹¨ì¼ ì‚¬ì „ í•™ìŠµëœ ë°€ì§‘ ëª¨ë¸ì˜ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì „ë¬¸ê°€ë¡œ ë³µì œí•˜ëŠ” ì—…ì‚¬ì´í´ë§ ë°©ì‹ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ëŠ” ì „ë¬¸ê°€ì˜ ë‹¤ì–‘ì„±ì„ ì œí•œí•©ë‹ˆë‹¤.
- 3. ë³¸ ë…¼ë¬¸ì€ ì—¬ëŸ¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì—ì„œ ì „ë¬¸ê°€ë¥¼ ê°€ì ¸ì™€ ê°•ë ¥í•œ MoE ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. Symphony-MoEë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ì„œë¡œ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ê³µê°„ì— ìˆëŠ” ëª¨ë¸ë“¤ì„ ì¡°í™”ë¡­ê²Œ í†µí•©í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
- 5. ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ì¤‘ ë„ë©”ì¸ ì‘ì—…ê³¼ ë¶„í¬ ì™¸ ì¼ë°˜í™”ì—ì„œ ê¸°ì¡´ì˜ ê¸°ì¤€ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:53:13*