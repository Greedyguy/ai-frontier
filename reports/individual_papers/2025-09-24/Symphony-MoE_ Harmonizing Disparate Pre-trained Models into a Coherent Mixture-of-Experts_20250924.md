<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:53:13.827151",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Mixture-of-Experts",
    "Pre-trained Models",
    "Activation-based Functional Alignment",
    "Router Training",
    "Multi-domain Tasks"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Mixture-of-Experts": 0.85,
    "Pre-trained Models": 0.78,
    "Activation-based Functional Alignment": 0.8,
    "Router Training": 0.75,
    "Multi-domain Tasks": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Mixture-of-Experts",
        "canonical": "Mixture-of-Experts",
        "aliases": [
          "MoE"
        ],
        "category": "specific_connectable",
        "rationale": "Mixture-of-Experts is a key concept in scalable model architectures, facilitating connections to various expert model discussions.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.8,
        "link_intent_score": 0.85
      },
      {
        "surface": "pre-trained models",
        "canonical": "Pre-trained Models",
        "aliases": [
          "pretrained models"
        ],
        "category": "broad_technical",
        "rationale": "Pre-trained models are foundational in modern machine learning, linking to discussions on model initialization and transfer learning.",
        "novelty_score": 0.4,
        "connectivity_score": 0.9,
        "specificity_score": 0.65,
        "link_intent_score": 0.78
      },
      {
        "surface": "activation-based functional alignment",
        "canonical": "Activation-based Functional Alignment",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "This novel technique addresses parameter misalignment, offering unique insights into model harmonization strategies.",
        "novelty_score": 0.72,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.8
      },
      {
        "surface": "router training",
        "canonical": "Router Training",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Router training is crucial for coordinating expert models, linking to optimization and training strategies.",
        "novelty_score": 0.68,
        "connectivity_score": 0.7,
        "specificity_score": 0.78,
        "link_intent_score": 0.75
      },
      {
        "surface": "multi-domain tasks",
        "canonical": "Multi-domain Tasks",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Multi-domain tasks highlight the versatility of MoE models, connecting to discussions on generalization and adaptability.",
        "novelty_score": 0.5,
        "connectivity_score": 0.82,
        "specificity_score": 0.72,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "upcycling",
      "shared backbone"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Mixture-of-Experts",
      "resolved_canonical": "Mixture-of-Experts",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.8,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "pre-trained models",
      "resolved_canonical": "Pre-trained Models",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.9,
        "specificity": 0.65,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "activation-based functional alignment",
      "resolved_canonical": "Activation-based Functional Alignment",
      "decision": "linked",
      "scores": {
        "novelty": 0.72,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "router training",
      "resolved_canonical": "Router Training",
      "decision": "linked",
      "scores": {
        "novelty": 0.68,
        "connectivity": 0.7,
        "specificity": 0.78,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "multi-domain tasks",
      "resolved_canonical": "Multi-domain Tasks",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.82,
        "specificity": 0.72,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18542.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18542](https://arxiv.org/abs/2509.18542)

## 🔗 유사한 논문
- [[2025-09-23/MoEs Are Stronger than You Think_ Hyper-Parallel Inference Scaling with RoE_20250923|MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE]] (88.6% similar)
- [[2025-09-23/Dynamic Expert Specialization_ Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation_20250923|Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation]] (88.4% similar)
- [[2025-09-22/DiEP_ Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning_20250922|DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning]] (86.5% similar)
- [[2025-09-22/MoE-CE_ Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework_20250922|MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework]] (86.3% similar)
- [[2025-09-18/Semi-MoE_ Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation_20250918|Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation]] (85.1% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Pre-trained Models|Pre-trained Models]]
**🔗 Specific Connectable**: [[keywords/Mixture-of-Experts|Mixture-of-Experts]], [[keywords/Multi-domain Tasks|Multi-domain Tasks]]
**⚡ Unique Technical**: [[keywords/Activation-based Functional Alignment|Activation-based Functional Alignment]], [[keywords/Router Training|Router Training]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18542v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.

## 📝 요약

이 논문은 다양한 사전 학습 모델에서 전문가를 가져와 강력한 혼합 전문가(MoE) 모델을 구축하는 방법을 제안합니다. 기존의 MoE 모델은 단일 사전 학습 밀집 모델을 복제하여 전문가를 구성하는데, 이는 전문가 다양성을 제한합니다. 이를 해결하기 위해, Symphony-MoE라는 새로운 프레임워크를 도입하여 여러 서로 다른 사전 학습 모델의 전문가를 조화롭게 통합합니다. 첫 번째 단계에서는 레이어 인식 융합 전략과 활성화 기반 기능 정렬을 통해 훈련 없이 모델 간의 조화를 이루고, 두 번째 단계에서는 경량의 라우터 훈련을 통해 전체 아키텍처를 조정합니다. 실험 결과, 이 방법은 이질적인 출처의 전문가를 성공적으로 통합하여 다중 도메인 작업 및 분포 외 일반화에서 기존 모델을 능가하는 성능을 보였습니다.

## 🎯 주요 포인트

- 1. Mixture-of-Experts (MoE) 모델은 대규모 파라미터 집합을 희소하게 활성화하여 성능을 확장하면서도 계산 비용을 최소화합니다.
- 2. 기존의 MoE 모델은 단일 사전 학습된 밀집 모델의 피드포워드 네트워크를 전문가로 복제하는 업사이클링 방식을 사용하지만, 이는 전문가의 다양성을 제한합니다.
- 3. 본 논문은 여러 사전 학습된 모델에서 전문가를 가져와 강력한 MoE 모델을 구축하는 방법을 제안합니다.
- 4. Symphony-MoE라는 새로운 프레임워크를 통해 서로 다른 파라미터 공간에 있는 모델들을 조화롭게 통합하여 성능 저하를 방지합니다.
- 5. 제안된 방법은 다중 도메인 작업과 분포 외 일반화에서 기존의 기준 모델을 능가하는 성과를 보여줍니다.


---

*Generated on 2025-09-24 13:53:13*