<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:09:11.936428",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Neural Reward Machines",
    "Deep Learning",
    "Linear Temporal Logic",
    "Symbol Grounding"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Neural Reward Machines": 0.8,
    "Deep Learning": 0.85,
    "Linear Temporal Logic": 0.78,
    "Symbol Grounding": 0.77
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Neural Reward Machines",
        "canonical": "Neural Reward Machines",
        "aliases": [
          "NRM"
        ],
        "category": "unique_technical",
        "rationale": "Neural Reward Machines represent a novel approach in reinforcement learning that integrates symbolic reasoning with neural networks.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.8
      },
      {
        "surface": "Deep Reinforcement Learning",
        "canonical": "Deep Learning",
        "aliases": [
          "DRL"
        ],
        "category": "broad_technical",
        "rationale": "Deep Reinforcement Learning is a key area of deep learning that enhances the connectivity with other machine learning concepts.",
        "novelty_score": 0.4,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.85
      },
      {
        "surface": "Linear Temporal Logic",
        "canonical": "Linear Temporal Logic",
        "aliases": [
          "LTL"
        ],
        "category": "specific_connectable",
        "rationale": "Linear Temporal Logic is crucial for expressing temporally extended objectives in reinforcement learning, linking to formal methods.",
        "novelty_score": 0.6,
        "connectivity_score": 0.75,
        "specificity_score": 0.8,
        "link_intent_score": 0.78
      },
      {
        "surface": "Symbol Grounding",
        "canonical": "Symbol Grounding",
        "aliases": [
          "SG"
        ],
        "category": "specific_connectable",
        "rationale": "Symbol Grounding is essential for mapping raw observations to symbolic representations, connecting to cognitive science and AI.",
        "novelty_score": 0.65,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      }
    ],
    "ban_list_suggestions": [
      "state-action pairs",
      "prior knowledge",
      "finite and compact nature"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Neural Reward Machines",
      "resolved_canonical": "Neural Reward Machines",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Deep Reinforcement Learning",
      "resolved_canonical": "Deep Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.4,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Linear Temporal Logic",
      "resolved_canonical": "Linear Temporal Logic",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.75,
        "specificity": 0.8,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Symbol Grounding",
      "resolved_canonical": "Symbol Grounding",
      "decision": "linked",
      "scores": {
        "novelty": 0.65,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    }
  ]
}
-->

# Fully Learnable Neural Reward Machines

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.19017.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.19017](https://arxiv.org/abs/2509.19017)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/ConfClip_ Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs_20250923|ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs]] (85.9% similar)
- [[2025-09-19/Zero-Shot LLMs in Human-in-the-Loop RL_ Replacing Human Feedback for Reward Shaping_20250919|Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping]] (85.0% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (84.8% similar)
- [[2025-09-23/Reinforcement Learning Meets Large Language Models_ A Survey of Advancements and Applications Across the LLM Lifecycle_20250923|Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle]] (84.6% similar)
- [[2025-09-24/Tackling GNARLy Problems_ Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning_20250924|Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning]] (84.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Deep Learning|Deep Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Linear Temporal Logic|Linear Temporal Logic]], [[keywords/Symbol Grounding|Symbol Grounding]]
**âš¡ Unique Technical**: [[keywords/Neural Reward Machines|Neural Reward Machines]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.19017v1 Announce Type: cross 
Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant challenges, as agents must reason over entire trajectories of state-action pairs to make optimal decisions. A common strategy to address this is through symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which provide a structured way to express temporally extended objectives. However, these approaches often rely on restrictive assumptions -- such as the availability of a predefined Symbol Grounding (SG) function mapping raw observations to high-level symbolic representations, or prior knowledge of the temporal task. In this work, we propose a fully learnable version of Neural Reward Machines (NRM), which can learn both the SG function and the automaton end-to-end, removing any reliance on prior knowledge. Our approach is therefore as easily applicable as classic deep RL (DRL) approaches, while being far more explainable, because of the finite and compact nature of automata. Furthermore, we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL, our method outperforms previous approaches based on Recurrent Neural Networks (RNNs).

## ğŸ“ ìš”ì•½

ë¹„ë§ˆë¥´ì½”í”„ ê°•í™” í•™ìŠµ(RL) ê³¼ì œëŠ” ìµœì ì˜ ê²°ì •ì„ ìœ„í•´ ìƒíƒœ-í–‰ë™ ìŒì˜ ì „ì²´ ê²½ë¡œë¥¼ ê³ ë ¤í•´ì•¼ í•˜ë¯€ë¡œ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„ í˜• ì‹œê°„ ë…¼ë¦¬(LTL)ë‚˜ ì˜¤í† ë§ˆíƒ€ì™€ ê°™ì€ ìƒì§•ì  í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ëŠ” ì‚¬ì „ ì •ì˜ëœ ìƒì§•ì  í‘œí˜„ìœ¼ë¡œì˜ ë§¤í•‘ì´ë‚˜ ì‹œê°„ì  ê³¼ì œì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ì— ì˜ì¡´í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ ì‚¬ì „ ì§€ì‹ ì—†ì´ ìƒì§•ì  í‘œí˜„ê³¼ ì˜¤í† ë§ˆíƒ€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì™„ì „ í•™ìŠµ ê°€ëŠ¥í•œ ì‹ ê²½ ë³´ìƒ ê¸°ê³„(NRM)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³ ì „ì  ì‹¬ì¸µ ê°•í™” í•™ìŠµ(DRL)ë§Œí¼ ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•˜ë©´ì„œë„ ì˜¤í† ë§ˆíƒ€ì˜ ìœ í•œí•˜ê³  ê°„ê²°í•œ íŠ¹ì„± ë•ë¶„ì— ì„¤ëª… ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë˜í•œ, ì™„ì „ í•™ìŠµ ê°€ëŠ¥í•œ ë³´ìƒ ê¸°ê³„(FLNRM)ë¥¼ DRLê³¼ í†µí•©í•¨ìœ¼ë¡œì¨, ìˆœí™˜ ì‹ ê²½ë§(RNN) ê¸°ë°˜ì˜ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¹„ë§ˆë¥´ì½”í”„ ê°•í™” í•™ìŠµ(RL) ê³¼ì œëŠ” ìµœì ì˜ ê²°ì •ì„ ìœ„í•´ ìƒíƒœ-í–‰ë™ ìŒì˜ ì „ì²´ ê¶¤ì ì„ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë„ì „ ê³¼ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
- 2. ê¸°ì¡´ ì ‘ê·¼ë²•ì€ ì„ í˜• ì‹œê°„ ë…¼ë¦¬(LTL)ë‚˜ ì˜¤í† ë§ˆíƒ€ì™€ ê°™ì€ ìƒì§•ì  í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ì ìœ¼ë¡œ í™•ì¥ëœ ëª©í‘œë¥¼ í‘œí˜„í•˜ì§€ë§Œ, ì´ëŠ” ì‚¬ì „ ì •ì˜ëœ ìƒì§•ì  í‘œí˜„ ë§¤í•‘ì´ë‚˜ ì‹œê°„ì  ê³¼ì œì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ì— ì˜ì¡´í•˜ëŠ” ì œí•œì ì¸ ê°€ì •ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.
- 3. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì „ ì§€ì‹ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ìƒì§•ì  í‘œí˜„ ë§¤í•‘ê³¼ ì˜¤í† ë§ˆíƒ€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì™„ì „ í•™ìŠµ ê°€ëŠ¥í•œ ì‹ ê²½ ë³´ìƒ ê¸°ê³„(NRM)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ ê³ ì „ì ì¸ ì‹¬ì¸µ ê°•í™” í•™ìŠµ(DRL) ì ‘ê·¼ë²•ë§Œí¼ ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•˜ë©°, ì˜¤í† ë§ˆíƒ€ì˜ ìœ í•œí•˜ê³  ê°„ê²°í•œ íŠ¹ì„± ë•ë¶„ì— ë” ì„¤ëª… ê°€ëŠ¥í•©ë‹ˆë‹¤.
- 5. ì™„ì „ í•™ìŠµ ê°€ëŠ¥í•œ ë³´ìƒ ê¸°ê³„(FLNRM)ë¥¼ DRLê³¼ í†µí•©í•¨ìœ¼ë¡œì¨, ì œì•ˆëœ ë°©ë²•ì´ ìˆœí™˜ ì‹ ê²½ë§(RNN) ê¸°ë°˜ì˜ ì´ì „ ì ‘ê·¼ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í•©ë‹ˆë‹¤.


---

*Generated on 2025-09-24 14:09:11*