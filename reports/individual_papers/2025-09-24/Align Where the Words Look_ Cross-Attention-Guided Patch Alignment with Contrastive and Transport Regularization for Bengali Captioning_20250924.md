<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:47:09.714820",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Attention Mechanism",
    "Contrastive Learning",
    "Optimal Transport",
    "Bengali Captioning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Attention Mechanism": 0.8,
    "Contrastive Learning": 0.78,
    "Optimal Transport": 0.7,
    "Bengali Captioning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language",
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on grounding in low-resource languages, linking to multimodal learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross-Attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-Attention is a key mechanism used in the paper's proposed model, relevant to linking with other attention-based models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Contrastive Regularization",
        "canonical": "Contrastive Learning",
        "aliases": [
          "Contrastive Loss"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive Regularization is a core component of the model's loss function, linking to self-supervised learning techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sinkhorn-based OT",
        "canonical": "Optimal Transport",
        "aliases": [
          "Sinkhorn Algorithm"
        ],
        "category": "unique_technical",
        "rationale": "Sinkhorn-based OT is a novel approach in the paper for ensuring balanced patch correspondence, offering unique insights.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Bengali Captioning",
        "canonical": "Bengali Captioning",
        "aliases": [
          "BN Captioning"
        ],
        "category": "unique_technical",
        "rationale": "Bengali Captioning is the specific application focus of the paper, crucial for linking to language-specific NLP research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Pipeline",
      "Dataset"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Contrastive Regularization",
      "resolved_canonical": "Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sinkhorn-based OT",
      "resolved_canonical": "Optimal Transport",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Bengali Captioning",
      "resolved_canonical": "Bengali Captioning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18369.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18369](https://arxiv.org/abs/2509.18369)

## 🔗 유사한 논문
- [[2025-09-23/GraDeT-HTR_ A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer_20250923|GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer]] (84.1% similar)
- [[2025-09-22/mucAI at BAREC Shared Task 2025_ Towards Uncertainty Aware Arabic Readability Assessment_20250922|mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment]] (82.1% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.1% similar)
- [[2025-09-23/Vision Language Models Are Not (Yet) Spelling Correctors_20250923|Vision Language Models Are Not (Yet) Spelling Correctors]] (82.1% similar)
- [[2025-09-22/VLA-Mark_ A cross modal watermark for large vision-language alignment model_20250922|VLA-Mark: A cross modal watermark for large vision-language alignment model]] (82.0% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Contrastive Learning|Contrastive Learning]]
**⚡ Unique Technical**: [[keywords/Optimal Transport|Optimal Transport]], [[keywords/Bengali Captioning|Bengali Captioning]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18369v1 Announce Type: cross 
Abstract: Grounding vision--language models in low-resource languages remains challenging, as they often produce fluent text about the wrong objects. This stems from scarce paired data, translation pivots that break alignment, and English-centric pretraining that ignores target-language semantics. We address this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT yields stable visual patches, a Bengali-native mBART-50 decodes, and a lightweight bridge links the modalities. Our core novelty is a tri-loss objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch descriptors using decoder cross-attention, InfoNCE enforces global real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the real--synthetic centroid gap by 41%.

## 📝 요약

이 논문은 저자들이 저자원이 부족한 언어에서의 비전-언어 모델의 문제를 해결하기 위해 제안한 벵골어 캡션 생성 파이프라인을 소개합니다. 주요 기여는 LaBSE로 검증된 영어-벵골어 쌍과 11만 개의 이중언어 프롬프트 기반 합성 이미지를 활용한 학습입니다. MaxViT를 사용하여 안정적인 시각 패치를 생성하고, 벵골어에 특화된 mBART-50을 통해 디코딩하며, 가벼운 브리지를 통해 모달리티를 연결합니다. 핵심 방법론은 Patch-Alignment Loss (PAL), InfoNCE, Sinkhorn 기반 최적 수송(OT)을 결합한 삼중 손실 목표로, 이는 실제와 합성 패치의 정렬을 개선하고 불필요한 매칭을 줄입니다. 결과적으로 Flickr30k-1k와 MSCOCO-1k에서 BLEU-4, METEOR, BERTScore-F1에서 높은 성능을 기록하며 기존의 강력한 CE 기반 모델을 능가합니다.

## 🎯 주요 포인트

- 1. 저자들은 LaBSE로 검증된 영어-벵골어 쌍과 11만 개의 이중언어 프롬프트로 생성된 이미지로 훈련된 벵골어 캡셔닝 파이프라인을 제안합니다.
- 2. MaxViT와 벵골어 mBART-50을 활용하여 시각적 패치의 안정성과 벵골어 디코딩을 구현하고, 경량 브리지를 통해 두 모달리티를 연결합니다.
- 3. 핵심 혁신은 패치 정렬 손실(PAL), InfoNCE, Sinkhorn 기반 OT를 결합한 삼중 손실 목표로, 이는 실제와 합성 패치의 정렬을 개선하고 잘못된 매칭을 줄입니다.
- 4. 제안된 방법은 Flickr30k-1k와 MSCOCO-1k 데이터셋에서 BLEU-4, METEOR, BERTScore-F1 지표에서 강력한 성능 향상을 보여줍니다.
- 5. 이 연구는 강력한 CE 기준선을 능가하며 실제와 합성 중심 간의 격차를 41% 줄였습니다.


---

*Generated on 2025-09-24 13:47:09*