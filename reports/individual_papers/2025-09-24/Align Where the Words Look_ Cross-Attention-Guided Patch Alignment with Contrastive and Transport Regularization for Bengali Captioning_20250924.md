<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T13:47:09.714820",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Vision-Language Model",
    "Attention Mechanism",
    "Contrastive Learning",
    "Optimal Transport",
    "Bengali Captioning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Vision-Language Model": 0.85,
    "Attention Mechanism": 0.8,
    "Contrastive Learning": 0.78,
    "Optimal Transport": 0.7,
    "Bengali Captioning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Vision-Language Models",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language",
          "VLM"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language Models are central to the paper's focus on grounding in low-resource languages, linking to multimodal learning.",
        "novelty_score": 0.55,
        "connectivity_score": 0.88,
        "specificity_score": 0.72,
        "link_intent_score": 0.85
      },
      {
        "surface": "Cross-Attention",
        "canonical": "Attention Mechanism",
        "aliases": [
          "Cross-Attention Mechanism"
        ],
        "category": "specific_connectable",
        "rationale": "Cross-Attention is a key mechanism used in the paper's proposed model, relevant to linking with other attention-based models.",
        "novelty_score": 0.5,
        "connectivity_score": 0.9,
        "specificity_score": 0.7,
        "link_intent_score": 0.8
      },
      {
        "surface": "Contrastive Regularization",
        "canonical": "Contrastive Learning",
        "aliases": [
          "Contrastive Loss"
        ],
        "category": "specific_connectable",
        "rationale": "Contrastive Regularization is a core component of the model's loss function, linking to self-supervised learning techniques.",
        "novelty_score": 0.6,
        "connectivity_score": 0.85,
        "specificity_score": 0.75,
        "link_intent_score": 0.78
      },
      {
        "surface": "Sinkhorn-based OT",
        "canonical": "Optimal Transport",
        "aliases": [
          "Sinkhorn Algorithm"
        ],
        "category": "unique_technical",
        "rationale": "Sinkhorn-based OT is a novel approach in the paper for ensuring balanced patch correspondence, offering unique insights.",
        "novelty_score": 0.75,
        "connectivity_score": 0.6,
        "specificity_score": 0.8,
        "link_intent_score": 0.7
      },
      {
        "surface": "Bengali Captioning",
        "canonical": "Bengali Captioning",
        "aliases": [
          "BN Captioning"
        ],
        "category": "unique_technical",
        "rationale": "Bengali Captioning is the specific application focus of the paper, crucial for linking to language-specific NLP research.",
        "novelty_score": 0.7,
        "connectivity_score": 0.65,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Pipeline",
      "Dataset"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Vision-Language Models",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.88,
        "specificity": 0.72,
        "link_intent": 0.85
      }
    },
    {
      "candidate_surface": "Cross-Attention",
      "resolved_canonical": "Attention Mechanism",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.9,
        "specificity": 0.7,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Contrastive Regularization",
      "resolved_canonical": "Contrastive Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.85,
        "specificity": 0.75,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Sinkhorn-based OT",
      "resolved_canonical": "Optimal Transport",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.6,
        "specificity": 0.8,
        "link_intent": 0.7
      }
    },
    {
      "candidate_surface": "Bengali Captioning",
      "resolved_canonical": "Bengali Captioning",
      "decision": "linked",
      "scores": {
        "novelty": 0.7,
        "connectivity": 0.65,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18369.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2509.18369](https://arxiv.org/abs/2509.18369)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/GraDeT-HTR_ A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer_20250923|GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer]] (84.1% similar)
- [[2025-09-22/mucAI at BAREC Shared Task 2025_ Towards Uncertainty Aware Arabic Readability Assessment_20250922|mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment]] (82.1% similar)
- [[2025-09-22/Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models_20250922|Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models]] (82.1% similar)
- [[2025-09-23/Vision Language Models Are Not (Yet) Spelling Correctors_20250923|Vision Language Models Are Not (Yet) Spelling Correctors]] (82.1% similar)
- [[2025-09-22/VLA-Mark_ A cross modal watermark for large vision-language alignment model_20250922|VLA-Mark: A cross modal watermark for large vision-language alignment model]] (82.0% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Attention Mechanism|Attention Mechanism]], [[keywords/Contrastive Learning|Contrastive Learning]]
**âš¡ Unique Technical**: [[keywords/Optimal Transport|Optimal Transport]], [[keywords/Bengali Captioning|Bengali Captioning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18369v1 Announce Type: cross 
Abstract: Grounding vision--language models in low-resource languages remains challenging, as they often produce fluent text about the wrong objects. This stems from scarce paired data, translation pivots that break alignment, and English-centric pretraining that ignores target-language semantics. We address this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT yields stable visual patches, a Bengali-native mBART-50 decodes, and a lightweight bridge links the modalities. Our core novelty is a tri-loss objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch descriptors using decoder cross-attention, InfoNCE enforces global real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the real--synthetic centroid gap by 41%.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì €ìë“¤ì´ ì €ìì›ì´ ë¶€ì¡±í•œ ì–¸ì–´ì—ì„œì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆí•œ ë²µê³¨ì–´ ìº¡ì…˜ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” LaBSEë¡œ ê²€ì¦ëœ ì˜ì–´-ë²µê³¨ì–´ ìŒê³¼ 11ë§Œ ê°œì˜ ì´ì¤‘ì–¸ì–´ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í•©ì„± ì´ë¯¸ì§€ë¥¼ í™œìš©í•œ í•™ìŠµì…ë‹ˆë‹¤. MaxViTë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ ì‹œê° íŒ¨ì¹˜ë¥¼ ìƒì„±í•˜ê³ , ë²µê³¨ì–´ì— íŠ¹í™”ëœ mBART-50ì„ í†µí•´ ë””ì½”ë”©í•˜ë©°, ê°€ë²¼ìš´ ë¸Œë¦¬ì§€ë¥¼ í†µí•´ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤. í•µì‹¬ ë°©ë²•ë¡ ì€ Patch-Alignment Loss (PAL), InfoNCE, Sinkhorn ê¸°ë°˜ ìµœì  ìˆ˜ì†¡(OT)ì„ ê²°í•©í•œ ì‚¼ì¤‘ ì†ì‹¤ ëª©í‘œë¡œ, ì´ëŠ” ì‹¤ì œì™€ í•©ì„± íŒ¨ì¹˜ì˜ ì •ë ¬ì„ ê°œì„ í•˜ê³  ë¶ˆí•„ìš”í•œ ë§¤ì¹­ì„ ì¤„ì…ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ Flickr30k-1kì™€ MSCOCO-1kì—ì„œ BLEU-4, METEOR, BERTScore-F1ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©° ê¸°ì¡´ì˜ ê°•ë ¥í•œ CE ê¸°ë°˜ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì €ìë“¤ì€ LaBSEë¡œ ê²€ì¦ëœ ì˜ì–´-ë²µê³¨ì–´ ìŒê³¼ 11ë§Œ ê°œì˜ ì´ì¤‘ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€ë¡œ í›ˆë ¨ëœ ë²µê³¨ì–´ ìº¡ì…”ë‹ íŒŒì´í”„ë¼ì¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 2. MaxViTì™€ ë²µê³¨ì–´ mBART-50ì„ í™œìš©í•˜ì—¬ ì‹œê°ì  íŒ¨ì¹˜ì˜ ì•ˆì •ì„±ê³¼ ë²µê³¨ì–´ ë””ì½”ë”©ì„ êµ¬í˜„í•˜ê³ , ê²½ëŸ‰ ë¸Œë¦¬ì§€ë¥¼ í†µí•´ ë‘ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
- 3. í•µì‹¬ í˜ì‹ ì€ íŒ¨ì¹˜ ì •ë ¬ ì†ì‹¤(PAL), InfoNCE, Sinkhorn ê¸°ë°˜ OTë¥¼ ê²°í•©í•œ ì‚¼ì¤‘ ì†ì‹¤ ëª©í‘œë¡œ, ì´ëŠ” ì‹¤ì œì™€ í•©ì„± íŒ¨ì¹˜ì˜ ì •ë ¬ì„ ê°œì„ í•˜ê³  ì˜ëª»ëœ ë§¤ì¹­ì„ ì¤„ì…ë‹ˆë‹¤.
- 4. ì œì•ˆëœ ë°©ë²•ì€ Flickr30k-1kì™€ MSCOCO-1k ë°ì´í„°ì…‹ì—ì„œ BLEU-4, METEOR, BERTScore-F1 ì§€í‘œì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- 5. ì´ ì—°êµ¬ëŠ” ê°•ë ¥í•œ CE ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ë©° ì‹¤ì œì™€ í•©ì„± ì¤‘ì‹¬ ê°„ì˜ ê²©ì°¨ë¥¼ 41% ì¤„ì˜€ìŠµë‹ˆë‹¤.


---

*Generated on 2025-09-24 13:47:09*