<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:36:05.980470",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Athena Process Reward Model",
    "Multimodal Learning",
    "Process Reward Model",
    "Vision-Language Model",
    "Reward Ranked Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Athena Process Reward Model": 0.78,
    "Multimodal Learning": 0.82,
    "Process Reward Model": 0.77,
    "Vision-Language Model": 0.79,
    "Reward Ranked Fine-Tuning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Athena-PRM",
        "canonical": "Athena Process Reward Model",
        "aliases": [
          "Athena-PRM"
        ],
        "category": "unique_technical",
        "rationale": "Athena-PRM is a specific model introduced in the paper, central to its contributions and findings.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is a key aspect of the paper, linking it to broader research in integrating multiple data types.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Process Reward Model",
        "canonical": "Process Reward Model",
        "aliases": [
          "PRM"
        ],
        "category": "unique_technical",
        "rationale": "Process Reward Models are central to the paper's methodology, offering a unique approach to evaluating reasoning steps.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language models are relevant to the paper's context, bridging visual and textual data processing.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      },
      {
        "surface": "Reward ranked fine-tuning",
        "canonical": "Reward Ranked Fine-Tuning",
        "aliases": [
          "Reward ranked fine-tuning"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a novel contribution of the paper, enhancing model performance through a specific fine-tuning strategy.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Monte Carlo estimation",
      "test time scaling"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Athena-PRM",
      "resolved_canonical": "Athena Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Process Reward Model",
      "resolved_canonical": "Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Reward ranked fine-tuning",
      "resolved_canonical": "Reward Ranked Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.09532.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2506.09532](https://arxiv.org/abs/2506.09532)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Med-PRM_ Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards_20250923|Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards]] (88.8% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (87.9% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (86.2% similar)
- [[2025-09-23/SCAN_ Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning_20250923|SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning]] (84.9% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (83.5% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**âš¡ Unique Technical**: [[keywords/Athena Process Reward Model|Athena Process Reward Model]], [[keywords/Process Reward Model|Process Reward Model]], [[keywords/Reward Ranked Fine-Tuning|Reward Ranked Fine-Tuning]]
**ğŸš€ Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.09532v2 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.

## ğŸ“ ìš”ì•½

Athena-PRMì€ ë³µì¡í•œ ì¶”ë¡  ë¬¸ì œ í•´ê²° ê³¼ì •ì˜ ê° ë‹¨ê³„ì— ëŒ€í•œ ë³´ìƒ ì ìˆ˜ë¥¼ í‰ê°€í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì•½í•œ ì™„ì„±ìì™€ ê°•í•œ ì™„ì„±ì ê°„ì˜ ì˜ˆì¸¡ ì¼ê´€ì„±ì„ í™œìš©í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í”„ë¡œì„¸ìŠ¤ ë¼ë²¨ì„ ì‹ë³„í•¨ìœ¼ë¡œì¨ ê³ í’ˆì§ˆì˜ í”„ë¡œì„¸ìŠ¤ ë¼ë²¨ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. Athena-PRMì€ 5,000ê°œì˜ ìƒ˜í”Œë¡œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ORM ì´ˆê¸°í™”ì™€ ë¶€ì • ë°ì´í„° ì—…ìƒ˜í”Œë§ì„ í†µí•´ PRMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ ëª¨ë¸ì€ í…ŒìŠ¤íŠ¸ ì‹œê°„ í™•ì¥, ì¶”ë¡  ë‹¨ê³„ì˜ ì •í™•ì„± í‰ê°€, ë³´ìƒ ìˆœìœ„ ë¯¸ì„¸ ì¡°ì • ë“± ì„¸ ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê²€ì¦ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, Qwen2.5-VL-7B ì •ì±… ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ WeMathì™€ MathVistaì—ì„œ ê°ê° 10.2ì ê³¼ 7.1ì ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, VisualProcessBenchì—ì„œ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ì„ 3.9 F1 ì ìˆ˜ë¡œ ëŠ¥ê°€í•˜ë©°, ì¶”ë¡  ë‹¨ê³„ì˜ ì •í™•ì„±ì„ í‰ê°€í•˜ëŠ” ë° ìˆì–´ ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Athena-PRMì„ ë³´ìƒ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ ê°œë°œí•œ Athena-7BëŠ” ë‹¤ì„¯ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ê¸°ì¤€ì„ í¬ê²Œ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ëƒˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Athena-PRMì€ ë³µì¡í•œ ì¶”ë¡  ë¬¸ì œ í•´ê²° ê³¼ì •ì—ì„œ ê° ë‹¨ê³„ì˜ ë³´ìƒ ì ìˆ˜ë¥¼ í‰ê°€í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì´ë‹¤.
- 2. Athena-PRMì€ ì•½í•œ ì™„ì„±ìì™€ ê°•í•œ ì™„ì„±ì ê°„ì˜ ì˜ˆì¸¡ ì¼ê´€ì„±ì„ í™œìš©í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í”„ë¡œì„¸ìŠ¤ ë ˆì´ë¸”ì„ ì‹ë³„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
- 3. Athena-PRMì€ 5,000ê°œì˜ ìƒ˜í”Œë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.
- 4. ORM ì´ˆê¸°í™”ì™€ ë¶€ì • ë°ì´í„° ì—…ìƒ˜í”Œë§ì„ í†µí•´ PRMì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë‘ ê°€ì§€ ì „ëµì„ ê°œë°œí•˜ì˜€ë‹¤.
- 5. Athena-PRMì€ VisualProcessBenchì—ì„œ ìµœì²¨ë‹¨ ì„±ê³¼ë¥¼ ê¸°ë¡í•˜ë©°, ê¸°ì¡´ ì„±ê³¼ë¥¼ 3.9 F1 ì ìˆ˜ë¡œ ëŠ¥ê°€í•œë‹¤.


---

*Generated on 2025-09-24 14:36:05*