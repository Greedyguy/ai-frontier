<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:36:05.980470",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Athena Process Reward Model",
    "Multimodal Learning",
    "Process Reward Model",
    "Vision-Language Model",
    "Reward Ranked Fine-Tuning"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Athena Process Reward Model": 0.78,
    "Multimodal Learning": 0.82,
    "Process Reward Model": 0.77,
    "Vision-Language Model": 0.79,
    "Reward Ranked Fine-Tuning": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Athena-PRM",
        "canonical": "Athena Process Reward Model",
        "aliases": [
          "Athena-PRM"
        ],
        "category": "unique_technical",
        "rationale": "Athena-PRM is a specific model introduced in the paper, central to its contributions and findings.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.78
      },
      {
        "surface": "Multimodal",
        "canonical": "Multimodal Learning",
        "aliases": [
          "Multimodal"
        ],
        "category": "specific_connectable",
        "rationale": "Multimodal learning is a key aspect of the paper, linking it to broader research in integrating multiple data types.",
        "novelty_score": 0.45,
        "connectivity_score": 0.88,
        "specificity_score": 0.7,
        "link_intent_score": 0.82
      },
      {
        "surface": "Process Reward Model",
        "canonical": "Process Reward Model",
        "aliases": [
          "PRM"
        ],
        "category": "unique_technical",
        "rationale": "Process Reward Models are central to the paper's methodology, offering a unique approach to evaluating reasoning steps.",
        "novelty_score": 0.75,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.77
      },
      {
        "surface": "Vision-Language",
        "canonical": "Vision-Language Model",
        "aliases": [
          "Vision-Language"
        ],
        "category": "evolved_concepts",
        "rationale": "Vision-Language models are relevant to the paper's context, bridging visual and textual data processing.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.75,
        "link_intent_score": 0.79
      },
      {
        "surface": "Reward ranked fine-tuning",
        "canonical": "Reward Ranked Fine-Tuning",
        "aliases": [
          "Reward ranked fine-tuning"
        ],
        "category": "unique_technical",
        "rationale": "This technique is a novel contribution of the paper, enhancing model performance through a specific fine-tuning strategy.",
        "novelty_score": 0.8,
        "connectivity_score": 0.6,
        "specificity_score": 0.88,
        "link_intent_score": 0.75
      }
    ],
    "ban_list_suggestions": [
      "Monte Carlo estimation",
      "test time scaling"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Athena-PRM",
      "resolved_canonical": "Athena Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Multimodal",
      "resolved_canonical": "Multimodal Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.45,
        "connectivity": 0.88,
        "specificity": 0.7,
        "link_intent": 0.82
      }
    },
    {
      "candidate_surface": "Process Reward Model",
      "resolved_canonical": "Process Reward Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.75,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.77
      }
    },
    {
      "candidate_surface": "Vision-Language",
      "resolved_canonical": "Vision-Language Model",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.75,
        "link_intent": 0.79
      }
    },
    {
      "candidate_surface": "Reward ranked fine-tuning",
      "resolved_canonical": "Reward Ranked Fine-Tuning",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.6,
        "specificity": 0.88,
        "link_intent": 0.75
      }
    }
  ]
}
-->

# Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.AI|cs.AI]]
**PDF**: [Download](https://arxiv.org/pdf/2506.09532.pdf)
**Category**: cs.AI
**Published**: 2025-09-24
**ArXiv ID**: [2506.09532](https://arxiv.org/abs/2506.09532)

## 🔗 유사한 논문
- [[2025-09-23/Med-PRM_ Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards_20250923|Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards]] (88.8% similar)
- [[2025-09-22/MT-RewardTree_ A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling_20250922|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling]] (87.9% similar)
- [[2025-09-22/Entropy-Regularized Process Reward Model_20250922|Entropy-Regularized Process Reward Model]] (86.2% similar)
- [[2025-09-23/SCAN_ Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning_20250923|SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning]] (84.9% similar)
- [[2025-09-18/TDRM_ Smooth Reward Models with Temporal Difference for LLM RL and Inference_20250918|TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference]] (83.5% similar)

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Multimodal Learning|Multimodal Learning]]
**⚡ Unique Technical**: [[keywords/Athena Process Reward Model|Athena Process Reward Model]], [[keywords/Process Reward Model|Process Reward Model]], [[keywords/Reward Ranked Fine-Tuning|Reward Ranked Fine-Tuning]]
**🚀 Evolved Concepts**: [[keywords/Vision-Language Model|Vision-Language Model]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2506.09532v2 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.

## 📝 요약

Athena-PRM은 복잡한 추론 문제 해결 과정의 각 단계에 대한 보상 점수를 평가하는 다중 모달 프로세스 보상 모델입니다. 이 모델은 약한 완성자와 강한 완성자 간의 예측 일관성을 활용하여 신뢰할 수 있는 프로세스 라벨을 식별함으로써 고품질의 프로세스 라벨 데이터를 효율적으로 생성합니다. Athena-PRM은 5,000개의 샘플로 다양한 시나리오와 벤치마크에서 뛰어난 성능을 보이며, ORM 초기화와 부정 데이터 업샘플링을 통해 PRM의 성능을 향상시킵니다. 이 모델은 테스트 시간 확장, 추론 단계의 정확성 평가, 보상 순위 미세 조정 등 세 가지 시나리오에서 검증되었으며, 다양한 벤치마크에서 우수한 성능을 입증했습니다. 특히, Qwen2.5-VL-7B 정책 모델을 사용할 때 WeMath와 MathVista에서 각각 10.2점과 7.1점의 성능 향상을 보였습니다. 또한, VisualProcessBench에서 기존 최고 성능을 3.9 F1 점수로 능가하며, 추론 단계의 정확성을 평가하는 데 있어 강력한 능력을 보여줍니다. Athena-PRM을 보상 모델로 사용하여 개발한 Athena-7B는 다섯 가지 벤치마크에서 기존 기준을 크게 능가하는 성과를 냈습니다.

## 🎯 주요 포인트

- 1. Athena-PRM은 복잡한 추론 문제 해결 과정에서 각 단계의 보상 점수를 평가하는 다중 모달 프로세스 보상 모델이다.
- 2. Athena-PRM은 약한 완성자와 강한 완성자 간의 예측 일관성을 활용하여 신뢰할 수 있는 프로세스 레이블을 식별하는 방법을 제안한다.
- 3. Athena-PRM은 5,000개의 샘플만으로도 다양한 시나리오와 벤치마크에서 뛰어난 효과를 보여준다.
- 4. ORM 초기화와 부정 데이터 업샘플링을 통해 PRM의 성능을 개선하는 두 가지 전략을 개발하였다.
- 5. Athena-PRM은 VisualProcessBench에서 최첨단 성과를 기록하며, 기존 성과를 3.9 F1 점수로 능가한다.


---

*Generated on 2025-09-24 14:36:05*