<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:51:05.733067",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Offline Reinforcement Learning",
    "Inverse Reinforcement Learning",
    "Kolmogorov-Arnold Networks",
    "Diffusion Policies",
    "Wearable Sensors"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Offline Reinforcement Learning": 0.78,
    "Inverse Reinforcement Learning": 0.8,
    "Kolmogorov-Arnold Networks": 0.72,
    "Diffusion Policies": 0.75,
    "Wearable Sensors": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Offline Reinforcement Learning",
        "canonical": "Offline Reinforcement Learning",
        "aliases": [
          "Offline RL"
        ],
        "category": "broad_technical",
        "rationale": "Offline Reinforcement Learning is a key method discussed in the paper, relevant for linking to broader AI and healthcare applications.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Inverse Reinforcement Learning",
        "canonical": "Inverse Reinforcement Learning",
        "aliases": [
          "IRL"
        ],
        "category": "specific_connectable",
        "rationale": "Inverse Reinforcement Learning is a central concept in the paper, crucial for understanding reward function estimation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Kolmogorov-Arnold Networks",
        "canonical": "Kolmogorov-Arnold Networks",
        "aliases": [
          "KANDI"
        ],
        "category": "unique_technical",
        "rationale": "Kolmogorov-Arnold Networks are a novel approach introduced in the paper, providing a unique method for function approximation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.72
      },
      {
        "surface": "Diffusion Policies",
        "canonical": "Diffusion Policies",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Diffusion Policies represent a novel generative approach for action refinement in the context of offline RL.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Wearable Sensors",
        "canonical": "Wearable Sensors",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Wearable Sensors are critical for data collection in the study, linking to broader applications in health monitoring.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "clinical trial",
      "healthcare applications"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Offline Reinforcement Learning",
      "resolved_canonical": "Offline Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Inverse Reinforcement Learning",
      "resolved_canonical": "Inverse Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Kolmogorov-Arnold Networks",
      "resolved_canonical": "Kolmogorov-Arnold Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Diffusion Policies",
      "resolved_canonical": "Diffusion Policies",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Wearable Sensors",
      "resolved_canonical": "Wearable Sensors",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors

## 📋 메타데이터

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18433.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18433](https://arxiv.org/abs/2509.18433)

## 🔗 유사한 논문
- [[2025-09-23/Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management_20250923|Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management]] (83.1% similar)
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (81.3% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (80.9% similar)
- [[2025-09-22/DiffusionNFT_ Online Diffusion Reinforcement with Forward Process_20250922|DiffusionNFT: Online Diffusion Reinforcement with Forward Process]] (80.7% similar)
- [[2025-09-24/NurseSchedRL_ Attention-Guided Reinforcement Learning for Nurse-Patient Assignment_20250924|NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment]] (80.6% similar)

## 🏷️ 카테고리화된 키워드
**🧠 Broad Technical**: [[keywords/Offline Reinforcement Learning|Offline Reinforcement Learning]]
**🔗 Specific Connectable**: [[keywords/Inverse Reinforcement Learning|Inverse Reinforcement Learning]], [[keywords/Wearable Sensors|Wearable Sensors]]
**⚡ Unique Technical**: [[keywords/Kolmogorov-Arnold Networks|Kolmogorov-Arnold Networks]], [[keywords/Diffusion Policies|Diffusion Policies]]

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.18433v1 Announce Type: new 
Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data is getting increasing attention in AI for healthcare. However, implementation poses significant challenges. Defining direct rewards is difficult, and inverse RL (IRL) struggles to infer accurate reward functions from expert behavior in complex environments. Offline RL also encounters challenges in aligning learned policies with observed human behavior in healthcare applications. To address challenges in applying offline RL to physical activity promotion for older adults at high risk of falls, based on wearable sensor activity monitoring, we introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning (KANDI). By leveraging the flexible function approximation in Kolmogorov-Arnold Networks, we estimate reward functions by learning free-living environment behavior from low-fall-risk older adults (experts), while diffusion-based policies within an Actor-Critic framework provide a generative approach for action refinement and efficiency in offline RL. We evaluate KANDI using wearable activity monitoring data in a two-arm clinical trial from our Physio-feedback Exercise Program (PEER) study, emphasizing its practical application in a fall-risk intervention program to promote physical activity among older adults. Additionally, KANDI outperforms state-of-the-art methods on the D4RL benchmark. These results underscore KANDI's potential to address key challenges in offline RL for healthcare applications, offering an effective solution for activity promotion intervention strategies in healthcare.

## 📝 요약

이 논문은 고령자의 신체 활동 증진을 위한 오프라인 강화 학습(Offline RL) 적용의 어려움을 해결하기 위해 Kolmogorov-Arnold Networks와 Diffusion Policies를 활용한 KANDI라는 새로운 방법론을 제안합니다. 기존 방법론의 보상 함수 정의 및 정책 학습의 한계를 극복하기 위해, KANDI는 저위험 고령자의 행동 데이터를 학습하여 보상 함수를 추정하고, Actor-Critic 프레임워크 내에서 행동의 정제 및 효율성을 높입니다. 이 방법은 임상 시험 데이터를 통해 평가되었으며, D4RL 벤치마크에서 기존 최첨단 방법보다 우수한 성능을 보였습니다. KANDI는 헬스케어 분야에서 오프라인 RL의 주요 문제를 해결할 잠재력을 지니고 있으며, 고령자의 신체 활동 증진 전략에 효과적인 솔루션을 제공합니다.

## 🎯 주요 포인트

- 1. 실제 임상 데이터를 활용한 오프라인 강화 학습은 의료 분야에서 점점 더 주목받고 있지만, 구현에는 상당한 어려움이 있다.
- 2. Kolmogorov-Arnold Networks와 Diffusion Policies를 활용한 KANDI는 고령자의 신체 활동 촉진을 위한 오프라인 역강화 학습에 적용된다.
- 3. KANDI는 낮은 낙상 위험을 가진 고령자들의 행동을 학습하여 보상 함수를 추정하고, Actor-Critic 프레임워크 내에서 확산 기반 정책을 통해 행동의 정제와 효율성을 제공한다.
- 4. KANDI는 PEER 연구의 임상 시험에서 착용형 활동 모니터링 데이터를 사용하여 평가되었으며, 낙상 위험 개입 프로그램에서의 실용성을 강조한다.
- 5. KANDI는 D4RL 벤치마크에서 최첨단 방법들을 능가하며, 의료 응용 분야에서 오프라인 강화 학습의 주요 과제를 해결할 잠재력을 보여준다.


---

*Generated on 2025-09-24 14:51:05*