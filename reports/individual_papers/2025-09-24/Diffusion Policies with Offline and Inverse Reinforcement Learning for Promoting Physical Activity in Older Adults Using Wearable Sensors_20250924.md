<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-24T14:51:05.733067",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Offline Reinforcement Learning",
    "Inverse Reinforcement Learning",
    "Kolmogorov-Arnold Networks",
    "Diffusion Policies",
    "Wearable Sensors"
  ],
  "rejected_keywords": [],
  "similarity_scores": {
    "Offline Reinforcement Learning": 0.78,
    "Inverse Reinforcement Learning": 0.8,
    "Kolmogorov-Arnold Networks": 0.72,
    "Diffusion Policies": 0.75,
    "Wearable Sensors": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true,
  "candidates_json": {
    "candidates": [
      {
        "surface": "Offline Reinforcement Learning",
        "canonical": "Offline Reinforcement Learning",
        "aliases": [
          "Offline RL"
        ],
        "category": "broad_technical",
        "rationale": "Offline Reinforcement Learning is a key method discussed in the paper, relevant for linking to broader AI and healthcare applications.",
        "novelty_score": 0.55,
        "connectivity_score": 0.85,
        "specificity_score": 0.7,
        "link_intent_score": 0.78
      },
      {
        "surface": "Inverse Reinforcement Learning",
        "canonical": "Inverse Reinforcement Learning",
        "aliases": [
          "IRL"
        ],
        "category": "specific_connectable",
        "rationale": "Inverse Reinforcement Learning is a central concept in the paper, crucial for understanding reward function estimation.",
        "novelty_score": 0.6,
        "connectivity_score": 0.82,
        "specificity_score": 0.75,
        "link_intent_score": 0.8
      },
      {
        "surface": "Kolmogorov-Arnold Networks",
        "canonical": "Kolmogorov-Arnold Networks",
        "aliases": [
          "KANDI"
        ],
        "category": "unique_technical",
        "rationale": "Kolmogorov-Arnold Networks are a novel approach introduced in the paper, providing a unique method for function approximation.",
        "novelty_score": 0.85,
        "connectivity_score": 0.65,
        "specificity_score": 0.9,
        "link_intent_score": 0.72
      },
      {
        "surface": "Diffusion Policies",
        "canonical": "Diffusion Policies",
        "aliases": [],
        "category": "unique_technical",
        "rationale": "Diffusion Policies represent a novel generative approach for action refinement in the context of offline RL.",
        "novelty_score": 0.8,
        "connectivity_score": 0.7,
        "specificity_score": 0.85,
        "link_intent_score": 0.75
      },
      {
        "surface": "Wearable Sensors",
        "canonical": "Wearable Sensors",
        "aliases": [],
        "category": "specific_connectable",
        "rationale": "Wearable Sensors are critical for data collection in the study, linking to broader applications in health monitoring.",
        "novelty_score": 0.5,
        "connectivity_score": 0.8,
        "specificity_score": 0.65,
        "link_intent_score": 0.7
      }
    ],
    "ban_list_suggestions": [
      "clinical trial",
      "healthcare applications"
    ]
  },
  "decisions": [
    {
      "candidate_surface": "Offline Reinforcement Learning",
      "resolved_canonical": "Offline Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.55,
        "connectivity": 0.85,
        "specificity": 0.7,
        "link_intent": 0.78
      }
    },
    {
      "candidate_surface": "Inverse Reinforcement Learning",
      "resolved_canonical": "Inverse Reinforcement Learning",
      "decision": "linked",
      "scores": {
        "novelty": 0.6,
        "connectivity": 0.82,
        "specificity": 0.75,
        "link_intent": 0.8
      }
    },
    {
      "candidate_surface": "Kolmogorov-Arnold Networks",
      "resolved_canonical": "Kolmogorov-Arnold Networks",
      "decision": "linked",
      "scores": {
        "novelty": 0.85,
        "connectivity": 0.65,
        "specificity": 0.9,
        "link_intent": 0.72
      }
    },
    {
      "candidate_surface": "Diffusion Policies",
      "resolved_canonical": "Diffusion Policies",
      "decision": "linked",
      "scores": {
        "novelty": 0.8,
        "connectivity": 0.7,
        "specificity": 0.85,
        "link_intent": 0.75
      }
    },
    {
      "candidate_surface": "Wearable Sensors",
      "resolved_canonical": "Wearable Sensors",
      "decision": "linked",
      "scores": {
        "novelty": 0.5,
        "connectivity": 0.8,
        "specificity": 0.65,
        "link_intent": 0.7
      }
    }
  ]
}
-->

# Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily_digest_20250924|20250924]] [[categories/cs.LG|cs.LG]]
**PDF**: [Download](https://arxiv.org/pdf/2509.18433.pdf)
**Category**: cs.LG
**Published**: 2025-09-24
**ArXiv ID**: [2509.18433](https://arxiv.org/abs/2509.18433)

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-23/Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management_20250923|Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management]] (83.1% similar)
- [[2025-09-19/Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (81.3% similar)
- [[2025-09-19/Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (80.9% similar)
- [[2025-09-22/DiffusionNFT_ Online Diffusion Reinforcement with Forward Process_20250922|DiffusionNFT: Online Diffusion Reinforcement with Forward Process]] (80.7% similar)
- [[2025-09-24/NurseSchedRL_ Attention-Guided Reinforcement Learning for Nurse-Patient Assignment_20250924|NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment]] (80.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ§  Broad Technical**: [[keywords/Offline Reinforcement Learning|Offline Reinforcement Learning]]
**ğŸ”— Specific Connectable**: [[keywords/Inverse Reinforcement Learning|Inverse Reinforcement Learning]], [[keywords/Wearable Sensors|Wearable Sensors]]
**âš¡ Unique Technical**: [[keywords/Kolmogorov-Arnold Networks|Kolmogorov-Arnold Networks]], [[keywords/Diffusion Policies|Diffusion Policies]]

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.18433v1 Announce Type: new 
Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data is getting increasing attention in AI for healthcare. However, implementation poses significant challenges. Defining direct rewards is difficult, and inverse RL (IRL) struggles to infer accurate reward functions from expert behavior in complex environments. Offline RL also encounters challenges in aligning learned policies with observed human behavior in healthcare applications. To address challenges in applying offline RL to physical activity promotion for older adults at high risk of falls, based on wearable sensor activity monitoring, we introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning (KANDI). By leveraging the flexible function approximation in Kolmogorov-Arnold Networks, we estimate reward functions by learning free-living environment behavior from low-fall-risk older adults (experts), while diffusion-based policies within an Actor-Critic framework provide a generative approach for action refinement and efficiency in offline RL. We evaluate KANDI using wearable activity monitoring data in a two-arm clinical trial from our Physio-feedback Exercise Program (PEER) study, emphasizing its practical application in a fall-risk intervention program to promote physical activity among older adults. Additionally, KANDI outperforms state-of-the-art methods on the D4RL benchmark. These results underscore KANDI's potential to address key challenges in offline RL for healthcare applications, offering an effective solution for activity promotion intervention strategies in healthcare.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê³ ë ¹ìì˜ ì‹ ì²´ í™œë™ ì¦ì§„ì„ ìœ„í•œ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ(Offline RL) ì ìš©ì˜ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Kolmogorov-Arnold Networksì™€ Diffusion Policiesë¥¼ í™œìš©í•œ KANDIë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë¡ ì˜ ë³´ìƒ í•¨ìˆ˜ ì •ì˜ ë° ì •ì±… í•™ìŠµì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, KANDIëŠ” ì €ìœ„í—˜ ê³ ë ¹ìì˜ í–‰ë™ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ê³ , Actor-Critic í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ í–‰ë™ì˜ ì •ì œ ë° íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì„ìƒ ì‹œí—˜ ë°ì´í„°ë¥¼ í†µí•´ í‰ê°€ë˜ì—ˆìœ¼ë©°, D4RL ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ìµœì²¨ë‹¨ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. KANDIëŠ” í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ì—ì„œ ì˜¤í”„ë¼ì¸ RLì˜ ì£¼ìš” ë¬¸ì œë¥¼ í•´ê²°í•  ì ì¬ë ¥ì„ ì§€ë‹ˆê³  ìˆìœ¼ë©°, ê³ ë ¹ìì˜ ì‹ ì²´ í™œë™ ì¦ì§„ ì „ëµì— íš¨ê³¼ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì‹¤ì œ ì„ìƒ ë°ì´í„°ë¥¼ í™œìš©í•œ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì€ ì˜ë£Œ ë¶„ì•¼ì—ì„œ ì ì  ë” ì£¼ëª©ë°›ê³  ìˆì§€ë§Œ, êµ¬í˜„ì—ëŠ” ìƒë‹¹í•œ ì–´ë ¤ì›€ì´ ìˆë‹¤.
- 2. Kolmogorov-Arnold Networksì™€ Diffusion Policiesë¥¼ í™œìš©í•œ KANDIëŠ” ê³ ë ¹ìì˜ ì‹ ì²´ í™œë™ ì´‰ì§„ì„ ìœ„í•œ ì˜¤í”„ë¼ì¸ ì—­ê°•í™” í•™ìŠµì— ì ìš©ëœë‹¤.
- 3. KANDIëŠ” ë‚®ì€ ë‚™ìƒ ìœ„í—˜ì„ ê°€ì§„ ê³ ë ¹ìë“¤ì˜ í–‰ë™ì„ í•™ìŠµí•˜ì—¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ê³ , Actor-Critic í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ í™•ì‚° ê¸°ë°˜ ì •ì±…ì„ í†µí•´ í–‰ë™ì˜ ì •ì œì™€ íš¨ìœ¨ì„±ì„ ì œê³µí•œë‹¤.
- 4. KANDIëŠ” PEER ì—°êµ¬ì˜ ì„ìƒ ì‹œí—˜ì—ì„œ ì°©ìš©í˜• í™œë™ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë˜ì—ˆìœ¼ë©°, ë‚™ìƒ ìœ„í—˜ ê°œì… í”„ë¡œê·¸ë¨ì—ì„œì˜ ì‹¤ìš©ì„±ì„ ê°•ì¡°í•œë‹¤.
- 5. KANDIëŠ” D4RL ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©°, ì˜ë£Œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµì˜ ì£¼ìš” ê³¼ì œë¥¼ í•´ê²°í•  ì ì¬ë ¥ì„ ë³´ì—¬ì¤€ë‹¤.


---

*Generated on 2025-09-24 14:51:05*