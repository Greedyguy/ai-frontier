# Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models

**Korean Title:** Causal2Vec: ë””ì½”ë” ì „ìš© LLMì„ ë‹¤ìš©ë„ ì„ë² ë”© ëª¨ë¸ë¡œ ê°œì„ í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Bidirectional Attention, Semantic Information Encoding

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Can Large Language Models Infer Causal Relationships from Real-World Text_20250922|Can Large Language Models Infer Causal Relationships from Real-World Text]] (83.4% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.7% similar)
- [[2025-09-22/Beyond Spurious Signals_ Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing_20250922|Beyond Spurious Signals Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing]] (81.8% similar)
- [[2025-09-19/VLM-E2E_ Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion_20250919|VLM-E2E Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion]] (81.8% similar)
- [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (80.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2507.23386v2 Announce Type: replace-cross 
Abstract: Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2507.23386v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì€ ìì—°ì–´ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ ì •ë³´ë¥¼ ë‹¤ì–‘í•œ ì„ë² ë”© ì‘ì—…ì„ ìœ„í•´ ë°€ì§‘ ë²¡í„° í‘œí˜„ìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° ì ì  ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ê¸°ì¡´ ë°©ë²•ë“¤ì€ LLMì—ì„œ ì¸ê³¼ì  ì£¼ì˜ ë§ˆìŠ¤í¬ë¥¼ ì œê±°í•˜ì—¬ ì–‘ë°©í–¥ ì£¼ì˜ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, ì´ëŠ” ì‚¬ì „ í•™ìŠµ ì¤‘ì— íšë“í•œ ì˜ë¯¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ ì ì¬ì ìœ¼ë¡œ ì €í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì£¼ìš” ë‹¨ë°©í–¥ ì ‘ê·¼ë²•ì€ ì¢…ì¢… ì¸ê³¼ì  ì£¼ì˜ì˜ ê³ ìœ í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¶”ê°€ ì…ë ¥ í…ìŠ¤íŠ¸ì— ì˜ì¡´í•˜ë©°, ì´ëŠ” í•„ì—°ì ìœ¼ë¡œ ê³„ì‚° ë¹„ìš©ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” ë””ì½”ë” ì „ìš© LLMì˜ ì›ë˜ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ìƒë‹¹í•œ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ ë„ì…í•˜ì§€ ì•Šê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì„¤ê³„ëœ ë²”ìš© ì„ë² ë”© ëª¨ë¸ì¸ Causal2Vecì„ ì œì•ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë¨¼ì € ê²½ëŸ‰ì˜ BERT ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì»¨í…ìŠ¤ì¶”ì–¼ í† í°ìœ¼ë¡œ ì‚¬ì „ ì¸ì½”ë”©í•˜ê³ , ì´ë¥¼ LLMì˜ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì•ì— ë°°ì¹˜í•˜ì—¬ ê° í† í°ì´ ë¯¸ë˜ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì§€ ì•Šê³ ë„ ì»¨í…ìŠ¤íŠ¸í™”ëœ ì •ë³´ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë˜í•œ, ë§ˆì§€ë§‰ í† í° í’€ë§ì— ì˜í•´ ë„ì…ëœ ìµœì‹  í¸í–¥ì„ ì™„í™”í•˜ê³  LLMì´ ì»¨í…ìŠ¤ì¶”ì–¼ í† í°ì— ì¸ì½”ë”©ëœ ì˜ë¯¸ ì •ë³´ë¥¼ ë” ì˜ í™œìš©í•  ìˆ˜ ìˆë„ë¡, ì»¨í…ìŠ¤ì¶”ì–¼ ë° EOS í† í°ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¥¼ ìµœì¢… í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ, Causal2Vecì€ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ë°ì´í„°ì…‹ì—ì„œë§Œ í›ˆë ¨ëœ ëª¨ë¸ ì¤‘ì—ì„œ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬(MTEB)ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë°©ë²•ì— ë¹„í•´ í•„ìš”í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ 85%, ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 82%ê¹Œì§€ ì¤„ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Causal2VecëŠ” ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì¼ë°˜ì ì¸ ì„ë² ë”© ëª¨ë¸ë¡œ, ê¸°ì¡´ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê²½ëŸ‰ì˜ BERT ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ì „ ì¸ì½”ë”©í•œ í›„, ì´ë¥¼ LLM ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ê° í† í°ì´ ë¯¸ë˜ í† í°ì„ ì°¸ì¡°í•˜ì§€ ì•Šê³ ë„ ë¬¸ë§¥ ì •ë³´ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ë§ˆì§€ë§‰ í† í° í’€ë§ì˜ í¸í–¥ì„ ì¤„ì´ê³  ë¬¸ë§¥ í† í°ì˜ ì˜ë¯¸ ì •ë³´ë¥¼ ë” ì˜ í™œìš©í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. Causal2VecëŠ” ê³µê°œëœ ë°ì´í„°ì…‹ë§Œìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©°, ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ 85%, ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 82%ê¹Œì§€ ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Causal2VecëŠ” ë””ì½”ë” ì „ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì„¤ê³„ëœ ë²”ìš© ì„ë² ë”© ëª¨ë¸ë¡œ, ê¸°ì¡´ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.

- 2. ì´ ëª¨ë¸ì€ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ì „ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ ê²½ëŸ‰ì˜ BERT ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ê° í† í°ì´ ë¯¸ë˜ì˜ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì§€ ì•Šê³ ë„ ë¬¸ë§¥í™”ëœ ì •ë³´ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

- 3. Causal2VecëŠ” ë§ˆì§€ë§‰ í† í° í’€ë§ì— ì˜í•œ ìµœì‹ ì„± í¸í–¥ì„ ì™„í™”í•˜ê³ , ë¬¸ë§¥í™”ëœ í† í°ì— ì¸ì½”ë”©ëœ ì˜ë¯¸ ì •ë³´ë¥¼ ë” ì˜ í™œìš©í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ìµœì¢… í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- 4. ì´ ëª¨ë¸ì€ ê³µê°œëœ ê²€ìƒ‰ ë°ì´í„°ì…‹ìœ¼ë¡œë§Œ í•™ìŠµëœ ëª¨ë¸ ì¤‘ì—ì„œ Massive Text Embeddings Benchmark (MTEB)ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ì—¬ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ 85%, ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 82%ê¹Œì§€ ì¤„ì…ë‹ˆë‹¤.

---

*Generated on 2025-09-22 14:57:40*