# MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements

**Korean Title:** MoCA: ë””ì§€í„¸ ê±´ê°• ì¸¡ì •ì„ ìœ„í•œ ë‹¤ì¤‘ ëª¨ë‹¬ êµì°¨ ë§ˆìŠ¤í¬ ì˜¤í† ì¸ì½”ë”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Correlation-aware Masking|Correlation-aware Masking]] [[keywords/specific/Masked Autoencoder|Masked Autoencoder]] [[keywords/broad/Transformer|Transformer]] [[keywords/broad/Self-supervised Learning|Self-supervised Learning]] [[keywords/unique/Multi-modal Cross-masked Autoencoder|Multi-modal Cross-masked Autoencoder]] [[categories/cs.LG|cs.LG]] [[2025-09-18/CSMoE_ An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts_20250918|CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts]] (82.8% similar) [[2025-09-22/MTS-DMAE_ Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning_20250922|MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning]] (82.7% similar) [[2025-09-22/Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture_20250922|Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture]] (82.3% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Correlation-aware Masking Strategy
**ğŸ”— Specific Connectable**: Masked Autoencoder
**ğŸ”¬ Broad Technical**: Transformer, Self-supervised Learning
**â­ Unique Technical**: Multi-modal Cross-masked Autoencoder
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-18/CSMoE_ An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts_20250918|CSMoE An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts]] (82.8% similar)
- [[2025-09-22/MTS-DMAE_ Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning_20250922|MTS-DMAE Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning]] (82.7% similar)
- [[2025-09-22/Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture_20250922|Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture]] (82.3% similar)
- [[2025-09-17/MOCHA_ Multi-modal Objects-aware Cross-arcHitecture Alignment_20250917|MOCHA Multi-modal Objects-aware Cross-arcHitecture Alignment]] (81.7% similar)
- [[2025-09-19/Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation_20250919|Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation]] (81.7% similar)


**ArXiv ID**: [2506.02260](https://arxiv.org/abs/2506.02260)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2506.02260.pdf)


**ArXiv ID**: [2506.02260](https://arxiv.org/abs/2506.02260)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2506.02260.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Correlation-aware Masking Strategy
**ğŸ”— Specific Connectable**: Masked Autoencoder
**â­ Unique Technical**: Multi-modal Cross-masked Autoencoder
**ğŸ”¬ Broad Technical**: Transformer, Self-supervised Learning

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Transformer` â€¢ 

`Self-supervised Learning` â€¢ 

`Masked Autoencoder` â€¢ 

`Kernelized Canonical Correlation Analysis` â€¢ 

`Multi-modal Cross-masked Autoencoder`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2506.02260v3 Announce Type: replace-cross 
Abstract: Wearable devices enable continuous multi-modal physiological and behavioral monitoring, yet analysis of these data streams faces fundamental challenges including the lack of gold-standard labels and incomplete sensor data. While self-supervised learning approaches have shown promise for addressing these issues, existing multi-modal extensions present opportunities to better leverage the rich temporal and cross-modal correlations inherent in simultaneously recorded wearable sensor data. We propose the Multi-modal Cross-masked Autoencoder (MoCA), a self-supervised learning framework that combines transformer architecture with masked autoencoder (MAE) methodology, using a principled cross-modality masking scheme that explicitly leverages correlation structures between sensor modalities. MoCA demonstrates strong performance boosts across reconstruction and downstream classification tasks on diverse benchmark datasets. We further establish theoretical guarantees by establishing a fundamental connection between multi-modal MAE loss and kernelized canonical correlation analysis through a Reproducing Kernel Hilbert Space framework, providing principled guidance for correlation-aware masking strategy design. Our approach offers a novel solution for leveraging unlabeled multi-modal wearable data while handling missing modalities, with broad applications across digital health domains.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2506.02260v3 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ì›¨ì–´ëŸ¬ë¸” ê¸°ê¸°ëŠ” ì§€ì†ì ì¸ ë‹¤ì¤‘ ëª¨ë“œ ìƒë¦¬ ë° í–‰ë™ ëª¨ë‹ˆí„°ë§ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì˜ ë¶„ì„ì€ í‘œì¤€ ë ˆì´ë¸”ì˜ ë¶€ì¡±ê³¼ ë¶ˆì™„ì „í•œ ì„¼ì„œ ë°ì´í„°ì™€ ê°™ì€ ê·¼ë³¸ì ì¸ ë¬¸ì œì— ì§ë©´í•´ ìˆìŠµë‹ˆë‹¤. ìê¸° ì§€ë„ í•™ìŠµ ì ‘ê·¼ë²•ì´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ê¸°ì¡´ì˜ ë‹¤ì¤‘ ëª¨ë“œ í™•ì¥ì€ ë™ì‹œì— ê¸°ë¡ëœ ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°ì— ë‚´ì¬ëœ í’ë¶€í•œ ì‹œê°„ì  ë° êµì°¨ ëª¨ë“œ ìƒê´€ê´€ê³„ë¥¼ ë³´ë‹¤ ì˜ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì™€ ë§ˆìŠ¤í¬ë“œ ì˜¤í† ì¸ì½”ë”(MAE) ë°©ë²•ë¡ ì„ ê²°í•©í•œ ìê¸° ì§€ë„ í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ ë‹¤ì¤‘ ëª¨ë“œ êµì°¨ ë§ˆìŠ¤í¬ë“œ ì˜¤í† ì¸ì½”ë”(MoCA)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì„¼ì„œ ëª¨ë“œ ê°„ì˜ ìƒê´€ êµ¬ì¡°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ì›ì¹™ì ì¸ êµì°¨ ëª¨ë“œ ë§ˆìŠ¤í‚¹ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. MoCAëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ì¬êµ¬ì„±ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë¶„ë¥˜ ì‘ì—… ì „ë°˜ì— ê±¸ì³ ê°•ë ¥í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ ì¬ìƒ ì»¤ë„ íë² ë¥´íŠ¸ ê³µê°„ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë‹¤ì¤‘ ëª¨ë“œ MAE ì†ì‹¤ê³¼ ì»¤ë„í™”ëœ ì •ì¤€ ìƒê´€ ë¶„ì„ ê°„ì˜ ê·¼ë³¸ì ì¸ ì—°ê²°ì„ í™•ë¦½í•¨ìœ¼ë¡œì¨ ì´ë¡ ì  ë³´ì¥ì„ í™•ë¦½í•˜ê³ , ìƒê´€ ì¸ì‹ ë§ˆìŠ¤í‚¹ ì „ëµ ì„¤ê³„ë¥¼ ìœ„í•œ ì›ì¹™ì ì¸ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ ëˆ„ë½ëœ ëª¨ë“œë¥¼ ì²˜ë¦¬í•˜ë©´ì„œ ë ˆì´ë¸”ì´ ì—†ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ì›¨ì–´ëŸ¬ë¸” ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ë©°, ë””ì§€í„¸ ê±´ê°• ë¶„ì•¼ ì „ë°˜ì— ê±¸ì³ ê´‘ë²”ìœ„í•œ ì‘ìš© ê°€ëŠ¥ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì›¨ì–´ëŸ¬ë¸” ê¸°ê¸°ë¥¼ í†µí•œ ë‹¤ì¤‘ ëª¨ë“œ ìƒë¦¬ ë° í–‰ë™ ëª¨ë‹ˆí„°ë§ì˜ ë°ì´í„° ë¶„ì„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ Multi-modal Cross-masked Autoencoder (MoCA)ë¼ëŠ” ìê¸° ì§€ë„ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. MoCAëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì™€ ë§ˆìŠ¤í¬ë“œ ì˜¤í† ì¸ì½”ë”(MAE) ë°©ë²•ë¡ ì„ ê²°í•©í•˜ì—¬ ì„¼ì„œ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒê´€ êµ¬ì¡°ë¥¼ í™œìš©í•˜ëŠ” êµì°¨ ëª¨ë‹¬ë¦¬í‹° ë§ˆìŠ¤í‚¹ ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„° ë³µì› ë° ë¶„ë¥˜ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ, MoCAëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ MAE ì†ì‹¤ê³¼ ì»¤ë„í™”ëœ ì •ì¤€ ìƒê´€ ë¶„ì„ ê°„ì˜ ì´ë¡ ì  ì—°ê²°ì„ í™•ë¦½í•˜ì—¬ ìƒê´€ ì¸ì‹ ë§ˆìŠ¤í‚¹ ì „ëµ ì„¤ê³„ì— ëŒ€í•œ ì´ë¡ ì  ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë””ì§€í„¸ í—¬ìŠ¤ ë¶„ì•¼ì—ì„œ ë¼ë²¨ì´ ì—†ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ì›¨ì–´ëŸ¬ë¸” ë°ì´í„°ë¥¼ í™œìš©í•˜ê³  ëˆ„ë½ëœ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ìœ ìš©í•œ ì†”ë£¨ì…˜ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ì›¨ì–´ëŸ¬ë¸” ê¸°ê¸°ì˜ ë‹¤ì¤‘ ëª¨ë“œ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ë¶„ì„ì—ì„œ ê¸ˆí‘œì¤€ ë ˆì´ë¸” ë¶€ì¡±ê³¼ ì„¼ì„œ ë°ì´í„° ë¶ˆì™„ì „ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìê¸° ì§€ë„ í•™ìŠµ ì ‘ê·¼ë²•ì´ ìœ ë§í•˜ë‹¤.

- 2. ì œì•ˆëœ MoCA(Multi-modal Cross-masked Autoencoder)ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì™€ ë§ˆìŠ¤í¬ë“œ ì˜¤í† ì¸ì½”ë” ë°©ë²•ë¡ ì„ ê²°í•©í•˜ì—¬ ì„¼ì„œ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒê´€ êµ¬ì¡°ë¥¼ í™œìš©í•œë‹¤.

- 3. MoCAëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ì¬êµ¬ì„±ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤€ë‹¤.

- 4. MoCAì˜ ì´ë¡ ì  ë³´ì¥ì€ ë‹¤ì¤‘ ëª¨ë“œ MAE ì†ì‹¤ê³¼ ì»¤ë„í™”ëœ ì •ì¤€ ìƒê´€ ë¶„ì„ ì‚¬ì´ì˜ ê·¼ë³¸ì ì¸ ì—°ê²°ì„ í†µí•´ ì œê³µëœë‹¤.

- 5. ì´ ì ‘ê·¼ë²•ì€ ë””ì§€í„¸ í—¬ìŠ¤ ë¶„ì•¼ì—ì„œ ë¼ë²¨ì´ ì—†ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ì›¨ì–´ëŸ¬ë¸” ë°ì´í„°ë¥¼ í™œìš©í•˜ê³  ëˆ„ë½ëœ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ìƒˆë¡œìš´ ì†”ë£¨ì…˜ì„ ì œì‹œí•œë‹¤.


---

*Generated on 2025-09-22 16:11:58*