# Once Upon a Time: Interactive Learning for Storytelling with Small Language Models

**Korean Title:** 옛날 옛적에: 소형 언어 모델을 활용한 스토리텔링을 위한 인터랙티브 학습

## 📋 메타데이터

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Interactive Storytelling

## 🔗 유사한 논문
- [[2025-09-19/From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (81.7% similar)
- [[2025-09-19/Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (81.4% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (81.2% similar)
- [[2025-09-19/Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (79.9% similar)
- [[2025-09-18/Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (79.5% similar)

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15714v1 Announce Type: cross 
Abstract: Children efficiently acquire language not just by listening, but by interacting with others in their social environment. Conversely, large language models are typically trained with next-word prediction on massive amounts of text. Motivated by this contrast, we investigate whether language models can be trained with less data by learning not only from next-word prediction but also from high-level, cognitively inspired feedback. We train a student model to generate stories, which a teacher model rates on readability, narrative coherence, and creativity. By varying the amount of pretraining before the feedback loop, we assess the impact of this interactive learning on formal and functional linguistic competence. We find that the high-level feedback is highly data efficient: With just 1 M words of input in interactive learning, storytelling skills can improve as much as with 410 M words of next-word prediction.

## 🔍 Abstract (한글 번역)

arXiv:2509.15714v1 발표 유형: 교차  
초록: 어린이는 단순히 듣는 것만으로가 아니라, 사회적 환경에서 다른 사람들과 상호작용함으로써 효율적으로 언어를 습득합니다. 반면에, 대형 언어 모델은 일반적으로 방대한 양의 텍스트에 대해 다음 단어 예측을 통해 훈련됩니다. 이러한 대조를 바탕으로, 우리는 언어 모델이 다음 단어 예측뿐만 아니라 고차원적이고 인지적으로 영감을 받은 피드백을 통해 학습함으로써 더 적은 데이터로 훈련될 수 있는지를 조사합니다. 우리는 학생 모델이 이야기를 생성하도록 훈련하고, 교사 모델이 가독성, 서사적 일관성, 창의성에 대해 평가합니다. 피드백 루프 이전의 사전 훈련 양을 조정하여, 이 상호작용 학습이 형식적 및 기능적 언어 능력에 미치는 영향을 평가합니다. 우리는 고차원 피드백이 매우 데이터 효율적임을 발견했습니다: 상호작용 학습에서 단 100만 단어의 입력만으로도 이야기 기술이 4억 1천만 단어의 다음 단어 예측만큼 향상될 수 있습니다.

## 📝 요약

이 논문은 아동이 언어를 습득하는 방식과 대조적으로, 대형 언어 모델이 대량의 텍스트를 통해 다음 단어 예측으로 학습되는 점에 주목하여, 상위 수준의 인지적 피드백을 통해 더 적은 데이터로 언어 모델을 훈련할 수 있는지를 탐구합니다. 연구에서는 학생 모델이 이야기를 생성하고, 교사 모델이 가독성, 서사적 일관성, 창의성 측면에서 이를 평가하는 방식으로 훈련을 진행했습니다. 피드백 루프 이전의 사전 훈련 양을 조절하여 이러한 상호작용적 학습이 언어적 능력에 미치는 영향을 평가한 결과, 상위 수준의 피드백이 데이터 효율성이 높다는 것을 발견했습니다. 즉, 상호작용적 학습에서 100만 단어의 입력만으로도 이야기 작성 능력이 4억 1천만 단어의 다음 단어 예측만큼 향상될 수 있음을 확인했습니다.

## 🎯 주요 포인트

- 1. 어린이는 사회적 상호작용을 통해 언어를 효율적으로 습득하는 반면, 대형 언어 모델은 주로 대량의 텍스트를 통한 다음 단어 예측으로 학습된다.

- 2. 본 연구는 다음 단어 예측뿐만 아니라 고차원적이고 인지적으로 영감을 받은 피드백을 통해 언어 모델을 적은 데이터로 학습할 수 있는지를 조사한다.

- 3. 학생 모델이 이야기를 생성하고, 교사 모델이 가독성, 서사적 일관성, 창의성 측면에서 이를 평가하는 방식으로 학습을 진행한다.

- 4. 상호작용 학습에서 100만 단어의 입력만으로도 이야기 구성 능력이 4억 1천만 단어의 다음 단어 예측만큼 향상될 수 있음을 발견했다.

- 5. 고차원적 피드백은 데이터 효율성이 매우 높아, 적은 양의 데이터로도 언어적 능력을 효과적으로 향상시킬 수 있다.

---

*Generated on 2025-09-22 14:09:14*