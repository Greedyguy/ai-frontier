
# Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI

**Korean Title:** 대규모 언어 모델에서의 정확도 역설: 생성적 AI에서 환각 위험 규제하기

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Epistemic Trustworthiness|Epistemic Trustworthiness]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Generative AI|Generative AI]] [[keywords/specific/Hallucination Risks|Hallucination Risks]] [[keywords/unique/Accuracy Paradox|Accuracy Paradox]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Epistemic Trustworthiness
**🔬 Broad Technical**: Large Language Models, Generative AI
**🔗 Specific Connectable**: Hallucination Risks
**⭐ Unique Technical**: Accuracy Paradox

**ArXiv ID**: [2509.13345](https://arxiv.org/abs/2509.13345)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.13345.pdf)


## 🏷️ 추출된 키워드



`Large Language Models` • 

`Generative AI` • 

`Hallucination Risks` • 

`Accuracy Paradox` • 

`Epistemic Trustworthiness`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13345v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their epistemic and societal risks demand urgent scrutiny. Hallucinations, the generation of fabricated, misleading, oversimplified or untrustworthy outputs, has emerged as imperative challenges. While regulatory, academic, and technical discourse position accuracy as the principal benchmark for mitigating such harms, this article contends that overreliance on accuracy misdiagnoses the problem and has counterproductive effect: the accuracy paradox. Drawing on interdisciplinary literatures, this article develops a taxonomy of hallucination types and shows the paradox along three intertwining dimensions: outputs, individuals and society. First, accuracy functions as a superficial proxy for reliability, incentivising the optimisation of rhetorical fluency and surface-level correctness over epistemic trustworthiness. This encourages passive user trust in outputs that appear accurate but epistemically untenable. Second, accuracy as a singular metric fails to detect harms that are not factually false but are nonetheless misleading, value-laden, or socially distorting, including consensus illusions, sycophantic alignment, and subtle manipulation. Third, regulatory overemphasis on accuracy obscures the wider societal consequences of hallucination, including social sorting, privacy violations, equity harms, epistemic convergence that marginalises dissent, reduces pluralism, and causes social deskilling. By examining the EU AI Act, GDPR, and DSA, the article argues that current regulations are not yet structurally equipped to address these epistemic, relational, and systemic harms and exacerbated by the overreliance on accuracy. By exposing such conceptual and practical challenges, this article calls for a fundamental shift towards pluralistic, context-aware, and manipulation-resilient approaches to AI trustworthy governance.

## 🔍 Abstract (한글 번역)

arXiv:2509.13345v1 발표 유형: 교차
요약: 대규모 언어 모델(LLMs)이 일상적인 의사 결정에 스며들면서 그들의 인식론적 및 사회적 위험은 긴급한 검토를 요구한다. 환각, 즉 조작된, 오도하는, 과대 단순화된 또는 신뢰할 수 없는 결과물을 생성하는 것은 중요한 도전 과제로 부상했다. 규제, 학술 및 기술적 논의는 정확성을 이러한 피해 완화의 주요 기준으로 위치시키지만, 본 논문은 정확성에 대한 과도한 의존이 문제를 오진하고 역효과를 일으키는 정확성 역설이라고 주장한다. 이 논문은 다학제적 문헌을 바탕으로 환각 유형의 분류를 개발하고 결과물, 개인 및 사회의 세 가지 엮인 차원을 통해 역설을 보여준다. 먼저, 정확성은 신뢰성의 얕은 대리물로 작용하여 논리적 유창성과 표면적 정확성을 지식적 신뢰성보다 우선시하도록 장려한다. 이는 정확해 보이지만 지식적으로 지지할 수 없는 결과물에 대한 수동적 사용자 신뢰를 촉진한다. 둘째, 단일 지표로서의 정확성은 사실적으로 거짓이 아니지만 그럼에도 불구하고 오도하는, 가치 부여된 또는 사회적으로 왜곡된 피해를 감지하지 못한다. 이는 합의 환상, 아첨적 조정, 섬세한 조작을 포함한다. 셋째, 정확성에 대한 규제적 과도한 강조는 사회적 분류, 개인 정보 침해, 공평 피해, 이견을 획일화하고 다양성을 줄이며 사회적 기술 미화를 일으키는 환각의 보다 넓은 사회적 결과를 가리는 결과를 낳는다. EU AI Act, GDPR 및 DSA를 검토함으로써, 이 논문은 현재의 규정이 이러한 인식론적, 관계적 및 체계적 피해를 해결하기에 구조적으로 아직 갖추어지지 않았으며 정확성에 대한 과도한 의존으로 악화되고 있다고 주장한다. 이러한 개념적 및 실용적 도전 과제를 드러내면서, 본 논문은 AI 신뢰성 있는 지배에 대한 다원론적, 맥락 인식적 및 조작에 강한 접근 방식으로의 근본적 전환을 촉구한다.

## 📝 요약

최근 대형 언어 모델이 일상적인 의사결정에 보급되면서 그들의 인식적 및 사회적 위험은 긴급한 검토를 요구한다. 이 논문은 환각, 즉 거짓, 오도, 과도 단순화 또는 신뢰할 수 없는 결과물을 생성하는 것이 중요한 도전 과제로 떠오르고 있다. 정확도를 해소하는 주요 기준으로 위치시키는 규제, 학술 및 기술적 논의는 문제를 오진단하고 정확도 역설을 초래한다고 주장한다. 이 논문은 환각 유형의 분류법을 개발하고 정확도 역설을 세 가지 차원을 통해 보여준다: 결과물, 개인 및 사회. 현재의 규정은 이러한 인식적, 관계적 및 체계적 피해를 해결할 만큼 구조적으로 갖추어지지 않았으며 정확도에 대한 과도한 의존으로 악화되고 있다. 이 논문은 이러한 개념적 및 실용적 도전에 대한 인식을 높이고 다원론적, 맥락에 민감하며 조작에 강한 AI 신뢰성 있는 지배 방식으로의 근본적 전환을 촉구한다.

## 🎯 주요 포인트


- 대형 언어 모델이 일상적인 의사결정에 침투함에 따라 환각은 중요한 도전 과제로 부각되고 있음

- 정확도가 주요 기준으로 제시되지만, 이는 문제를 오진단하고 역효과를 일으키는 정확도 역설을 초래할 수 있음

- 정확도는 신뢰성의 표면적 대리인으로 작용하여 지식적 신뢰성보다는 표면적인 정확성을 장려함.


---

*Generated on 2025-09-18 16:17:15*