# Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems

**Korean Title:** 다중 물리: 중국 다중 과목 물리 문제에 대한 다중 모달 대형 언어 모델의 추론을 위한 종합 벤치마크

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Multimodal Reasoning Process|Multimodal Reasoning Process]] [[keywords/specific/Chain-of-thought Reasoning|Chain-of-thought Reasoning]] [[keywords/broad/Multimodal LLMs|Multimodal LLMs]] [[keywords/unique/Multi-Physics Benchmark|Multi-Physics Benchmark]] [[categories/cs.CL|cs.CL]] [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (84.8% similar) [[2025-09-22/HiPhO_ How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark_20250922|HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?]] (84.7% similar) [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (84.2% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Visual Information Impact
**🔗 Specific Connectable**: Chain of Thought Reasoning
**🔬 Broad Technical**: Multimodal LLM, Physics Education
**⭐ Unique Technical**: Multi-Physics Benchmark
## 🔗 유사한 논문
- [[2025-09-22/Perception-R1_ Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward_20250922|Perception-R1 Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward]] (84.8% similar)
- [[2025-09-22/HiPhO_ How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark_20250922|HiPhO How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark]] (84.7% similar)
- [[2025-09-22/DivLogicEval_ A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models_20250922|DivLogicEval A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models]] (84.2% similar)
- [[2025-09-19/A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation_20250919|A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation]] (82.8% similar)
- [[2025-09-22/Best-of-L_ Cross-Lingual Reward Modeling for Mathematical Reasoning_20250922|Best-of-L Cross-Lingual Reward Modeling for Mathematical Reasoning]] (81.8% similar)


**ArXiv ID**: [2509.15839](https://arxiv.org/abs/2509.15839)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15839.pdf)


**ArXiv ID**: [2509.15839](https://arxiv.org/abs/2509.15839)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15839.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Multimodal Reasoning Process
**🔗 Specific Connectable**: Chain of Thought Reasoning
**⭐ Unique Technical**: Multi-Physics Benchmark
**🔬 Broad Technical**: Multimodal LLMs

## 🏷️ 추출된 키워드



`Multimodal LLMs` • 

`Chain-of-thought Reasoning` • 

`Multi-Physics Benchmark` • 

`Multimodal Reasoning Process`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15839v1 Announce Type: new 
Abstract: While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress, their application in specialized scientific domains like physics reveals significant gaps in current evaluation benchmarks. Specifically, existing benchmarks often lack fine-grained subject coverage, neglect the step-by-step reasoning process, and are predominantly English-centric, failing to systematically evaluate the role of visual information. Therefore, we introduce \textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive benchmark that includes 5 difficulty levels, featuring 1,412 image-associated, multiple-choice questions spanning 11 high-school physics subjects. We employ a dual evaluation framework to evaluate 20 different MLLMs, analyzing both final answer accuracy and the step-by-step integrity of their chain-of-thought. Furthermore, we systematically study the impact of difficulty level and visual information by comparing the model performance before and after changing the input mode. Our work provides not only a fine-grained resource for the community but also offers a robust methodology for dissecting the multimodal reasoning process of state-of-the-art MLLMs, and our dataset and code have been open-sourced: https://github.com/luozhongze/Multi-Physics.

## 🔍 Abstract (한글 번역)

arXiv:2509.15839v1 발표 유형: 신규  
초록: 다중 모달 대형 언어 모델(MLLMs)은 놀라운 추론 발전을 보여주지만, 물리학과 같은 전문 과학 분야에서의 적용은 현재 평가 기준에서 상당한 격차를 드러냅니다. 구체적으로, 기존의 벤치마크는 종종 세부적인 주제 범위가 부족하고, 단계별 추론 과정을 간과하며, 주로 영어 중심적이어서 시각 정보의 역할을 체계적으로 평가하지 못합니다. 따라서 우리는 중국어 물리학 추론을 위한 포괄적인 벤치마크인 \textbf{Multi-Physics}를 도입합니다. 이 벤치마크는 11개의 고등학교 물리학 과목에 걸쳐 1,412개의 이미지 관련 객관식 질문을 포함하며, 5개의 난이도 수준을 특징으로 합니다. 우리는 최종 답변 정확도와 사고의 연쇄 과정의 단계별 무결성을 분석하여 20개의 다양한 MLLMs을 평가하는 이중 평가 프레임워크를 사용합니다. 또한, 입력 모드를 변경하기 전후의 모델 성능을 비교하여 난이도 수준과 시각 정보의 영향을 체계적으로 연구합니다. 우리의 연구는 커뮤니티에 세부적인 자원을 제공할 뿐만 아니라 최첨단 MLLMs의 다중 모달 추론 과정을 해부하는 강력한 방법론을 제공합니다. 우리의 데이터셋과 코드는 오픈 소스로 제공됩니다: https://github.com/luozhongze/Multi-Physics.

## 📝 요약

이 논문은 물리학 분야에서의 멀티모달 대형 언어 모델(MLLMs)의 평가 기준의 한계를 지적하고, 이를 보완하기 위해 중국 물리학 추론을 위한 \textbf{Multi-Physics}라는 포괄적인 벤치마크를 제안합니다. 이 벤치마크는 5단계 난이도로 구성된 1,412개의 이미지 연관 다지선다형 문제를 포함하며, 고등학교 물리학의 11개 주제를 다룹니다. 20개의 다양한 MLLM을 평가하기 위해 최종 답변 정확도와 단계별 사고 과정의 무결성을 분석하는 이중 평가 프레임워크를 사용합니다. 또한, 입력 모드 변경 전후의 모델 성능을 비교하여 난이도와 시각 정보의 영향을 체계적으로 연구합니다. 이 연구는 커뮤니티에 세밀한 자원을 제공하고, 최신 MLLM의 멀티모달 추론 과정을 분석하는 견고한 방법론을 제시합니다. 데이터셋과 코드는 공개되어 있습니다.

## 🎯 주요 포인트


- 1. 기존 평가 기준은 물리학과 같은 전문 과학 분야에서 세부적인 주제 범위와 단계별 추론 과정을 충분히 다루지 못하고 있습니다.

- 2. Multi-Physics는 중국 물리학 추론을 위한 포괄적인 벤치마크로, 11개의 고등학교 물리 과목에 걸쳐 1,412개의 이미지 연관 다지선다형 질문을 포함하고 있습니다.

- 3. 20개의 다양한 MLLM을 평가하기 위해 최종 답변 정확도와 단계별 사고 과정의 무결성을 분석하는 이중 평가 프레임워크를 사용합니다.

- 4. 난이도 수준과 시각적 정보의 영향을 체계적으로 연구하여 입력 모드를 변경하기 전후의 모델 성능을 비교합니다.

- 5. 본 연구는 커뮤니티에 세밀한 자원을 제공할 뿐만 아니라 최첨단 MLLM의 다중 모드 추론 과정을 분석할 수 있는 강력한 방법론을 제시합니다.


---

*Generated on 2025-09-22 16:28:15*