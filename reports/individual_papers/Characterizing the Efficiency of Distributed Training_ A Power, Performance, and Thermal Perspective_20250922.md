# Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective

**Korean Title:** ë¶„ì‚° í•™ìŠµì˜ íš¨ìœ¨ì„± íŠ¹ì„±í™”: ì „ë ¥, ì„±ëŠ¥ ë° ì—´ ê´€ì 

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/evolved/Compute-Communication Overlap|Compute-Communication Overlap]] [[keywords/specific/Activation Recomputation|Activation Recomputation]] [[keywords/broad/Large Language Models|Large Language Models]] [[keywords/broad/Distributed Training|Distributed Training]] [[keywords/unique/CharLLM-PPT|CharLLM-PPT]] [[categories/cs.LG|cs.LG]] [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (84.9% similar) [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM: Language of Browsing]] (84.1% similar) [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (83.6% similar)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Compute-Communication Overlap
**ğŸ”— Specific Connectable**: Parallelism Strategies
**ğŸ”¬ Broad Technical**: Large Language Models, Distributed Training
**â­ Unique Technical**: CharLLM-PPT
## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning_20250922|Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning]] (84.9% similar)
- [[2025-09-22/Subjective Behaviors and Preferences in LLM_ Language of Browsing_20250922|Subjective Behaviors and Preferences in LLM Language of Browsing]] (84.1% similar)
- [[2025-09-18/The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning_20250918|The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning]] (83.6% similar)
- [[2025-09-19/{lambda}Scale_ Enabling Fast Scaling for Serverless Large Language Model Inference_20250919|{lambda}Scale Enabling Fast Scaling for Serverless Large Language Model Inference]] (83.5% similar)
- [[2025-09-22/Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges_20250922|Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges]] (83.0% similar)


**ArXiv ID**: [2509.10371](https://arxiv.org/abs/2509.10371)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.10371.pdf)


**ArXiv ID**: [2509.10371](https://arxiv.org/abs/2509.10371)
**Published**: 2025-09-22
**Category**: cs.LG
**PDF**: [Download](https://arxiv.org/pdf/2509.10371.pdf)

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Parallelism Strategies
**ğŸ”— Specific Connectable**: Activation Recomputation, Compute-Communication Overlap
**ğŸ”¬ Broad Technical**: Large Language Models, Distributed Training

## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Large Language Models` â€¢ 

`Distributed Training` â€¢ 

`Activation Recomputation` â€¢ 

`Compute-Communication Overlap` â€¢ 

`Parallelism Strategies`



## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸

Similar papers will be displayed here based on embedding similarity.

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.10371v2 Announce Type: replace-cross 
Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at https://github.com/sitar-lab/CharLLM-PPT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.10371v2 ë°œí‘œ ìœ í˜•: êµì°¨ êµì²´  
ì´ˆë¡: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê¸‰ì†í•œ í™•ì¥ì€ ë‹¨ì¼ ë…¸ë“œ ë¶„ì„ì˜ í•œê³„ë¥¼ í›¨ì”¬ ë„˜ì–´ì„œëŠ” í›ˆë ¨ ì‘ì—…ì„ ìš”êµ¬í•˜ë©°, ì´ëŸ¬í•œ ëª¨ë¸ì´ ëŒ€ê·œëª¨, ë‹¤ì¤‘ GPU ì‹œìŠ¤í…œì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NVIDIA H100/H200 ë° AMD MI250 GPUë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ì—… ë¶€í•˜ì™€ í•˜ë“œì›¨ì–´ í”Œë«í¼ì—ì„œ LLM í›ˆë ¨ì˜ í¬ê´„ì ì¸ íŠ¹ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í…ì„œ, íŒŒì´í”„ë¼ì¸, ë°ì´í„°, ì „ë¬¸ê°€ ë“± ë‹¤ì–‘í•œ ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ í•˜ì—ì„œ ë°€ì§‘ ë° í¬ì†Œ ëª¨ë¸ì„ ë¶„ì„í•˜ê³ , í•˜ë“œì›¨ì–´ í™œìš©ë„, ì „ë ¥ ì†Œë¹„ ë° ì—´ì  í–‰ë™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ í™œì„±í™” ì¬ê³„ì‚° ë° ê³„ì‚°-í†µì‹  ì¤‘ì²©ê³¼ ê°™ì€ ìµœì í™”ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” ì„±ëŠ¥ì´ ë‹¨ìˆœíˆ í•˜ë“œì›¨ì–´ ìš©ëŸ‰ì˜ í™•ì¥ì— ì˜í•´ ê²°ì •ë˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë©”ëª¨ë¦¬ê°€ ë” ë§ì€ GPUê°€ ì ì€ ìŠ¤ì¼€ì¼ ì—… ì‹œìŠ¤í…œì€ í†µì‹ ì— ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì‹œìŠ¤í…œë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ì‹ ì¤‘í•˜ê²Œ ì¡°ì •ëœ êµ¬ì„±ì—ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ìŠ¤ì¼€ì¼ ì•„ì›ƒ ë°°í¬ê°€ ë” ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ë˜í•œ í…ì„œì™€ íŒŒì´í”„ë¼ì¸ê³¼ ê°™ì€ íŠ¹ì • ë³‘ë ¬ ì²˜ë¦¬ ì¡°í•©ì´ ë¹„íš¨ìœ¨ì ì¸ ë°ì´í„° ì²­í‚¹ìœ¼ë¡œ ì¸í•´ ëŒ€ì—­í­ í™œìš©ë„ë¥¼ ì €í•˜ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ë§ˆì´í¬ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ íŠ¹ì • ì§€ì  ì´ìƒìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ë©´ ë²„ìŠ¤íŠ¸ ì‹¤í–‰ê³¼ í”¼í¬ ì „ë ¥ ê¸‰ì¦ì„ ìœ ë°œí•˜ì—¬ ì—´ ìŠ¤ë¡œí‹€ë§ì„ ì•…í™”ì‹œí‚µë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ë ¥ì€ í•˜ë“œì›¨ì–´, ì‹œìŠ¤í…œ í† í´ë¡œì§€ ë° ëª¨ë¸ ì‹¤í–‰ ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì— ì˜í•´ í›ˆë ¨ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ í˜•ì„±ë˜ëŠ”ì§€ë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¯¸ë˜ì˜ LLM ì‹œìŠ¤í…œê³¼ ì‘ì—… ë¶€í•˜ì˜ í™•ì¥ì„±ê³¼ ì‹ ë¢°ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ ë° í•˜ë“œì›¨ì–´ ì„¤ê³„ì— ëŒ€í•œ ê¶Œì¥ ì‚¬í•­ì„ ì œì‹œí•˜ë©° ê²°ë¡ ì„ ë§ºìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ì˜ ì†ŒìŠ¤ ì½”ë“œëŠ” https://github.com/sitar-lab/CharLLM-PPTì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í›ˆë ¨ì„ ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ì—…ê³¼ í•˜ë“œì›¨ì–´ í”Œë«í¼(NVIDIA H100/H200, AMD MI250 GPU í¬í•¨)ì—ì„œ ë¶„ì„í•©ë‹ˆë‹¤. ë°€ì§‘ ë° í¬ì†Œ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë³‘ë ¬í™” ì „ëµ(í…ì„œ, íŒŒì´í”„ë¼ì¸, ë°ì´í„°, ì „ë¬¸ê°€) í•˜ì— í‰ê°€í•˜ì—¬ í•˜ë“œì›¨ì–´ í™œìš©ë„, ì „ë ¥ ì†Œë¹„, ì—´ì  íŠ¹ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. í™œì„±í™” ì¬ê³„ì‚° ë° ê³„ì‚°-í†µì‹  ì¤‘ì²©ê³¼ ê°™ì€ ìµœì í™”ì˜ íš¨ê³¼ë„ í‰ê°€í•©ë‹ˆë‹¤. ì£¼ìš” ë°œê²¬ì€ í•˜ë“œì›¨ì–´ ìš©ëŸ‰ì˜ í™•ì¥ì´ ì„±ëŠ¥ì„ ê²°ì •í•˜ì§€ ì•Šìœ¼ë©°, ê³ ë©”ëª¨ë¦¬ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤ì¼€ì¼ì—… ì‹œìŠ¤í…œì´ í†µì‹ ì— ì œì•½ì´ ìˆëŠ” ìƒí™©ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ì„¸ì‹¬í•œ ì¡°ì •ì´ í•„ìš”í•˜ë©°, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìŠ¤ì¼€ì¼ì•„ì›ƒ ë°°ì¹˜ê°€ ë” ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ë˜í•œ, íŠ¹ì • ë³‘ë ¬í™” ì¡°í•©ì€ ëŒ€ì—­í­ í™œìš©ì„ ì €í•´í•˜ê³ , ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸°ì˜ ê³¼ë„í•œ ì¦ê°€ëŠ” ì—´ ìŠ¤ë¡œí‹€ë§ì„ ì•…í™”ì‹œí‚µë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” í•˜ë“œì›¨ì–´, ì‹œìŠ¤í…œ í† í´ë¡œì§€, ëª¨ë¸ ì‹¤í–‰ ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì´ í›ˆë ¨ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í–¥í›„ LLM ì‹œìŠ¤í…œê³¼ ì‘ì—…ì˜ í™•ì¥ì„±ê³¼ ì‹ ë¢°ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ ë° í•˜ë“œì›¨ì–´ ì„¤ê³„ì— ëŒ€í•œ ê¶Œì¥ ì‚¬í•­ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) í›ˆë ¨ì€ ë‹¨ì¼ ë…¸ë“œ ë¶„ì„ì˜ í•œê³„ë¥¼ ë„˜ì–´ ë‹¤ì¤‘ GPU ì‹œìŠ¤í…œì—ì„œì˜ ëª¨ë¸ ë™ì‘ ì´í•´ê°€ í•„ìš”í•˜ë‹¤.

- 2. ë‹¤ì–‘í•œ ë³‘ë ¬í™” ì „ëµ(í…ì„œ, íŒŒì´í”„ë¼ì¸, ë°ì´í„°, ì „ë¬¸ê°€)ì„ í†µí•´ í•˜ë“œì›¨ì–´ í™œìš©, ì „ë ¥ ì†Œë¹„, ì—´ì  íŠ¹ì„±ì„ í‰ê°€í•˜ì˜€ë‹¤.

- 3. í•˜ë“œì›¨ì–´ ìš©ëŸ‰ì˜ í™•ì¥ë§Œìœ¼ë¡œ ì„±ëŠ¥ì´ ê²°ì •ë˜ì§€ ì•Šìœ¼ë©°, ì ì ˆíˆ ì¡°ì •ëœ êµ¬ì„±ì—ì„œ ì†Œìˆ˜ì˜ ê³ ìš©ëŸ‰ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹œìŠ¤í…œì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤.

- 4. íŠ¹ì • ë³‘ë ¬í™” ì¡°í•©ì€ ëŒ€ì—­í­ í™œìš©ì„ ì €í•´í•˜ë©°, ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸°ì˜ ê³¼ë„í•œ ì¦ê°€ëŠ” ì—´ ìŠ¤ë¡œí‹€ë§ì„ ì•…í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

- 5. í•˜ë“œì›¨ì–´, ì‹œìŠ¤í…œ í† í´ë¡œì§€, ëª¨ë¸ ì‹¤í–‰ ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì´ í›ˆë ¨ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.


---

*Generated on 2025-09-22 16:16:22*