---
keywords:
  - Large Language Models
  - Malware Behavior Auditing
  - Static Reachability Analysis
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 23:00:15.441454",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Large Language Models",
    "Malware Behavior Auditing",
    "Static Reachability Analysis"
  ],
  "rejected_keywords": [
    "Function-Level Structural Representations"
  ],
  "similarity_scores": {
    "Large Language Models": 0.8,
    "Malware Behavior Auditing": 0.78,
    "Static Reachability Analysis": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing

**Korean Title:** 분류를 넘어서: 세분화된 자동 악성코드 행동 감사에 대한 대형 언어 모델(LLM)의 평가

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250917|2025-09-17]]        [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Large Language Models|Large Language Models]], [[keywords/Static Reachability Analysis|Static Reachability Analysis]]
**⚡ Unique Technical**: [[keywords/Malware Behavior Auditing|Malware Behavior Auditing]]

## 🔗 유사한 논문
- [[MedVAL_ Toward Expert-Level Medical Text Validation with Language Models_20250919|MedVAL Toward Expert-Level Medical Text Validation with Language Models]] (83.7% similar)
- [[BEACON_ Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning_20250919|BEACON Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning]] (83.7% similar)
- [[MADAR_ Efficient Continual Learning for Malware Analysis with Distribution-Aware Replay_20250919|MADAR Efficient Continual Learning for Malware Analysis with Distribution-Aware Replay]] (83.6% similar)
- [[LNE-Blocking_ An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models_20250918|LNE-Blocking An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models]] (83.3% similar)
- [[Simulating a Bias Mitigation Scenario in Large Language Models_20250917|Simulating a Bias Mitigation Scenario in Large Language Models]] (83.0% similar)

## 📋 저자 정보

**Authors:** Xinran Zheng, Xingzhi Qian, Yiling He, Shuo Yang, Lorenzo Cavallaro

## 📄 Abstract (원문)

Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

## 🔍 Abstract (한글 번역)

자동화된 악성코드 분류는 강력한 탐지 성능을 달성했습니다. 그러나 악성코드 행동 감사는 악의적인 활동에 대한 인과적이고 검증 가능한 설명을 추구합니다. 이는 악성코드가 무엇을 하는지를 밝히는 것뿐만 아니라 그러한 주장을 증거로 뒷받침하는 데 필수적입니다. 이 작업은 도전적입니다. 왜냐하면 적대적인 의도가 종종 복잡하고 프레임워크가 무거운 애플리케이션 내에 숨겨져 있어 수동 감사가 느리고 비용이 많이 들기 때문입니다. 대형 언어 모델(LLM)은 이 격차를 해소하는 데 도움을 줄 수 있지만, 세 가지 제한 사항으로 인해 감사 잠재력은 대부분 탐구되지 않았습니다: (1) 공정한 평가를 위한 세밀한 주석의 부족; (2) 악성 신호를 가리는 풍부한 정상 코드; (3) 검증할 수 없고 환각에 취약한 출력이 속성의 신뢰성을 저해함. 이 격차를 해소하기 위해 우리는 MalEval을 소개합니다. 이는 세밀한 안드로이드 악성코드 감사를 위한 포괄적인 프레임워크로, 실제 제약 조건 하에서 LLM이 감사를 얼마나 효과적으로 지원하는지를 평가하도록 설계되었습니다. MalEval은 전문가가 검증한 보고서와 업데이트된 민감한 API 목록을 제공하여 진실의 부족을 완화하고 정적 도달 가능성 분석을 통해 잡음을 줄입니다. 함수 수준의 구조적 표현은 검증 가능한 평가를 위한 중간 속성 단위로 사용됩니다. 이를 바탕으로 우리는 네 가지 분석가 정렬 작업 -- 함수 우선순위 지정, 증거 속성, 행동 합성, 샘플 구별 -- 을 도메인별 메트릭 및 통합된 작업량 지향 점수와 함께 정의합니다. 우리는 최근 악성코드와 오분류된 정상 앱의 큐레이션된 데이터셋에서 널리 사용되는 7개의 LLM을 평가하여 그들의 감사 능력에 대한 최초의 체계적인 평가를 제공합니다. MalEval은 감사 단계 전반에 걸쳐 유망한 잠재력과 중요한 한계를 모두 드러내며, LLM 강화 악성코드 행동 감사에 대한 미래 연구의 재현 가능한 벤치마크와 기초를 제공합니다. MalEval은 https://github.com/ZhengXR930/MalEval.git에서 공개적으로 이용 가능합니다.

## 📝 요약

이 논문은 자동화된 악성코드 분류의 한계를 극복하기 위해 MalEval이라는 프레임워크를 제안합니다. MalEval은 대형 언어 모델(LLM)을 활용하여 안드로이드 악성코드의 세밀한 감사 작업을 지원하며, 실제 환경에서의 효과성을 평가합니다. 이를 위해 전문가 검증 보고서와 최신 민감 API 목록을 제공하고, 정적 도달 가능성 분석을 통해 노이즈를 줄입니다. 기능 우선순위 결정, 증거 귀속, 행동 합성, 샘플 구분의 네 가지 작업을 정의하고, 도메인별 지표와 통합 점수를 제시합니다. 7개의 LLM을 평가한 결과, MalEval은 LLM의 감사 능력의 잠재력과 한계를 드러내며, 향후 연구의 기초를 제공합니다.

## 🎯 주요 포인트

- 1. 자동화된 악성코드 분류는 강력한 탐지 성능을 보였으나, 악성코드 행동 감사는 인과적이고 검증 가능한 설명을 요구하여 수작업 감사가 느리고 비용이 많이 든다.

- 2. 대형 언어 모델(LLM)은 악성코드 감사의 잠재력을 가지고 있지만, 세 가지 제한 사항으로 인해 그 가능성이 충분히 탐구되지 않았다.

- 3. MalEval은 LLM이 실제 환경에서 악성코드 감사를 얼마나 효과적으로 지원하는지를 평가하기 위한 종합적인 프레임워크로, 전문가 검증 보고서와 업데이트된 민감 API 목록을 제공한다.

- 4. MalEval은 기능 우선순위화, 증거 귀속, 행동 합성, 샘플 구분의 네 가지 분석가 정렬 작업과 도메인별 메트릭 및 통합된 작업량 지향 점수를 정의한다.

- 5. MalEval은 LLM의 감사 능력을 체계적으로 평가하여 감사 단계 전반에 걸쳐 유망한 잠재력과 중요한 한계를 드러내며, LLM 기반 악성코드 행동 감사 연구의 기초를 제공한다.

---

*Generated on 2025-09-20 07:38:39*