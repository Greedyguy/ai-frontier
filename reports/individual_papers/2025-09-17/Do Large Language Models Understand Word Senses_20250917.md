# Do Large Language Models Understand Word Senses?

**Korean Title:** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ”ê°€?

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Domenico Meconi|Domenico Meconi]] [[authors/Simone Stirpe|Simone Stirpe]] [[authors/Federico Martelli|Federico Martelli]] [[authors/Leonardo Lavalle|Leonardo Lavalle]] [[authors/Roberto Navigli|Roberto Navigli]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Instruction-tuned LLMs

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Large Multi-modal Models Can Interpret Features in Large Multi-modal Models_20250919|Large Multi-modal Models Can Interpret Features in Large Multi-modal Models]] (85.0% similar)
- [[Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (84.7% similar)
- [[Correct-Detect_ Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs_20250917|Correct-Detect Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs]] (84.1% similar)
- [[From Automation to Autonomy_ A Survey on Large Language Models in Scientific Discovery_20250918|From Automation to Autonomy A Survey on Large Language Models in Scientific Discovery]] (83.9% similar)
- [[DetectAnyLLM_ Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models_20250919|DetectAnyLLM Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models]] (83.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Domenico Meconi, Simone Stirpe, Federico Martelli, Leonardo Lavalle, Roberto Navigli

## ğŸ“„ Abstract (ì›ë¬¸)

Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë¬¸ë§¥ì—ì„œ ì´í•´í•˜ëŠ” ê²ƒì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê¸°ë³¸ì ì¸ ëŠ¥ë ¥ì…ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ í‰ê°€ ë…¸ë ¥ì—ë„ ë¶ˆêµ¬í•˜ê³ , LLMì´ ì‹¤ì œë¡œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì§„ì •ìœ¼ë¡œ ì´í•´í•˜ê³  ìˆë‹¤ëŠ” ì¦ê±°ë¥¼ ì–´ëŠ ì •ë„ ë³´ì—¬ì£¼ëŠ”ì§€ëŠ” ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” i) ì§€ì‹œ ì¡°ì •ëœ LLMì˜ ë‹¨ì–´ ì˜ë¯¸ ì¤‘ì˜ì„± í•´ì†Œ(WSD) ëŠ¥ë ¥ì„ í‰ê°€í•˜ì—¬, ì´ ì‘ì—…ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ ìµœì²¨ë‹¨ ì‹œìŠ¤í…œê³¼ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³ , ii) ì •ì˜ ìƒì„±, ììœ  í˜•ì‹ ì„¤ëª…, ì˜ˆì œ ìƒì„±ì˜ ì„¸ ê°€ì§€ ìƒì„± ì„¤ì •ì—ì„œ ë‘ ê°œì˜ ìµœê³  ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ë° í´ë¡œì¦ˆë“œ ì†ŒìŠ¤ LLMì´ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•¨ìœ¼ë¡œì¨ ì´ ê²©ì°¨ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì£¼ëª©í•  ë§Œí•œ ì ì€, WSD ì‘ì—…ì—ì„œ GPT-4o ë° DeepSeek-V3ì™€ ê°™ì€ ì„ ë„ì ì¸ ëª¨ë¸ì´ ì „ë¬¸í™”ëœ WSD ì‹œìŠ¤í…œê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œë„, ë„ë©”ì¸ ë° ë‚œì´ë„ ìˆ˜ì¤€ì— ê±¸ì³ ë” í° ê²¬ê³ ì„±ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìƒì„± ì‘ì—…ì—ì„œëŠ” LLMì´ ë¬¸ë§¥ì—ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ìµœëŒ€ 98% ì •í™•ë„ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë“¤ì˜ ìƒì„± ëŠ¥ë ¥ê³¼ ê°€ì¥ ì˜ ë§ëŠ” ììœ  í˜•ì‹ ì„¤ëª… ì‘ì—…ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì´ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë¬¸ë§¥ ë‚´ ë‹¨ì–´ ì˜ë¯¸ ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì²«ì§¸, ëª…ë ¹ ì¡°ì •ëœ LLMì˜ ë‹¨ì–´ ì˜ë¯¸ ì¤‘ì˜ì„± í•´ì†Œ(WSD) ëŠ¥ë ¥ì„ í‰ê°€í•˜ì—¬, ì´ë“¤ì´ í•´ë‹¹ ì‘ì—…ì— íŠ¹í™”ëœ ìµœì²¨ë‹¨ ì‹œìŠ¤í…œê³¼ ë¹„êµí–ˆì„ ë•Œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, GPT-4oì™€ DeepSeek-V3 ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ê³¼ ë‚œì´ë„ì—ì„œ ë†’ì€ ê²¬ê³ ì„±ì„ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, ë‘ ê°œì˜ LLMì´ ì •ì˜ ìƒì„±, ììœ  í˜•ì‹ ì„¤ëª…, ì˜ˆì‹œ ìƒì„±ì˜ ì„¸ ê°€ì§€ ìƒì„± ì„¤ì •ì—ì„œ ë‹¨ì–´ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ììœ  í˜•ì‹ ì„¤ëª… ì‘ì—…ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ë³´ì´ë©°, ë¬¸ë§¥ ë‚´ ë‹¨ì–´ ì˜ë¯¸ë¥¼ ìµœëŒ€ 98%ì˜ ì •í™•ë„ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë¬¸ë§¥ì—ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì´ ì¤‘ìš”í•˜ë‹¤.

- 2. LLMì˜ ë‹¨ì–´ ì˜ë¯¸ êµ¬ë³„(WSD) ëŠ¥ë ¥ì€ ì „ë¬¸ ì‹œìŠ¤í…œê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

- 3. GPT-4oì™€ DeepSeek-V3ëŠ” ë‹¤ì–‘í•œ ë„ë©”ì¸ê³¼ ë‚œì´ë„ì—ì„œ ë›°ì–´ë‚œ ê²¬ê³ ì„±ì„ ë³´ì¸ë‹¤.

- 4. LLMì€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ëŠ” ìƒì„± ì‘ì—…ì—ì„œ ìµœëŒ€ 98%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤.

- 5. ììœ  í˜•ì‹ ì„¤ëª… ì‘ì—…ì—ì„œ LLMì˜ ìƒì„± ëŠ¥ë ¥ì´ ê°€ì¥ ì˜ ë°œíœ˜ëœë‹¤.

---

*Generated on 2025-09-20 09:25:13*