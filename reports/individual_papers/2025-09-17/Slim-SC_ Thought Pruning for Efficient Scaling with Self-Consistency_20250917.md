# Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency

**Korean Title:** ìŠ¬ë¦¼-SC: ìê¸° ì¼ê´€ì„±ì„ í†µí•œ íš¨ìœ¨ì ì¸ í™•ì¥ì„ ìœ„í•œ ì‚¬ê³  ê°€ì§€ì¹˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Colin Hong|Colin Hong]] [[authors/Xu Guo|Xu Guo]] [[authors/Anand Chaanan Singh|Anand Chaanan Singh]] [[authors/Esha Choukse|Esha Choukse]] [[authors/Dmitrii Ustiugov|Dmitrii Ustiugov]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Thought Pruning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Reasoning Efficiently Through Adaptive Chain-of-Thought Compression_ A Self-Optimizing Framework_20250917|Reasoning Efficiently Through Adaptive Chain-of-Thought Compression A Self-Optimizing Framework]] (83.8% similar)
- [[ASCoT_ An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs_20250919|ASCoT An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs]] (83.4% similar)
- [[Early Stopping Chain-of-thoughts in Large Language Models_20250918|Early Stopping Chain-of-thoughts in Large Language Models]] (82.5% similar)
- [[Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies_20250918|Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies]] (80.9% similar)
- [[From Correction to Mastery_ Reinforced Distillation of Large Language Model Agents_20250919|From Correction to Mastery Reinforced Distillation of Large Language Model Agents]] (80.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Colin Hong, Xu Guo, Anand Chaanan Singh, Esha Choukse, Dmitrii Ustiugov

## ğŸ“„ Abstract (ì›ë¬¸)

Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ìµœê·¼ í…ŒìŠ¤íŠ¸ ì‹œ í™•ì¥(Test-Time Scaling, TTS)ì€ ëª¨ë¸ì„ ì¬í›ˆë ¨í•˜ì§€ ì•Šê³ ë„ í…ŒìŠ¤íŠ¸ ì‹œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì ì  ë” ë§ì€ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ëª©í•  ë§Œí•œ TTS ê¸°ë²• ì¤‘ í•˜ë‚˜ëŠ” ìê¸° ì¼ê´€ì„±(Self-Consistency, SC)ìœ¼ë¡œ, ì—¬ëŸ¬ ê°œì˜ ì¶”ë¡  ì²´ì¸ì„ ë³‘ë ¬ë¡œ ìƒì„±í•˜ê³  ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ í†µí•´ ìµœì¢… ë‹µë³€ì„ ì„ íƒí•©ë‹ˆë‹¤. íš¨ê³¼ì ì´ê¸´ í•˜ì§€ë§Œ, ëŒ€ê·œëª¨ì˜ ê³„ì‚° ì˜¤ë²„í—¤ë“œëŠ” ê´‘ë²”ìœ„í•œ ë°°í¬ë¥¼ ì œí•œí•©ë‹ˆë‹¤. SCë¥¼ ê°€ì†í™”í•˜ë ¤ëŠ” ì´ì „ì˜ ì‹œë„ë“¤ì€ ì£¼ë¡œ ëª¨ë¸ ê¸°ë°˜ì˜ ì‹ ë¢°ë„ ì ìˆ˜ë‚˜ ê²½í—˜ì  ì§€ì›ì´ ì œí•œì ì¸ íœ´ë¦¬ìŠ¤í‹±ì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì²˜ìŒìœ¼ë¡œ SCì˜ ë¹„íš¨ìœ¨ì„±ì„ ì´ë¡ ì  ë° ê²½í—˜ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„ ì„ ìœ„í•œ ì‹¤ì§ˆì ì¸ ê¸°íšŒë¥¼ ë°í˜€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ëŠ” Slim-SCë¼ëŠ” ë‹¨ê³„ë³„ ê°€ì§€ì¹˜ê¸° ì „ëµì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ì‚¬ê³  ìˆ˜ì¤€ì—ì„œ ì²´ì¸ ê°„ ìœ ì‚¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µëœ ì²´ì¸ì„ ì‹ë³„í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ STEM ì¶”ë¡  ë°ì´í„°ì…‹ê³¼ ë‘ ê°€ì§€ ìµœì‹  LLM ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, Slim-SCëŠ” R1-Distillì„ í†µí•´ ì¶”ë¡  ì§€ì—° ì‹œê°„ê³¼ KVC ì‚¬ìš©ì„ ê°ê° ìµœëŒ€ 45% ë° 26%ê¹Œì§€ ì¤„ì´ë©´ì„œë„ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ í–¥ìƒì‹œì¼œ SCì— ëŒ€í•œ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ìœ¨ì ì¸ TTS ëŒ€ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ LLMì˜ ì¶”ë¡  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ Test-Time Scaling(TTS) ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ Self-Consistency(SC)ì˜ ë¹„íš¨ìœ¨ì„±ì„ ì´ë¡ ì , ì‹¤í—˜ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„  ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. SCëŠ” ë‹¤ìˆ˜ê²°ì„ í†µí•´ ìµœì¢… ë‹µë³€ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ íš¨ê³¼ì ì´ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë¹„ìš©ì´ ë¬¸ì œì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ Slim-SCëŠ” ì‚¬ê³  ìˆ˜ì¤€ì—ì„œì˜ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¶”ë¡  ì²´ì¸ì„ ì œê±°í•˜ëŠ” ë‹¨ê³„ì  ê°€ì§€ì¹˜ê¸° ì „ëµì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, Slim-SCëŠ” ì„¸ ê°€ì§€ STEM ì¶”ë¡  ë°ì´í„°ì…‹ê³¼ ë‘ ê°€ì§€ LLM ì•„í‚¤í…ì²˜ì—ì„œ ì¶”ë¡  ì§€ì—°ê³¼ KVC ì‚¬ìš©ì„ ê°ê° ìµœëŒ€ 45%ì™€ 26% ì¤„ì´ë©´ì„œë„ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” SCì˜ íš¨ìœ¨ì ì¸ ëŒ€ì•ˆìœ¼ë¡œ ì œì‹œë©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Test-Time Scaling (TTS)ëŠ” ëª¨ë¸ì„ ì¬í›ˆë ¨í•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ ì‹œ LLMì˜ ì¶”ë¡  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

- 2. Self-Consistency (SC)ëŠ” ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì¶”ë¡  ì²´ì¸ì„ ìƒì„±í•˜ê³  ë‹¤ìˆ˜ê²°ë¡œ ìµœì¢… ë‹µì„ ì„ íƒí•˜ëŠ” TTS ê¸°ë²•ì´ì§€ë§Œ, ê³„ì‚° ë¹„ìš©ì´ ë†’ì•„ ë„ë¦¬ ì‚¬ìš©ë˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

- 3. Slim-SCëŠ” ì²´ì¸ ê°„ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì²´ì¸ì„ ì œê±°í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì§€ì¹˜ê¸° ì „ëµìœ¼ë¡œ, SCì˜ ë¹„íš¨ìœ¨ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.

- 4. Slim-SCëŠ” ì„¸ ê°€ì§€ STEM ì¶”ë¡  ë°ì´í„°ì…‹ê³¼ ë‘ ê°€ì§€ ìµœì‹  LLM ì•„í‚¤í…ì²˜ ì‹¤í—˜ì—ì„œ ì¶”ë¡  ì§€ì—°ê³¼ KVC ì‚¬ìš©ì„ ê°ê° ìµœëŒ€ 45%ì™€ 26% ì¤„ì´ë©´ì„œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

- 5. Slim-SCëŠ” SCë¥¼ ìœ„í•œ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ìœ¨ì ì¸ TTS ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆë©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:20:54*