# Apertus: Democratizing Open and Compliant LLMs for Global Language Environments

**Korean Title:** Apertus: ê¸€ë¡œë²Œ ì–¸ì–´ í™˜ê²½ì„ ìœ„í•œ ê°œë°©ì ì´ê³  ê·œë²”ì ì¸ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë¯¼ì£¼í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Alejandro HernÃ¡ndez-Cano|Alejandro HernÃ¡ndez-Cano]] [[authors/Alexander HÃ¤gele|Alexander HÃ¤gele]] [[authors/Allen Hao Huang|Allen Hao Huang]] [[authors/Angelika Romanou|Angelika Romanou]] [[authors/Antoni-Joan Solergibert|Antoni-Joan Solergibert]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Goldfish Objective

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[LNE-Blocking_ An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models_20250918|LNE-Blocking An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models]] (82.3% similar)
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (82.0% similar)
- [[Enterprise AI Must Enforce Participant-Aware Access Control_20250919|Enterprise AI Must Enforce Participant-Aware Access Control]] (81.3% similar)
- [[Reveal and Release_ Iterative LLM Unlearning with Self-generated Data_20250918|Reveal and Release Iterative LLM Unlearning with Self-generated Data]] (81.3% similar)
- [[MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE Metadata Extraction and Validation in Scientific Papers Using LLMs]] (80.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Alejandro HernÃ¡ndez-Cano, Alexander HÃ¤gele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Äurech, Ido Hakimi, Juan GarcÃ­a Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko SabolÄec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian BÃ¶sch, Maximilian BÃ¶ther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, MarÃ­a Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike LÃ¼beck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique MendoncÃ§a, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, LÃ©o Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian TramÃ¨r, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, Imanol Schlag

## ğŸ“„ Abstract (ì›ë¬¸)

We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ApertusëŠ” í˜„ì¬ì˜ ì˜¤í”ˆ ëª¨ë¸ ìƒíƒœê³„ì—ì„œ ë‘ ê°€ì§€ ì²´ê³„ì ì¸ ê²°í•¨ì¸ ë°ì´í„° ì¤€ìˆ˜ì™€ ë‹¤êµ­ì–´ í‘œí˜„ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì™„ì „ ê°œë°©í˜• ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs) ëª¨ìŒì…ë‹ˆë‹¤. ë§ì€ ì´ì „ ëª¨ë¸ë“¤ì´ ì¬í˜„ ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì—†ì´ ë˜ëŠ” ì½˜í…ì¸  ì†Œìœ ìì˜ ê¶Œë¦¬ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ê°€ì¤‘ì¹˜ë¥¼ ê³µê°œí•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, Apertus ëª¨ë¸ì€ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ë˜ë©°, ì‚¬í›„ì ìœ¼ë¡œ robots.txt ì œì™¸ ê·œì •ì„ ì¤€ìˆ˜í•˜ê³  ë¹„í—ˆìš©, ìœ í•´, ê°œì¸ ì‹ë³„ ê°€ëŠ¥ ì½˜í…ì¸ ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤. ê¸°ì–µí™”ì˜ ìœ„í—˜ì„ ì™„í™”í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì‚¬ì „ í›ˆë ¨ ì¤‘ Goldfish ëª©í‘œë¥¼ ì±„íƒí•˜ì—¬ ë°ì´í„°ì˜ ë¬¸ì ê·¸ëŒ€ë¡œì˜ íšŒìƒì„ ê°•í•˜ê²Œ ì–µì œí•˜ë©´ì„œë„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. Apertus ëª¨ë¸ì€ ë˜í•œ ë‹¤êµ­ì–´ ë²”ìœ„ë¥¼ í™•ì¥í•˜ì—¬ 1800ê°œ ì´ìƒì˜ ì–¸ì–´ì—ì„œ 15T í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ë©°, ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì˜ ì•½ 40%ê°€ ë¹„ì˜ì–´ ì½˜í…ì¸ ì— í• ë‹¹ë©ë‹ˆë‹¤. 8B ë° 70B ê·œëª¨ë¡œ ì¶œì‹œëœ ApertusëŠ” ë‹¤êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì™„ì „ ê°œë°©í˜• ëª¨ë¸ ì¤‘ ìµœì²¨ë‹¨ ê²°ê³¼ì— ì ‘ê·¼í•˜ë©°, ê°œë°©í˜• ê°€ì¤‘ì¹˜ ëª¨ë¸ê³¼ ê²½ìŸí•˜ê±°ë‚˜ ì´ë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤. ëª¨ë¸ ê°€ì¤‘ì¹˜ ì™¸ì—ë„, ìš°ë¦¬ëŠ” ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸, ì²´í¬í¬ì¸íŠ¸, í‰ê°€ ìŠ¤ìœ„íŠ¸, í›ˆë ¨ ì½”ë“œ ë“± ê°œë°œ ì£¼ê¸°ì˜ ëª¨ë“  ê³¼í•™ì  ì‚°ì¶œë¬¼ì„ í—ˆìš©ì ì¸ ë¼ì´ì„ ìŠ¤ë¡œ ê³µê°œí•˜ì—¬ íˆ¬ëª…í•œ ê°ì‚¬ì™€ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ApertusëŠ” ë°ì´í„° ì¤€ìˆ˜ì™€ ë‹¤êµ­ì–´ í‘œí˜„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµë˜ë©°, robots.txt ì œì™¸ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ê³  ë¹„í—ˆê°€, ìœ í•´, ê°œì¸ ì‹ë³„ ê°€ëŠ¥ ì½˜í…ì¸ ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤. ë°ì´í„° ì•”ê¸°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ Goldfish ëª©í‘œë¥¼ ì±„íƒí•˜ì—¬ ë°ì´í„°ì˜ ì •í™•í•œ íšŒìƒì„ ì–µì œí•˜ë©´ì„œë„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. 1800ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ êµ¬ì„±ëœ 15ì¡° ê°œì˜ í† í°ì„ í•™ìŠµí•˜ì—¬ ë‹¤êµ­ì–´ ì§€ì›ì„ í™•ì¥í•˜ë©°, ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì˜ ì•½ 40%ëŠ” ë¹„ì˜ì–´ ì½˜í…ì¸ ì— í• ë‹¹ë˜ì—ˆìŠµë‹ˆë‹¤. 8B ë° 70B ê·œëª¨ë¡œ ì¶œì‹œëœ ApertusëŠ” ë‹¤êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ì— ë„ë‹¬í•˜ë©°, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸, ì²´í¬í¬ì¸íŠ¸, í‰ê°€ ë„êµ¬ ë° í›ˆë ¨ ì½”ë“œë¥¼ í¬í•¨í•œ ëª¨ë“  ê³¼í•™ì  ì‚°ì¶œë¬¼ì„ ê³µê°œí•˜ì—¬ íˆ¬ëª…í•œ ê°ì‚¬ì™€ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ApertusëŠ” ë°ì´í„° ì¤€ìˆ˜ì™€ ë‹¤êµ­ì–´ í‘œí˜„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì…ë‹ˆë‹¤.

- 2. Apertus ëª¨ë¸ì€ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ë˜ë©°, robots.txt ì œì™¸ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ê³  ë¹„í—ˆìš©, ìœ í•´, ê°œì¸ ì‹ë³„ ê°€ëŠ¥ ì½˜í…ì¸ ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.

- 3. ê¸°ì–µí™” ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´ Goldfish ëª©í‘œë¥¼ ì±„íƒí•˜ì—¬ ë°ì´í„°ì˜ ë¬¸ì ê·¸ëŒ€ë¡œ íšŒìƒì„ ì–µì œí•˜ë©´ì„œë„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.

- 4. Apertus ëª¨ë¸ì€ 1800ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ êµ¬ì„±ëœ 15ì¡° ê°œì˜ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤êµ­ì–´ ë²”ìœ„ë¥¼ í™•ì¥í•˜ë©°, ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì˜ ì•½ 40%ë¥¼ ë¹„ì˜ì–´ ì½˜í…ì¸ ì— í• ë‹¹í•©ë‹ˆë‹¤.

- 5. ëª¨ë¸ ê°€ì¤‘ì¹˜ ì™¸ì—ë„ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸, ì²´í¬í¬ì¸íŠ¸, í‰ê°€ ìŠ¤ìœ„íŠ¸ ë° í›ˆë ¨ ì½”ë“œë¥¼ í¬í•¨í•œ ëª¨ë“  ê³¼í•™ì  ì‚°ì¶œë¬¼ì„ í—ˆìš©ì ì¸ ë¼ì´ì„ ìŠ¤ë¡œ ê³µê°œí•˜ì—¬ íˆ¬ëª…í•œ ê°ì‚¬ì™€ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 07:40:05*