---
keywords:
  - Linear Regression
  - Gaussian Approximation
  - Convergence Rate
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:49:08.209649",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Linear Regression",
    "Gaussian Approximation",
    "Convergence Rate"
  ],
  "rejected_keywords": [
    "Design Matrix"
  ],
  "similarity_scores": {
    "Linear Regression": 0.85,
    "Gaussian Approximation": 0.78,
    "Convergence Rate": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# On the Rate of Gaussian Approximation for Linear Regression Problems

**Korean Title:** ì„ í˜• íšŒê·€ ë¬¸ì œì— ëŒ€í•œ ê°€ìš°ì‹œì•ˆ ê·¼ì‚¬ì˜ ì†ë„ì— ê´€í•˜ì—¬

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250917|2025-09-17]]       [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Linear Regression|linear regression]]
**âš¡ Unique Technical**: [[keywords/Gaussian Approximation|Gaussian approximation]], [[keywords/Convergence Rate|convergence rate]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (77.8% similar)
- [[Optimal Learning from Label Proportions with General Loss Functions_20250919|Optimal Learning from Label Proportions with General Loss Functions]] (77.0% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (76.9% similar)
- [[Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (76.7% similar)
- [[Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis_20250918|Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis]] (76.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Marat Khusainov, Marina Sheshukova, Alain Durmus, Sergey Samsonov

## ğŸ“„ Abstract (ì›ë¬¸)

In this paper, we consider the problem of Gaussian approximation for the
online linear regression task. We derive the corresponding rates for the
setting of a constant learning rate and study the explicit dependence of the
convergence rate upon the problem dimension $d$ and quantities related to the
design matrix. When the number of iterations $n$ is known in advance, our
results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$,
provided that the sample size $n$ is large enough.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì˜¨ë¼ì¸ ì„ í˜• íšŒê·€ ì‘ì—…ì— ëŒ€í•œ ê°€ìš°ì‹œì•ˆ ê·¼ì‚¬ì˜ ë¬¸ì œë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¼ì •í•œ í•™ìŠµë¥  ì„¤ì •ì— ëŒ€í•œ í•´ë‹¹ ë¹„ìœ¨ì„ ë„ì¶œí•˜ê³ , ë¬¸ì œì˜ ì°¨ì› $d$ ë° ì„¤ê³„ í–‰ë ¬ê³¼ ê´€ë ¨ëœ ì–‘ì— ëŒ€í•œ ìˆ˜ë ´ ì†ë„ì˜ ëª…ì‹œì  ì˜ì¡´ì„±ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ë°˜ë³µ íšŸìˆ˜ $n$ì´ ì‚¬ì „ì— ì•Œë ¤ì§„ ê²½ìš°, ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” í‘œë³¸ í¬ê¸° $n$ì´ ì¶©ë¶„íˆ í° ê²½ìš°, $\sqrt{\log{n}/n}$ ìˆœì„œì˜ ì •ê·œ ê·¼ì‚¬ ë¹„ìœ¨ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì˜¨ë¼ì¸ ì„ í˜• íšŒê·€ ì‘ì—…ì— ëŒ€í•œ ê°€ìš°ì‹œì•ˆ ê·¼ì‚¬ì˜ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì¼ì •í•œ í•™ìŠµë¥  ì„¤ì •ì—ì„œ ë¬¸ì œ ì°¨ì› $d$ì™€ ì„¤ê³„ í–‰ë ¬ê³¼ ê´€ë ¨ëœ ì–‘ì— ë”°ë¥¸ ìˆ˜ë ´ ì†ë„ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤. ì‚¬ì „ì ìœ¼ë¡œ ë°˜ë³µ íšŸìˆ˜ $n$ì´ ì•Œë ¤ì§„ ê²½ìš°, ì¶©ë¶„íˆ í° ìƒ˜í”Œ í¬ê¸° $n$ì—ì„œ $\sqrt{\log{n}/n}$ì˜ ì •ê·œ ê·¼ì‚¬ ì†ë„ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ìˆ˜ë ´ ì†ë„ì˜ ëª…ì‹œì  ì˜ì¡´ì„±ì„ ë¶„ì„í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì€ ì˜¨ë¼ì¸ ì„ í˜• íšŒê·€ ì‘ì—…ì—ì„œ ê°€ìš°ì‹œì•ˆ ê·¼ì‚¬ì˜ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

- 2. ì¼ì •í•œ í•™ìŠµë¥  ì„¤ì •ì—ì„œ ìˆ˜ë ´ ì†ë„ì˜ ë¬¸ì œ ì°¨ì› $d$ì™€ ì„¤ê³„ í–‰ë ¬ê³¼ ê´€ë ¨ëœ ì–‘ì— ëŒ€í•œ ëª…ì‹œì  ì˜ì¡´ì„±ì„ ì—°êµ¬í•©ë‹ˆë‹¤.

- 3. ì‚¬ì „ ì•Œê³  ìˆëŠ” ë°˜ë³µ íšŸìˆ˜ $n$ì— ëŒ€í•´, í‘œë³¸ í¬ê¸° $n$ì´ ì¶©ë¶„íˆ í´ ê²½ìš° $\sqrt{\log{n}/n}$ ìˆœì„œì˜ ì •ìƒ ê·¼ì‚¬ ì†ë„ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:14:52*