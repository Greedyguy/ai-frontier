---
keywords:
  - Linear Regression
  - Gaussian Approximation
  - Convergence Rate
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:49:08.209649",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Linear Regression",
    "Gaussian Approximation",
    "Convergence Rate"
  ],
  "rejected_keywords": [
    "Design Matrix"
  ],
  "similarity_scores": {
    "Linear Regression": 0.85,
    "Gaussian Approximation": 0.78,
    "Convergence Rate": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# On the Rate of Gaussian Approximation for Linear Regression Problems

**Korean Title:** 선형 회귀 문제에 대한 가우시안 근사의 속도에 관하여

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250917|2025-09-17]]       [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🌐 Broad Technical**: [[keywords/Linear Regression|linear regression]]
**⚡ Unique Technical**: [[keywords/Gaussian Approximation|Gaussian approximation]], [[keywords/Convergence Rate|convergence rate]]

## 🔗 유사한 논문
- [[Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (77.8% similar)
- [[Optimal Learning from Label Proportions with General Loss Functions_20250919|Optimal Learning from Label Proportions with General Loss Functions]] (77.0% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (76.9% similar)
- [[Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (76.7% similar)
- [[Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis_20250918|Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis]] (76.6% similar)

## 📋 저자 정보

**Authors:** Marat Khusainov, Marina Sheshukova, Alain Durmus, Sergey Samsonov

## 📄 Abstract (원문)

In this paper, we consider the problem of Gaussian approximation for the
online linear regression task. We derive the corresponding rates for the
setting of a constant learning rate and study the explicit dependence of the
convergence rate upon the problem dimension $d$ and quantities related to the
design matrix. When the number of iterations $n$ is known in advance, our
results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$,
provided that the sample size $n$ is large enough.

## 🔍 Abstract (한글 번역)

이 논문에서는 온라인 선형 회귀 작업에 대한 가우시안 근사의 문제를 고려합니다. 우리는 일정한 학습률 설정에 대한 해당 비율을 도출하고, 문제의 차원 $d$ 및 설계 행렬과 관련된 양에 대한 수렴 속도의 명시적 의존성을 연구합니다. 반복 횟수 $n$이 사전에 알려진 경우, 우리의 결과는 표본 크기 $n$이 충분히 큰 경우, $\sqrt{\log{n}/n}$ 순서의 정규 근사 비율을 제공합니다.

## 📝 요약

이 논문에서는 온라인 선형 회귀 작업에 대한 가우시안 근사의 문제를 다룹니다. 일정한 학습률 설정에서 문제 차원 $d$와 설계 행렬과 관련된 양에 따른 수렴 속도를 도출했습니다. 사전적으로 반복 횟수 $n$이 알려진 경우, 충분히 큰 샘플 크기 $n$에서 $\sqrt{\log{n}/n}$의 정규 근사 속도를 제시합니다. 주요 기여는 수렴 속도의 명시적 의존성을 분석한 것입니다.

## 🎯 주요 포인트

- 1. 본 논문은 온라인 선형 회귀 작업에서 가우시안 근사의 문제를 다룹니다.

- 2. 일정한 학습률 설정에서 수렴 속도의 문제 차원 $d$와 설계 행렬과 관련된 양에 대한 명시적 의존성을 연구합니다.

- 3. 사전 알고 있는 반복 횟수 $n$에 대해, 표본 크기 $n$이 충분히 클 경우 $\sqrt{\log{n}/n}$ 순서의 정상 근사 속도를 도출합니다.

---

*Generated on 2025-09-20 09:14:52*