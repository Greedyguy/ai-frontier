---
keywords:
  - Graph Neural Networks
  - Optimization
  - Partial Separability
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:57:31.935274",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Graph Neural Networks",
    "Optimization",
    "Partial Separability"
  ],
  "rejected_keywords": [
    "Randomized Local Coordination"
  ],
  "similarity_scores": {
    "Graph Neural Networks": 0.82,
    "Optimization": 0.78,
    "Partial Separability": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Decentralized Optimization with Topology-Independent Communication

**Korean Title:** í† í´ë¡œì§€ ë…ë¦½ì  í†µì‹ ì„ í†µí•œ ë¶„ì‚° ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250917|2025-09-17]]       [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Optimization|Distributed optimization]]
**ğŸ”— Specific Connectable**: [[keywords/Graph Neural Networks|Graph-guided regularizers]]
**âš¡ Unique Technical**: [[keywords/Partial Separability|Partial separability]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning_20250918|Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning]] (84.5% similar)
- [[Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (83.6% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (81.4% similar)
- [[Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters_20250919|Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters]] (79.7% similar)
- [[Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization_20250918|Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization]] (79.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Ying Lin, Yao Kuang, Ahmet Alacaoglu, Michael P. Friedlander

## ğŸ“„ Abstract (ì›ë¬¸)

Distributed optimization requires nodes to coordinate, yet full
synchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise
regularizers, standard methods demand $\mathcal{O}(m)$ communications per
iteration. This paper proposes randomized local coordination: each node
independently samples one regularizer uniformly and coordinates only with nodes
sharing that term. This exploits partial separability, where each regularizer
$G_j$ depends on a subset $S_j \subseteq \{1,\ldots,n\}$ of nodes. For
graph-guided regularizers where $|S_j|=2$, expected communication drops to
exactly 2 messages per iteration. This method achieves
$\tilde{\mathcal{O}}(\varepsilon^{-2})$ iterations for convex objectives and
under strong convexity, $\mathcal{O}(\varepsilon^{-1})$ to an
$\varepsilon$-solution and $\mathcal{O}(\log(1/\varepsilon))$ to a
neighborhood. Replacing the proximal map of the sum $\sum_j G_j$ with the
proximal map of a single randomly selected regularizer $G_j$ preserves
convergence while eliminating global coordination. Experiments validate both
convergence rates and communication efficiency across synthetic and real-world
datasets.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë¶„ì‚° ìµœì í™”ëŠ” ë…¸ë“œë“¤ì´ í˜‘ë ¥í•´ì•¼ í•˜ì§€ë§Œ, ì™„ì „í•œ ë™ê¸°í™”ëŠ” í™•ì¥ì„±ì´ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. $n$ê°œì˜ ë…¸ë“œê°€ $m$ê°œì˜ ìŒë³„ ì •ê·œí™” í•­ì„ í†µí•´ í˜‘ë ¥í•  ë•Œ, í‘œì¤€ ë°©ë²•ì€ ê° ë°˜ë³µë§ˆë‹¤ $\mathcal{O}(m)$ì˜ í†µì‹ ì„ ìš”êµ¬í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ë¬´ì‘ìœ„ ì§€ì—­ í˜‘ë ¥ì„ ì œì•ˆí•©ë‹ˆë‹¤: ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì •ê·œí™” í•­ì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ê³ , ê·¸ í•­ì„ ê³µìœ í•˜ëŠ” ë…¸ë“œë“¤ê³¼ë§Œ í˜‘ë ¥í•©ë‹ˆë‹¤. ì´ëŠ” ê° ì •ê·œí™” í•­ $G_j$ê°€ ë…¸ë“œì˜ ë¶€ë¶„ ì§‘í•© $S_j \subseteq \{1,\ldots,n\}$ì— ì˜ì¡´í•˜ëŠ” ë¶€ë¶„ì  ë¶„ë¦¬ ê°€ëŠ¥ì„±ì„ í™œìš©í•©ë‹ˆë‹¤. $|S_j|=2$ì¸ ê·¸ë˜í”„ ìœ ë„ ì •ê·œí™” í•­ì˜ ê²½ìš°, ê¸°ëŒ€ë˜ëŠ” í†µì‹ ëŸ‰ì€ ë°˜ë³µë‹¹ ì •í™•íˆ 2ê°œì˜ ë©”ì‹œì§€ë¡œ ê°ì†Œí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë³¼ë¡ ëª©ì  í•¨ìˆ˜ì— ëŒ€í•´ $\tilde{\mathcal{O}}(\varepsilon^{-2})$ì˜ ë°˜ë³µì„ ë‹¬ì„±í•˜ë©°, ê°•í•œ ë³¼ë¡ì„± í•˜ì—ì„œëŠ” $\varepsilon$-í•´ì— ëŒ€í•´ $\mathcal{O}(\varepsilon^{-1})$ ë° ê·¼ë°©ì— ëŒ€í•´ $\mathcal{O}(\log(1/\varepsilon))$ì˜ ë°˜ë³µì„ ë‹¬ì„±í•©ë‹ˆë‹¤. $\sum_j G_j$ì˜ ê·¼ì‚¬ ì‚¬ìƒ(proximal map)ì„ ë‹¨ì¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ì •ê·œí™” í•­ $G_j$ì˜ ê·¼ì‚¬ ì‚¬ìƒìœ¼ë¡œ ëŒ€ì²´í•¨ìœ¼ë¡œì¨, ì „ì—­ í˜‘ë ¥ì„ ì œê±°í•˜ë©´ì„œ ìˆ˜ë ´ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì‹¤í—˜ì€ í•©ì„± ë° ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ ìˆ˜ë ´ ì†ë„ì™€ í†µì‹  íš¨ìœ¨ì„±ì„ ëª¨ë‘ ê²€ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë¶„ì‚° ìµœì í™”ì—ì„œ ë…¸ë“œ ê°„ì˜ ì™„ì „í•œ ë™ê¸°í™”ê°€ ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ ë¡œì»¬ ì¡°ì • ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì •ê·œí™” í•­ì„ ìƒ˜í”Œë§í•˜ê³  í•´ë‹¹ í•­ì„ ê³µìœ í•˜ëŠ” ë…¸ë“œì™€ë§Œ ì¡°ì •í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¶€ë¶„ ê°€ë¶„ì„±ì„ í™œìš©í•˜ì—¬, ê·¸ë˜í”„ ê¸°ë°˜ ì •ê·œí™”ì˜ ê²½ìš° í†µì‹ ëŸ‰ì„ ë°˜ë³µë‹¹ í‰ê·  2ê°œì˜ ë©”ì‹œì§€ë¡œ ì¤„ì…ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë³¼ë¡ ëª©ì  í•¨ìˆ˜ì— ëŒ€í•´ $\tilde{\mathcal{O}}(\varepsilon^{-2})$ì˜ ë°˜ë³µì„ ë‹¬ì„±í•˜ë©°, ê°•í•œ ë³¼ë¡ì„± í•˜ì—ì„œëŠ” $\varepsilon$-í•´ì— ëŒ€í•´ $\mathcal{O}(\varepsilon^{-1})$, $\varepsilon$-ê·¼ë°©ì— ëŒ€í•´ $\mathcal{O}(\log(1/\varepsilon))$ì˜ ë°˜ë³µì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ì œì•ˆëœ ë°©ë²•ì´ ìˆ˜ë ´ ì†ë„ì™€ í†µì‹  íš¨ìœ¨ì„±ì„ ëª¨ë‘ í–¥ìƒì‹œí‚´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë¶„ì‚° ìµœì í™”ì—ì„œ ë…¸ë“œ ê°„ì˜ ì™„ì „í•œ ë™ê¸°í™”ëŠ” í™•ì¥ì„±ì´ ë‚®ìœ¼ë©°, ì œì•ˆëœ ë°©ë²•ì€ ë¬´ì‘ìœ„ ì§€ì—­ ì¡°ì •ì„ í†µí•´ ì´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.

- 2. ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ê·œì œìë¥¼ ìƒ˜í”Œë§í•˜ê³  í•´ë‹¹ ê·œì œìë¥¼ ê³µìœ í•˜ëŠ” ë…¸ë“œì™€ë§Œ ì¡°ì •í•˜ì—¬ í†µì‹ ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.

- 3. ê·¸ë˜í”„ ê¸°ë°˜ ê·œì œìì˜ ê²½ìš°, ê¸°ëŒ€ë˜ëŠ” í†µì‹  íšŸìˆ˜ëŠ” ë°˜ë³µë‹¹ ì •í™•íˆ 2ê°œì˜ ë©”ì‹œì§€ë¡œ ê°ì†Œí•©ë‹ˆë‹¤.

- 4. ì œì•ˆëœ ë°©ë²•ì€ ë³¼ë¡ ëª©ì  í•¨ìˆ˜ì— ëŒ€í•´ $\tilde{\mathcal{O}}(\varepsilon^{-2})$ ë°˜ë³µì„ ë‹¬ì„±í•˜ë©°, ê°•í•œ ë³¼ë¡ì„± í•˜ì—ì„œëŠ” $\varepsilon$-í•´ì— ëŒ€í•´ $\mathcal{O}(\varepsilon^{-1})$ ë°˜ë³µì„, ê·¼ë°©ì—ì„œëŠ” $\mathcal{O}(\log(1/\varepsilon))$ ë°˜ë³µì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼ëŠ” ì œì•ˆëœ ë°©ë²•ì˜ ìˆ˜ë ´ ì†ë„ì™€ í†µì‹  íš¨ìœ¨ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 05:54:34*