# Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures

**Korean Title:** ì–‘ì ê°•í™” í•™ìŠµ ê¸°ë°˜ í™•ì‚° ëª¨ë¸ì„ í†µí•œ ì´ë¯¸ì§€ í•©ì„±ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ì–‘ì-ê³ ì „ ìƒì„± ëª¨ë¸ ì•„í‚¤í…ì²˜

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Chi-Sheng Chen|Chi-Sheng Chen]] [[authors/En-Jui Kuo|En-Jui Kuo]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Proximal Policy Optimization, Variational Quantum Circuit

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (82.1% similar)
- [[Trainability of Quantum Models Beyond Known Classical Simulability_20250919|Trainability of Quantum Models Beyond Known Classical Simulability]] (81.6% similar)
- [[Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (81.1% similar)
- [[Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (80.7% similar)
- [[Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (80.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Chi-Sheng Chen, En-Jui Kuo

## ğŸ“„ Abstract (ì›ë¬¸)

Diffusion models typically employ static or heuristic classifier-free
guidance (CFG) schedules, which often fail to adapt across timesteps and noise
conditions. In this work, we introduce a quantum reinforcement learning (QRL)
controller that dynamically adjusts CFG at each denoising step. The controller
adopts a hybrid quantum--classical actor--critic architecture: a shallow
variational quantum circuit (VQC) with ring entanglement generates policy
features, which are mapped by a compact multilayer perceptron (MLP) into
Gaussian actions over $\Delta$CFG, while a classical critic estimates value
functions. The policy is optimized using Proximal Policy Optimization (PPO)
with Generalized Advantage Estimation (GAE), guided by a reward that balances
classification confidence, perceptual improvement, and action regularization.
Experiments on CIFAR-10 demonstrate that our QRL policy improves perceptual
quality (LPIPS, PSNR, SSIM) while reducing parameter count compared to
classical RL actors and fixed schedules. Ablation studies on qubit number and
circuit depth reveal trade-offs between accuracy and efficiency, and extended
evaluations confirm robust generation under long diffusion schedules.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

í™•ì‚° ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì •ì  ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹±í•œ ë¶„ë¥˜ê¸°-í”„ë¦¬ ê°€ì´ë“œë¼ì¸(CFG) ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•˜ë©°, ì´ëŠ” ì¢…ì¢… ì‹œê°„ ë‹¨ê³„ì™€ ì¡ìŒ ì¡°ê±´ì— ë”°ë¼ ì ì‘í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê° ë””ë…¸ì´ì§• ë‹¨ê³„ì—ì„œ CFGë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì–‘ì ê°•í™” í•™ìŠµ(QRL) ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì–‘ì-ê³ ì „ì  ì•¡í„°-í¬ë¦¬í‹± êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤: ë§ ì–½í˜ì„ ê°€ì§„ ì–•ì€ ë³€ë¶„ ì–‘ì íšŒë¡œ(VQC)ê°€ ì •ì±… íŠ¹ì§•ì„ ìƒì„±í•˜ë©°, ì´ëŠ” ì»´íŒ©íŠ¸í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ì— ì˜í•´ $\Delta$CFGì— ëŒ€í•œ ê°€ìš°ì‹œì•ˆ í–‰ë™ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤. í•œí¸, ê³ ì „ì  í¬ë¦¬í‹±ì€ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ì •ì±…ì€ Proximal Policy Optimization (PPO)ê³¼ Generalized Advantage Estimation (GAE)ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ë˜ë©°, ë¶„ë¥˜ ì‹ ë¢°ë„, ì§€ê°ì  ê°œì„ , í–‰ë™ ê·œì œë¥¼ ê· í˜• ìˆê²Œ ê³ ë ¤í•˜ëŠ” ë³´ìƒì— ì˜í•´ ì•ˆë‚´ë©ë‹ˆë‹¤. CIFAR-10 ì‹¤í—˜ ê²°ê³¼, ìš°ë¦¬ì˜ QRL ì •ì±…ì€ ê³ ì „ì  RL ì•¡í„° ë° ê³ ì • ìŠ¤ì¼€ì¤„ê³¼ ë¹„êµí•˜ì—¬ ë§¤ê°œë³€ìˆ˜ ìˆ˜ë¥¼ ì¤„ì´ë©´ì„œ ì§€ê°ì  í’ˆì§ˆ(LPIPS, PSNR, SSIM)ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. íë¹— ìˆ˜ì™€ íšŒë¡œ ê¹Šì´ì— ëŒ€í•œ ì ˆì œ ì—°êµ¬ëŠ” ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ê°„ì˜ ìƒì¶© ê´€ê³„ë¥¼ ë“œëŸ¬ë‚´ë©°, í™•ì¥ëœ í‰ê°€ì—ì„œëŠ” ê¸´ í™•ì‚° ìŠ¤ì¼€ì¤„ì—ì„œë„ ê°•ë ¥í•œ ìƒì„± ëŠ¥ë ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” í™•ì‚° ëª¨ë¸ì—ì„œ ì •ì  ë˜ëŠ” ê²½í—˜ì  ë¶„ë¥˜ê¸° ë¹„ìœ ë„(CFG) ì¼ì •ì´ ì‹œê°„ ë‹¨ê³„ì™€ ë…¸ì´ì¦ˆ ì¡°ê±´ì— ì ì‘í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° ë…¸ì´ì¦ˆ ì œê±° ë‹¨ê³„ì—ì„œ CFGë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì–‘ì ê°•í™” í•™ìŠµ(QRL) ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì–‘ì-ê³ ì „ì  ì•¡í„°-í¬ë¦¬í‹± êµ¬ì¡°ë¥¼ ì±„íƒí•˜ë©°, ì–•ì€ ë³€ë¶„ ì–‘ì íšŒë¡œ(VQC)ê°€ ì •ì±… íŠ¹ì§•ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ì´ ê°€ìš°ì‹œì•ˆ í–‰ë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì •ì±…ì€ Proximal Policy Optimization(PPO)ì™€ Generalized Advantage Estimation(GAE)ë¥¼ ì‚¬ìš©í•´ ìµœì í™”ë˜ë©°, ë³´ìƒì€ ë¶„ë¥˜ ì‹ ë¢°ë„, ì§€ê°ì  ê°œì„ , í–‰ë™ ê·œì œë¥¼ ê· í˜• ìˆê²Œ ê³ ë ¤í•©ë‹ˆë‹¤. CIFAR-10 ì‹¤í—˜ì—ì„œ QRL ì •ì±…ì€ ì§€ê°ì  í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê³ , ê¸°ì¡´ì˜ ê³ ì „ì  RL ì•¡í„°ì™€ ê³ ì • ì¼ì •ì— ë¹„í•´ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤. íë¹„íŠ¸ ìˆ˜ì™€ íšŒë¡œ ê¹Šì´ì— ëŒ€í•œ ì†Œê±° ì—°êµ¬ëŠ” ì •í™•ë„ì™€ íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì„ ë³´ì—¬ì£¼ë©°, í™•ì¥ëœ í‰ê°€ì—ì„œëŠ” ê¸´ í™•ì‚° ì¼ì •ì—ì„œë„ ê°•ë ¥í•œ ìƒì„± ëŠ¥ë ¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê° ë””ë…¸ì´ì§• ë‹¨ê³„ì—ì„œ CFGë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì–‘ì ê°•í™” í•™ìŠµ(QRL) ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.

- 2. QRL ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì–‘ì-ê³ ì „ì  ì•¡í„°-í¬ë¦¬í‹± êµ¬ì¡°ë¥¼ ì±„íƒí•˜ì—¬, ì–•ì€ ë³€ë¶„ ì–‘ì íšŒë¡œ(VQC)ì™€ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

- 3. ì •ì±… ìµœì í™”ëŠ” Proximal Policy Optimization (PPO)ì™€ Generalized Advantage Estimation (GAE)ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.

- 4. CIFAR-10 ì‹¤í—˜ ê²°ê³¼, QRL ì •ì±…ì€ ì¸ì§€ì  í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ë©´ì„œë„ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

- 5. íë¹— ìˆ˜ì™€ íšŒë¡œ ê¹Šì´ì— ëŒ€í•œ ì†Œê±° ì—°êµ¬ëŠ” ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ë³´ì—¬ì£¼ë©°, í™•ì¥ëœ í‰ê°€ì—ì„œëŠ” ê¸´ í™•ì‚° ì¼ì •ì—ì„œë„ ê²¬ê³ í•œ ìƒì„±ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 07:46:20*