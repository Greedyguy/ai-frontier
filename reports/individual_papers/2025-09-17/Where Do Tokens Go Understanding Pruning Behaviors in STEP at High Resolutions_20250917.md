# Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions

**Korean Title:** í† í°ì€ ì–´ë””ë¡œ ê°€ëŠ”ê°€? ê³ í•´ìƒë„ì—ì„œ STEPì˜ ê°€ì§€ì¹˜ê¸° ë™ì‘ ì´í•´í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Michal Szczepanski|Michal Szczepanski]] [[authors/Martyna Poreba|Martyna Poreba]] [[authors/Karim Haroun|Karim Haroun]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Dynamic Patch Merging

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[AToken_ A Unified Tokenizer for Vision_20250919|AToken A Unified Tokenizer for Vision]] (80.9% similar)
- [[STEP_ Structured Training and Evaluation Platform for benchmarking trajectory prediction models_20250919|STEP Structured Training and Evaluation Platform for benchmarking trajectory prediction models]] (79.8% similar)
- [[Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (78.6% similar)
- [[[Re] Improving Interpretation Faithfulness for Vision Transformers_20250918|[Re] Improving Interpretation Faithfulness for Vision Transformers]] (78.5% similar)
- [[Communication Efficient Split Learning of ViTs with Attention-based Double Compression_20250919|Communication Efficient Split Learning of ViTs with Attention-based Double Compression]] (78.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Michal Szczepanski, Martyna Poreba, Karim Haroun

## ğŸ“„ Abstract (ì›ë¬¸)

Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViTs)ëŠ” ì˜ë¯¸ë¡ ì  ë¶„í• ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ìœ¼ë¡œ ì¸í•´ ì œì•½ì„ ë°›ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” íš¨ìœ¨ì„±ì„ ë†’ì´ë©´ì„œ ì •í™•ì„±ì„ í¬ê²Œ ì†ìƒì‹œí‚¤ì§€ ì•ŠëŠ” ë™ì  íŒ¨ì¹˜ ë³‘í•©ê³¼ í† í° ê°€ì§€ì¹˜ê¸°ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ í† í° ê°ì†Œ í”„ë ˆì„ì›Œí¬ì¸ STEP(SuperToken and Early-Pruning)ì„ ì œì•ˆí•©ë‹ˆë‹¤. STEPì˜ í•µì‹¬ì€ ìœ ì—°í•œ ë³‘í•©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê²½ëŸ‰ CNN ê¸°ë°˜ ì •ì±… ë„¤íŠ¸ì›Œí¬ì¸ dCTSë¡œ, ì´ë¥¼ í†µí•´ ìŠˆí¼íŒ¨ì¹˜ë¡œì˜ ë³‘í•©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì¸ì½”ë” ë¸”ë¡ì€ ë˜í•œ ë†’ì€ ì‹ ë¢°ë„ì˜ ìŠˆí¼í† í°ì„ ì œê±°í•˜ì—¬ ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ëŠ” ì¡°ê¸° ì¢…ë£Œë¥¼ í†µí•©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” 1024 x 1024ê¹Œì§€ì˜ ì´ë¯¸ì§€ ë“± ê³ í•´ìƒë„ ì˜ë¯¸ë¡ ì  ë¶„í•  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ë¦¬ì˜ ë°©ë²•ì„ í‰ê°€í•˜ì˜€ìœ¼ë©°, dCTSë¥¼ ë‹¨ë…ìœ¼ë¡œ ì ìš©í•  ë•Œ í† í° ìˆ˜ê°€ í‘œì¤€ 16 x 16 í”½ì…€ íŒ¨ì¹­ ë°©ì‹ì— ë¹„í•´ 2.5ë°° ê°ì†Œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ViT-Largeë¥¼ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ ê³„ì‚° ë¹„ìš©ì„ 2.6ë°° ì¤„ì´ê³  ì²˜ë¦¬ëŸ‰ì„ 3.4ë°° ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ì „ì²´ STEP í”„ë ˆì„ì›Œí¬ë¥¼ ì ìš©í•˜ë©´ íš¨ìœ¨ì„±ì´ ë”ìš± í–¥ìƒë˜ì–´ ê³„ì‚° ë³µì¡ë„ê°€ ìµœëŒ€ 4ë°° ê°ì†Œí•˜ê³  ì¶”ë¡  ì†ë„ê°€ 1.7ë°° ì¦ê°€í•˜ë©°, ìµœëŒ€ ì •í™•ë„ ê°ì†ŒëŠ” 2.0%ë¥¼ ë„˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì œì•ˆëœ STEP êµ¬ì„±ìœ¼ë¡œ, ìµœëŒ€ 40%ì˜ í† í°ì´ ìµœì¢… ì¸ì½”ë” ë ˆì´ì–´ì— ë„ë‹¬í•˜ê¸° ì „ì— ì‹ ë¢°ì„± ìˆê²Œ ì˜ˆì¸¡ë˜ê³  ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Vision Transformers(ViTs)ëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ ë†’ì€ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš©ì´ ë¬¸ì œì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ STEP(SuperToken and Early-Pruning)ì´ë¼ëŠ” í•˜ì´ë¸Œë¦¬ë“œ í† í° ê°ì†Œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. STEPëŠ” ë™ì  íŒ¨ì¹˜ ë³‘í•©ê³¼ í† í° í”„ë£¨ë‹ì„ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. í•µì‹¬ì€ dCTSë¼ëŠ” ê²½ëŸ‰ CNN ê¸°ë°˜ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ, ìœ ì—°í•œ ìŠˆí¼íŒ¨ì¹˜ ë³‘í•©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ì¸ì½”ë” ë¸”ë¡ì—ì„œ ê³ ì‹ ë¢°ë„ ìŠˆí¼í† í°ì„ ì¡°ê¸°ì— ì œê±°í•˜ì—¬ ê³„ì‚° ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, dCTSë§Œìœ¼ë¡œë„ í† í° ìˆ˜ë¥¼ 2.5ë°° ì¤„ì´ê³ , ê³„ì‚° ë¹„ìš©ì„ 2.6ë°° ì¤„ì´ë©°, ì²˜ë¦¬ëŸ‰ì„ 3.4ë°° ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤. ì „ì²´ STEP í”„ë ˆì„ì›Œí¬ë¥¼ ì ìš©í•˜ë©´ ê³„ì‚° ë³µì¡ì„±ì€ 4ë°° ê°ì†Œí•˜ê³  ì¶”ë¡  ì†ë„ëŠ” 1.7ë°° ì¦ê°€í•˜ë©°, ì •í™•ë„ ì†ì‹¤ì€ ìµœëŒ€ 2.0%ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. ì œì•ˆëœ STEP êµ¬ì„±ìœ¼ë¡œ ìµœëŒ€ 40%ì˜ í† í°ì„ ìµœì¢… ì¸ì½”ë” ë ˆì´ì–´ ì „ì— ì˜ˆì¸¡ ë° ì¤‘ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Vision Transformersì˜ ë†’ì€ ì—°ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ STEP(ìŠˆí¼í† í° ë° ì´ˆê¸°-í”„ë£¨ë‹) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- 2. STEPì˜ í•µì‹¬ì€ dCTSë¼ëŠ” ê²½ëŸ‰ CNN ê¸°ë°˜ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ, ìœ ì—°í•œ ìŠˆí¼íŒ¨ì¹˜ ë³‘í•©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

- 3. STEP í”„ë ˆì„ì›Œí¬ëŠ” ìµœëŒ€ 4ë°°ì˜ ì—°ì‚° ë³µì¡ë„ ê°ì†Œì™€ 1.7ë°°ì˜ ì¶”ë¡  ì†ë„ í–¥ìƒì„ ì´ë£¨ë©°, ì •í™•ë„ ì†ì‹¤ì€ ìµœëŒ€ 2.0%ì— ë¶ˆê³¼í•©ë‹ˆë‹¤.

- 4. dCTSë¥¼ ë‹¨ë…ìœ¼ë¡œ ì ìš©í•  ê²½ìš°, í† í° ìˆ˜ê°€ 2.5ë°° ê°ì†Œí•˜ë©°, ì—°ì‚° ë¹„ìš©ì€ 2.6ë°° ì¤„ê³  ì²˜ë¦¬ëŸ‰ì€ 3.4ë°° ì¦ê°€í•©ë‹ˆë‹¤.

- 5. STEP êµ¬ì„±ìœ¼ë¡œ ìµœëŒ€ 40%ì˜ í† í°ì´ ìµœì¢… ì¸ì½”ë” ë ˆì´ì–´ì— ë„ë‹¬í•˜ê¸° ì „ì— ìì‹  ìˆê²Œ ì˜ˆì¸¡ ë° ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 07:45:52*