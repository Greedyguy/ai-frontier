---
keywords:
  - Differential Privacy
  - Matrix Factorization Norms
  - Continual Counting
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:49:34.570773",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Differential Privacy",
    "Matrix Factorization Norms",
    "Continual Counting"
  ],
  "rejected_keywords": [
    "Deep Learning"
  ],
  "similarity_scores": {
    "Differential Privacy": 0.85,
    "Matrix Factorization Norms": 0.78,
    "Continual Counting": 0.72
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Normalized Square Root: Sharper Matrix Factorization Bounds for Differentially Private Continual Counting

**Korean Title:** ì •ê·œí™”ëœ ì œê³±ê·¼: ì°¨ë“± í”„ë¼ì´ë²„ì‹œë¥¼ ìœ„í•œ ì§€ì†ì  ì¹´ìš´íŒ…ì˜ ë” ë‚ ì¹´ë¡œìš´ í–‰ë ¬ ë¶„í•´ ê²½ê³„

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250917|2025-09-17]]      [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Differential Privacy|differential privacy]]
**âš¡ Unique Technical**: [[keywords/Matrix Factorization Norms|factorization norms]], [[keywords/Continual Counting|continual counting]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Stochastic Bilevel Optimization with Heavy-Tailed Noise_20250918|Stochastic Bilevel Optimization with Heavy-Tailed Noise]] (77.7% similar)
- [[Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (77.1% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (77.0% similar)
- [[Gap-Dependent Bounds for Federated $Q$-learning_20250919|Gap-Dependent Bounds for Federated $Q$-learning]] (76.1% similar)
- [[Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (76.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Monika Henzinger, Nikita P. Kalinin, Jalaj Upadhyay

## ğŸ“„ Abstract (ì›ë¬¸)

The factorization norms of the lower-triangular all-ones $n \times n$ matrix,
$\gamma_2(M_{count})$ and $\gamma_{F}(M_{count})$, play a central role in
differential privacy as they are used to give theoretical justification of the
accuracy of the only known production-level private training algorithm of deep
neural networks by Google. Prior to this work, the best known upper bound on
$\gamma_2(M_{count})$ was $1 + \frac{\log n}{\pi}$ by Mathias (Linear Algebra
and Applications, 1993), and the best known lower bound was $\frac{1}{\pi}(2 +
\log(\frac{2n+1}{3})) \approx 0.507 + \frac{\log n}{\pi}$ (Matou\v{s}ek,
Nikolov, Talwar, IMRN 2020), where $\log$ denotes the natural logarithm.
Recently, Henzinger and Upadhyay (SODA 2025) gave the first explicit
factorization that meets the bound of Mathias (1993) and asked whether there
exists an explicit factorization that improves on Mathias' bound. We answer
this question in the affirmative. Additionally, we improve the lower bound
significantly. More specifically, we show that $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_2(M_{count}) \;\leq\; 0.846
+ \frac{\log n}{\pi} + o(1). $$ That is, we reduce the gap between the upper
and lower bound to $0.14 + o(1)$.
  We also show that our factors achieve a better upper bound for
$\gamma_{F}(M_{count})$ compared to prior work, and we establish an improved
lower bound: $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_{F}(M_{count}) \;\leq\;
0.748 + \frac{\log n}{\pi} + o(1). $$ That is, the gap between the lower and
upper bound provided by our explicit factorization is $0.047 + o(1)$.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

í•˜ì‚¼ê°í˜•ì˜ ëª¨ë“  ìš”ì†Œê°€ 1ì¸ $n \times n$ í–‰ë ¬ì˜ ë¶„í•´ ê·œë²”, $\gamma_2(M_{count})$ì™€ $\gamma_{F}(M_{count})$ëŠ” ì°¨ë“± ê°œì¸ì •ë³´ ë³´í˜¸ì—ì„œ ì¤‘ì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ëŠ” Googleì˜ ìœ ì¼í•œ ìƒì‚° ìˆ˜ì¤€ì˜ ë¹„ê³µê°œ ì‹¬ì¸µ ì‹ ê²½ë§ í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ì„±ì„ ì´ë¡ ì ìœ¼ë¡œ ì •ë‹¹í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ì—°êµ¬ ì´ì „ì—ëŠ” $\gamma_2(M_{count})$ì— ëŒ€í•œ ê°€ì¥ ì˜ ì•Œë ¤ì§„ ìƒí•œì€ Mathiasê°€ ì œì‹œí•œ $1 + \frac{\log n}{\pi}$ (Linear Algebra and Applications, 1993)ì˜€ìœ¼ë©°, ê°€ì¥ ì˜ ì•Œë ¤ì§„ í•˜í•œì€ MatouÅ¡ek, Nikolov, Talwarê°€ ì œì‹œí•œ $\frac{1}{\pi}(2 + \log(\frac{2n+1}{3})) \approx 0.507 + \frac{\log n}{\pi}$ (IMRN 2020)ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ $\log$ëŠ” ìì—° ë¡œê·¸ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìµœê·¼ Henzingerì™€ Upadhyay (SODA 2025)ëŠ” Mathias (1993)ì˜ ê²½ê³„ë¥¼ ì¶©ì¡±í•˜ëŠ” ì²« ë²ˆì§¸ ëª…ì‹œì  ë¶„í•´ë¥¼ ì œì‹œí•˜ì˜€ê³ , Mathiasì˜ ê²½ê³„ë¥¼ ê°œì„ í•˜ëŠ” ëª…ì‹œì  ë¶„í•´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì œê¸°í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì§ˆë¬¸ì— ê¸ì •ì ìœ¼ë¡œ ë‹µí•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” í•˜í•œì„ ìƒë‹¹íˆ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤. ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ë³´ì…ë‹ˆë‹¤: $$ 0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_2(M_{count}) \;\leq\; 0.846 + \frac{\log n}{\pi} + o(1). $$ ì¦‰, ìƒí•œê³¼ í•˜í•œ ì‚¬ì´ì˜ ê°„ê²©ì„ $0.14 + o(1)$ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” $\gamma_{F}(M_{count})$ì— ëŒ€í•´ ì´ì „ ì—°êµ¬ë³´ë‹¤ ë” ë‚˜ì€ ìƒí•œì„ ë‹¬ì„±í•˜ëŠ” ìš”ì†Œë¥¼ ì œì‹œí•˜ê³ , ê°œì„ ëœ í•˜í•œì„ í™•ë¦½í•©ë‹ˆë‹¤: $$ 0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_{F}(M_{count}) \;\leq\; 0.748 + \frac{\log n}{\pi} + o(1). $$ ì¦‰, ìš°ë¦¬ì˜ ëª…ì‹œì  ë¶„í•´ê°€ ì œê³µí•˜ëŠ” í•˜í•œê³¼ ìƒí•œ ì‚¬ì´ì˜ ê°„ê²©ì€ $0.047 + o(1)$ì…ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ í•˜ì‚¼ê°í˜•ì˜ ëª¨ë“  ì›ì†Œê°€ 1ì¸ $n \times n$ í–‰ë ¬ì˜ ë¶„í•´ ë…¸ë¦„ $\gamma_2(M_{count})$ì™€ $\gamma_{F}(M_{count})$ì— ëŒ€í•œ ìƒˆë¡œìš´ ìƒí•œ ë° í•˜í•œì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” êµ¬ê¸€ì˜ ì‹¬ì¸µ ì‹ ê²½ë§ ë¹„ê³µê°œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ì„±ì„ ì´ë¡ ì ìœ¼ë¡œ ë’·ë°›ì¹¨í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ê¸°ì¡´ ìƒí•œì€ Mathias(1993)ì— ì˜í•´ $1 + \frac{\log n}{\pi}$ë¡œ ì•Œë ¤ì ¸ ìˆì—ˆê³ , í•˜í•œì€ MatouÅ¡ek ë“±(2020)ì— ì˜í•´ $\frac{1}{\pi}(2 + \log(\frac{2n+1}{3}))$ë¡œ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” Mathiasì˜ ìƒí•œì„ ê°œì„ í•˜ëŠ” ëª…ì‹œì  ë¶„í•´ë¥¼ ì œì‹œí•˜ê³ , í•˜í•œë„ $0.701 + \frac{\log n}{\pi} + o(1)$ë¡œ í¬ê²Œ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ $\gamma_{F}(M_{count})$ì— ëŒ€í•´ì„œë„ ìƒí•œê³¼ í•˜í•œì„ ê°ê° $0.748 + \frac{\log n}{\pi} + o(1)$ê³¼ $0.701 + \frac{\log n}{\pi} + o(1)$ë¡œ ì œì‹œí•˜ì—¬, ìƒí•˜í•œ ê°„ê²©ì„ $0.047 + o(1)$ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ ì—°êµ¬ëŠ” í•˜ì‚¼ê°í˜• ëª¨ë“  ì›ì†Œê°€ 1ì¸ $n \times n$ í–‰ë ¬ì˜ ë¶„í•´ ë…¸ë¦„ $\gamma_2(M_{count})$ì™€ $\gamma_{F}(M_{count})$ì˜ ìƒí•œê³¼ í•˜í•œì„ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤.

- 2. ê¸°ì¡´ì˜ ìƒí•œì€ Mathias(1993)ì— ì˜í•´ $1 + \frac{\log n}{\pi}$ë¡œ ì•Œë ¤ì ¸ ìˆì—ˆê³ , í•˜í•œì€ MatouÅ¡ek, Nikolov, Talwar(2020)ì— ì˜í•´ $\frac{1}{\pi}(2 + \log(\frac{2n+1}{3}))$ë¡œ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.

- 3. ë³¸ ì—°êµ¬ëŠ” $\gamma_2(M_{count})$ì˜ í•˜í•œì„ $0.701 + \frac{\log n}{\pi} + o(1)$ë¡œ, ìƒí•œì„ $0.846 + \frac{\log n}{\pi} + o(1)$ë¡œ ì œì‹œí•˜ì—¬ ìƒí•œê³¼ í•˜í•œì˜ ì°¨ì´ë¥¼ $0.14 + o(1)$ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

- 4. ë˜í•œ, $\gamma_{F}(M_{count})$ì— ëŒ€í•´ì„œë„ ìƒí•œì„ ê°œì„ í•˜ê³  í•˜í•œì„ $0.701 + \frac{\log n}{\pi} + o(1)$ë¡œ, ìƒí•œì„ $0.748 + \frac{\log n}{\pi} + o(1)$ë¡œ ì œì‹œí•˜ì—¬ ì°¨ì´ë¥¼ $0.047 + o(1)$ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.

- 5. ì´ ì—°êµ¬ëŠ” Googleì˜ ë”¥ëŸ¬ë‹ í”„ë¼ì´ë¹— íŠ¸ë ˆì´ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì´ë¡ ì  ì •í™•ì„±ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 07:39:16*