---
keywords:
  - Reinforcement Learning
  - Uncertainty Quantification
  - Online Contextual Multi-Arm Bandits
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:48:56.871332",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Uncertainty Quantification",
    "Online Contextual Multi-Arm Bandits"
  ],
  "rejected_keywords": [
    "Bayesian Risk Markov Decision Process"
  ],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Uncertainty Quantification": 0.8,
    "Online Contextual Multi-Arm Bandits": 0.75
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Online Bayesian Risk-Averse Reinforcement Learning

**Korean Title:** ì˜¨ë¼ì¸ ë² ì´ì§€ì•ˆ ìœ„í—˜ íšŒí”¼ ê°•í™” í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250917|2025-09-17]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]], [[keywords/Uncertainty Quantification|Epistemic Uncertainty]]
**âš¡ Unique Technical**: [[keywords/Online Contextual Multi-Arm Bandits|Online Contextual Multi-Arm Bandits]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization_20250919|Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization]] (84.1% similar)
- [[Online Learning of Deceptive Policies under Intermittent Observation_20250919|Online Learning of Deceptive Policies under Intermittent Observation]] (82.3% similar)
- [[Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits_20250919|Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits]] (80.2% similar)
- [[Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (79.9% similar)
- [[Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (79.6% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Yuhao Wang, Enlu Zhou

## ğŸ“„ Abstract (ì›ë¬¸)

In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì´ ë…¼ë¬¸ì—ì„œëŠ” ê°•í™” í•™ìŠµ(RL)ì—ì„œ ë² ì´ì§€ì•ˆ ìœ„í—˜ íšŒí”¼ ê³µì‹í™”ë¥¼ ì—°êµ¬í•©ë‹ˆë‹¤. ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì¸ì‹ë¡ ì  ë¶ˆí™•ì‹¤ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¯¸ì§€ì˜ ê¸°ë³¸ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ë² ì´ì§€ì•ˆ ìœ„í—˜ ë§ˆë¥´ì½”í”„ ê²°ì • í”„ë¡œì„¸ìŠ¤(BRMDP)ë¥¼ ì±„íƒí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë² ì´ì§€ì•ˆ ìœ„í—˜ ê°€ì¹˜ í•¨ìˆ˜ì™€ ì‹¤ì œ ë¯¸ì§€ì˜ ë¶„í¬ í•˜ì—ì„œì˜ ì›ë˜ ê°€ì¹˜ í•¨ìˆ˜ ê°„ì˜ ì°¨ì´ë¥¼ íŠ¹ì§•ì§“ëŠ” ì ê·¼ì  ì •ê·œì„±ì„ ë„ì¶œí•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ë² ì´ì§€ì•ˆ ìœ„í—˜ íšŒí”¼ ì ‘ê·¼ë²•ì´ ì›ë˜ì˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë¹„ê´€ì ìœ¼ë¡œ ê³¼ì†Œí‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ë¶ˆì¼ì¹˜ëŠ” ë” ê°•í•œ ìœ„í—˜ íšŒí”¼ì™€ í•¨ê»˜ ì¦ê°€í•˜ë©°, ë” ë§ì€ ë°ì´í„°ê°€ ì œê³µë ìˆ˜ë¡ ê°ì†Œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ìš°ë¦¬ëŠ” ì˜¨ë¼ì¸ RL ì„¤ì • ë° ì˜¨ë¼ì¸ ì»¨í…ìŠ¤ì¶”ì–¼ ë©€í‹°ì•”ë“œ ë°´ë”§(CMAB), ì¦‰ ì˜¨ë¼ì¸ RLì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ì—ì„œ ì´ ì ì‘ì  íŠ¹ì„±ì„ í™œìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¼ë°˜ì ì¸ RL ë¬¸ì œì™€ CMAB ë¬¸ì œ ëª¨ë‘ì— ëŒ€í•´ ì‚¬í›„ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ì ˆì°¨ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” RL ë° CMAB ì„¤ì • ëª¨ë‘ì— ëŒ€í•´ ì „í†µì ì¸ í›„íšŒë¡œ ì •ì˜ëœ ì„œë¸Œ-ë¦¬ë‹ˆì–´ í›„íšŒ ê²½ê³„ë¥¼ í™•ë¦½í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë² ì´ì§€ì•ˆ ìœ„í—˜ í›„íšŒë¡œ ì •ì˜ëœ CMAB ì„¤ì •ì— ëŒ€í•œ ì„œë¸Œ-ë¦¬ë‹ˆì–´ í›„íšŒ ê²½ê³„ë¥¼ í™•ë¦½í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì¸ì‹ë¡ ì  ë¶ˆí™•ì‹¤ì„±ì„ í•´ê²°í•˜ê³  ì´ë¡ ì  ì†ì„±ì„ ê²€ì¦í•˜ëŠ” ë° ìˆì–´ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ê¸° ìœ„í•´ ìˆ˜ì¹˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµì—ì„œ ë² ì´ì§€ì•ˆ ìœ„í—˜ íšŒí”¼ ì ‘ê·¼ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ë¯¸ì§€ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ ë² ì´ì§€ì•ˆ ìœ„í—˜ ë§ˆë¥´ì½”í”„ ê²°ì • í”„ë¡œì„¸ìŠ¤(BRMDP)ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ë² ì´ì§€ì•ˆ ìœ„í—˜ íšŒí”¼ ì ‘ê·¼ë²•ì€ ì›ë˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë¹„ê´€ì ìœ¼ë¡œ ê³¼ì†Œí‰ê°€í•˜ë©°, ì´ëŠ” ìœ„í—˜ íšŒí”¼ê°€ ê°•í• ìˆ˜ë¡ ì¦ê°€í•˜ê³  ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê°ì†Œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ì‘ì  íŠ¹ì„±ì„ ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµê³¼ ì˜¨ë¼ì¸ ë¬¸ë§¥ ê¸°ë°˜ ë‹¤ì¤‘ ìŠ¬ë¡¯ë¨¸ì‹  ë¬¸ì œ(CMAB)ì— ì ìš©í•˜ì˜€ìœ¼ë©°, ë‘ ê°€ì§€ ìƒí™© ëª¨ë‘ì— ëŒ€í•´ í›„í–‰ ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ëŠ” ì ˆì°¨ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê°•í™” í•™ìŠµ ë¬¸ì œì™€ CMAB ë¬¸ì œì—ì„œ ì „í†µì ì¸ í›„íšŒì™€ ë² ì´ì§€ì•ˆ ìœ„í—˜ í›„íšŒì— ëŒ€í•œ ì„œë¸Œ-ì„ í˜• í›„íšŒ ê²½ê³„ë¥¼ í™•ë¦½í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ì„±ì„ ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê°•í™” í•™ìŠµì—ì„œ ë² ì´ì§€ì•ˆ ìœ„í—˜ íšŒí”¼ ê³µì‹ì„ ì—°êµ¬í•˜ê³ , ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì¸ì‹ ë¶ˆí™•ì‹¤ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ BRMDPë¥¼ ì±„íƒí•©ë‹ˆë‹¤.

- 2. ë² ì´ì§€ì•ˆ ìœ„í—˜ ê°€ì¹˜ í•¨ìˆ˜ì™€ ì›ë˜ ê°€ì¹˜ í•¨ìˆ˜ ê°„ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•˜ëŠ” ì ê·¼ì  ì •ê·œì„±ì„ ë„ì¶œí•˜ì˜€ìœ¼ë©°, ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì´ ì›ë˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê³¼ì†Œí‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

- 3. ìœ„í—˜ íšŒí”¼ê°€ ê°•í• ìˆ˜ë¡ ì´ ì°¨ì´ëŠ” ì¦ê°€í•˜ê³ , ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

- 4. ì˜¨ë¼ì¸ ê°•í™” í•™ìŠµ ë° ì˜¨ë¼ì¸ ë¬¸ë§¥ì  ë‹¤ì¤‘ ë¬´ì¥ ê°•ë„ ë¬¸ì œ ì„¤ì •ì—ì„œ í›„í–‰ í‘œë³¸ ì¶”ì¶œì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ ì ˆì°¨ë¥¼ ì œì•ˆí•˜ê³ , ì„œë¸Œ-ì„ í˜• í›„íšŒë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

- 5. ìˆ˜ì¹˜ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì´ ì¸ì‹ ë¶ˆí™•ì‹¤ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ê³  ì´ë¡ ì  íŠ¹ì„±ì„ ê²€ì¦í•¨ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 07:49:28*