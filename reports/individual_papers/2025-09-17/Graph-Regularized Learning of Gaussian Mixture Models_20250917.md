---
keywords:
  - Gaussian Mixture Models
  - Graph Neural Networks
  - Federated Learning
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:54:10.313563",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Gaussian Mixture Models",
    "Graph Neural Networks",
    "Federated Learning"
  ],
  "rejected_keywords": [
    "Graph-Regularized Learning"
  ],
  "similarity_scores": {
    "Gaussian Mixture Models": 0.8,
    "Graph Neural Networks": 0.77,
    "Federated Learning": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# Graph-Regularized Learning of Gaussian Mixture Models

**Korean Title:** 그래프 정규화된 가우시안 혼합 모델 학습

## 📋 메타데이터

**Links**: [[digests/daily_digest_20250917|2025-09-17]]     [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🔗 Specific Connectable**: [[keywords/Gaussian Mixture Models|Gaussian Mixture Models]], [[keywords/Graph Neural Networks|similarity graph]], [[keywords/Federated Learning|distributed settings]]

## 🔗 유사한 논문
- [[Online reinforcement learning via sparse Gaussian mixture model Q-functions_20250919|Online reinforcement learning via sparse Gaussian mixture model Q-functions]] (79.4% similar)
- [[Federated Hypergraph Learning with Local Differential Privacy_ Toward Privacy-Aware Hypergraph Structure Completion_20250919|Federated Hypergraph Learning with Local Differential Privacy Toward Privacy-Aware Hypergraph Structure Completion]] (78.1% similar)
- [[Decentralized Optimization with Topology-Independent Communication_20250917|Decentralized Optimization with Topology-Independent Communication]] (77.9% similar)
- [[Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data_20250919|Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data]] (77.9% similar)
- [[Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models_20250919|Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models]] (77.8% similar)

## 📋 저자 정보

**Authors:** Shamsiiat Abdurakhmanova, Alex Jung

## 📄 Abstract (원문)

We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

## 🔍 Abstract (한글 번역)

다음은 이질적이고 제한된 로컬 데이터가 있는 분산 환경에서 그래프 정규화된 가우시안 혼합 모델(GMM)의 학습을 제시합니다. 이 방법은 제공된 유사성 그래프를 활용하여 노드 간의 매개변수 공유를 안내하며, 원시 데이터의 전송을 피합니다. 결과적으로 생성된 모델은 이웃의 매개변수를 유연하게 집계할 수 있으며, 이질적이고 샘플 수가 적은 환경에서 중앙 집중식 및 로컬로 학습된 GMM보다 뛰어난 성능을 발휘합니다.

## 📝 요약

이 논문은 이질적이고 제한된 지역 데이터를 가진 분산 환경에서 그래프 정규화된 가우시안 혼합 모델(GMM)의 학습 방법을 제안합니다. 이 방법은 유사성 그래프를 활용하여 노드 간의 매개변수 공유를 유도하며, 원시 데이터 전송을 피할 수 있습니다. 결과적으로, 이 모델은 이웃 노드의 매개변수를 유연하게 집계할 수 있으며, 이질적이고 샘플 수가 적은 환경에서 중앙 집중식 및 지역적으로 학습된 GMM보다 우수한 성능을 보입니다.

## 🎯 주요 포인트

- 1. 이 연구는 분산 환경에서 이질적이고 제한된 지역 데이터를 사용하는 그래프 정규화 가우시안 혼합 모델(GMM) 학습을 제안합니다.

- 2. 제공된 유사성 그래프를 활용하여 노드 간의 매개변수 공유를 유도하고, 원시 데이터 전송을 피합니다.

- 3. 제안된 모델은 이웃 노드의 매개변수를 유연하게 집계할 수 있으며, 이질적이고 샘플이 적은 환경에서 중앙 집중식 및 지역적으로 훈련된 GMM보다 우수한 성능을 보입니다.

---

*Generated on 2025-09-20 09:28:34*