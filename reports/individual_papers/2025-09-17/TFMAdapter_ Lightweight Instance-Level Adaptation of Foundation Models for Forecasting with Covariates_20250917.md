# TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates

**Korean Title:** TFMAdapter: ê³µë³€ëŸ‰ì„ í™œìš©í•œ ì˜ˆì¸¡ì„ ìœ„í•œ ê¸°ì´ˆ ëª¨ë¸ì˜ ê²½ëŸ‰ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ì ì‘

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Afrin Dange|Afrin Dange]] [[authors/Sunita Sarawagi|Sunita Sarawagi]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Instance-Level Adaptation

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Super-Linear_ A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting_20250918|Super-Linear A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting]] (79.5% similar)
- [[Bridging Past and Future_ Distribution-Aware Alignment for Time Series Forecasting_20250917|Bridging Past and Future Distribution-Aware Alignment for Time Series Forecasting]] (79.2% similar)
- [[FroM_ Frobenius Norm-Based Data-Free Adaptive Model Merging_20250918|FroM Frobenius Norm-Based Data-Free Adaptive Model Merging]] (77.1% similar)
- [[DAG_ A Dual Causal Network for Time Series Forecasting with Exogenous Variables_20250919|DAG A Dual Causal Network for Time Series Forecasting with Exogenous Variables]] (76.9% similar)
- [[Masked Feature Modeling Enhances Adaptive Segmentation_20250918|Masked Feature Modeling Enhances Adaptive Segmentation]] (76.7% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Afrin Dange, Sunita Sarawagi

## ğŸ“„ Abstract (ì›ë¬¸)

Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì‹œê³„ì—´ ê¸°ì´ˆ ëª¨ë¸(TSFMs)ì€ ìµœê·¼ ìƒˆë¡œìš´ ì‹œê³„ì—´ì˜ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì—ì„œ ê³¼ê±° ê°’ì˜ ì§§ì€ ê¸°ë¡ì— ì¡°ê±´í™”í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ë“¤ì˜ ì„±ê³µì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ê±¸ì¹œ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµì´ ì§§ì€ ê¸°ë¡ì˜ ì‹œê°„ì  íŒ¨í„´ìœ¼ë¡œë¶€í„° ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ê·€ë‚©ì  í¸í–¥ì„ íšë“í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ TSFMsëŠ” ë„ë©”ì¸ íŠ¹ìœ ì˜ íŠ¹ì„±ê³¼ ê´€ë ¨ëœ ê·€ë‚©ì  í¸í–¥ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë§ì€ ì‘ìš© ë¶„ì•¼ì—ì„œ ì •í™•í•œ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ë¯¸ë˜ì— ì‚¬ìš© ê°€ëŠ¥í•œ ì™¸ìƒ ë³€ìˆ˜ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” TFMAdapterë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” TSFMsì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ê³µë³€ëŸ‰ ì •ë³´ë¥¼ ë³´ê°•í•˜ëŠ” ê²½ëŸ‰ì˜ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ì–´ëŒ‘í„°ì…ë‹ˆë‹¤. ì¬í•™ìŠµ ëŒ€ì‹ , TFMAdapterëŠ” ë‹¨ì¼ ëª¨ë¸ í˜¸ì¶œ ë™ì•ˆ ì œê³µë˜ëŠ” ì œí•œëœ ê¸°ë¡ì—ì„œ ì‘ë™í•˜ì—¬ ë‹¨ë³€ëŸ‰ TSFM ì˜ˆì¸¡ê³¼ ê³µë³€ëŸ‰ì„ ê²°í•©í•˜ëŠ” ë¹„ëª¨ìˆ˜ì  ìºìŠ¤ì¼€ì´ë“œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ í•™ìŠµì€ ê¸°ë¡ì˜ ëª¨ë“  ë‹¨ê³„ì—ì„œ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì„ í•„ìš”ë¡œ í•˜ë©°, ì´ëŠ” TSFMì— ëŒ€í•œ ë„ˆë¬´ ë§ì€ í˜¸ì¶œì„ ìš”êµ¬í•©ë‹ˆë‹¤. ì „ì²´ ì—­ì‚¬ì  ë§¥ë½ì—ì„œì˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©´ì„œ TSFM í˜¸ì¶œì„ ì œí•œí•˜ê¸° ìœ„í•´, TFMAdapterëŠ” ë‘ ë‹¨ê³„ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: (1) ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ë¡œ ì˜ì‚¬ ì˜ˆì¸¡ ìƒì„±, (2) ê³µë³€ëŸ‰ê³¼ í•¨ê»˜ ì˜ì‚¬ ì˜ˆì¸¡ ë° TSFM ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ì •ì œí•˜ê¸° ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ì í•™ìŠµ. ì‹¤ì œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ TFMAdapterê°€ ê¸°ì´ˆ ëª¨ë¸ê³¼ ê°ë…ëœ ê¸°ì¤€ ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ë©°, ìµœì†Œí•œì˜ ë°ì´í„° ë° ê³„ì‚° ì˜¤ë²„í—¤ë“œë¡œ ê¸°ë³¸ ê¸°ì´ˆ ëª¨ë¸ ëŒ€ë¹„ 24-27%ì˜ í–¥ìƒì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ê²½ëŸ‰ ì–´ëŒ‘í„°ê°€ ì¼ë°˜ì ì¸ ê¸°ì´ˆ ëª¨ë¸ê³¼ ë„ë©”ì¸ íŠ¹ìœ ì˜ ì˜ˆì¸¡ ìš”êµ¬ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ë©”ìš¸ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

Time Series Foundation Models (TSFMs)ëŠ” ê³¼ê±° ë°ì´í„°ì˜ ì§§ì€ ê¸°ë¡ë§Œìœ¼ë¡œë„ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ë„ë©”ì¸ íŠ¹ìœ ì˜ ì™¸ìƒ ë³€ìˆ˜(ê³µë³€ëŸ‰)ë¥¼ í™œìš©í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” TFMAdapterë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” TSFMì„ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ê³µë³€ëŸ‰ ì •ë³´ë¥¼ í†µí•©í•  ìˆ˜ ìˆëŠ” ê²½ëŸ‰ ì–´ëŒ‘í„°ë¡œ, ë‹¨ì¼ ëª¨ë¸ í˜¸ì¶œ ì‹œ ì œí•œëœ ê¸°ë¡ì„ í™œìš©í•˜ì—¬ ë¹„ëª¨ìˆ˜ì  ë°©ë²•ìœ¼ë¡œ ê³µë³€ëŸ‰ê³¼ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ì„ ê²°í•©í•©ë‹ˆë‹¤. TFMAdapterëŠ” ë‘ ë‹¨ê³„ë¡œ ì‘ë™í•˜ë©°, ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ë¡œ ì˜ì‚¬ ì˜ˆì¸¡ì„ ìƒì„±í•œ í›„, ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ë¥¼ í†µí•´ ì´ë¥¼ ì •ì œí•©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼, TFMAdapterëŠ” ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ 24-27% ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ë©°, ê²½ëŸ‰ ì–´ëŒ‘í„°ê°€ ë²”ìš© ëª¨ë¸ê³¼ ë„ë©”ì¸ íŠ¹í™” ì˜ˆì¸¡ ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. TSFMsëŠ” ê³¼ê±° ê°’ì˜ ì§§ì€ ê¸°ë¡ë§Œìœ¼ë¡œë„ ìµœì‹ ì˜ ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

- 2. TSFMsëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë„ë©”ì¸ íŠ¹í™”ëœ ì™¸ìƒ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì§€ ëª»í•˜ë©°, ì´ëŠ” ì •í™•í•œ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.

- 3. TFMAdapterëŠ” TSFMsì— ì™¸ìƒ ë³€ìˆ˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ë¯¸ì„¸ ì¡°ì • ì—†ì´ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 4. TFMAdapterëŠ” ë‘ ë‹¨ê³„ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì—­ì‚¬ì  ë§¥ë½ì—ì„œì˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

- 5. ì‹¤í—˜ ê²°ê³¼, TFMAdapterëŠ” ìµœì†Œí•œì˜ ë°ì´í„° ë° ê³„ì‚° ì˜¤ë²„í—¤ë“œë¡œ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ 24-27% ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:24:54*