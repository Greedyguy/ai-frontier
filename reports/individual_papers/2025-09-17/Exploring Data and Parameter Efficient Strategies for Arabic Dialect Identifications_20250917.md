# Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications

**Korean Title:** ì•„ëì–´ ë°©ì–¸ ì‹ë³„ì„ ìœ„í•œ ë°ì´í„° ë° ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì  ì „ëµ íƒìƒ‰

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Vani Kanjirangat|Vani Kanjirangat]] [[authors/Ljiljana Dolamic|Ljiljana Dolamic]] [[authors/Fabio Rinaldi|Fabio Rinaldi]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: Few-shot Learning, Zero-shot Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Hala Technical Report_ Building Arabic-Centric Instruction & Translation Models at Scale_20250917|Hala Technical Report Building Arabic-Centric Instruction & Translation Models at Scale]] (82.4% similar)
- [[Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (80.7% similar)
- [[Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox A practical guide to getting the most out of human ratings]] (80.7% similar)
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (80.1% similar)
- [[Advancing Conversational AI with Shona Slang_ A Dataset and Hybrid Model for Digital Inclusion_20250919|Advancing Conversational AI with Shona Slang A Dataset and Hybrid Model for Digital Inclusion]] (80.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Vani Kanjirangat, Ljiljana Dolamic, Fabio Rinaldi

## ğŸ“„ Abstract (ì›ë¬¸)

This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì´ ë…¼ë¬¸ì€ ì•„ëì–´ ë°©ì–¸ ì‹ë³„(ADI)ì— ëŒ€í•œ ë°ì´í„° íš¨ìœ¨ì  ë° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ì ‘ê·¼ë²•ì˜ íƒêµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ, prefix-tuning, prompt-tuning, P-tuning, P-tuning V2ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŒ… ì „ëµê³¼ LoRA ì¬íŒŒë¼ë¯¸í„°í™”ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ë°ì´í„° íš¨ìœ¨ì  ì „ëµìœ¼ë¡œëŠ”, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°©ì–¸ ì‹ë³„ ëŠ¥ë ¥ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ì œë¡œìƒ· ë° í“¨ìƒ· ì¶”ë¡ ì„ í†µí•œ í•˜ë“œ í”„ë¡¬í”„íŒ…ì„ ë¶„ì„í•©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  PEFT ì ‘ê·¼ë²•ì˜ ê²½ìš°, ì—¬ëŸ¬ ì£¼ìš” ë°ì´í„°ì…‹ì—ì„œ ì•„ëì–´ íŠ¹í™” ì¸ì½”ë” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì˜¤í”ˆ ì†ŒìŠ¤ ë””ì½”ë” ì „ìš© ëª¨ë¸, ì¼ë°˜ ë‹¤êµ­ì–´ ëª¨ë¸(Phi-3.5), ì•„ëì–´ íŠ¹í™” ëª¨ë¸(SILMA)ì—ì„œ n-ìƒ· ì¶”ë¡ ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. LLMì€ ì¼ë°˜ì ìœ¼ë¡œ í“¨ìƒ· ë˜ëŠ” ì œë¡œìƒ· ì„¤ì •ì—ì„œ ë°©ì–¸ì˜ ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì†Œí”„íŠ¸ í”„ë¡¬í”„íŒ…ëœ ì¸ì½”ë” ë³€í˜•ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, LoRA ê¸°ë°˜ì˜ ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ì´ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì„ ì´ˆê³¼í•˜ì—¬ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì•„ëì–´ ë°©ì–¸ ì‹ë³„ì„ ìœ„í•œ ë°ì´í„° íš¨ìœ¨ì  ë° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ì ‘ê·¼ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ëŠ” ì†Œí”„íŠ¸ í”„ë¡¬í”„íŒ… ì „ëµ(í”„ë¦¬í”½ìŠ¤ íŠœë‹, í”„ë¡¬í”„íŠ¸ íŠœë‹, P-íŠœë‹, P-íŠœë‹ V2)ê³¼ LoRA ì¬íŒŒë¼ë¯¸í„°í™”ë¥¼ ì¡°ì‚¬í•œ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° íš¨ìœ¨ì  ì „ëµìœ¼ë¡œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°©ì–¸ ì‹ë³„ ëŠ¥ë ¥ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ì œë¡œìƒ· ë° ì†Œìˆ˜ìƒ· ì¶”ë¡ ì„ ì‚¬ìš©í•œ í•˜ë“œ í”„ë¡¬í”„íŒ…ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ì ‘ê·¼ë²•ì—ì„œëŠ” ì•„ëì–´ ì „ìš© ì¸ì½”ë” ëª¨ë¸ì„ ì‚¬ìš©í•´ ì£¼ìš” ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì˜¤í”ˆì†ŒìŠ¤ ë””ì½”ë” ì „ìš© ëª¨ë¸, ë‹¤êµ­ì–´ ëª¨ë¸(Phi-3.5), ì•„ëì–´ ì „ìš© ëª¨ë¸(SILMA)ì—ì„œ n-ìƒ· ì¶”ë¡ ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, LLMì€ ì†Œìˆ˜ìƒ· ë˜ëŠ” ì œë¡œìƒ· ì„¤ì •ì—ì„œ ë°©ì–¸ì˜ ë¯¸ì„¸í•œ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì§€ë§Œ, ì†Œí”„íŠ¸ í”„ë¡¬í”„íŒ… ì¸ì½”ë” ë³€í˜•ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, LoRA ê¸°ë°˜ ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ì´ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì„ ëŠ¥ê°€í•˜ëŠ” ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì´ ë…¼ë¬¸ì€ ì•„ëì–´ ë°©ì–¸ ì‹ë³„ì„ ìœ„í•œ ë°ì´í„° íš¨ìœ¨ì  ë° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ì ‘ê·¼ ë°©ì‹ì„ íƒêµ¬í•©ë‹ˆë‹¤.

- 2. ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ì „ëµìœ¼ë¡œëŠ” í”„ë¦¬í”½ìŠ¤ íŠœë‹, í”„ë¡¬í”„íŠ¸ íŠœë‹, P-íŠœë‹, P-íŠœë‹ V2 ë° LoRA ì¬ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤.

- 3. ë°ì´í„° íš¨ìœ¨ì  ì „ëµìœ¼ë¡œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°©ì–¸ ì‹ë³„ ëŠ¥ë ¥ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ì œë¡œìƒ· ë° í“¨ìƒ· ì¶”ë¡ ì„ ì‚¬ìš©í•œ í•˜ë“œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

- 4. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  PEFT ì ‘ê·¼ ë°©ì‹ì—ì„œëŠ” ì•„ëì–´ ì „ìš© ì¸ì½”ë” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì£¼ìš” ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

- 5. LoRA ê¸°ë°˜ì˜ ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ì´ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì„ ì´ˆê³¼í•˜ì—¬ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:33:43*