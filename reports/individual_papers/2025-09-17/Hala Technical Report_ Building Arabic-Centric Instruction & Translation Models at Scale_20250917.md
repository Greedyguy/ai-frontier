# Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale

**Korean Title:** í• ë¼ ê¸°ìˆ  ë³´ê³ ì„œ: ëŒ€ê·œëª¨ ì•„ëì–´ ì¤‘ì‹¬ì˜ êµìœ¡ ë° ë²ˆì—­ ëª¨ë¸ êµ¬ì¶•

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Hasan Abed Al Kader Hammoud|Hasan Abed Al Kader Hammoud]] [[authors/Mohammad Zbeeb|Mohammad Zbeeb]] [[authors/Bernard Ghanem|Bernard Ghanem]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Translate and Tune Pipeline

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Apertus_ Democratizing Open and Compliant LLMs for Global Language Environments_20250917|Apertus Democratizing Open and Compliant LLMs for Global Language Environments]] (79.9% similar)
- [[MOLE_ Metadata Extraction and Validation in Scientific Papers Using LLMs_20250919|MOLE Metadata Extraction and Validation in Scientific Papers Using LLMs]] (79.4% similar)
- [[Controlling Language Difficulty in Dialogues with Linguistic Features_20250919|Controlling Language Difficulty in Dialogues with Linguistic Features]] (79.2% similar)
- [[CARGO_ A Framework for Confidence-Aware Routing of Large Language Models_20250918|CARGO A Framework for Confidence-Aware Routing of Large Language Models]] (78.9% similar)
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (78.8% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem

## ğŸ“„ Abstract (ì›ë¬¸)

We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

HalaëŠ” ë²ˆì—­ ë° íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ êµ¬ì¶•ëœ ì•„ëì–´ ì¤‘ì‹¬ì˜ ì§€ì‹œ ë° ë²ˆì—­ ëª¨ë¸êµ°ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¨¼ì € ê°•ë ¥í•œ AR$\leftrightarrow$EN êµì‚¬ë¥¼ FP8ë¡œ ì••ì¶•í•˜ì—¬ í’ˆì§ˆ ì†ì‹¤ ì—†ì´ ì•½ 2ë°°ì˜ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì´ì¤‘ ì–¸ì–´ ê°ë…ì„ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê²½ëŸ‰ ì–¸ì–´ ëª¨ë¸ LFM2-1.2Bë¥¼ ì´ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì˜ì–´ ì§€ì‹œ ì„¸íŠ¸ë¥¼ ì•„ëì–´ë¡œ ë²ˆì—­í•˜ì—¬ ì§€ì‹œ ë”°ë¥´ê¸°ì— ë§ì¶˜ ë°±ë§Œ ê·œëª¨ì˜ ì½”í¼ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Hala ëª¨ë¸ì„ 350M, 700M, 1.2B ë° 9B ë§¤ê°œë³€ìˆ˜ë¡œ í›ˆë ¨í•˜ê³ , slerp ë³‘í•©ì„ ì ìš©í•˜ì—¬ ì•„ëì–´ íŠ¹í™”ì™€ ê¸°ë³¸ ëª¨ë¸ì˜ ê°•ì ì„ ê· í˜• ìˆê²Œ ì¡°ì •í•©ë‹ˆë‹¤. ì•„ëì–´ ì¤‘ì‹¬ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ HalaëŠ” "nano" (â‰¤2B) ë° "small" (7-9B) ì¹´í…Œê³ ë¦¬ ëª¨ë‘ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë©°, ê¸°ë³¸ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì•„ëì–´ NLP ì—°êµ¬ë¥¼ ê°€ì†í™”í•˜ê¸° ìœ„í•´ ëª¨ë¸, ë°ì´í„°, í‰ê°€ ë° ë ˆì‹œí”¼ë¥¼ ê³µê°œí•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

HalaëŠ” ì•„ëì–´ ì¤‘ì‹¬ì˜ ì§€ì‹œ ë° ë²ˆì—­ ëª¨ë¸ë¡œ, translate-and-tune íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê°•ë ¥í•œ ARâ†”EN êµì‚¬ë¥¼ FP8ë¡œ ì••ì¶•í•˜ì—¬ ì²˜ë¦¬ëŸ‰ì„ ë‘ ë°°ë¡œ ëŠ˜ë¦¬ë©´ì„œ í’ˆì§ˆ ì†ì‹¤ ì—†ì´ ê³ í’ˆì§ˆì˜ ì´ì¤‘ ì–¸ì–´ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ëŸ‰ ì–¸ì–´ ëª¨ë¸ LFM2-1.2Bë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì˜ì–´ ì§€ì‹œë¬¸ì„ ì•„ëì–´ë¡œ ë²ˆì—­, ì§€ì‹œë¬¸ì— íŠ¹í™”ëœ ë°±ë§Œ ê·œëª¨ì˜ ì½”í¼ìŠ¤ë¥¼ ì œì‘í–ˆìŠµë‹ˆë‹¤. Hala ëª¨ë¸ì€ 350M, 700M, 1.2B, 9B ë§¤ê°œë³€ìˆ˜ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, slerp ë³‘í•©ì„ í†µí•´ ì•„ëì–´ íŠ¹í™”ì™€ ê¸°ë³¸ ëª¨ë¸ì˜ ê°•ì ì„ ê· í˜• ìˆê²Œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤. ì•„ëì–´ ì¤‘ì‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ HalaëŠ” "nano" (â‰¤2B) ë° "small" (7-9B) ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸, ë°ì´í„°, í‰ê°€ ë° ë ˆì‹œí”¼ë¥¼ ê³µê°œí•˜ì—¬ ì•„ëì–´ NLP ì—°êµ¬ë¥¼ ê°€ì†í™”í•˜ê³ ì í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. HalaëŠ” ë²ˆì—­ ë° íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ê°œë°œëœ ì•„ëì–´ ì¤‘ì‹¬ì˜ ëª…ë ¹ ë° ë²ˆì—­ ëª¨ë¸ì…ë‹ˆë‹¤.

- 2. FP8ë¡œ ì••ì¶•ëœ ARâ†”EN êµì‚¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì´ì¤‘ ì–¸ì–´ ê°ë… ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

- 3. LFM2-1.2B ì–¸ì–´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì˜ì–´ ëª…ë ¹ ì„¸íŠ¸ë¥¼ ì•„ëì–´ë¡œ ë²ˆì—­í•˜ê³ , ëª…ë ¹ ìˆ˜í–‰ì— ë§ì¶˜ ë°±ë§Œ ê·œëª¨ì˜ ì½”í¼ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.

- 4. Hala ëª¨ë¸ì€ 350M, 700M, 1.2B, 9B íŒŒë¼ë¯¸í„°ë¡œ í›ˆë ¨ë˜ë©°, slerp ë³‘í•©ì„ í†µí•´ ì•„ëì–´ ì „ë¬¸ì„±ê³¼ ê¸°ë³¸ ëª¨ë¸ì˜ ê°•ì ì„ ê· í˜• ìˆê²Œ ì¡°ì •í•©ë‹ˆë‹¤.

- 5. HalaëŠ” ì•„ëì–´ ì¤‘ì‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, "nano" (â‰¤2B) ë° "small" (7-9B) ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ë°˜ ëª¨ë¸ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:19:18*