---
keywords:
  - Reinforcement Learning
  - Large Language Models
  - Vision-Language Models
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 22:44:39.255190",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Reinforcement Learning",
    "Large Language Models",
    "Vision-Language Models"
  ],
  "rejected_keywords": [
    "Tree-Guided Preference Optimization"
  ],
  "similarity_scores": {
    "Reinforcement Learning": 0.85,
    "Large Language Models": 0.8,
    "Vision-Language Models": 0.78
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->


# TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning

**Korean Title:** TGPO: ê°•ê±´í•œ ì›¹ ì—ì´ì „íŠ¸ ê°•í™” í•™ìŠµì„ ìœ„í•œ íŠ¸ë¦¬ ì•ˆë‚´í˜• ì„ í˜¸ ìµœì í™”

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250917|2025-09-17]]        [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸ”— Specific Connectable**: [[keywords/Reinforcement Learning|Reinforcement Learning]]
**ğŸš€ Evolved Concepts**: [[keywords/Large Language Models|Large Language Models]], [[keywords/Vision-Language Models|Vision-Language Models]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[TGPO Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning]] (99.2% similar)
- [[Controllable Pareto Trade-off between Fairness and Accuracy]] (80.4% similar)
- [[DRDT3 Diffusion-Refined Decision Test-Time Training Model]] (79.9% similar)
- [[Video-Language Critic Transferable Reward Functions for Language-Conditioned Robotics]] (79.8% similar)
- [[AgentCTG Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation]] (79.5% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye, Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, Guitao Cao

## ğŸ“„ Abstract (ì›ë¬¸)

With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë° ì‹œê°-ì–¸ì–´ ëª¨ë¸ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì›¹ ì—ì´ì „íŠ¸ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì— í•„ìˆ˜ì ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°•í™” í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ì‹ ìš© í• ë‹¹ ì˜¤ë¥˜, ì§€ë‚˜ì¹˜ê²Œ ë†’ì€ ì£¼ì„ ë¹„ìš© ë° ë³´ìƒ í¬ì†Œì„±ê³¼ ê°™ì€ ì¤‘ìš”í•œ ë„ì „ì— ì§ë©´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” Tree-Guided Preference Optimization (TGPO)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ê¸° ìœ„í•´ ê¶¤ì  í‘œí˜„ì„ í•©ì¹˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ë¡œ êµ¬ì„±ëœ ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ í”„ë ˆì„ì›Œí¬ëŠ” í•˜ìœ„ ëª©í‘œ ì§„í–‰, ì¤‘ë³µ ê°ì§€ ë° ì‘ì—… í™•ì¸ì„ í†µí•´ ì„¸ë¶€ì ì¸ ë³´ìƒì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” Process Reward Modelì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, í›ˆë ¨ ì¤‘ì— ê³ ì˜í–¥ ê²°ì • ì§€ì ì„ ìš°ì„ ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ë™ì  ê°€ì¤‘ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. Online-Mind2Web ë° ìš°ë¦¬ê°€ ì§ì ‘ êµ¬ì¶•í•œ C-WebShop ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼ëŠ” TGPOê°€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì—¬ì£¼ë©°, ë” ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¡œ ë” ë†’ì€ ì„±ê³µë¥ ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ë° ì‹œê°-ì–¸ì–´ ëª¨ë¸ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì›¹ ì—ì´ì „íŠ¸ë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì— í•„ìˆ˜ì ì´ ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°•í™” í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ì‹ ìš© í• ë‹¹ ì˜¤ë¥˜, ê³¼ë„í•œ ì£¼ì„ ë¹„ìš© ë° ë³´ìƒ í¬ì†Œì„±ê³¼ ê°™ì€ ì¤‘ìš”í•œ ë„ì „ì— ì§ë©´í•˜ê³  ìˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” Tree-Guided Preference Optimization (TGPO)ì´ë¼ëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤. TGPOëŠ” ë ˆì´ë¸” ì¶©ëŒì„ ì œê±°í•˜ê¸° ìœ„í•´ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœë¥¼ ë³‘í•©í•˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê²½ë¡œ í‘œí˜„ì„ ì œì•ˆí•œë‹¤. ë˜í•œ ì„¸ë¶€ ëª©í‘œ ì§„í–‰, ì¤‘ë³µ ê°ì§€ ë° ì‘ì—… í™•ì¸ì„ í†µí•´ ì„¸ë¶„í™”ëœ ë³´ìƒì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” Process Reward Modelì„ í¬í•¨í•˜ê³  ìˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” TGPOê°€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©° ë” ì ì€ ì¤‘ë³µ ë‹¨ê³„ë¡œ ë” ë†’ì€ ì„±ê³µë¥ ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë° ì‹œê°-ì–¸ì–´ ëª¨ë¸ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì›¹ ì—ì´ì „íŠ¸ë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ ìë™í™”ëœ ì›¹ ìƒí˜¸ì‘ìš©ì— í•„ìˆ˜ì ì´ ë˜ì—ˆë‹¤.

- TGPOëŠ” ì˜¤í”„ë¼ì¸ ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, ë¼ë²¨ ì¶©ëŒì„ ì œê±°í•˜ê¸° ìœ„í•´ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœë¥¼ ë³‘í•©í•˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ê²½ë¡œ í‘œí˜„ì„ ì œì•ˆí•œë‹¤.

- í”„ë¡œì„¸ìŠ¤ ë³´ìƒ ëª¨ë¸ì„ í†µí•´ í•˜ìœ„ ëª©í‘œ ì§„í–‰, ì¤‘ë³µ ê°ì§€ ë° ì‘ì—… í™•ì¸ì„ í†µí•´ ì„¸ë¶€ì ì¸ ë³´ìƒì„ ìë™ìœ¼ë¡œ ìƒì„±í•œë‹¤.

- TGPOëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì„±ê³µë¥ ì„ ë‹¬ì„±í•˜ë©´ì„œ ì¤‘ë³µ ë‹¨ê³„ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ì‹¤í—˜ì„ í†µí•´ ì…ì¦í–ˆë‹¤.

---

*Generated on 2025-09-18 17:04:30*