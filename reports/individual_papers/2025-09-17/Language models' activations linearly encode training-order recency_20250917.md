# Language models' activations linearly encode training-order recency

**Korean Title:** ì–¸ì–´ ëª¨ë¸ì˜ í™œì„±í™”ëŠ” í›ˆë ¨ ìˆœì„œì˜ ìµœì‹ ì„±ì„ ì„ í˜•ì ìœ¼ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Dmitrii Krasheninnikov|Dmitrii Krasheninnikov]] [[authors/Richard E. Turner|Richard E. Turner]] [[authors/David Krueger|David Krueger]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Temporal Encoding in Language Models

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (81.4% similar)
- [[Large Multi-modal Models Can Interpret Features in Large Multi-modal Models_20250919|Large Multi-modal Models Can Interpret Features in Large Multi-modal Models]] (80.9% similar)
- [[Decoupled Proxy Alignment_ Mitigating Language Prior Conflict for Multimodal Alignment in MLLM_20250919|Decoupled Proxy Alignment Mitigating Language Prior Conflict for Multimodal Alignment in MLLM]] (80.6% similar)
- [[Modular Machine Learning_ An Indispensable Path towards New-Generation Large Language Models_20250919|Modular Machine Learning An Indispensable Path towards New-Generation Large Language Models]] (80.3% similar)
- [[Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (80.2% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Dmitrii Krasheninnikov, Richard E. Turner, David Krueger

## ğŸ“„ Abstract (ì›ë¬¸)

We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì–¸ì–´ ëª¨ë¸ì˜ í™œì„±í™”ê°€ í•™ìŠµ ì¤‘ ì •ë³´ê°€ í•™ìŠµëœ ì‹œì ì„ ì„ í˜•ì ìœ¼ë¡œ ì¸ì½”ë”©í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì„¤ì •ì€ Llama-3.2-1Bë¥¼ ì—¬ì„¯ ê°œì˜ ë¶„ë¦¬ëœ, ê·¸ëŸ¬ë‚˜ ìœ ì‚¬í•œ ëª…ëª…ëœ ì—”í‹°í‹°ì— ê´€í•œ ë°ì´í„°ì…‹ì— ìˆœì°¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ì—¬ ì•Œë ¤ì§„ í•™ìŠµ ìˆœì„œë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—¬ì„¯ ê°œì˜ í•™ìŠµ ë°ì´í„°ì…‹ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì˜ í‰ê·  í™œì„±í™”ê°€ í•™ìŠµ ìˆœì„œë¥¼ ì¸ì½”ë”©í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. 2D ë¶€ë¶„ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ë  ë•Œ, ì´ëŸ¬í•œ ì¤‘ì‹¬ì ë“¤ì€ ì •í™•íˆ í•™ìŠµ ìˆœì„œëŒ€ë¡œ ë°°ì—´ë˜ë©° ì§ì„  ìœ„ì— ë†“ì…ë‹ˆë‹¤. ë” ë‚˜ì•„ê°€, ì„ í˜• í”„ë¡œë¸Œê°€ "ì´ˆê¸°"ì™€ "í›„ê¸°" ì—”í‹°í‹°ë¥¼ ì•½ 90%ì˜ ì •í™•ë„ë¡œ êµ¬ë³„í•  ìˆ˜ ìˆìœ¼ë©°, í”„ë¡œë¸Œ ìì²´ì˜ í•™ìŠµ ì¤‘ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì—ë„ ì¼ë°˜í™”í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë¸ì€ ë˜í•œ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì˜ í•™ìŠµ ë‹¨ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ í•˜ë„ë¡ ì•½ 80%ì˜ ì •í™•ë„ë¡œ íŒŒì¸íŠœë‹ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¥ë¯¸ë¡­ê²Œë„, ì´ëŸ¬í•œ ì‹œê°„ì  ì‹ í˜¸ëŠ” í™œì„±í™” í¬ê¸°, ì†ì‹¤ ë˜ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ë„ì™€ ê°™ì€ ë‹¨ìˆœí•œ ì°¨ì´ë¡œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë…¼ë¬¸ì€ ëª¨ë¸ì´ ì •ë³´ì˜ íšë“ ì‹œì ì— ë”°ë¼ ì •ë³´ë¥¼ êµ¬ë³„í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ëª¨ë¸ì´ ìƒì¶©ë˜ëŠ” ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê³  ì§€ì‹ ìˆ˜ì •ì— ëŒ€ì‘í•˜ëŠ” ë°©ì‹ì— ì¤‘ëŒ€í•œ í•¨ì˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ì—°êµ¬ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ í™œì„±í™”ê°€ í•™ìŠµ ì¤‘ ì •ë³´ê°€ ìŠµë“ëœ ì‹œì ì„ ì„ í˜•ì ìœ¼ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Llama-3.2-1B ëª¨ë¸ì„ ì—¬ì„¯ ê°œì˜ ë¶„ë¦¬ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ í•™ìŠµ ìˆœì„œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì˜ í‰ê·  í™œì„±í™”ê°€ í•™ìŠµ ìˆœì„œë¥¼ ì¸ì½”ë”©í•˜ë©°, 2D ê³µê°„ì— íˆ¬ì˜í–ˆì„ ë•Œ ì´ ì¤‘ì‹¬ì ë“¤ì´ í•™ìŠµ ìˆœì„œëŒ€ë¡œ ì§ì„ ìƒì— ë°°ì—´ë¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„ í˜• í”„ë¡œë¸Œë¥¼ í†µí•´ 'ì´ˆê¸°'ì™€ 'í›„ê¸°' ì—”í‹°í‹°ë¥¼ ì•½ 90%ì˜ ì •í™•ë„ë¡œ êµ¬ë³„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í”„ë¡œë¸Œ í•™ìŠµ ì‹œ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì—ë„ ì¼ë°˜í™”ë©ë‹ˆë‹¤. ëª¨ë¸ì€ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì˜ í•™ìŠµ ë‹¨ê³„ë„ ì•½ 80%ì˜ ì •í™•ë„ë¡œ ë³´ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹œê°„ ì‹ í˜¸ëŠ” ë‹¨ìˆœí•œ í™œì„±í™” í¬ê¸°, ì†ì‹¤, ëª¨ë¸ì˜ ì‹ ë¢°ë„ ì°¨ì´ì— ê¸°ì¸í•˜ì§€ ì•Šìœ¼ë©°, ëª¨ë¸ì´ ì •ë³´ ìŠµë“ ì‹œì ì„ êµ¬ë³„í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ìƒì¶©ë˜ëŠ” ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê³  ì§€ì‹ ìˆ˜ì •ì— ëŒ€ì‘í•˜ëŠ” ë°©ì‹ì— ì¤‘ìš”í•œ í•¨ì˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ì–¸ì–´ ëª¨ë¸ì˜ í™œì„±í™”ëŠ” ì •ë³´ê°€ í•™ìŠµëœ ì‹œì ì„ ì„ í˜•ì ìœ¼ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ìˆë‹¤.

- 2. Llama-3.2-1B ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ í›ˆë ¨ ìˆœì„œë¥¼ ì•Œ ìˆ˜ ìˆëŠ” ì„¤ì •ì„ ë§Œë“¤ì—ˆë‹¤.

- 3. í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì˜ í‰ê·  í™œì„±í™”ê°€ í›ˆë ¨ ìˆœì„œë¥¼ ì¸ì½”ë”©í•˜ë©°, ì´ëŠ” 2D ì„œë¸ŒìŠ¤í˜ì´ìŠ¤ì— íˆ¬ì˜ë  ë•Œ í›ˆë ¨ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœë‹¤.

- 4. ì„ í˜• í”„ë¡œë¸ŒëŠ” "ì´ˆê¸°"ì™€ "í›„ê¸°" ì—”í‹°í‹°ë¥¼ ì•½ 90%ì˜ ì •í™•ë„ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.

- 5. ëª¨ë¸ì€ ë³´ì§€ ëª»í•œ ì—”í‹°í‹°ì˜ í›ˆë ¨ ë‹¨ê³„ë¥¼ ì•½ 80%ì˜ ì •í™•ë„ë¡œ ë³´ê³ í•˜ë„ë¡ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìˆë‹¤.

---

*Generated on 2025-09-20 07:42:07*