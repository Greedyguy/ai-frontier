---
keywords:
  - Deep Learning
  - Activation Functions
  - Kuramoto Model
category: cs.AI
publish_date: 2025-09-17
arxiv_id:
---

<!-- KEYWORD_LINKING_METADATA:
{
  "processed_timestamp": "2025-09-22 23:03:05.713850",
  "vocabulary_version": "1.0",
  "selected_keywords": [
    "Deep Learning",
    "Activation Functions",
    "Kuramoto Model"
  ],
  "rejected_keywords": [
    "Nonlocal Conservation Laws"
  ],
  "similarity_scores": {
    "Deep Learning": 0.85,
    "Activation Functions": 0.8,
    "Kuramoto Model": 0.7
  },
  "extraction_method": "AI_prompt_based",
  "budget_applied": true
}
-->

# A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation

**Korean Title:** ë™ì¼í•œ ì¿ ë¼ë§ˆí†  ë°©ì •ì‹ì„ ìœ„í•œ ì‹ ê²½ë§: êµ¬ì¡°ì  ê³ ë ¤ì‚¬í•­ ë° ì„±ëŠ¥ í‰ê°€

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[digests/daily_digest_20250917|2025-09-17]]     [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸŒ Broad Technical**: [[keywords/Deep Learning|Deep Neural Networks]]
**ğŸ”— Specific Connectable**: [[keywords/Activation Functions|activation function]]
**âš¡ Unique Technical**: [[keywords/Kuramoto Model|Kuramoto model]]

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers_20250918|Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers]] (81.4% similar)
- [[Data coarse graining can improve model performance_20250918|Data coarse graining can improve model performance]] (80.9% similar)
- [[Probabilistic and nonlinear compressive sensing_20250918|Probabilistic and nonlinear compressive sensing]] (80.4% similar)
- [[Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks_20250917|Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks]] (79.8% similar)
- [[A Universal Banach--Bregman Framework for Stochastic Iterations_ Unifying Stochastic Mirror Descent, Learning and LLM Training_20250917|A Universal Banach--Bregman Framework for Stochastic Iterations Unifying Stochastic Mirror Descent, Learning and LLM Training]] (79.4% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Nishantak Panigrahi, Mayank Patwal

## ğŸ“„ Abstract (ì›ë¬¸)

In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)
to approximate the solution of a nonlocal conservation law derived from the
identical-oscillator Kuramoto model, focusing on the evaluation of an
architectural choice and its impact on solution accuracy based on the energy
norm and computation time. Through systematic experimentation, we demonstrate
that network configuration parameters-specifically, activation function
selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width
(64-256 neurons), and training methodology (collocation points, epoch
count)-significantly influence convergence characteristics. We observe that
tanh activation yields stable convergence across configurations, whereas sine
activation can attain marginally lower errors and training times in isolated
cases, but occasionally produce nonphysical artefacts. Our comparative analysis
with traditional numerical methods shows that optimally configured DNNs offer
competitive accuracy with notably different computational trade-offs.
Furthermore, we identify fundamental limitations of standard feed-forward
architectures when handling singular or piecewise-constant solutions, providing
empirical evidence that such networks inherently oversmooth sharp features due
to the natural function space limitations of standard activation functions.
This work contributes to the growing body of research on neural network-based
scientific computing by providing practitioners with empirical guidelines for
DNN implementation while illuminating fundamental theoretical constraints that
must be overcome to expand their applicability to more challenging physical
systems with discontinuities.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë™ì¼í•œ ì§„ë™ì ì¿ ë¼ë§ˆí†  ëª¨ë¸ì—ì„œ ìœ ë„ëœ ë¹„êµ­ì†Œ ë³´ì¡´ ë²•ì¹™ì˜ í•´ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•œ ì‹¬ì¸µ ì‹ ê²½ë§(Deep Neural Networks, DNNs)ì˜ íš¨ìœ¨ì„±ì„ ì¡°ì‚¬í•˜ë©°, ì—ë„ˆì§€ ë…¸ë¦„ê³¼ ê³„ì‚° ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í•´ì˜ ì •í™•ì„±ì— ë¯¸ì¹˜ëŠ” ê±´ì¶•ì  ì„ íƒì˜ í‰ê°€ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì²´ê³„ì ì¸ ì‹¤í—˜ì„ í†µí•´ ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ë§¤ê°œë³€ìˆ˜, ì¦‰ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ(tanh vs. sin vs. ReLU), ë„¤íŠ¸ì›Œí¬ ê¹Šì´(4-8ê°œì˜ ì€ë‹‰ì¸µ), í­(64-256ê°œì˜ ë‰´ëŸ°), í›ˆë ¨ ë°©ë²•ë¡ (ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸, ì—í¬í¬ ìˆ˜)ì´ ìˆ˜ë ´ íŠ¹ì„±ì— ìƒë‹¹í•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” tanh í™œì„±í™”ê°€ êµ¬ì„± ì „ë°˜ì— ê±¸ì³ ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ì œê³µí•˜ëŠ” ë°˜ë©´, ì‚¬ì¸ í™œì„±í™”ëŠ” íŠ¹ì • ê²½ìš°ì— ì•½ê°„ ë” ë‚®ì€ ì˜¤ë¥˜ì™€ í›ˆë ¨ ì‹œê°„ì„ ë‹¬ì„±í•  ìˆ˜ ìˆì§€ë§Œ ë•Œë•Œë¡œ ë¹„ë¬¼ë¦¬ì  ì¸ê³µë¬¼ì„ ìƒì„±í•  ìˆ˜ ìˆìŒì„ ê´€ì°°í•©ë‹ˆë‹¤. ì „í†µì ì¸ ìˆ˜ì¹˜ ë°©ë²•ê³¼ì˜ ë¹„êµ ë¶„ì„ì„ í†µí•´ ìµœì ì˜ DNN êµ¬ì„±ì€ ìƒë‹¹íˆ ë‹¤ë¥¸ ê³„ì‚°ì  ì ˆì¶©ì„ ê°€ì§€ë©´ì„œë„ ê²½ìŸë ¥ ìˆëŠ” ì •í™•ì„±ì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, í‘œì¤€ í”¼ë“œí¬ì›Œë“œ ì•„í‚¤í…ì²˜ê°€ íŠ¹ì´ì ì´ë‚˜ êµ¬ê°„ë³„ ìƒìˆ˜ í•´ë¥¼ ì²˜ë¦¬í•  ë•Œì˜ ê·¼ë³¸ì ì¸ í•œê³„ë¥¼ ì‹ë³„í•˜ë©°, ì´ëŸ¬í•œ ë„¤íŠ¸ì›Œí¬ê°€ í‘œì¤€ í™œì„±í™” í•¨ìˆ˜ì˜ ìì—°ì  í•¨ìˆ˜ ê³µê°„ ì œí•œìœ¼ë¡œ ì¸í•´ ë³¸ì§ˆì ìœ¼ë¡œ ê¸‰ê²©í•œ íŠ¹ì§•ì„ ê³¼ë„í•˜ê²Œ ë¶€ë“œëŸ½ê²Œ ë§Œë“ ë‹¤ëŠ” ì‹¤ì¦ì  ì¦ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì‹ ê²½ë§ ê¸°ë°˜ ê³¼í•™ ê³„ì‚°ì— ëŒ€í•œ ì—°êµ¬ì˜ ì¦ê°€í•˜ëŠ” ë³¸ë¬¸ì— ê¸°ì—¬í•˜ë©°, ì‹¤ë¬´ìì—ê²Œ DNN êµ¬í˜„ì— ëŒ€í•œ ì‹¤ì¦ì  ì§€ì¹¨ì„ ì œê³µí•˜ê³  ë¶ˆì—°ì†ì„±ì„ ê°€ì§„ ë³´ë‹¤ ë„ì „ì ì¸ ë¬¼ë¦¬ì  ì‹œìŠ¤í…œì— ëŒ€í•œ ì ìš© ê°€ëŠ¥ì„±ì„ í™•ì¥í•˜ê¸° ìœ„í•´ ê·¹ë³µí•´ì•¼ í•  ê·¼ë³¸ì ì¸ ì´ë¡ ì  ì œì•½ì„ ì¡°ëª…í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë™ì¼ ì§„ë™ì ì¿ ë¼ëª¨í†  ëª¨ë¸ì—ì„œ ìœ ë„ëœ ë¹„êµ­ì†Œ ë³´ì¡´ ë²•ì¹™ì˜ í•´ë¥¼ ê·¼ì‚¬í•˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬(DNN)ì˜ íš¨ìœ¨ì„±ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ìš”ì†Œì¸ í™œì„±í™” í•¨ìˆ˜(tanh, sin, ReLU), ë„¤íŠ¸ì›Œí¬ ê¹Šì´ì™€ ë„ˆë¹„, í›ˆë ¨ ë°©ë²•ë¡ ì´ ìˆ˜ë ´ íŠ¹ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤. tanh í™œì„±í™” í•¨ìˆ˜ëŠ” ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ë³´ì—¬ì£¼ë©°, sin í™œì„±í™” í•¨ìˆ˜ëŠ” íŠ¹ì • ìƒí™©ì—ì„œ ë‚®ì€ ì˜¤ë¥˜ì™€ í›ˆë ¨ ì‹œê°„ì„ ì œê³µí•˜ì§€ë§Œ ë¹„ë¬¼ë¦¬ì  ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „í†µì  ìˆ˜ì¹˜ í•´ë²•ê³¼ ë¹„êµí•˜ì—¬ ìµœì í™”ëœ DNNì€ ê²½ìŸë ¥ ìˆëŠ” ì •í™•ì„±ì„ ì œê³µí•˜ì§€ë§Œ ê³„ì‚°ì  ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, í‘œì¤€ í”¼ë“œí¬ì›Œë“œ êµ¬ì¡°ê°€ ê¸‰ê²©í•œ íŠ¹ì§•ì„ ë‹¤ë£¨ëŠ” ë° í•œê³„ê°€ ìˆìŒì„ ë°í˜”ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ê³¼í•™ì  ê³„ì‚°ì—ì„œ DNNì˜ êµ¬í˜„ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•˜ê³ , ë¶ˆì—°ì†ì„±ì„ ê°€ì§„ ë¬¼ë¦¬ ì‹œìŠ¤í…œì— ëŒ€í•œ ì ìš©ì„±ì„ í™•ì¥í•˜ê¸° ìœ„í•œ ì´ë¡ ì  ì œì•½ì„ ì¡°ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì„± ìš”ì†Œ, íŠ¹íˆ í™œì„±í™” í•¨ìˆ˜ ì„ íƒê³¼ ë„¤íŠ¸ì›Œí¬ì˜ ê¹Šì´ ë° ë„ˆë¹„ê°€ ìˆ˜ë ´ íŠ¹ì„±ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

- 2. tanh í™œì„±í™” í•¨ìˆ˜ëŠ” ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ë³´ì´ë©°, sine í™œì„±í™” í•¨ìˆ˜ëŠ” íŠ¹ì • ê²½ìš°ì— ë” ë‚®ì€ ì˜¤ë¥˜ì™€ ì§§ì€ í•™ìŠµ ì‹œê°„ì„ ì œê³µí•  ìˆ˜ ìˆì§€ë§Œ ë¹„ë¬¼ë¦¬ì  ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.

- 3. ìµœì í™”ëœ DNNì€ ì „í†µì ì¸ ìˆ˜ì¹˜ í•´ì„ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ê²½ìŸë ¥ ìˆëŠ” ì •í™•ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ ê³„ì‚°ì  íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ë‹¤ë¥´ë‹¤.

- 4. í‘œì¤€ í”¼ë“œí¬ì›Œë“œ ì•„í‚¤í…ì²˜ëŠ” íŠ¹ì´ì ì´ë‚˜ ì¡°ê°ë³„ ìƒìˆ˜ ì†”ë£¨ì…˜ì„ ì²˜ë¦¬í•  ë•Œ ë³¸ì§ˆì ìœ¼ë¡œ ë‚ ì¹´ë¡œìš´ íŠ¹ì§•ì„ ê³¼ë„í•˜ê²Œ ë¶€ë“œëŸ½ê²Œ ë§Œë“œëŠ” í•œê³„ë¥¼ ê°€ì§„ë‹¤.

- 5. ì´ ì—°êµ¬ëŠ” ì‹ ê²½ë§ ê¸°ë°˜ ê³¼í•™ ê³„ì‚°ì— ëŒ€í•œ ì‹¤ì¦ì  ì§€ì¹¨ì„ ì œê³µí•˜ë©°, ë¶ˆì—°ì†ì„±ì„ ê°€ì§„ ë¬¼ë¦¬ ì‹œìŠ¤í…œì— ëŒ€í•œ ì ìš©ì„±ì„ í™•ì¥í•˜ê¸° ìœ„í•œ ì´ë¡ ì  ì œì•½ì„ ë°íŒë‹¤.

---

*Generated on 2025-09-20 07:37:12*