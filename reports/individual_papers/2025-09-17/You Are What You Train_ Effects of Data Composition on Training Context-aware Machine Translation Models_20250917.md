# You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models

**Korean Title:** ë‹¹ì‹ ì€ ë‹¹ì‹ ì´ í›ˆë ¨í•œ ê²ƒ: ë¬¸ë§¥ ì¸ì‹ ê¸°ê³„ ë²ˆì—­ ëª¨ë¸ í›ˆë ¨ì— ëŒ€í•œ ë°ì´í„° êµ¬ì„±ì˜ íš¨ê³¼

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/PaweÅ‚ MÄ…ka|PaweÅ‚ MÄ…ka]] [[authors/Yusuf Can Semerci|Yusuf Can Semerci]] [[authors/Jan Scholtes|Jan Scholtes]] [[authors/Gerasimos Spanakis|Gerasimos Spanakis]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Context-aware Training Strategies

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Translate, then Detect_ Leveraging Machine Translation for Cross-Lingual Toxicity Classification_20250919|Translate, then Detect Leveraging Machine Translation for Cross-Lingual Toxicity Classification]] (80.1% similar)
- [[Internalizing Self-Consistency in Language Models_ Multi-Agent Consensus Alignment_20250919|Internalizing Self-Consistency in Language Models Multi-Agent Consensus Alignment]] (79.5% similar)
- [[Middo_ Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning_20250919|Middo Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning]] (79.3% similar)
- [[Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (79.2% similar)
- [[TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (79.0% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** PaweÅ‚ MÄ…ka, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis

## ğŸ“„ Abstract (ì›ë¬¸)

Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ì¸ê°„ ìˆ˜ì¤€ì˜ ë²ˆì—­ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ë§¥ë½ì„ í™œìš©í•˜ì—¬ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê³  ëŒ€ëª…ì‚¬ í•´ì†Œì™€ ê°™ì€ ë³µì¡í•œ í˜„ìƒì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. í‘œì¤€ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë§¥ë½ì ìœ¼ë¡œ í’ë¶€í•œ ì˜ˆì‹œì˜ í¬ì†Œì„±ì€ ë§¥ë½ í™œìš©ì˜ ì–´ë ¤ì›€ì˜ ì›ì¸ìœ¼ë¡œ ê°€ì„¤ì´ ì œê¸°ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‹¨ì¼ ë° ë‹¤êµ­ì–´ í™˜ê²½ ëª¨ë‘ì—ì„œ ë§¥ë½ì ìœ¼ë¡œ ê´€ë ¨ ìˆëŠ” ì˜ˆì‹œì˜ ë¹„ìœ¨ì„ ì¡°ì ˆí•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•¨ìœ¼ë¡œì¨ ì´ ì£¼ì¥ì„ ì²´ê³„ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ í¬ì†Œì„±ê³¼ ëª¨ë¸ ì„±ëŠ¥ ê°„ì˜ ê°•í•œ ì—°ê´€ì„±ì„ ì…ì¦í•˜ì—¬ í¬ì†Œì„±ì´ ì£¼ìš” ë³‘ëª©ì„ì„ í™•ì¸í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€, í•˜ë‚˜ì˜ ë§¥ë½ì  í˜„ìƒì—ì„œì˜ ê°œì„ ì´ ë‹¤ë¥¸ í˜„ìƒìœ¼ë¡œ ì¼ë°˜í™”ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì–¸ì–´ ê°„ ì „ì´ê°€ ê´€ì°°ë˜ì—ˆìœ¼ë‚˜, ë™ì¼í•œ í•˜ìœ„ ê³„ì—´ ë‚´ ì–¸ì–´ ê°„ì—ì„œëŠ” ê·¸ë‹¤ì§€ ë†’ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ë‘ ê°€ì§€ í›ˆë ¨ ì „ëµì„ ì œì•ˆí•˜ê³  ì‹¤ì¦ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „ëµì€ ë§¥ë½ í™œìš©ì„ ê°œì„ í•˜ì—¬ ë‹¨ì¼ ë° ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ ê°ê° ctxPro í‰ê°€ì—ì„œ ìµœëŒ€ 6% ë° 8% í¬ì¸íŠ¸ì˜ ì •í™•ë„ í–¥ìƒì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ì¸ê°„ ìˆ˜ì¤€ì˜ ë²ˆì—­ì„ ìœ„í•´ ë¬¸ë§¥ í™œìš©ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ë¬¸ë§¥ì ìœ¼ë¡œ í’ë¶€í•œ ì˜ˆì‹œì˜ ë¶€ì¡±ì´ ë²ˆì—­ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ ì›ì¸ì„ì„ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤. ì—°êµ¬ëŠ” ë‹¨ì¼ ë° ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ ë¬¸ë§¥ ê´€ë ¨ ì˜ˆì‹œì˜ ë¹„ìœ¨ì„ ì¡°ì ˆí•œ í›ˆë ¨ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬, ë°ì´í„° í¬ì†Œì„±ê³¼ ëª¨ë¸ ì„±ëŠ¥ ê°„ì˜ ê°•í•œ ì—°ê´€ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, í•œ ë¬¸ë§¥ í˜„ìƒì˜ ê°œì„ ì´ ë‹¤ë¥¸ í˜„ìƒìœ¼ë¡œ ì¼ë°˜í™”ë˜ì§€ ì•ŠìŒì„ ë°í˜”ìœ¼ë©°, ë™ì¼ ì–¸ì–´ ê³„ì—´ ë‚´ì—ì„œì˜ ì–¸ì–´ ê°„ ì „ì´ëŠ” ë¯¸ë¯¸í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë¬¸ë§¥ í™œìš©ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ í›ˆë ¨ ì „ëµì„ ì œì•ˆí•˜ê³  í‰ê°€í•˜ì—¬, ë‹¨ì¼ ë° ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ ê°ê° ìµœëŒ€ 6%ì™€ 8%ì˜ ì •í™•ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ë§¥ë½ì ìœ¼ë¡œ í’ë¶€í•œ ì˜ˆì‹œì˜ ë¶€ì¡±ì´ ë²ˆì—­ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ì˜ ì£¼ìš” ì›ì¸ìœ¼ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.

- 2. ë‹¨ì¼ ì–¸ì–´ ë° ë‹¤êµ­ì–´ ì„¤ì •ì—ì„œ ë§¥ë½ í™œìš©ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ í›ˆë ¨ ì „ëµì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

- 3. ì œì•ˆëœ í›ˆë ¨ ì „ëµì€ ë‹¨ì¼ ì–¸ì–´ ë° ë‹¤êµ­ì–´ ì„¤ì •ì—ì„œ ê°ê° ìµœëŒ€ 6%ì™€ 8%ì˜ ì •í™•ë„ í–¥ìƒì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

- 4. í•œ ë§¥ë½ì  í˜„ìƒì˜ ê°œì„ ì´ ë‹¤ë¥¸ í˜„ìƒìœ¼ë¡œ ì¼ë°˜í™”ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.

- 5. ê°™ì€ ì–¸ì–´ ê³„ì—´ ë‚´ì—ì„œì˜ êµì°¨ ì–¸ì–´ ì „ì´ëŠ” í¬ê²Œ ì¦ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:16:16*