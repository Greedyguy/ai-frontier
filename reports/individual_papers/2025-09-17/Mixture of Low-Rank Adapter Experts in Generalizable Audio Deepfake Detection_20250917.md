# Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection

**Korean Title:** ì¼ë°˜í™” ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€ì—ì„œ ì €ì°¨ì› ì–´ëŒ‘í„° ì „ë¬¸ê°€ì˜ í˜¼í•©

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-17|2025-09-17]] [[authors/Janne Laakkonen|Janne Laakkonen]] [[authors/Ivan Kukanov|Ivan Kukanov]] [[authors/Ville HautamÃ¤ki|Ville HautamÃ¤ki]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Generalizable Audio Deepfake Detection

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning_20250918|Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning]] (84.5% similar)
- [[Opening the Black Box_ Interpretable LLMs via Semantic Resonance Architecture_20250919|Opening the Black Box Interpretable LLMs via Semantic Resonance Architecture]] (83.0% similar)
- [[FURINA_ Free from Unmergeable Router via LINear Aggregation of mixed experts_20250919|FURINA Free from Unmergeable Router via LINear Aggregation of mixed experts]] (82.8% similar)
- [[Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting_20250918|Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting]] (82.3% similar)
- [[Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation_20250919|Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation]] (81.3% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** Janne Laakkonen, Ivan Kukanov, Ville HautamÃ¤ki

## ğŸ“„ Abstract (ì›ë¬¸)

Foundation models such as Wav2Vec2 excel at representation learning in speech
tasks, including audio deepfake detection. However, after being fine-tuned on a
fixed set of bonafide and spoofed audio clips, they often fail to generalize to
novel deepfake methods not represented in training. To address this, we propose
a mixture-of-LoRA-experts approach that integrates multiple low-rank adapters
(LoRA) into the model's attention layers. A routing mechanism selectively
activates specialized experts, enhancing adaptability to evolving deepfake
attacks. Experimental results show that our method outperforms standard
fine-tuning in both in-domain and out-of-domain scenarios, reducing equal error
rates relative to baseline models. Notably, our best MoE-LoRA model lowers the
average out-of-domain EER from 8.55\% to 6.08\%, demonstrating its
effectiveness in achieving generalizable audio deepfake detection.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

ê¸°ì´ˆ ëª¨ë¸ì¸ Wav2Vec2ëŠ” ì˜¤ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€ë¥¼ í¬í•¨í•œ ìŒì„± ì‘ì—…ì—ì„œ í‘œí˜„ í•™ìŠµì— ë›°ì–´ë‚©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³ ì •ëœ ë³¸ë˜ ë° ìœ„ì¡° ì˜¤ë””ì˜¤ í´ë¦½ ì„¸íŠ¸ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •ëœ í›„ì—ëŠ” í›ˆë ¨ì— í¬í•¨ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë”¥í˜ì´í¬ ë°©ë²•ì— ì¼ë°˜í™”í•˜ëŠ” ë° ì¢…ì¢… ì‹¤íŒ¨í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— ì—¬ëŸ¬ ì €ìˆœìœ„ ì–´ëŒ‘í„°(LoRA)ë¥¼ í†µí•©í•˜ëŠ” LoRA ì „ë¬¸ê°€ í˜¼í•© ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì€ íŠ¹í™”ëœ ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ ì§„í™”í•˜ëŠ” ë”¥í˜ì´í¬ ê³µê²©ì— ëŒ€í•œ ì ì‘ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ì— ë”°ë¥´ë©´, ìš°ë¦¬ ë°©ë²•ì€ ë„ë©”ì¸ ë‚´ ë° ë„ë©”ì¸ ì™¸ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‘ì—ì„œ í‘œì¤€ ë¯¸ì„¸ ì¡°ì •ë³´ë‹¤ ë›°ì–´ë‚˜ë©°, ê¸°ì¤€ ëª¨ë¸ì— ë¹„í•´ ë™ë“± ì˜¤ë¥˜ìœ¨ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤. íŠ¹íˆ, ìš°ë¦¬ì˜ ìµœìƒì˜ MoE-LoRA ëª¨ë¸ì€ ë„ë©”ì¸ ì™¸ í‰ê·  EERì„ 8.55%ì—ì„œ 6.08%ë¡œ ë‚®ì¶”ì–´ ì¼ë°˜í™” ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€ì—ì„œì˜ íš¨ê³¼ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ìŒì„± ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” Wav2Vec2ì™€ ê°™ì€ ê¸°ì´ˆ ëª¨ë¸ì˜ ì¼ë°˜í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°©ë²•ë¡ ì„ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì€ ê³ ì •ëœ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •ë˜ë©´ ìƒˆë¡œìš´ ë”¥í˜ì´í¬ ë°©ë²•ì— ëŒ€í•œ ì¼ë°˜í™”ê°€ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— ì—¬ëŸ¬ ì €ìˆœìœ„ ì–´ëŒ‘í„°(LoRA)ë¥¼ í†µí•©í•œ ì „ë¬¸ê°€ í˜¼í•© ì ‘ê·¼ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ íŠ¹ì • ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ ì§„í™”í•˜ëŠ” ë”¥í˜ì´í¬ ê³µê²©ì— ëŒ€í•œ ì ì‘ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ì˜ ë¯¸ì„¸ ì¡°ì • ë°©ì‹ë³´ë‹¤ ë„ë©”ì¸ ë‚´ì™¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, íŠ¹íˆ í‰ê·  ë„ë©”ì¸ ì™¸ EERì„ 8.55%ì—ì„œ 6.08%ë¡œ ë‚®ì¶”ì–´ ì¼ë°˜í™”ëœ ì˜¤ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€ì—ì„œì˜ íš¨ê³¼ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. Wav2Vec2ì™€ ê°™ì€ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì€ ì˜¤ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ìƒˆë¡œìš´ ë”¥í˜ì´í¬ ë°©ë²•ì— ì¼ë°˜í™”í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

- 2. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— ì—¬ëŸ¬ ì €ìˆœìœ„ ì–´ëŒ‘í„°(LoRA)ë¥¼ í†µí•©í•˜ëŠ” mixture-of-LoRA-experts ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

- 3. ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ íŠ¹í™”ëœ ì „ë¬¸ê°€ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ ì§„í™”í•˜ëŠ” ë”¥í˜ì´í¬ ê³µê²©ì— ëŒ€í•œ ì ì‘ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

- 4. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì€ í‘œì¤€ ë¯¸ì„¸ ì¡°ì •ë³´ë‹¤ ì¸ë„ë©”ì¸ ë° ì•„ì›ƒë„ë©”ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, í‰ê·  ì•„ì›ƒë„ë©”ì¸ EERì„ 8.55%ì—ì„œ 6.08%ë¡œ ë‚®ì¶”ì—ˆìŠµë‹ˆë‹¤.

- 5. MoE-LoRA ëª¨ë¸ì€ ì¼ë°˜í™” ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€ì—ì„œ íš¨ê³¼ì ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

*Generated on 2025-09-20 09:27:28*