# Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment

**Korean Title:** 대규모 다중 모달 모델의 미세 조정을 통한 자동 발음 평가

## 📋 메타데이터

## 📋 메타데이터

**Links**: [[reports/digests/daily_digest_20250922|2025-09-22]] [[keywords/specific/Fine Tuning|Fine Tuning]] [[keywords/specific/Automatic Pronunciation Assessment|Automatic Pronunciation Assessment]] [[keywords/broad/Computer Assisted Language Learning|Computer Assisted Language Learning]] [[keywords/broad/Multimodal Models|Multimodal Models]] [[categories/cs.CL|cs.CL]] [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings]] (86.8% similar) [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (86.0% similar) [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (85.5% similar)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Rank-aware Evaluation
**🔗 Specific Connectable**: Automatic Pronunciation Assessment, Fine-tuning
**🔬 Broad Technical**: Multimodal Models
## 🔗 유사한 논문
- [[2025-09-19/Adding LLMs to the psycholinguistic norming toolbox_ A practical guide to getting the most out of human ratings_20250919|Adding LLMs to the psycholinguistic norming toolbox A practical guide to getting the most out of human ratings]] (86.8% similar)
- [[2025-09-22/Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data_20250922|Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data]] (86.0% similar)
- [[2025-09-22/Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning_20250922|Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning]] (85.5% similar)
- [[2025-09-22/Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding_20250922|Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding]] (85.2% similar)
- [[2025-09-22/Predicting Language Models' Success at Zero-Shot Probabilistic Prediction_20250922|Predicting Language Models' Success at Zero-Shot Probabilistic Prediction]] (84.2% similar)


**ArXiv ID**: [2509.15701](https://arxiv.org/abs/2509.15701)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15701.pdf)


**ArXiv ID**: [2509.15701](https://arxiv.org/abs/2509.15701)
**Published**: 2025-09-22
**Category**: cs.CL
**PDF**: [Download](https://arxiv.org/pdf/2509.15701.pdf)

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Fine-grained Modeling
**🔗 Specific Connectable**: Automatic Pronunciation Assessment
**⭐ Unique Technical**: Speechocean762 Dataset
**🔬 Broad Technical**: Multimodal Models

## 🏷️ 추출된 키워드



`Multimodal Models` • 

`Automatic Pronunciation Assessment` • 

`Speechocean762 Dataset` • 

`Rank-aware Evaluation`



## 🔗 유사한 논문

Similar papers will be displayed here based on embedding similarity.

## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.15701v1 Announce Type: new 
Abstract: Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted Language Learning (CALL), requiring evaluation across multiple granularities and aspects. Large Multimodal Models (LMMs) present new opportunities for APA, but their effectiveness in fine-grained assessment remains uncertain. This work investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a private corpus. Fine-tuning significantly outperforms zero-shot settings and achieves competitive results on single-granularity tasks compared to public and commercial systems. The model performs well at word and sentence levels, while phoneme-level assessment remains challenging. We also observe that the Pearson Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects ordinal consistency. These findings highlight both the promise and limitations of LMMs for APA and point to future work on fine-grained modeling and rank-aware evaluation.

## 🔍 Abstract (한글 번역)

arXiv:2509.15701v1 발표 유형: 신규  
초록: 자동 발음 평가(APA)는 컴퓨터 보조 언어 학습(CALL)에 있어 매우 중요하며, 여러 세분성과 측면에 대한 평가가 필요합니다. 대형 멀티모달 모델(LMMs)은 APA에 새로운 기회를 제공하지만, 세밀한 평가에서의 효과는 아직 불확실합니다. 본 연구는 Speechocean762 데이터셋과 비공개 코퍼스를 사용하여 APA를 위한 LMMs의 미세 조정을 조사합니다. 미세 조정은 제로샷 설정보다 훨씬 뛰어난 성능을 보이며, 공공 및 상업 시스템과 비교하여 단일 세분성 작업에서 경쟁력 있는 결과를 달성합니다. 모델은 단어 및 문장 수준에서 잘 작동하지만, 음소 수준의 평가는 여전히 어려운 과제로 남아 있습니다. 또한, 피어슨 상관 계수(PCC)가 0.9에 도달하는 반면, 스피어만 순위 상관 계수(SCC)는 약 0.6에 머물러 있어 SCC가 서열 일관성을 더 잘 반영한다는 것을 관찰했습니다. 이러한 발견은 APA를 위한 LMMs의 가능성과 한계를 모두 강조하며, 세밀한 모델링과 순위 인식 평가에 대한 향후 연구 방향을 제시합니다.

## 📝 요약

이 논문은 컴퓨터 기반 언어 학습(CALL)에서 중요한 자동 발음 평가(APA)를 위해 대형 멀티모달 모델(LMMs)을 활용하는 방법을 연구합니다. Speechocean762 데이터셋과 비공개 코퍼스를 사용하여 LMMs를 미세 조정한 결과, 제로샷 설정보다 뛰어난 성능을 보였으며, 단일 세분화 작업에서 공공 및 상업 시스템과 경쟁력 있는 결과를 얻었습니다. 단어 및 문장 수준에서는 우수한 성능을 보였으나, 음소 수준에서는 여전히 어려움이 있었습니다. Pearson 상관 계수(PCC)는 0.9에 도달했지만, Spearman 순위 상관 계수(SCC)는 약 0.6에 머물러, SCC가 순위 일관성을 더 잘 반영함을 시사합니다. 이 연구는 LMMs의 가능성과 한계를 강조하며, 향후 세분화된 모델링과 순위 인식 평가에 대한 연구 방향을 제시합니다.

## 🎯 주요 포인트


- 1. 대규모 멀티모달 모델(LMMs)을 활용한 자동 발음 평가(APA)의 가능성과 한계를 조사했습니다.

- 2. Speechocean762 데이터셋과 개인 코퍼스를 사용한 미세 조정이 zero-shot 설정보다 성능이 뛰어났습니다.

- 3. 단일 세분화 작업에서 공공 및 상업 시스템과 경쟁력 있는 결과를 달성했습니다.

- 4. 단어 및 문장 수준에서는 좋은 성능을 보였으나, 음소 수준 평가는 여전히 어려운 과제로 남아 있습니다.

- 5. Pearson 상관 계수(PCC)는 0.9에 도달했지만, Spearman 순위 상관 계수(SCC)는 약 0.6으로, SCC가 순서 일관성을 더 잘 반영함을 시사합니다.


---

*Generated on 2025-09-22 16:26:08*