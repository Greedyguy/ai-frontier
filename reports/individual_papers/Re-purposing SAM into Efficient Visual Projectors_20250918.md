
# Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation

**Korean Title:** MLLM 기반 참조 이미지 세분화를 위한 효율적인 시각 프로젝터로 SAM을 재활용하기

## 📋 메타데이터

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Compressive Visual Projectors|Compressive Visual Projectors]] [[keywords/broad/Referring Image Segmentation|Referring Image Segmentation]] [[keywords/broad/Multimodal Large Language Model|Multimodal Large Language Model]] [[keywords/specific/Semantic Visual Projector|Semantic Visual Projector]] [[keywords/unique/Semantic Superpixel Positional Embedding|Semantic Superpixel Positional Embedding]] [[categories/cs.AI|cs.AI]]

## 🏷️ 카테고리화된 키워드
**🚀 Evolved Concepts**: Compressive Visual Projectors
**🔬 Broad Technical**: Referring Image Segmentation, Multimodal Large Language Model
**🔗 Specific Connectable**: Semantic Visual Projector
**⭐ Unique Technical**: SAM-MLLM Integration

**ArXiv ID**: [2509.13676](https://arxiv.org/abs/2509.13676)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.13676.pdf)


## 🏷️ 추출된 키워드



`Referring Image Segmentation` • 

`Multimodal Large Language Model` • 

`Semantic Visual Projector` • 

`SAM-Generated Semantic Superpixels` • 

`Compressive Visual Projectors`



## 📋 저자 정보

**Authors:** 

## 📄 Abstract (원문)

arXiv:2509.13676v1 Announce Type: cross 
Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive, primarily due to visual token redundancy. We observe that traditional patch-wise visual projectors struggle to strike a balance between reducing the number of visual tokens and preserving semantic clarity, often retaining overly long token sequences to avoid performance drops. Inspired by text tokenizers, we propose a novel semantic visual projector that leverages semantic superpixels generated by SAM to identify "visual words" in an image. By compressing and projecting semantic superpixels as visual tokens, our approach adaptively shortens the token sequence according to scene complexity while minimizing semantic loss in compression. To mitigate loss of information, we propose a semantic superpixel positional embedding to strengthen MLLM's awareness of superpixel geometry and position, alongside a semantic superpixel aggregator to preserve both fine-grained details inside superpixels and global context outside. Experiments show that our method cuts visual tokens by 93% without compromising performance, notably speeding up MLLM training and inference, and outperforming existing compressive visual projectors on RIS.

## 🔍 Abstract (한글 번역)

arXiv:2509.13676v1 발표 유형: 교차
요약: 최근에는 다중 모달 대형 언어 모델 (MLLM)을 세그멘트 어떤 것 모델 (SAM)과 짝을 이루는 참조 이미지 분할 (RIS) 프레임워크가 인상적인 결과를 얻었습니다. 그러나 세그멘테이션에 MLLM을 적응하는 것은 주로 시각 토큰의 중복으로 인해 계산적으로 많은 비용이 발생합니다. 우리는 전통적인 패치별 시각 프로젝터가 시각 토큰 수를 줄이고 의미적 명확성을 보존하는 사이에서 균형을 맞추기 어렵다는 것을 관찰했습니다. 종종 성능 하락을 피하기 위해 지나치게 긴 토큰 시퀀스를 유지하는 경향이 있습니다. 텍스트 토크나이저에서 영감을 받아 우리는 SAM에 의해 생성된 의미적 슈퍼픽셀을 활용하여 이미지에서 "시각적 단어"를 식별하는 새로운 의미적 시각 프로젝터를 제안합니다. 의미적 슈퍼픽셀을 시각적 토큰으로 압축하고 투영함으로써 우리의 접근 방식은 장면 복잡성에 따라 토큰 시퀀스를 적응적으로 줄이면서 압축에서 의미적 손실을 최소화합니다. 정보 손실을 완화하기 위해 우리는 의미적 슈퍼픽셀 위치 임베딩을 제안하여 MLLM이 슈퍼픽셀 지오메트리와 위치에 대한 인식을 강화하고, 세마틱 슈퍼픽셀 집계기를 제안하여 슈퍼픽셀 내부의 세부적인 세부 정보와 외부의 전역적 맥락을 보존합니다. 실험 결과는 우리의 방법이 시각 토큰을 93%로 줄이면서 성능을 저해하지 않으며, 특히 MLLM의 훈련 및 추론을 가속화하고 RIS에서 기존의 압축 시각 프로젝터보다 우수한 성과를 보여준다는 것을 보여줍니다.

## 📝 요약

한국어 요약:
최근 Referring Image Segmentation (RIS) 프레임워크는 Multimodal Large Language Model (MLLM)과 Segment Anything Model (SAM)을 결합하여 높은 성과를 거두었습니다. 그러나 MLLM을 세분화에 적응시키는 것은 시각 토큰의 중복으로 계산적으로 비효율적입니다. 우리는 전통적인 패치별 시각 프로젝터가 시각 토큰 수를 줄이고 의미를 보존하는 것 사이의 균형을 유지하는 데 어려움을 겪는다는 것을 관찰했습니다. 우리는 텍스트 토크나이저에서 영감을 받아 SAM에 의해 생성된 의미적 슈퍼픽셀을 활용하여 이미지에서 "시각 단어"를 식별하는 새로운 의미적 시각 프로젝터를 제안합니다. 실험 결과, 우리의 방법은 시각 토큰을 93% 줄이면서도 성능을 희생하지 않고 MLLM의 훈련과 추론을 빠르게 하며 RIS에서 기존의 압축 시각 프로젝터를 능가합니다.

## 🎯 주요 포인트


- 1. Referring Image Segmentation (RIS)에서 새로운 시맨틱 비주얼 프로젝터가 제안되었다.

- 2. 시맨틱 슈퍼픽셀을 이용하여 이미지 내 "비주얼 워드"를 식별하고 압축하여 시맨틱 손실을 최소화했다.

- 3. 제안된 방법은 시각 토큰을 93% 줄이고 성능을 유지하며 MLLM의 학습 및 추론 속도를 높였다.


---

*Generated on 2025-09-18 16:21:32*