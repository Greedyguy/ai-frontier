
# Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation

**Korean Title:** MLLM ê¸°ë°˜ ì°¸ì¡° ì´ë¯¸ì§€ ì„¸ë¶„í™”ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ ì‹œê° í”„ë¡œì í„°ë¡œ SAMì„ ì¬í™œìš©í•˜ê¸°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-18|2025-09-18]] [[keywords/evolved/Compressive Visual Projectors|Compressive Visual Projectors]] [[keywords/broad/Referring Image Segmentation|Referring Image Segmentation]] [[keywords/broad/Multimodal Large Language Model|Multimodal Large Language Model]] [[keywords/specific/Semantic Visual Projector|Semantic Visual Projector]] [[keywords/unique/Semantic Superpixel Positional Embedding|Semantic Superpixel Positional Embedding]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Compressive Visual Projectors
**ğŸ”¬ Broad Technical**: Referring Image Segmentation, Multimodal Large Language Model
**ğŸ”— Specific Connectable**: Semantic Visual Projector
**â­ Unique Technical**: SAM-MLLM Integration

**ArXiv ID**: [2509.13676](https://arxiv.org/abs/2509.13676)
**Published**: 2025-09-18
**Category**: cs.AI
**PDF**: [Download](https://arxiv.org/pdf/2509.13676.pdf)


## ğŸ·ï¸ ì¶”ì¶œëœ í‚¤ì›Œë“œ



`Referring Image Segmentation` â€¢ 

`Multimodal Large Language Model` â€¢ 

`Semantic Visual Projector` â€¢ 

`SAM-Generated Semantic Superpixels` â€¢ 

`Compressive Visual Projectors`



## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.13676v1 Announce Type: cross 
Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive, primarily due to visual token redundancy. We observe that traditional patch-wise visual projectors struggle to strike a balance between reducing the number of visual tokens and preserving semantic clarity, often retaining overly long token sequences to avoid performance drops. Inspired by text tokenizers, we propose a novel semantic visual projector that leverages semantic superpixels generated by SAM to identify "visual words" in an image. By compressing and projecting semantic superpixels as visual tokens, our approach adaptively shortens the token sequence according to scene complexity while minimizing semantic loss in compression. To mitigate loss of information, we propose a semantic superpixel positional embedding to strengthen MLLM's awareness of superpixel geometry and position, alongside a semantic superpixel aggregator to preserve both fine-grained details inside superpixels and global context outside. Experiments show that our method cuts visual tokens by 93% without compromising performance, notably speeding up MLLM training and inference, and outperforming existing compressive visual projectors on RIS.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.13676v1 ë°œí‘œ ìœ í˜•: êµì°¨
ìš”ì•½: ìµœê·¼ì—ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ (MLLM)ì„ ì„¸ê·¸ë©˜íŠ¸ ì–´ë–¤ ê²ƒ ëª¨ë¸ (SAM)ê³¼ ì§ì„ ì´ë£¨ëŠ” ì°¸ì¡° ì´ë¯¸ì§€ ë¶„í•  (RIS) í”„ë ˆì„ì›Œí¬ê°€ ì¸ìƒì ì¸ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì— MLLMì„ ì ì‘í•˜ëŠ” ê²ƒì€ ì£¼ë¡œ ì‹œê° í† í°ì˜ ì¤‘ë³µìœ¼ë¡œ ì¸í•´ ê³„ì‚°ì ìœ¼ë¡œ ë§ì€ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì „í†µì ì¸ íŒ¨ì¹˜ë³„ ì‹œê° í”„ë¡œì í„°ê°€ ì‹œê° í† í° ìˆ˜ë¥¼ ì¤„ì´ê³  ì˜ë¯¸ì  ëª…í™•ì„±ì„ ë³´ì¡´í•˜ëŠ” ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶”ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ì¢…ì¢… ì„±ëŠ¥ í•˜ë½ì„ í”¼í•˜ê¸° ìœ„í•´ ì§€ë‚˜ì¹˜ê²Œ ê¸´ í† í° ì‹œí€€ìŠ¤ë¥¼ ìœ ì§€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €ì—ì„œ ì˜ê°ì„ ë°›ì•„ ìš°ë¦¬ëŠ” SAMì— ì˜í•´ ìƒì„±ëœ ì˜ë¯¸ì  ìŠˆí¼í”½ì…€ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ "ì‹œê°ì  ë‹¨ì–´"ë¥¼ ì‹ë³„í•˜ëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ì  ì‹œê° í”„ë¡œì í„°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì˜ë¯¸ì  ìŠˆí¼í”½ì…€ì„ ì‹œê°ì  í† í°ìœ¼ë¡œ ì••ì¶•í•˜ê³  íˆ¬ì˜í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì¥ë©´ ë³µì¡ì„±ì— ë”°ë¼ í† í° ì‹œí€€ìŠ¤ë¥¼ ì ì‘ì ìœ¼ë¡œ ì¤„ì´ë©´ì„œ ì••ì¶•ì—ì„œ ì˜ë¯¸ì  ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤. ì •ë³´ ì†ì‹¤ì„ ì™„í™”í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ì˜ë¯¸ì  ìŠˆí¼í”½ì…€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì œì•ˆí•˜ì—¬ MLLMì´ ìŠˆí¼í”½ì…€ ì§€ì˜¤ë©”íŠ¸ë¦¬ì™€ ìœ„ì¹˜ì— ëŒ€í•œ ì¸ì‹ì„ ê°•í™”í•˜ê³ , ì„¸ë§ˆí‹± ìŠˆí¼í”½ì…€ ì§‘ê³„ê¸°ë¥¼ ì œì•ˆí•˜ì—¬ ìŠˆí¼í”½ì…€ ë‚´ë¶€ì˜ ì„¸ë¶€ì ì¸ ì„¸ë¶€ ì •ë³´ì™€ ì™¸ë¶€ì˜ ì „ì—­ì  ë§¥ë½ì„ ë³´ì¡´í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ì‹œê° í† í°ì„ 93%ë¡œ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ ì €í•´í•˜ì§€ ì•Šìœ¼ë©°, íŠ¹íˆ MLLMì˜ í›ˆë ¨ ë° ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê³  RISì—ì„œ ê¸°ì¡´ì˜ ì••ì¶• ì‹œê° í”„ë¡œì í„°ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

í•œêµ­ì–´ ìš”ì•½:
ìµœê·¼ Referring Image Segmentation (RIS) í”„ë ˆì„ì›Œí¬ëŠ” Multimodal Large Language Model (MLLM)ê³¼ Segment Anything Model (SAM)ì„ ê²°í•©í•˜ì—¬ ë†’ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ MLLMì„ ì„¸ë¶„í™”ì— ì ì‘ì‹œí‚¤ëŠ” ê²ƒì€ ì‹œê° í† í°ì˜ ì¤‘ë³µìœ¼ë¡œ ê³„ì‚°ì ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì „í†µì ì¸ íŒ¨ì¹˜ë³„ ì‹œê° í”„ë¡œì í„°ê°€ ì‹œê° í† í° ìˆ˜ë¥¼ ì¤„ì´ê³  ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ëŠ” ê²ƒ ì‚¬ì´ì˜ ê· í˜•ì„ ìœ ì§€í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €ì—ì„œ ì˜ê°ì„ ë°›ì•„ SAMì— ì˜í•´ ìƒì„±ëœ ì˜ë¯¸ì  ìŠˆí¼í”½ì…€ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ "ì‹œê° ë‹¨ì–´"ë¥¼ ì‹ë³„í•˜ëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ì  ì‹œê° í”„ë¡œì í„°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ìš°ë¦¬ì˜ ë°©ë²•ì€ ì‹œê° í† í°ì„ 93% ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ì„ í¬ìƒí•˜ì§€ ì•Šê³  MLLMì˜ í›ˆë ¨ê³¼ ì¶”ë¡ ì„ ë¹ ë¥´ê²Œ í•˜ë©° RISì—ì„œ ê¸°ì¡´ì˜ ì••ì¶• ì‹œê° í”„ë¡œì í„°ë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸


- 1. Referring Image Segmentation (RIS)ì—ì„œ ìƒˆë¡œìš´ ì‹œë§¨í‹± ë¹„ì£¼ì–¼ í”„ë¡œì í„°ê°€ ì œì•ˆë˜ì—ˆë‹¤.

- 2. ì‹œë§¨í‹± ìŠˆí¼í”½ì…€ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë‚´ "ë¹„ì£¼ì–¼ ì›Œë“œ"ë¥¼ ì‹ë³„í•˜ê³  ì••ì¶•í•˜ì—¬ ì‹œë§¨í‹± ì†ì‹¤ì„ ìµœì†Œí™”í–ˆë‹¤.

- 3. ì œì•ˆëœ ë°©ë²•ì€ ì‹œê° í† í°ì„ 93% ì¤„ì´ê³  ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©° MLLMì˜ í•™ìŠµ ë° ì¶”ë¡  ì†ë„ë¥¼ ë†’ì˜€ë‹¤.


---

*Generated on 2025-09-18 16:21:32*