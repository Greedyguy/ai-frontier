# Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data

**Korean Title:** ì‹ ê²½ë§ì„ í†µí•œ í•™ìŠµ ê°€ëŠ¥í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì˜í–¥ ì¶”ì •: ì§€ì‹œ ì„¸ë¶€ ì¡°ì • ë°ì´í„°

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Efficient Instruction Fine-Tuning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Toward Efficient Influence Function_ Dropout as a Compression Tool_20250922|Toward Efficient Influence Function Dropout as a Compression Tool]] (83.8% similar)
- [[2025-09-18/Pre-training under infinite compute_20250918|Pre-training under infinite compute]] (80.3% similar)
- [[2025-09-18/MetaSel_ A Test Selection Approach for Fine-tuned DNN Models_20250918|MetaSel A Test Selection Approach for Fine-tuned DNN Models]] (80.0% similar)
- [[2025-09-17/Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning_20250917|Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning]] (79.9% similar)
- [[2025-09-18/TICL_ Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models_20250918|TICL Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models]] (79.9% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2502.09969v3 Announce Type: replace-cross 
Abstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2502.09969v3 ë°œí‘œ ìœ í˜•: êµì°¨ ëŒ€ì²´  
ì´ˆë¡: ì˜í–¥ í•¨ìˆ˜ëŠ” ëª¨ë¸ í•™ìŠµì— ì¤‘ìš”í•œ í†µì°°ë ¥ì„ ì œê³µí•˜ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ë“¤ì€ í° ê³„ì‚° ë¹„ìš©ê³¼ ì œí•œëœ ì¼ë°˜í™” ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ìµœê·¼ ì—°êµ¬ë“¤ì€ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì˜í–¥ì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì§€í‘œì™€ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí–ˆì§€ë§Œ, ì´ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì— ì˜ í™•ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ê³„ì‚°ì„ ìœ„í•œ ê³ ë¹„ìš©ì˜ ìˆœë°©í–¥ ë° ì—­ë°©í–¥ ì „íŒŒ, ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìƒë‹¹í•œ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜í–¥ ì¶”ì •ì˜ ë‚®ì€ ì¼ë°˜í™” ë•Œë¬¸ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì˜í–¥ ê°’ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ 'ì˜í–¥ ë„¤íŠ¸ì›Œí¬(InfluenceNetwork)'ë¼ê³  ë¶€ë¥´ëŠ” ì†Œí˜• ì‹ ê²½ë§ì˜ ì‚¬ìš©ì„ íƒêµ¬í•˜ì—¬ ìµœëŒ€ 99%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í‰ê°€ì— ë”°ë¥´ë©´, ì˜í–¥ ê°’ì€ ì „ì²´ ì–¸ì–´ ëª¨ë¸ í¬ê¸°ì˜ ë‹¨ 0.0027%ì— ë¶ˆê³¼í•œ ëª¨ë¸ë¡œë„ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ìš°ë¦¬ëŠ” 7B ë° 8B ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤). ìš°ë¦¬ëŠ” ì¼ë°˜ì ì¸ ì§€ì‹œ ì„¸ë¶€ ì¡°ì •ì„ ìœ„í•œ í•˜ìœ„ ì§‘í•© ì„ íƒì´ë¼ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì˜í–¥ ê°’ ì¶”ì • ì•Œê³ ë¦¬ì¦˜(NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning)ì„ ì ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ì—ì„œëŠ” ë„¤ ê°€ì§€ ìµœì²¨ë‹¨ ì˜í–¥ í•¨ìˆ˜ë¥¼ í¬í•¨í•˜ë©°, NN-CIFTì™€ ì›ë˜ì˜ ì˜í–¥ í•¨ìˆ˜ ê°„ì— í° ì†ë„ í–¥ìƒì—ë„ ë¶ˆêµ¬í•˜ê³  ì„±ëŠ¥ì— íƒ€í˜‘ì´ ì—†ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” NN-CIFTì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì— ëŒ€í•œ ì½”ë“œëŠ” ë‹¤ìŒì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤: https://github.com/agarwalishika/NN-CIFT.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë°ì´í„°ì˜ ì˜í–¥ë ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì •í•˜ê¸° ìœ„í•´ ì‘ì€ ì‹ ê²½ë§ì¸ InfluenceNetworkë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ ê³„ì‚° ë¹„ìš©ì´ í¬ê³  ì¼ë°˜í™”ê°€ ì œí•œì ì´ì—ˆìœ¼ë‚˜, ì œì•ˆëœ ë°©ë²•ì€ ìµœëŒ€ 99%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. 7B ë° 8B ëª¨ë¸ í¬ê¸°ì˜ 0.0027%ì— ë¶ˆê³¼í•œ ëª¨ë¸ë¡œë„ ì˜í–¥ë ¥ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, NN-CIFT ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì¼ë°˜ì ì¸ ì§€ì‹œ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ë¶€ë¶„ ì§‘í•© ì„ íƒ ì‘ì—…ì— ì ìš©í–ˆìŠµë‹ˆë‹¤. ë„¤ ê°€ì§€ ìµœì‹  ì˜í–¥ í•¨ìˆ˜ì™€ ë¹„êµí•´ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìœ¼ë©°, NN-CIFTì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„ë„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. ê¸°ì¡´ì˜ ì˜í–¥ í•¨ìˆ˜ ê³„ì‚° ë°©ë²•ì€ í° ê³„ì‚° ë¹„ìš©ê³¼ ì¼ë°˜í™”ì˜ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤.

- 2. InfluenceNetworkë¼ëŠ” ì‘ì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì˜í–¥ ê°’ì„ ì¶”ì •í•¨ìœ¼ë¡œì¨ ìµœëŒ€ 99%ì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í–ˆë‹¤.

- 3. ì „ì²´ ì–¸ì–´ ëª¨ë¸ì˜ 0.0027% í¬ê¸°ì¸ ëª¨ë¸ë¡œë„ ì˜í–¥ ê°’ì„ ì •í™•í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.

- 4. NN-CIFT ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ì ì¸ ì§€ì‹œ ì„¸ë¶€ ì¡°ì •ì˜ í•˜ìœ„ ì§‘í•© ì„ íƒ ì‘ì—…ì— ì ìš©í–ˆìœ¼ë©° ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ë‹¤.

- 5. NN-CIFTì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•˜ë©°, ê´€ë ¨ ì½”ë“œëŠ” GitHubì—ì„œ ì œê³µëœë‹¤.

---

*Generated on 2025-09-22 14:42:57*