# Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control

**Korean Title:** ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì œì–´ë¥¼ ìœ„í•œ ì €ìˆ˜ì¤€ MPCë¥¼ í™œìš©í•œ ê³„ì¸µì  ê°•í™” í•™ìŠµ

## ğŸ“‹ ë©”íƒ€ë°ì´í„°

**Links**: [[daily/2025-09-22|2025-09-22]] [[categories/cs.AI|cs.AI]]

## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ
**ğŸš€ Evolved Concepts**: Structured Learning

## ğŸ”— ìœ ì‚¬í•œ ë…¼ë¬¸
- [[2025-09-22/Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems_20250922|Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems]] (84.9% similar)
- [[2025-09-19/Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution_20250919|Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution]] (84.7% similar)
- [[2025-09-19/Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning_20250919|Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning]] (84.6% similar)
- [[2025-09-19/Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control_20250919|Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control]] (84.3% similar)
- [[2025-09-19/ActivePusher_ Active Learning and Planning with Residual Physics for Nonprehensile Manipulation_20250919|ActivePusher Active Learning and Planning with Residual Physics for Nonprehensile Manipulation]] (84.1% similar)

## ğŸ“‹ ì €ì ì •ë³´

**Authors:** 

## ğŸ“„ Abstract (ì›ë¬¸)

arXiv:2509.15799v1 Announce Type: cross 
Abstract: Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.

## ğŸ” Abstract (í•œê¸€ ë²ˆì—­)

arXiv:2509.15799v1 ë°œí‘œ ìœ í˜•: êµì°¨  
ì´ˆë¡: ë™ì ì´ê³  ì œì•½ì´ ë§ì€ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê³  ì¡°ì •ëœ í–‰ë™ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ í•™ìŠµ ê¸°ë°˜ ì œì–´ì— ìˆì–´ ì—¬ì „íˆ ì£¼ìš” ê³¼ì œì…ë‹ˆë‹¤. ìˆœìˆ˜í•œ ì¢…ë‹¨ ê°„ í•™ìŠµì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ë‚®ê³  ì‹ ë¢°ì„±ì´ ì œí•œë˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë©°, ëª¨ë¸ ê¸°ë°˜ ë°©ë²•ì€ ì‚¬ì „ ì •ì˜ëœ ì°¸ì¡°ì— ì˜ì¡´í•˜ê³  ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê°•í™” í•™ìŠµ(RL)ì„ í†µí•œ ì „ìˆ ì  ì˜ì‚¬ ê²°ì •ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ì œì–´(MPC)ë¥¼ í†µí•œ ì €ìˆ˜ì¤€ ì‹¤í–‰ì„ ê²°í•©í•˜ëŠ” ê³„ì¸µì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ê²½ìš°, ì´ëŠ” ê³ ìˆ˜ì¤€ ì •ì±…ì´ ê´€ì‹¬ ì˜ì—­(ROIs)ì—ì„œ ì¶”ìƒì  ëª©í‘œë¥¼ ì„ íƒí•˜ëŠ” ë°˜ë©´, MPCê°€ ë™ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ì•ˆì „í•œ ì›€ì§ì„ì„ ë³´ì¥í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í¬ì‹ì-í”¼ì‹ì ë²¤ì¹˜ë§ˆí¬ì—ì„œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ë³´ìƒ, ì•ˆì „ì„± ë° ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ì¢…ë‹¨ ê°„ ë° ë³´í˜¸ ê¸°ë°˜ RL ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ì—¬ êµ¬ì¡°í™”ëœ í•™ìŠµê³¼ ëª¨ë¸ ê¸°ë°˜ ì œì–´ë¥¼ ê²°í•©í•œ ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ“ ìš”ì•½

ì´ ë…¼ë¬¸ì€ ë™ì ì´ê³  ì œì•½ì´ ë§ì€ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê³  ì¡°ì •ëœ í–‰ë™ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í•™ìŠµ ê¸°ë°˜ ì œì–´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìˆœìˆ˜ í•™ìŠµ ë°©ì‹ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ì‹ ë¢°ì„±ì—ì„œ í•œê³„ë¥¼ ë³´ì´ë©°, ëª¨ë¸ ê¸°ë°˜ ë°©ë²•ì€ ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°•í™” í•™ìŠµ(RL)ì„ í†µí•œ ì „ìˆ ì  ì˜ì‚¬ê²°ì •ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ì œì–´(MPC)ë¥¼ í†µí•œ ì €ìˆ˜ì¤€ ì‹¤í–‰ì„ ê²°í•©í•œ ê³„ì¸µì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ê²½ìš°, ê³ ìˆ˜ì¤€ ì •ì±…ì´ ê´€ì‹¬ ì˜ì—­(ROI)ì—ì„œ ì¶”ìƒì  ëª©í‘œë¥¼ ì„ íƒí•˜ê³ , MPCê°€ ë™ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•ˆì „í•œ ì›€ì§ì„ì„ ë³´ì¥í•©ë‹ˆë‹¤. í¬ì‹ì-í”¼ì‹ì ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ì—ì„œ ì´ ì ‘ê·¼ë²•ì€ ë³´ìƒ, ì•ˆì „ì„±, ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ê¸°ì¡´ì˜ RL ê¸°ë°˜ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©°, êµ¬ì¡°í™”ëœ í•™ìŠµê³¼ ëª¨ë¸ ê¸°ë°˜ ì œì–´ì˜ ê²°í•©ì´ ì£¼ëŠ” ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

- 1. í•™ìŠµ ê¸°ë°˜ ì œì–´ì—ì„œ ì•ˆì „í•˜ê³  ì¡°ì •ëœ í–‰ë™ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ì—¬ì „íˆ ì£¼ìš” ê³¼ì œë¡œ ë‚¨ì•„ìˆë‹¤.

- 2. ìˆœìˆ˜í•œ ì¢…ë‹¨ ê°„ í•™ìŠµì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ì‹ ë¢°ì„±ì´ ë¶€ì¡±í•˜ë©°, ëª¨ë¸ ê¸°ë°˜ ë°©ë²•ì€ ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤.

- 3. ê°•í™” í•™ìŠµì„ í†µí•œ ì „ìˆ ì  ì˜ì‚¬ê²°ì •ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ì œì–´ë¥¼ í†µí•œ ì €ìˆ˜ì¤€ ì‹¤í–‰ì„ ê²°í•©í•œ ê³„ì¸µì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤.

- 4. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ê²½ìš°, ê³ ìˆ˜ì¤€ ì •ì±…ì´ ê´€ì‹¬ ì˜ì—­ì—ì„œ ì¶”ìƒì  ëª©í‘œë¥¼ ì„ íƒí•˜ê³ , MPCê°€ ì—­ë™ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•ˆì „í•œ ì›€ì§ì„ì„ ë³´ì¥í•œë‹¤.

- 5. í¬ì‹ì-í”¼ì‹ì ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ì—ì„œ ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì€ ë³´ìƒ, ì•ˆì „ì„±, ì¼ê´€ì„± ì¸¡ë©´ì—ì„œ ê¸°ì¡´ì˜ RL ê¸°ë°˜ ë°©ë²•ì„ ëŠ¥ê°€í•œë‹¤.

---

*Generated on 2025-09-22 14:11:47*