"""Daily and weekly digest generation for collected papers."""

import os
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from collections import defaultdict
import re

from ..arxiv.client import ArxivPaper
from ..translation.translator import get_translator
from ..summarization.summarizer import get_summarizer


class DigestGenerator:
    """Generate daily and weekly digests from collected papers."""

    def __init__(self, translation_provider: str = "openai", summarization_provider: str = "openai"):
        self.translator = get_translator(provider=translation_provider)
        self.summarizer = get_summarizer(provider=summarization_provider)
        self.logger = logging.getLogger(__name__)

    def _parse_paper_from_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Parse paper information from markdown file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Extract title
            title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
            title = title_match.group(1) if title_match else "Unknown Title"

            # Extract Korean title
            korean_title_match = re.search(r'\*\*Korean Title:\*\* (.+)$', content, re.MULTILINE)
            korean_title = korean_title_match.group(1) if korean_title_match else title

            # Extract ArXiv ID from Links section
            arxiv_id_match = re.search(r'\*\*ArXiv ID:\*\* \[([^\]]+)\]', content)
            arxiv_id = arxiv_id_match.group(1) if arxiv_id_match else "unknown"

            # Extract published date from Links section
            published_match = re.search(r'\*\*Published:\*\* (.+)$', content, re.MULTILINE)
            published_str = published_match.group(1) if published_match else None
            published = None
            if published_str:
                try:
                    published = datetime.strptime(published_str, "%Y-%m-%d")
                except ValueError:
                    pass

            # Extract category from Links section
            category_match = re.search(r'\*\*Category:\*\* (.+)$', content, re.MULTILINE)
            category = category_match.group(1) if category_match else "unknown"

            # Extract Korean abstract
            korean_abstract_match = re.search(r'## ðŸ” Abstract \(í•œê¸€ ë²ˆì—­\)\s*\n\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            korean_abstract = korean_abstract_match.group(1).strip() if korean_abstract_match else ""

            # Extract summary
            summary_match = re.search(r'## ðŸ“ ìš”ì•½\s*\n\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""

            # Extract key points
            key_points_match = re.search(r'## ðŸŽ¯ ì£¼ìš” í¬ì¸íŠ¸\s*\n\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            key_points = []
            if key_points_match:
                key_points_text = key_points_match.group(1).strip()
                key_points = [line.strip('- ').strip() for line in key_points_text.split('\n') if line.strip().startswith('- ')]

            # Extract keywords from Links section
            links_match = re.search(r'\*\*Links\*\*: (.+)$', content, re.MULTILINE)
            keywords = []
            if links_match:
                links_text = links_match.group(1)
                keyword_matches = re.findall(r'\[\[keywords/([^\|]+)\|[^\]]+\]\]', links_text)
                keywords = [kw.strip() for kw in keyword_matches]

            return {
                'title': title,
                'korean_title': korean_title,
                'arxiv_id': arxiv_id,
                'published': published,
                'category': category,
                'korean_abstract': korean_abstract,
                'summary': summary,
                'key_points': key_points,
                'keywords': keywords,
                'file_path': str(file_path)
            }

        except Exception as e:
            self.logger.error(f"Error parsing file {file_path}: {e}")
            return None

    def _collect_papers_from_directory(self, directory: Path, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Collect papers from directory within date range."""
        papers = []

        if not directory.exists():
            self.logger.warning(f"Directory does not exist: {directory}")
            return papers

        for file_path in directory.glob("*.md"):
            paper_data = self._parse_paper_from_file(file_path)
            if paper_data and paper_data['published']:
                if start_date <= paper_data['published'] <= end_date:
                    papers.append(paper_data)

        # Sort by published date
        papers.sort(key=lambda x: x['published'])
        return papers

    def _generate_digest_summary(self, papers: List[Dict[str, Any]], digest_type: str = "daily") -> str:
        """Generate an AI summary of the papers for the digest."""
        if not papers:
            return "ì´ ê¸°ê°„ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

        # Prepare text for summarization
        papers_text = f"{digest_type} ë…¼ë¬¸ ìš”ì•½:\n\n"

        for i, paper in enumerate(papers, 1):
            papers_text += f"{i}. {paper['korean_title']} ({paper['category']})\n"
            papers_text += f"   ìš”ì•½: {paper['summary'][:200]}...\n"
            papers_text += f"   í‚¤ì›Œë“œ: {', '.join(paper['keywords'][:5])}\n\n"

        # Generate summary using AI
        try:
            prompt = f"""ë‹¤ìŒ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ {digest_type} ìš”ì•½ì„ ìž‘ì„±í•´ì£¼ì„¸ìš”:

{papers_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ì „ì²´ ê°œìš” (2-3ë¬¸ìž¥)
2. ì£¼ìš” ì—°êµ¬ ë™í–¥ (3-4ê°œ í¬ì¸íŠ¸)
3. ì£¼ëª©í•  ë§Œí•œ ë…¼ë¬¸ (2-3íŽ¸)
4. í‚¤ì›Œë“œ íŠ¸ë Œë“œ"""

            summary = self.summarizer.summarize_text(prompt)
            return summary

        except Exception as e:
            self.logger.error(f"Failed to generate AI summary: {e}")
            return f"ì´ {len(papers)}íŽ¸ì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_daily_digest(self, target_date: datetime, papers_dir: Path = None) -> str:
        """Generate daily digest for a specific date."""
        if papers_dir is None:
            papers_dir = Path("reports/individual_papers")

        # Set date range for the day
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        # Collect papers
        papers = self._collect_papers_from_directory(papers_dir, start_date, end_date)

        # Generate AI summary
        ai_summary = self._generate_digest_summary(papers, "ì¼ì¼")

        # Generate markdown content
        date_str = target_date.strftime("%Y-%m-%d")
        digest_content = f"""# ì¼ì¼ ë…¼ë¬¸ ë‹¤ì´ì œìŠ¤íŠ¸ - {date_str}

## ðŸ“Š ê°œìš”

**ìˆ˜ì§‘ ì¼ìž**: {date_str}
**ì´ ë…¼ë¬¸ ìˆ˜**: {len(papers)}íŽ¸

## ðŸ¤– AI ìš”ì•½

{ai_summary}

## ðŸ“‹ ë…¼ë¬¸ ëª©ë¡

"""

        # Add paper list
        if papers:
            for i, paper in enumerate(papers, 1):
                digest_content += f"""### {i}. {paper['korean_title']}

**ì›ì œ**: {paper['title']}
**ì¹´í…Œê³ ë¦¬**: {paper['category']}
**ArXiv ID**: [{paper['arxiv_id']}](https://arxiv.org/abs/{paper['arxiv_id']})
**í‚¤ì›Œë“œ**: {', '.join(paper['keywords'])}

**ìš”ì•½**: {paper['summary']}

**ì£¼ìš” í¬ì¸íŠ¸**:
"""
                for point in paper['key_points']:
                    digest_content += f"- {point}\n"

                digest_content += f"\n**íŒŒì¼**: [{paper['file_path'].split('/')[-1]}]({paper['file_path']})\n\n---\n\n"
        else:
            digest_content += "ì´ ë‚ ì§œì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"

        digest_content += f"*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

        return digest_content

    def generate_weekly_digest(self, target_date: datetime, papers_dir: Path = None) -> str:
        """Generate weekly digest for the week containing target_date."""
        if papers_dir is None:
            papers_dir = Path("reports/individual_papers")

        # Calculate week range (Monday to Sunday)
        days_since_monday = target_date.weekday()
        start_date = target_date - timedelta(days=days_since_monday)
        end_date = start_date + timedelta(days=6)

        # Set time ranges
        start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        # Collect papers
        papers = self._collect_papers_from_directory(papers_dir, start_date, end_date)

        # Group by date
        papers_by_date = defaultdict(list)
        for paper in papers:
            date_key = paper['published'].strftime("%Y-%m-%d")
            papers_by_date[date_key].append(paper)

        # Generate AI summary
        ai_summary = self._generate_digest_summary(papers, "ì£¼ê°„")

        # Generate markdown content
        week_str = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
        digest_content = f"""# ì£¼ê°„ ë…¼ë¬¸ ë‹¤ì´ì œìŠ¤íŠ¸ - {week_str}

## ðŸ“Š ê°œìš”

**ìˆ˜ì§‘ ê¸°ê°„**: {week_str}
**ì´ ë…¼ë¬¸ ìˆ˜**: {len(papers)}íŽ¸
**ìˆ˜ì§‘ ì¼ìˆ˜**: {len(papers_by_date)}ì¼

## ðŸ¤– AI ìš”ì•½

{ai_summary}

## ðŸ“ˆ ì¼ë³„ í†µê³„

"""

        # Add daily statistics
        for date_key in sorted(papers_by_date.keys()):
            day_papers = papers_by_date[date_key]
            categories = defaultdict(int)
            for paper in day_papers:
                categories[paper['category']] += 1

            digest_content += f"**{date_key}**: {len(day_papers)}íŽ¸"
            if categories:
                category_str = ", ".join([f"{cat}({count})" for cat, count in categories.items()])
                digest_content += f" - {category_str}"
            digest_content += "\n"

        digest_content += "\n## ðŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë…¼ë¬¸\n\n"

        # Group by category
        papers_by_category = defaultdict(list)
        for paper in papers:
            papers_by_category[paper['category']].append(paper)

        # Add papers by category
        for category in sorted(papers_by_category.keys()):
            category_papers = papers_by_category[category]
            digest_content += f"### {category} ({len(category_papers)}íŽ¸)\n\n"

            for paper in category_papers:
                digest_content += f"- **{paper['korean_title']}** ({paper['published'].strftime('%m-%d')})\n"
                digest_content += f"  *{paper['summary'][:150]}...*\n"
                digest_content += f"  [[{paper['arxiv_id']}](https://arxiv.org/abs/{paper['arxiv_id']})] "
                digest_content += f"í‚¤ì›Œë“œ: {', '.join(paper['keywords'][:3])}\n\n"

        if not papers:
            digest_content += "ì´ ì£¼ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"

        digest_content += f"*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

        return digest_content

    def save_daily_digest(self, target_date: datetime, output_dir: Path = None) -> Path:
        """Generate and save daily digest."""
        if output_dir is None:
            output_dir = Path("reports/digests")

        output_dir.mkdir(parents=True, exist_ok=True)

        # Generate content
        content = self.generate_daily_digest(target_date)

        # Save file
        filename = f"daily_digest_{target_date.strftime('%Y%m%d')}.md"
        file_path = output_dir / filename

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.info(f"Daily digest saved: {file_path}")
        return file_path

    def save_weekly_digest(self, target_date: datetime, output_dir: Path = None) -> Path:
        """Generate and save weekly digest."""
        if output_dir is None:
            output_dir = Path("reports/digests")

        output_dir.mkdir(parents=True, exist_ok=True)

        # Generate content
        content = self.generate_weekly_digest(target_date)

        # Calculate week info for filename
        days_since_monday = target_date.weekday()
        week_start = target_date - timedelta(days=days_since_monday)

        # Save file
        filename = f"weekly_digest_{week_start.strftime('%Y%m%d')}.md"
        file_path = output_dir / filename

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.info(f"Weekly digest saved: {file_path}")
        return file_path