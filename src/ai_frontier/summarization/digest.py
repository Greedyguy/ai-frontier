"""Daily and weekly digest generation for collected papers."""

import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from collections import defaultdict, Counter
import re

from ..translation.translator import get_translator
from ..summarization.summarizer import get_summarizer
from ..notification.keyword_subscription_manager import get_paper_filter_service


class DigestGenerator:
    """Generate daily and weekly digests from collected papers."""

    def __init__(self, translation_provider: str = "openai", summarization_provider: str = "openai"):
        self.translator = get_translator(provider=translation_provider)
        self.summarizer = get_summarizer(provider=summarization_provider)
        self.paper_filter_service = get_paper_filter_service()
        self.logger = logging.getLogger(__name__)

    def _parse_paper_from_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Parse paper information from markdown file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Extract title
            title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
            title = title_match.group(1) if title_match else "Unknown Title"

            # Extract Korean title
            korean_title_match = re.search(r'\*\*Korean Title:\*\* (.+)$', content, re.MULTILINE)
            korean_title = korean_title_match.group(1) if korean_title_match else title

            # Extract ArXiv ID from Links section
            arxiv_id_match = re.search(r'\*\*ArXiv ID:\*\* \[([^\]]+)\]', content)
            arxiv_id = arxiv_id_match.group(1) if arxiv_id_match else "unknown"

            # Extract published date from Links section
            published_match = re.search(r'\*\*Published\*\*: (.+)$', content, re.MULTILINE)
            published_str = published_match.group(1) if published_match else None
            published = None
            if published_str:
                try:
                    published = datetime.strptime(published_str, "%Y-%m-%d")
                except ValueError:
                    pass

            # Extract category from Links section
            category_match = re.search(r'\*\*Category\*\*: (.+)$', content, re.MULTILINE)
            category = category_match.group(1) if category_match else "unknown"

            # Extract Korean abstract
            korean_abstract_match = re.search(r'## ðŸ” Abstract \(í•œê¸€ ë²ˆì—­\)\s*\n\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            korean_abstract = korean_abstract_match.group(1).strip() if korean_abstract_match else ""

            # Extract summary
            summary_match = re.search(r'## ðŸ“ ìš”ì•½\s*\n\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""

            # Extract key points
            key_points_match = re.search(r'## ðŸŽ¯ ì£¼ìš” í¬ì¸íŠ¸\s*\n\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            key_points = []
            if key_points_match:
                key_points_text = key_points_match.group(1).strip()
                key_points = [line.strip('- ').strip() for line in key_points_text.split('\n') if line.strip().startswith('- ')]

            # Extract keywords from categorized keywords section
            keywords = []
            keywords_section_match = re.search(r'## ðŸ·ï¸ ì¹´í…Œê³ ë¦¬í™”ëœ í‚¤ì›Œë“œ\s*\n(.+?)(?=\n##|\n---|\Z)', content, re.DOTALL)
            if keywords_section_match:
                keywords_text = keywords_section_match.group(1)
                keyword_matches = re.findall(r'\[\[keywords/([^\|]+)\|[^\]]+\]\]', keywords_text)
                keywords = [kw.strip() for kw in keyword_matches]

            return {
                'title': title,
                'korean_title': korean_title,
                'arxiv_id': arxiv_id,
                'published': published,
                'category': category,
                'korean_abstract': korean_abstract,
                'summary': summary,
                'key_points': key_points,
                'keywords': keywords,
                'file_path': str(file_path)
            }

        except Exception as e:
            self.logger.error(f"Error parsing file {file_path}: {e}")
            return None

    def _collect_papers_from_directory(self, directory: Path, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Collect papers from directory within date range, including date subdirectories."""
        papers = []

        if not directory.exists():
            self.logger.warning(f"Directory does not exist: {directory}")
            return papers

        # Search in main directory and date subdirectories
        search_paths = [directory]

        # Add date-specific subdirectories that might exist
        for single_date in self._date_range(start_date, end_date):
            date_str = single_date.strftime("%Y-%m-%d")
            date_dir = directory / date_str
            if date_dir.exists():
                search_paths.append(date_dir)

        for search_path in search_paths:
            for file_path in search_path.glob("*.md"):
                paper_data = self._parse_paper_from_file(file_path)
                if paper_data and paper_data['published']:
                    if start_date <= paper_data['published'] <= end_date:
                        papers.append(paper_data)

        # Sort by published date
        papers.sort(key=lambda x: x['published'])
        return papers

    def _date_range(self, start_date: datetime, end_date: datetime):
        """Generate date range between start and end dates."""
        current_date = start_date
        while current_date <= end_date:
            yield current_date
            current_date += timedelta(days=1)

    def _analyze_papers_statistics(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze papers for statistics and trends."""
        if not papers:
            return {}

        # Category analysis
        category_counts = Counter(paper['category'] for paper in papers)

        # Keyword analysis
        all_keywords = []
        for paper in papers:
            all_keywords.extend(paper['keywords'])
        keyword_counts = Counter(all_keywords)

        # Daily distribution for weekly digest
        daily_counts = defaultdict(int)
        for paper in papers:
            date_key = paper['published'].strftime("%Y-%m-%d")
            daily_counts[date_key] += 1

        return {
            'total_papers': len(papers),
            'categories': dict(category_counts.most_common()),
            'top_keywords': dict(keyword_counts.most_common(15)),
            'daily_distribution': dict(daily_counts),
            'category_count': len(category_counts),
            'unique_keywords': len(keyword_counts)
        }

    def _generate_digest_summary(self, papers: List[Dict[str, Any]], digest_type: str = "daily") -> str:
        """Generate an AI summary focused on insights and trends."""
        if not papers:
            return "ì´ ê¸°ê°„ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

        # Get statistics
        stats = self._analyze_papers_statistics(papers)

        # Prepare detailed analysis text for AI with key points from each paper
        analysis_text = f"{digest_type} ë…¼ë¬¸ ë¶„ì„ ë°ì´í„°:\n\n"
        analysis_text += f"ì „ì²´ ë…¼ë¬¸ ìˆ˜: {stats['total_papers']}íŽ¸\n"
        analysis_text += f"ì—°êµ¬ ë¶„ì•¼ ìˆ˜: {stats['category_count']}ê°œ\n"
        analysis_text += f"ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {stats['unique_keywords']}ê°œ\n\n"

        analysis_text += "ì£¼ìš” ì—°êµ¬ ë¶„ì•¼:\n"
        for cat, count in list(stats['categories'].items())[:5]:
            analysis_text += f"- {cat}: {count}íŽ¸\n"

        analysis_text += "\nì£¼ìš” í‚¤ì›Œë“œ:\n"
        for keyword, count in list(stats['top_keywords'].items())[:10]:
            analysis_text += f"- {keyword}: {count}íšŒ\n"

        # Group papers by category for detailed analysis
        analysis_text += "\nì¹´í…Œê³ ë¦¬ë³„ ë…¼ë¬¸ ì£¼ìš” í¬ì¸íŠ¸ ë¶„ì„:\n\n"
        papers_by_category = defaultdict(list)
        for paper in papers:
            papers_by_category[paper['category']].append(paper)

        for category, category_papers in papers_by_category.items():
            analysis_text += f"=== {category} ({len(category_papers)}íŽ¸) ===\n"
            for paper in category_papers[:8]:  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 8íŽ¸ê¹Œì§€
                analysis_text += f"ë…¼ë¬¸: {paper['korean_title']}\n"
                analysis_text += f"ì£¼ìš” í¬ì¸íŠ¸: {', '.join(paper['key_points'][:3])}\n"
                analysis_text += f"ìš”ì•½: {paper['summary'][:200]}...\n"
                analysis_text += f"í‚¤ì›Œë“œ: {', '.join(paper['keywords'][:5])}\n\n"

        # Generate comprehensive AI analysis
        try:
            prompt = f"""ë‹¤ìŒ {digest_type} ë…¼ë¬¸ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ìˆ  ë°œì „ê³¼ ìƒˆë¡œìš´ ê¸°ìˆ  ë“±ìž¥ ê´€ì ì—ì„œ ê¹Šì´ ìžˆê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{analysis_text}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ìƒì„¸í•œ ê¸°ìˆ  ë°œì „ ì¤‘ì‹¬ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”:

## ðŸš€ ê¸°ìˆ  ë°œì „ ë° í˜ì‹  ë™í–¥
ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°œë³„ ë…¼ë¬¸ë“¤ì˜ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬:
- ìƒˆë¡­ê²Œ ë“±ìž¥í•˜ëŠ” ê¸°ìˆ ì´ë‚˜ ë°©ë²•ë¡ ì˜ íŠ¹ì§•
- ê¸°ì¡´ ê¸°ìˆ ì˜ ê°œì„ ì ì´ë‚˜ í•œê³„ ê·¹ë³µ ë°©ì•ˆ
- ê¸°ìˆ  ê°„ ìœµí•©ì´ë‚˜ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì˜ ë“±ìž¥

## ðŸ”¬ ì¹´í…Œê³ ë¦¬ë³„ ê¸°ìˆ  ì§„í™” ë¶„ì„
ê° ì—°êµ¬ ë¶„ì•¼ë³„ë¡œ:
- ì£¼ìš” ê¸°ìˆ ì  ë¸Œë ˆì´í¬ìŠ¤ë£¨ë‚˜ ê°œì„ ì‚¬í•­
- ì‹¤ìš©í™” ê°€ëŠ¥ì„±ì´ ë†’ì€ ìƒˆë¡œìš´ ê¸°ìˆ 
- ê¸°ì¡´ íŒ¨ëŸ¬ë‹¤ìž„ì˜ ë³€í™”ë‚˜ ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥

## ðŸ’¡ í˜ì‹ ì  ì—°êµ¬ í•˜ì´ë¼ì´íŠ¸
- ê¸°ìˆ ì ìœ¼ë¡œ ê°€ìž¥ í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì„ ë³´ì¸ ì—°êµ¬ë“¤
- ì‚°ì—…ê³„ì— ì‹¤ì§ˆì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìžˆëŠ” ê¸°ìˆ ë“¤
- í–¥í›„ ì—°êµ¬ íŠ¸ë Œë“œë¥¼ ì£¼ë„í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ê¸°ìˆ ë“¤

ê° ë…¼ë¬¸ì˜ êµ¬ì²´ì ì¸ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ ê·¼ê±°ë¡œ ê¸°ìˆ  ë°œì „ ê´€ì ì—ì„œ ì‹¬ë„ ìžˆëŠ” ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            # AI ìš”ì•½ ìƒì„± (í† í° ìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ ì™„ì „í•œ ì‘ë‹µ ìƒì„±)
            summary = self.summarizer.summarize_text(prompt, max_tokens=2500)
            return summary

        except Exception as e:
            self.logger.error(f"Failed to generate AI summary: {e}")
            return f"ì´ {len(papers)}íŽ¸ì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_daily_digest(self, target_date: datetime, papers_dir: Path = None) -> str:
        """Generate daily digest focused on insights and trends."""
        if papers_dir is None:
            papers_dir = Path("reports/individual_papers")

        # Set date range for the day
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        # Collect papers
        papers = self._collect_papers_from_directory(papers_dir, start_date, end_date)

        # Get statistics
        stats = self._analyze_papers_statistics(papers)

        # Generate AI summary
        ai_summary = self._generate_digest_summary(papers, "ì¼ì¼")

        # Generate markdown content
        date_str = target_date.strftime("%Y-%m-%d")
        digest_content = f"""# ì¼ì¼ ë…¼ë¬¸ ë‹¤ì´ì œìŠ¤íŠ¸ - {date_str}

## ðŸ“Š ìˆ˜ì§‘ í†µê³„

**ìˆ˜ì§‘ ì¼ìž**: {date_str}
**ì´ ë…¼ë¬¸ ìˆ˜**: {len(papers)}íŽ¸
**ì—°êµ¬ ë¶„ì•¼ ìˆ˜**: {stats.get('category_count', 0)}ê°œ
**ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜**: {stats.get('unique_keywords', 0)}ê°œ

## ðŸ“ˆ ë¶„ì•¼ë³„ ë¶„í¬

"""

        if papers:
            # Add category distribution
            for category, count in list(stats['categories'].items())[:10]:
                percentage = (count / len(papers)) * 100
                digest_content += f"- **{category}**: {count}íŽ¸ ({percentage:.1f}%)\n"

            digest_content += f"\n## ðŸ”– ì£¼ìš” í‚¤ì›Œë“œ íŠ¸ë Œë“œ\n\n"

            # Add top keywords
            for keyword, count in list(stats['top_keywords'].items())[:15]:
                digest_content += f"- **{keyword}**: {count}íšŒ\n"

            digest_content += f"\n{ai_summary}\n\n"

            digest_content += f"## ðŸ“‹ ê°œë³„ ë…¼ë¬¸ ëª©ë¡\n\n"

            # Simple list with title and PDF link only
            for paper in papers:
                digest_content += f"- [{paper['korean_title']}](https://arxiv.org/pdf/{paper['arxiv_id']}.pdf)\n"

        else:
            digest_content += "ì´ ë‚ ì§œì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"

        digest_content += f"\n---\n*ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

        return digest_content

    def generate_weekly_digest(self, target_date: datetime, papers_dir: Path = None) -> str:
        """Generate weekly digest for the week containing target_date."""
        if papers_dir is None:
            papers_dir = Path("reports/individual_papers")

        # Calculate week range (Monday to Sunday)
        days_since_monday = target_date.weekday()
        start_date = target_date - timedelta(days=days_since_monday)
        end_date = start_date + timedelta(days=6)

        # Set time ranges
        start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        # Collect papers
        papers = self._collect_papers_from_directory(papers_dir, start_date, end_date)

        # Get statistics
        stats = self._analyze_papers_statistics(papers)

        # Group by date
        papers_by_date = defaultdict(list)
        for paper in papers:
            date_key = paper['published'].strftime("%Y-%m-%d")
            papers_by_date[date_key].append(paper)

        # Generate AI summary
        ai_summary = self._generate_digest_summary(papers, "ì£¼ê°„")

        # Generate markdown content
        week_str = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
        digest_content = f"""# ì£¼ê°„ ë…¼ë¬¸ ë‹¤ì´ì œìŠ¤íŠ¸ - {week_str}

## ðŸ“Š ìˆ˜ì§‘ í†µê³„

**ìˆ˜ì§‘ ê¸°ê°„**: {week_str}
**ì´ ë…¼ë¬¸ ìˆ˜**: {len(papers)}íŽ¸
**ìˆ˜ì§‘ ì¼ìˆ˜**: {len(papers_by_date)}ì¼
**ì—°êµ¬ ë¶„ì•¼ ìˆ˜**: {stats.get('category_count', 0)}ê°œ
**ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜**: {stats.get('unique_keywords', 0)}ê°œ

## ðŸ“ˆ ë¶„ì•¼ë³„ ë¶„í¬

"""

        if papers:
            # Add category distribution
            for category, count in list(stats['categories'].items())[:10]:
                percentage = (count / len(papers)) * 100
                digest_content += f"- **{category}**: {count}íŽ¸ ({percentage:.1f}%)\n"

            digest_content += f"\n## ðŸ”– ì£¼ìš” í‚¤ì›Œë“œ íŠ¸ë Œë“œ\n\n"

            # Add top keywords
            for keyword, count in list(stats['top_keywords'].items())[:20]:
                digest_content += f"- **{keyword}**: {count}íšŒ\n"

            digest_content += f"\n{ai_summary}\n\n"

            digest_content += f"## ðŸ“ˆ ì¼ë³„ ë¶„í¬\n\n"

            # Add daily statistics
            for date_key in sorted(papers_by_date.keys()):
                day_papers = papers_by_date[date_key]
                categories = defaultdict(int)
                for paper in day_papers:
                    categories[paper['category']] += 1

                digest_content += f"**{date_key}**: {len(day_papers)}íŽ¸"
                if categories:
                    category_str = ", ".join([f"{cat}({count})" for cat, count in categories.items()])
                    digest_content += f" - {category_str}"
                digest_content += "\n"

            digest_content += f"\n## ðŸ“‹ ê°œë³„ ë…¼ë¬¸ ëª©ë¡\n\n"

            # Simple list with title and PDF link only
            for paper in papers:
                digest_content += f"- [{paper['korean_title']}](https://arxiv.org/pdf/{paper['arxiv_id']}.pdf)\n"
            digest_content += f"\n"

        else:
            digest_content += "ì´ ì£¼ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"

        digest_content += f"---\n*ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

        return digest_content

    def generate_filtered_digest(self, target_date: datetime, keywords: List[str], digest_type: str = "daily", papers_dir: Path = None) -> str:
        """Generate a keyword-filtered digest for personalized subscriptions."""
        if papers_dir is None:
            papers_dir = Path("reports/individual_papers")

        # Set date range based on digest type
        if digest_type == "daily":
            start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        else:  # weekly
            days_since_monday = target_date.weekday()
            start_date = target_date - timedelta(days=days_since_monday)
            end_date = start_date + timedelta(days=6)
            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        # Collect all papers for the period
        all_papers = self._collect_papers_from_directory(papers_dir, start_date, end_date)

        # Filter papers by keywords
        filtered_papers = self.paper_filter_service.filter_papers_by_keywords(all_papers, keywords)

        # Get statistics
        stats = self._analyze_papers_statistics(filtered_papers)

        # Generate AI summary for filtered papers
        ai_summary = self._generate_digest_summary(filtered_papers, digest_type)

        # Generate personalized digest content
        date_str = target_date.strftime("%Y-%m-%d")
        keywords_str = ", ".join(keywords) if keywords else "ëª¨ë“  í‚¤ì›Œë“œ"

        if digest_type == "daily":
            digest_content = f"""# ðŸŽ¯ ë§žì¶¤í˜• ì¼ì¼ ë…¼ë¬¸ ë‹¤ì´ì œìŠ¤íŠ¸ - {date_str}

## ðŸ“Œ ê°œì¸í™” ì„¤ì •

**êµ¬ë… í‚¤ì›Œë“œ**: {keywords_str}
**ìˆ˜ì§‘ ì¼ìž**: {date_str}
**ì´ ë§¤ì¹­ ë…¼ë¬¸**: {len(filtered_papers)}íŽ¸ (ì „ì²´ {len(all_papers)}íŽ¸ ì¤‘)
**ë§¤ì¹­ë¥ **: {(len(filtered_papers) / len(all_papers) * 100) if all_papers else 0:.1f}%
**ì—°êµ¬ ë¶„ì•¼ ìˆ˜**: {stats.get('category_count', 0)}ê°œ
**ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜**: {stats.get('unique_keywords', 0)}ê°œ

"""
        else:  # weekly
            week_str = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
            digest_content = f"""# ðŸŽ¯ ë§žì¶¤í˜• ì£¼ê°„ ë…¼ë¬¸ ë‹¤ì´ì œìŠ¤íŠ¸ - {week_str}

## ðŸ“Œ ê°œì¸í™” ì„¤ì •

**êµ¬ë… í‚¤ì›Œë“œ**: {keywords_str}
**ìˆ˜ì§‘ ê¸°ê°„**: {week_str}
**ì´ ë§¤ì¹­ ë…¼ë¬¸**: {len(filtered_papers)}íŽ¸ (ì „ì²´ {len(all_papers)}íŽ¸ ì¤‘)
**ë§¤ì¹­ë¥ **: {(len(filtered_papers) / len(all_papers) * 100) if all_papers else 0:.1f}%
**ì—°êµ¬ ë¶„ì•¼ ìˆ˜**: {stats.get('category_count', 0)}ê°œ
**ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜**: {stats.get('unique_keywords', 0)}ê°œ

"""

        if filtered_papers:
            # Add category distribution
            digest_content += f"## ðŸ“ˆ ë¶„ì•¼ë³„ ë¶„í¬\n\n"
            for category, count in list(stats['categories'].items())[:10]:
                percentage = (count / len(filtered_papers)) * 100
                digest_content += f"- **{category}**: {count}íŽ¸ ({percentage:.1f}%)\n"

            digest_content += f"\n## ðŸ”– ì£¼ìš” í‚¤ì›Œë“œ íŠ¸ë Œë“œ\n\n"

            # Add top keywords
            for keyword, count in list(stats['top_keywords'].items())[:15]:
                digest_content += f"- **{keyword}**: {count}íšŒ\n"

            digest_content += f"\n{ai_summary}\n\n"

            if digest_type == "weekly":
                # Group by date for weekly digest
                papers_by_date = defaultdict(list)
                for paper in filtered_papers:
                    date_key = paper['published'].strftime("%Y-%m-%d")
                    papers_by_date[date_key].append(paper)

                digest_content += f"## ðŸ“ˆ ì¼ë³„ ë¶„í¬\n\n"
                for date_key in sorted(papers_by_date.keys()):
                    day_papers = papers_by_date[date_key]
                    categories = defaultdict(int)
                    for paper in day_papers:
                        categories[paper['category']] += 1

                    digest_content += f"**{date_key}**: {len(day_papers)}íŽ¸"
                    if categories:
                        category_str = ", ".join([f"{cat}({count})" for cat, count in categories.items()])
                        digest_content += f" - {category_str}"
                    digest_content += "\n"

            digest_content += f"\n## ðŸ“‹ ê°œë³„ ë…¼ë¬¸ ëª©ë¡\n\n"

            # Simple list with title and PDF link only
            for paper in filtered_papers:
                digest_content += f"- [{paper['korean_title']}](https://arxiv.org/pdf/{paper['arxiv_id']}.pdf)\n"

        else:
            digest_content += f"""## âš ï¸ ë§¤ì¹­ ê²°ê³¼ ì—†ìŒ

ì´ ê¸°ê°„ì— ê·€í•˜ì˜ í‚¤ì›Œë“œ **"{keywords_str}"**ì™€ ë§¤ì¹­ë˜ëŠ” ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.

### ðŸ’¡ í‚¤ì›Œë“œ ì œì•ˆ
- ë” ë„“ì€ ë²”ìœ„ì˜ í‚¤ì›Œë“œ ì‚¬ìš©ì„ ê³ ë ¤í•´ë³´ì„¸ìš”
- ë™ì˜ì–´ë‚˜ ê´€ë ¨ ìš©ì–´ ì¶”ê°€ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”
- í‚¤ì›Œë“œë¥¼ ì¤„ì´ê±°ë‚˜ ë” ì¼ë°˜ì ì¸ ìš©ì–´ë¡œ ë³€ê²½í•´ë³´ì„¸ìš”

ì „ì²´ {len(all_papers)}íŽ¸ì˜ ë…¼ë¬¸ì´ ì´ ê¸°ê°„ì— ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.

"""

        digest_content += f"\n---\n*ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        digest_content += f"*ê°œì¸í™” í•„í„° ì ìš©: í‚¤ì›Œë“œ ê¸°ë°˜ ë§žì¶¤í˜• ë‹¤ì´ì œìŠ¤íŠ¸*\n"

        return digest_content

    def save_daily_digest(self, target_date: datetime, output_dir: Path = None) -> Path:
        """Generate and save daily digest."""
        if output_dir is None:
            output_dir = Path("reports/digests")

        output_dir.mkdir(parents=True, exist_ok=True)

        # Generate content
        content = self.generate_daily_digest(target_date)

        # Save file
        filename = f"daily_digest_{target_date.strftime('%Y%m%d')}.md"
        file_path = output_dir / filename

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.info(f"Daily digest saved: {file_path}")

        # Send notifications if configured
        try:
            from ..notification.notification_manager import NotificationManager
            notification_manager = NotificationManager()
            notification_results = notification_manager.send_digest_notifications(
                digest_file_path=file_path,
                digest_type="daily",
                digest_date=target_date.strftime("%Y-%m-%d")
            )
            self.logger.info(f"Daily digest notifications sent: {notification_results}")
        except Exception as e:
            self.logger.warning(f"Failed to send daily digest notifications: {e}")

        return file_path

    def save_weekly_digest(self, target_date: datetime, output_dir: Path = None) -> Path:
        """Generate and save weekly digest."""
        if output_dir is None:
            output_dir = Path("reports/digests")

        output_dir.mkdir(parents=True, exist_ok=True)

        # Generate content
        content = self.generate_weekly_digest(target_date)

        # Calculate week info for filename
        days_since_monday = target_date.weekday()
        week_start = target_date - timedelta(days=days_since_monday)

        # Save file
        filename = f"weekly_digest_{week_start.strftime('%Y%m%d')}.md"
        file_path = output_dir / filename

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.info(f"Weekly digest saved: {file_path}")

        # Send notifications if configured
        try:
            from ..notification.notification_manager import NotificationManager
            notification_manager = NotificationManager()
            notification_results = notification_manager.send_digest_notifications(
                digest_file_path=file_path,
                digest_type="weekly",
                digest_date=target_date.strftime("%Y-%m-%d")
            )
            self.logger.info(f"Weekly digest notifications sent: {notification_results}")
        except Exception as e:
            self.logger.warning(f"Failed to send weekly digest notifications: {e}")

        return file_path