"""FastAPI server for AI Frontier service."""

import asyncio
import uuid
import time
import logging
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any
from datetime import datetime
from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import uvicorn

# Configure logging for API server
def setup_api_logging():
    """Configure logging for API server."""
    from pathlib import Path
    
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # API specific log file
    api_handler = logging.FileHandler(
        log_dir / f"api_server_{time.strftime('%Y%m%d')}.log",
        encoding='utf-8'
    )
    api_handler.setLevel(logging.INFO)
    api_handler.setFormatter(formatter)
    
    # Get or create logger
    api_logger = logging.getLogger("api_server")
    api_logger.setLevel(logging.INFO)
    api_logger.addHandler(api_handler)
    
    return api_logger

# Initialize API logger
api_logger = setup_api_logging()

from .models import (
    PaperCollectionRequest,
    CollectionStatus,
    CollectionProgress,
    CollectionResult,
    PaperInfo,
    DigestRequest,
    DigestInfo,
    DigestResult,
    EmailSubscribeRequest,
    EmailUnsubscribeRequest,
    NotificationSettingsRequest,
    NotificationTestRequest,
    NotificationStatus,
    NotificationResult,
    MailingListResponse,
    KeywordSubscribeRequest,
    KeywordSubscriptionInfo,
    KeywordSubscriptionsResponse,
    KeywordSubscriptionResponse,
    PaperSearchRequest,
    SimilarPapersRequest,
    PaperSearchResponse,
    SimilarPapersResponse,
    DatabaseStatsResponse,
    HybridSearchRequest,
    HybridSearchResponse,
    HybridStatsResponse
)
from ..main import ArxivReportingService
from .webhook import router as webhook_router
from ..summarization.digest import DigestGenerator
from ..notification.notification_manager import NotificationManager
from ..notification.keyword_subscription_manager import KeywordSubscriptionManager, PaperFilterService
from ..search.rag_service import RAGSearchService
from ..search.hybrid_service import HybridSearchService

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI Frontier API",
    description="ArXiv ë…¼ë¬¸ ìë™ ìˆ˜ì§‘ ë° ë²ˆì—­ ì„œë¹„ìŠ¤ API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:3001", "http://127.0.0.1:3001", "http://localhost:3002", "http://127.0.0.1:3002", "http://localhost:3003", "http://127.0.0.1:3003", "http://localhost:3004", "http://127.0.0.1:3004", "http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include webhook router
app.include_router(webhook_router)

# Thread pool executor for background tasks
executor = ThreadPoolExecutor(max_workers=2)

# ì „ì—­ ìƒíƒœ ê´€ë¦¬
collection_tasks: Dict[str, CollectionStatus] = {}

# ì•Œë¦¼ ê´€ë¦¬ì ì´ˆê¸°í™”
notification_manager = NotificationManager()
keyword_subscription_manager = KeywordSubscriptionManager()
paper_filter_service = PaperFilterService()

# RAG ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
try:
    rag_service = RAGSearchService()
    api_logger.info("RAG search service initialized successfully")
except Exception as e:
    api_logger.warning(f"Failed to initialize RAG search service: {e}")
    rag_service = None

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
try:
    hybrid_service = HybridSearchService()
    api_logger.info("Hybrid search service initialized successfully")
except Exception as e:
    api_logger.warning(f"Failed to initialize hybrid search service: {e}")
    hybrid_service = None


class CollectionManager:
    """ë…¼ë¬¸ ìˆ˜ì§‘ ì‘ì—… ê´€ë¦¬ì"""

    def __init__(self):
        self.tasks = collection_tasks
        self.cancellation_flags = {}  # ì‘ì—… ì¤‘ë‹¨ í”Œë˜ê·¸

    def create_task(self, request: PaperCollectionRequest) -> str:
        """ìƒˆë¡œìš´ ìˆ˜ì§‘ ì‘ì—… ìƒì„±"""
        task_id = str(uuid.uuid4())

        status = CollectionStatus(
            task_id=task_id,
            status="created",
            progress=CollectionProgress(
                status="created",
                current_step="ì‘ì—…ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
                papers_found=0,
                papers_processed=0,
                total_papers=0,
                progress_percentage=0
            )
        )

        self.tasks[task_id] = status
        self.cancellation_flags[task_id] = False  # ì¤‘ë‹¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
        return task_id

    def update_progress(self, task_id: str, progress: CollectionProgress):
        """ì‘ì—… ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        if task_id in self.tasks:
            self.tasks[task_id].progress = progress
            self.tasks[task_id].status = progress.status

    def complete_task(self, task_id: str, result: CollectionResult):
        """ì‘ì—… ì™„ë£Œ ì²˜ë¦¬"""
        if task_id in self.tasks:
            self.tasks[task_id].result = result
            self.tasks[task_id].status = "completed" if result.success else "error"

    def get_task(self, task_id: str) -> CollectionStatus:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        return self.tasks.get(task_id)
    
    def cancel_task(self, task_id: str) -> bool:
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        if task_id in self.cancellation_flags:
            self.cancellation_flags[task_id] = True
            api_logger.info(f"ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ë¨ - Task ID: {task_id}")
            return True
        return False
    
    def is_cancelled(self, task_id: str) -> bool:
        """ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.cancellation_flags.get(task_id, False)


# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
manager = CollectionManager()


def collect_papers_sync(task_id: str, request: PaperCollectionRequest):
    """Thread poolì—ì„œ ì‹¤í–‰í•  ë™ê¸° í•¨ìˆ˜"""
    # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ)
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        return loop.run_until_complete(collect_papers_background(task_id, request))
    finally:
        loop.close()


async def collect_papers_background(task_id: str, request: PaperCollectionRequest):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹¤í–‰"""
    start_time = time.time()
    
    api_logger.info(f"=== ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ - Task ID: {task_id} ===")
    api_logger.info(f"ìš”ì²­ ì„¤ì •: {request.dict()}")

    try:
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸: ì‹œì‘
        manager.update_progress(task_id, CollectionProgress(
            status="initializing",
            current_step="ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            progress_percentage=5
        ))

        api_logger.info(f"ArxivReportingService ì´ˆê¸°í™” ì¤‘ - Task: {task_id}")
        # ArxivReportingService ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        search_method = "RSS feeds" if request.use_rss else "ArXiv API"
        api_logger.info(f"ê²€ìƒ‰ ë°©ì‹: {search_method}")
        service = ArxivReportingService(
            translation_provider=request.translation_provider,
            summarization_provider=request.summarization_provider,
            keyword_provider="openai",  # í‚¤ì›Œë“œ ì¶”ì¶œì€ í˜„ì¬ OpenAIë§Œ ì§€ì›
            output_dir="reports",
            use_rss=request.use_rss
        )

        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸: ê²€ìƒ‰ ì‹œì‘
        manager.update_progress(task_id, CollectionProgress(
            status="searching",
            current_step="ArXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            progress_percentage=10
        ))

        api_logger.info(f"ë…¼ë¬¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ - Task: {task_id}")
        api_logger.info(f"ìˆ˜ì§‘ íŒŒë¼ë¯¸í„° ìƒì„¸:")
        api_logger.info(f"  - categories: {request.categories}")
        api_logger.info(f"  - days_back: {request.days_back}")
        api_logger.info(f"  - keywords: {request.keywords}")
        api_logger.info(f"  - search_mode: {request.search_mode}")
        api_logger.info(f"  - start_date: {request.start_date}")
        api_logger.info(f"  - end_date: {request.end_date}")
        api_logger.info(f"  - keyword_only: {request.search_mode == 'keyword-only'}")
        
        # ì¤‘ë‹¨ í”Œë˜ê·¸ë¥¼ í™•ì¸í•˜ëŠ” ì½œë°± í•¨ìˆ˜
        def progress_callback_with_cancellation(progress):
            # ì¤‘ë‹¨ ìš”ì²­ì´ ìˆëŠ”ì§€ í™•ì¸
            if manager.is_cancelled(task_id):
                api_logger.info(f"ì‘ì—… ì¤‘ë‹¨ ê°ì§€ - Task ID: {task_id}")
                raise Exception("ì‘ì—…ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            manager.update_progress(task_id, progress)
        
        # ì‹¤ì œ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹¤í–‰
        report_result = await service.process_papers(
            categories=request.categories if request.categories else None,
            days_back=request.days_back,
            keywords=request.keywords if request.keywords else None,
            keyword_only=(request.search_mode == "keyword-only"),
            start_date=request.start_date,
            end_date=request.end_date,
            target_language=request.target_language,
            progress_callback=progress_callback_with_cancellation
        )

        # ë°˜í™˜ê°’ ì²˜ë¦¬ (íŠœí”Œ ë˜ëŠ” ë‹¨ì¼ ê°’)
        if isinstance(report_result, tuple):
            report_path, papers_collected = report_result
        else:
            # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
            report_path = report_result
            papers_collected = 0

        processing_time = time.time() - start_time
        api_logger.info(f"ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ - Task: {task_id}, ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ, ìˆ˜ì§‘ëœ ë…¼ë¬¸: {papers_collected}ê°œ, ë³´ê³ ì„œ: {report_path}")

        # ì„±ê³µ ê²°ê³¼ ì €ì¥
        result = CollectionResult(
            success=True,
            message="ë…¼ë¬¸ ìˆ˜ì§‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            papers_collected=papers_collected,  # ì‹¤ì œ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜
            report_path=report_path,
            processing_time=processing_time
        )

        manager.complete_task(task_id, result)

    except Exception as e:
        processing_time = time.time() - start_time
        error_message = str(e)
        
        # ì¤‘ë‹¨ëœ ê²½ìš°ì™€ ì‹¤ì œ ì˜¤ë¥˜ë¥¼ êµ¬ë¶„
        if "ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤" in error_message:
            api_logger.info(f"ì‘ì—… ì¤‘ë‹¨ ì™„ë£Œ - Task: {task_id}, ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            # ì¤‘ë‹¨ ê²°ê³¼ ì €ì¥
            result = CollectionResult(
                success=False,
                message="ì‘ì—…ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                papers_collected=0,
                processing_time=processing_time,
                error_details="User cancelled"
            )
            manager.complete_task(task_id, result)
            
            # ì¤‘ë‹¨ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            manager.update_progress(task_id, CollectionProgress(
                status="cancelled",
                current_step="ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                progress_percentage=100
            ))
        else:
            api_logger.error(f"ë…¼ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ - Task: {task_id}, ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ, ì˜¤ë¥˜: {error_message}")

            # ì‹¤íŒ¨ ê²°ê³¼ ì €ì¥
            result = CollectionResult(
                success=False,
                message="ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                papers_collected=0,
                processing_time=processing_time,
                error_details=error_message
            )

            manager.complete_task(task_id, result)

            # ì—ëŸ¬ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            manager.update_progress(task_id, CollectionProgress(
                status="error",
                current_step="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                progress_percentage=100,
                error_message=error_message
            ))


@app.get("/")
async def root():
    """API ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI Frontier API Server",
        "version": "1.0.0",
        "status": "running"
    }


@app.post("/api/collect", response_model=Dict[str, str])
async def start_collection(
    request: PaperCollectionRequest,
    client_request: Request
):
    """ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘"""
    client_ip = client_request.client.host
    api_logger.info(f"ë…¼ë¬¸ ìˆ˜ì§‘ ìš”ì²­ ì‹œì‘ - IP: {client_ip}")
    api_logger.info(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {request.dict()}")
    
    try:
        # ìƒˆ ì‘ì—… ìƒì„±
        task_id = manager.create_task(request)
        api_logger.info(f"ìƒˆ ì‘ì—… ìƒì„± ì™„ë£Œ - Task ID: {task_id}")

        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ (Thread Poolì—ì„œ ì‹¤í–‰)
        executor.submit(collect_papers_sync, task_id, request)
        api_logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ë¨ (Thread Pool) - Task ID: {task_id}")

        return {"task_id": task_id, "message": "ë…¼ë¬¸ ìˆ˜ì§‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}

    except Exception as e:
        api_logger.error(f"ìˆ˜ì§‘ ì‹œì‘ ì‹¤íŒ¨ - IP: {client_ip}, ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìˆ˜ì§‘ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")


@app.get("/api/status/{task_id}", response_model=CollectionStatus)
async def get_collection_status(task_id: str, client_request: Request):
    """ë…¼ë¬¸ ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ"""
    client_ip = client_request.client.host
    
    status = manager.get_task(task_id)
    if not status:
        api_logger.warning(f"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ - Task ID: {task_id}, IP: {client_ip} (ì‘ì—… ì—†ìŒ)")
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ìƒíƒœ ë³€ê²½ì´ ìˆì„ ë•Œë§Œ ë¡œê¹… (ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€)
    if hasattr(get_collection_status, '_last_status'):
        if get_collection_status._last_status.get(task_id) != status.status:
            api_logger.info(f"ì‘ì—… ìƒíƒœ ë³€ê²½ - Task ID: {task_id}, ìƒíƒœ: {status.status}, ì§„í–‰ë¥ : {status.progress.progress_percentage if status.progress else 0}%")
            get_collection_status._last_status[task_id] = status.status
    else:
        get_collection_status._last_status = {task_id: status.status}
        api_logger.info(f"ì‘ì—… ìƒíƒœ ì¡°íšŒ - Task ID: {task_id}, ìƒíƒœ: {status.status}")

    return status


@app.get("/api/tasks", response_model=Dict[str, Any])
async def get_all_tasks():
    """ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    return {
        "total_tasks": len(collection_tasks),
        "tasks": list(collection_tasks.keys()),
        "active_tasks": [
            task_id for task_id, status in collection_tasks.items()
            if status.status in ["created", "searching", "collecting", "processing"]
        ]
    }


@app.get("/api/download/{task_id}")
async def download_report(task_id: str):
    """ìƒì„±ëœ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ"""
    status = manager.get_task(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not status.result or not status.result.success:
        raise HTTPException(status_code=400, detail="ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    if not status.result.report_path:
        raise HTTPException(status_code=404, detail="ë³´ê³ ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return FileResponse(
        path=status.result.report_path,
        filename=f"arxiv_report_{task_id[:8]}.md",
        media_type="text/markdown"
    )


@app.post("/api/cancel/{task_id}")
async def cancel_collection(task_id: str, client_request: Request):
    """ë…¼ë¬¸ ìˆ˜ì§‘ ì‘ì—… ì¤‘ë‹¨"""
    client_ip = client_request.client.host
    api_logger.info(f"ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ - Task ID: {task_id}, IP: {client_ip}")
    
    if not manager.get_task(task_id):
        api_logger.warning(f"ì‘ì—… ì¤‘ë‹¨ ì‹¤íŒ¨ - Task ID: {task_id}, IP: {client_ip} (ì‘ì—… ì—†ìŒ)")
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    success = manager.cancel_task(task_id)
    if success:
        # ì‘ì—… ìƒíƒœë¥¼ ì¤‘ë‹¨ë¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        manager.update_progress(task_id, CollectionProgress(
            status="cancelled",
            current_step="ì‘ì—…ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
            progress_percentage=0
        ))
        api_logger.info(f"ì‘ì—… ì¤‘ë‹¨ ì„±ê³µ - Task ID: {task_id}")
        return {"message": "ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.", "task_id": task_id}
    else:
        api_logger.error(f"ì‘ì—… ì¤‘ë‹¨ ì‹¤íŒ¨ - Task ID: {task_id}")
        raise HTTPException(status_code=400, detail="ì‘ì—…ì„ ì¤‘ë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


@app.delete("/api/tasks/{task_id}")
async def delete_task(task_id: str):
    """ì‘ì—… ì‚­ì œ"""
    if task_id not in collection_tasks:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    del collection_tasks[task_id]
    return {"message": "ì‘ì—…ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.get("/api/files")
async def get_paper_files():
    """ìƒì„±ëœ ë…¼ë¬¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    import os
    from pathlib import Path
    import re
    from datetime import datetime
    
    try:
        # ê°œë³„ ë…¼ë¬¸ íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
        papers_dir = Path("reports/individual_papers")
        if not papers_dir.exists():
            return {"files": [], "total": 0}
        
        files = []
        for file_path in papers_dir.glob("*.md"):
            try:
                # íŒŒì¼ ì •ë³´ ì½ê¸°
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
                title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
                korean_title_match = re.search(r'\*\*Korean Title:\*\* (.+)$', content, re.MULTILINE)
                keywords_match = re.search(r'## ğŸ·ï¸ í‚¤ì›Œë“œ\n\n(.+)', content, re.MULTILINE | re.DOTALL)
                authors_match = re.search(r'- \*\*Authors:\*\* (.+)$', content, re.MULTILINE)
                arxiv_id_match = re.search(r'- \*\*ArXiv ID:\*\* \[(.+?)\]', content)
                published_match = re.search(r'- \*\*Published:\*\* (.+)$', content, re.MULTILINE)
                category_match = re.search(r'- \*\*Category:\*\* (.+)$', content, re.MULTILINE)
                
                # í‚¤ì›Œë“œ íŒŒì‹±
                keywords = []
                if keywords_match:
                    keywords_text = keywords_match.group(1).split('\n')[0]
                    keywords = [kw.strip('`').strip() for kw in keywords_text.split('â€¢') if kw.strip() and 'keyword' not in kw.lower()]
                
                file_info = {
                    "filename": file_path.name,
                    "title": title_match.group(1) if title_match else file_path.stem,
                    "title_ko": korean_title_match.group(1) if korean_title_match else "",
                    "authors": authors_match.group(1).split(', ') if authors_match else [],
                    "arxiv_id": arxiv_id_match.group(1) if arxiv_id_match else "",
                    "published": published_match.group(1) if published_match else "",
                    "category": category_match.group(1) if category_match else "",
                    "keywords": keywords,
                    "file_size": file_path.stat().st_size,
                    "created_at": datetime.fromtimestamp(file_path.stat().st_ctime).isoformat(),
                    "modified_at": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                    "pdf_url": f"https://arxiv.org/pdf/{arxiv_id_match.group(1)}.pdf" if arxiv_id_match else "",
                    "arxiv_url": f"https://arxiv.org/abs/{arxiv_id_match.group(1)}" if arxiv_id_match else ""
                }
                files.append(file_info)
                
            except Exception as e:
                api_logger.error(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜ - {file_path}: {str(e)}")
                continue
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        files.sort(key=lambda x: x['modified_at'], reverse=True)
        
        api_logger.info(f"ë…¼ë¬¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {len(files)}ê°œ íŒŒì¼")
        return {"files": files, "total": len(files)}
        
    except Exception as e:
        api_logger.error(f"ë…¼ë¬¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/api/files/{filename}")
async def get_paper_file(filename: str):
    """íŠ¹ì • ë…¼ë¬¸ íŒŒì¼ ë‚´ìš© ì¡°íšŒ"""
    from pathlib import Path
    
    try:
        papers_dir = Path("reports/individual_papers")
        file_path = papers_dir / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return {"filename": filename, "content": content}
        
    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨ - {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/digest/daily")
async def generate_daily_digest(date: str = None):
    """ì¼ì¼ ë‹¤ì´ì œìŠ¤íŠ¸ ìƒì„±"""
    try:
        from datetime import datetime

        # Parse target date
        if date:
            try:
                target_date = datetime.strptime(date, "%Y%m%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid date format. Expected YYYYMMDD.")
        else:
            target_date = datetime.now()

        # Create digest generator
        digest_generator = DigestGenerator()

        # Generate and save digest
        digest_path = digest_generator.save_daily_digest(target_date)

        # Send notifications only if not already sent by webhook
        # Check if digest was created in the last minute (to avoid duplicate sends)
        import os
        from datetime import datetime, timedelta

        try:
            file_stat = os.path.getmtime(digest_path)
            file_time = datetime.fromtimestamp(file_stat)
            time_diff = datetime.now() - file_time

            # If file was created more than 1 minute ago, send notifications
            if time_diff > timedelta(minutes=1):
                notification_results = notification_manager.send_digest_notifications(
                    digest_file_path=digest_path,
                    digest_type="daily",
                    digest_date=target_date.strftime("%Y-%m-%d")
                )
                api_logger.info(f"Daily digest notifications sent: {notification_results}")
            else:
                api_logger.info("Daily digest was recently created - skipping duplicate notifications")
        except Exception as e:
            api_logger.error(f"Failed to send daily digest notifications: {e}")

        return {
            "success": True,
            "message": "Daily digest generated successfully",
            "digest_path": str(digest_path),
            "date": target_date.strftime("%Y-%m-%d")
        }

    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"Daily digest generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/digest/weekly")
async def generate_weekly_digest(date: str = None):
    """ì£¼ê°„ ë‹¤ì´ì œìŠ¤íŠ¸ ìƒì„±"""
    try:
        from datetime import datetime

        # Parse target date
        if date:
            try:
                target_date = datetime.strptime(date, "%Y%m%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid date format. Expected YYYYMMDD.")
        else:
            target_date = datetime.now()

        # Create digest generator
        digest_generator = DigestGenerator()

        # Generate and save digest
        digest_path = digest_generator.save_weekly_digest(target_date)

        # Send notifications
        try:
            notification_results = notification_manager.send_digest_notifications(
                digest_file_path=digest_path,
                digest_type="weekly",
                digest_date=target_date.strftime("%Y-%m-%d")
            )
            api_logger.info(f"Weekly digest notifications sent: {notification_results}")
        except Exception as e:
            api_logger.error(f"Failed to send weekly digest notifications: {e}")

        return {
            "success": True,
            "message": "Weekly digest generated successfully",
            "digest_path": str(digest_path),
            "date": target_date.strftime("%Y-%m-%d")
        }

    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"Weekly digest generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/digests")
async def list_digests():
    """ìƒì„±ëœ ë‹¤ì´ì œìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        from pathlib import Path

        digests_dir = Path("reports/digests")
        digests = []

        if digests_dir.exists():
            for file_path in digests_dir.glob("*.md"):
                try:
                    digest_info = {
                        "filename": file_path.name,
                        "path": str(file_path),
                        "modified_at": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                        "size": file_path.stat().st_size
                    }

                    # Determine type from filename
                    if "daily" in file_path.name:
                        digest_info["type"] = "daily"
                    elif "weekly" in file_path.name:
                        digest_info["type"] = "weekly"
                    else:
                        digest_info["type"] = "unknown"

                    digests.append(digest_info)

                except Exception as e:
                    api_logger.error(f"Error processing digest file {file_path}: {e}")
                    continue

        # Sort by modification time (newest first)
        digests.sort(key=lambda x: x['modified_at'], reverse=True)

        return {"digests": digests, "total": len(digests)}

    except Exception as e:
        api_logger.error(f"Failed to list digests: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Notification API endpoints
@app.post("/api/notifications/email/subscribe", response_model=dict)
async def subscribe_email(request: EmailSubscribeRequest):
    """ì´ë©”ì¼ êµ¬ë… ì¶”ê°€"""
    try:
        success = notification_manager.add_email_subscriber(
            request.email, request.digest_type
        )
        if success:
            return {"success": True, "message": f"Successfully subscribed {request.email} to {request.digest_type} digest"}
        else:
            return {"success": False, "message": "Email already subscribed"}
    except Exception as e:
        api_logger.error(f"Failed to subscribe email: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/notifications/email/unsubscribe", response_model=dict)
async def unsubscribe_email(request: EmailUnsubscribeRequest):
    """ì´ë©”ì¼ êµ¬ë… í•´ì œ"""
    try:
        success = notification_manager.remove_email_subscriber(
            request.email, request.digest_type
        )
        if success:
            return {"success": True, "message": f"Successfully unsubscribed {request.email} from {request.digest_type} digest"}
        else:
            return {"success": False, "message": "Email not found in subscription list"}
    except Exception as e:
        api_logger.error(f"Failed to unsubscribe email: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/notifications/email/subscribers", response_model=MailingListResponse)
async def get_email_subscribers():
    """ë©”ì¼ë§ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
    try:
        daily_subscribers = notification_manager.get_email_subscribers("daily")
        weekly_subscribers = notification_manager.get_email_subscribers("weekly")

        return MailingListResponse(
            daily_subscribers=daily_subscribers,
            weekly_subscribers=weekly_subscribers
        )
    except Exception as e:
        api_logger.error(f"Failed to get email subscribers: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# í‚¤ì›Œë“œ êµ¬ë… ê´€ë¦¬ API
# =============================================================================

@app.post("/api/notifications/keywords/subscribe", response_model=KeywordSubscriptionResponse)
async def subscribe_keywords(request: KeywordSubscribeRequest):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì´ë©”ì¼ êµ¬ë…"""
    try:
        success = keyword_subscription_manager.add_subscription(
            email=str(request.email),
            keywords=request.keywords,
            digest_type=request.digest_type
        )

        if success:
            subscription = keyword_subscription_manager.get_subscription(str(request.email))
            return KeywordSubscriptionResponse(
                success=True,
                message="í‚¤ì›Œë“œ êµ¬ë…ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€/ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                subscription=KeywordSubscriptionInfo(
                    email=subscription.email,
                    keywords=subscription.keywords,
                    digest_type=subscription.digest_type,
                    created_at=subscription.created_at,
                    updated_at=subscription.updated_at,
                    active=subscription.active
                )
            )
        else:
            return KeywordSubscriptionResponse(
                success=False,
                message="í‚¤ì›Œë“œ êµ¬ë… ì¶”ê°€/ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            )
    except Exception as e:
        api_logger.error(f"Failed to subscribe keywords: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/notifications/keywords/unsubscribe", response_model=KeywordSubscriptionResponse)
async def unsubscribe_keywords(email: str):
    """í‚¤ì›Œë“œ êµ¬ë… í•´ì œ"""
    try:
        success = keyword_subscription_manager.remove_subscription(email)

        if success:
            return KeywordSubscriptionResponse(
                success=True,
                message="í‚¤ì›Œë“œ êµ¬ë…ì´ ì„±ê³µì ìœ¼ë¡œ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            )
        else:
            return KeywordSubscriptionResponse(
                success=False,
                message="í•´ë‹¹ ì´ë©”ì¼ì˜ êµ¬ë…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    except Exception as e:
        api_logger.error(f"Failed to unsubscribe keywords: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/notifications/keywords/subscription/{email}", response_model=KeywordSubscriptionResponse)
async def get_keyword_subscription(email: str):
    """íŠ¹ì • ì‚¬ìš©ìì˜ í‚¤ì›Œë“œ êµ¬ë… ì •ë³´ ì¡°íšŒ"""
    try:
        subscription = keyword_subscription_manager.get_subscription(email)

        if subscription:
            return KeywordSubscriptionResponse(
                success=True,
                message="êµ¬ë… ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
                subscription=KeywordSubscriptionInfo(
                    email=subscription.email,
                    keywords=subscription.keywords,
                    digest_type=subscription.digest_type,
                    created_at=subscription.created_at,
                    updated_at=subscription.updated_at,
                    active=subscription.active
                )
            )
        else:
            return KeywordSubscriptionResponse(
                success=False,
                message="í•´ë‹¹ ì´ë©”ì¼ì˜ êµ¬ë…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    except Exception as e:
        api_logger.error(f"Failed to get keyword subscription: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/notifications/keywords/subscribers", response_model=KeywordSubscriptionsResponse)
async def get_keyword_subscribers(digest_type: str = None):
    """í‚¤ì›Œë“œ êµ¬ë…ì ëª©ë¡ ì¡°íšŒ"""
    try:
        subscriptions = keyword_subscription_manager.get_all_subscriptions(digest_type=digest_type)

        subscription_infos = [
            KeywordSubscriptionInfo(
                email=sub.email,
                keywords=sub.keywords,
                digest_type=sub.digest_type,
                created_at=sub.created_at,
                updated_at=sub.updated_at,
                active=sub.active
            )
            for sub in subscriptions
        ]

        return KeywordSubscriptionsResponse(
            subscriptions=subscription_infos,
            total_count=len(subscription_infos)
        )
    except Exception as e:
        api_logger.error(f"Failed to get keyword subscribers: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/notifications/status", response_model=NotificationStatus)
async def get_notification_status():
    """ì•Œë¦¼ ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
    try:
        status = notification_manager.get_notification_status()
        return NotificationStatus(**status)
    except Exception as e:
        api_logger.error(f"Failed to get notification status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/notifications/test", response_model=dict)
async def test_notifications(request: NotificationTestRequest):
    """ì•Œë¦¼ ì„œë¹„ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        results = {}

        if request.test_email:
            results["email"] = notification_manager.email_service.test_connection()

        if request.test_slack:
            results["slack"] = notification_manager.slack_service.test_connection()

        if request.test_webhooks:
            results["webhooks"] = notification_manager.webhook_service.test_webhooks()

        return {
            "success": True,
            "results": results,
            "message": "Test completed"
        }
    except Exception as e:
        api_logger.error(f"Failed to test notifications: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/notifications/send/digest", response_model=NotificationResult)
async def send_digest_notifications(
    digest_file: str,
    digest_type: str = "daily",
    digest_date: str = None,
    settings: NotificationSettingsRequest = None
):
    """ë‹¤ì´ì œìŠ¤íŠ¸ ì•Œë¦¼ ì „ì†¡"""
    try:
        from pathlib import Path

        digest_path = Path(digest_file)
        if not digest_path.exists():
            raise HTTPException(status_code=404, detail="Digest file not found")

        # ì•Œë¦¼ ì „ì†¡ ì„¤ì •
        send_email = settings.send_email if settings else None
        send_slack = settings.send_slack if settings else None
        send_webhooks = settings.send_webhooks if settings else True
        custom_recipients = settings.custom_email_recipients if settings else None

        # ì•Œë¦¼ ì „ì†¡
        results = notification_manager.send_digest_notifications(
            digest_file_path=digest_path,
            digest_type=digest_type,
            digest_date=digest_date,
            send_email=send_email,
            send_slack=send_slack,
            send_webhooks=send_webhooks,
            custom_email_recipients=custom_recipients
        )

        return NotificationResult(**results)

    except Exception as e:
        api_logger.error(f"Failed to send digest notifications: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# RAG Search API endpoints
@app.post("/api/search/papers", response_model=PaperSearchResponse)
async def search_papers(request: PaperSearchRequest):
    """ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ë…¼ë¬¸ ê²€ìƒ‰"""
    try:
        if not rag_service:
            raise HTTPException(status_code=503, detail="RAG search service not available")

        # Parse dates if provided
        start_date = None
        end_date = None

        if request.start_date:
            try:
                from datetime import datetime
                start_date = datetime.strptime(request.start_date, "%Y-%m-%d").date()
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid start_date format. Use YYYY-MM-DD")

        if request.end_date:
            try:
                from datetime import datetime
                end_date = datetime.strptime(request.end_date, "%Y-%m-%d").date()
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid end_date format. Use YYYY-MM-DD")

        # Perform search
        results = rag_service.search_papers(
            query=request.query,
            top_k=request.top_k,
            min_similarity=request.min_similarity,
            categories=request.categories,
            start_date=start_date,
            end_date=end_date,
            page=request.page,
            page_size=request.page_size
        )

        api_logger.info(f"Paper search completed: '{request.query}' -> {results['total_results']} results")
        return PaperSearchResponse(**results)

    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"Paper search failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/search/similar", response_model=SimilarPapersResponse)
async def find_similar_papers(request: SimilarPapersRequest):
    """íŠ¹ì • ë…¼ë¬¸ê³¼ ìœ ì‚¬í•œ ë…¼ë¬¸ ê²€ìƒ‰"""
    try:
        if not rag_service:
            raise HTTPException(status_code=503, detail="RAG search service not available")

        results = rag_service.find_similar_papers(
            arxiv_id=request.arxiv_id,
            top_k=request.top_k,
            min_similarity=request.min_similarity
        )

        api_logger.info(f"Similar papers search completed: {request.arxiv_id} -> {results['total_found']} results")
        return SimilarPapersResponse(**results)

    except Exception as e:
        api_logger.error(f"Similar papers search failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/search/stats", response_model=DatabaseStatsResponse)
async def get_search_stats():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
    try:
        if not rag_service:
            raise HTTPException(status_code=503, detail="RAG search service not available")

        stats = rag_service.get_database_stats()
        return DatabaseStatsResponse(**stats)

    except Exception as e:
        api_logger.error(f"Failed to get search stats: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/search/suggestions")
async def get_query_suggestions(query: str):
    """ê²€ìƒ‰ ì¿¼ë¦¬ ê°œì„  ì œì•ˆ"""
    try:
        if not rag_service:
            raise HTTPException(status_code=503, detail="RAG search service not available")

        suggestions = rag_service.suggest_query_improvements(query)
        return {"query": query, "suggestions": suggestions}

    except Exception as e:
        api_logger.error(f"Failed to get query suggestions: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Hybrid Search API endpoints
@app.post("/api/search/hybrid", response_model=HybridSearchResponse)
async def hybrid_search(request: HybridSearchRequest):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„° ê²€ìƒ‰)"""
    try:
        if not hybrid_service:
            raise HTTPException(status_code=503, detail="Hybrid search service not available")

        # Parse dates if provided
        start_date = None
        end_date = None

        if request.start_date:
            try:
                from datetime import datetime
                start_date = datetime.strptime(request.start_date, "%Y-%m-%d").date()
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid start_date format. Use YYYY-MM-DD")

        if request.end_date:
            try:
                from datetime import datetime
                end_date = datetime.strptime(request.end_date, "%Y-%m-%d").date()
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid end_date format. Use YYYY-MM-DD")

        # Perform hybrid search
        results = hybrid_service.hybrid_search(
            query=request.query,
            keyword_weight=request.keyword_weight,
            vector_weight=request.vector_weight,
            top_k=request.top_k,
            min_similarity=request.min_similarity,
            categories=request.categories,
            start_date=start_date,
            end_date=end_date,
            page=request.page,
            page_size=request.page_size
        )

        api_logger.info(f"Hybrid search completed: '{request.query}' (kw:{request.keyword_weight:.2f}, vec:{request.vector_weight:.2f}) -> {results['total_results']} results")
        return HybridSearchResponse(**results)

    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"Hybrid search failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/search/hybrid/stats", response_model=HybridStatsResponse)
async def get_hybrid_search_stats():
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ"""
    try:
        if not hybrid_service:
            raise HTTPException(status_code=503, detail="Hybrid search service not available")

        stats = hybrid_service.get_search_stats()
        return HybridStatsResponse(**stats)

    except Exception as e:
        api_logger.error(f"Failed to get hybrid search stats: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "active_tasks": len([
            t for t in collection_tasks.values()
            if t.status in ["created", "searching", "collecting", "processing"]
        ])
    }


def run_server(host: str = "localhost", port: int = 8080):
    """ì„œë²„ ì‹¤í–‰"""
    uvicorn.run(app, host=host, port=port)


if __name__ == "__main__":
    run_server()